Kind: captions
Language: en
hey we are
live hey you
live hey you
guys uh I found something freaking
crazy um it turns out all the stuff that
crazy um it turns out all the stuff that
I was talking about on stream yesterday
I was talking about on stream yesterday
I was
I was
right and uh Atari is
right and uh Atari is
broken because of a stupid
broken because of a stupid
rapper that was proposed
rapper that was proposed
in order to address a hypothetical issue
in order to address a hypothetical issue
that was never shown to actually happen
that was never shown to actually happen
in
in
practice
practice
Yeah so
Yeah so
uh this is just going to be a short
uh this is just going to be a short
little stream because it's late I've
little stream because it's late I've
been working on other things all day but
been working on other things all day but
uh I'm going to look at these
uh I'm going to look at these
experiments I'm going to finish writing
experiments I'm going to finish writing
this article on stream I'm going to post
this article on stream I'm going to post
this article in the morning tomorrow and
this article in the morning tomorrow and
I'll stream all day
I'll stream all day
tomorrow um but today we're just going
tomorrow um but today we're just going
to finish the article going to hang out
to finish the article going to hang out
here for like an hour
here for like an hour
maybe and we're going to look at some
maybe and we're going to look at some
experiments we're going to relaunch some
experiments we're going to relaunch some
new
new
experiments and we're going to take it
experiments and we're going to take it
from
there like I don't know how this happens
there like I don't know how this happens
that I keep finding errors in like huge
that I keep finding errors in like huge
papers that have just gone unnoticed for
papers that have just gone unnoticed for
years like they're not okay they're kind
years like they're not okay they're kind
of subtle to be fair it's broken yeah
of subtle to be fair it's broken yeah
it's
it's
broken look this is this is
pong here you know what we'll show you
pong here you know what we'll show you
pong before and
after
after
um is this
um is this
it yeah uh
it yeah uh
wait hold on let me make sure I have the
wait hold on let me make sure I have the
right
right
graphs uh nope these aren't the right
graphs uh nope these aren't the right
ones we have even better
ones we have even better
ones I think this one
okay here's pong fixed right this what
okay here's pong fixed right this what
it's supposed to be really easy
it's supposed to be really easy
environment you solve
environment you solve
it and here is
it and here is
pong with the Terrible Bad rapper
thing broken
600 plus citation
paper so I'm going to just finish out
paper so I'm going to just finish out
writing this article on stream because I
writing this article on stream because I
may as
well uh I'll go through it a little bit
well uh I'll go through it a little bit
on stream because may as
on stream because may as
well so the problem is sticky
well so the problem is sticky
actions uh they're real bad
actions uh they're real bad
sticky actions mean that when your agent
sticky actions mean that when your agent
takes an action it holds the key
takes an action it holds the key
down sometimes so it makes you repeat
down sometimes so it makes you repeat
actions and uh this was proposed as a
actions and uh this was proposed as a
method to break
method to break
determinism so you couldn't just
determinism so you couldn't just
memorize the optimal thing to do and the
memorize the optimal thing to do and the
author said that this doesn't break dqn
author said that this doesn't break dqn
this doesn't break like actual learning
this doesn't break like actual learning
algorithms but actually you can prove
algorithms but actually you can prove
based on the data in the original paper
based on the data in the original paper
that it
that it
does and uh it replicates on PO
does and uh it replicates on PO
replicates on my latest you know all the
replicates on my latest you know all the
new
new
infrastructure and it totally breaks
infrastructure and it totally breaks
like one of the easiest environments out
like one of the easiest environments out
there with this modification it makes it
there with this modification it makes it
I don't know if it's impossible it makes
I don't know if it's impossible it makes
it a heck of a lot harder and if it is
it a heck of a lot harder and if it is
possible it's not happening with this
possible it's not happening with this
giant hyper parameter sweep I'm running
so welcome YouTube
so welcome YouTube
folks I'm just going to finish this
folks I'm just going to finish this
article up on stream tonight
article up on stream tonight
uh I figured I could stream writing this
uh I figured I could stream writing this
in like chat and hanging out or I could
in like chat and hanging out or I could
not stream it so figured I'd stream it
not stream it so figured I'd stream it
uh let me see let me finish I think I
uh let me see let me finish I think I
had yeah I was working on just fixing up
had yeah I was working on just fixing up
this
this
figure hold
on
cubert so I need the original Atari
cubert so I need the original Atari
environment out of this which is
like
here cqu Quest is there any other ones
here cqu Quest is there any other ones
one two 3 four
five wait
five wait
six yeah just Space
Invaders let's grab space Vaders and
Invaders let's grab space Vaders and
then I'll tell you why this chart is um
then I'll tell you why this chart is um
so
important I guess I should have expected
important I guess I should have expected
a few people to uh top in the Stream
a few people to uh top in the Stream
when I make a title like this huh W
when I make a title like this huh W
welcome folks RL is in fact broken I
welcome folks RL is in fact broken I
have in fact fixed
it specifically Atari which is the most
it specifically Atari which is the most
widely used Benchmark out there right it
widely used Benchmark out there right it
is
is
broken it's broken by a dumb rapper
broken it's broken by a dumb rapper
that's become the
that's become the
standard it's very
sad let me get this finish this chart up
sad let me get this finish this chart up
think if I just cut it
think if I just cut it
here yeah there we go was there a
here yeah there we go was there a
caption that I'm
caption that I'm
missing no that's
missing no that's
it
it
so uh save
so uh save
DQ uh
DQ uh
aado download
save so as soon as I get this as soon as
save so as soon as I get this as soon as
I crop this figure and put it in the
I crop this figure and put it in the
article I'll explain
article I'll explain
um I'll explain what the data
um I'll explain what the data
this will be in the article tomorrow
this will be in the article tomorrow
but you know you'll get a early preview
but you know you'll get a early preview
of
of
it uh
it uh
edit and then I just do
wait
wait
save okay
dqn
makato where my table
makato where my table
go
what come
on okay there we go
so the data
so the data
here is the wrapper that they say does
here is the wrapper that they say does
not hurt
not hurt
performance and this is their result
performance and this is their result
they say look we run it deterministic
they say look we run it deterministic
which is normal we run it stochastic
which is normal we run it stochastic
which is with our wrapper and this is a
which is with our wrapper and this is a
dumb Baseline that we expect to be
dumb Baseline that we expect to be
broken by stochasticity it's broken dqn
broken by stochasticity it's broken dqn
the results don't go
the results don't go
down but the thing is you can go look at
down but the thing is you can go look at
the original dqn results which has not
the original dqn results which has not
just three these three environments but
just three these three environments but
has seven original environments and then
has seven original environments and then
you can go down to the makato paper
you can go down to the makato paper
which has full Atari experiment results
which has full Atari experiment results
with uh their wrapper and you can see
with uh their wrapper and you can see
that in their paper they do not do well
that in their paper they do not do well
on pong or breakout which of which are
on pong or breakout which of which are
two of the best known and most
two of the best known and most
consistent environments out there they
consistent environments out there they
substantially underperform dqm so in
substantially underperform dqm so in
fact if you look at all seven
fact if you look at all seven
environments and not just the three that
environments and not just the three that
they happen to pick you can see that
they happen to pick you can see that
clearly this wrapper does in fact
clearly this wrapper does in fact
degrade the performance of dqn
degrade the performance of dqn
and then what I'm going to continue
and then what I'm going to continue
writing here the experiments that I did
writing here the experiments that I did
uh this replicates on PO this replicates
uh this replicates on PO this replicates
on like the latest RL infrastructure if
on like the latest RL infrastructure if
you try to run pong with and you run a
you try to run pong with and you run a
big hyper parameter sweep right to make
big hyper parameter sweep right to make
absolutely certain of everything which
absolutely certain of everything which
is more than they did um if you do that
is more than they did um if you do that
then it pong is like a trivial
then it pong is like a trivial
environment without sticky actions and
environment without sticky actions and
thus far I've run like aund something
thus far I've run like aund something
experiments and it hasn't been able to
experiments and it hasn't been able to
do above 15ish with the the sticky
do above 15ish with the the sticky
actions which is actually the score that
actions which is actually the score that
they report as well so there you
they report as well so there you
go this broke reinforcement
learning how does 30 to 40,000 step per
learning how does 30 to 40,000 step per
second Atari training on one
second Atari training on one
GPU um
GPU um
with without a dumb rapper that makes
with without a dumb rapper that makes
the environments hard how's that sound
the environments hard how's that sound
that's what we've got in puffer now
gonna be real
nice okay comparing this to the original
nice okay comparing this to the original
DEC
results
results
substantially under perform on
substantially under perform on
pong and break
out anything else they perform
on Enduro is
on Enduro is
470 oh they do fine on
470 oh they do fine on
Enduro
cubert oh they don't do well on cubert
cubert oh they don't do well on cubert
do
do
they no wait they're good on cubert
yeah so it's
yeah so it's
mixed that's fine
are two of the best known most
are two of the best known most
consistent Atari environments
uh let's
see as further
see as further
Evidence I ran a
Evidence I ran a
large paper parameter
large paper parameter
sweep on
sweep on
Hong using puffer
Hong using puffer
Libs Popo
plus
plus
lstm
mation which
is with and without sticky
is with and without sticky
actions the environment is solved almost
actions the environment is solved almost
immediately without sticky
but only gets a
but only gets a
maximum of around I think it's 16
score yeah 16
score
score
score with them which closely matches
score with them which closely matches
kados
Al
Al
reported
performance let's go copy some
performance let's go copy some
graphs
um
this we put the
this we put the
filter let's see if it looks better with
filter let's see if it looks better with
the filter or without it
this looks
nice we'll do like
this let's let's refresh the page so we
this let's let's refresh the page so we
get this nicely
this is going to be a very nice finding
this is going to be a very nice finding
because this is going to make it so much
because this is going to make it so much
easier to do uh like I think Atari is
easier to do uh like I think Atari is
actually going to be reasonably usable I
actually going to be reasonably usable I
never thought I would say that but with
never thought I would say that but with
how fast I have it training now and
how fast I have it training now and
without this dumb sticky action thing
without this dumb sticky action thing
polluting results
polluting results
like pretty
possible for folks unfamiliar sticky
possible for folks unfamiliar sticky
actions are like this is something that
actions are like this is something that
basically anybody everybody who's worked
basically anybody everybody who's worked
in RL for any period of time knows about
in RL for any period of time knows about
like for this thing to be wrong is
nuts frankly I'd rather that somebody
nuts frankly I'd rather that somebody
like I'd rather that I'm wrong about
like I'd rather that I'm wrong about
this somehow though I don't see how that
this somehow though I don't see how that
would be possible like I'd rather that I
would be possible like I'd rather that I
somehow be wrong about this and sticky
somehow be wrong about this and sticky
actions not be wrong just because of the
actions not be wrong just because of the
amount of work this will
overturn let me just make absolutely
overturn let me just make absolutely
sure that I have the parameter right so
sure that I have the parameter right so
I use this
I use this
graph if I just click any of
graph if I just click any of
these and I look for the sticky action
parameter where is it it's not in train
parameter where is it it's not in train
it's in
it's in
M repeat action prob zero so no sticky
M repeat action prob zero so no sticky
actions all that's
right sticky actions don't make a
right sticky actions don't make a
difference oh they make a huge
difference oh they make a huge
difference they make a huge difference
difference they make a huge difference
difference I'll show you the two figures
difference I'll show you the two figures
side by
side by
side sticky it's like this this article
side sticky it's like this this article
is literally titled sticky actions
is literally titled sticky actions
considered harmful that's how bad it
is now it's not going to be for all
is now it's not going to be for all
environments obviously but the key is
environments obviously but the key is
that for some environments yeah what did
that for some environments yeah what did
you find out okay
you find out okay
so here is can I make this
so here is can I make this
larger I think I can just do it like
larger I think I can just do it like
this
this
right okay so here's pong full sweep
right okay so here's pong full sweep
without sticky actions immediately
without sticky actions immediately
solves the environment 20 is Max
solves the environment 20 is Max
score here's a full Sweep with sticky
score here's a full Sweep with sticky
actions doesn't solve the environment
actions doesn't solve the environment
score caps around 16 and if you go into
score caps around 16 and if you go into
the original makado paper uh they omit
the original makado paper uh they omit
pong results and break out another M's
pong results and break out another M's
from their reasoning uh is to why sticky
from their reasoning uh is to why sticky
actions don't hurt algorithmic
actions don't hurt algorithmic
performance but if you look at their
performance but if you look at their
original paper I have the exerpts in
original paper I have the exerpts in
this article you can actually see that
this article you can actually see that
they do not solve pong they get around
they do not solve pong they get around
the same score
the same score
15ish uh with their implementation
15ish uh with their implementation
compared to the original dqn solving it
compared to the original dqn solving it
at 20 and they also substantially under
at 20 and they also substantially under
perform on breakout which is a very
perform on breakout which is a very
consistent task uh compared to original
consistent task uh compared to original
dqn at 168 they get like half the score
dqn at 168 they get like half the score
so crazy finding
absolutely crazy
finding get this out of the way so I can
finding get this out of the way so I can
have it on the stream
nice random question how do you think
nice random question how do you think
Tesla SD
Tesla SD
works oh man that's a way longer topic
there I think is the talk public Andre
there I think is the talk public Andre
gave a really good talk about it when he
gave a really good talk about it when he
was a chief
was a chief
scientist he gave a really good talk at
scientist he gave a really good talk at
MIT I don't know if this if it was made
MIT I don't know if this if it was made
public you might be able to find it
public you might be able to find it
online
um it's changed a lot since then I'm
um it's changed a lot since then I'm
sure but
there's a good on YouTube yeah I mean
there's a good on YouTube yeah I mean
that's a beast of a topic it's not just
that's a beast of a topic it's not just
straight reinforcement
straight reinforcement
learning it's like imitation for the
learning it's like imitation for the
most part
nowadays I do think that FSD is the most
nowadays I do think that FSD is the most
impressive
impressive
deployed um like ml application in
deployed um like ml application in
general
okay let me make sure I title this right
I don't know sometimes PA gives you 21
I don't know sometimes PA gives you 21
score I'm pretty sure the max is 20 so I
score I'm pretty sure the max is 20 so I
don't know what's up with that
don't know what's up with that
but I think that's like a well-known
odity uh
currently Nas working on backend systems
currently Nas working on backend systems
thinking getting AI not sure where to
thinking getting AI not sure where to
start how much math is required depends
start how much math is required depends
where an AI you work either not very
where an AI you work either not very
much or a ton in the type of work I do
much or a ton in the type of work I do
my playbook I always say is about 5%
my playbook I always say is about 5%
math 15% experimental science and 80%
math 15% experimental science and 80%
straight
engineering you should probably at least
engineering you should probably at least
know Matrix calculus which is just
know Matrix calculus which is just
normal calculus when you replace the
normal calculus when you replace the
variables with a matrix and occasionally
variables with a matrix and occasionally
have to flip
have to flip
one what exactly do sticky actions do in
one what exactly do sticky actions do in
reinforcement learning okay so if
reinforcement learning okay so if
anybody if you've played games ever it's
anybody if you've played games ever it's
something that's incredibly stupid and
something that's incredibly stupid and
like is immediately obvious that would
like is immediately obvious that would
mess you up it makes your button sticky
mess you up it makes your button sticky
so
like if you press the button and then
like if you press the button and then
you go to press another button sometimes
you go to press another button sometimes
the first button will stick down for a
the first button will stick down for a
little bit and won't let you press the
little bit and won't let you press the
other button for a second so
other button for a second so
like obviously that's going to get you
like obviously that's going to get you
killed in a lot of games
right it's actually kind of obvious when
right it's actually kind of obvious when
you think about it for pong like you in
you think about it for pong like you in
order to solve pong you have to win 20
order to solve pong you have to win 20
straight
straight
games
games
right so like yeah of course over 20
right so like yeah of course over 20
games like having your buttons be sticky
games like having your buttons be sticky
is going to mess you up on a few of them
is going to mess you up on a few of them
for
that doesn't sound useful why is it even
that doesn't sound useful why is it even
a feature in for this model it's like
a feature in for this model it's like
the typical scientist thing like this is
the typical scientist thing like this is
like such an academic thing so the
like such an academic thing so the
justification for this was that uh if
justification for this was that uh if
you since a Atari is deterministic you
you since a Atari is deterministic you
can construct a really stupid algorithm
can construct a really stupid algorithm
that just memorizes the optimal sequence
that just memorizes the optimal sequence
of actions right but if you mess with
of actions right but if you mess with
the buttons every so often then you're
the buttons every so often then you're
not going to be able to just memorize
not going to be able to just memorize
stuff because you're going to go off of
stuff because you're going to go off of
like the exact sequence of actions that
like the exact sequence of actions that
you memorized right and then they did
you memorized right and then they did
some experiments that show that ah the
some experiments that show that ah the
sticky action thing doesn't seem to hurt
sticky action thing doesn't seem to hurt
they didn't do comprehensive experiments
they didn't do comprehensive experiments
despite later the same authors is
despite later the same authors is
writing a paper about how RL is all
writing a paper about how RL is all
wrong because nobody does comprehensive
wrong because nobody does comprehensive
experiments and then this just like got
experiments and then this just like got
600 citations and got put into the
600 citations and got put into the
default rappers of everything and is now
default rappers of everything and is now
making all of like RL
wrong it's like such a like you know
wrong it's like such a like you know
like scientist who's never played games
like scientist who's never played games
kind of a thing as well cuz if you think
kind of a thing as well cuz if you think
about that for 2 seconds like this is
about that for 2 seconds like this is
how I figured out that this was wrong in
how I figured out that this was wrong in
the first place right like I was getting
the first place right like I was getting
slightly weird results on pong and then
slightly weird results on pong and then
I noticed sticky actions were default
I noticed sticky actions were default
and I just went like
and I just went like
huh sticky action seems like it would
huh sticky action seems like it would
make it really hard to play a game
right FD
that's not Ramy Ismail like the
that's not Ramy Ismail like the
well-known Game Dev right I
well-known Game Dev right I
assume
fdsp what's
fdsp isn't determinism
fdsp isn't determinism
bad
um sticky actions might act as a
um sticky actions might act as a
regularizer yeah but in this case it
regularizer yeah but in this case it
provably breaks the
provably breaks the
environment so
so it's a sucky yeah but it's not just a
so it's a sucky yeah but it's not just a
sucky regularizer it's a sucky
sucky regularizer it's a sucky
regularizer that's baked into the
regularizer that's baked into the
environment is a default setting when
environment is a default setting when
you instantiate so when you like make
you instantiate so when you like make
the environment by default this thing is
the environment by default this thing is
enabled what's the tldr in the story uh
enabled what's the tldr in the story uh
default rapper in the most common
default rapper in the most common
Benchmark in exist distance makes the
Benchmark in exist distance makes the
environments
unsolvable where's entropy in here
entropy is
entropy is
okay and with this
one oh
so I need to include these loss curves
right for
oh I this needs to go down
here a really low resed
here a really low resed
screenshot welcome everyone
screenshot welcome everyone
that is a lot of people on YouTube it
that is a lot of people on YouTube it
seems when you make a really clickbait
seems when you make a really clickbait
though true title people show up um the
though true title people show up um the
tldr for folks that just got here I
tldr for folks that just got here I
found a bug in the default pre the I
found a bug in the default pre the I
found that the default processor used
found that the default processor used
for Atari that is proposed in a landmark
for Atari that is proposed in a landmark
paper and is said not to degrade
paper and is said not to degrade
performance during training
performance during training
significantly degrades performance
significantly degrades performance
during training for at least one
during training for at least one
environment probably multiple
environment probably multiple
based on both my own extensive
based on both my own extensive
experiments and the data presented in
experiments and the data presented in
the original paper when you cross check
the original paper when you cross check
it against uh cross check their appendix
it against uh cross check their appendix
against the paper that they refer to so
against the paper that they refer to so
yeah this is a big deal um this article
yeah this is a big deal um this article
will be published tomorrow I will talk
will be published tomorrow I will talk
about it a little bit on stream it will
about it a little bit on stream it will
be on my ex which let me I'll post the
be on my ex which let me I'll post the
link uh because it will be
link uh because it will be
here and if you are generally interested
here and if you are generally interested
in my work which is attempting to make
in my work which is attempting to make
reinforcement learning sane easy and
reinforcement learning sane easy and
usable all the stuff that I'm working on
usable all the stuff that I'm working on
including the stuff I use for the
including the stuff I use for the
experiments is available in poer lib
experiments is available in poer lib
this is the main library that I develop
this is the main library that I develop
it's all free and open source if you
it's all free and open source if you
want to help me out a whole ton give it
want to help me out a whole ton give it
a star it really helps me out that's the
a star it really helps me out that's the
TA drr going to finish up working on
TA drr going to finish up working on
this article here um cuz so another
this article here um cuz so another
thing this was got a general question on
thing this was got a general question on
puffer lib seem you leverage syon to
puffer lib seem you leverage syon to
speed up your M's why did you choose
speed up your M's why did you choose
that over a number jack or going end to
that over a number jack or going end to
endend on the GPU with something like
endend on the GPU with something like
Puda uh because Jax is a domain specific
Puda uh because Jax is a domain specific
language which heavily restricts the
language which heavily restricts the
type of environments that you can make
type of environments that you can make
I'm able to get millions of steps per
I'm able to get millions of steps per
second single threaded with way way way
second single threaded with way way way
simpler code for much more complex
simpler code for much more complex
simulations I have a miniature version
simulations I have a miniature version
of DOTA that is about 1300 lines of
of DOTA that is about 1300 lines of
scyon that essentially looks like normal
scyon that essentially looks like normal
python code it runs at a million steps
python code it runs at a million steps
per second single-threaded I've got this
per second single-threaded I've got this
massively multi-agent snake that's like
massively multi-agent snake that's like
200 lines of scon looks like normal
200 lines of scon looks like normal
python runs it over 10 million steps per
python runs it over 10 million steps per
second single threaded all without
second single threaded all without
having to write your code in a weird
having to write your code in a weird
like vector domain specific language
like vector domain specific language
that is why I actually have an article
that is why I actually have an article
on this as well uh this is more of a
on this as well uh this is more of a
bait article honestly but people liked
it where's this
here's this
here's this
article I am not a fan of Jax as a
article I am not a fan of Jax as a
general replacement for uh environment
general replacement for uh environment
infrastructure in RL I think it is a
infrastructure in RL I think it is a
major step in the wrong direction Jax is
major step in the wrong direction Jax is
incredibly useful for physics stuff and
incredibly useful for physics stuff and
for like specific aife work not as a
for like specific aife work not as a
general replacement for environment code
let me grab that
let me grab that
figure how do I get this to
be this give me reasonable font
sizes yeah there we
sizes yeah there we
go R
go R
entropy so here's the entropy loss you
entropy so here's the entropy loss you
can see that this is stable it's not
can see that this is stable it's not
crashing
entropy and let me make sure I grabbed
entropy and let me make sure I grabbed
the right chart as
the right chart as
well gotcha that's very clarifying and
well gotcha that's very clarifying and
for number is the idea since the code is
for number is the idea since the code is
running in C there's nothing to gain
running in C there's nothing to gain
from numbers jit yes my environments
from numbers jit yes my environments
will run uh like the mobile runs in pure
will run uh like the mobile runs in pure
C with no calls back to
C with no calls back to
python at least know the init is slow
python at least know the init is slow
but who cares if the init is slow I
but who cares if the init is slow I
didn't bother optimizing it but like the
didn't bother optimizing it but like the
step function will run at Native C
speed y I did this right right
let me uh filter
let me uh filter
this
this
by uh let me filter this by performance
by uh let me filter this by performance
of over
10 another good way of showing the uh
10 another good way of showing the uh
the jack thing is just like go read the
the jack thing is just like go read the
uh the environment simulation code in
uh the environment simulation code in
the dev Branch for like some of my M's
the dev Branch for like some of my M's
and you'll see how easy it is compared
and you'll see how easy it is compared
to trying to like wrap your head around
to trying to like wrap your head around
how to do everything in a ray ops it's
how to do everything in a ray ops it's
really easy
1 e- 4 to 5 eus
1 e- 4 to 5 eus
3 for
IUS 3
IUS 3
right 6us
3 where's the minimum
3 where's the minimum
value what was the minimum entropy value
value what was the minimum entropy value
I allowed
default I want to make sure that I
default I want to make sure that I
didn't I allowed uh pretty low entropy I
didn't I allowed uh pretty low entropy I
think I allowed pretty darn low entropy
right yeah one minus 5 that's what I
right yeah one minus 5 that's what I
thought
oh wait wait wait I did this backwards
oh wait wait wait I did this backwards
yeah yeah I got to do the other one
yeah yeah I got to do the other one
right
right
cuz yes I I have to do the other one
cuz yes I I have to do the other one
okay that was close almost screwed that
okay that was close almost screwed that
up that would have been very very
up that would have been very very
embarrassing and it's about the
same so we'll
same so we'll
do environment return greater than equal
do environment return greater than equal
to
to
15 got to be careful if you're going to
15 got to be careful if you're going to
claim that everything's broken you got
claim that everything's broken you got
to make sure that you do your analys say
to make sure that you do your analys say
bulletproof
right e
1.3 to
1.5
1.0 1.0 to
1.0 1.0 to
1.5 1.4
okay that was
close
so I need to increase the size of this
so I need to increase the size of this
right
at 125%
okay so based on this analysis right
the the best
the the best
runs in this one which is
runs in this one which is
the uh this is the the good
the uh this is the the good
one yes so we find entropy values of
one yes so we find entropy values of
around
around
0.9 to
0.9 to
1.4 compared to
1.4 compared to
1.3 to
1.5 honestly I'm shocked that sticky
1.5 honestly I'm shocked that sticky
actions only they don't always hurt
actions only they don't always hurt
training they don't always hurt
training they don't always hurt
training in fact it's kind of weird um
training in fact it's kind of weird um
looks like they do better on some some
looks like they do better on some some
enes but the thing is the claim was that
enes but the thing is the claim was that
sticky actions do not hurt training that
sticky actions do not hurt training that
claim is wrong and they are incredibly
claim is wrong and they are incredibly
harmful to some
environments and if you're going to use
environments and if you're going to use
sticky actions okay here's a really
sticky actions okay here's a really
simple thing right if you're going to
simple thing right if you're going to
use sticky actions you sweep it as a
use sticky actions you sweep it as a
hyperparameter you don't fix it across
hyperparameter you don't fix it across
all the environments and you especially
all the environments and you especially
don't fix it to a crazy high value like
don't fix it to a crazy high value like
0.25 which is the default one in four
0.25 which is the default one in four
button presses the button sticks
down isn't that
down isn't that
crazy I mean I'm sure that like there
crazy I mean I'm sure that like there
probably at least a few people in the
probably at least a few people in the
audience here who do not play games so I
audience here who do not play games so I
mean this is a crazy analogy but like
mean this is a crazy analogy but like
imagine you're trying to play a game and
imagine you're trying to play a game and
then every couple seconds somebody comes
then every couple seconds somebody comes
by and slaps you in the face that'd be
by and slaps you in the face that'd be
pretty distracting right well okay
pretty distracting right well okay
that's kind of what it's like when your
that's kind of what it's like when your
buttons stick down and you can't like
buttons stick down and you can't like
actually press the buttons you want
so there's not a huge entropy difference
here where's the uh The Sweep
here where's the uh The Sweep
graph
oops where's entropy
entropy coefficient
entropy coefficient
is
0.09
0.09
to 0.04 perfect
hate when i d rank because someone keeps
hate when i d rank because someone keeps
slapping me in the face yeah well I hate
slapping me in the face yeah well I hate
when my RL agent when I like I spin my
when my RL agent when I like I spin my
gpus all night and my RL agent doesn't
gpus all night and my RL agent doesn't
work CU somebody keeps holding my
work CU somebody keeps holding my
buttons down it's a great way to push my
buttons down it's a great way to push my
buttons
so I have 18 graphs
here I think I could do this one a
here I think I could do this one a
little better
right oh no I did do this right
like I swear this is not just like this
like I swear this is not just like this
is not normal in science like just the
is not normal in science like just the
amount of stuff that I find in RL here
amount of stuff that I find in RL here
like and I wasn't necessarily looking
like and I wasn't necessarily looking
for this when I was a PhD student right
for this when I was a PhD student right
cuz there's like not a ton of incentive
cuz there's like not a ton of incentive
to um but now that I'm just like
to um but now that I'm just like
actually from a very practical and
actually from a very practical and
grounded perspective just trying to make
grounded perspective just trying to make
stuff work by whatever means necessary I
stuff work by whatever means necessary I
just find this stuff all the time it's
nuts why is this one so much smaller
okay there we
go I got to say though the progress in
go I got to say though the progress in
reinforcement learning
reinforcement learning
lately just like the progress I've made
lately just like the progress I've made
in puffer lib in the last couple of
in puffer lib in the last couple of
months RL is just such a different place
months RL is just such a different place
already and it's going to continue to
already and it's going to continue to
get better and
get better and
better um at this point like carb sweeps
better um at this point like carb sweeps
are pretty close to just solving
are pretty close to just solving
whatever out of the box and all we need
whatever out of the box and all we need
really is fast environments and like I
really is fast environments and like I
have the snake M I've got the particle
have the snake M I've got the particle
M's I've almost you know I'm decently
M's I've almost you know I'm decently
through on the Moa I've got a couple
through on the Moa I've got a couple
other Ms I'm working on behind the
other Ms I'm working on behind the
scenes like and once we get this like
scenes like and once we get this like
the template out like once we get the
the template out like once we get the
the methodology down for just cranking
the methodology down for just cranking
out these fast environments like we can
out these fast environments like we can
do say like seven Atari environments I
do say like seven Atari environments I
could probably crank those out into two
could probably crank those out into two
weeks so we'd have like an Atari 7 but
weeks so we'd have like an Atari 7 but
it runs at a million FPS instead of like
it runs at a million FPS instead of like
40K million FPS on One Core even we
40K million FPS on One Core even we
could do as long as we're not rendering
could do as long as we're not rendering
fully like State based
fully like State based
um yeah we can do a lot of stuff we can
um yeah we can do a lot of stuff we can
do a lot of stuff really
do a lot of stuff really
quickly maybe we can even do low F low
quickly maybe we can even do low F low
Fidelity rendering for all I know might
Fidelity rendering for all I know might
not be that
not be that
hard I mean they they always down sample
hard I mean they they always down sample
them like crazy anyways probably the
them like crazy anyways probably the
observations we'll be given them will be
observations we'll be given them will be
more reasonable than the squashed little
more reasonable than the squashed little
frames that we use
normally going to say one more time
normally going to say one more time
because the stream is doing particularly
because the stream is doing particularly
well tonight uh if you want to support
well tonight uh if you want to support
all this stuff it's all open source all
all this stuff it's all open source all
the sweeps all the tools and things I'm
the sweeps all the tools and things I'm
using they're all in puffer lib it's
using they're all in puffer lib it's
right
right
here working on this fulltime if you
here working on this fulltime if you
want to start the repo it helps me out
want to start the repo it helps me out
an absolute
an absolute
ton the growth has been just just nuts
ton the growth has been just just nuts
I'm really really happy with the how
I'm really really happy with the how
this has done over the last couple of
this has done over the last couple of
months and I'm not going anywhere you
months and I'm not going anywhere you
know it's it's going to keep going from
know it's it's going to keep going from
here okay let's do a little bit of
here okay let's do a little bit of
analysis since that's what people are
analysis since that's what people are
here for not puffer advertising
here for not puffer advertising
um the axis are weird on this graph why
um the axis are weird on this graph why
did they do
this it's okay
this it's okay
it's
it's
okay
so entropy of best runs or greater than
so entropy of best runs or greater than
15 without sticky actions ranges from
15 without sticky actions ranges from
0.9 to
1.4 entropy
1.4 entropy
coefficient for these runs
is entropy
is that's one e-
3 no it's one eus two right one-
2 4 - 3
be of best runs score greater than 10
be of best runs score greater than 10
with
with
sticky ranges from
1.3 to
1.3 to
1.5 entropy coefficient for these runs
1.5 entropy coefficient for these runs
is and let's go grab the entropy
is and let's go grab the entropy
coefficient for these
that's 60 minus
3 wait 1 eus 3 to 6 eus
3 1us 3 to 6 eus 3 is that right
1 eus 3 6 eus
3 don't need this
um where's my
draft okay
good I'll put this note somewhere else
H I don't really need to say this
H I don't really need to say this
because I have 100 and 200 mil and I
because I have 100 and 200 mil and I
checked it against
both okay let's go through
both okay let's go through
this and then we'll find a we'll make
this and then we'll find a we'll make
like a fun graphic for this and uh I'm
like a fun graphic for this and uh I'm
going to order myself some food and
going to order myself some food and
we'll end just like that this will just
we'll end just like that this will just
be a short stream answer a couple
be a short stream answer a couple
questions if anybody has any but this is
questions if anybody has any but this is
like oh yeah yeah I got to check the
like oh yeah yeah I got to check the
MOBA results as well cuz we have some
MOBA results as well cuz we have some
sweeps that have
sweeps that have
run I actually trained a cool policy
run I actually trained a cool policy
that hopefully is
good sticky actions considered harmful
good sticky actions considered harmful
Atari is the most widely used Benchmark
Atari is the most widely used Benchmark
in RL it is standard practice to
in RL it is standard practice to
randomly repeat agent key presses making
randomly repeat agent key presses making
action sticky this technique was
action sticky this technique was
introduced by a landmark paper in 2017
introduced by a landmark paper in 2017
with 600 plus citations and it is the
with 600 plus citations and it is the
default setting in Al Pi the motivation
default setting in Al Pi the motivation
was to prevent algorithms from
was to prevent algorithms from
hypothetically memorizing a fixed set of
hypothetically memorizing a fixed set of
key presses to win the game which is
key presses to win the game which is
possible because Atari is deterministic
possible because Atari is deterministic
sticky actions were broadly adopted
sticky actions were broadly adopted
because they cover this Edge case and
because they cover this Edge case and
according to the authors they do not
according to the authors they do not
degrade the performance of dqn today I
degrade the performance of dqn today I
use data in the original paper to show
use data in the original paper to show
that this claim is false I further
that this claim is false I further
demonstrate that sticky actions can
demonstrate that sticky actions can
massively degrade the performance of Po
massively degrade the performance of Po
which is not learning deterministic
which is not learning deterministic
policies to begin
with
with
uh the dqn
paper does this go here wait I don't
paper does this go here wait I don't
think that figure goes there original
think that figure goes there original
dqn
dqn
result wait this
result wait this
goes yeah just pasted this one here this
goes yeah just pasted this one here this
shouldn't go
shouldn't go
here and then I have my little Joseph
here and then I have my little Joseph
Suarez is a new Li mined MIT PhD and
Suarez is a new Li mined MIT PhD and
fulltime oral Exorcist I build yeah this
fulltime oral Exorcist I build yeah this
is fine I'm just checking for grammar
is fine I'm just checking for grammar
errors and reading through
errors and reading through
uh here's the table of blading
uh here's the table of blading
stochastic sticky versus deterministic
stochastic sticky versus deterministic
actions and it's cropped nicely the
actions and it's cropped nicely the
impact of sto stoas staticity on
impact of sto stoas staticity on
different algorithms asteris beam Rider
different algorithms asteris beam Rider
free yep here are the original dqn
results notice that makato at Al only
results notice that makato at Al only
oblate three of the original
oblate three of the original
environments the three overlapping
environments the three overlapping
results roughly match original dqn does
results roughly match original dqn does
a bit better on cqu makato
out uh do a bit better on beam Rider and
out uh do a bit better on beam Rider and
Space Invaders makato at all lists full
Space Invaders makato at all lists full
Atari results with sticky actions in the
Atari results with sticky actions in the
appendix comparing this to the original
appendix comparing this to the original
dqn results we see that makato at all
dqn results we see that makato at all
substantially underperform on pong and
substantially underperform on pong and
breakout these are two of the best known
breakout these are two of the best known
and most consistent Atari environments
and most consistent Atari environments
as further Evidence I ran a large hyper
as further Evidence I ran a large hyper
perimeter sweep on pong using puffer
perimeter sweep on pong using puffer
lips poo plus lstm implementation with
lips poo plus lstm implementation with
and without sticky actions the
and without sticky actions the
environment is solved almost immediately
environment is solved almost immediately
without sticky actions but only gets a
without sticky actions but only gets a
maximum of around 16 score with them
maximum of around 16 score with them
which closely matches makato at 's
which closely matches makato at 's
reported performance
make sure I got the right
make sure I got the right
graphs carb sweep without sticky actions
graphs carb sweep without sticky actions
solves the
M yep quickly solves the environment at
M yep quickly solves the environment at
20 Max
20 Max
score carbs hyper parameter Sweep with
score carbs hyper parameter Sweep with
sticky action score gets stuck around 16
sticky action score gets stuck around 16
close to makato Al's reported
close to makato Al's reported
result but isn't determined as in bad
result but isn't determined as in bad
maybe but po isn't learning a
maybe but po isn't learning a
deterministic policy even without sticky
deterministic policy even without sticky
actions entropy stays above 1.0 for all
actions entropy stays above 1.0 for all
runs note that the entropy coefficient
runs note that the entropy coefficient
is included in the hyperparameter sweep
is included in the hyperparameter sweep
so low entropy zones are explored all
so low entropy zones are explored all
the best runs above 10
score range
score range
from hold
from hold
on 60 minus 3 is the low end 4 eus 3 to
on 60 minus 3 is the low end 4 eus 3 to
1 eus 2
is it 4us
is it 4us
yeah is it really like that wait 4 e
yeah is it really like that wait 4 e
minus
3 that's 4 e
3 that's 4 e
[Music]
[Music]
minus yeah that's 1 E minus
minus yeah that's 1 E minus
3 and then this one
4 minus 3 thanks for the stream night
4 minus 3 thanks for the stream night
good night thanks for dropping
good night thanks for dropping
by this will be published in the
morning obviously I'm not going to
morning obviously I'm not going to
publish it at this hour because
publish it at this hour because
algorithm
eus
3 look forward to reading it good luck
3 look forward to reading it good luck
thank you this is a pretty quick
thank you this is a pretty quick
write I'm not doing uh super long I'm
write I'm not doing uh super long I'm
not spending tons of time writing stuff
not spending tons of time writing stuff
these days it's like if I see something
these days it's like if I see something
I write a quick
I write a quick
article and uh then I move on back to
article and uh then I move on back to
more Dev
more Dev
right these are like pretty high impact
right these are like pretty high impact
for the amount of
for the amount of
uh the amount of time they take which is
uh the amount of time they take which is
nice and the opposite of
papers entropy stays above 1.0 for
papers entropy stays above 1.0 for
all all runs is it all runs that it says
all all runs is it all runs that it says
above 1.0 or just the good runs
that one's
good 0.8
note that the entropy coefficient is
note that the entropy coefficient is
included in the hyper parameter sweep so
included in the hyper parameter sweep so
low entropy zones are explored the sweep
low entropy zones are explored the sweep
was allowed to set as low as 1 E was
was allowed to set as low as 1 E was
allowed to set as low as 1 E minus 5 but
allowed to set as low as 1 E minus 5 but
there were no good runs in this range
there were no good runs in this range
all the best runs set entropy above 1
all the best runs set entropy above 1
minus 3
enty of best runs without sticky
enty of best runs without sticky
actions from 0.9 to
1.4 what's up
linky we found uh crazy
linky we found uh crazy
bugging like the
bugging like the
biggest RL library rln library out there
biggest RL library rln library out there
entropy coefficient from these runs
entropy coefficient from these runs
range from 4us 3 to oneus two this is
range from 4us 3 to oneus two this is
good
good
entropy of best runs with sticky
actions so bad
and we can just stop using it right
bet has raided the
bet has raided the
stream welcome
bet we found a crazy
bet we found a crazy
thing we found a real crazy thing
we're all in voice
we're all in voice
chat uh let me finish this paragraph I
chat uh let me finish this paragraph I
will order my food answer a couple
will order my food answer a couple
questions if there are any here and then
questions if there are any here and then
I'll come
by an assassination tweet what
what what did you
say wait did I miss like a major news
thing oh oh dude okay you confused the
thing oh oh dude okay you confused the
heck out of me uh no I said that
heck out of me uh no I said that
um I said that let me finish this
um I said that let me finish this
article let me finish writing this
article let me finish writing this
paragraph let me order my food and then
paragraph let me order my food and then
I'll swing by uh The Voice after maybe I
I'll swing by uh The Voice after maybe I
answer a few questions I don't know what
answer a few questions I don't know what
the heck you thought you heard me
the heck you thought you heard me
say you said that and I thought I missed
say you said that and I thought I missed
like a major US
like a major US
event derisive
event derisive
tweet no I'm just posting I'm not going
tweet no I'm just posting I'm not going
to post this article now this is a bad
to post this article now this is a bad
time zone to post on I'll post this in
time zone to post on I'll post this in
the
morning
e
e
e
e
e
e e
I should rename this discussion
we're
we're
moving I'm going to make a cool point
moving I'm going to make a cool point
for uh for some of the viewers here
for uh for some of the viewers here
since some folks are still
since some folks are still
around
um so here's the point and then I'm
um so here's the point and then I'm
going to write this up into a paragraph
going to write this up into a paragraph
um you have to take into account that
um you have to take into account that
this is not just a onef frame delay
this is not just a onef frame delay
right you're training with frame frame
right you're training with frame frame
skip usually frame skip uh 3 to five
skip usually frame skip uh 3 to five
usually four or five Mak used frame skip
usually four or five Mak used frame skip
five so you submit your action right it
five so you submit your action right it
delays five
delays five
frames and then
frames and then
potentially your action gets
potentially your action gets
repeated and then it skips another five
repeated and then it skips another five
frames so you're delayed a full 10
frames so you're delayed a full 10
frames right which is a sixth of a
frames right which is a sixth of a
second which is quite a substantial
second which is quite a substantial
amount of time but then there's just
amount of time but then there's just
another one in five chance that it skips
another one in five chance that it skips
again so 15 frames right which is now uh
again so 15 frames right which is now uh
a quarter of a second which is
a quarter of a second which is
definitely enough time in like pong for
definitely enough time in like pong for
you to miss the ball
you to miss the ball
right so that's one in uh one and 16
right so that's one in uh one and 16
chance for that and then a 1 and 64
chance for that and then a 1 and 64
chance that you miss 20
chance that you miss 20
frames and mind you you're not just
frames and mind you you're not just
missing the frame your key is being held
missing the frame your key is being held
down so like your paddle is going past
down so like your paddle is going past
the ball or
something
yeah
e
e e
okay this is wrong I do like
okay this is wrong I do like
this
for e
let me do the full math on
this 17
seconds oh I have to be a little careful
seconds oh I have to be a little careful
with this right
okay I'm going to explain this math in a
okay I'm going to explain this math in a
second let me finish writing it
out
e e
the one additional thing I should do is
the one additional thing I should do is
I should try to actually play the stupid
I should try to actually play the stupid
game with the frame
game with the frame
skip I actually don't think that the
skip I actually don't think that the
rapper will let you though I think if
rapper will let you though I think if
you do human mode it won't let
you for
and then there was one other point so
and then there was one other point so
let me let me go through this again so
let me let me go through this again so
um if you just do out the basic math I'd
um if you just do out the basic math I'd
kind of done this in my head but I
kind of done this in my head but I
didn't even realize it was this bad now
didn't even realize it was this bad now
I understand why you're losing like four
I understand why you're losing like four
or five points a game instead of just
or five points a game instead of just
one based on this
one based on this
um if you do the math with atari's frame
um if you do the math with atari's frame
rate uh about every 85 seconds your
rate uh about every 85 seconds your
controls just lock up for a half of a
controls just lock up for a half of a
second so you know these games are
second so you know these games are
pretty reactive like the Atari games are
pretty reactive like the Atari games are
actually pretty
actually pretty
fast they're actually pretty hard um
fast they're actually pretty hard um
your controls locking up for a second a
your controls locking up for a second a
half a second means the paddle is going
half a second means the paddle is going
to slide right by the ball and you just
to slide right by the ball and you just
lose a
lose a
point simple as
point simple as
that you know the really sad thing about
that you know the really sad thing about
this is that this could have been
this is that this could have been
present prevented by somebody having
present prevented by somebody having
like tried to play any of these games
like tried to play any of these games
with sticky actions
on can we even do that
we're going to definitely do that before
we're going to definitely do that before
I post this
I post this
article it's getting late so I'm not
article it's getting late so I'm not
going to do it right now I'm going to
going to do it right now I'm going to
finish writing
it what was the other
objection
for
e e
why is regularizer not a
why is regularizer not a
word go
away
e
e
e e
[Music]
yeah e
okay hold the ble could have been
okay hold the ble could have been
prevented by Trying by spending a couple
prevented by Trying by spending a couple
hours trying to play Atari with sticky
hours trying to play Atari with sticky
actions
rerunning sweeps
we'll just duplicate the
sentence
sentence
okay let me make sure these last couple
okay let me make sure these last couple
paragraphs are good I'll obviously read
paragraphs are good I'll obviously read
this one more time in the
this one more time in the
morning won't but won't removing sticky
morning won't but won't removing sticky
actions make Atari too easy maybe but
actions make Atari too easy maybe but
this is a dumb source of difficulty the
this is a dumb source of difficulty the
math for this is very simple and obvious
math for this is very simple and obvious
to anyone who's actually played Atari
to anyone who's actually played Atari
makato all use frames use a frame skip
makato all use frames use a frame skip
of five this means that the agent
of five this means that the agent
submits an action five frames pass and
submits an action five frames pass and
then control returns to the player there
then control returns to the player there
is a one and four chance that their
is a one and four chance that their
action gets repeated delaying the return
action gets repeated delaying the return
of control for another five frames so
of control for another five frames so
one and four for a 10 frame delay yada
one and four for a 10 frame delay yada
yada
yada
yada accounting for frame skip you
yada accounting for frame skip you
should expect a 30 frame delay every
should expect a 30 frame delay every
1024 * 5 is 5 5120 frames or every 85
1024 * 5 is 5 5120 frames or every 85
seconds with atari's 60 FPS so every 85
seconds with atari's 60 FPS so every 85
seconds if you're playing
seconds if you're playing
pong your controls freeze up for half a
pong your controls freeze up for half a
second and the paddle keep sliding not
second and the paddle keep sliding not
exactly the best source of difficulty I
exactly the best source of difficulty I
noticed makado Al do better on some
noticed makado Al do better on some
environments not all games will be lost
environments not all games will be lost
if you lose control for half a second in
if you lose control for half a second in
these
these
cases it is well in the span
of not all games will be lost in the
of not all games will be lost in the
span of half half a second in these
span of half half a second in these
cases it is possible that sticky actions
cases it is possible that sticky actions
act as a regularizer there are probably
act as a regularizer there are probably
better ways a to achieve the same result
better ways a to achieve the same result
but if you really want to use sticky
but if you really want to use sticky
actions treat it as a hyper parameter
actions treat it as a hyper parameter
sweeps can then automatically discover
sweeps can then automatically discover
the best value based on the environment
the best value based on the environment
hey JBL we found something crazy one
hey JBL we found something crazy one
second and I'll tell you sweeps can then
second and I'll tell you sweeps can then
automatically discover the best value
automatically discover the best value
based on the
based on the
environment but definitely don't set it
environment but definitely don't set it
to 0.25 for all environments in an
to 0.25 for all environments in an
earlier sweep I didn't find a good
earlier sweep I didn't find a good
setting for pong above
0.13 okay conclusions one of the biggest
0.13 okay conclusions one of the biggest
advantages of using games as research
advantages of using games as research
environments is that they are easy to
environments is that they are easy to
interpret all you have to do is play
interpret all you have to do is play
them this whole debacle could have been
them this whole debacle could have been
prevented by spending a couple hours
prevented by spending a couple hours
trying to play atar with sticky
trying to play atar with sticky
actions I will be removing the setting
actions I will be removing the setting
from puffer lib defaults and rerunning
from puffer lib defaults and rerunning
hyper parameter sweeps you can support
hyper parameter sweeps you can support
my work by following me in starring
my work by following me in starring
puffer perfect okay
puffer perfect okay
um it seems like most of the crowd has
um it seems like most of the crowd has
gotten bored of watching me write an
gotten bored of watching me write an
article which is very
article which is very
understandable we got a couple stars out
understandable we got a couple stars out
of this stream I'm very happy with that
of this stream I'm very happy with that
um did it end up being possible to song
um did it end up being possible to song
Hey reu and JBL okay welcome back no I
Hey reu and JBL okay welcome back no I
was completely right this thing is
was completely right this thing is
cursed and this is destroyed Atari as a
cursed and this is destroyed Atari as a
benchmark it's that bad um yes I proved
benchmark it's that bad um yes I proved
it yo will it's it's that bad so I ran
it yo will it's it's that bad so I ran
comprehensive hyper parameter Sweep with
comprehensive hyper parameter Sweep with
their sticky actions uh it caps out
their sticky actions uh it caps out
about 16 which is exactly the number
about 16 which is exactly the number
that they reported in their original
that they reported in their original
results with dqn
results with dqn
uh I ran you know 168 experiments and
uh I ran you know 168 experiments and
then if I run it without sticky
then if I run it without sticky
actions it immediately finds hyper
actions it immediately finds hyper
parameters that solve the environment at
parameters that solve the environment at
20 it's that
20 it's that
bad so uh I did a whole bunch of
bad so uh I did a whole bunch of
additional analysis I've got an article
additional analysis I've got an article
ready to go for tomorrow but yeah
ready to go for tomorrow but yeah
like completely destroy the field with
like completely destroy the field with
this one easy
this one easy
trick I mean can you imagine how stupid
trick I mean can you imagine how stupid
that is like of course reinforce M looks
that is like of course reinforce M looks
dumb if like of course reinforcement
dumb if like of course reinforcement
learning looks stupid right if like we
learning looks stupid right if like we
can't solve pong well it turns out we
can't solve pong well it turns out we
can't solve pong because we're we have
can't solve pong because we're we have
this stupid rapper well let me put it
this stupid rapper well let me put it
this way I did some back of the envelope
this way I did some back of the envelope
every 85 seconds on average that the
every 85 seconds on average that the
agent plays pong the controls freeze up
agent plays pong the controls freeze up
for a full half of a second and the
for a full half of a second and the
paddle keeps sliding if you've played
paddle keeps sliding if you've played
Atari it's kind of fast right yeah you
Atari it's kind of fast right yeah you
totally lose in that half a second that
totally lose in that half a second that
the paddle slides back you're
the paddle slides back you're
done yeah this is Earth shattering
right
crazy so uh sticky actions considered
crazy so uh sticky actions considered
harmful publish this
harmful publish this
tomorrow
tomorrow
um dude this field is so cursed and it's
um dude this field is so cursed and it's
so avoidable
so avoidable
like think about it if anybody bothered
like think about it if anybody bothered
to play this for a couple hours yeah see
to play this for a couple hours yeah see
well if anybody bothered to play this
well if anybody bothered to play this
for half an hour even like you'd really
for half an hour even like you'd really
quickly see that you can't play the game
quickly see that you can't play the game
with these sticky actions on
right well we got this ready to go for
right well we got this ready to go for
tomorrow hopefully people see this one
tomorrow hopefully people see this one
oh let's go get a I'm going to go order
oh let's go get a I'm going to go order
myself dinner on the side here real
myself dinner on the side here real
quick uh and then I'm going to find a
quick uh and then I'm going to find a
cool title for this I'll answer some
cool title for this I'll answer some
chat questions if there are any and I'll
chat questions if there are any and I'll
go because it's already 8:00 p.m. I
go because it's already 8:00 p.m. I
actually usually go to bed by 9:00 um
actually usually go to bed by 9:00 um
because I get up early let me figure out
because I get up early let me figure out
what the heck I want to eat order
what the heck I want to eat order
something it's too late for me to do
something it's too late for me to do
anything else
U let me get some pasta real
quick wait so what happened let me
quick wait so what happened let me
explain in a second let me just order my
explain in a second let me just order my
dinner because otherwise like I'm going
dinner because otherwise like I'm going
to forget to and I'm going to be hungry
to forget to and I'm going to be hungry
um
parm just order myself like some chicken
parm just order myself like some chicken
parm or
parm or
something
perfect just order this and then I'll
perfect just order this and then I'll
tell you
cool just go
through I've been uh co-working
through I've been uh co-working
on uh a different cool environment that
on uh a different cool environment that
I'll I'll talk about well we'll see when
I'll I'll talk about well we'll see when
I talk about but all day so I haven't
I talk about but all day so I haven't
had time to do too much um okay so JBL
had time to do too much um okay so JBL
the thing that
the thing that
happened is I found in the default
happened is I found in the default
rapper of Atari so if you use Al
rapper of Atari so if you use Al
Pi so
what's yeah so this is a 2,000 star
what's yeah so this is a 2,000 star
repo
repo
right this is used in like every paper
right this is used in like every paper
like 90% of Atari uh 90% of RL papers
like 90% of Atari uh 90% of RL papers
out there the default setting in this
out there the default setting in this
includes sticky actions by default
includes sticky actions by default
uh if you don't look at it it has scky
actions here's a paper that introduced
actions here's a paper that introduced
sticky actions it has over 600 citations
sticky actions it has over 600 citations
massive paper for reinforcement
massive paper for reinforcement
learning the paper says that sticky
learning the paper says that sticky
actions do not hurt performance in dqn
actions do not hurt performance in dqn
it is wrong I've proved I can prove it's
it is wrong I've proved I can prove it's
wrong based on only the data in this
wrong based on only the data in this
paper and the original dqn paper you can
paper and the original dqn paper you can
see it's wrong I ran 300 follow-up uh
see it's wrong I ran 300 follow-up uh
experiments 300 trials like 250
experiments 300 trials like 250
experiment hyper parameter sweeps on
experiment hyper parameter sweeps on
pong with po as well I replicate almost
pong with po as well I replicate almost
exactly their result which is that they
exactly their result which is that they
do not even learn how to play Pong with
do not even learn how to play Pong with
sticky actions immediately solve it
sticky actions immediately solve it
within a couple of Trials of hyper pram
within a couple of Trials of hyper pram
sweep without sticky
actions show that the original
actions show that the original
motivation for doing this doesn't make
motivation for doing this doesn't make
any sense because these algorith are not
any sense because these algorith are not
learning deterministic policies anyways
learning deterministic policies anyways
it was like this weird hypothetical
it was like this weird hypothetical
construction like an invented problem
construction like an invented problem
this solves an invented problem and
this solves an invented problem and
invents a real problem that's what this
invents a real problem that's what this
does this solves an invented problem and
does this solves an invented problem and
invents a real
problem so yeah big day for RL I mean
problem so yeah big day for RL I mean
you got to think about it as well like
you got to think about it as well like
how stupid does reinforcement learning
how stupid does reinforcement learning
look when you can't even solve pong
look when you can't even solve pong
right
right
how stupid does reinforcement learning
how stupid does reinforcement learning
look when you can't even solve pong well
look when you can't even solve pong well
it turns out you can solve pong you can
it turns out you can solve pong you can
solve
solve
pong how quickly do you think we can
pong how quickly do you think we can
solve pong I haven't even checked I'm
solve pong I haven't even checked I'm
confident let's do this live as we'll do
confident let's do this live as we'll do
it
live filter above
15 we're going to consider 19 score to
15 we're going to consider 19 score to
be solved I'd say right 20 score okay
be solved I'd say right 20 score okay
whatever 20 score in 3 minutes we solve
whatever 20 score in 3 minutes we solve
Paul in 3 minutes one
Paul in 3 minutes one
GPU not even using C++
GPU not even using C++
vectorization no fancy tricks we solve
vectorization no fancy tricks we solve
it in 3
minutes if we do it by samples because
minutes if we do it by samples because
people like sample efficiency for some
people like sample efficiency for some
reason it's not even going to be that
reason it's not even going to be that
bad
yeah it's like four million samples or
yeah it's like four million samples or
whatever compared to like 2 million if
whatever compared to like 2 million if
you use a sample efficient algorithm
you use a sample efficient algorithm
that takes two hours now this isn't dqn
that takes two hours now this isn't dqn
this is PO with an lstm this is just
this is PO with an lstm this is just
whatever I have in the puffer
defaults dqn does solve pong though it's
defaults dqn does solve pong though it's
dqn with this stupid rapper that doesn't
dqn with this stupid rapper that doesn't
solve
solve
pong article will be live tomorrow
pong article will be live tomorrow
but
um where is
it if you look at the original charts in
it if you look at the original charts in
the paper basically and then you look at
the paper basically and then you look at
the dqn charts original dqn solves pong
the dqn charts original dqn solves pong
perfectly their version with sticky
perfectly their version with sticky
actions does not solve pong they get a
actions does not solve pong they get a
score of about 15 my PO with an lstm
score of about 15 my PO with an lstm
gets a score of about 15 if you use
gets a score of about 15 if you use
their stupid sticky actions thing uh if
their stupid sticky actions thing uh if
you do the math that checks out uh
you do the math that checks out uh
basically sticky actions make you lose a
basically sticky actions make you lose a
quarter of your games that
quarter of your games that
simple and it makes a ton of sense if
simple and it makes a ton of sense if
you think through it as well I do some
you think through it as well I do some
like envelope math down
like envelope math down
here ain't that
here ain't that
funny is
funny is
stupid stop doing
stupid stop doing
that stop inventing
that stop inventing
problems enough
problems chat chippity has got to get me
problems chat chippity has got to get me
a tit all
narrate a
suitably
suitably
epic cover art in a 3:1 ratio landscape
epic cover art in a 3:1 ratio landscape
for this article let's see if it does
for this article let's see if it does
anything cool how do you go about
anything cool how do you go about
choosing the RL algorithm you use for
choosing the RL algorithm you use for
example poo and td3 can be used
example poo and td3 can be used
interchangeably but I usually just use
interchangeably but I usually just use
which I'm more familiar with without any
which I'm more familiar with without any
proper reasoning uh my reasoning is that
proper reasoning uh my reasoning is that
poo solves DOTA with a one layer lstm so
poo solves DOTA with a one layer lstm so
it's probably fine Poo's been the
it's probably fine Poo's been the
default since 2017 nothing has
default since 2017 nothing has
conclusively shown its
conclusively shown its
better this is kind of
funny this is actually kind of funny
funny this is actually kind of funny
artwork isn't
artwork isn't
it I kind of like
that all right I just said like hey I I
that all right I just said like hey I I
made a I proved like a crazy thing I
made a I proved like a crazy thing I
found a crazy thing generate me some
found a crazy thing generate me some
cool cover
cool cover
art yeah I generally like I don't even
art yeah I generally like I don't even
here's here's the thing I don't even
here's here's the thing I don't even
do a shoot I have to
like ah it's not going to be able to do
like ah it's not going to be able to do
the same image
again here
again here
same image let's see um I here's the
same image let's see um I here's the
thing though uh if you're still there
thing though uh if you're still there
like I don't even focus on algorithm
like I don't even focus on algorithm
implementations in puffer I've been
implementations in puffer I've been
making so so much progress in RL without
making so so much progress in RL without
even doing anything on the algorithm
even doing anything on the algorithm
implementation side because it's like
implementation side because it's like
infra being cursed is the main thing
infra being cursed is the main thing
infra is crazy
cursed more than anything
which one of these is the best
this one's pretty
cool the stupid aspect ratio is wrong
cool the stupid aspect ratio is wrong
though
I don't know if it actually can do the
I don't know if it actually can do the
aspect ratio I
aspect ratio I
want I really wish it had the ability to
want I really wish it had the ability to
like edit existing
like edit existing
images oh yeah thanks you made it less
images oh yeah thanks you made it less
wide perfect
I don't know we'll see maybe it crops
differently that's not bad
I like
that
that
cool um well this is ready to go for
cool um well this is ready to go for
tomorrow you all get a sneak peek for uh
tomorrow you all get a sneak peek for uh
dropping by the stream tonight I I'll
dropping by the stream tonight I I'll
say this one more time because this
say this one more time because this
stream keeps turning new people over um
stream keeps turning new people over um
this is all public work like the hyper
this is all public work like the hyper
pram sweeps for this the optimizer
pram sweeps for this the optimizer
algorithm the like solving pong in 3
algorithm the like solving pong in 3
minutes this is all free and available
minutes this is all free and available
in puffer lib if you want to help my
in puffer lib if you want to help my
workout I'm working on this full-time
workout I'm working on this full-time
all you have to do is start the repo
all you have to do is start the repo
that's all I ask it helps me out a whole
that's all I ask it helps me out a whole
ton this has been the growth on the
ton this has been the growth on the
project since I started working on it
project since I started working on it
full-time this is the exact type of
full-time this is the exact type of
thing that puffer lib needs like in
thing that puffer lib needs like in
order for me to actually get all this
order for me to actually get all this
out there and to fix all this stuff in
out there and to fix all this stuff in
RL I'm not going anywhere but the
RL I'm not going anywhere but the
support sure helps so thank
you other than that this article will be
you other than that this article will be
live tomorrow on X my ex
live tomorrow on X my ex
is strictly for uh research it's pretty
is strictly for uh research it's pretty
much that's all I do
much that's all I do
um I don't know occasionally I get bored
um I don't know occasionally I get bored
and post dumb replies to stuff but the
and post dumb replies to stuff but the
posts are just about
posts are just about
research um I'll put it in chat one more
research um I'll put it in chat one more
time cuz this is where the article will
time cuz this is where the article will
be tomorrow and uh I will be streaming
be tomorrow and uh I will be streaming
tomorrow
tomorrow
madev uh I will we'll see maybe if I
madev uh I will we'll see maybe if I
have time in the morning to like mess
have time in the morning to like mess
with these sweeps but
with these sweeps but
um I do
um I do
have a sneak preview which is this run
have a sneak preview which is this run
from last uh from this morning
we have a five billion step mobo
we have a five billion step mobo
run which is rather interesting right so
run which is rather interesting right so
I ran this for 5 billion steps it looks
I ran this for 5 billion steps it looks
like it takes 99 Towers 99 Problems 99
like it takes 99 Towers 99 Problems 99
towers and uh yeah it's pretty
towers and uh yeah it's pretty
good so we'll look at what this does
good so we'll look at what this does
tomorrow I'm working on like miniature
tomorrow I'm working on like miniature
DOTA as an RL environment at a million
DOTA as an RL environment at a million
FPS so if that sounds interesting drop
FPS so if that sounds interesting drop
by tomorrow I'll be live a lot earlier
by tomorrow I'll be live a lot earlier
I've been streaming all my Dev lately
I've been streaming all my Dev lately
for fun but otherwise I will catch you
for fun but otherwise I will catch you
later I got to go grab dinner see you
later I got to go grab dinner see you
around

Kind: captions
Language: en
hey we are
live hey you
live hey you
guys uh I found something freaking
crazy um it turns out all the stuff that
crazy um it turns out all the stuff that
I was talking about on stream yesterday
I was talking about on stream yesterday
I was
I was
right and uh Atari is
right and uh Atari is
broken because of a stupid
broken because of a stupid
rapper that was proposed
rapper that was proposed
in order to address a hypothetical issue
in order to address a hypothetical issue
that was never shown to actually happen
that was never shown to actually happen
in
in
practice
practice
Yeah so
Yeah so
uh this is just going to be a short
uh this is just going to be a short
little stream because it's late I've
little stream because it's late I've
been working on other things all day but
been working on other things all day but
uh I'm going to look at these
uh I'm going to look at these
experiments I'm going to finish writing
experiments I'm going to finish writing
this article on stream I'm going to post
this article on stream I'm going to post
this article in the morning tomorrow and
this article in the morning tomorrow and
I'll stream all day
I'll stream all day
tomorrow um but today we're just going
tomorrow um but today we're just going
to finish the article going to hang out
to finish the article going to hang out
here for like an hour
here for like an hour
maybe and we're going to look at some
maybe and we're going to look at some
experiments we're going to relaunch some
experiments we're going to relaunch some
new
new
experiments and we're going to take it
experiments and we're going to take it
from
there like I don't know how this happens
there like I don't know how this happens
that I keep finding errors in like huge
that I keep finding errors in like huge
papers that have just gone unnoticed for
papers that have just gone unnoticed for
years like they're not okay they're kind
years like they're not okay they're kind
of subtle to be fair it's broken yeah
of subtle to be fair it's broken yeah
it's
it's
broken look this is this is
pong here you know what we'll show you
pong here you know what we'll show you
pong before and
after
after
um is this
um is this
it yeah uh
it yeah uh
wait hold on let me make sure I have the
wait hold on let me make sure I have the
right
right
graphs uh nope these aren't the right
graphs uh nope these aren't the right
ones we have even better
ones we have even better
ones I think this one
okay here's pong fixed right this what
okay here's pong fixed right this what
it's supposed to be really easy
it's supposed to be really easy
environment you solve
environment you solve
it and here is
it and here is
pong with the Terrible Bad rapper
thing broken
600 plus citation
paper so I'm going to just finish out
paper so I'm going to just finish out
writing this article on stream because I
writing this article on stream because I
may as
well uh I'll go through it a little bit
well uh I'll go through it a little bit
on stream because may as
on stream because may as
well so the problem is sticky
well so the problem is sticky
actions uh they're real bad
actions uh they're real bad
sticky actions mean that when your agent
sticky actions mean that when your agent
takes an action it holds the key
takes an action it holds the key
down sometimes so it makes you repeat
down sometimes so it makes you repeat
actions and uh this was proposed as a
actions and uh this was proposed as a
method to break
method to break
determinism so you couldn't just
determinism so you couldn't just
memorize the optimal thing to do and the
memorize the optimal thing to do and the
author said that this doesn't break dqn
author said that this doesn't break dqn
this doesn't break like actual learning
this doesn't break like actual learning
algorithms but actually you can prove
algorithms but actually you can prove
based on the data in the original paper
based on the data in the original paper
that it
that it
does and uh it replicates on PO
does and uh it replicates on PO
replicates on my latest you know all the
replicates on my latest you know all the
new
new
infrastructure and it totally breaks
infrastructure and it totally breaks
like one of the easiest environments out
like one of the easiest environments out
there with this modification it makes it
there with this modification it makes it
I don't know if it's impossible it makes
I don't know if it's impossible it makes
it a heck of a lot harder and if it is
it a heck of a lot harder and if it is
possible it's not happening with this
possible it's not happening with this
giant hyper parameter sweep I'm running
so welcome YouTube
so welcome YouTube
folks I'm just going to finish this
folks I'm just going to finish this
article up on stream tonight
article up on stream tonight
uh I figured I could stream writing this
uh I figured I could stream writing this
in like chat and hanging out or I could
in like chat and hanging out or I could
not stream it so figured I'd stream it
not stream it so figured I'd stream it
uh let me see let me finish I think I
uh let me see let me finish I think I
had yeah I was working on just fixing up
had yeah I was working on just fixing up
this
this
figure hold
on
cubert so I need the original Atari
cubert so I need the original Atari
environment out of this which is
like
here cqu Quest is there any other ones
here cqu Quest is there any other ones
one two 3 four
five wait
five wait
six yeah just Space
Invaders let's grab space Vaders and
Invaders let's grab space Vaders and
then I'll tell you why this chart is um
then I'll tell you why this chart is um
so
important I guess I should have expected
important I guess I should have expected
a few people to uh top in the Stream
a few people to uh top in the Stream
when I make a title like this huh W
when I make a title like this huh W
welcome folks RL is in fact broken I
welcome folks RL is in fact broken I
have in fact fixed
it specifically Atari which is the most
it specifically Atari which is the most
widely used Benchmark out there right it
widely used Benchmark out there right it
is
is
broken it's broken by a dumb rapper
broken it's broken by a dumb rapper
that's become the
that's become the
standard it's very
sad let me get this finish this chart up
sad let me get this finish this chart up
think if I just cut it
think if I just cut it
here yeah there we go was there a
here yeah there we go was there a
caption that I'm
caption that I'm
missing no that's
missing no that's
it
it
so uh save
so uh save
DQ uh
DQ uh
aado download
save so as soon as I get this as soon as
save so as soon as I get this as soon as
I crop this figure and put it in the
I crop this figure and put it in the
article I'll explain
article I'll explain
um I'll explain what the data
um I'll explain what the data
this will be in the article tomorrow
this will be in the article tomorrow
but you know you'll get a early preview
but you know you'll get a early preview
of
of
it uh
it uh
edit and then I just do
wait
wait
save okay
dqn
makato where my table
makato where my table
go
what come
on okay there we go
so the data
so the data
here is the wrapper that they say does
here is the wrapper that they say does
not hurt
not hurt
performance and this is their result
performance and this is their result
they say look we run it deterministic
they say look we run it deterministic
which is normal we run it stochastic
which is normal we run it stochastic
which is with our wrapper and this is a
which is with our wrapper and this is a
dumb Baseline that we expect to be
dumb Baseline that we expect to be
broken by stochasticity it's broken dqn
broken by stochasticity it's broken dqn
the results don't go
the results don't go
down but the thing is you can go look at
down but the thing is you can go look at
the original dqn results which has not
the original dqn results which has not
just three these three environments but
just three these three environments but
has seven original environments and then
has seven original environments and then
you can go down to the makato paper
you can go down to the makato paper
which has full Atari experiment results
which has full Atari experiment results
with uh their wrapper and you can see
with uh their wrapper and you can see
that in their paper they do not do well
that in their paper they do not do well
on pong or breakout which of which are
on pong or breakout which of which are
two of the best known and most
two of the best known and most
consistent environments out there they
consistent environments out there they
substantially underperform dqm so in
substantially underperform dqm so in
fact if you look at all seven
fact if you look at all seven
environments and not just the three that
environments and not just the three that
they happen to pick you can see that
they happen to pick you can see that
clearly this wrapper does in fact
clearly this wrapper does in fact
degrade the performance of dqn
degrade the performance of dqn
and then what I'm going to continue
and then what I'm going to continue
writing here the experiments that I did
writing here the experiments that I did
uh this replicates on PO this replicates
uh this replicates on PO this replicates
on like the latest RL infrastructure if
on like the latest RL infrastructure if
you try to run pong with and you run a
you try to run pong with and you run a
big hyper parameter sweep right to make
big hyper parameter sweep right to make
absolutely certain of everything which
absolutely certain of everything which
is more than they did um if you do that
is more than they did um if you do that
then it pong is like a trivial
then it pong is like a trivial
environment without sticky actions and
environment without sticky actions and
thus far I've run like aund something
thus far I've run like aund something
experiments and it hasn't been able to
experiments and it hasn't been able to
do above 15ish with the the sticky
do above 15ish with the the sticky
actions which is actually the score that
actions which is actually the score that
they report as well so there you
they report as well so there you
go this broke reinforcement
learning how does 30 to 40,000 step per
learning how does 30 to 40,000 step per
second Atari training on one
second Atari training on one
GPU um
GPU um
with without a dumb rapper that makes
with without a dumb rapper that makes
the environments hard how's that sound
the environments hard how's that sound
that's what we've got in puffer now
gonna be real
nice okay comparing this to the original
nice okay comparing this to the original
DEC
results
results
substantially under perform on
substantially under perform on
pong and break
out anything else they perform
on Enduro is
on Enduro is
470 oh they do fine on
470 oh they do fine on
Enduro
cubert oh they don't do well on cubert
cubert oh they don't do well on cubert
do
do
they no wait they're good on cubert
yeah so it's
yeah so it's
mixed that's fine
are two of the best known most
are two of the best known most
consistent Atari environments
uh let's
see as further
see as further
Evidence I ran a
Evidence I ran a
large paper parameter
large paper parameter
sweep on
sweep on
Hong using puffer
Hong using puffer
Libs Popo
plus
plus
lstm
mation which
is with and without sticky
is with and without sticky
actions the environment is solved almost
actions the environment is solved almost
immediately without sticky
but only gets a
but only gets a
maximum of around I think it's 16
score yeah 16
score
score
score with them which closely matches
score with them which closely matches
kados
Al
Al
reported
performance let's go copy some
performance let's go copy some
graphs
um
this we put the
this we put the
filter let's see if it looks better with
filter let's see if it looks better with
the filter or without it
this looks
nice we'll do like
this let's let's refresh the page so we
this let's let's refresh the page so we
get this nicely
this is going to be a very nice finding
this is going to be a very nice finding
because this is going to make it so much
because this is going to make it so much
easier to do uh like I think Atari is
easier to do uh like I think Atari is
actually going to be reasonably usable I
actually going to be reasonably usable I
never thought I would say that but with
never thought I would say that but with
how fast I have it training now and
how fast I have it training now and
without this dumb sticky action thing
without this dumb sticky action thing
polluting results
polluting results
like pretty
possible for folks unfamiliar sticky
possible for folks unfamiliar sticky
actions are like this is something that
actions are like this is something that
basically anybody everybody who's worked
basically anybody everybody who's worked
in RL for any period of time knows about
in RL for any period of time knows about
like for this thing to be wrong is
nuts frankly I'd rather that somebody
nuts frankly I'd rather that somebody
like I'd rather that I'm wrong about
like I'd rather that I'm wrong about
this somehow though I don't see how that
this somehow though I don't see how that
would be possible like I'd rather that I
would be possible like I'd rather that I
somehow be wrong about this and sticky
somehow be wrong about this and sticky
actions not be wrong just because of the
actions not be wrong just because of the
amount of work this will
overturn let me just make absolutely
overturn let me just make absolutely
sure that I have the parameter right so
sure that I have the parameter right so
I use this
I use this
graph if I just click any of
graph if I just click any of
these and I look for the sticky action
parameter where is it it's not in train
parameter where is it it's not in train
it's in
it's in
M repeat action prob zero so no sticky
M repeat action prob zero so no sticky
actions all that's
right sticky actions don't make a
right sticky actions don't make a
difference oh they make a huge
difference oh they make a huge
difference they make a huge difference
difference they make a huge difference
difference I'll show you the two figures
difference I'll show you the two figures
side by
side by
side sticky it's like this this article
side sticky it's like this this article
is literally titled sticky actions
is literally titled sticky actions
considered harmful that's how bad it
is now it's not going to be for all
is now it's not going to be for all
environments obviously but the key is
environments obviously but the key is
that for some environments yeah what did
that for some environments yeah what did
you find out okay
you find out okay
so here is can I make this
so here is can I make this
larger I think I can just do it like
larger I think I can just do it like
this
this
right okay so here's pong full sweep
right okay so here's pong full sweep
without sticky actions immediately
without sticky actions immediately
solves the environment 20 is Max
solves the environment 20 is Max
score here's a full Sweep with sticky
score here's a full Sweep with sticky
actions doesn't solve the environment
actions doesn't solve the environment
score caps around 16 and if you go into
score caps around 16 and if you go into
the original makado paper uh they omit
the original makado paper uh they omit
pong results and break out another M's
pong results and break out another M's
from their reasoning uh is to why sticky
from their reasoning uh is to why sticky
actions don't hurt algorithmic
actions don't hurt algorithmic
performance but if you look at their
performance but if you look at their
original paper I have the exerpts in
original paper I have the exerpts in
this article you can actually see that
this article you can actually see that
they do not solve pong they get around
they do not solve pong they get around
the same score
the same score
15ish uh with their implementation
15ish uh with their implementation
compared to the original dqn solving it
compared to the original dqn solving it
at 20 and they also substantially under
at 20 and they also substantially under
perform on breakout which is a very
perform on breakout which is a very
consistent task uh compared to original
consistent task uh compared to original
dqn at 168 they get like half the score
dqn at 168 they get like half the score
so crazy finding
absolutely crazy
finding get this out of the way so I can
finding get this out of the way so I can
have it on the stream
nice random question how do you think
nice random question how do you think
Tesla SD
Tesla SD
works oh man that's a way longer topic
there I think is the talk public Andre
there I think is the talk public Andre
gave a really good talk about it when he
gave a really good talk about it when he
was a chief
was a chief
scientist he gave a really good talk at
scientist he gave a really good talk at
MIT I don't know if this if it was made
MIT I don't know if this if it was made
public you might be able to find it
public you might be able to find it
online
um it's changed a lot since then I'm
um it's changed a lot since then I'm
sure but
there's a good on YouTube yeah I mean
there's a good on YouTube yeah I mean
that's a beast of a topic it's not just
that's a beast of a topic it's not just
straight reinforcement
straight reinforcement
learning it's like imitation for the
learning it's like imitation for the
most part
nowadays I do think that FSD is the most
nowadays I do think that FSD is the most
impressive
impressive
deployed um like ml application in
deployed um like ml application in
general
okay let me make sure I title this right
I don't know sometimes PA gives you 21
I don't know sometimes PA gives you 21
score I'm pretty sure the max is 20 so I
score I'm pretty sure the max is 20 so I
don't know what's up with that
don't know what's up with that
but I think that's like a well-known
odity uh
currently Nas working on backend systems
currently Nas working on backend systems
thinking getting AI not sure where to
thinking getting AI not sure where to
start how much math is required depends
start how much math is required depends
where an AI you work either not very
where an AI you work either not very
much or a ton in the type of work I do
much or a ton in the type of work I do
my playbook I always say is about 5%
my playbook I always say is about 5%
math 15% experimental science and 80%
math 15% experimental science and 80%
straight
engineering you should probably at least
engineering you should probably at least
know Matrix calculus which is just
know Matrix calculus which is just
normal calculus when you replace the
normal calculus when you replace the
variables with a matrix and occasionally
variables with a matrix and occasionally
have to flip
have to flip
one what exactly do sticky actions do in
one what exactly do sticky actions do in
reinforcement learning okay so if
reinforcement learning okay so if
anybody if you've played games ever it's
anybody if you've played games ever it's
something that's incredibly stupid and
something that's incredibly stupid and
like is immediately obvious that would
like is immediately obvious that would
mess you up it makes your button sticky
mess you up it makes your button sticky
so
like if you press the button and then
like if you press the button and then
you go to press another button sometimes
you go to press another button sometimes
the first button will stick down for a
the first button will stick down for a
little bit and won't let you press the
little bit and won't let you press the
other button for a second so
other button for a second so
like obviously that's going to get you
like obviously that's going to get you
killed in a lot of games
right it's actually kind of obvious when
right it's actually kind of obvious when
you think about it for pong like you in
you think about it for pong like you in
order to solve pong you have to win 20
order to solve pong you have to win 20
straight
straight
games
games
right so like yeah of course over 20
right so like yeah of course over 20
games like having your buttons be sticky
games like having your buttons be sticky
is going to mess you up on a few of them
is going to mess you up on a few of them
for
that doesn't sound useful why is it even
that doesn't sound useful why is it even
a feature in for this model it's like
a feature in for this model it's like
the typical scientist thing like this is
the typical scientist thing like this is
like such an academic thing so the
like such an academic thing so the
justification for this was that uh if
justification for this was that uh if
you since a Atari is deterministic you
you since a Atari is deterministic you
can construct a really stupid algorithm
can construct a really stupid algorithm
that just memorizes the optimal sequence
that just memorizes the optimal sequence
of actions right but if you mess with
of actions right but if you mess with
the buttons every so often then you're
the buttons every so often then you're
not going to be able to just memorize
not going to be able to just memorize
stuff because you're going to go off of
stuff because you're going to go off of
like the exact sequence of actions that
like the exact sequence of actions that
you memorized right and then they did
you memorized right and then they did
some experiments that show that ah the
some experiments that show that ah the
sticky action thing doesn't seem to hurt
sticky action thing doesn't seem to hurt
they didn't do comprehensive experiments
they didn't do comprehensive experiments
despite later the same authors is
despite later the same authors is
writing a paper about how RL is all
writing a paper about how RL is all
wrong because nobody does comprehensive
wrong because nobody does comprehensive
experiments and then this just like got
experiments and then this just like got
600 citations and got put into the
600 citations and got put into the
default rappers of everything and is now
default rappers of everything and is now
making all of like RL
wrong it's like such a like you know
wrong it's like such a like you know
like scientist who's never played games
like scientist who's never played games
kind of a thing as well cuz if you think
kind of a thing as well cuz if you think
about that for 2 seconds like this is
about that for 2 seconds like this is
how I figured out that this was wrong in
how I figured out that this was wrong in
the first place right like I was getting
the first place right like I was getting
slightly weird results on pong and then
slightly weird results on pong and then
I noticed sticky actions were default
I noticed sticky actions were default
and I just went like
and I just went like
huh sticky action seems like it would
huh sticky action seems like it would
make it really hard to play a game
right FD
that's not Ramy Ismail like the
that's not Ramy Ismail like the
well-known Game Dev right I
well-known Game Dev right I
assume
fdsp what's
fdsp isn't determinism
fdsp isn't determinism
bad
um sticky actions might act as a
um sticky actions might act as a
regularizer yeah but in this case it
regularizer yeah but in this case it
provably breaks the
provably breaks the
environment so
so it's a sucky yeah but it's not just a
so it's a sucky yeah but it's not just a
sucky regularizer it's a sucky
sucky regularizer it's a sucky
regularizer that's baked into the
regularizer that's baked into the
environment is a default setting when
environment is a default setting when
you instantiate so when you like make
you instantiate so when you like make
the environment by default this thing is
the environment by default this thing is
enabled what's the tldr in the story uh
enabled what's the tldr in the story uh
default rapper in the most common
default rapper in the most common
Benchmark in exist distance makes the
Benchmark in exist distance makes the
environments
unsolvable where's entropy in here
entropy is
entropy is
okay and with this
one oh
so I need to include these loss curves
right for
oh I this needs to go down
here a really low resed
here a really low resed
screenshot welcome everyone
screenshot welcome everyone
that is a lot of people on YouTube it
that is a lot of people on YouTube it
seems when you make a really clickbait
seems when you make a really clickbait
though true title people show up um the
though true title people show up um the
tldr for folks that just got here I
tldr for folks that just got here I
found a bug in the default pre the I
found a bug in the default pre the I
found that the default processor used
found that the default processor used
for Atari that is proposed in a landmark
for Atari that is proposed in a landmark
paper and is said not to degrade
paper and is said not to degrade
performance during training
performance during training
significantly degrades performance
significantly degrades performance
during training for at least one
during training for at least one
environment probably multiple
environment probably multiple
based on both my own extensive
based on both my own extensive
experiments and the data presented in
experiments and the data presented in
the original paper when you cross check
the original paper when you cross check
it against uh cross check their appendix
it against uh cross check their appendix
against the paper that they refer to so
against the paper that they refer to so
yeah this is a big deal um this article
yeah this is a big deal um this article
will be published tomorrow I will talk
will be published tomorrow I will talk
about it a little bit on stream it will
about it a little bit on stream it will
be on my ex which let me I'll post the
be on my ex which let me I'll post the
link uh because it will be
link uh because it will be
here and if you are generally interested
here and if you are generally interested
in my work which is attempting to make
in my work which is attempting to make
reinforcement learning sane easy and
reinforcement learning sane easy and
usable all the stuff that I'm working on
usable all the stuff that I'm working on
including the stuff I use for the
including the stuff I use for the
experiments is available in poer lib
experiments is available in poer lib
this is the main library that I develop
this is the main library that I develop
it's all free and open source if you
it's all free and open source if you
want to help me out a whole ton give it
want to help me out a whole ton give it
a star it really helps me out that's the
a star it really helps me out that's the
TA drr going to finish up working on
TA drr going to finish up working on
this article here um cuz so another
this article here um cuz so another
thing this was got a general question on
thing this was got a general question on
puffer lib seem you leverage syon to
puffer lib seem you leverage syon to
speed up your M's why did you choose
speed up your M's why did you choose
that over a number jack or going end to
that over a number jack or going end to
endend on the GPU with something like
endend on the GPU with something like
Puda uh because Jax is a domain specific
Puda uh because Jax is a domain specific
language which heavily restricts the
language which heavily restricts the
type of environments that you can make
type of environments that you can make
I'm able to get millions of steps per
I'm able to get millions of steps per
second single threaded with way way way
second single threaded with way way way
simpler code for much more complex
simpler code for much more complex
simulations I have a miniature version
simulations I have a miniature version
of DOTA that is about 1300 lines of
of DOTA that is about 1300 lines of
scyon that essentially looks like normal
scyon that essentially looks like normal
python code it runs at a million steps
python code it runs at a million steps
per second single-threaded I've got this
per second single-threaded I've got this
massively multi-agent snake that's like
massively multi-agent snake that's like
200 lines of scon looks like normal
200 lines of scon looks like normal
python runs it over 10 million steps per
python runs it over 10 million steps per
second single threaded all without
second single threaded all without
having to write your code in a weird
having to write your code in a weird
like vector domain specific language
like vector domain specific language
that is why I actually have an article
that is why I actually have an article
on this as well uh this is more of a
on this as well uh this is more of a
bait article honestly but people liked
it where's this
here's this
here's this
article I am not a fan of Jax as a
article I am not a fan of Jax as a
general replacement for uh environment
general replacement for uh environment
infrastructure in RL I think it is a
infrastructure in RL I think it is a
major step in the wrong direction Jax is
major step in the wrong direction Jax is
incredibly useful for physics stuff and
incredibly useful for physics stuff and
for like specific aife work not as a
for like specific aife work not as a
general replacement for environment code
let me grab that
let me grab that
figure how do I get this to
be this give me reasonable font
sizes yeah there we
sizes yeah there we
go R
go R
entropy so here's the entropy loss you
entropy so here's the entropy loss you
can see that this is stable it's not
can see that this is stable it's not
crashing
entropy and let me make sure I grabbed
entropy and let me make sure I grabbed
the right chart as
the right chart as
well gotcha that's very clarifying and
well gotcha that's very clarifying and
for number is the idea since the code is
for number is the idea since the code is
running in C there's nothing to gain
running in C there's nothing to gain
from numbers jit yes my environments
from numbers jit yes my environments
will run uh like the mobile runs in pure
will run uh like the mobile runs in pure
C with no calls back to
C with no calls back to
python at least know the init is slow
python at least know the init is slow
but who cares if the init is slow I
but who cares if the init is slow I
didn't bother optimizing it but like the
didn't bother optimizing it but like the
step function will run at Native C
speed y I did this right right
let me uh filter
let me uh filter
this
this
by uh let me filter this by performance
by uh let me filter this by performance
of over
10 another good way of showing the uh
10 another good way of showing the uh
the jack thing is just like go read the
the jack thing is just like go read the
uh the environment simulation code in
uh the environment simulation code in
the dev Branch for like some of my M's
the dev Branch for like some of my M's
and you'll see how easy it is compared
and you'll see how easy it is compared
to trying to like wrap your head around
to trying to like wrap your head around
how to do everything in a ray ops it's
how to do everything in a ray ops it's
really easy
1 e- 4 to 5 eus
1 e- 4 to 5 eus
3 for
IUS 3
IUS 3
right 6us
3 where's the minimum
3 where's the minimum
value what was the minimum entropy value
value what was the minimum entropy value
I allowed
default I want to make sure that I
default I want to make sure that I
didn't I allowed uh pretty low entropy I
didn't I allowed uh pretty low entropy I
think I allowed pretty darn low entropy
right yeah one minus 5 that's what I
right yeah one minus 5 that's what I
thought
oh wait wait wait I did this backwards
oh wait wait wait I did this backwards
yeah yeah I got to do the other one
yeah yeah I got to do the other one
right
right
cuz yes I I have to do the other one
cuz yes I I have to do the other one
okay that was close almost screwed that
okay that was close almost screwed that
up that would have been very very
up that would have been very very
embarrassing and it's about the
same so we'll
same so we'll
do environment return greater than equal
do environment return greater than equal
to
to
15 got to be careful if you're going to
15 got to be careful if you're going to
claim that everything's broken you got
claim that everything's broken you got
to make sure that you do your analys say
to make sure that you do your analys say
bulletproof
right e
1.3 to
1.5
1.0 1.0 to
1.0 1.0 to
1.5 1.4
okay that was
close
so I need to increase the size of this
so I need to increase the size of this
right
at 125%
okay so based on this analysis right
the the best
the the best
runs in this one which is
runs in this one which is
the uh this is the the good
the uh this is the the good
one yes so we find entropy values of
one yes so we find entropy values of
around
around
0.9 to
0.9 to
1.4 compared to
1.4 compared to
1.3 to
1.5 honestly I'm shocked that sticky
1.5 honestly I'm shocked that sticky
actions only they don't always hurt
actions only they don't always hurt
training they don't always hurt
training they don't always hurt
training in fact it's kind of weird um
training in fact it's kind of weird um
looks like they do better on some some
looks like they do better on some some
enes but the thing is the claim was that
enes but the thing is the claim was that
sticky actions do not hurt training that
sticky actions do not hurt training that
claim is wrong and they are incredibly
claim is wrong and they are incredibly
harmful to some
environments and if you're going to use
environments and if you're going to use
sticky actions okay here's a really
sticky actions okay here's a really
simple thing right if you're going to
simple thing right if you're going to
use sticky actions you sweep it as a
use sticky actions you sweep it as a
hyperparameter you don't fix it across
hyperparameter you don't fix it across
all the environments and you especially
all the environments and you especially
don't fix it to a crazy high value like
don't fix it to a crazy high value like
0.25 which is the default one in four
0.25 which is the default one in four
button presses the button sticks
down isn't that
down isn't that
crazy I mean I'm sure that like there
crazy I mean I'm sure that like there
probably at least a few people in the
probably at least a few people in the
audience here who do not play games so I
audience here who do not play games so I
mean this is a crazy analogy but like
mean this is a crazy analogy but like
imagine you're trying to play a game and
imagine you're trying to play a game and
then every couple seconds somebody comes
then every couple seconds somebody comes
by and slaps you in the face that'd be
by and slaps you in the face that'd be
pretty distracting right well okay
pretty distracting right well okay
that's kind of what it's like when your
that's kind of what it's like when your
buttons stick down and you can't like
buttons stick down and you can't like
actually press the buttons you want
so there's not a huge entropy difference
here where's the uh The Sweep
here where's the uh The Sweep
graph
oops where's entropy
entropy coefficient
entropy coefficient
is
0.09
0.09
to 0.04 perfect
hate when i d rank because someone keeps
hate when i d rank because someone keeps
slapping me in the face yeah well I hate
slapping me in the face yeah well I hate
when my RL agent when I like I spin my
when my RL agent when I like I spin my
gpus all night and my RL agent doesn't
gpus all night and my RL agent doesn't
work CU somebody keeps holding my
work CU somebody keeps holding my
buttons down it's a great way to push my
buttons down it's a great way to push my
buttons
so I have 18 graphs
here I think I could do this one a
here I think I could do this one a
little better
right oh no I did do this right
like I swear this is not just like this
like I swear this is not just like this
is not normal in science like just the
is not normal in science like just the
amount of stuff that I find in RL here
amount of stuff that I find in RL here
like and I wasn't necessarily looking
like and I wasn't necessarily looking
for this when I was a PhD student right
for this when I was a PhD student right
cuz there's like not a ton of incentive
cuz there's like not a ton of incentive
to um but now that I'm just like
to um but now that I'm just like
actually from a very practical and
actually from a very practical and
grounded perspective just trying to make
grounded perspective just trying to make
stuff work by whatever means necessary I
stuff work by whatever means necessary I
just find this stuff all the time it's
nuts why is this one so much smaller
okay there we
go I got to say though the progress in
go I got to say though the progress in
reinforcement learning
reinforcement learning
lately just like the progress I've made
lately just like the progress I've made
in puffer lib in the last couple of
in puffer lib in the last couple of
months RL is just such a different place
months RL is just such a different place
already and it's going to continue to
already and it's going to continue to
get better and
get better and
better um at this point like carb sweeps
better um at this point like carb sweeps
are pretty close to just solving
are pretty close to just solving
whatever out of the box and all we need
whatever out of the box and all we need
really is fast environments and like I
really is fast environments and like I
have the snake M I've got the particle
have the snake M I've got the particle
M's I've almost you know I'm decently
M's I've almost you know I'm decently
through on the Moa I've got a couple
through on the Moa I've got a couple
other Ms I'm working on behind the
other Ms I'm working on behind the
scenes like and once we get this like
scenes like and once we get this like
the template out like once we get the
the template out like once we get the
the methodology down for just cranking
the methodology down for just cranking
out these fast environments like we can
out these fast environments like we can
do say like seven Atari environments I
do say like seven Atari environments I
could probably crank those out into two
could probably crank those out into two
weeks so we'd have like an Atari 7 but
weeks so we'd have like an Atari 7 but
it runs at a million FPS instead of like
it runs at a million FPS instead of like
40K million FPS on One Core even we
40K million FPS on One Core even we
could do as long as we're not rendering
could do as long as we're not rendering
fully like State based
fully like State based
um yeah we can do a lot of stuff we can
um yeah we can do a lot of stuff we can
do a lot of stuff really
do a lot of stuff really
quickly maybe we can even do low F low
quickly maybe we can even do low F low
Fidelity rendering for all I know might
Fidelity rendering for all I know might
not be that
not be that
hard I mean they they always down sample
hard I mean they they always down sample
them like crazy anyways probably the
them like crazy anyways probably the
observations we'll be given them will be
observations we'll be given them will be
more reasonable than the squashed little
more reasonable than the squashed little
frames that we use
normally going to say one more time
normally going to say one more time
because the stream is doing particularly
because the stream is doing particularly
well tonight uh if you want to support
well tonight uh if you want to support
all this stuff it's all open source all
all this stuff it's all open source all
the sweeps all the tools and things I'm
the sweeps all the tools and things I'm
using they're all in puffer lib it's
using they're all in puffer lib it's
right
right
here working on this fulltime if you
here working on this fulltime if you
want to start the repo it helps me out
want to start the repo it helps me out
an absolute
an absolute
ton the growth has been just just nuts
ton the growth has been just just nuts
I'm really really happy with the how
I'm really really happy with the how
this has done over the last couple of
this has done over the last couple of
months and I'm not going anywhere you
months and I'm not going anywhere you
know it's it's going to keep going from
know it's it's going to keep going from
here okay let's do a little bit of
here okay let's do a little bit of
analysis since that's what people are
analysis since that's what people are
here for not puffer advertising
here for not puffer advertising
um the axis are weird on this graph why
um the axis are weird on this graph why
did they do
this it's okay
this it's okay
it's
it's
okay
so entropy of best runs or greater than
so entropy of best runs or greater than
15 without sticky actions ranges from
15 without sticky actions ranges from
0.9 to
1.4 entropy
1.4 entropy
coefficient for these runs
is entropy
is that's one e-
3 no it's one eus two right one-
2 4 - 3
be of best runs score greater than 10
be of best runs score greater than 10
with
with
sticky ranges from
1.3 to
1.3 to
1.5 entropy coefficient for these runs
1.5 entropy coefficient for these runs
is and let's go grab the entropy
is and let's go grab the entropy
coefficient for these
that's 60 minus
3 wait 1 eus 3 to 6 eus
3 1us 3 to 6 eus 3 is that right
1 eus 3 6 eus
3 don't need this
um where's my
draft okay
good I'll put this note somewhere else
H I don't really need to say this
H I don't really need to say this
because I have 100 and 200 mil and I
because I have 100 and 200 mil and I
checked it against
both okay let's go through
both okay let's go through
this and then we'll find a we'll make
this and then we'll find a we'll make
like a fun graphic for this and uh I'm
like a fun graphic for this and uh I'm
going to order myself some food and
going to order myself some food and
we'll end just like that this will just
we'll end just like that this will just
be a short stream answer a couple
be a short stream answer a couple
questions if anybody has any but this is
questions if anybody has any but this is
like oh yeah yeah I got to check the
like oh yeah yeah I got to check the
MOBA results as well cuz we have some
MOBA results as well cuz we have some
sweeps that have
sweeps that have
run I actually trained a cool policy
run I actually trained a cool policy
that hopefully is
good sticky actions considered harmful
good sticky actions considered harmful
Atari is the most widely used Benchmark
Atari is the most widely used Benchmark
in RL it is standard practice to
in RL it is standard practice to
randomly repeat agent key presses making
randomly repeat agent key presses making
action sticky this technique was
action sticky this technique was
introduced by a landmark paper in 2017
introduced by a landmark paper in 2017
with 600 plus citations and it is the
with 600 plus citations and it is the
default setting in Al Pi the motivation
default setting in Al Pi the motivation
was to prevent algorithms from
was to prevent algorithms from
hypothetically memorizing a fixed set of
hypothetically memorizing a fixed set of
key presses to win the game which is
key presses to win the game which is
possible because Atari is deterministic
possible because Atari is deterministic
sticky actions were broadly adopted
sticky actions were broadly adopted
because they cover this Edge case and
because they cover this Edge case and
according to the authors they do not
according to the authors they do not
degrade the performance of dqn today I
degrade the performance of dqn today I
use data in the original paper to show
use data in the original paper to show
that this claim is false I further
that this claim is false I further
demonstrate that sticky actions can
demonstrate that sticky actions can
massively degrade the performance of Po
massively degrade the performance of Po
which is not learning deterministic
which is not learning deterministic
policies to begin
with
with
uh the dqn
paper does this go here wait I don't
paper does this go here wait I don't
think that figure goes there original
think that figure goes there original
dqn
dqn
result wait this
result wait this
goes yeah just pasted this one here this
goes yeah just pasted this one here this
shouldn't go
shouldn't go
here and then I have my little Joseph
here and then I have my little Joseph
Suarez is a new Li mined MIT PhD and
Suarez is a new Li mined MIT PhD and
fulltime oral Exorcist I build yeah this
fulltime oral Exorcist I build yeah this
is fine I'm just checking for grammar
is fine I'm just checking for grammar
errors and reading through
errors and reading through
uh here's the table of blading
uh here's the table of blading
stochastic sticky versus deterministic
stochastic sticky versus deterministic
actions and it's cropped nicely the
actions and it's cropped nicely the
impact of sto stoas staticity on
impact of sto stoas staticity on
different algorithms asteris beam Rider
different algorithms asteris beam Rider
free yep here are the original dqn
results notice that makato at Al only
results notice that makato at Al only
oblate three of the original
oblate three of the original
environments the three overlapping
environments the three overlapping
results roughly match original dqn does
results roughly match original dqn does
a bit better on cqu makato
out uh do a bit better on beam Rider and
out uh do a bit better on beam Rider and
Space Invaders makato at all lists full
Space Invaders makato at all lists full
Atari results with sticky actions in the
Atari results with sticky actions in the
appendix comparing this to the original
appendix comparing this to the original
dqn results we see that makato at all
dqn results we see that makato at all
substantially underperform on pong and
substantially underperform on pong and
breakout these are two of the best known
breakout these are two of the best known
and most consistent Atari environments
and most consistent Atari environments
as further Evidence I ran a large hyper
as further Evidence I ran a large hyper
perimeter sweep on pong using puffer
perimeter sweep on pong using puffer
lips poo plus lstm implementation with
lips poo plus lstm implementation with
and without sticky actions the
and without sticky actions the
environment is solved almost immediately
environment is solved almost immediately
without sticky actions but only gets a
without sticky actions but only gets a
maximum of around 16 score with them
maximum of around 16 score with them
which closely matches makato at 's
which closely matches makato at 's
reported performance
make sure I got the right
make sure I got the right
graphs carb sweep without sticky actions
graphs carb sweep without sticky actions
solves the
M yep quickly solves the environment at
M yep quickly solves the environment at
20 Max
20 Max
score carbs hyper parameter Sweep with
score carbs hyper parameter Sweep with
sticky action score gets stuck around 16
sticky action score gets stuck around 16
close to makato Al's reported
close to makato Al's reported
result but isn't determined as in bad
result but isn't determined as in bad
maybe but po isn't learning a
maybe but po isn't learning a
deterministic policy even without sticky
deterministic policy even without sticky
actions entropy stays above 1.0 for all
actions entropy stays above 1.0 for all
runs note that the entropy coefficient
runs note that the entropy coefficient
is included in the hyperparameter sweep
is included in the hyperparameter sweep
so low entropy zones are explored all
so low entropy zones are explored all
the best runs above 10
score range
score range
from hold
from hold
on 60 minus 3 is the low end 4 eus 3 to
on 60 minus 3 is the low end 4 eus 3 to
1 eus 2
is it 4us
is it 4us
yeah is it really like that wait 4 e
yeah is it really like that wait 4 e
minus
3 that's 4 e
3 that's 4 e
[Music]
[Music]
minus yeah that's 1 E minus
minus yeah that's 1 E minus
3 and then this one
4 minus 3 thanks for the stream night
4 minus 3 thanks for the stream night
good night thanks for dropping
good night thanks for dropping
by this will be published in the
morning obviously I'm not going to
morning obviously I'm not going to
publish it at this hour because
publish it at this hour because
algorithm
eus
3 look forward to reading it good luck
3 look forward to reading it good luck
thank you this is a pretty quick
thank you this is a pretty quick
write I'm not doing uh super long I'm
write I'm not doing uh super long I'm
not spending tons of time writing stuff
not spending tons of time writing stuff
these days it's like if I see something
these days it's like if I see something
I write a quick
I write a quick
article and uh then I move on back to
article and uh then I move on back to
more Dev
more Dev
right these are like pretty high impact
right these are like pretty high impact
for the amount of
for the amount of
uh the amount of time they take which is
uh the amount of time they take which is
nice and the opposite of
papers entropy stays above 1.0 for
papers entropy stays above 1.0 for
all all runs is it all runs that it says
all all runs is it all runs that it says
above 1.0 or just the good runs
that one's
good 0.8
note that the entropy coefficient is
note that the entropy coefficient is
included in the hyper parameter sweep so
included in the hyper parameter sweep so
low entropy zones are explored the sweep
low entropy zones are explored the sweep
was allowed to set as low as 1 E was
was allowed to set as low as 1 E was
allowed to set as low as 1 E minus 5 but
allowed to set as low as 1 E minus 5 but
there were no good runs in this range
there were no good runs in this range
all the best runs set entropy above 1
all the best runs set entropy above 1
minus 3
enty of best runs without sticky
enty of best runs without sticky
actions from 0.9 to
1.4 what's up
linky we found uh crazy
linky we found uh crazy
bugging like the
bugging like the
biggest RL library rln library out there
biggest RL library rln library out there
entropy coefficient from these runs
entropy coefficient from these runs
range from 4us 3 to oneus two this is
range from 4us 3 to oneus two this is
good
good
entropy of best runs with sticky
actions so bad
and we can just stop using it right
bet has raided the
bet has raided the
stream welcome
bet we found a crazy
bet we found a crazy
thing we found a real crazy thing
we're all in voice
we're all in voice
chat uh let me finish this paragraph I
chat uh let me finish this paragraph I
will order my food answer a couple
will order my food answer a couple
questions if there are any here and then
questions if there are any here and then
I'll come
by an assassination tweet what
what what did you
say wait did I miss like a major news
thing oh oh dude okay you confused the
thing oh oh dude okay you confused the
heck out of me uh no I said that
heck out of me uh no I said that
um I said that let me finish this
um I said that let me finish this
article let me finish writing this
article let me finish writing this
paragraph let me order my food and then
paragraph let me order my food and then
I'll swing by uh The Voice after maybe I
I'll swing by uh The Voice after maybe I
answer a few questions I don't know what
answer a few questions I don't know what
the heck you thought you heard me
the heck you thought you heard me
say you said that and I thought I missed
say you said that and I thought I missed
like a major US
like a major US
event derisive
event derisive
tweet no I'm just posting I'm not going
tweet no I'm just posting I'm not going
to post this article now this is a bad
to post this article now this is a bad
time zone to post on I'll post this in
time zone to post on I'll post this in
the
morning
e
e
e
e
e
e e
I should rename this discussion
we're
we're
moving I'm going to make a cool point
moving I'm going to make a cool point
for uh for some of the viewers here
for uh for some of the viewers here
since some folks are still
since some folks are still
around
um so here's the point and then I'm
um so here's the point and then I'm
going to write this up into a paragraph
going to write this up into a paragraph
um you have to take into account that
um you have to take into account that
this is not just a onef frame delay
this is not just a onef frame delay
right you're training with frame frame
right you're training with frame frame
skip usually frame skip uh 3 to five
skip usually frame skip uh 3 to five
usually four or five Mak used frame skip
usually four or five Mak used frame skip
five so you submit your action right it
five so you submit your action right it
delays five
delays five
frames and then
frames and then
potentially your action gets
potentially your action gets
repeated and then it skips another five
repeated and then it skips another five
frames so you're delayed a full 10
frames so you're delayed a full 10
frames right which is a sixth of a
frames right which is a sixth of a
second which is quite a substantial
second which is quite a substantial
amount of time but then there's just
amount of time but then there's just
another one in five chance that it skips
another one in five chance that it skips
again so 15 frames right which is now uh
again so 15 frames right which is now uh
a quarter of a second which is
a quarter of a second which is
definitely enough time in like pong for
definitely enough time in like pong for
you to miss the ball
you to miss the ball
right so that's one in uh one and 16
right so that's one in uh one and 16
chance for that and then a 1 and 64
chance for that and then a 1 and 64
chance that you miss 20
chance that you miss 20
frames and mind you you're not just
frames and mind you you're not just
missing the frame your key is being held
missing the frame your key is being held
down so like your paddle is going past
down so like your paddle is going past
the ball or
something
yeah
e
e e
okay this is wrong I do like
okay this is wrong I do like
this
for e
let me do the full math on
this 17
seconds oh I have to be a little careful
seconds oh I have to be a little careful
with this right
okay I'm going to explain this math in a
okay I'm going to explain this math in a
second let me finish writing it
out
e e
the one additional thing I should do is
the one additional thing I should do is
I should try to actually play the stupid
I should try to actually play the stupid
game with the frame
game with the frame
skip I actually don't think that the
skip I actually don't think that the
rapper will let you though I think if
rapper will let you though I think if
you do human mode it won't let
you for
and then there was one other point so
and then there was one other point so
let me let me go through this again so
let me let me go through this again so
um if you just do out the basic math I'd
um if you just do out the basic math I'd
kind of done this in my head but I
kind of done this in my head but I
didn't even realize it was this bad now
didn't even realize it was this bad now
I understand why you're losing like four
I understand why you're losing like four
or five points a game instead of just
or five points a game instead of just
one based on this
one based on this
um if you do the math with atari's frame
um if you do the math with atari's frame
rate uh about every 85 seconds your
rate uh about every 85 seconds your
controls just lock up for a half of a
controls just lock up for a half of a
second so you know these games are
second so you know these games are
pretty reactive like the Atari games are
pretty reactive like the Atari games are
actually pretty
actually pretty
fast they're actually pretty hard um
fast they're actually pretty hard um
your controls locking up for a second a
your controls locking up for a second a
half a second means the paddle is going
half a second means the paddle is going
to slide right by the ball and you just
to slide right by the ball and you just
lose a
lose a
point simple as
point simple as
that you know the really sad thing about
that you know the really sad thing about
this is that this could have been
this is that this could have been
present prevented by somebody having
present prevented by somebody having
like tried to play any of these games
like tried to play any of these games
with sticky actions
on can we even do that
we're going to definitely do that before
we're going to definitely do that before
I post this
I post this
article it's getting late so I'm not
article it's getting late so I'm not
going to do it right now I'm going to
going to do it right now I'm going to
finish writing
it what was the other
objection
for
e e
why is regularizer not a
why is regularizer not a
word go
away
e
e
e e
[Music]
yeah e
okay hold the ble could have been
okay hold the ble could have been
prevented by Trying by spending a couple
prevented by Trying by spending a couple
hours trying to play Atari with sticky
hours trying to play Atari with sticky
actions
rerunning sweeps
we'll just duplicate the
sentence
sentence
okay let me make sure these last couple
okay let me make sure these last couple
paragraphs are good I'll obviously read
paragraphs are good I'll obviously read
this one more time in the
this one more time in the
morning won't but won't removing sticky
morning won't but won't removing sticky
actions make Atari too easy maybe but
actions make Atari too easy maybe but
this is a dumb source of difficulty the
this is a dumb source of difficulty the
math for this is very simple and obvious
math for this is very simple and obvious
to anyone who's actually played Atari
to anyone who's actually played Atari
makato all use frames use a frame skip
makato all use frames use a frame skip
of five this means that the agent
of five this means that the agent
submits an action five frames pass and
submits an action five frames pass and
then control returns to the player there
then control returns to the player there
is a one and four chance that their
is a one and four chance that their
action gets repeated delaying the return
action gets repeated delaying the return
of control for another five frames so
of control for another five frames so
one and four for a 10 frame delay yada
one and four for a 10 frame delay yada
yada
yada
yada accounting for frame skip you
yada accounting for frame skip you
should expect a 30 frame delay every
should expect a 30 frame delay every
1024 * 5 is 5 5120 frames or every 85
1024 * 5 is 5 5120 frames or every 85
seconds with atari's 60 FPS so every 85
seconds with atari's 60 FPS so every 85
seconds if you're playing
seconds if you're playing
pong your controls freeze up for half a
pong your controls freeze up for half a
second and the paddle keep sliding not
second and the paddle keep sliding not
exactly the best source of difficulty I
exactly the best source of difficulty I
noticed makado Al do better on some
noticed makado Al do better on some
environments not all games will be lost
environments not all games will be lost
if you lose control for half a second in
if you lose control for half a second in
these
these
cases it is well in the span
of not all games will be lost in the
of not all games will be lost in the
span of half half a second in these
span of half half a second in these
cases it is possible that sticky actions
cases it is possible that sticky actions
act as a regularizer there are probably
act as a regularizer there are probably
better ways a to achieve the same result
better ways a to achieve the same result
but if you really want to use sticky
but if you really want to use sticky
actions treat it as a hyper parameter
actions treat it as a hyper parameter
sweeps can then automatically discover
sweeps can then automatically discover
the best value based on the environment
the best value based on the environment
hey JBL we found something crazy one
hey JBL we found something crazy one
second and I'll tell you sweeps can then
second and I'll tell you sweeps can then
automatically discover the best value
automatically discover the best value
based on the
based on the
environment but definitely don't set it
environment but definitely don't set it
to 0.25 for all environments in an
to 0.25 for all environments in an
earlier sweep I didn't find a good
earlier sweep I didn't find a good
setting for pong above
0.13 okay conclusions one of the biggest
0.13 okay conclusions one of the biggest
advantages of using games as research
advantages of using games as research
environments is that they are easy to
environments is that they are easy to
interpret all you have to do is play
interpret all you have to do is play
them this whole debacle could have been
them this whole debacle could have been
prevented by spending a couple hours
prevented by spending a couple hours
trying to play atar with sticky
trying to play atar with sticky
actions I will be removing the setting
actions I will be removing the setting
from puffer lib defaults and rerunning
from puffer lib defaults and rerunning
hyper parameter sweeps you can support
hyper parameter sweeps you can support
my work by following me in starring
my work by following me in starring
puffer perfect okay
puffer perfect okay
um it seems like most of the crowd has
um it seems like most of the crowd has
gotten bored of watching me write an
gotten bored of watching me write an
article which is very
article which is very
understandable we got a couple stars out
understandable we got a couple stars out
of this stream I'm very happy with that
of this stream I'm very happy with that
um did it end up being possible to song
um did it end up being possible to song
Hey reu and JBL okay welcome back no I
Hey reu and JBL okay welcome back no I
was completely right this thing is
was completely right this thing is
cursed and this is destroyed Atari as a
cursed and this is destroyed Atari as a
benchmark it's that bad um yes I proved
benchmark it's that bad um yes I proved
it yo will it's it's that bad so I ran
it yo will it's it's that bad so I ran
comprehensive hyper parameter Sweep with
comprehensive hyper parameter Sweep with
their sticky actions uh it caps out
their sticky actions uh it caps out
about 16 which is exactly the number
about 16 which is exactly the number
that they reported in their original
that they reported in their original
results with dqn
results with dqn
uh I ran you know 168 experiments and
uh I ran you know 168 experiments and
then if I run it without sticky
then if I run it without sticky
actions it immediately finds hyper
actions it immediately finds hyper
parameters that solve the environment at
parameters that solve the environment at
20 it's that
20 it's that
bad so uh I did a whole bunch of
bad so uh I did a whole bunch of
additional analysis I've got an article
additional analysis I've got an article
ready to go for tomorrow but yeah
ready to go for tomorrow but yeah
like completely destroy the field with
like completely destroy the field with
this one easy
this one easy
trick I mean can you imagine how stupid
trick I mean can you imagine how stupid
that is like of course reinforce M looks
that is like of course reinforce M looks
dumb if like of course reinforcement
dumb if like of course reinforcement
learning looks stupid right if like we
learning looks stupid right if like we
can't solve pong well it turns out we
can't solve pong well it turns out we
can't solve pong because we're we have
can't solve pong because we're we have
this stupid rapper well let me put it
this stupid rapper well let me put it
this way I did some back of the envelope
this way I did some back of the envelope
every 85 seconds on average that the
every 85 seconds on average that the
agent plays pong the controls freeze up
agent plays pong the controls freeze up
for a full half of a second and the
for a full half of a second and the
paddle keeps sliding if you've played
paddle keeps sliding if you've played
Atari it's kind of fast right yeah you
Atari it's kind of fast right yeah you
totally lose in that half a second that
totally lose in that half a second that
the paddle slides back you're
the paddle slides back you're
done yeah this is Earth shattering
right
crazy so uh sticky actions considered
crazy so uh sticky actions considered
harmful publish this
harmful publish this
tomorrow
tomorrow
um dude this field is so cursed and it's
um dude this field is so cursed and it's
so avoidable
so avoidable
like think about it if anybody bothered
like think about it if anybody bothered
to play this for a couple hours yeah see
to play this for a couple hours yeah see
well if anybody bothered to play this
well if anybody bothered to play this
for half an hour even like you'd really
for half an hour even like you'd really
quickly see that you can't play the game
quickly see that you can't play the game
with these sticky actions on
right well we got this ready to go for
right well we got this ready to go for
tomorrow hopefully people see this one
tomorrow hopefully people see this one
oh let's go get a I'm going to go order
oh let's go get a I'm going to go order
myself dinner on the side here real
myself dinner on the side here real
quick uh and then I'm going to find a
quick uh and then I'm going to find a
cool title for this I'll answer some
cool title for this I'll answer some
chat questions if there are any and I'll
chat questions if there are any and I'll
go because it's already 8:00 p.m. I
go because it's already 8:00 p.m. I
actually usually go to bed by 9:00 um
actually usually go to bed by 9:00 um
because I get up early let me figure out
because I get up early let me figure out
what the heck I want to eat order
what the heck I want to eat order
something it's too late for me to do
something it's too late for me to do
anything else
U let me get some pasta real
quick wait so what happened let me
quick wait so what happened let me
explain in a second let me just order my
explain in a second let me just order my
dinner because otherwise like I'm going
dinner because otherwise like I'm going
to forget to and I'm going to be hungry
to forget to and I'm going to be hungry
um
parm just order myself like some chicken
parm just order myself like some chicken
parm or
parm or
something
perfect just order this and then I'll
perfect just order this and then I'll
tell you
cool just go
through I've been uh co-working
through I've been uh co-working
on uh a different cool environment that
on uh a different cool environment that
I'll I'll talk about well we'll see when
I'll I'll talk about well we'll see when
I talk about but all day so I haven't
I talk about but all day so I haven't
had time to do too much um okay so JBL
had time to do too much um okay so JBL
the thing that
the thing that
happened is I found in the default
happened is I found in the default
rapper of Atari so if you use Al
rapper of Atari so if you use Al
Pi so
what's yeah so this is a 2,000 star
what's yeah so this is a 2,000 star
repo
repo
right this is used in like every paper
right this is used in like every paper
like 90% of Atari uh 90% of RL papers
like 90% of Atari uh 90% of RL papers
out there the default setting in this
out there the default setting in this
includes sticky actions by default
includes sticky actions by default
uh if you don't look at it it has scky
actions here's a paper that introduced
actions here's a paper that introduced
sticky actions it has over 600 citations
sticky actions it has over 600 citations
massive paper for reinforcement
massive paper for reinforcement
learning the paper says that sticky
learning the paper says that sticky
actions do not hurt performance in dqn
actions do not hurt performance in dqn
it is wrong I've proved I can prove it's
it is wrong I've proved I can prove it's
wrong based on only the data in this
wrong based on only the data in this
paper and the original dqn paper you can
paper and the original dqn paper you can
see it's wrong I ran 300 follow-up uh
see it's wrong I ran 300 follow-up uh
experiments 300 trials like 250
experiments 300 trials like 250
experiment hyper parameter sweeps on
experiment hyper parameter sweeps on
pong with po as well I replicate almost
pong with po as well I replicate almost
exactly their result which is that they
exactly their result which is that they
do not even learn how to play Pong with
do not even learn how to play Pong with
sticky actions immediately solve it
sticky actions immediately solve it
within a couple of Trials of hyper pram
within a couple of Trials of hyper pram
sweep without sticky
actions show that the original
actions show that the original
motivation for doing this doesn't make
motivation for doing this doesn't make
any sense because these algorith are not
any sense because these algorith are not
learning deterministic policies anyways
learning deterministic policies anyways
it was like this weird hypothetical
it was like this weird hypothetical
construction like an invented problem
construction like an invented problem
this solves an invented problem and
this solves an invented problem and
invents a real problem that's what this
invents a real problem that's what this
does this solves an invented problem and
does this solves an invented problem and
invents a real
problem so yeah big day for RL I mean
problem so yeah big day for RL I mean
you got to think about it as well like
you got to think about it as well like
how stupid does reinforcement learning
how stupid does reinforcement learning
look when you can't even solve pong
look when you can't even solve pong
right
right
how stupid does reinforcement learning
how stupid does reinforcement learning
look when you can't even solve pong well
look when you can't even solve pong well
it turns out you can solve pong you can
it turns out you can solve pong you can
solve
solve
pong how quickly do you think we can
pong how quickly do you think we can
solve pong I haven't even checked I'm
solve pong I haven't even checked I'm
confident let's do this live as we'll do
confident let's do this live as we'll do
it
live filter above
15 we're going to consider 19 score to
15 we're going to consider 19 score to
be solved I'd say right 20 score okay
be solved I'd say right 20 score okay
whatever 20 score in 3 minutes we solve
whatever 20 score in 3 minutes we solve
Paul in 3 minutes one
Paul in 3 minutes one
GPU not even using C++
GPU not even using C++
vectorization no fancy tricks we solve
vectorization no fancy tricks we solve
it in 3
minutes if we do it by samples because
minutes if we do it by samples because
people like sample efficiency for some
people like sample efficiency for some
reason it's not even going to be that
reason it's not even going to be that
bad
yeah it's like four million samples or
yeah it's like four million samples or
whatever compared to like 2 million if
whatever compared to like 2 million if
you use a sample efficient algorithm
you use a sample efficient algorithm
that takes two hours now this isn't dqn
that takes two hours now this isn't dqn
this is PO with an lstm this is just
this is PO with an lstm this is just
whatever I have in the puffer
defaults dqn does solve pong though it's
defaults dqn does solve pong though it's
dqn with this stupid rapper that doesn't
dqn with this stupid rapper that doesn't
solve
solve
pong article will be live tomorrow
pong article will be live tomorrow
but
um where is
it if you look at the original charts in
it if you look at the original charts in
the paper basically and then you look at
the paper basically and then you look at
the dqn charts original dqn solves pong
the dqn charts original dqn solves pong
perfectly their version with sticky
perfectly their version with sticky
actions does not solve pong they get a
actions does not solve pong they get a
score of about 15 my PO with an lstm
score of about 15 my PO with an lstm
gets a score of about 15 if you use
gets a score of about 15 if you use
their stupid sticky actions thing uh if
their stupid sticky actions thing uh if
you do the math that checks out uh
you do the math that checks out uh
basically sticky actions make you lose a
basically sticky actions make you lose a
quarter of your games that
quarter of your games that
simple and it makes a ton of sense if
simple and it makes a ton of sense if
you think through it as well I do some
you think through it as well I do some
like envelope math down
like envelope math down
here ain't that
here ain't that
funny is
funny is
stupid stop doing
stupid stop doing
that stop inventing
that stop inventing
problems enough
problems chat chippity has got to get me
problems chat chippity has got to get me
a tit all
narrate a
suitably
suitably
epic cover art in a 3:1 ratio landscape
epic cover art in a 3:1 ratio landscape
for this article let's see if it does
for this article let's see if it does
anything cool how do you go about
anything cool how do you go about
choosing the RL algorithm you use for
choosing the RL algorithm you use for
example poo and td3 can be used
example poo and td3 can be used
interchangeably but I usually just use
interchangeably but I usually just use
which I'm more familiar with without any
which I'm more familiar with without any
proper reasoning uh my reasoning is that
proper reasoning uh my reasoning is that
poo solves DOTA with a one layer lstm so
poo solves DOTA with a one layer lstm so
it's probably fine Poo's been the
it's probably fine Poo's been the
default since 2017 nothing has
default since 2017 nothing has
conclusively shown its
conclusively shown its
better this is kind of
funny this is actually kind of funny
funny this is actually kind of funny
artwork isn't
artwork isn't
it I kind of like
that all right I just said like hey I I
that all right I just said like hey I I
made a I proved like a crazy thing I
made a I proved like a crazy thing I
found a crazy thing generate me some
found a crazy thing generate me some
cool cover
cool cover
art yeah I generally like I don't even
art yeah I generally like I don't even
here's here's the thing I don't even
here's here's the thing I don't even
do a shoot I have to
like ah it's not going to be able to do
like ah it's not going to be able to do
the same image
again here
again here
same image let's see um I here's the
same image let's see um I here's the
thing though uh if you're still there
thing though uh if you're still there
like I don't even focus on algorithm
like I don't even focus on algorithm
implementations in puffer I've been
implementations in puffer I've been
making so so much progress in RL without
making so so much progress in RL without
even doing anything on the algorithm
even doing anything on the algorithm
implementation side because it's like
implementation side because it's like
infra being cursed is the main thing
infra being cursed is the main thing
infra is crazy
cursed more than anything
which one of these is the best
this one's pretty
cool the stupid aspect ratio is wrong
cool the stupid aspect ratio is wrong
though
I don't know if it actually can do the
I don't know if it actually can do the
aspect ratio I
aspect ratio I
want I really wish it had the ability to
want I really wish it had the ability to
like edit existing
like edit existing
images oh yeah thanks you made it less
images oh yeah thanks you made it less
wide perfect
I don't know we'll see maybe it crops
differently that's not bad
I like
that
that
cool um well this is ready to go for
cool um well this is ready to go for
tomorrow you all get a sneak peek for uh
tomorrow you all get a sneak peek for uh
dropping by the stream tonight I I'll
dropping by the stream tonight I I'll
say this one more time because this
say this one more time because this
stream keeps turning new people over um
stream keeps turning new people over um
this is all public work like the hyper
this is all public work like the hyper
pram sweeps for this the optimizer
pram sweeps for this the optimizer
algorithm the like solving pong in 3
algorithm the like solving pong in 3
minutes this is all free and available
minutes this is all free and available
in puffer lib if you want to help my
in puffer lib if you want to help my
workout I'm working on this full-time
workout I'm working on this full-time
all you have to do is start the repo
all you have to do is start the repo
that's all I ask it helps me out a whole
that's all I ask it helps me out a whole
ton this has been the growth on the
ton this has been the growth on the
project since I started working on it
project since I started working on it
full-time this is the exact type of
full-time this is the exact type of
thing that puffer lib needs like in
thing that puffer lib needs like in
order for me to actually get all this
order for me to actually get all this
out there and to fix all this stuff in
out there and to fix all this stuff in
RL I'm not going anywhere but the
RL I'm not going anywhere but the
support sure helps so thank
you other than that this article will be
you other than that this article will be
live tomorrow on X my ex
live tomorrow on X my ex
is strictly for uh research it's pretty
is strictly for uh research it's pretty
much that's all I do
much that's all I do
um I don't know occasionally I get bored
um I don't know occasionally I get bored
and post dumb replies to stuff but the
and post dumb replies to stuff but the
posts are just about
posts are just about
research um I'll put it in chat one more
research um I'll put it in chat one more
time cuz this is where the article will
time cuz this is where the article will
be tomorrow and uh I will be streaming
be tomorrow and uh I will be streaming
tomorrow
tomorrow
madev uh I will we'll see maybe if I
madev uh I will we'll see maybe if I
have time in the morning to like mess
have time in the morning to like mess
with these sweeps but
with these sweeps but
um I do
um I do
have a sneak preview which is this run
have a sneak preview which is this run
from last uh from this morning
we have a five billion step mobo
we have a five billion step mobo
run which is rather interesting right so
run which is rather interesting right so
I ran this for 5 billion steps it looks
I ran this for 5 billion steps it looks
like it takes 99 Towers 99 Problems 99
like it takes 99 Towers 99 Problems 99
towers and uh yeah it's pretty
towers and uh yeah it's pretty
good so we'll look at what this does
good so we'll look at what this does
tomorrow I'm working on like miniature
tomorrow I'm working on like miniature
DOTA as an RL environment at a million
DOTA as an RL environment at a million
FPS so if that sounds interesting drop
FPS so if that sounds interesting drop
by tomorrow I'll be live a lot earlier
by tomorrow I'll be live a lot earlier
I've been streaming all my Dev lately
I've been streaming all my Dev lately
for fun but otherwise I will catch you
for fun but otherwise I will catch you
later I got to go grab dinner see you
later I got to go grab dinner see you
around
