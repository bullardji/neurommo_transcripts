Kind: captions
Language: en
It should be live here.
It should be live here.
Hi everyone.
O fix the lighting in here. The uh haze.
O fix the lighting in here. The uh haze.
Not great.
Not great.
And with this desk, I don't have
And with this desk, I don't have
anywhere to stick this big ring light.
I can do this.
Let me see if I can get this to actually
slightly better.
slightly better.
slightly better. There we go. Hello,
slightly better. There we go. Hello,
Andrew.
So, here's the uh the plan for today.
So, here's the uh the plan for today.
I've got my boxes back.
Got a nice 5090 here. Um plan number
Got a nice 5090 here. Um plan number
one,
one,
we're going to do a quick couple
we're going to do a quick couple
environment modifications for Kathy
environment modifications for Kathy
Woo's lab. They want um contextual RL
Woo's lab. They want um contextual RL
problems. Basically, they want to like
problems. Basically, they want to like
take cart pole, be able to adjust the
take cart pole, be able to adjust the
speed of the cart, the length of the
speed of the cart, the length of the
pole, stuff like that. Same thing for
pole, stuff like that. Same thing for
breakout. Just going to do this for a
breakout. Just going to do this for a
couple M's for them. Uh that should be
couple M's for them. Uh that should be
pretty quick. And then we're going to do
pretty quick. And then we're going to do
some more work on the imitation learning
some more work on the imitation learning
algorithm. And I think what I'm going to
algorithm. And I think what I'm going to
do with this is I couldn't figure out
do with this is I couldn't figure out
why breakout was hard. So as a sanity,
why breakout was hard. So as a sanity,
I'm going to use our standard uh 30 to
I'm going to use our standard uh 30 to
train a breakout agent and I'm going to
train a breakout agent and I'm going to
use that as the expert to generate data
use that as the expert to generate data
and see if it trains with IIL via that.
and see if it trains with IIL via that.
And then we can kind of work backwards
And then we can kind of work backwards
from there and we can figure out why it
from there and we can figure out why it
is that uh we can't like bootstrap data
is that uh we can't like bootstrap data
from scratch to do the exact same thing.
from scratch to do the exact same thing.
See how much data it takes, we'll see
See how much data it takes, we'll see
how much training it takes, all that
how much training it takes, all that
type of stuff. And um that should get us
type of stuff. And um that should get us
into a good spot. We had some pretty
into a good spot. We had some pretty
promising results yesterday, but they
promising results yesterday, but they
were mixed. We had some really good
were mixed. We had some really good
results and some really like
results and some really like
surprisingly bad results.
surprisingly bad results.
We'll start on this though.
Okay. So, Carpole doesn't expose
Okay. So, Carpole doesn't expose
any of this crap.
Pull mass length
total mass
total mass
mass cart mass pole.
and then the magnitude of force.
Let me see if I can go find the messages
Let me see if I can go find the messages
seeing what specific variables they
seeing what specific variables they
wanted
and uh we will implement that for them.
and uh we will implement that for them.
We will make sure it trains.
Mass of part. Mass of pole. Length of
Mass of part. Mass of pole. Length of
pole.
pole.
Okay.
Okay.
They actually gave me a full list.
They actually gave me a full list.
Perfect.
But all I have to do
gravity.
first magnifier
and then
update interval.
This is good. Yeah,
a lot of variables,
a lot of variables,
but that's not particularly difficult.
but that's not particularly difficult.
Going to do like this
m speed and the continuous
m speed and the continuous
do
do
mass mass
All
right. What do we think about this? Just
right. What do we think about this? Just
do exact same thing. Yeah.
And that is all of them. Yeah.
possible context
possible context
want to avoid multi- aent environments
new breakout as well
do those two for them right now they can
do those two for them right now they can
play with this.
And both of these essentially are going
And both of these essentially are going
to get
to get
these are going to get nuked, right?
Astro Jess, hello. Don't want to let you
Astro Jess, hello. Don't want to let you
know you're an absolute inspiration. I
know you're an absolute inspiration. I
love efficiency, but can I please ask a
love efficiency, but can I please ask a
super dumb question, genuinely a sincere
super dumb question, genuinely a sincere
I feel like I'm missing something.
I feel like I'm missing something.
Absolutely. Kind of the point of this
Absolutely. Kind of the point of this
stream.
Can ask all sorts of stuff.
All we need to do here is uh use these
All we need to do here is uh use these
instead of the previous one
instead of the previous one
threshold.
threshold.
Why do we not just add max steps in as
Why do we not just add max steps in as
well? Right.
Guess we'll leave this. Well, they we'll
Guess we'll leave this. Well, they we'll
do the ones they asked for.
Puffer is optimizing for speed in terms
Puffer is optimizing for speed in terms
of steps per second.
of steps per second.
Um,
Um,
puffer lib is technically we're
puffer lib is technically we're
optimizing for speed in terms of uh wall
optimizing for speed in terms of uh wall
clock time to train an agent.
clock time to train an agent.
That is the when we run our hyper pram
That is the when we run our hyper pram
sweeps and stuff. That is the metric we
sweeps and stuff. That is the metric we
go for
which has very high correlation with
which has very high correlation with
environment steps per second of course
environment steps per second of course
and training steps per second.
and training steps per second.
There are some caveats on that, right?
There are some caveats on that, right?
Like if I really wanted to, I can make
Like if I really wanted to, I can make
puffer hit 20 million steps per second,
puffer hit 20 million steps per second,
but it won't train any faster in terms
but it won't train any faster in terms
of wall clock. So there's no point.
Where's Bet when you need him?
He's
in the Discord.
in the Discord.
What? Bet. Yeah, of course. Bet's in
What? Bet. Yeah, of course. Bet's in
Discord. Not in the voice or anything.
Discord. Not in the voice or anything.
Right.
Added him in there.
are reward functions for the classic M
are reward functions for the classic M
like cartpull solve problems or are they
like cartpull solve problems or are they
still improving?
still improving?
Um,
it kind of depends what you mean. Like
it kind of depends what you mean. Like
so technically
so technically
you could make the reward function like
you could make the reward function like
an oracle, right? Like if you did like
an oracle, right? Like if you did like
really domain specific engineering, you
really domain specific engineering, you
could make this oracle reward where
could make this oracle reward where
obviously you'll do better.
I don't really look at like is the
I don't really look at like is the
reward function a solved problem or not,
reward function a solved problem or not,
right?
right?
It's more so can you get it to learn
It's more so can you get it to learn
with like a baseline thing that doesn't
with like a baseline thing that doesn't
take a whole bunch of domain specific
take a whole bunch of domain specific
engineering versus like how much do you
engineering versus like how much do you
get out of doing more domain specific
get out of doing more domain specific
engineering. I don't look at like reward
engineering. I don't look at like reward
function as like a research question so
function as like a research question so
much in it of itself. There occasionally
much in it of itself. There occasionally
a couple best practices that come out of
a couple best practices that come out of
stuff but it's not it's really not the
stuff but it's not it's really not the
first thing I think about.
Like there's not really a point in
Like there's not really a point in
improving the reward function of
improving the reward function of
cartpole, right?
Like it's solved in 3 seconds or
Like it's solved in 3 seconds or
whatever right now. So
whatever right now. So
you could say like if you improve the
you could say like if you improve the
reward function and solve it in 2
reward function and solve it in 2
seconds
seconds
that really doesn't let you do anything
that really doesn't let you do anything
you couldn't do before and it doesn't
you couldn't do before and it doesn't
like tell you anything new about the uh
like tell you anything new about the uh
the algorithms. So the only time in my
the algorithms. So the only time in my
mind it makes sense to do heavy heavy
mind it makes sense to do heavy heavy
reward engineering is when you're
reward engineering is when you're
specifically trying to solve a hard
specifically trying to solve a hard
unsolved problem, right? That you
unsolved problem, right? That you
independently care about that problem
independently care about that problem
outside of the research value.
outside of the research value.
I commented out the prints too and also
I commented out the prints too and also
it broke. So yeah, got to figure that
it broke. So yeah, got to figure that
out.
Okay, so Bett's going to come look at
Okay, so Bett's going to come look at
this for us.
7 mil rewards per step.
What is the uh the post you just made up
What is the uh the post you just made up
FBR?
FBR?
Howdy. No rash. When you get a minute,
Howdy. No rash. When you get a minute,
there's a drone PR that's been sitting
there's a drone PR that's been sitting
around for a while. Um I can just merge
around for a while. Um I can just merge
it and trust that you haven't broken
it and trust that you haven't broken
some stuff if that is convenient to you.
some stuff if that is convenient to you.
Did I forget to do RK4?
And like does it still train?
I have a whole bunch to merge. It looks
I have a whole bunch to merge. It looks
like I also have a bunch of work to do
like I also have a bunch of work to do
on stuff today. I'll merge yours.
on stuff today. I'll merge yours.
Oh yeah, this is the drone RK4.
Okay. So, I think you PR into drone
Okay. So, I think you PR into drone
race.
I'm going to have to do some work, I
I'm going to have to do some work, I
think, to get this into drone swarm,
think, to get this into drone swarm,
right?
right?
If it's Oh, you did it in drone swarm as
If it's Oh, you did it in drone swarm as
well, actually.
well, actually.
And this still trains.
There was a big one that like took me a
There was a big one that like took me a
whole bunch of time. The last one I
whole bunch of time. The last one I
merged in I think was my error, not
merged in I think was my error, not
yours.
Okay,
I will merge the big PR and just hope
I will merge the big PR and just hope
that uh
that uh
hope that I don't have anything screwy.
hope that I don't have anything screwy.
I'm going to have to read through drone
I'm going to have to read through drone
stuff. Ideally once you get like either
stuff. Ideally once you get like either
you get something on the current drone
you get something on the current drone
or you send me like new drone and you
or you send me like new drone and you
can get something on that I will go
can get something on that I will go
spend a bunch of time helping on uh on
spend a bunch of time helping on uh on
that side like to get drones working
that side like to get drones working
kind of doing research in the move in
kind of doing research in the move in
the meantime. Thank you though that like
the meantime. Thank you though that like
our having RK4 is really useful cuz like
our having RK4 is really useful cuz like
we'll be able to like grab this and use
we'll be able to like grab this and use
it for other stuff as well.
Okay. My learning is mostly from Grock.
Okay. My learning is mostly from Grock.
Uh oh. Major bottleneck like board
Uh oh. Major bottleneck like board
passes.
passes.
Wait said something like the major
Wait said something like the major
bottleneck like the passes.
bottleneck like the passes.
That doesn't make sense on its own
That doesn't make sense on its own
because when I say millions of steps per
because when I say millions of steps per
second, that is end to end training
second, that is end to end training
time. Our environments are much faster
time. Our environments are much faster
than millions of steps per second,
than millions of steps per second,
right? It's millions of steps per second
right? It's millions of steps per second
end to end training time
end to end training time
on one GPU.
What? What did you do here, bat? Don't
What? What did you do here, bat? Don't
ever do that.
ever do that.
Okay,
that's crazy. Exactly.
that's crazy. Exactly.
That's puffer.
I mean, literally, if you just like if
I mean, literally, if you just like if
you just go play around with the
you just go play around with the
library, you'll see right like on a good
library, you'll see right like on a good
GPU, you train breakout in 25 seconds.
The task is just solved in 25 seconds.
There are other complaints that you can
There are other complaints that you can
make about the way that we've done
make about the way that we've done
certain things, but like ultimately
certain things, but like ultimately
they'd have to be pretty petty.
they'd have to be pretty petty.
It's kind of what you see is what you
It's kind of what you see is what you
get.
How much time have you spent on helping
How much time have you spent on helping
Yaxian with his super secret? Really not
Yaxian with his super secret? Really not
that much. I've had a couple
that much. I've had a couple
conversations with him. I mean, the
conversations with him. I mean, the
thing is like we literally have a drone
thing is like we literally have a drone
environment to pull from. So if you just
environment to pull from. So if you just
hack on that then like the RL side of
hack on that then like the RL side of
that is already very good
that is already very good
and then it kind of becomes a hardware
and then it kind of becomes a hardware
project right.
So basically like you can look at what
So basically like you can look at what
we have here is like puffer lilib just
we have here is like puffer lilib just
takes one portion of that project that
takes one portion of that project that
would be otherwise completely unfeasible
would be otherwise completely unfeasible
and it just like makes that side solved.
Uh, hang on. Did I mess up total mass?
training poly time touring machine to
training poly time touring machine to
solve. If that actually works, I'll be
solve. If that actually works, I'll be
very impressed. Actually, uh you should
very impressed. Actually, uh you should
look at Weston, the Weston in the
look at Weston, the Weston in the
Discord. It's a high school student who
Discord. It's a high school student who
made Sudoku and he says he has it
made Sudoku and he says he has it
working.
working.
uh with puffer.
I'll be very impressed if you get
I'll be very impressed if you get
freaking Sudoku to be solved with a
freaking Sudoku to be solved with a
touring machine. That'd be crazy.
Not the M to work. That's crazy.
Risk of sounding s shall and
Risk of sounding s shall and
capitalistic
capitalistic
create a lot of complex environments in
create a lot of complex environments in
puffer
puffer
have huge commercial upside. It's not
have huge commercial upside. It's not
shallow and like capitalistic. That's
shallow and like capitalistic. That's
like yeah there's business and we do all
like yeah there's business and we do all
the science stuff for free and we make
the science stuff for free and we make
all these environments for free but that
all these environments for free but that
is what we do on the business side right
is what we do on the business side right
is like companies pay us to either make
is like companies pay us to either make
them environments or make RL work on
them environments or make RL work on
their environments or make their
their environments or make their
environments fast that is the business
environments fast that is the business
model
model
because all of our tools are free we
because all of our tools are free we
don't charge for those right
don't charge for those right
so literally it's only the things that
so literally it's only the things that
uh really wouldn't have any value to
uh really wouldn't have any value to
general research and science that we
general research and science that we
monetize.
I do that at great personal cost.
Like if this exact same library were
Like if this exact same library were
closed source, very easily sell
closed source, very easily sell
expensive licenses.
expensive licenses.
So that's pretty much the only way we
So that's pretty much the only way we
monetize.
In fact, I can sort two arrays
In fact, I can sort two arrays
simultaneously.
simultaneously.
Huh, that's pretty awesome. I will be
Huh, that's pretty awesome. I will be
very interested to see that uh the
very interested to see that uh the
results of that when I eventually get
results of that when I eventually get
the touring machine.
the touring machine.
It'll be a cool thing.
I wouldn't have expected that to work
I wouldn't have expected that to work
honestly.
honestly.
probably took a lot of a lot of uh
probably took a lot of a lot of uh
effort to make that work.
That works. Good job. Like fair play to
That works. Good job. Like fair play to
you.
I don't really consider it charity so
I don't really consider it charity so
much.
much.
I mean, it's advancing science.
I mean, it's advancing science.
If you consider all science work
If you consider all science work
charity, then sure. It's just in my
charity, then sure. It's just in my
mind, it's like my own goal is to
mind, it's like my own goal is to
advance science and also make money. But
advance science and also make money. But
like I it doesn't make any sense for me
like I it doesn't make any sense for me
to go make money and do all the science
to go make money and do all the science
behind closed doors. It likewise doesn't
behind closed doors. It likewise doesn't
make sense for me to like just do all
make sense for me to like just do all
science and not make any money, right?
science and not make any money, right?
But they have to fit together in a way
But they have to fit together in a way
that's clean. Not in a way where you're
that's clean. Not in a way where you're
like, you know, making money at the
like, you know, making money at the
expense of science or spending all your
expense of science or spending all your
time doing science at the expense of not
time doing science at the expense of not
actually growing the project ever and
actually growing the project ever and
thereby not giving anybody who's
thereby not giving anybody who's
involved any sort of financial
involved any sort of financial
opportunities out of it.
I feel way way better this week as well.
I feel way way better this week as well.
Like mentally, I have so so much more
Like mentally, I have so so much more
clarity. I'll tell you what my routine
clarity. I'll tell you what my routine
has been uh yesterday and today.
has been uh yesterday and today.
So, I get up pretty early. I go I run a
So, I get up pretty early. I go I run a
10K around the dish.
10K around the dish.
It's like a good, you know, a solid
It's like a good, you know, a solid
solid hilly course. I come back. back. I
solid hilly course. I come back. back. I
lift some weights. I go walk down. I get
lift some weights. I go walk down. I get
myself some breakfast. Uh I go through
myself some breakfast. Uh I go through
messages and stuff and then I just work
messages and stuff and then I just work
on this. It's like
on this. It's like
there's so much freaking noise with
there's so much freaking noise with
everything else. Having that mental
everything else. Having that mental
clarity is so so important. Like I now
clarity is so so important. Like I now
this week I know exactly what to work
this week I know exactly what to work
on. I have like thought through exactly
on. I have like thought through exactly
how and like why everything needs to be
how and like why everything needs to be
done.
done.
kind of just in a good spot
kind of just in a good spot
for all.
I honestly don't even think I'm losing
I honestly don't even think I'm losing
any sort of productivity by spending
any sort of productivity by spending
that amount of time doing exercise in
that amount of time doing exercise in
the morning. like I feel so so so much
the morning. like I feel so so so much
better.
I don't waste time working on dumb
I don't waste time working on dumb
stuff. The result
did it just add that crap.
Welcome, Kvert. I see you have a whole
Welcome, Kvert. I see you have a whole
bunch of PRs. I will get to them.
bunch of PRs. I will get to them.
Going through my laundry list of things
Going through my laundry list of things
to do.
working on some of it. Anyways, about
working on some of it. Anyways, about
this new sparse learning. Oh, the
this new sparse learning. Oh, the
imitation thing. Um, I will be working
imitation thing. Um, I will be working
on that soon. Later today, as soon as I
on that soon. Later today, as soon as I
finish this, uh, I'm adding a couple
finish this, uh, I'm adding a couple
changes to Puffer for Kathy Woo's group.
Oh, imitation learning. Yes, that is
Oh, imitation learning. Yes, that is
what it is called. Uh, it's not You're
what it is called. Uh, it's not You're
not going to find good literature though
not going to find good literature though
because the thing I'm doing is weird.
the heck.
Oh,
watch your branch. The thing is I'm
watch your branch. The thing is I'm
doing imitation learning, but there's no
doing imitation learning, but there's no
expert data. Does that make sense?
expert data. Does that make sense?
I'm using imitation learning algorithms
I'm using imitation learning algorithms
with data that is not derived from
with data that is not derived from
expert
expert
There's some details for how that works.
There's some details for how that works.
Let me go message them.
Wait, why is FBR linking me?
Huh?
Sam and I have been messing around with
Sam and I have been messing around with
using inverse RL with no expert and got
using inverse RL with no expert and got
some results. Inverse RL with no expert.
some results. Inverse RL with no expert.
Why would you do inverse RL?
Why would you do inverse RL?
Like learning the reward function,
Like learning the reward function,
right?
right?
You have the reward function, don't you?
I would think that you have the reward
I would think that you have the reward
function.
Why is the Why does breakout have hyper
Why is the Why does breakout have hyper
Paul on it?
Bars to dense.
Okay, interesting. Well, if you guys
Okay, interesting. Well, if you guys
want to do research side stuff on
want to do research side stuff on
puffer, like there's definitely room for
puffer, like there's definitely room for
that as well.
that as well.
So, I would suggest the uh initial
So, I would suggest the uh initial
contract stuff will be likely to come
contract stuff will be likely to come
from drone. Um research side, I'm trying
from drone. Um research side, I'm trying
to think how the heck that makes any
to think how the heck that makes any
sense.
sense.
They're trying to label the data.
They're trying to label the data.
I'd have to see the details of that.
I'd have to see the details of that.
That's kind of wonky.
The imitation with no expert makes more
The imitation with no expert makes more
sense to me cuz you can just do like a
sense to me cuz you can just do like a
best event type thing or like a top K.
So, there's actually something to
So, there's actually something to
bootstrap. I don't know what you're
bootstrap. I don't know what you're
bootstrapping in uh the inverse RL
bootstrapping in uh the inverse RL
setting.
work with some tricks. H
work with some tricks. H
well, if you have cool stuff there,
well, if you have cool stuff there,
you know the research like agenda here
you know the research like agenda here
in Puffer, right? like we try stuff on
in Puffer, right? like we try stuff on
everything and if you have convincing
everything and if you have convincing
results
results
that would be uh
that would be uh
the type of thing that could make a a
the type of thing that could make a a
major puffer release. Yeah.
See what variables they gave me.
No gravity.
Is it possible for an RL newbie like me
Is it possible for an RL newbie like me
to add any value
to add any value
value anyway in a short time horizon? I
value anyway in a short time horizon? I
mean, this is how most of the people
mean, this is how most of the people
here got started, right? I've literally
here got started, right? I've literally
made like I've made a guide that is I
made like I've made a guide that is I
guarantee you the quickest way possible
guarantee you the quickest way possible
to get into reinforcement learning. It's
to get into reinforcement learning. It's
not trivial. If you have it depends on
not trivial. If you have it depends on
your programming and math background,
your programming and math background,
mostly programming background for how
mostly programming background for how
long it will take you. I have two
long it will take you. I have two
guides, right? I've got my advice for
guides, right? I've got my advice for
programming in ML. I've got my
programming in ML. I've got my
opinionated guide, really ultra
opinionated guide, really ultra
opinionated guide to reinforcement
opinionated guide to reinforcement
learning. If you read this and actually
learning. If you read this and actually
follow this and know what is going on
follow this and know what is going on
here, you will be a very capable
here, you will be a very capable
contributor to RL. It's just a matter of
contributor to RL. It's just a matter of
how difficult it is for you to do so,
how difficult it is for you to do so,
whether you have the time to do it.
weird.
Going to have to go through this in a
Going to have to go through this in a
little bit more detail. I think
little bit more detail. I think
it's hardcoded.
Let's
turn with DQ install things.
No, like vanilla DQN is a useless
No, like vanilla DQN is a useless
algorithm. It doesn't do anything. So
algorithm. It doesn't do anything. So
there are off policy methods um which
there are off policy methods um which
are like DQN with a whole boatload of
are like DQN with a whole boatload of
like things bolted onto it that seem to
like things bolted onto it that seem to
work pretty well, but like the evidence
work pretty well, but like the evidence
is very mixed and a lot of the research
is very mixed and a lot of the research
is not super high quality. And I just
is not super high quality. And I just
spent a week talking to a bunch of deep
spent a week talking to a bunch of deep
mind researchers who worked on these
mind researchers who worked on these
papers and I got like I got some good
papers and I got like I got some good
information on promising directions
information on promising directions
there and that is the thing that I'm
there and that is the thing that I'm
going to be working on later today after
going to be working on later today after
this. It's not quite the same flavor of
this. It's not quite the same flavor of
thing but it's like a similar
thing but it's like a similar
motivation.
I will say it's definitely there is no
I will say it's definitely there is no
there is not clear evidence that like oh
there is not clear evidence that like oh
yeah off policy is just better or
yeah off policy is just better or
anything like that. In fact, most of
anything like that. In fact, most of
it's kind of the opposite. And like off
it's kind of the opposite. And like off
policy has a lot of problems with
I really should go through a lot of
I really should go through a lot of
these like environments we use a lot and
these like environments we use a lot and
just clean up code. This is kind of
just clean up code. This is kind of
embarrassing to have
embarrassing to have
to have some of these be such a mess.
to have some of these be such a mess.
Just clean like pure refactor clean up
Just clean like pure refactor clean up
that
I mean, it kind of is what it is, right?
I mean, it kind of is what it is, right?
Like the M's are written by contributors
Like the M's are written by contributors
of very different skill levels
of very different skill levels
and like the M's are independently
and like the M's are independently
useful in research anyways, but the ones
useful in research anyways, but the ones
we use a lot. I'd like the code to be
we use a lot. I'd like the code to be
just very clean.
I honestly think being a good
I honestly think being a good
programmer, like a very good programmer,
programmer, like a very good programmer,
is harder than being a very good AI
is harder than being a very good AI
researcher.
I don't know why you would have used
I don't know why you would have used
breakout as the template when I have
breakout as the template when I have
like I've linked you other environments
like I've linked you other environments
that are cleaner. Right?
This is why I made the template
This is why I made the template
environment and like target and I I made
environment and like target and I I made
all the other ones like plus even snake
all the other ones like plus even snake
is decently clean.
is decently clean.
I have all these other environments
I have all these other environments
there so that you can see what like a
there so that you can see what like a
clean environment would look like.
clean environment would look like.
My first day, man, that's all right. But
My first day, man, that's all right. But
that is how you eventually get better,
that is how you eventually get better,
right? Like if you go look at I think
right? Like if you go look at I think
Spencer's first environment was Triple
Spencer's first environment was Triple
Triad. If you go look at the horri like
Triad. If you go look at the horri like
like the horrendous code that that is
like the horrendous code that that is
versus the awesome stuff he's building
versus the awesome stuff he's building
now, it's like night and day.
now, it's like night and day.
What the hell even is this? Right? Like
What the hell even is this? Right? Like
like literally what am I looking at with
like literally what am I looking at with
this?
this?
And it's like, oh, it's just formatting
And it's like, oh, it's just formatting
code. No, it's important. Like don't
code. No, it's important. Like don't
make your stuff a freaking mess. Like I
make your stuff a freaking mess. Like I
don't even need to be following a style
don't even need to be following a style
guide to like
see that obviously
see that obviously
this is better than whatever was there
this is better than whatever was there
before.
Heck, you can even do this.
Heck, you can even do this.
It's a little long, but it's fine.
Hello Desh welcome.
Oh, we already have ball whip.
watching this mostly in the background.
Yeah, no worries.
Yeah, no worries.
You're not required to literally spend
You're not required to literally spend
all day watching me work.
Yes. All puffer contributors must be
Yes. All puffer contributors must be
glued to all streams. All like 40 hours
glued to all streams. All like 40 hours
a week of them or however many I do.
It'll probably end up being about 40 a
It'll probably end up being about 40 a
week.
The next block is likely to be um
The next block is likely to be um
focused but not crazy long hours if that
focused but not crazy long hours if that
makes sense because it's mostly
makes sense because it's mostly
algorithm.
algorithm.
Miss sending you a message.
Miss sending you a message.
Oh.
Oh,
Oh,
like what?
like what?
I am sorry I can't watch you all day.
I am sorry I can't watch you all day.
You'd be surprised the number of people
You'd be surprised the number of people
uh the number of people that like said
uh the number of people that like said
that they knew me from the stream at the
that they knew me from the stream at the
conference.
I met a Kovac.
I met a Kovac.
I've seen in here a few times.
link in discord for breakout and I can
link in discord for breakout and I can
apply
apply
change tracer. Oh, this I'm not really.
change tracer. Oh, this I'm not really.
So what I'm doing here, this is like
So what I'm doing here, this is like
exposing variables because Kathy Woo's
exposing variables because Kathy Woo's
group wants to use this as like a
group wants to use this as like a
contextual RL problem. A full refactor
contextual RL problem. A full refactor
of this would take me a lot more time.
of this would take me a lot more time.
Like that could be a full day, like an
Like that could be a full day, like an
all day project, frankly. I'm not doing
all day project, frankly. I'm not doing
a full refactor now. I'm just exposing a
a full refactor now. I'm just exposing a
bunch of variables and making sure I
bunch of variables and making sure I
don't break anything. And then we're
don't break anything. And then we're
going to do imitation learning today.
Yeah, that's the idea.
Such a freaking mess, you know.
Did I do this? I don't think I would
Did I do this? I don't think I would
have. Maybe I was on like some jank
have. Maybe I was on like some jank
editor.
editor.
Better at least.
What? What did I do here?
What? What did I do here?
Expected.
Expected.
Oh, I don't know why it does this, but
Oh, I don't know why it does this, but
um my editor,
um my editor,
it likes to add a G character to the
it likes to add a G character to the
start of files for some reason. And like
start of files for some reason. And like
it replaces
it replaces
weird
off policy could help getting good
off policy could help getting good
initializ initialization.
Um,
Um,
you'll actually like what it is that
you'll actually like what it is that
we're going to do after this then
we're going to do after this then
because I think I have a just a better
because I think I have a just a better
form of that.
I think I have just a better form of
I think I have just a better form of
that.
that.
So the problem is, as far as I'm aware
So the problem is, as far as I'm aware
and as far as the people I've talked to
and as far as the people I've talked to
have been concerned, it seems is like
have been concerned, it seems is like
off policy is kind of a lie. Like the
off policy is kind of a lie. Like the
idea behind off policy is you can just
idea behind off policy is you can just
infinitely reuse your data.
infinitely reuse your data.
And it doesn't seem like that actually
And it doesn't seem like that actually
works.
Okay, so this runs exactly the same as
Okay, so this runs exactly the same as
before.
And that trains perfect.
And that trains perfect.
Super fast. Still 25 seconds
in today's stream. Yes, it will be right
in today's stream. Yes, it will be right
after this. I'm committing this use a
after this. I'm committing this use a
restroom and then we're going to start
restroom and then we're going to start
on the imitation stuff and it's it's
on the imitation stuff and it's it's
like a better it's a weird form of
like a better it's a weird form of
imitation learning that's kind of a
imitation learning that's kind of a
substitute for off off policy. You'll
substitute for off off policy. You'll
see and it doesn't use expert data
see and it doesn't use expert data
though we are going to get some expert
though we are going to get some expert
data as a baseline but it doesn't need
data as a baseline but it doesn't need
expert data.
throwing my poorly constructed thoughts
throwing my poorly constructed thoughts
in here
in here
from light controls. What
a uh message might have eaten something
a uh message might have eaten something
there and it doesn't make sense.
flight control certification back
flight control certification back
where inter Oh, okay. I see.
where inter Oh, okay. I see.
Um, you can't like
Um, you can't like
Yeah, you just can't
Yeah, you just can't
like like deep learning as a whole is
like like deep learning as a whole is
not interpretable at all. Um, it's like
not interpretable at all. Um, it's like
asking how can we get the like it's not
asking how can we get the like it's not
the same and it's a really dumb analog
the same and it's a really dumb analog
to say this, but it's the best one we
to say this, but it's the best one we
have. It's kind of like asking to make a
have. It's kind of like asking to make a
person fully interpretable,
which it's just not like that's just not
which it's just not like that's just not
a thing.
No.
Okay, so this is the imitation thing
Okay, so this is the imitation thing
that I have, right? Let me show this off
that I have, right? Let me show this off
and then I'm going to take two minutes
and then I'm going to take two minutes
and then we'll work on it.
and then we'll work on it.
But we'll kind of show you guys the uh
But we'll kind of show you guys the uh
initial thing first. So
initial thing first. So
here's the train function. You can see
here's the train function. You can see
that all of the logic here that's like
that all of the logic here that's like
the normal RL is commented out. Okay.
the normal RL is commented out. Okay.
And then you have this super short
And then you have this super short
imitation learning loop. Okay. Not that
imitation learning loop. Okay. Not that
short, but it's literally just imitation
short, but it's literally just imitation
loss with like entropy coefficient.
loss with like entropy coefficient.
Okay.
Okay.
So there's not actually any RL happening
So there's not actually any RL happening
in here.
Hang on.
Got to rebuild the ends and then we'll
Got to rebuild the ends and then we'll
show you.
There you go. So
There you go. So
solves cart pole just like before,
solves cart pole just like before,
right?
start solving pong. If we train that for
start solving pong. If we train that for
like an additional second or two, that
like an additional second or two, that
would full solve pong.
would full solve pong.
All right.
If we train breakout, it doesn't
If we train breakout, it doesn't
it doesn't remotely solve it, but it
it doesn't remotely solve it, but it
does substantially better than random.
And also
And also
the thing that's kind of crazy about
the thing that's kind of crazy about
this
this
is that it actually does really really
is that it actually does really really
well at the first part of neural MMO 3.
well at the first part of neural MMO 3.
Like way better than RL does.
Like way better than RL does.
You're talking about hard initial early
You're talking about hard initial early
exploration. The question here is like
exploration. The question here is like
why does it work on some problems even
why does it work on some problems even
some of which are pretty hard and not on
some of which are pretty hard and not on
others? So, what we're going to do
others? So, what we're going to do
is we're going to train a breakout agent
is we're going to train a breakout agent
with like puffer 30 RL and we're going
with like puffer 30 RL and we're going
to see how much data you need from an
to see how much data you need from an
expert to IIL and we're going to use
expert to IIL and we're going to use
this to assess what's going wrong with
this to assess what's going wrong with
this
this
sent pull request. I will look at that.
sent pull request. I will look at that.
I want to do imitation learning stuff
I want to do imitation learning stuff
right now while I'm in the mindset for
right now while I'm in the mindset for
it. But I am very excited to see how big
it. But I am very excited to see how big
is this. I will look at how big this is.
is this. I will look at how big this is.
Okay, that's about what you'd expect.
Okay, that's about what you'd expect.
It's kind of going to be chunky, right?
Oh jeez. Why did you have to do all this
Oh jeez. Why did you have to do all this
pie object crap?
pie object crap?
Wait, we literally have a binding thing
Wait, we literally have a binding thing
so that you don't have to mess with
so that you don't have to mess with
this, don't we?
Holy.
Holy.
All right. And then thousandish lines.
All right. And then thousandish lines.
Wait, why is it all in the C?
Wait, how is this all in the C, man? Cuz
Wait, how is this all in the C, man? Cuz
the C doesn't get imported, right?
This for testing. Okay. You were
This for testing. Okay. You were
training it though, so somehow you were
training it though, so somehow you were
training it. Wouldn't it have to be in
training it. Wouldn't it have to be in
theh for that? Because theh is what's
theh for that? Because theh is what's
gets included by um by puffer.
How much of this is GPT?
I get suspicious when I see that number
I get suspicious when I see that number
of comments.
20% claw. Okay, that is an acceptable
20% claw. Okay, that is an acceptable
percentage.
I have to ask because when I get mad
I have to ask because when I get mad
when people ask me to like review things
when people ask me to like review things
that are like 80% just AI generated,
that are like 80% just AI generated,
right?
Funny. I just I never trust um I never
Funny. I just I never trust um I never
trust these models to touch my code.
trust these models to touch my code.
They just cause so many problems.
I mean this is about what I would expect
I mean this is about what I would expect
though. This is like just straight up
though. This is like just straight up
yeah you implement this the
yeah you implement this the
singlepurpose physics in about the
singlepurpose physics in about the
amount of code that I expected it would
amount of code that I expected it would
be. If you take the comments out,
be. If you take the comments out,
actually it's probably like the thousand
actually it's probably like the thousand
lines is what I said it would be about a
lines is what I said it would be about a
thousand lines.
Okay. So,
Okay. So,
get this thing. I I don't understand how
get this thing. I I don't understand how
that this is even training if all the
that this is even training if all the
codes in the C. Um because you did send
codes in the C. Um because you did send
me training demos. like get whatever the
me training demos. like get whatever the
like the final cleaned up version is.
like the final cleaned up version is.
Get some cool experiments in. uh comment
Get some cool experiments in. uh comment
on the PR like comment a link to either
on the PR like comment a link to either
W to be or Neptune or one of those so I
W to be or Neptune or one of those so I
can review some of the train curves and
can review some of the train curves and
like add a couple images or a couple
like add a couple images or a couple
gifts or videos or whatever and then uh
gifts or videos or whatever and then uh
we will set up a time this week and I
we will set up a time this week and I
will give you a full review of this
will give you a full review of this
because uh well you can decide when
because uh well you can decide when
basically you can decide how much more
basically you can decide how much more
time you want to spend on getting better
time you want to spend on getting better
results or whatnot first because this is
results or whatnot first because this is
the type of thing where like if you can
the type of thing where like if you can
actually get a sixderee of freedom arm
actually get a sixderee of freedom arm
to work really well in a way that is
to work really well in a way that is
very robust. This is like direct link
very robust. This is like direct link
into contracts with us
into contracts with us
but it depends fully on uh how good the
but it depends fully on uh how good the
actual implementation is and on
actual implementation is and on
follow-up stuff
references for the math as well. Well,
references for the math as well. Well,
where did you get the math from if you
where did you get the math from if you
don't have references?
Like where did you get the Huh?
Oh, I'm sorry. That was
Oh, I'm sorry. That was
I read those is the same name. I just
I read those is the same name. I just
looked over. Okay, that's Finn asking
looked over. Okay, that's Finn asking
short name that starts with F. All
short name that starts with F. All
right, cool. Um, well, you know what?
right, cool. Um, well, you know what?
Then you know the the basics. I mean,
Then you know the the basics. I mean,
you know the procedure at this point.
you know the procedure at this point.
I'm going to use the restroom pretty
I'm going to use the restroom pretty
quick and then I'll be back in a couple
quick and then I'll be back in a couple
minutes. Grab myself a drink and uh we
minutes. Grab myself a drink and uh we
will start on the imitation learning
will start on the imitation learning
segment for the day. Be right back.
All
right.
First things first here.
We got to train ourselves a baseline.
Might just have to play with the manny
Might just have to play with the manny
scale pole.
scale pole.
You can ask me stuff if you need cuz
You can ask me stuff if you need cuz
like there was a line that you have to
like there was a line that you have to
modify. I can probably help you find it.
So this is fine. And now what we do is
have to not got my here.
die token.
I can't figure it out. I probably won't
I can't figure it out. I probably won't
be. Yeah. The thing that's difficult
be. Yeah. The thing that's difficult
with Manny skill, so I played with it
with Manny skill, so I played with it
for a couple days and I still want to do
for a couple days and I still want to do
stuff cuz stone's awesome, but like I
stuff cuz stone's awesome, but like I
couldn't find decent settings to run it
couldn't find decent settings to run it
fast enough to make it like to really be
fast enough to make it like to really be
able to do much, right? When you're
able to do much, right? When you're
training 50k steps per second, it's just
training 50k steps per second, it's just
not a good spot to be in with RL, which
not a good spot to be in with RL, which
is why like doing our own armor or
is why like doing our own armor or
something like that would probably be,
something like that would probably be,
as hard as that is, probably more
as hard as that is, probably more
reasonable just cuz you need to be able
reasonable just cuz you need to be able
to train at a decent speed to do
to train at a decent speed to do
anything base.
like tens and tens of thousands of lines
like tens and tens of thousands of lines
of code across like several different
of code across like several different
frameworks to make anything worthwhile.
Okay. Good.
Okay. Good.
Yes.
Now,
how are we going to use the expert
cuz like we need to still know how well
We need to still know how well um our
We need to still know how well um our
agent doing.
agent doing.
Congrats on RLC. Hey Aar, thank you.
Congrats on RLC. Hey Aar, thank you.
We're doing all sorts of crazy stuff on
We're doing all sorts of crazy stuff on
Algo side now.
Algo side now.
Yeah, I put the talk up on X and YouTube
Yeah, I put the talk up on X and YouTube
and I've got to archive the paper, but
and I've got to archive the paper, but
I'll have the uh paper on archive soon.
Working on experimental research for
Working on experimental research for
potentially puffer 4
potentially puffer 4
the moment.
Why aren't you at Why weren't you at
Why aren't you at Why weren't you at
RLC, man? It's like the RL conference.
RLC, man? It's like the RL conference.
You off doing a life stuff now?
I think if I just totally hack the um
I just totally hack
You'd have to literally have a second
You'd have to literally have a second
set of environments though. No.
Not really RL.
Now if you consider like the mainstream
Now if you consider like the mainstream
thing done 10,000x faster at times
thing done 10,000x faster at times
thousandx at others.
thousandx at others.
Suppose this is mainstream.
Suppose this is mainstream.
Not really at all though.
What would I do with this?
Want to hack this to have
Want to hack this to have
second set of environments. Is there
second set of environments. Is there
nothing easier I can do?
Right.
Oh, the wrong branch.
Good thing I caught that before I messed
Good thing I caught that before I messed
it up too much.
So we just take all this data All right.
Yes.
a contract with you guys
a contract with you guys
that about. So the way that puffer works
that about. So the way that puffer works
right is uh everything on the business
right is uh everything on the business
side is contract based. So it is uh can
side is contract based. So it is uh can
you get companies
you get companies
interested in working with us based off
interested in working with us based off
of simulations that you've built or like
of simulations that you've built or like
other domain experience that you have in
other domain experience that you have in
addition to uh the RL stuff with puffer.
addition to uh the RL stuff with puffer.
So, the reason that six degree of
So, the reason that six degree of
freedom arms is likely to be um
freedom arms is likely to be um
contractw worthy is that there are a ton
contractw worthy is that there are a ton
of robotics companies and all of the
of robotics companies and all of the
current sims for six degree freedom uh
current sims for six degree freedom uh
sixderee freedom arms are really slow
sixderee freedom arms are really slow
because it's the same sim as is used for
because it's the same sim as is used for
higher fidelity uh robotics like
higher fidelity uh robotics like
humanoid hands, full humanoids, things
humanoid hands, full humanoids, things
like that.
like that.
So having a good and very fast sim doing
So having a good and very fast sim doing
impressive tasks and then also doing
impressive tasks and then also doing
some work on like talking to robotics
some work on like talking to robotics
companies, right? And like getting them
companies, right? And like getting them
interested in doing stuff with Puffer.
interested in doing stuff with Puffer.
If you're able to do that and we're able
If you're able to do that and we're able
to swing a contract off of it, you will
to swing a contract off of it, you will
be on the contract.
That is essentially how you get into um
That is essentially how you get into um
business side stuff of puffer. It is by
business side stuff of puffer. It is by
being good enough to build out
being good enough to build out
applications specific domains around
applications specific domains around
Puffer and then using your work plus uh
Puffer and then using your work plus uh
plus the general recognition of Puffer
plus the general recognition of Puffer
in order for us to partner on a contract
in order for us to partner on a contract
for a company. How that works.
It has to be good though. Like it can't
It has to be good though. Like it can't
be the type of thing where you kind of
be the type of thing where you kind of
build a sim and
build a sim and
you know we have to go get a contract
you know we have to go get a contract
and you're not going to be capable of
and you're not going to be capable of
working on it, right? Like it's really
working on it, right? Like it's really
much more independently driven. You
much more independently driven. You
know, occasionally I do go to people who
know, occasionally I do go to people who
are who I know are good in specific
are who I know are good in specific
areas for contracts that I have found,
areas for contracts that I have found,
but the way more consistent way of being
but the way more consistent way of being
able to do this is to be able to
able to do this is to be able to
actually go build a thing, find a
actually go build a thing, find a
company that's interested, getting them
company that's interested, getting them
talking to us.
talking to us.
That's the easiest way to do it.
Okay,
Okay,
that's something.
What this should the What should this be
What this should the What should this be
doing now?
doing now?
Expert vac receive.
Expert vac receive.
Uh, this should be populating
Uh, this should be populating
buffers with expert data.
And we should be getting
stats.
Yeah, we should be getting stats from
Yeah, we should be getting stats from
something totally different.
That's right.
That's right.
Instead,
we have KL blowing up.
Anning out instant. Cool.
Anning out instant. Cool.
Very cool. Um,
Very cool. Um,
okay.
okay.
Let's just move this Here
to here.
Okay. So, this is the expert data.
Okay. So, this is the expert data.
See, it actually does get the
See, it actually does get the
or it's supposed to be
Uh the heck.
What?
Why are we on 300 branch?
Why are we on 300 branch?
I literally checked this so that it
I literally checked this so that it
would be on the correct
would be on the correct
Damn it.
Let's see if this is close enough that
Let's see if this is close enough that
it's actually
it's actually
done something for us.
This we got this
Rebuild this. And now we're going to try
Rebuild this. And now we're going to try
to train this again.
to train this again.
Should be imitation
atically different, right?
If we're doing this correctly,
If we're doing this correctly,
yes, this being done correctly
is what we're logging here.
Is this never going to eval?
Why is this not going to eval
now?
Yeah. Why is this not um evaluate
this bot?
weird
weird
using to do the final evaluation.
Why this is not evaling? We need this to
Why this is not evaling? We need this to
eval so we can actually get
eval so we can actually get
final data.
It's crazy, by the way, just how stupid
It's crazy, by the way, just how stupid
easy this is. Like, oh, we can just do
easy this is. Like, oh, we can just do
imitation learning if we just have data.
imitation learning if we just have data.
Yeah, if you actually have data,
Yeah, if you actually have data,
not even a problem to solve.
Well, I know what it is.
But it is fine. It's just I forgoting
But it is fine. It's just I forgoting
forgetting to close the end.
96 closes. Perfect.
96 closes. Perfect.
Okay,
Okay,
be happy with that.
be happy with that.
Except
my
How is
Now it's just training on all the data.
And this does a little better.
And this does a little better.
Interestingly, does not solve the task.
Interestingly, does not solve the task.
Not solve the task.
Not solve the task.
Oh, wait. Hang on. Is this might be
Okay, that actually makes a difference.
Increase the update epochs.
Yeah, like increasing update epoch seems
Yeah, like increasing update epoch seems
to actually do to make a big difference
to actually do to make a big difference
here, right?
here, right?
Lo and behold, supervised learning
Lo and behold, supervised learning
scales.
This little dip here. Is it getting over
This little dip here. Is it getting over
the first screen of bricks? That's an
the first screen of bricks? That's an
expected dip.
Only 500
Only 500
or epox.
ass
about the same
about the same
better with two
better with two
that Just noise
that Just noise
is m magically better with two.
is m magically better with two.
That's interesting though. It doesn't
That's interesting though. It doesn't
just if it does not learn from this as
just if it does not learn from this as
well as from the RL. Kind of odd, isn't
well as from the RL. Kind of odd, isn't
it?
Okay, now two update epox is better. We
Okay, now two update epox is better. We
get 600 score.
And now I know it would solve it if we
And now I know it would solve it if we
just kept we just kept going, right?
Let's ignore that. Um, it should be able
Let's ignore that. Um, it should be able
to learn from six four length segments,
to learn from six four length segments,
right?
Isn't that kind of crazy?
RL being more efficient than
RL being more efficient than
itation
be the other way around though.
be the other way around though.
Absolutely should be.
Maybe there's like
there's just gank
now that like these make very slight
now that like these make very slight
differences at most.
I do somewhat worry. The thing is the
I do somewhat worry. The thing is the
episode lengths are super long and
episode lengths are super long and
breakout, right?
I do somewhat worry about that. It's
I do somewhat worry about that. It's
just that
Yeah, there we go.
Yeah, there we go.
So, it's just that the number of
So, it's just that the number of
environments is super large.
environments is super large.
Um, you're not even really seeing data
Um, you're not even really seeing data
from the whole game, I don't think.
from the whole game, I don't think.
We'll do the back of the envelope math
We'll do the back of the envelope math
in a second.
in a second.
Soon
Soon
as we know it actually solves
actually appear to be a little stuck,
actually appear to be a little stuck,
doesn't it?
Okay. 725. So, a good score, but not
Okay. 725. So, a good score, but not
actually solved.
actually solved.
14.
This is Yeah, there you go. So pretty
This is Yeah, there you go. So pretty
much
much
uh if you start just like running a
uh if you start just like running a
ideal policy
ideal policy
with this many environments, it won't
with this many environments, it won't
even finish a full game
even finish a full game
by this number of steps.
by this number of steps.
That's the main problem.
kind of happened. Huh?
I just like
725. Let me just make sure.
725. Let me just make sure.
Not like super sensitive.
Not like super sensitive.
And then after this,
And then after this,
I'm trying to think what this tells us.
I mean, this does tell us, right? like
I mean, this does tell us, right? like
and let's like ignore a second the fact
and let's like ignore a second the fact
that we're kind of doing this in a way
that we're kind of doing this in a way
that's a little janky.
that's a little janky.
Uh if you have
Uh if you have
that just nans out. That's funny.
So if you have a good uh aisle and let's
So if you have a good uh aisle and let's
actually just do one other thing.
make sure that
make sure that
actually going to solve.
actually going to solve.
Um,
if you actually have like a good policy
if you actually have like a good policy
can pretty easily I
can pretty easily I
another one based on it, right?
purely just by matching actions over
purely just by matching actions over
segments.
I mean, this is kind of obvious, right?
I mean, this is kind of obvious, right?
But now the question is,
But now the question is,
what's so special about the RL, right?
what's so special about the RL, right?
Like why can't
Like why can't
why can't we just I a policy from
why can't we just I a policy from
scratch just by like keeping around the
scratch just by like keeping around the
best data
best data
the best trajectory segments.
Okay. Interesting. So
uh this actually doesn't fully solve it.
Likewise, we get like this highish score
Likewise, we get like this highish score
not fully solved.
That's a little odd,
That's a little odd,
but it does do way way better than our
but it does do way way better than our
IIL from scratch, obviously.
So, what is the difference here? Right.
Well, hang on. We have a few things,
Well, hang on. We have a few things,
don't we?
So, if I just take our gold samples,
if we do it like this,
if we do it like this,
how well does this do?
Okay. So, even this does pretty
Okay. So, even this does pretty
decently. So, what this is doing here
decently. So, what this is doing here
is just keeping the best data from uh
is just keeping the best data from uh
it's keeping like the best batch of data
it's keeping like the best batch of data
basically just like grabbing all the
basically just like grabbing all the
best segments based on reward.
Now we know that this approach also
Now we know that this approach also
can do decently well.
So then what is it that is preventing us
So then what is it that is preventing us
from doing this without the expert
from doing this without the expert
policy?
Because if we don't make any change to
Because if we don't make any change to
this,
if we make no change to this except
if we make no change to this except
swapping out
expert for the policy, now it has to
expert for the policy, now it has to
generate its own data to learn on.
Not like it doesn't learn anything,
Not like it doesn't learn anything,
right? It learns something.
right? It learns something.
Very inefficient.
Very inefficient.
Let's go mess around with the the
Let's go mess around with the the
original formula bit.
I think that this buffer
I think that this buffer
we're going to make this buffer way
we're going to make this buffer way
bigger.
for like way bigger.
We have a big buffer.
Way harder to overfit now, right?
Good transitions.
caps out though. H
starts getting worse.
starts getting worse.
actually starts getting worse.
It makes sense for me to try to get like
It makes sense for me to try to get like
a batch IIL thing going.
not like showing um
wrong dependence on
we would think that it would be more
we would think that it would be more
sample efficient even in like it would
sample efficient even in like it would
be better by all metrics. Even in the
be better by all metrics. Even in the
easiest or like the simplest case like
easiest or like the simplest case like
IIL should just be easier than RL.
We have basically a perfect data
We have basically a perfect data
generator.
like so disconcerting to me in some
like so disconcerting to me in some
sense.
sense.
Back to a few different
Let's add in
this way.
Exact opposite. Wait, I can't follow
Exact opposite. Wait, I can't follow
along perfectly due to my limited
along perfectly due to my limited
understanding.
understanding.
What does off policy method have to do
What does off policy method have to do
having more data efficient
having more data efficient
computation efficiency
computation efficiency
opposite of on policy have anything to
opposite of on policy have anything to
do with this?
do with this?
Aren't going through. Yeah, YouTube
Aren't going through. Yeah, YouTube
sometimes will filter things if it
sometimes will filter things if it
thinks you're sending links, whatever.
thinks you're sending links, whatever.
It's kind of weird. Um,
It's kind of weird. Um,
the thing is I don't actually know if
the thing is I don't actually know if
off policy methods are substantially
off policy methods are substantially
more sample efficient
more sample efficient
like based on some recent stuff I've
like based on some recent stuff I've
seen.
seen.
And this is not either. This is
And this is not either. This is
imitation learning.
This should definitely do something. And
This should definitely do something. And
like it's working, but it's actually
like it's working, but it's actually
it's funny. It's not closing out the
it's funny. It's not closing out the
environment.
environment.
not learning the final
not learning the final
final bit that it needs.
Wonder if I can just do
be able to just fill it either way.
be able to just fill it either way.
So odd
Just do one thing that's like super
Just do one thing that's like super
basic.
Yeah.
Kind of tricky.
Okay. What if we do
Okay. What if we do
This is a boss.
need restroom real quick. Puffer li
need restroom real quick. Puffer li
rendering to wand.
rendering to wand.
Um,
Um,
so technically, yes, Kevin, it's just
so technically, yes, Kevin, it's just
the massive amount of data that you end
the massive amount of data that you end
up accumulating as soon as you start
up accumulating as soon as you start
doing that by default.
doing that by default.
But I'd probably want to do it as like
But I'd probably want to do it as like
an off by default thing. And the other
an off by default thing. And the other
annoying thing is like we actually have
annoying thing is like we actually have
to figure out how to do that with
to figure out how to do that with
uh like you need headless rendering for
uh like you need headless rendering for
ocean ends to do that, which is doable.
ocean ends to do that, which is doable.
I think a couple people have gotten it
I think a couple people have gotten it
to work, but I haven't integrated it
to work, but I haven't integrated it
yet. I'll be right back once.
the heck happened to episode return at
the heck happened to episode return at
nann.
nann.
All right, maybe the maybe this is just
All right, maybe the maybe this is just
like weird hypers being weird, you know?
like weird hypers being weird, you know?
Maybe this is just weird hypers.
Maybe this is just weird hypers.
They should not be nanning
Yeah, just hovers around these 700s.
Well, there's some 800s in there.
It's actually closer
It's actually closer
and it nanss out. Okay, Nan's out at the
PO always better than stack when data
PO always better than stack when data
efficiency isn't an issue. Yeah, I don't
efficiency isn't an issue. Yeah, I don't
know. And nobody does comparisons
know. And nobody does comparisons
correctly on these things. Um, and I
correctly on these things. Um, and I
honestly I don't even know if SAC is
honestly I don't even know if SAC is
better when data efficiency is an issue
better when data efficiency is an issue
because
because
I talked to a bunch of DeepMind people
I talked to a bunch of DeepMind people
about like all the off policy stuff that
about like all the off policy stuff that
they do and such and it's actually
they do and such and it's actually
become kind of unclear to me
like off policy RL in general is kind of
like off policy RL in general is kind of
a mess.
right? Why the heck does this not just
right? Why the heck does this not just
solve? Right? This is basic ass
solve? Right? This is basic ass
imitation learning with an expert.
Like why is this being so weird?
Okay, it's pretty close to just solving,
Okay, it's pretty close to just solving,
right?
Policy.
Policy.
Um
because like nobody's ever really done
because like nobody's ever really done
controlled experiments at all, Kevin.
controlled experiments at all, Kevin.
Like it's everybody's kind of been
Like it's everybody's kind of been
pushing sample efficiency sample
pushing sample efficiency sample
efficiency on these offpaul methods and
efficiency on these offpaul methods and
like they have crazy settings for these
like they have crazy settings for these
things. Like the sheer amount of compute
things. Like the sheer amount of compute
you're using with these methods like
you're using with these methods like
nobody does this with PO, right? They've
nobody does this with PO, right? They've
gotten themselves in this like insane
gotten themselves in this like insane
data regime where like they're spending
data regime where like they're spending
a huge amount of compute on tiny models
a huge amount of compute on tiny models
with tiny amounts of data and like
with tiny amounts of data and like
nobody's ever really tried that in the
nobody's ever really tried that in the
on pulse settings. It's basically just
on pulse settings. It's basically just
two diverging branches of research where
two diverging branches of research where
like nobody's ever bothered to compare
like nobody's ever bothered to compare
correctly and like the data hygiene
correctly and like the data hygiene
practices in both settings are awful.
This is like close or though at least
This is like close or though at least
like
like
800 almost
like fairly confident we could make
like fairly confident we could make
something,
something,
right?
I mean regardless though
I mean regardless though
just like at least does something
an Astro I shouldn't need a bigger model
an Astro I shouldn't need a bigger model
because the expert is not a bigger
because the expert is not a bigger
model. I should not have to distill into
model. I should not have to distill into
a bigger model.
anything I should be able to distill
anything I should be able to distill
into a smaller model.
Trying to think how to interpret this
What if it's just a spruy reward
What if it's just a spruy reward
sparity? No.
There's no value function.
It's a very sparse reward, right?
Heal versus sack is like the most basic
Heal versus sack is like the most basic
thing.
thing.
Try to decide if you
Try to decide if you
value base. You typically don't
value base. You typically don't
you typically just use your favorite
like bas like none of the basic stuff in
like bas like none of the basic stuff in
RL has actually been done correctly
RL has actually been done correctly
Kevin
like at all.
Okay. What did I break? Because now it's
Okay. What did I break? Because now it's
like way worse. Oh, now I don't have the
like way worse. Oh, now I don't have the
gold buck
which it it works this way as well.
But the simple proof of this, right?
But the simple proof of this, right?
Okay, I'm able to train at millions of
Okay, I'm able to train at millions of
steps per second, right? That means if I
steps per second, right? That means if I
want to run a 100 million step
want to run a 100 million step
experiment, it's done in under a minute.
experiment, it's done in under a minute.
Well under a minute, right?
Well under a minute, right?
All right. So if I have a thousand step
All right. So if I have a thousand step
per second RL library and a lot of them
per second RL library and a lot of them
are even slower than that. Okay. And I
are even slower than that. Okay. And I
want to do 100 million steps.
want to do 100 million steps.
Okay.
Okay.
Uh and then let's do in hours. That's 27
Uh and then let's do in hours. That's 27
hours.
hours.
So it takes longer than a day to run a
So it takes longer than a day to run a
single experiment on a GPU or like a
single experiment on a GPU or like a
super basic environment.
super basic environment.
Your dev loop is sucks. You're not going
Your dev loop is sucks. You're not going
to get anything done. You're not running
to get anything done. You're not running
enough experiments. And you're not doing
enough experiments. And you're not doing
basic hygiene like running many seeds
basic hygiene like running many seeds
and hyperparameter sweeps except maybe
and hyperparameter sweeps except maybe
like a few seeds for the final
like a few seeds for the final
experiments.
experiments.
All right. And then they actually with
All right. And then they actually with
Ataru then they run all 57 of them. So
Ataru then they run all 57 of them. So
they're not doing any of that stuff.
they're not doing any of that stuff.
Like it's literally just impossible on
Like it's literally just impossible on
the speeds and on the academic budgets
the speeds and on the academic budgets
that people have access to for the
that people have access to for the
science to be any good.
science to be any good.
That's it.
Yeah. So, we could astro if I were to do
Yeah. So, we could astro if I were to do
high perf SAC, but like that's not a
high perf SAC, but like that's not a
trivial thing to do. Like it would take
trivial thing to do. Like it would take
me a while to do that.
Not like
Not like
I don't think that there's any data I've
I don't think that there's any data I've
seen that's like so massively in favor
seen that's like so massively in favor
of SACE that I expect it to do anything
of SACE that I expect it to do anything
for
Yeah. So, this is the
I break it again.
Oh, yeah. Hang on.
Oh, yeah. Hang on.
Well, we want to let's let it actually
Well, we want to let's let it actually
run like this.
run like this.
Um,
Um,
then we'll go back to the number of Ms.
then we'll go back to the number of Ms.
That makes sense.
We should try this on some more
We should try this on some more
environments.
environments.
See if we can like predict
See if we can like predict
what this is going to be good on.
Just sparity.
Get up.
What ms do I have in ocean I could is
triple try didn't work Right.
How does this get zero rewards?
How does this get zero rewards?
Doesn't make any sense, does it?
Oh, negative. Is that the case?
Negative reward.
do anything?
I think this change is in here.
Engine here.
Press
Press
that
Maybe there is just something weird with
Maybe there is just something weird with
like the reward sparity going on, right?
This should be an environment that's
This should be an environment that's
like not a super easy environment.
Doing definitely non-trivially well.
Doing definitely non-trivially well.
You still get the same effect where like
You still get the same effect where like
on the harder ends it fails to close out
on the harder ends it fails to close out
like it gets stuck.
need a bigger data buffer maybe.
need a bigger data buffer maybe.
I think it's just that, right? It's not
I think it's just that, right? It's not
just like, oh, give it a bigger data
just like, oh, give it a bigger data
buffer.
The fact that that like so quickly.
The fact that that like so quickly.
Oh, that is a little better, huh?
Oh, that is a little better, huh?
Maybe this is the NF to use for testing.
It has like very short episodes
and a very well- definfined score.
then it gets stuck.
then it gets stuck.
five,
which is very close to what we have in
which is very close to what we have in
the buffer. I believe it's just a
the buffer. I believe it's just a
positive one.
Okay.
I do this
mentioned at the beginning basically
mentioned at the beginning basically
solves it from an RL side and it be a
solves it from an RL side and it be a
hardware problem. What does RL
hardware problem. What does RL
accomplish and what is there to do
accomplish and what is there to do
afterward in terms of hardware? Well, I
afterward in terms of hardware? Well, I
mean you can fly the drones around in
mean you can fly the drones around in
simulation. you can randomize them in
simulation. you can randomize them in
simulation. Uh the actual learned
simulation. Uh the actual learned
control works very well. Obviously, you
control works very well. Obviously, you
then have to actually put it on a drone
then have to actually put it on a drone
and if it's a custom drone, then you
and if it's a custom drone, then you
have to build said custom drone and have
have to build said custom drone and have
the sensors be sane and make sure that
the sensors be sane and make sure that
whatever the heck that drone is is at
whatever the heck that drone is is at
least somewhat matched by something in
least somewhat matched by something in
the randomized training scenario.
Without puffer lib, you have to both do
Without puffer lib, you have to both do
that and figure out how like the crazy
that and figure out how like the crazy
cursed RL that doesn't make sense can be
cursed RL that doesn't make sense can be
made to work.
It is interesting to me
It is interesting to me
the gold reward is getting higher the
the gold reward is getting higher the
more I make the bigger I make this
more I make the bigger I make this
buffer
Now, it could just be that this is
Now, it could just be that this is
getting filled up with easy levels,
getting filled up with easy levels,
right?
right?
That would be a problem with doing it
That would be a problem with doing it
this way.
this way.
Filling up with easy levels.
Filling up with easy levels.
Yeah,
cuz now unlike before, right, we have
cuz now unlike before, right, we have
a pretty solid buffer.
The PF is maybe a little higher than
The PF is maybe a little higher than
before, but not really.
I wouldn't be surprised at some way that
I wouldn't be surprised at some way that
it's something about the way I'm doing
it's something about the way I'm doing
this.
Why are there some shillers of Isaac and
Why are there some shillers of Isaac and
on Twitter?
Well, the thing is like if you're doing
Well, the thing is like if you're doing
humanoids, for instance, you're not
humanoids, for instance, you're not
going to go write your own humanoid sim.
going to go write your own humanoid sim.
The problem is those packages are these
The problem is those packages are these
really big bulky physics sims that are
really big bulky physics sims that are
super slow when you have like relatively
super slow when you have like relatively
simple form factors. Um,
simple form factors. Um,
but like the vast majority of
but like the vast majority of
roboticists are not simulation
roboticists are not simulation
engineers. Like they're not going to go
engineers. Like they're not going to go
spend their time working on how can I
spend their time working on how can I
make a better sim, right? They spend
make a better sim, right? They spend
their time like fiddling with control
their time like fiddling with control
algorithms and training and setups and
algorithms and training and setups and
stuff like that.
Do you see puffer training robotics over
Do you see puffer training robotics over
the next few years? Well, I mean,
the next few years? Well, I mean,
robotics is really just one application
robotics is really just one application
area for us, right? And it's not even
area for us, right? And it's not even
necessarily the biggest one. Um, we're
necessarily the biggest one. Um, we're
going to have our drone project. I think
going to have our drone project. I think
that we should be able to get something
that we should be able to get something
pretty cool out of that. So, you see
pretty cool out of that. So, you see
like, yeah, you can do this stuff. You
like, yeah, you can do this stuff. You
can do it in fast simulation. You can do
can do it in fast simulation. You can do
it very well. probably going to have and
it very well. probably going to have and
we've already had one person working on
we've already had one person working on
like a six degree of arm uh sim. See if
like a six degree of arm uh sim. See if
we can get that to work and throw that
we can get that to work and throw that
on real hardware and depending on how
on real hardware and depending on how
those go. Then we'll see what else we do
those go. Then we'll see what else we do
in robotics, right?
in robotics, right?
Like one of many problems that we can
Like one of many problems that we can
solve though.
Something weird happened because
Something weird happened because
Ward in the buffer kept going up.
The performance did not
performance stalled.
I guess this is the main the main
I guess this is the main the main
difficulty is how do we actually
difficulty is how do we actually
make sure
make sure
we're getting good data in this buffer
we're getting good data in this buffer
that's like diverse and interesting.
that's like diverse and interesting.
Okay.
Yeah, that's the key problem here.
cuz like we know that we can learn.
I mean initially what happens if I just
what happens if I just
say we kick out four samples
that do for
We'll see how this compares.
We'll see how this compares.
If there's a substantial difference,
If there's a substantial difference,
then we'll know it's the bias in the
then we'll know it's the bias in the
data. If not, then we'll have to think
data. If not, then we'll have to think
about this more.
It's so weird to me how this will get
It's so weird to me how this will get
you like a relatively decent policy so
you like a relatively decent policy so
quickly,
quickly,
not even using any RL. Hell.
It's tough because this feels like
It's tough because this feels like
incredibly powerful, but it has weak
incredibly powerful, but it has weak
spots and they're different weak spots
spots and they're different weak spots
from what I'm used to with um EPO and
from what I'm used to with um EPO and
such.
Very different weak spots.
Very different weak spots.
This appears to be doing like
This appears to be doing like
substantially worse than before.
substantially worse than before.
We'll see by the end of training.
Fact does something on our hard M though
Fact does something on our hard M though
is crazy.
Not a super at all.
If this has to be full episode, it still
If this has to be full episode, it still
wouldn't work though because you could
wouldn't work though because you could
still save like the easiest levels for
still save like the easiest levels for
instance.
instance.
But actually prioritizing your data just
But actually prioritizing your data just
off of reward
off of reward
pretty tough.
I mean, we there's a massively less
I mean, we there's a massively less
sample efficient thing we can do. Kind
sample efficient thing we can do. Kind
of defeats the point, but is a good
of defeats the point, but is a good
experiment to run.
experiment to run.
Going through CS30 2 CS231N lectures and
Going through CS30 2 CS231N lectures and
problem sets as suggested.
problem sets as suggested.
Thoughts on what Justin is building? You
Thoughts on what Justin is building? You
know, I actually I meant to go visit
know, I actually I meant to go visit
because I just heard that he's actually
because I just heard that he's actually
an SF.
an SF.
Um,
Um,
I meant to go visit because I haven't
I meant to go visit because I haven't
talked to him in years.
talked to him in years.
I was uh I interned with him as a high
I was uh I interned with him as a high
school student actually. It was like the
school student actually. It was like the
first bit of research that I did,
but I haven't talked with him in a long
but I haven't talked with him in a long
time.
So, if this fails completely, this will
So, if this fails completely, this will
be interesting.
I expect this to basically just save the
I expect this to basically just save the
easy levels.
easy levels.
Basically give itself a very bad
Basically give itself a very bad
curriculum.
curriculum.
Like if you study for a math test by
Like if you study for a math test by
only solving the uh the first few
only solving the uh the first few
problems in the the section
that would be consistent with um
that would be consistent with um
a lower score
a lower score
like a lower but still reasonable for
well there's one thing that we can
well there's one thing that we can
definitely try
definitely try
um that we would expect to be like
um that we would expect to be like
substantially less sample efficient
substantially less sample efficient
which again kind of defeats the point.
It still won't
and might not be want to use testing
and might not be want to use testing
with this.
This is lower.
Get something that's like more
Get something that's like more
monolithic, I guess.
No, I'm surprised that triple triad
No, I'm surprised that triple triad
doesn't work. I'm really kind of
doesn't work. I'm really kind of
surprised with that. Like, why why
surprised with that. Like, why why
shouldn't triple triad work, right?
You looked at what building at World
You looked at what building at World
Labs.
Labs.
I have no idea what they're doing.
I have no idea what they're doing.
I really have no idea what they're
I really have no idea what they're
doing.
See like this failing is weird.
Something like this failing
at least is positive score.
at least is positive score.
Very poor.
Very poor.
Very very poor.
Very very poor.
Negative 0.
Negative 0.
Really?
Oh,
try this. But I doubt this.
It's kind of weird how I can't really
It's kind of weird how I can't really
predict what MSIS does stuff on versus
Can I not do connect four?
are connectable.
All right. Cuz it's kind of slow with
All right. Cuz it's kind of slow with
the opponent.
I don't want to just fiddle with this
I don't want to just fiddle with this
forever. I'm trying to think how I can
forever. I'm trying to think how I can
leverage this.
Getting
a value function RL giving you a value
a value function RL giving you a value
function is so powerful because
you get like reasonable estimates
you get like reasonable estimates
of segments even when there's not reward
of segments even when there's not reward
there like you get get state estimation
there like you get get state estimation
out of it. the tough thing. So this this
out of it. the tough thing. So this this
does not have any state estimation
does not have any state estimation
basically.
But then the problem is when you have
But then the problem is when you have
when you have a value function
like that gets stale.
like that gets stale.
And also if it has errors in it,
And also if it has errors in it,
tough to build a data set out of
I am kind of confused. Why?
How does this not work?
I tried this.
Other way around.
Maybe this does work and it's just very
Maybe this does work and it's just very
data inefficient. Here
data inefficient. Here
I like
I like do this
This is like a straight up best event,
This is like a straight up best event,
right?
get stuck already. Like 18.
Guess there's just super little variance
Guess there's just super little variance
in the data then of that, huh?
Entropy is super low.
Oh no.
problem is this is now completely off of
problem is this is now completely off of
the original objective of this ample
the original objective of this ample
efficiency.
Not even faster wall clock even though
Not even faster wall clock even though
fan super
fan super
even though like high SPS
that is in some sense a very easily
that is in some sense a very easily
scalable method.
Best of n.
What if we try to do like something
What if we try to do like something
offline?
problems when you go to offline, you end
problems when you go to offline, you end
up with the same bloody problems off
up with the same bloody problems off
policy has, don't you?
policy has, don't you?
Yeah. Because then you end up with like
Yeah. Because then you end up with like
Yeah. Literally all the same problems.
What are the issues with off policy?
What are the issues with off policy?
Well, it's the methods get very
Well, it's the methods get very
complicated very quickly because
complicated very quickly because
the base algorithm in on policy of
the base algorithm in on policy of
policy gradients, you don't need to bolt
policy gradients, you don't need to bolt
that much stuff onto it before you have
that much stuff onto it before you have
a soda algorithm essentially. Like if
a soda algorithm essentially. Like if
you just take on pol like just policy
you just take on pol like just policy
gradients and you bolt on advantage
gradients and you bolt on advantage
estimation and clipping, you already
estimation and clipping, you already
have PO pretty much. Whereas if you take
have PO pretty much. Whereas if you take
DQN, it just fails horribly at
DQN, it just fails horribly at
everything and you have to bolt on like
everything and you have to bolt on like
six or seven different tricks to even
six or seven different tricks to even
get it to be comparable to PO. Well,
I'm also trying to figure out how
How's a Q function even for you from
function? behavioral
cloning.
cloning.
What problem do you big enough data set
What problem do you big enough data set
you don't have any problems?
you don't have any problems?
Of course, I tried to do a peace meal,
Of course, I tried to do a peace meal,
so I didn't I hit problems.
so I didn't I hit problems.
big enough data set
have any problems cloning
with sparity problems online
with sparity problems online
major Of
course, you can't reuse
course, you can't reuse
data
data
to be expert quality data.
There any other way we can go about
There any other way we can go about
this?
I don't want to type this rock. I'm
I don't want to type this rock. I'm
trying to think what else I can come up
trying to think what else I can come up
with that'll like give me a good answer.
who I spoke with at
who I spoke with at
at Nurups.
This the on is this off policy?
Oh, okay. So, this is off policy. The
Oh, okay. So, this is off policy. The
VMPO
VMPO
separate
Okay, so this is the on policy. I didn't
Okay, so this is the on policy. I didn't
realize this. There are two versions.
realize this. There are two versions.
There's an off policy version and
There's an off policy version and
there's an on policy.
So, this is probably
So, this is probably
what I was directed to.
Super math heavy paper.
Very very different out potentially
Very very different out potentially
here.
here.
Do they have a pseudo code block?
I can understand this at least a little
I can understand this at least a little
bit.
Yeah.
Okay, this is an M application.
like more experiments.
What would you say scal would you say
What would you say scal would you say
scalability is one of the drawbacks of
scalability is one of the drawbacks of
off policy?
No, it's just
No, it's just
look basically the only actually good
look basically the only actually good
off policy research it's like a small
off policy research it's like a small
handful of deep mind papers behind
handful of deep mind papers behind
closed doors that don't have a ton of
closed doors that don't have a ton of
context
context
like all the random p like all the
like all the random p like all the
random robotics papers using SACE for a
random robotics papers using SACE for a
million steps on the same three tasks
million steps on the same three tasks
like you can basically just ignore that
does bother me here that they only do
does bother me here that they only do
control task.
Why did they do
they do this?
Okay, actually this will be a better
Okay, actually this will be a better
look at
run from full state.
Is this
Is this
I think they don't have comparisons but
I think they don't have comparisons but
I think we together
task name equals do
Seven.
Seven.
Wait, what?
Same problem.
Same problem.
Any luck on material science? Uh, I've
Any luck on material science? Uh, I've
talked to a couple people. I need to go
talked to a couple people. I need to go
back to doing it. I'm doing core
back to doing it. I'm doing core
research at the moment, though. I'm
research at the moment, though. I'm
taking a break from doing
taking a break from doing
like building more applications to do a
like building more applications to do a
little bit of research.
to run from
to run from
more experiments in here or this I have
more experiments in here or this I have
to work
Wait, what?
SACE and PO are on top of each other
SACE and PO are on top of each other
here, right?
M no up here 25 mil different steps
what the heck is this
build
build
the different architecture True. Well,
oh, we do have MO on here.
They have MO as the best baseline
They have MO as the best baseline
for this stuff.
I probably do have to actually
I probably do have to actually
understand this thing
though.
I think is there anything I can take
I think is there anything I can take
from segment though?
At the very least
can say for certain that we're nowhere
can say for certain that we're nowhere
near optimal with our current algorithm.
The fact
you can say that for certain tasks
you can say that for certain tasks
in a very simple like data collection
in a very simple like data collection
strategy
strategy
with imitation learning
with imitation learning
works very well. also a fact. Also say
works very well. also a fact. Also say
it does shockingly well on some hard
it does shockingly well on some hard
problems, including at least better
problems, including at least better
initially than
initially than
uh our best online RL method. Also a
uh our best online RL method. Also a
fact.
They ever in
they wrote this followup.
they wrote this followup.
Why would they write this followup?
They tell us
it's interesting that they wrote this
it's interesting that they wrote this
paper.
They just want to adapt this original to
They just want to adapt this original to
high data regime.
That the idea
top K advantages.
Yes, this is actually
Yes, this is actually
this is in ours as well. Okay.
Fact that they wrote the original off
Fact that they wrote the original off
policy paper then they wrote a follow-up
policy paper then they wrote a follow-up
on policy is kind of telling in some
on policy is kind of telling in some
sense isn't it
sense isn't it
like why would they do That
Pass smart.
LM
I didn't they
I didn't they
is slow
is slow
like why they didn't even give
like why they didn't even give
themselves reasonable MO baselines right
themselves reasonable MO baselines right
like look they just treat
like look they just treat
where is it
where is it
okay look how crazy this is right this
okay look how crazy this is right this
is what I say about science in this
is what I say about science in this
space just being weird
space just being weird
why wouldn't you just run mo
like why wouldn't you do a learning
like why wouldn't you do a learning
curve for MO.
curve for MO.
It's literally your algorithm.
It's literally your algorithm.
These are both deep minds similar like
These are both deep minds similar like
shared authorship.
shared authorship.
So they just have this line at 40
So they just have this line at 40
million steps. What they say here,
million steps. What they say here,
right, is that MO is better than SVG,
right, is that MO is better than SVG,
better than uh where is it?
better than uh where is it?
They say this is better,
They say this is better,
but they don't actually do a comparison
but they don't actually do a comparison
curve. And this thing is a tiny number
curve. And this thing is a tiny number
of samples,
of samples,
40 million. And then here they do better
40 million. And then here they do better
than mo once they have two billion
than mo once they have two billion
environment steps.
So according to their own experiments
So according to their own experiments
and here's like 600 million
and here's like 600 million
according to their own experiments. This
according to their own experiments. This
thing is like massively more sample
thing is like massively more sample
efficient at least up to this level of
efficient at least up to this level of
PF.
PF.
But they still wrote this algorithm and
But they still wrote this algorithm and
have this paper.
So presumably like there's got to be a
So presumably like there's got to be a
big difference in computation applied
big difference in computation applied
somehow.
somehow.
I have to understand the algorithms more
I have to understand the algorithms more
for that. So definitely going to look at
for that. So definitely going to look at
these two.
The other thing that I think is
The other thing that I think is
interesting, right?
This is as far as I am aware.
This is like the last core algorithm
This is like the last core algorithm
paper.
MP and BMP. Yeah.
this thing also include algorithm
this thing also include algorithm
changes. I mean uh network changes
Did they use a transform
or network?
I think I trust this way more.
The other one.
already senior on this.
When you go to this paper here, right,
it's only was this two years later?
This is just a ResNet.
They didn't keep the architecture
They didn't keep the architecture
for this
actually. Do we have curves?
Wait, this makes no bloody sense, right?
Yeah. Yeah. Yeah. This thing makes no
Yeah. Yeah. Yeah. This thing makes no
sense at all. Look at this.
sense at all. Look at this.
They're doing billions of steps to solve
They're doing billions of steps to solve
Breakout.
Yeah, I don't trust this as far as I can
Yeah, I don't trust this as far as I can
throw it.
Multitask stuff is interesting,
but then that's um not a heavily
but then that's um not a heavily
contested benchmark.
contested benchmark.
Basically, just keep mind doing that.
Basically, just keep mind doing that.
Okay, so they did adapt this thing
Okay, so they did adapt this thing
probably some high throughput thing as
probably some high throughput thing as
well.
But like the results kind of suck.
Unless I'm missing something here,
Unless I'm missing something here,
right?
right?
But don't these results just kind of
But don't these results just kind of
suck?
like bill like fancy architecture,
like bill like fancy architecture,
billions of frames or freaking Atari
billions of frames or freaking Atari
like Breakout. Five billion frames.
Yeah. And then they're baselining versus
Yeah. And then they're baselining versus
like PO at three million steps. Like,
like PO at three million steps. Like,
oh, it's down here. It's three million
oh, it's down here. It's three million
steps.
steps.
Honestly, the interesting one is
the MO being higher in sample F on this
ulser versus SACE being very close to PO
ulser versus SACE being very close to PO
on some of these
I don't think what we take from
I think we have to start looking at
I think we have to start looking at
the details of this paper
about this.
I saw some random,
dude. What are you doing?
Absolutely deranged programmer.
very complicated paper.
This all of it.
Yeah, this is just a network
Yeah, this is just a network
just a network
buffer. Okay, so at least this is the
buffer. Okay, so at least this is the
entire implementation.
Fire in
There's also this update critic.
Me see if I can understand at least a
Me see if I can understand at least a
little bit of this
policy evaluation update. Critic
lost.
lost.
This is down here.
The forward pass.
The forward pass.
The categorical
take log probed
two.
discounting. So this is the onestep
boss event. Why
did
This is fine. This is fine. This is mini
This is fine. This is fine. This is mini
batch right
batch right
this loss
credits.
Then you get
Then you get
dual function.
Just compute.
Do multiple iterations on this.
There's the
There's the
word pass.
a totally crazy algorithm.
Obviously, there are ton of details to
Obviously, there are ton of details to
this
I have to write Martin an email, I
I have to write Martin an email, I
think, before I go too crazy into this.
think, before I go too crazy into this.
I'm trying to think what's the most
I'm trying to think what's the most
productive way for me to go about things
productive way for me to go about things
right now. Um,
right now. Um,
I have some initial results.
I have some initial results.
They're not good enough to turn into a
They're not good enough to turn into a
new algorithm just off the bat. They are
new algorithm just off the bat. They are
enough to convince me though that there
enough to convince me though that there
are substantial holes in the current
are substantial holes in the current
algorithm like qualitative holes in the
algorithm like qualitative holes in the
current algorithm.
Neural MMO experiment probably being the
Neural MMO experiment probably being the
most convincing bad
most convincing bad
mostly around reward sparity. I think
mostly around reward sparity. I think
sample reviews
I'm going to have to email Martin on
I'm going to have to email Martin on
this and I'm going to have to do some
this and I'm going to have to do some
thinking
thinking
as to like do I want to invest in doing
as to like do I want to invest in doing
a full MO implementation.
a full MO implementation.
I think there are more things I don't
I think there are more things I don't
understand. The other thing I really
understand. The other thing I really
don't understand
don't understand
here's the question to people actually
here's the question to people actually
here's the question for the audience if
here's the question for the audience if
anybody has any idea off policy is
anybody has any idea off policy is
supposed to be able to is supposed to
supposed to be able to is supposed to
enable sample reuse right
enable sample reuse right
because you're no longer the problem in
because you're no longer the problem in
on policy right is you're getting data
on policy right is you're getting data
from your policy
from your policy
um but then you're actually evaluating
um but then you're actually evaluating
your data with a value function and when
your data with a value function and when
the data is old the value function is
the data is old the value function is
stale fail. Um, you're not actually
stale fail. Um, you're not actually
going to be able to like data that came
going to be able to like data that came
from a previous policy is not going to
from a previous policy is not going to
have a good value estimate and you're
have a good value estimate and you're
not going to be able to compute
not going to be able to compute
advantages and the whole thing falls
advantages and the whole thing falls
apart.
apart.
The thing I don't understand is how
The thing I don't understand is how
uh how does off policy fix this if when
uh how does off policy fix this if when
you have a Q function
you have a Q function
right like okay q function gives you the
right like okay q function gives you the
value for each uh each action you could
value for each uh each action you could
take right so for that one step that
take right so for that one step that
makes sense but the problem is you're
makes sense but the problem is you're
still training it against this
still training it against this
discounted return of what happens if uh
discounted return of what happens if uh
I take this action and then follow my
I take this action and then follow my
own policy or whatever. And if I follow
own policy or whatever. And if I follow
my own policy from there, then like,
my own policy from there, then like,
you're still going to get off policy,
you're still going to get off policy,
aren't you? I'm probably not phrasing
aren't you? I'm probably not phrasing
that great, but that's the thing I don't
that great, but that's the thing I don't
understand. Like, does this even fully
understand. Like, does this even fully
make sense? Um, cuz I talked to to Pablo
make sense? Um, cuz I talked to to Pablo
at RLC. He said like, "Yeah, we can't
at RLC. He said like, "Yeah, we can't
even really do more than eight update
even really do more than eight update
epochs on our data with off policy.
epochs on our data with off policy.
So, that seems really weird to me.
3:00 3:12.
3:00 3:12.
Uh, I have a separate project that I
Uh, I have a separate project that I
could work on now if we're stuck on
could work on now if we're stuck on
this. Unless anybody has any ideas
this. Unless anybody has any ideas
there.
there.
I have a separate thing that I can work
I have a separate thing that I can work
on.
on.
I'm going to go I'll be right back and
I'm going to go I'll be right back and
think about this for a minute and then
think about this for a minute and then
we'll see.
Let me get another.
Let me get another.
Oh, I'm still good here.
Okay,
Okay,
so
so
let me think a little bit more about
let me think a little bit more about
this problem before I just move to
this problem before I just move to
something else because I'm kind of in
something else because I'm kind of in
the right mind space for this at the
the right mind space for this at the
moment.
You're going to collect
policy pi
date.
This gives you action.
B.
You
kind of value as well, right? It gives
kind of value as well, right? It gives
you a value.
I'm going to collect a bunch of data.
And I also have the actions.
Words.
Okay. So, we have some segment here.
What I'm just doing now,
I'm saying this segment is good.
If we have
it just has high summed rewards.
All right. So this is what we have now.
All right. So this is what we have now.
This is one thing
This is one thing
we could do.
Did we just add the value?
Could add the value of the final state
Could add the value of the final state
to this.
Not easily though, right? Because we
Not easily though, right? Because we
have to recomputee it.
Theoretically though, at the very least,
Theoretically though, at the very least,
we could do plus
we could do plus
a
a
do V
quality of the segment. Of
course, the problem with doing this is
course, the problem with doing this is
you have to recomputee it every time you
you have to recomputee it every time you
update the bloody network.
But theoretically, we could do something
But theoretically, we could do something
like this.
And then this could fix the problem
And then this could fix the problem
of not being able to rank experience by
of not being able to rank experience by
reward alone.
What does a Q function give you for
What does a Q function give you for
this?
this?
Delay the network. You can,
Delay the network. You can,
but um
but um
what the heck does a Q function even
what the heck does a Q function even
give you?
sample transitions or whatever or
sample transitions or whatever or
segments
this does still not just get stale.
Yes, bet. Like generally what does um
Yes, bet. Like generally what does um
how precisely
how precisely
does a Q function let you go off policy?
does a Q function let you go off policy?
I'm not actually seeing it when I'm
I'm not actually seeing it when I'm
looking at it closely.
Suppose you sample a batch of
Suppose you sample a batch of
experience.
Is it just that it's recursively
Is it just that it's recursively
computed?
I keep thinking of this in terms of
I keep thinking of this in terms of
segments and I forget that a lot of the
segments and I forget that a lot of the
time people are like Q function off off
time people are like Q function off off
policy you're literally just looking at
policy you're literally just looking at
at like state action state or reward or
at like state action state or reward or
whatever.
If this actually makes sense to me
If this actually makes sense to me
that this would make a fix problem.
Yeah. So this is correct because this is
Yeah. So this is correct because this is
QST star which is optimal policy. Okay.
How does this work when you do not have
How does this work when you do not have
QST Are
It's a very different case.
It's a very different case.
The tabular case you can actually prove
The tabular case you can actually prove
that it will converge.
I suppose it's state action state pair
I suppose it's state action state pair
like it's triples or whatever.
It's kind of a world modeling objective
It's kind of a world modeling objective
in some sense, isn't it?
But it's kind of hidden in here, you
But it's kind of hidden in here, you
know,
because you take the difference
because you take the difference
you take the difference of these Q
you take the difference of these Q
functions.
You literally just get a value estimate
You literally just get a value estimate
out of it
versus the idea. I suppose
versus the idea. I suppose
the thing that stays fixed and is policy
the thing that stays fixed and is policy
independent, right? The idea is that if
independent, right? The idea is that if
you're in a state
you're in a state
and you take an action in that state,
and you take an action in that state,
then at least with the same probability,
then at least with the same probability,
you will see the next state, right? Like
you will see the next state, right? Like
that that transition stays the same.
Yes, it's off policy.
But the key actual idea is that the
But the key actual idea is that the
transition like the transitions don't
transition like the transitions don't
change.
change.
That's the thing that you're exploiting.
That's the thing that you're exploiting.
It is not dependent on the policy.
Of course, it's hard to freaking see
Of course, it's hard to freaking see
that and all this math that's like
that and all this math that's like
wrapped up with other stuff,
wrapped up with other stuff,
but I don't think there's anything else
but I don't think there's anything else
magic here.
I think that's the key idea of it.
Let me just see if there's any other
Let me just see if there's any other
framing of it in here.
One.
Okay. So
Okay. So
this is a pure iterative form in a
this is a pure iterative form in a
tabular case, right?
tabular case, right?
Step size Reward at next time step
Step size Reward at next time step
discount factor.
discount factor.
The Q val the maximum uh the val the
The Q val the maximum uh the val the
take the action such if you take the
take the action such if you take the
action such that the Q value is
action such that the Q value is
maximized. What is that value?
maximized. What is that value?
What this piece is and this is at the
What this piece is and this is at the
next state minus the previous state.
So wait,
do you this state
do you this state
take this action gets you to state t +
take this action gets you to state t +
one
maximum action?
Oh wait, wait. There Q directly
Oh wait, wait. There Q directly
approximates Qstar which is the optimal
approximates Qstar which is the optimal
action value function independent of the
action value function independent of the
policy being followed
directly approximate to star which is
directly approximate to star which is
the optimal option
the optimal option
is true in the
is true in the
neural net case
determines which state action didn't
all pairs update
at the guarantee.
Well, clearly you can break it in the um
Well, clearly you can break it in the um
the neural net case, right?
Because here you're only ever updating
Because here you're only ever updating
you're maintaining an estimate for each
you're maintaining an estimate for each
state in action, right? each transition.
state in action, right? each transition.
So like at the very least you can have a
So like at the very least you can have a
continual learning fail in the function
continual learning fail in the function
approximation case, right? If you sample
approximation case, right? If you sample
if you don't sample evenly in your data
one policy that this is not a neural net
one policy that this is not a neural net
bed it's literally a table
bed it's literally a table
the tabular learning case
the tabular learning case
the original this is the original predqn
the original this is the original predqn
algorithm. This is just this is standard
algorithm. This is just this is standard
Q-learning.
And essentially what I'm trying to do
And essentially what I'm trying to do
for the folks who just joined, right, is
for the folks who just joined, right, is
I'm trying to figure out how strong of a
I'm trying to figure out how strong of a
guarantee off policy algorithms actually
guarantee off policy algorithms actually
give you that you can continue to reuse
give you that you can continue to reuse
whatever data you want without breaking
whatever data you want without breaking
it.
I suppose in some sense though it is a
I suppose in some sense though it is a
recursive
like whenever you go to update your Q
like whenever you go to update your Q
function
function
you're not going to have a stale value
you're not going to have a stale value
because like you're calling your value
because like you're calling your value
function again. So you're getting fresh
function again. So you're getting fresh
value data.
value data.
The problem is also it's based on a
The problem is also it's based on a
one-step bootstrap at the same time,
one-step bootstrap at the same time,
right?
So, doesn't this break as soon as you
So, doesn't this break as soon as you
try to do more than one step?
Cuz I think that this makes sense in
Cuz I think that this makes sense in
this like onestep case, right?
this like onestep case, right?
Nothing is ever stale
Nothing is ever stale
because you're evaluating your Q
because you're evaluating your Q
function on this is
function on this is
uh state action reward state action.
uh state action reward state action.
It's SARSA, right?
You have SARSA data like you have state
You have SARSA data like you have state
action reward state action data.
So nothing can ever get stale here.
Now the problem is that like as soon as
Now the problem is that like as soon as
you want to do a multi-step bootstrap
you want to do a multi-step bootstrap
right
right
then you are actually off of your
then you are actually off of your
function.
I'm pretty damn sure at least
I'm pretty damn sure at least
now I was looking at this on the plane.
now I was looking at this on the plane.
There is
There is
there is something that handles this
there is something that handles this
later on. Let me find it.
Yeah. So here
endstep off policy. Let's start with
endstep off policy. Let's start with
this
endstep offs policy.
Okay. So there is an important sampling
Okay. So there is an important sampling
thing that you can do here.
Does this actually fully correct for it
Does this actually fully correct for it
though?
I suppose it does, doesn't it?
Wait,
Wait,
hang on. Hang on. No, there's a there's
hang on. Hang on. No, there's a there's
a gotcha here. Hold on.
Why is the stale one.
Uh yeah, this can blow up.
Can it not? Yeah, this can totally blow
Can it not? Yeah, this can totally blow
up.
Yeah. So, the thing is right this Yes.
Yeah. So, the thing is right this Yes.
Okay. So, here's the issue.
Okay. So, here's the issue.
You can still technically get some
You can still technically get some
information
information
out of uh an old sequence that's with a
out of uh an old sequence that's with a
very different policy.
But like it's not actually
suppose the only case in which it's
suppose the only case in which it's
actually useful is if it's like expert
actually useful is if it's like expert
data, right?
Makes sense if it's expert data.
It does.
So yeah, I mean in that case then right
So yeah, I mean in that case then right
data that's collected with a clumsy
data that's collected with a clumsy
policy
like a segment that's collected from a
like a segment that's collected from a
clumsy policy is simply not going to be
clumsy policy is simply not going to be
useful for anything other than a world
useful for anything other than a world
modeling objective.
But if you have a good segment, let's
But if you have a good segment, let's
say you get lucky even
then you can still kind of use it.
then you can still kind of use it.
If you have expert data,
I think this important sampling ratio
I think this important sampling ratio
actually helps you a lot.
Yes.
solved end policy.
Yeah, that's I did that before today.
Yeah, that's I did that before today.
Bet it's something slightly different.
Bet it's something slightly different.
Um it mostly works if you do that even
Um it mostly works if you do that even
the mo even if you do like the most
the mo even if you do like the most
naive possible way. It's not perfect
naive possible way. It's not perfect
surprisingly
surprisingly
like the behavioral cloning objective is
like the behavioral cloning objective is
surprisingly not just perfect
surprisingly not just perfect
but it does like it does train something
but it does like it does train something
in pretty much all cases.
I mean regardless though, right? Like
I don't think the same convergence
I don't think the same convergence
guarantees hold.
because it's a neural net, right? So, if
because it's a neural net, right? So, if
you if you think of a neural net like a
you if you think of a neural net like a
water balloon
water balloon
or like a you know, whatever, like you
or like a you know, whatever, like you
push it in one area and it's going to
push it in one area and it's going to
blub out in another area.
So like the reason that
So like the reason that
if you train it on like bad historical
if you train it on like bad historical
data, it's going to be bad.
Should it be better? Should it be
Should it be better? Should it be
strictly better to train on more data
strictly better to train on more data
than less data?
Let's say the data set is sufficiently
Let's say the data set is sufficiently
large.
I think the answer in theory is yes, but
I think the answer in theory is yes, but
in practice there are a lot of other
in practice there are a lot of other
factors.
There's no label. The rewards are always
There's no label. The rewards are always
correct. It's ground truth data. RL's
correct. It's ground truth data. RL's
ground truth data always.
I'm asking in the RL case specifically.
You assume the rewards are always
You assume the rewards are always
perfectly correct.
perfectly correct.
You know, it's it's more important in
You know, it's it's more important in
the in this case though, right? It's
the in this case though, right? It's
like the RL case is different here.
So I suppose if you take the supervised
So I suppose if you take the supervised
case then the answer is no. So the
case then the answer is no. So the
answer here
answer here
probably also no for sufficiently
probably also no for sufficiently
degenerate data which you could very
degenerate data which you could very
easily end up getting.
Okay,
we have this endstep off policy. Cool.
And let's see what this is.
Is this actually a clean algorithm?
Is this actually a clean algorithm?
Hang on.
clean as in it's not rainbow. Like Rambo
clean as in it's not rainbow. Like Rambo
has a bunch of crap bolted onto
Why is this
Why is this
bursting?
Hang on. That's sketchy as hell, isn't
Hang on. That's sketchy as hell, isn't
it?
Wait, I
Okay. So, this actually doesn't work.
Okay. So, this actually doesn't work.
Yeah, this does not work. This is
Yeah, this does not work. This is
continuous control specific.
continuous control specific.
That's a good thing that I found that
That's a good thing that I found that
doesn't remotely work out of the box.
doesn't remotely work out of the box.
Okay.
have to read a bunch of this map at some
have to read a bunch of this map at some
point. This definitely is not an out of
point. This definitely is not an out of
the box. It's a good thing that I found
the box. It's a good thing that I found
that ablation.
on this.
It is
We get rid of dueling doesn't do
We get rid of dueling doesn't do
anything.
Does a little bit something
Does a little bit something
get rid of double
double is funny that
up.
A
lot of these hurt.
It's actually funny. The distributional
It's actually funny. The distributional
one gets the closest.
one gets the closest.
You just took a couple of these tricks.
Prioritize replay plus
Prioritize replay plus
probably get most of it.
Move dual.
Only four tricks.
Only four tricks.
Cut out these dumb ones.
How many of these do I have in our
How many of these do I have in our
current thing?
current thing?
We have priority. We have multistep.
What's noisy?
distribution is what um Spencer was
distribution is what um Spencer was
doing. What is noisy?
The castic network layers for
The castic network layers for
exploration. That's horrifying.
But we kind of have most of these now,
But we kind of have most of these now,
don't we?
How do you do um
How do you do um
what's the continuous sample look like
what's the continuous sample look like
for a Q function?
for a Q function?
Is it bucketed?
I'd like to try
It's funny though that they you get rid
It's funny though that they you get rid
of DDQN and it still works.
It's actually cleaner, isn't it?
4:00.
4:00.
Okay. Well,
I think it makes sense for me to start
I think it makes sense for me to start
messing around with off Paul tomorrow.
messing around with off Paul tomorrow.
Um
definitely something being able to keep
definitely something being able to keep
data around.
Likewise,
Likewise,
I haven't been able to get any form of
I haven't been able to get any form of
um pure imitation
um pure imitation
like best of end data.
like best of end data.
be remotely as close
be remotely as close
on polic as like on policy RL.
Suppose the question then should be
if I replace the on policy formulation
if I replace the on policy formulation
with an off policy formulation
with an off policy formulation
something way closer that should be
something way closer that should be
theoretically even almost directly
theoretically even almost directly
comparable
comparable
what happens
what like kind of haven't done that
what like kind of haven't done that
sensors done HL gaus for the uh the
sensors done HL gaus for the uh the
value function specifically But that's
value function specifically But that's
kind of a different thing.
You don't need HL Gaus on Hang on. What
You don't need HL Gaus on Hang on. What
is What is the distributional? Let me
is What is the distributional? Let me
look up this last thing.
I see
Oh, I think I get it. Yeah, because you
Oh, I think I get it. Yeah, because you
get a quadratic loss with uh standard
get a quadratic loss with uh standard
DQN, right?
DQN, right?
You actually want something like HL
You actually want something like HL
Gaus.
Gaus.
Let's see.
Yeah, this one
G for this
test. Okay,
message Spencer on this real quick.
Yeah, funny.
Yeah, funny.
I mentioned it for this action thing.
I mentioned it for this action thing.
All
All
right,
right,
that's pretty much a good view. Let me
that's pretty much a good view. Let me
let me find Spencer.
Okay. So, we have a few extra hours.
Okay. So, we have a few extra hours.
I think what we can do is we can start
I think what we can do is we can start
on the other new project.
There are two projects that I came away
There are two projects that I came away
from RLC with. Right. One is actually
from RLC with. Right. One is actually
figuring out the sample efficiency
figuring out the sample efficiency
thing. You see I've been doing imitation
thing. You see I've been doing imitation
when a try off policy. There are lots of
when a try off policy. There are lots of
things to mess around with there, right?
things to mess around with there, right?
Um, the other thing is getting an
Um, the other thing is getting an
actually clean test suite of
actually clean test suite of
environments for really determining
environments for really determining
which of this works and doesn't.
So there's this paper which is
So there's this paper which is
a well motivated but well motivated but
a well motivated but well motivated but
with a clumsy execution in my mind.
So what they try to do is they try to
So what they try to do is they try to
come up with environments that measure
come up with environments that measure
agent capabilities like Pokemon on this
agent capabilities like Pokemon on this
like well what will forever be in my
like well what will forever be in my
mind the Pokemon chart because that's
mind the Pokemon chart because that's
where I first saw it. Um, but these
where I first saw it. Um, but these
different axes,
different axes,
like I don't think these are actually
like I don't think these are actually
the axes
the axes
that you want.
Make a dock for
Oh, perfect. Spencer's running sweeps.
for puffer bench. Let's think of some
for puffer bench. Let's think of some
things here. Um,
let's just type some keywords in. So,
let's just type some keywords in. So,
they they talk about like
they they talk about like
generalization,
generalization,
exploration,
exploration,
and more
and more
ways.
credit
credit
I don't think this is a good basis
I don't think this is a good basis
because in my mind right
what's this so this is
what's this so this is
robustness noise right
just take these out immediately
Credit assignment require memory?
It does, doesn't it?
Well, it doesn't require the agent to
Well, it doesn't require the agent to
have memory. It requires the algorithm
have memory. It requires the algorithm
to have like a multi-step
to have like a multi-step
thing to it.
But in our case, right, actual
But in our case, right, actual
intelligence,
intelligence,
credit assignment requires memory
credit assignment requires memory
fundamentally.
memory.
Credit assignment as a problem is
Credit assignment as a problem is
actually not a component vector, is it?
Like credit assignment is a higher level
Like credit assignment is a higher level
topic in some sense, right?
or is it fundamental?
How would you set up an RL problem that
How would you set up an RL problem that
does not require credit assignment?
Are you limited to like one-step
Are you limited to like one-step
problems?
Credit assignment is like a blanket term
Credit assignment is like a blanket term
now.
and let me answer one message about
and let me answer one message about
this.
These are bad. Okay, let me sort of
These are bad. Okay, let me sort of
explain what I'm thinking here.
explain what I'm thinking here.
Uh the problem is that these are bad
Uh the problem is that these are bad
basis vector, right? This was like if I
basis vector, right? This was like if I
were instead of giving you uh if I'm
were instead of giving you uh if I'm
trying to cover a space and in this case
trying to cover a space and in this case
the space we're trying to cover is like
the space we're trying to cover is like
difficulty in RL and instead of giving
difficulty in RL and instead of giving
you like X Y Z I give you like 2X + Z
you like X Y Z I give you like 2X + Z
and like X - Y + 3 Z and then like 3 Z
and like X - Y + 3 Z and then like 3 Z
minus Y or something that's even
minus Y or something that's even
independent like yeah probably is has to
independent like yeah probably is has to
be. But yeah, it's just like a really
be. But yeah, it's just like a really
crappy basis.
We're trying to think like what are the
We're trying to think like what are the
actual I like in order for something to
actual I like in order for something to
be able basis vector we have to be able
be able basis vector we have to be able
to test it on its own cleanly somehow
I suppose one of the things that messes
I suppose one of the things that messes
with this, right, is there's a
with this, right, is there's a
difference between the algorithm being
difference between the algorithm being
able to do credit assignment and uh the
able to do credit assignment and uh the
agent being able to do credit
agent being able to do credit
assignment, right? Because they have
assignment, right? Because they have
RNN boot DQN. Where is this?
RNN boot DQN. Where is this?
Is this
charts
credit assignment? Okay, so this is D
credit assignment? Okay, so this is D
this is tiny but this is DQN having
this is tiny but this is DQN having
positive score in all the credit
positive score in all the credit
assignment tasks. So basically what they
assignment tasks. So basically what they
they have this umbrella length task
they have this umbrella length task
where it's like you you have to make a
where it's like you you have to make a
decision and then you only get the
decision and then you only get the
reward a long time after that. So an
reward a long time after that. So an
algorithm can do credit assignment even
algorithm can do credit assignment even
if the agent has no idea.
if the agent has no idea.
The agent is taught to make the right
The agent is taught to make the right
decision based on the context but it it
decision based on the context but it it
itself cannot do credit assignment which
itself cannot do credit assignment which
means it can't do in context learning or
means it can't do in context learning or
anything either.
Maybe I looking at this the correct way.
Maybe I looking at this the correct way.
Instead, think about hard problems that
Instead, think about hard problems that
we solve and what makes them hard.
is counting horizon.
What if we wanted to build
like what are the actual hard problems
like what are the actual hard problems
about?
I kind of want to just like
I kind of want to just like
can I instead think of like very simple
can I instead think of like very simple
games that has stuff that's not in
games that has stuff that's not in
Puffer Lab.
What if we do like a recipe game?
What if we do like a recipe game?
All right.
Each reset has several rounds.
Each reset has several rounds.
Round presents some ingredients.
Round
Round
and
target item.
Um,
a damn
that does what's this thing test?
that does what's this thing test?
This thing tests
tests in context
tests in context
learning for sure.
Is that any different though?
Or you could have all the ingredients.
Yeah, you could have all the
Yeah, you could have all the
ingredients, right?
Okay, I think that this one is decent.
This is like a cool little nifty game
This is like a cool little nifty game
and I come up with a few others like
and I come up with a few others like
this.
This is this is like this in context
This is this is like this in context
learning type thing.
kind of metaly and context learny
kind of metaly and context learny
whatever.
Is there something I can do to test like
Is there something I can do to test like
really long games?
Because progressive games have like
Because progressive games have like
there's several challenges to them,
there's several challenges to them,
right?
Do I want to test pure memory though?
puffer be applied
puffer be applied
basically anything that you know to make
basically anything that you know to make
it better. Uh we focus on fast
it better. Uh we focus on fast
environments. So like all our defaults
environments. So like all our defaults
are optimized around environments being
are optimized around environments being
fast. Obviously, we have ways to make a
fast. Obviously, we have ways to make a
lot of environments fast, but yeah, it
lot of environments fast, but yeah, it
can be a lot applied to a lot of stuff
can be a lot applied to a lot of stuff
like our implementation is just
like our implementation is just
generally better. It's not exactly PO.
generally better. It's not exactly PO.
It's a better algorithm.
I like this recipe game.
Generalization,
Generalization,
exploration,
exploration,
memory,
Okay, there's control as a problem
sweet test scenarios pretty much.
binding peptide generation.
I don't know what the simulator used is
I don't know what the simulator used is
for that. If there if that's a simulator
for that. If there if that's a simulator
in a ye a loop, then um
in a ye a loop, then um
I mean like molecular simulation is
I mean like molecular simulation is
probably the thing I'm most interested
probably the thing I'm most interested
in overall in RL. It's just hard.
Really is the most interesting thing out
Really is the most interesting thing out
there.
What is something like So bon
It's hard to actually come up with very
It's hard to actually come up with very
good test agents, right? Takes a lot of
good test agents, right? Takes a lot of
thought.
Oh, literally that concept merger game
Oh, literally that concept merger game
does this, doesn't it?
There's not going to be a mathematical
There's not going to be a mathematical
categorization like that.
Like you could do that, but that's not
Like you could do that, but that's not
actually the important one.
Okay, an alternative version of this
Okay, an alternative version of this
would be like
I don't know if this one has enough
I don't know if this one has enough
structure.
Yeah, I don't know if that one has
Yeah, I don't know if that one has
enough structure. The recipe game sounds
enough structure. The recipe game sounds
better to me.
Are these not the same game?
Oh yeah, these literally are the same
Oh yeah, these literally are the same
game, aren't they?
game, aren't they?
Yeah, these literally are two different
Yeah, these literally are two different
just flavors of the same game.
That's exploration in some sense, isn't
That's exploration in some sense, isn't
it?
That's exploration. That's in context
That's exploration. That's in context
learning
that actually tests a lot of good stuff.
that actually tests a lot of good stuff.
A bunch of related ideas.
There's precise control.
Precise control even really a thing?
load down to you.
There's also the really long horizon
There's also the really long horizon
stuff. How do we do that?
Like a lot of these tasks are kind of
Like a lot of these tasks are kind of
hard just because
hard just because
they change a little bit over time.
they change a little bit over time.
Horizons are really long and you
Horizons are really long and you
introduce new types of stuff over long
introduce new types of stuff over long
horizons. That's true of neural MMO.
What are some other environments that I
What are some other environments that I
liked that I've done before? Right.
liked that I've done before? Right.
So, I've done stuff like passcodes, like
So, I've done stuff like passcodes, like
guess a passcodes, like memory related.
That's an exploration problem as well in
That's an exploration problem as well in
some sense.
How is this different from recipe game?
How is this different from recipe game?
Recipe game has an additional element to
Recipe game has an additional element to
it.
Password guessing game is pure
Password guessing game is pure
exploration is interesting.
not pure exploration. It's also memory.
There is really no credit assignment
There is really no credit assignment
though, which is kind of interesting.
The two things missing.
The two things missing.
What other things are missing? Really?
What other things are missing? Really?
Like the length and progression of a
Like the length and progression of a
game.
Hang on. No, you can actually get you
Hang on. No, you can actually get you
can get both of these in this. You can
can get both of these in this. You can
actually get both of these.
actually get both of these.
always say Tetris looks nice. It does.
always say Tetris looks nice. It does.
Tetris looks very nice.
Length and progression we can get just
Length and progression we can get just
by making these very long, right?
by making these very long, right?
The recipe game in particular.
Okay, let's say that we have a game
Let's say that we have a game in which
Let's say that we have a game in which
we have
we have like a bunch of ingredients,
we have like a bunch of ingredients,
right?
right?
Let's say it's like a decent number of
Let's say it's like a decent number of
them like this.
Like suppose we extend this, right?
Like suppose we extend this, right?
Let's say that we start off with five
Let's say that we start off with five
ingredients, okay?
ingredients, okay?
And
And
we need to make a two slot recipe.
You pick two of these, fill the slots,
You pick two of these, fill the slots,
and you get out a result.
and you get out a result.
And the recipes don't change. So they're
And the recipes don't change. So they're
already set. You just don't know what
already set. You just don't know what
they are.
All right.
And then you know we do some of these
And then you know we do some of these
you end up getting
always do two slot recipes more than two
always do two slot recipes more than two
slot recipes.
And it could be that
you end up being able to use
you end up being able to use
products that you've made before. So the
products that you've made before. So the
initial set of recipes can use anything
initial set of recipes can use anything
in here and then whenever you make a
in here and then whenever you make a
product it can also use that
And the game can progress.
And the game can progress.
At a certain point, you unlock new
At a certain point, you unlock new
ingredients to use.
ingredients to use.
Now, there are a whole bunch of new
Now, there are a whole bunch of new
recipes that can also use these.
Keep going with this.
What
What
What qualities does this game have?
What qualities does this game have?
Well, obviously it requires memory.
It requires exploration.
The combination of both of these over
The combination of both of these over
several rounds is what you would call in
several rounds is what you would call in
context learning. Right?
context learning. Right?
So, it does require in context learning.
There is a concrete strategy from game
There is a concrete strategy from game
to game that you can learn.
It has the potential to scale to very
It has the potential to scale to very
long horizons.
long horizons.
It's a progressive game where you
It's a progressive game where you
actually have to play it for a while to
actually have to play it for a while to
see all the content, which is one of the
see all the content, which is one of the
key things that's difficult because like
key things that's difficult because like
you have to play through the early game
you have to play through the early game
to get to the end game.
You get dramatically less experience at
You get dramatically less experience at
the end game unless you get good at
the end game unless you get good at
getting there quickly.
It doesn't have any control aspect.
Main thing missing is like some sort of
Main thing missing is like some sort of
control aspect.
Now I suppose the thing that this
Now I suppose the thing that this
doesn't do, right? This gives you like a
doesn't do, right? This gives you like a
good total task.
The thing that this doesn't do is
The thing that this doesn't do is
separate them into individual tests.
Like it's possible we end up with a
Like it's possible we end up with a
really good environment for research.
Nice thing about it is you only have to
Nice thing about it is you only have to
run one hyperparameters. Well,
but disentangling these is very
but disentangling these is very
different.
Memory is pretty easy to test on a
or no because it's still credit
or no because it's still credit
assignment.
How do you test memory without credit
How do you test memory without credit
assignment in RL? Right.
up with a good joint test is actually
there a reason that you want to separate
there a reason that you want to separate
these
can't come up with a memory independent
can't come up with a memory independent
of credit assignment task.
of credit assignment task.
They're basically
They're basically
completely coingled.
Okay.
What about exploration?
What does exploration mean?
Again, I don't think it has meaning on
Again, I don't think it has meaning on
its own. It has to be
its own. It has to be
It has to mean learning an exploration
It has to mean learning an exploration
strategy more so than the algorithm
strategy more so than the algorithm
having one. Right?
The recipe game counts as that.
We could come up with a purer test.
Oh,
Oh,
Emory
could do
Red
passive.
passive.
What variation is that bet?
Oh,
so that would mean like the algorithm
so that would mean like the algorithm
itself that would be uh exploration
itself that would be uh exploration
encoded at the algorithm level.
encoded at the algorithm level.
The RL one though that's for memory
The RL one though that's for memory
independent
Let me think.
I guess there's like a question of what
I guess there's like a question of what
it means for what it means for an
it means for what it means for an
algorithm to be good at exploration
algorithm to be good at exploration
versus
Okay.
Oh, wait. Yeah, you can do this, right?
Oh, wait. Yeah, you can do this, right?
Because
Oh, yeah. You just do like the umbrella
Oh, yeah. You just do like the umbrella
problem, right? Yeah. It's like, okay,
problem, right? Yeah. It's like, okay,
you just do the memory problem, but you
you just do the memory problem, but you
can observe your choice.
Right.
Wait, actually, what did they do for
Wait, actually, what did they do for
pure memory?
Memory cards.
A long teammates.
Oh, is it because for the final action
Oh, is it because for the final action
you immediately get
I see you just flip it. So, it's like
I see you just flip it. So, it's like
you get presented the thing at the
you get presented the thing at the
start.
You have to remember it the full way,
You have to remember it the full way,
but you get rewarded instantly.
And then credit assignment. Your action
And then credit assignment. Your action
doesn't matter at the end. Okay. So, we
doesn't matter at the end. Okay. So, we
can actually do this. Yeah. Yeah, we can
can actually do this. Yeah. Yeah, we can
actually do this.
actually do this.
So for so for memory
So for so for memory
all you need to do is um
first
final action sides
take
credit
first action.
Okay, so you can actually perfectly
Okay, so you can actually perfectly
isolate memory and credit assignment.
isolate memory and credit assignment.
Okay, that's fine.
I don't know if there are any of these
I don't know if there are any of these
others that you can actually isolate so
others that you can actually isolate so
cleanly.
Honestly, you can very easily come up
Honestly, you can very easily come up
with a task that requires exploration
with a task that requires exploration
and
and
like exploration has to be learned.
like exploration has to be learned.
The password game requires exploration.
The password game requires exploration.
The um recipe game requires exploration.
Password game also requires memory.
Well, I suppose it depends, right?
You could make a version of it that
I guess there are like a few different
I guess there are like a few different
versions of this people talk about like
versions of this people talk about like
what about like just the big empty room
what about like just the big empty room
thing.
Yeah, it's difficult to come up with a
Yeah, it's difficult to come up with a
good version of that, right?
good version of that, right?
What's like an actual reasonable
What's like an actual reasonable
exploration task?
I guess there's there's the learn an
I guess there's there's the learn an
exploration strategy and uh there's the
exploration strategy and uh there's the
does your learning algorithm have an
does your learning algorithm have an
exploration strategy in it regardless of
exploration strategy in it regardless of
reward I
Tough to think what to what extent I
Tough to think what to what extent I
care about these. I guess
RC. today.
I could frame a lot of these as lock
I could frame a lot of these as lock
games, couldn't I?
Think about this.
Depending if I can like phrase these all
Depending if I can like phrase these all
is like passcode guessing games somehow.
This would actually be cool because I
This would actually be cool because I
could potentially test them all at the
could potentially test them all at the
same time, right?
Generalization.
Generalization.
General
I'm trying to think what the way that
I'm trying to think what the way that
you structure something like right
just have the bar
You can observe just have a sequence
You can observe just have a sequence
that you can observe.
First action decides yeah given many
Let me see how this would work. Right?
So if we have something like this
have like
have like
tape
tape
like your clue
your final action decides which one is
your final action decides which one is
given on the first like shade this this
given on the first like shade this this
is essentially
take this action
This is shown on the first
you get any results from IIL. Uh we got
you get any results from IIL. Uh we got
some crazy results but not enough to
some crazy results but not enough to
like go sub it in just right away. So we
like go sub it in just right away. So we
have it working like surprisingly well
have it working like surprisingly well
on some hard environments. It does
on some hard environments. It does
really well on the first chunk of neural
really well on the first chunk of neural
MMO 3. It instas solves uh cartpole and
MMO 3. It instas solves uh cartpole and
pong, but then like it does very poorly.
pong, but then like it does very poorly.
It does better than random on most
It does better than random on most
stuff, but like it does very poorly on
stuff, but like it does very poorly on
breakout and triple triad and a bunch of
breakout and triple triad and a bunch of
other tasks. So,
other tasks. So,
I think that um we definitely have
I think that um we definitely have
learned from this that we need to be
learned from this that we need to be
able to reuse samples, especially the
able to reuse samples, especially the
high value ones,
high value ones,
and that the objective is pretty good.
and that the objective is pretty good.
But it's going to take some time to
But it's going to take some time to
figure out how to integrate that nicely.
figure out how to integrate that nicely.
I'm taking a little break at the moment
I'm taking a little break at the moment
and I'm working on
and I'm working on
um some benchmark environments.
Okay, so the clue can change on each
Okay, so the clue can change on each
time step.
Loop can change on each time stamp.
Passcode is random. Password doesn't
Passcode is random. Password doesn't
change.
that in context learning.
How's in context learning different from
How's in context learning different from
memory?
An application of memory, right?
You pushed the code somewhere so I can
You pushed the code somewhere so I can
check that out. Uh the slightly old
check that out. Uh the slightly old
version is on I believe the imitate
version is on I believe the imitate
branch
branch
and I will push this stuff today as
and I will push this stuff today as
well.
just maintain my train of thought a
just maintain my train of thought a
little bit here. Uh, this segment of the
little bit here. Uh, this segment of the
stream is probably fairly boring, but
stream is probably fairly boring, but
it's very important because I'm like I'm
it's very important because I'm like I'm
trying to think about uh simple test
trying to think about uh simple test
environments for specific things.
environments for specific things.
I'd like to have some more of these.
I'd like to have some more of these.
basically an a puffer ocean sanity uh V2
basically an a puffer ocean sanity uh V2
Hill.
It's in puffer lib.
It's in puffer lib.
It's just a branch. Imitate branch.
I'll push all the new stuff. I added in
I'll push all the new stuff. I added in
like an expert model so that you can
like an expert model so that you can
like load in an expert if you want and
like load in an expert if you want and
like test imitation on that to see that
like test imitation on that to see that
mostly works.
You hear?
problem.
problem.
Yeah,
Yeah,
funny
just solved everything. I don't know.
Um, yeah, that's kind of obnoxious to
Um, yeah, that's kind of obnoxious to
do. Bet.
Well, one, it's pedantic, and two, I
Well, one, it's pedantic, and two, I
guarantee you a bunch of people aren't
guarantee you a bunch of people aren't
going to want to like
going to want to like
have to prepare an oral defense of their
have to prepare an oral defense of their
code on like live on the I am.
That is mostly how an oral defense
That is mostly how an oral defense
works.
help me figure out how to do these
help me figure out how to do these
environments instead.
Um,
there are several tricky things here,
there are several tricky things here,
right?
Not quite exploration of the same bloody
Not quite exploration of the same bloody
sequence of actions is always correct.
Well, it's not quite
the memory and credit assignment ones
the memory and credit assignment ones
are solid.
are solid.
I think the sparsity one is good.
I think the sparsity one is good.
Why rust and the devs are fighting on X?
Why rust and the devs are fighting on X?
They have nothing better to do.
can go grab some of the C devs and they
can go grab some of the C devs and they
can come work on this if they know what
can come work on this if they know what
they're
The pros of Rust are that you feel
The pros of Rust are that you feel
smart. Cons are that you're not. The
smart. Cons are that you're not. The
pros of C are that uh you are smart. The
pros of C are that uh you are smart. The
cons are you don't feel smart. Take your
cons are you don't feel smart. Take your
pick.
Totally not bait.
the exploration task. Oh,
Yeah, I'm trying to think if there's a
Yeah, I'm trying to think if there's a
better way of doing any of this stuff.
See this is still not memory
See this is still not memory
actually. Your final action decides
actually. Your final action decides
reward. Which one to take is given on
reward. Which one to take is given on
the first step.
the first step.
You still have to do credit assignment,
You still have to do credit assignment,
don't you?
Or association.
You still have to do association.
Am I wrong? Bet.
Am I wrong? Bet.
The credit assignment one works. Your
The credit assignment one works. Your
first action decides reward which is
first action decides reward which is
then given many steps later.
The credit assignment task is correct.
The credit assignment task is correct.
The memory task seems wrong to me.
Why the no op? What do you mean?
So the memory game is supposed to be I
So the memory game is supposed to be I
tell you what like what action to take
tell you what like what action to take
on the last step and I tell you it on
on the last step and I tell you it on
the first step and nothing else matters.
the first step and nothing else matters.
The problem though is that you still
The problem though is that you still
have to understand the instruction. So
have to understand the instruction. So
like if you don't understand the
like if you don't understand the
instruction that's not a pure test of
instruction that's not a pure test of
memory. So it doesn't work. tabularasa.
Yeah, we're using RL Well,
minimal tasks I think are more useful
minimal tasks I think are more useful
for diagnosing like specific
for diagnosing like specific
architecture deficiencies. It's like I
architecture deficiencies. It's like I
have a memory one that's linked to
have a memory one that's linked to
credit assignment that's kind of decent
credit assignment that's kind of decent
cuz it's like a fundamental check.
cuz it's like a fundamental check.
I don't know if the other ones are even
I don't know if the other ones are even
good.
I kind of like this recipe game
reset has several rounds.
reset has several rounds.
You can increase sparsity by just
You can increase sparsity by just
changing the number of ingredients.
The only other thing we haven't done is
The only other thing we haven't done is
like control, I guess, right?
You don't want to introduce a control
You don't want to introduce a control
problem to this.
problem to this.
Doesn't this do everything but control?
Doesn't this do everything but control?
Actually, let's see. Is there anything
Actually, let's see. Is there anything
that is in any of these M that is not
that is in any of these M that is not
control that is not covered by this? So,
control that is not covered by this? So,
what does neural MMO require you to do?
what does neural MMO require you to do?
You have to explore. There's sparcity.
You have to explore. There's sparcity.
You have to learn to fight agents.
You have to learn to fight agents.
They're like associations that go over
They're like associations that go over
time. I suppose
the associations over time part.
Is this it?
They tell you the recipe.
Yeah, it's different though.
Oh, I suppose. Yeah, it doesn't have any
Oh, I suppose. Yeah, it doesn't have any
of the multi-agent in it yet, right?
So this is not
Near MMO has some of these actions are
Near MMO has some of these actions are
poorly extended.
poorly extended.
I suppose the recipes get longer.
Never get that long though, do they?
It's kind of interesting that like
It's kind of interesting that like
making a pure task for this is hard.
Well, of course it's hard, right? We
Well, of course it's hard, right? We
wouldn't need all these tasks
wouldn't need all these tasks
otherwise.
It's completely missing control.
Like honestly, outside of these little
Like honestly, outside of these little
probe environments, I'm actually
probe environments, I'm actually
reconsidering the idea,
reconsidering the idea,
like isn't literally just having these
like isn't literally just having these
different little RL environments better?
different little RL environments better?
These different game environments?
These different game environments?
I still think the recipe environment is
I still think the recipe environment is
on its own pretty reasonable.
like in context learning as well.
Good.
Okay. Well, I like the recipe game. I
Okay. Well, I like the recipe game. I
like the idea of sprucing up some of the
like the idea of sprucing up some of the
individual ocean like sparse like test
individual ocean like sparse like test
environments.
environments.
I have a decent number of ideas here.
I have a decent number of ideas here.
Enough that I could start implementing
Enough that I could start implementing
stuff.
stuff.
It is it's only it's 5:30 still.
It is it's only it's 5:30 still.
Let me go check couple quick things off
Let me go check couple quick things off
to the side.
Okay. Um,
Okay. Um,
want to start on these.
I want to try
also. Do I want to have
separate C environments forities
kind of tired for today. I've thought a
kind of tired for today. I've thought a
lot about a lot of different things
lot about a lot of different things
and done a lot of thinking about all
and done a lot of thinking about all
this stuff.
this stuff.
Um
a quick look at what we have in ocean
a quick look at what we have in ocean
right now.
right now.
Oh, also commit this right.
All right.
Bandit memory multi- aent password
Bandit memory multi- aent password
spaces
spaces
all the environments. Could
modernize uh these up for sure.
More
heat.
There is an alchemy benchmark.
Oh, wait. There is this is a thing.
Well, I mean, this is a good idea
Well, I mean, this is a good idea
implemented in the stupidest possible
implemented in the stupidest possible
way.
Deep mind.
Do this in freaking unity. You got to be
Do this in freaking unity. You got to be
kidding me.
Yep.
Cool.
All right. Well, there's literally a
All right. Well, there's literally a
basis for this thing. How many citations
basis for this thing. How many citations
this paper have? 43.
That's so funny. Wait, Nurup's 2025.
That's so funny. Wait, Nurup's 2025.
This is 2021.
Did they then submit this? Oh, no. This
Did they then submit this? Oh, no. This
is 2021. Okay.
is 2021. Okay.
It's It has the date wrong.
Why is it in the proceedings 2025?
Why is it in the proceedings 2025?
No, it says papers 2021. Okay, it's just
No, it says papers 2021. Okay, it's just
mislabeled. It's not V2. It's just
mislabeled. It's not V2. It's just
mislabeled. Well, of course, nobody ever
mislabeled. Well, of course, nobody ever
used it, right? Like literally nobody
used it, right? Like literally nobody
used it cuz they implemented it in the
used it cuz they implemented it in the
dumbest possible way. Like imagine like
dumbest possible way. Like imagine like
imagine having a good idea and then
imagine having a good idea and then
implementing it as a Unity
implementing it as a Unity
environment. Like are you insane?
I can still take the name Alchemy for
I can still take the name Alchemy for
this.
I find it very funny though that like I
I find it very funny though that like I
come up with the name alchemy for this.
come up with the name alchemy for this.
I Google alchemy RL just to see and then
I Google alchemy RL just to see and then
somebody has a very very similar idea
somebody has a very very similar idea
under the same name already just not
under the same name already just not
executed well.
It's It's more just they don't have good
It's It's more just they don't have good
um I think they have engineers and they
um I think they have engineers and they
have scientists, but like they don't
have scientists, but like they don't
actually think about what the other
actually think about what the other
person needs at all.
annexating the current. Yeah,
it's just kind of stupid if you think
it's just kind of stupid if you think
about it, right?
Next time I'm trying to do is something
Next time I'm trying to do is something
stupid
stupid
genie except for puffer. I'm making
genie except for puffer. I'm making
actions the output.
actions the output.
What
actions space the output?
Yeah, if you're going to use RL to learn
Yeah, if you're going to use RL to learn
a world model, that's a terrible idea.
a world model, that's a terrible idea.
Um, what you would do is you would do
Um, what you would do is you would do
that at the algorithm level. If you want
that at the algorithm level. If you want
to do a world modeling objective, you do
to do a world modeling objective, you do
that at the algorithm level. And you can
that at the algorithm level. And you can
do it on pixels or not pixels.
do it on pixels or not pixels.
World models are not constrained to just
World models are not constrained to just
pixels. They can work on different obs
pixels. They can work on different obs
types.
Ideally though know a lot like sometimes
Ideally though know a lot like sometimes
you do world model objectives not in
you do world model objectives not in
pixel space or in obsp space you do it
pixel space or in obsp space you do it
in embedding space so that you don't
in embedding space so that you don't
have to deal with the reconstruction as
probably
probably
well I mean it depends like if you come
well I mean it depends like if you come
up with some sort of world model thing
up with some sort of world model thing
that's actually generally useful on the
that's actually generally useful on the
algorithm side then yes but that's hard
algorithm side then yes but that's hard
to do that's a very high bar
and just think of like some basic stuff
and just think of like some basic stuff
I'm going to want in here So
I'm going to want in here So
you have
Um
want recipe. What do we want? I want to
want recipe. What do we want? I want to
do this.
make the recipe big array.
No, we can't make the recipes because we
No, we can't make the recipes because we
array because we have to check them,
array because we have to check them,
right?
Check them.
Make a big ND array. Don't you
You'd make the recipes would be a big ND
You'd make the recipes would be a big ND
array.
want to restrict it to two ingredients.
Hello.
Well, we would just do like
Well, we would just do like
We're not going to have a huge number of
We're not going to have a huge number of
different ones, right?
Just do 2D.
Just do 2D.
probably interesting enough, right?
Okay,
this
we have to generate all the recipes,
we have to generate all the recipes,
right?
going to have to come up with a
going to have to come up with a
generation algorithm. I'm sure
this
What's the Maybe we just do this top to
What's the Maybe we just do this top to
bottom like let's start on the
bottom like let's start on the
generation algorithm.
Okay. So,
Okay. So,
know what level we're in.
know what level we're in.
know how many
products There.
do products per level or something as
do products per level or something as
well.
well.
But like it's going to be something to
But like it's going to be something to
the effect of
All
right. And so you just pick two
right. And so you just pick two
ingredients
the product. We have to think through
the product. We have to think through
the dynamics quite a bit of this.
the dynamics quite a bit of this.
Hello. How's it going?
Hello. How's it going?
Hey, Spencer. Yeah, it's um
Hey, Spencer. Yeah, it's um
it's a dedicated test environment for
it's a dedicated test environment for
some things that's kind of complicated,
some things that's kind of complicated,
but I don't know, not that bad.
but I don't know, not that bad.
There's actually kind of precedent for
There's actually kind of precedent for
it as well. There's an old Deep Mind
it as well. There's an old Deep Mind
paper that's kind of similar. I came up
paper that's kind of similar. I came up
with the idea, then I Googled to see if
with the idea, then I Googled to see if
anybody had done it, and there's like
anybody had done it, and there's like
something similar, but of course it's
something similar, but of course it's
like this one. That one's like a 100,000
like this one. That one's like a 100,000
times slower than it should be.
I got some pretty cool results though
I got some pretty cool results though
throughout the day and I'm about to end
throughout the day and I'm about to end
stream because it's sick. So, I'm going
stream because it's sick. So, I'm going
to go get dinner. Um, but yeah, Spencer,
to go get dinner. Um, but yeah, Spencer,
I got a lot of uh interesting results.
I got a lot of uh interesting results.
Cracked out HL G for more
Cracked out HL G for more
eight.
eight.
Um,
maybe we try both.
maybe we try both.
Did it push VF? If it pushed clipping to
Did it push VF? If it pushed clipping to
like really low, then don't I don't want
like really low, then don't I don't want
that one.
that one.
Clipping really low usually means like
Clipping really low usually means like
cracked out, unstable, like completely
cracked out, unstable, like completely
unreproducible.
I want to do like off policy learning
I want to do like off policy learning
stuff tomorrow.
What about the clipping? Oh, yeah.
What about the clipping? Oh, yeah.
You're right. That might be a problem.
You're right. That might be a problem.
Like not clipping the loss somehow.
Yeah.
Yeah.
Well, try the slightly more conservative
Well, try the slightly more conservative
one. Um, we can try both to be fair. I
one. Um, we can try both to be fair. I
just want to run it on the uh the 5090
just want to run it on the uh the 5090
bin max value bins.
bin max value bins.
Yeah, a different kind type of clipping
Yeah, a different kind type of clipping
though. That's that's prediction space
though. That's that's prediction space
clipping. I mean lost base clipping.
clipping. I mean lost base clipping.
Okay.
have that up here one. Cool.
have that up here one. Cool.
Um,
Um,
yeah, I'm going to want to try to play
yeah, I'm going to want to try to play
with that tomorrow. I'm pretty tired at
with that tomorrow. I'm pretty tired at
the moment. So, stuff I did today for
the moment. So, stuff I did today for
record Spencer. Um, so pure imitation
record Spencer. Um, so pure imitation
learning with no reinforcement learning
learning with no reinforcement learning
can get like 67% on tower climb.
can get like 67% on tower climb.
like the same number of steps and
like the same number of steps and
everything.
It also solves the first chunk of neural
It also solves the first chunk of neural
MMO faster than our current RL and it
MMO faster than our current RL and it
solves cart pull instantly and pulling
solves cart pull instantly and pulling
instant.
instant.
It fails hard on a lot of other ends,
It fails hard on a lot of other ends,
even simple ones, though. So, I don't
even simple ones, though. So, I don't
think necessarily this is going to be
think necessarily this is going to be
like, oh yeah, we immediately just use
like, oh yeah, we immediately just use
this. But I think this is pretty strong
this. But I think this is pretty strong
evidence that if we get an off policy
evidence that if we get an off policy
thing or like an auxiliary thing working
thing or like an auxiliary thing working
that uh we can definitely use past data
that uh we can definitely use past data
in a way that's very effective.
It's pretty nutty.
I also found some good references.
I also found some good references.
I have one question to email uh to send
I have one question to email uh to send
via email to someone but um
via email to someone but um
yeah
yeah
suffice to say it's I mean research
suffice to say it's I mean research
progress is always slow in the sense
progress is always slow in the sense
that like no nothing just like works out
that like no nothing just like works out
of the box on everything yet but we got
of the box on everything yet but we got
some cool information from today. It is
some cool information from today. It is
very tiring. I also did a lot of work
very tiring. I also did a lot of work
looking through identity tasks and like
looking through identity tasks and like
B Suite stuff and like how we can
B Suite stuff and like how we can
isolate different tasks and I got some
isolate different tasks and I got some
ideas for that as well as one idea for
ideas for that as well as one idea for
this like combined environment that
this like combined environment that
would be a very good benchmark for uh I
would be a very good benchmark for uh I
think it'll just end up being a very
think it'll just end up being a very
good benchmark overall more than
good benchmark overall more than
anything.
Spencer, I don't know. It's probably
Spencer, I don't know. It's probably
just learning. I don't know if he has
just learning. I don't know if he has
done as action masking or stuff. It
done as action masking or stuff. It
could just be learning invalid actions.
What did this say?
This thing gets to be quadratic is the
This thing gets to be quadratic is the
problem.
All
right. Well, I got to go for dinner,
right. Well, I got to go for dinner,
guys. Um, I'm on a pretty funny schedule
guys. Um, I'm on a pretty funny schedule
these days. Be back in the morning for
these days. Be back in the morning for
tuning into the uh the research stream.
tuning into the uh the research stream.
Tomorrow we will be doing off policy,
Tomorrow we will be doing off policy,
possibly looking at the environment if I
possibly looking at the environment if I
get bored of that. Um, but yeah, lots of
get bored of that. Um, but yeah, lots of
work to done. But if you're interested
work to done. But if you're interested
in my stuff, generally
in my stuff, generally
everything I do is at puffer.ai. I
everything I do is at puffer.ai. I
Somebody please fix these graphics. They
Somebody please fix these graphics. They
should be cars. Um, but yeah, all my
should be cars. Um, but yeah, all my
stuff's at puffer.ai.
stuff's at puffer.ai.
Go the repo. It's free. Helps me out.
Go the repo. It's free. Helps me out.
You want to get involved with research,
You want to get involved with research,
join the Discord. If you want to build
join the Discord. If you want to build
environments, join the Discord. Other
environments, join the Discord. Other
than that, all my stuff is at X here.

Kind: captions
Language: en
It should be live here.
It should be live here.
Hi everyone.
O fix the lighting in here. The uh haze.
O fix the lighting in here. The uh haze.
Not great.
Not great.
And with this desk, I don't have
And with this desk, I don't have
anywhere to stick this big ring light.
I can do this.
Let me see if I can get this to actually
slightly better.
slightly better.
slightly better. There we go. Hello,
slightly better. There we go. Hello,
Andrew.
So, here's the uh the plan for today.
So, here's the uh the plan for today.
I've got my boxes back.
Got a nice 5090 here. Um plan number
Got a nice 5090 here. Um plan number
one,
one,
we're going to do a quick couple
we're going to do a quick couple
environment modifications for Kathy
environment modifications for Kathy
Woo's lab. They want um contextual RL
Woo's lab. They want um contextual RL
problems. Basically, they want to like
problems. Basically, they want to like
take cart pole, be able to adjust the
take cart pole, be able to adjust the
speed of the cart, the length of the
speed of the cart, the length of the
pole, stuff like that. Same thing for
pole, stuff like that. Same thing for
breakout. Just going to do this for a
breakout. Just going to do this for a
couple M's for them. Uh that should be
couple M's for them. Uh that should be
pretty quick. And then we're going to do
pretty quick. And then we're going to do
some more work on the imitation learning
some more work on the imitation learning
algorithm. And I think what I'm going to
algorithm. And I think what I'm going to
do with this is I couldn't figure out
do with this is I couldn't figure out
why breakout was hard. So as a sanity,
why breakout was hard. So as a sanity,
I'm going to use our standard uh 30 to
I'm going to use our standard uh 30 to
train a breakout agent and I'm going to
train a breakout agent and I'm going to
use that as the expert to generate data
use that as the expert to generate data
and see if it trains with IIL via that.
and see if it trains with IIL via that.
And then we can kind of work backwards
And then we can kind of work backwards
from there and we can figure out why it
from there and we can figure out why it
is that uh we can't like bootstrap data
is that uh we can't like bootstrap data
from scratch to do the exact same thing.
from scratch to do the exact same thing.
See how much data it takes, we'll see
See how much data it takes, we'll see
how much training it takes, all that
how much training it takes, all that
type of stuff. And um that should get us
type of stuff. And um that should get us
into a good spot. We had some pretty
into a good spot. We had some pretty
promising results yesterday, but they
promising results yesterday, but they
were mixed. We had some really good
were mixed. We had some really good
results and some really like
results and some really like
surprisingly bad results.
surprisingly bad results.
We'll start on this though.
Okay. So, Carpole doesn't expose
Okay. So, Carpole doesn't expose
any of this crap.
Pull mass length
total mass
total mass
mass cart mass pole.
and then the magnitude of force.
Let me see if I can go find the messages
Let me see if I can go find the messages
seeing what specific variables they
seeing what specific variables they
wanted
and uh we will implement that for them.
and uh we will implement that for them.
We will make sure it trains.
Mass of part. Mass of pole. Length of
Mass of part. Mass of pole. Length of
pole.
pole.
Okay.
Okay.
They actually gave me a full list.
They actually gave me a full list.
Perfect.
But all I have to do
gravity.
first magnifier
and then
update interval.
This is good. Yeah,
a lot of variables,
a lot of variables,
but that's not particularly difficult.
but that's not particularly difficult.
Going to do like this
m speed and the continuous
m speed and the continuous
do
do
mass mass
All
right. What do we think about this? Just
right. What do we think about this? Just
do exact same thing. Yeah.
And that is all of them. Yeah.
possible context
possible context
want to avoid multi- aent environments
new breakout as well
do those two for them right now they can
do those two for them right now they can
play with this.
And both of these essentially are going
And both of these essentially are going
to get
to get
these are going to get nuked, right?
Astro Jess, hello. Don't want to let you
Astro Jess, hello. Don't want to let you
know you're an absolute inspiration. I
know you're an absolute inspiration. I
love efficiency, but can I please ask a
love efficiency, but can I please ask a
super dumb question, genuinely a sincere
super dumb question, genuinely a sincere
I feel like I'm missing something.
I feel like I'm missing something.
Absolutely. Kind of the point of this
Absolutely. Kind of the point of this
stream.
Can ask all sorts of stuff.
All we need to do here is uh use these
All we need to do here is uh use these
instead of the previous one
instead of the previous one
threshold.
threshold.
Why do we not just add max steps in as
Why do we not just add max steps in as
well? Right.
Guess we'll leave this. Well, they we'll
Guess we'll leave this. Well, they we'll
do the ones they asked for.
Puffer is optimizing for speed in terms
Puffer is optimizing for speed in terms
of steps per second.
of steps per second.
Um,
Um,
puffer lib is technically we're
puffer lib is technically we're
optimizing for speed in terms of uh wall
optimizing for speed in terms of uh wall
clock time to train an agent.
clock time to train an agent.
That is the when we run our hyper pram
That is the when we run our hyper pram
sweeps and stuff. That is the metric we
sweeps and stuff. That is the metric we
go for
which has very high correlation with
which has very high correlation with
environment steps per second of course
environment steps per second of course
and training steps per second.
and training steps per second.
There are some caveats on that, right?
There are some caveats on that, right?
Like if I really wanted to, I can make
Like if I really wanted to, I can make
puffer hit 20 million steps per second,
puffer hit 20 million steps per second,
but it won't train any faster in terms
but it won't train any faster in terms
of wall clock. So there's no point.
Where's Bet when you need him?
He's
in the Discord.
in the Discord.
What? Bet. Yeah, of course. Bet's in
What? Bet. Yeah, of course. Bet's in
Discord. Not in the voice or anything.
Discord. Not in the voice or anything.
Right.
Added him in there.
are reward functions for the classic M
are reward functions for the classic M
like cartpull solve problems or are they
like cartpull solve problems or are they
still improving?
still improving?
Um,
it kind of depends what you mean. Like
it kind of depends what you mean. Like
so technically
so technically
you could make the reward function like
you could make the reward function like
an oracle, right? Like if you did like
an oracle, right? Like if you did like
really domain specific engineering, you
really domain specific engineering, you
could make this oracle reward where
could make this oracle reward where
obviously you'll do better.
I don't really look at like is the
I don't really look at like is the
reward function a solved problem or not,
reward function a solved problem or not,
right?
right?
It's more so can you get it to learn
It's more so can you get it to learn
with like a baseline thing that doesn't
with like a baseline thing that doesn't
take a whole bunch of domain specific
take a whole bunch of domain specific
engineering versus like how much do you
engineering versus like how much do you
get out of doing more domain specific
get out of doing more domain specific
engineering. I don't look at like reward
engineering. I don't look at like reward
function as like a research question so
function as like a research question so
much in it of itself. There occasionally
much in it of itself. There occasionally
a couple best practices that come out of
a couple best practices that come out of
stuff but it's not it's really not the
stuff but it's not it's really not the
first thing I think about.
Like there's not really a point in
Like there's not really a point in
improving the reward function of
improving the reward function of
cartpole, right?
Like it's solved in 3 seconds or
Like it's solved in 3 seconds or
whatever right now. So
whatever right now. So
you could say like if you improve the
you could say like if you improve the
reward function and solve it in 2
reward function and solve it in 2
seconds
seconds
that really doesn't let you do anything
that really doesn't let you do anything
you couldn't do before and it doesn't
you couldn't do before and it doesn't
like tell you anything new about the uh
like tell you anything new about the uh
the algorithms. So the only time in my
the algorithms. So the only time in my
mind it makes sense to do heavy heavy
mind it makes sense to do heavy heavy
reward engineering is when you're
reward engineering is when you're
specifically trying to solve a hard
specifically trying to solve a hard
unsolved problem, right? That you
unsolved problem, right? That you
independently care about that problem
independently care about that problem
outside of the research value.
outside of the research value.
I commented out the prints too and also
I commented out the prints too and also
it broke. So yeah, got to figure that
it broke. So yeah, got to figure that
out.
Okay, so Bett's going to come look at
Okay, so Bett's going to come look at
this for us.
7 mil rewards per step.
What is the uh the post you just made up
What is the uh the post you just made up
FBR?
FBR?
Howdy. No rash. When you get a minute,
Howdy. No rash. When you get a minute,
there's a drone PR that's been sitting
there's a drone PR that's been sitting
around for a while. Um I can just merge
around for a while. Um I can just merge
it and trust that you haven't broken
it and trust that you haven't broken
some stuff if that is convenient to you.
some stuff if that is convenient to you.
Did I forget to do RK4?
And like does it still train?
I have a whole bunch to merge. It looks
I have a whole bunch to merge. It looks
like I also have a bunch of work to do
like I also have a bunch of work to do
on stuff today. I'll merge yours.
on stuff today. I'll merge yours.
Oh yeah, this is the drone RK4.
Okay. So, I think you PR into drone
Okay. So, I think you PR into drone
race.
I'm going to have to do some work, I
I'm going to have to do some work, I
think, to get this into drone swarm,
think, to get this into drone swarm,
right?
right?
If it's Oh, you did it in drone swarm as
If it's Oh, you did it in drone swarm as
well, actually.
well, actually.
And this still trains.
There was a big one that like took me a
There was a big one that like took me a
whole bunch of time. The last one I
whole bunch of time. The last one I
merged in I think was my error, not
merged in I think was my error, not
yours.
Okay,
I will merge the big PR and just hope
I will merge the big PR and just hope
that uh
that uh
hope that I don't have anything screwy.
hope that I don't have anything screwy.
I'm going to have to read through drone
I'm going to have to read through drone
stuff. Ideally once you get like either
stuff. Ideally once you get like either
you get something on the current drone
you get something on the current drone
or you send me like new drone and you
or you send me like new drone and you
can get something on that I will go
can get something on that I will go
spend a bunch of time helping on uh on
spend a bunch of time helping on uh on
that side like to get drones working
that side like to get drones working
kind of doing research in the move in
kind of doing research in the move in
the meantime. Thank you though that like
the meantime. Thank you though that like
our having RK4 is really useful cuz like
our having RK4 is really useful cuz like
we'll be able to like grab this and use
we'll be able to like grab this and use
it for other stuff as well.
Okay. My learning is mostly from Grock.
Okay. My learning is mostly from Grock.
Uh oh. Major bottleneck like board
Uh oh. Major bottleneck like board
passes.
passes.
Wait said something like the major
Wait said something like the major
bottleneck like the passes.
bottleneck like the passes.
That doesn't make sense on its own
That doesn't make sense on its own
because when I say millions of steps per
because when I say millions of steps per
second, that is end to end training
second, that is end to end training
time. Our environments are much faster
time. Our environments are much faster
than millions of steps per second,
than millions of steps per second,
right? It's millions of steps per second
right? It's millions of steps per second
end to end training time
end to end training time
on one GPU.
What? What did you do here, bat? Don't
What? What did you do here, bat? Don't
ever do that.
ever do that.
Okay,
that's crazy. Exactly.
that's crazy. Exactly.
That's puffer.
I mean, literally, if you just like if
I mean, literally, if you just like if
you just go play around with the
you just go play around with the
library, you'll see right like on a good
library, you'll see right like on a good
GPU, you train breakout in 25 seconds.
The task is just solved in 25 seconds.
There are other complaints that you can
There are other complaints that you can
make about the way that we've done
make about the way that we've done
certain things, but like ultimately
certain things, but like ultimately
they'd have to be pretty petty.
they'd have to be pretty petty.
It's kind of what you see is what you
It's kind of what you see is what you
get.
How much time have you spent on helping
How much time have you spent on helping
Yaxian with his super secret? Really not
Yaxian with his super secret? Really not
that much. I've had a couple
that much. I've had a couple
conversations with him. I mean, the
conversations with him. I mean, the
thing is like we literally have a drone
thing is like we literally have a drone
environment to pull from. So if you just
environment to pull from. So if you just
hack on that then like the RL side of
hack on that then like the RL side of
that is already very good
that is already very good
and then it kind of becomes a hardware
and then it kind of becomes a hardware
project right.
So basically like you can look at what
So basically like you can look at what
we have here is like puffer lilib just
we have here is like puffer lilib just
takes one portion of that project that
takes one portion of that project that
would be otherwise completely unfeasible
would be otherwise completely unfeasible
and it just like makes that side solved.
Uh, hang on. Did I mess up total mass?
training poly time touring machine to
training poly time touring machine to
solve. If that actually works, I'll be
solve. If that actually works, I'll be
very impressed. Actually, uh you should
very impressed. Actually, uh you should
look at Weston, the Weston in the
look at Weston, the Weston in the
Discord. It's a high school student who
Discord. It's a high school student who
made Sudoku and he says he has it
made Sudoku and he says he has it
working.
working.
uh with puffer.
I'll be very impressed if you get
I'll be very impressed if you get
freaking Sudoku to be solved with a
freaking Sudoku to be solved with a
touring machine. That'd be crazy.
Not the M to work. That's crazy.
Risk of sounding s shall and
Risk of sounding s shall and
capitalistic
capitalistic
create a lot of complex environments in
create a lot of complex environments in
puffer
puffer
have huge commercial upside. It's not
have huge commercial upside. It's not
shallow and like capitalistic. That's
shallow and like capitalistic. That's
like yeah there's business and we do all
like yeah there's business and we do all
the science stuff for free and we make
the science stuff for free and we make
all these environments for free but that
all these environments for free but that
is what we do on the business side right
is what we do on the business side right
is like companies pay us to either make
is like companies pay us to either make
them environments or make RL work on
them environments or make RL work on
their environments or make their
their environments or make their
environments fast that is the business
environments fast that is the business
model
model
because all of our tools are free we
because all of our tools are free we
don't charge for those right
don't charge for those right
so literally it's only the things that
so literally it's only the things that
uh really wouldn't have any value to
uh really wouldn't have any value to
general research and science that we
general research and science that we
monetize.
I do that at great personal cost.
Like if this exact same library were
Like if this exact same library were
closed source, very easily sell
closed source, very easily sell
expensive licenses.
expensive licenses.
So that's pretty much the only way we
So that's pretty much the only way we
monetize.
In fact, I can sort two arrays
In fact, I can sort two arrays
simultaneously.
simultaneously.
Huh, that's pretty awesome. I will be
Huh, that's pretty awesome. I will be
very interested to see that uh the
very interested to see that uh the
results of that when I eventually get
results of that when I eventually get
the touring machine.
the touring machine.
It'll be a cool thing.
I wouldn't have expected that to work
I wouldn't have expected that to work
honestly.
honestly.
probably took a lot of a lot of uh
probably took a lot of a lot of uh
effort to make that work.
That works. Good job. Like fair play to
That works. Good job. Like fair play to
you.
I don't really consider it charity so
I don't really consider it charity so
much.
much.
I mean, it's advancing science.
I mean, it's advancing science.
If you consider all science work
If you consider all science work
charity, then sure. It's just in my
charity, then sure. It's just in my
mind, it's like my own goal is to
mind, it's like my own goal is to
advance science and also make money. But
advance science and also make money. But
like I it doesn't make any sense for me
like I it doesn't make any sense for me
to go make money and do all the science
to go make money and do all the science
behind closed doors. It likewise doesn't
behind closed doors. It likewise doesn't
make sense for me to like just do all
make sense for me to like just do all
science and not make any money, right?
science and not make any money, right?
But they have to fit together in a way
But they have to fit together in a way
that's clean. Not in a way where you're
that's clean. Not in a way where you're
like, you know, making money at the
like, you know, making money at the
expense of science or spending all your
expense of science or spending all your
time doing science at the expense of not
time doing science at the expense of not
actually growing the project ever and
actually growing the project ever and
thereby not giving anybody who's
thereby not giving anybody who's
involved any sort of financial
involved any sort of financial
opportunities out of it.
I feel way way better this week as well.
I feel way way better this week as well.
Like mentally, I have so so much more
Like mentally, I have so so much more
clarity. I'll tell you what my routine
clarity. I'll tell you what my routine
has been uh yesterday and today.
has been uh yesterday and today.
So, I get up pretty early. I go I run a
So, I get up pretty early. I go I run a
10K around the dish.
10K around the dish.
It's like a good, you know, a solid
It's like a good, you know, a solid
solid hilly course. I come back. back. I
solid hilly course. I come back. back. I
lift some weights. I go walk down. I get
lift some weights. I go walk down. I get
myself some breakfast. Uh I go through
myself some breakfast. Uh I go through
messages and stuff and then I just work
messages and stuff and then I just work
on this. It's like
on this. It's like
there's so much freaking noise with
there's so much freaking noise with
everything else. Having that mental
everything else. Having that mental
clarity is so so important. Like I now
clarity is so so important. Like I now
this week I know exactly what to work
this week I know exactly what to work
on. I have like thought through exactly
on. I have like thought through exactly
how and like why everything needs to be
how and like why everything needs to be
done.
done.
kind of just in a good spot
kind of just in a good spot
for all.
I honestly don't even think I'm losing
I honestly don't even think I'm losing
any sort of productivity by spending
any sort of productivity by spending
that amount of time doing exercise in
that amount of time doing exercise in
the morning. like I feel so so so much
the morning. like I feel so so so much
better.
I don't waste time working on dumb
I don't waste time working on dumb
stuff. The result
did it just add that crap.
Welcome, Kvert. I see you have a whole
Welcome, Kvert. I see you have a whole
bunch of PRs. I will get to them.
bunch of PRs. I will get to them.
Going through my laundry list of things
Going through my laundry list of things
to do.
working on some of it. Anyways, about
working on some of it. Anyways, about
this new sparse learning. Oh, the
this new sparse learning. Oh, the
imitation thing. Um, I will be working
imitation thing. Um, I will be working
on that soon. Later today, as soon as I
on that soon. Later today, as soon as I
finish this, uh, I'm adding a couple
finish this, uh, I'm adding a couple
changes to Puffer for Kathy Woo's group.
Oh, imitation learning. Yes, that is
Oh, imitation learning. Yes, that is
what it is called. Uh, it's not You're
what it is called. Uh, it's not You're
not going to find good literature though
not going to find good literature though
because the thing I'm doing is weird.
the heck.
Oh,
watch your branch. The thing is I'm
watch your branch. The thing is I'm
doing imitation learning, but there's no
doing imitation learning, but there's no
expert data. Does that make sense?
expert data. Does that make sense?
I'm using imitation learning algorithms
I'm using imitation learning algorithms
with data that is not derived from
with data that is not derived from
expert
expert
There's some details for how that works.
There's some details for how that works.
Let me go message them.
Wait, why is FBR linking me?
Huh?
Sam and I have been messing around with
Sam and I have been messing around with
using inverse RL with no expert and got
using inverse RL with no expert and got
some results. Inverse RL with no expert.
some results. Inverse RL with no expert.
Why would you do inverse RL?
Why would you do inverse RL?
Like learning the reward function,
Like learning the reward function,
right?
right?
You have the reward function, don't you?
I would think that you have the reward
I would think that you have the reward
function.
Why is the Why does breakout have hyper
Why is the Why does breakout have hyper
Paul on it?
Bars to dense.
Okay, interesting. Well, if you guys
Okay, interesting. Well, if you guys
want to do research side stuff on
want to do research side stuff on
puffer, like there's definitely room for
puffer, like there's definitely room for
that as well.
that as well.
So, I would suggest the uh initial
So, I would suggest the uh initial
contract stuff will be likely to come
contract stuff will be likely to come
from drone. Um research side, I'm trying
from drone. Um research side, I'm trying
to think how the heck that makes any
to think how the heck that makes any
sense.
sense.
They're trying to label the data.
They're trying to label the data.
I'd have to see the details of that.
I'd have to see the details of that.
That's kind of wonky.
The imitation with no expert makes more
The imitation with no expert makes more
sense to me cuz you can just do like a
sense to me cuz you can just do like a
best event type thing or like a top K.
So, there's actually something to
So, there's actually something to
bootstrap. I don't know what you're
bootstrap. I don't know what you're
bootstrapping in uh the inverse RL
bootstrapping in uh the inverse RL
setting.
work with some tricks. H
work with some tricks. H
well, if you have cool stuff there,
well, if you have cool stuff there,
you know the research like agenda here
you know the research like agenda here
in Puffer, right? like we try stuff on
in Puffer, right? like we try stuff on
everything and if you have convincing
everything and if you have convincing
results
results
that would be uh
that would be uh
the type of thing that could make a a
the type of thing that could make a a
major puffer release. Yeah.
See what variables they gave me.
No gravity.
Is it possible for an RL newbie like me
Is it possible for an RL newbie like me
to add any value
to add any value
value anyway in a short time horizon? I
value anyway in a short time horizon? I
mean, this is how most of the people
mean, this is how most of the people
here got started, right? I've literally
here got started, right? I've literally
made like I've made a guide that is I
made like I've made a guide that is I
guarantee you the quickest way possible
guarantee you the quickest way possible
to get into reinforcement learning. It's
to get into reinforcement learning. It's
not trivial. If you have it depends on
not trivial. If you have it depends on
your programming and math background,
your programming and math background,
mostly programming background for how
mostly programming background for how
long it will take you. I have two
long it will take you. I have two
guides, right? I've got my advice for
guides, right? I've got my advice for
programming in ML. I've got my
programming in ML. I've got my
opinionated guide, really ultra
opinionated guide, really ultra
opinionated guide to reinforcement
opinionated guide to reinforcement
learning. If you read this and actually
learning. If you read this and actually
follow this and know what is going on
follow this and know what is going on
here, you will be a very capable
here, you will be a very capable
contributor to RL. It's just a matter of
contributor to RL. It's just a matter of
how difficult it is for you to do so,
how difficult it is for you to do so,
whether you have the time to do it.
weird.
Going to have to go through this in a
Going to have to go through this in a
little bit more detail. I think
little bit more detail. I think
it's hardcoded.
Let's
turn with DQ install things.
No, like vanilla DQN is a useless
No, like vanilla DQN is a useless
algorithm. It doesn't do anything. So
algorithm. It doesn't do anything. So
there are off policy methods um which
there are off policy methods um which
are like DQN with a whole boatload of
are like DQN with a whole boatload of
like things bolted onto it that seem to
like things bolted onto it that seem to
work pretty well, but like the evidence
work pretty well, but like the evidence
is very mixed and a lot of the research
is very mixed and a lot of the research
is not super high quality. And I just
is not super high quality. And I just
spent a week talking to a bunch of deep
spent a week talking to a bunch of deep
mind researchers who worked on these
mind researchers who worked on these
papers and I got like I got some good
papers and I got like I got some good
information on promising directions
information on promising directions
there and that is the thing that I'm
there and that is the thing that I'm
going to be working on later today after
going to be working on later today after
this. It's not quite the same flavor of
this. It's not quite the same flavor of
thing but it's like a similar
thing but it's like a similar
motivation.
I will say it's definitely there is no
I will say it's definitely there is no
there is not clear evidence that like oh
there is not clear evidence that like oh
yeah off policy is just better or
yeah off policy is just better or
anything like that. In fact, most of
anything like that. In fact, most of
it's kind of the opposite. And like off
it's kind of the opposite. And like off
policy has a lot of problems with
I really should go through a lot of
I really should go through a lot of
these like environments we use a lot and
these like environments we use a lot and
just clean up code. This is kind of
just clean up code. This is kind of
embarrassing to have
embarrassing to have
to have some of these be such a mess.
to have some of these be such a mess.
Just clean like pure refactor clean up
Just clean like pure refactor clean up
that
I mean, it kind of is what it is, right?
I mean, it kind of is what it is, right?
Like the M's are written by contributors
Like the M's are written by contributors
of very different skill levels
of very different skill levels
and like the M's are independently
and like the M's are independently
useful in research anyways, but the ones
useful in research anyways, but the ones
we use a lot. I'd like the code to be
we use a lot. I'd like the code to be
just very clean.
I honestly think being a good
I honestly think being a good
programmer, like a very good programmer,
programmer, like a very good programmer,
is harder than being a very good AI
is harder than being a very good AI
researcher.
I don't know why you would have used
I don't know why you would have used
breakout as the template when I have
breakout as the template when I have
like I've linked you other environments
like I've linked you other environments
that are cleaner. Right?
This is why I made the template
This is why I made the template
environment and like target and I I made
environment and like target and I I made
all the other ones like plus even snake
all the other ones like plus even snake
is decently clean.
is decently clean.
I have all these other environments
I have all these other environments
there so that you can see what like a
there so that you can see what like a
clean environment would look like.
clean environment would look like.
My first day, man, that's all right. But
My first day, man, that's all right. But
that is how you eventually get better,
that is how you eventually get better,
right? Like if you go look at I think
right? Like if you go look at I think
Spencer's first environment was Triple
Spencer's first environment was Triple
Triad. If you go look at the horri like
Triad. If you go look at the horri like
like the horrendous code that that is
like the horrendous code that that is
versus the awesome stuff he's building
versus the awesome stuff he's building
now, it's like night and day.
now, it's like night and day.
What the hell even is this? Right? Like
What the hell even is this? Right? Like
like literally what am I looking at with
like literally what am I looking at with
this?
this?
And it's like, oh, it's just formatting
And it's like, oh, it's just formatting
code. No, it's important. Like don't
code. No, it's important. Like don't
make your stuff a freaking mess. Like I
make your stuff a freaking mess. Like I
don't even need to be following a style
don't even need to be following a style
guide to like
see that obviously
see that obviously
this is better than whatever was there
this is better than whatever was there
before.
Heck, you can even do this.
Heck, you can even do this.
It's a little long, but it's fine.
Hello Desh welcome.
Oh, we already have ball whip.
watching this mostly in the background.
Yeah, no worries.
Yeah, no worries.
You're not required to literally spend
You're not required to literally spend
all day watching me work.
Yes. All puffer contributors must be
Yes. All puffer contributors must be
glued to all streams. All like 40 hours
glued to all streams. All like 40 hours
a week of them or however many I do.
It'll probably end up being about 40 a
It'll probably end up being about 40 a
week.
The next block is likely to be um
The next block is likely to be um
focused but not crazy long hours if that
focused but not crazy long hours if that
makes sense because it's mostly
makes sense because it's mostly
algorithm.
algorithm.
Miss sending you a message.
Miss sending you a message.
Oh.
Oh,
Oh,
like what?
like what?
I am sorry I can't watch you all day.
I am sorry I can't watch you all day.
You'd be surprised the number of people
You'd be surprised the number of people
uh the number of people that like said
uh the number of people that like said
that they knew me from the stream at the
that they knew me from the stream at the
conference.
I met a Kovac.
I met a Kovac.
I've seen in here a few times.
link in discord for breakout and I can
link in discord for breakout and I can
apply
apply
change tracer. Oh, this I'm not really.
change tracer. Oh, this I'm not really.
So what I'm doing here, this is like
So what I'm doing here, this is like
exposing variables because Kathy Woo's
exposing variables because Kathy Woo's
group wants to use this as like a
group wants to use this as like a
contextual RL problem. A full refactor
contextual RL problem. A full refactor
of this would take me a lot more time.
of this would take me a lot more time.
Like that could be a full day, like an
Like that could be a full day, like an
all day project, frankly. I'm not doing
all day project, frankly. I'm not doing
a full refactor now. I'm just exposing a
a full refactor now. I'm just exposing a
bunch of variables and making sure I
bunch of variables and making sure I
don't break anything. And then we're
don't break anything. And then we're
going to do imitation learning today.
Yeah, that's the idea.
Such a freaking mess, you know.
Did I do this? I don't think I would
Did I do this? I don't think I would
have. Maybe I was on like some jank
have. Maybe I was on like some jank
editor.
editor.
Better at least.
What? What did I do here?
What? What did I do here?
Expected.
Expected.
Oh, I don't know why it does this, but
Oh, I don't know why it does this, but
um my editor,
um my editor,
it likes to add a G character to the
it likes to add a G character to the
start of files for some reason. And like
start of files for some reason. And like
it replaces
it replaces
weird
off policy could help getting good
off policy could help getting good
initializ initialization.
Um,
Um,
you'll actually like what it is that
you'll actually like what it is that
we're going to do after this then
we're going to do after this then
because I think I have a just a better
because I think I have a just a better
form of that.
I think I have just a better form of
I think I have just a better form of
that.
that.
So the problem is, as far as I'm aware
So the problem is, as far as I'm aware
and as far as the people I've talked to
and as far as the people I've talked to
have been concerned, it seems is like
have been concerned, it seems is like
off policy is kind of a lie. Like the
off policy is kind of a lie. Like the
idea behind off policy is you can just
idea behind off policy is you can just
infinitely reuse your data.
infinitely reuse your data.
And it doesn't seem like that actually
And it doesn't seem like that actually
works.
Okay, so this runs exactly the same as
Okay, so this runs exactly the same as
before.
And that trains perfect.
And that trains perfect.
Super fast. Still 25 seconds
in today's stream. Yes, it will be right
in today's stream. Yes, it will be right
after this. I'm committing this use a
after this. I'm committing this use a
restroom and then we're going to start
restroom and then we're going to start
on the imitation stuff and it's it's
on the imitation stuff and it's it's
like a better it's a weird form of
like a better it's a weird form of
imitation learning that's kind of a
imitation learning that's kind of a
substitute for off off policy. You'll
substitute for off off policy. You'll
see and it doesn't use expert data
see and it doesn't use expert data
though we are going to get some expert
though we are going to get some expert
data as a baseline but it doesn't need
data as a baseline but it doesn't need
expert data.
throwing my poorly constructed thoughts
throwing my poorly constructed thoughts
in here
in here
from light controls. What
a uh message might have eaten something
a uh message might have eaten something
there and it doesn't make sense.
flight control certification back
flight control certification back
where inter Oh, okay. I see.
where inter Oh, okay. I see.
Um, you can't like
Um, you can't like
Yeah, you just can't
Yeah, you just can't
like like deep learning as a whole is
like like deep learning as a whole is
not interpretable at all. Um, it's like
not interpretable at all. Um, it's like
asking how can we get the like it's not
asking how can we get the like it's not
the same and it's a really dumb analog
the same and it's a really dumb analog
to say this, but it's the best one we
to say this, but it's the best one we
have. It's kind of like asking to make a
have. It's kind of like asking to make a
person fully interpretable,
which it's just not like that's just not
which it's just not like that's just not
a thing.
No.
Okay, so this is the imitation thing
Okay, so this is the imitation thing
that I have, right? Let me show this off
that I have, right? Let me show this off
and then I'm going to take two minutes
and then I'm going to take two minutes
and then we'll work on it.
and then we'll work on it.
But we'll kind of show you guys the uh
But we'll kind of show you guys the uh
initial thing first. So
initial thing first. So
here's the train function. You can see
here's the train function. You can see
that all of the logic here that's like
that all of the logic here that's like
the normal RL is commented out. Okay.
the normal RL is commented out. Okay.
And then you have this super short
And then you have this super short
imitation learning loop. Okay. Not that
imitation learning loop. Okay. Not that
short, but it's literally just imitation
short, but it's literally just imitation
loss with like entropy coefficient.
loss with like entropy coefficient.
Okay.
Okay.
So there's not actually any RL happening
So there's not actually any RL happening
in here.
Hang on.
Got to rebuild the ends and then we'll
Got to rebuild the ends and then we'll
show you.
There you go. So
There you go. So
solves cart pole just like before,
solves cart pole just like before,
right?
start solving pong. If we train that for
start solving pong. If we train that for
like an additional second or two, that
like an additional second or two, that
would full solve pong.
would full solve pong.
All right.
If we train breakout, it doesn't
If we train breakout, it doesn't
it doesn't remotely solve it, but it
it doesn't remotely solve it, but it
does substantially better than random.
And also
And also
the thing that's kind of crazy about
the thing that's kind of crazy about
this
this
is that it actually does really really
is that it actually does really really
well at the first part of neural MMO 3.
well at the first part of neural MMO 3.
Like way better than RL does.
Like way better than RL does.
You're talking about hard initial early
You're talking about hard initial early
exploration. The question here is like
exploration. The question here is like
why does it work on some problems even
why does it work on some problems even
some of which are pretty hard and not on
some of which are pretty hard and not on
others? So, what we're going to do
others? So, what we're going to do
is we're going to train a breakout agent
is we're going to train a breakout agent
with like puffer 30 RL and we're going
with like puffer 30 RL and we're going
to see how much data you need from an
to see how much data you need from an
expert to IIL and we're going to use
expert to IIL and we're going to use
this to assess what's going wrong with
this to assess what's going wrong with
this
this
sent pull request. I will look at that.
sent pull request. I will look at that.
I want to do imitation learning stuff
I want to do imitation learning stuff
right now while I'm in the mindset for
right now while I'm in the mindset for
it. But I am very excited to see how big
it. But I am very excited to see how big
is this. I will look at how big this is.
is this. I will look at how big this is.
Okay, that's about what you'd expect.
Okay, that's about what you'd expect.
It's kind of going to be chunky, right?
Oh jeez. Why did you have to do all this
Oh jeez. Why did you have to do all this
pie object crap?
pie object crap?
Wait, we literally have a binding thing
Wait, we literally have a binding thing
so that you don't have to mess with
so that you don't have to mess with
this, don't we?
Holy.
Holy.
All right. And then thousandish lines.
All right. And then thousandish lines.
Wait, why is it all in the C?
Wait, how is this all in the C, man? Cuz
Wait, how is this all in the C, man? Cuz
the C doesn't get imported, right?
This for testing. Okay. You were
This for testing. Okay. You were
training it though, so somehow you were
training it though, so somehow you were
training it. Wouldn't it have to be in
training it. Wouldn't it have to be in
theh for that? Because theh is what's
theh for that? Because theh is what's
gets included by um by puffer.
How much of this is GPT?
I get suspicious when I see that number
I get suspicious when I see that number
of comments.
20% claw. Okay, that is an acceptable
20% claw. Okay, that is an acceptable
percentage.
I have to ask because when I get mad
I have to ask because when I get mad
when people ask me to like review things
when people ask me to like review things
that are like 80% just AI generated,
that are like 80% just AI generated,
right?
Funny. I just I never trust um I never
Funny. I just I never trust um I never
trust these models to touch my code.
trust these models to touch my code.
They just cause so many problems.
I mean this is about what I would expect
I mean this is about what I would expect
though. This is like just straight up
though. This is like just straight up
yeah you implement this the
yeah you implement this the
singlepurpose physics in about the
singlepurpose physics in about the
amount of code that I expected it would
amount of code that I expected it would
be. If you take the comments out,
be. If you take the comments out,
actually it's probably like the thousand
actually it's probably like the thousand
lines is what I said it would be about a
lines is what I said it would be about a
thousand lines.
Okay. So,
Okay. So,
get this thing. I I don't understand how
get this thing. I I don't understand how
that this is even training if all the
that this is even training if all the
codes in the C. Um because you did send
codes in the C. Um because you did send
me training demos. like get whatever the
me training demos. like get whatever the
like the final cleaned up version is.
like the final cleaned up version is.
Get some cool experiments in. uh comment
Get some cool experiments in. uh comment
on the PR like comment a link to either
on the PR like comment a link to either
W to be or Neptune or one of those so I
W to be or Neptune or one of those so I
can review some of the train curves and
can review some of the train curves and
like add a couple images or a couple
like add a couple images or a couple
gifts or videos or whatever and then uh
gifts or videos or whatever and then uh
we will set up a time this week and I
we will set up a time this week and I
will give you a full review of this
will give you a full review of this
because uh well you can decide when
because uh well you can decide when
basically you can decide how much more
basically you can decide how much more
time you want to spend on getting better
time you want to spend on getting better
results or whatnot first because this is
results or whatnot first because this is
the type of thing where like if you can
the type of thing where like if you can
actually get a sixderee of freedom arm
actually get a sixderee of freedom arm
to work really well in a way that is
to work really well in a way that is
very robust. This is like direct link
very robust. This is like direct link
into contracts with us
into contracts with us
but it depends fully on uh how good the
but it depends fully on uh how good the
actual implementation is and on
actual implementation is and on
follow-up stuff
references for the math as well. Well,
references for the math as well. Well,
where did you get the math from if you
where did you get the math from if you
don't have references?
Like where did you get the Huh?
Oh, I'm sorry. That was
Oh, I'm sorry. That was
I read those is the same name. I just
I read those is the same name. I just
looked over. Okay, that's Finn asking
looked over. Okay, that's Finn asking
short name that starts with F. All
short name that starts with F. All
right, cool. Um, well, you know what?
right, cool. Um, well, you know what?
Then you know the the basics. I mean,
Then you know the the basics. I mean,
you know the procedure at this point.
you know the procedure at this point.
I'm going to use the restroom pretty
I'm going to use the restroom pretty
quick and then I'll be back in a couple
quick and then I'll be back in a couple
minutes. Grab myself a drink and uh we
minutes. Grab myself a drink and uh we
will start on the imitation learning
will start on the imitation learning
segment for the day. Be right back.
All
right.
First things first here.
We got to train ourselves a baseline.
Might just have to play with the manny
Might just have to play with the manny
scale pole.
scale pole.
You can ask me stuff if you need cuz
You can ask me stuff if you need cuz
like there was a line that you have to
like there was a line that you have to
modify. I can probably help you find it.
So this is fine. And now what we do is
have to not got my here.
die token.
I can't figure it out. I probably won't
I can't figure it out. I probably won't
be. Yeah. The thing that's difficult
be. Yeah. The thing that's difficult
with Manny skill, so I played with it
with Manny skill, so I played with it
for a couple days and I still want to do
for a couple days and I still want to do
stuff cuz stone's awesome, but like I
stuff cuz stone's awesome, but like I
couldn't find decent settings to run it
couldn't find decent settings to run it
fast enough to make it like to really be
fast enough to make it like to really be
able to do much, right? When you're
able to do much, right? When you're
training 50k steps per second, it's just
training 50k steps per second, it's just
not a good spot to be in with RL, which
not a good spot to be in with RL, which
is why like doing our own armor or
is why like doing our own armor or
something like that would probably be,
something like that would probably be,
as hard as that is, probably more
as hard as that is, probably more
reasonable just cuz you need to be able
reasonable just cuz you need to be able
to train at a decent speed to do
to train at a decent speed to do
anything base.
like tens and tens of thousands of lines
like tens and tens of thousands of lines
of code across like several different
of code across like several different
frameworks to make anything worthwhile.
Okay. Good.
Okay. Good.
Yes.
Now,
how are we going to use the expert
cuz like we need to still know how well
We need to still know how well um our
We need to still know how well um our
agent doing.
agent doing.
Congrats on RLC. Hey Aar, thank you.
Congrats on RLC. Hey Aar, thank you.
We're doing all sorts of crazy stuff on
We're doing all sorts of crazy stuff on
Algo side now.
Algo side now.
Yeah, I put the talk up on X and YouTube
Yeah, I put the talk up on X and YouTube
and I've got to archive the paper, but
and I've got to archive the paper, but
I'll have the uh paper on archive soon.
Working on experimental research for
Working on experimental research for
potentially puffer 4
potentially puffer 4
the moment.
Why aren't you at Why weren't you at
Why aren't you at Why weren't you at
RLC, man? It's like the RL conference.
RLC, man? It's like the RL conference.
You off doing a life stuff now?
I think if I just totally hack the um
I just totally hack
You'd have to literally have a second
You'd have to literally have a second
set of environments though. No.
Not really RL.
Now if you consider like the mainstream
Now if you consider like the mainstream
thing done 10,000x faster at times
thing done 10,000x faster at times
thousandx at others.
thousandx at others.
Suppose this is mainstream.
Suppose this is mainstream.
Not really at all though.
What would I do with this?
Want to hack this to have
Want to hack this to have
second set of environments. Is there
second set of environments. Is there
nothing easier I can do?
Right.
Oh, the wrong branch.
Good thing I caught that before I messed
Good thing I caught that before I messed
it up too much.
So we just take all this data All right.
Yes.
a contract with you guys
a contract with you guys
that about. So the way that puffer works
that about. So the way that puffer works
right is uh everything on the business
right is uh everything on the business
side is contract based. So it is uh can
side is contract based. So it is uh can
you get companies
you get companies
interested in working with us based off
interested in working with us based off
of simulations that you've built or like
of simulations that you've built or like
other domain experience that you have in
other domain experience that you have in
addition to uh the RL stuff with puffer.
addition to uh the RL stuff with puffer.
So, the reason that six degree of
So, the reason that six degree of
freedom arms is likely to be um
freedom arms is likely to be um
contractw worthy is that there are a ton
contractw worthy is that there are a ton
of robotics companies and all of the
of robotics companies and all of the
current sims for six degree freedom uh
current sims for six degree freedom uh
sixderee freedom arms are really slow
sixderee freedom arms are really slow
because it's the same sim as is used for
because it's the same sim as is used for
higher fidelity uh robotics like
higher fidelity uh robotics like
humanoid hands, full humanoids, things
humanoid hands, full humanoids, things
like that.
like that.
So having a good and very fast sim doing
So having a good and very fast sim doing
impressive tasks and then also doing
impressive tasks and then also doing
some work on like talking to robotics
some work on like talking to robotics
companies, right? And like getting them
companies, right? And like getting them
interested in doing stuff with Puffer.
interested in doing stuff with Puffer.
If you're able to do that and we're able
If you're able to do that and we're able
to swing a contract off of it, you will
to swing a contract off of it, you will
be on the contract.
That is essentially how you get into um
That is essentially how you get into um
business side stuff of puffer. It is by
business side stuff of puffer. It is by
being good enough to build out
being good enough to build out
applications specific domains around
applications specific domains around
Puffer and then using your work plus uh
Puffer and then using your work plus uh
plus the general recognition of Puffer
plus the general recognition of Puffer
in order for us to partner on a contract
in order for us to partner on a contract
for a company. How that works.
It has to be good though. Like it can't
It has to be good though. Like it can't
be the type of thing where you kind of
be the type of thing where you kind of
build a sim and
build a sim and
you know we have to go get a contract
you know we have to go get a contract
and you're not going to be capable of
and you're not going to be capable of
working on it, right? Like it's really
working on it, right? Like it's really
much more independently driven. You
much more independently driven. You
know, occasionally I do go to people who
know, occasionally I do go to people who
are who I know are good in specific
are who I know are good in specific
areas for contracts that I have found,
areas for contracts that I have found,
but the way more consistent way of being
but the way more consistent way of being
able to do this is to be able to
able to do this is to be able to
actually go build a thing, find a
actually go build a thing, find a
company that's interested, getting them
company that's interested, getting them
talking to us.
talking to us.
That's the easiest way to do it.
Okay,
Okay,
that's something.
What this should the What should this be
What this should the What should this be
doing now?
doing now?
Expert vac receive.
Expert vac receive.
Uh, this should be populating
Uh, this should be populating
buffers with expert data.
And we should be getting
stats.
Yeah, we should be getting stats from
Yeah, we should be getting stats from
something totally different.
That's right.
That's right.
Instead,
we have KL blowing up.
Anning out instant. Cool.
Anning out instant. Cool.
Very cool. Um,
Very cool. Um,
okay.
okay.
Let's just move this Here
to here.
Okay. So, this is the expert data.
Okay. So, this is the expert data.
See, it actually does get the
See, it actually does get the
or it's supposed to be
Uh the heck.
What?
Why are we on 300 branch?
Why are we on 300 branch?
I literally checked this so that it
I literally checked this so that it
would be on the correct
would be on the correct
Damn it.
Let's see if this is close enough that
Let's see if this is close enough that
it's actually
it's actually
done something for us.
This we got this
Rebuild this. And now we're going to try
Rebuild this. And now we're going to try
to train this again.
to train this again.
Should be imitation
atically different, right?
If we're doing this correctly,
If we're doing this correctly,
yes, this being done correctly
is what we're logging here.
Is this never going to eval?
Why is this not going to eval
now?
Yeah. Why is this not um evaluate
this bot?
weird
weird
using to do the final evaluation.
Why this is not evaling? We need this to
Why this is not evaling? We need this to
eval so we can actually get
eval so we can actually get
final data.
It's crazy, by the way, just how stupid
It's crazy, by the way, just how stupid
easy this is. Like, oh, we can just do
easy this is. Like, oh, we can just do
imitation learning if we just have data.
imitation learning if we just have data.
Yeah, if you actually have data,
Yeah, if you actually have data,
not even a problem to solve.
Well, I know what it is.
But it is fine. It's just I forgoting
But it is fine. It's just I forgoting
forgetting to close the end.
96 closes. Perfect.
96 closes. Perfect.
Okay,
Okay,
be happy with that.
be happy with that.
Except
my
How is
Now it's just training on all the data.
And this does a little better.
And this does a little better.
Interestingly, does not solve the task.
Interestingly, does not solve the task.
Not solve the task.
Not solve the task.
Oh, wait. Hang on. Is this might be
Okay, that actually makes a difference.
Increase the update epochs.
Yeah, like increasing update epoch seems
Yeah, like increasing update epoch seems
to actually do to make a big difference
to actually do to make a big difference
here, right?
here, right?
Lo and behold, supervised learning
Lo and behold, supervised learning
scales.
This little dip here. Is it getting over
This little dip here. Is it getting over
the first screen of bricks? That's an
the first screen of bricks? That's an
expected dip.
Only 500
Only 500
or epox.
ass
about the same
about the same
better with two
better with two
that Just noise
that Just noise
is m magically better with two.
is m magically better with two.
That's interesting though. It doesn't
That's interesting though. It doesn't
just if it does not learn from this as
just if it does not learn from this as
well as from the RL. Kind of odd, isn't
well as from the RL. Kind of odd, isn't
it?
Okay, now two update epox is better. We
Okay, now two update epox is better. We
get 600 score.
And now I know it would solve it if we
And now I know it would solve it if we
just kept we just kept going, right?
Let's ignore that. Um, it should be able
Let's ignore that. Um, it should be able
to learn from six four length segments,
to learn from six four length segments,
right?
Isn't that kind of crazy?
RL being more efficient than
RL being more efficient than
itation
be the other way around though.
be the other way around though.
Absolutely should be.
Maybe there's like
there's just gank
now that like these make very slight
now that like these make very slight
differences at most.
I do somewhat worry. The thing is the
I do somewhat worry. The thing is the
episode lengths are super long and
episode lengths are super long and
breakout, right?
I do somewhat worry about that. It's
I do somewhat worry about that. It's
just that
Yeah, there we go.
Yeah, there we go.
So, it's just that the number of
So, it's just that the number of
environments is super large.
environments is super large.
Um, you're not even really seeing data
Um, you're not even really seeing data
from the whole game, I don't think.
from the whole game, I don't think.
We'll do the back of the envelope math
We'll do the back of the envelope math
in a second.
in a second.
Soon
Soon
as we know it actually solves
actually appear to be a little stuck,
actually appear to be a little stuck,
doesn't it?
Okay. 725. So, a good score, but not
Okay. 725. So, a good score, but not
actually solved.
actually solved.
14.
This is Yeah, there you go. So pretty
This is Yeah, there you go. So pretty
much
much
uh if you start just like running a
uh if you start just like running a
ideal policy
ideal policy
with this many environments, it won't
with this many environments, it won't
even finish a full game
even finish a full game
by this number of steps.
by this number of steps.
That's the main problem.
kind of happened. Huh?
I just like
725. Let me just make sure.
725. Let me just make sure.
Not like super sensitive.
Not like super sensitive.
And then after this,
And then after this,
I'm trying to think what this tells us.
I mean, this does tell us, right? like
I mean, this does tell us, right? like
and let's like ignore a second the fact
and let's like ignore a second the fact
that we're kind of doing this in a way
that we're kind of doing this in a way
that's a little janky.
that's a little janky.
Uh if you have
Uh if you have
that just nans out. That's funny.
So if you have a good uh aisle and let's
So if you have a good uh aisle and let's
actually just do one other thing.
make sure that
make sure that
actually going to solve.
actually going to solve.
Um,
if you actually have like a good policy
if you actually have like a good policy
can pretty easily I
can pretty easily I
another one based on it, right?
purely just by matching actions over
purely just by matching actions over
segments.
I mean, this is kind of obvious, right?
I mean, this is kind of obvious, right?
But now the question is,
But now the question is,
what's so special about the RL, right?
what's so special about the RL, right?
Like why can't
Like why can't
why can't we just I a policy from
why can't we just I a policy from
scratch just by like keeping around the
scratch just by like keeping around the
best data
best data
the best trajectory segments.
Okay. Interesting. So
uh this actually doesn't fully solve it.
Likewise, we get like this highish score
Likewise, we get like this highish score
not fully solved.
That's a little odd,
That's a little odd,
but it does do way way better than our
but it does do way way better than our
IIL from scratch, obviously.
So, what is the difference here? Right.
Well, hang on. We have a few things,
Well, hang on. We have a few things,
don't we?
So, if I just take our gold samples,
if we do it like this,
if we do it like this,
how well does this do?
Okay. So, even this does pretty
Okay. So, even this does pretty
decently. So, what this is doing here
decently. So, what this is doing here
is just keeping the best data from uh
is just keeping the best data from uh
it's keeping like the best batch of data
it's keeping like the best batch of data
basically just like grabbing all the
basically just like grabbing all the
best segments based on reward.
Now we know that this approach also
Now we know that this approach also
can do decently well.
So then what is it that is preventing us
So then what is it that is preventing us
from doing this without the expert
from doing this without the expert
policy?
Because if we don't make any change to
Because if we don't make any change to
this,
if we make no change to this except
if we make no change to this except
swapping out
expert for the policy, now it has to
expert for the policy, now it has to
generate its own data to learn on.
Not like it doesn't learn anything,
Not like it doesn't learn anything,
right? It learns something.
right? It learns something.
Very inefficient.
Very inefficient.
Let's go mess around with the the
Let's go mess around with the the
original formula bit.
I think that this buffer
I think that this buffer
we're going to make this buffer way
we're going to make this buffer way
bigger.
for like way bigger.
We have a big buffer.
Way harder to overfit now, right?
Good transitions.
caps out though. H
starts getting worse.
starts getting worse.
actually starts getting worse.
It makes sense for me to try to get like
It makes sense for me to try to get like
a batch IIL thing going.
not like showing um
wrong dependence on
we would think that it would be more
we would think that it would be more
sample efficient even in like it would
sample efficient even in like it would
be better by all metrics. Even in the
be better by all metrics. Even in the
easiest or like the simplest case like
easiest or like the simplest case like
IIL should just be easier than RL.
We have basically a perfect data
We have basically a perfect data
generator.
like so disconcerting to me in some
like so disconcerting to me in some
sense.
sense.
Back to a few different
Let's add in
this way.
Exact opposite. Wait, I can't follow
Exact opposite. Wait, I can't follow
along perfectly due to my limited
along perfectly due to my limited
understanding.
understanding.
What does off policy method have to do
What does off policy method have to do
having more data efficient
having more data efficient
computation efficiency
computation efficiency
opposite of on policy have anything to
opposite of on policy have anything to
do with this?
do with this?
Aren't going through. Yeah, YouTube
Aren't going through. Yeah, YouTube
sometimes will filter things if it
sometimes will filter things if it
thinks you're sending links, whatever.
thinks you're sending links, whatever.
It's kind of weird. Um,
It's kind of weird. Um,
the thing is I don't actually know if
the thing is I don't actually know if
off policy methods are substantially
off policy methods are substantially
more sample efficient
more sample efficient
like based on some recent stuff I've
like based on some recent stuff I've
seen.
seen.
And this is not either. This is
And this is not either. This is
imitation learning.
This should definitely do something. And
This should definitely do something. And
like it's working, but it's actually
like it's working, but it's actually
it's funny. It's not closing out the
it's funny. It's not closing out the
environment.
environment.
not learning the final
not learning the final
final bit that it needs.
Wonder if I can just do
be able to just fill it either way.
be able to just fill it either way.
So odd
Just do one thing that's like super
Just do one thing that's like super
basic.
Yeah.
Kind of tricky.
Okay. What if we do
Okay. What if we do
This is a boss.
need restroom real quick. Puffer li
need restroom real quick. Puffer li
rendering to wand.
rendering to wand.
Um,
Um,
so technically, yes, Kevin, it's just
so technically, yes, Kevin, it's just
the massive amount of data that you end
the massive amount of data that you end
up accumulating as soon as you start
up accumulating as soon as you start
doing that by default.
doing that by default.
But I'd probably want to do it as like
But I'd probably want to do it as like
an off by default thing. And the other
an off by default thing. And the other
annoying thing is like we actually have
annoying thing is like we actually have
to figure out how to do that with
to figure out how to do that with
uh like you need headless rendering for
uh like you need headless rendering for
ocean ends to do that, which is doable.
ocean ends to do that, which is doable.
I think a couple people have gotten it
I think a couple people have gotten it
to work, but I haven't integrated it
to work, but I haven't integrated it
yet. I'll be right back once.
the heck happened to episode return at
the heck happened to episode return at
nann.
nann.
All right, maybe the maybe this is just
All right, maybe the maybe this is just
like weird hypers being weird, you know?
like weird hypers being weird, you know?
Maybe this is just weird hypers.
Maybe this is just weird hypers.
They should not be nanning
Yeah, just hovers around these 700s.
Well, there's some 800s in there.
It's actually closer
It's actually closer
and it nanss out. Okay, Nan's out at the
PO always better than stack when data
PO always better than stack when data
efficiency isn't an issue. Yeah, I don't
efficiency isn't an issue. Yeah, I don't
know. And nobody does comparisons
know. And nobody does comparisons
correctly on these things. Um, and I
correctly on these things. Um, and I
honestly I don't even know if SAC is
honestly I don't even know if SAC is
better when data efficiency is an issue
better when data efficiency is an issue
because
because
I talked to a bunch of DeepMind people
I talked to a bunch of DeepMind people
about like all the off policy stuff that
about like all the off policy stuff that
they do and such and it's actually
they do and such and it's actually
become kind of unclear to me
like off policy RL in general is kind of
like off policy RL in general is kind of
a mess.
right? Why the heck does this not just
right? Why the heck does this not just
solve? Right? This is basic ass
solve? Right? This is basic ass
imitation learning with an expert.
Like why is this being so weird?
Okay, it's pretty close to just solving,
Okay, it's pretty close to just solving,
right?
Policy.
Policy.
Um
because like nobody's ever really done
because like nobody's ever really done
controlled experiments at all, Kevin.
controlled experiments at all, Kevin.
Like it's everybody's kind of been
Like it's everybody's kind of been
pushing sample efficiency sample
pushing sample efficiency sample
efficiency on these offpaul methods and
efficiency on these offpaul methods and
like they have crazy settings for these
like they have crazy settings for these
things. Like the sheer amount of compute
things. Like the sheer amount of compute
you're using with these methods like
you're using with these methods like
nobody does this with PO, right? They've
nobody does this with PO, right? They've
gotten themselves in this like insane
gotten themselves in this like insane
data regime where like they're spending
data regime where like they're spending
a huge amount of compute on tiny models
a huge amount of compute on tiny models
with tiny amounts of data and like
with tiny amounts of data and like
nobody's ever really tried that in the
nobody's ever really tried that in the
on pulse settings. It's basically just
on pulse settings. It's basically just
two diverging branches of research where
two diverging branches of research where
like nobody's ever bothered to compare
like nobody's ever bothered to compare
correctly and like the data hygiene
correctly and like the data hygiene
practices in both settings are awful.
This is like close or though at least
This is like close or though at least
like
like
800 almost
like fairly confident we could make
like fairly confident we could make
something,
something,
right?
I mean regardless though
I mean regardless though
just like at least does something
an Astro I shouldn't need a bigger model
an Astro I shouldn't need a bigger model
because the expert is not a bigger
because the expert is not a bigger
model. I should not have to distill into
model. I should not have to distill into
a bigger model.
anything I should be able to distill
anything I should be able to distill
into a smaller model.
Trying to think how to interpret this
What if it's just a spruy reward
What if it's just a spruy reward
sparity? No.
There's no value function.
It's a very sparse reward, right?
Heal versus sack is like the most basic
Heal versus sack is like the most basic
thing.
thing.
Try to decide if you
Try to decide if you
value base. You typically don't
value base. You typically don't
you typically just use your favorite
like bas like none of the basic stuff in
like bas like none of the basic stuff in
RL has actually been done correctly
RL has actually been done correctly
Kevin
like at all.
Okay. What did I break? Because now it's
Okay. What did I break? Because now it's
like way worse. Oh, now I don't have the
like way worse. Oh, now I don't have the
gold buck
which it it works this way as well.
But the simple proof of this, right?
But the simple proof of this, right?
Okay, I'm able to train at millions of
Okay, I'm able to train at millions of
steps per second, right? That means if I
steps per second, right? That means if I
want to run a 100 million step
want to run a 100 million step
experiment, it's done in under a minute.
experiment, it's done in under a minute.
Well under a minute, right?
Well under a minute, right?
All right. So if I have a thousand step
All right. So if I have a thousand step
per second RL library and a lot of them
per second RL library and a lot of them
are even slower than that. Okay. And I
are even slower than that. Okay. And I
want to do 100 million steps.
want to do 100 million steps.
Okay.
Okay.
Uh and then let's do in hours. That's 27
Uh and then let's do in hours. That's 27
hours.
hours.
So it takes longer than a day to run a
So it takes longer than a day to run a
single experiment on a GPU or like a
single experiment on a GPU or like a
super basic environment.
super basic environment.
Your dev loop is sucks. You're not going
Your dev loop is sucks. You're not going
to get anything done. You're not running
to get anything done. You're not running
enough experiments. And you're not doing
enough experiments. And you're not doing
basic hygiene like running many seeds
basic hygiene like running many seeds
and hyperparameter sweeps except maybe
and hyperparameter sweeps except maybe
like a few seeds for the final
like a few seeds for the final
experiments.
experiments.
All right. And then they actually with
All right. And then they actually with
Ataru then they run all 57 of them. So
Ataru then they run all 57 of them. So
they're not doing any of that stuff.
they're not doing any of that stuff.
Like it's literally just impossible on
Like it's literally just impossible on
the speeds and on the academic budgets
the speeds and on the academic budgets
that people have access to for the
that people have access to for the
science to be any good.
science to be any good.
That's it.
Yeah. So, we could astro if I were to do
Yeah. So, we could astro if I were to do
high perf SAC, but like that's not a
high perf SAC, but like that's not a
trivial thing to do. Like it would take
trivial thing to do. Like it would take
me a while to do that.
Not like
Not like
I don't think that there's any data I've
I don't think that there's any data I've
seen that's like so massively in favor
seen that's like so massively in favor
of SACE that I expect it to do anything
of SACE that I expect it to do anything
for
Yeah. So, this is the
I break it again.
Oh, yeah. Hang on.
Oh, yeah. Hang on.
Well, we want to let's let it actually
Well, we want to let's let it actually
run like this.
run like this.
Um,
Um,
then we'll go back to the number of Ms.
then we'll go back to the number of Ms.
That makes sense.
We should try this on some more
We should try this on some more
environments.
environments.
See if we can like predict
See if we can like predict
what this is going to be good on.
Just sparity.
Get up.
What ms do I have in ocean I could is
triple try didn't work Right.
How does this get zero rewards?
How does this get zero rewards?
Doesn't make any sense, does it?
Oh, negative. Is that the case?
Negative reward.
do anything?
I think this change is in here.
Engine here.
Press
Press
that
Maybe there is just something weird with
Maybe there is just something weird with
like the reward sparity going on, right?
This should be an environment that's
This should be an environment that's
like not a super easy environment.
Doing definitely non-trivially well.
Doing definitely non-trivially well.
You still get the same effect where like
You still get the same effect where like
on the harder ends it fails to close out
on the harder ends it fails to close out
like it gets stuck.
need a bigger data buffer maybe.
need a bigger data buffer maybe.
I think it's just that, right? It's not
I think it's just that, right? It's not
just like, oh, give it a bigger data
just like, oh, give it a bigger data
buffer.
The fact that that like so quickly.
The fact that that like so quickly.
Oh, that is a little better, huh?
Oh, that is a little better, huh?
Maybe this is the NF to use for testing.
It has like very short episodes
and a very well- definfined score.
then it gets stuck.
then it gets stuck.
five,
which is very close to what we have in
which is very close to what we have in
the buffer. I believe it's just a
the buffer. I believe it's just a
positive one.
Okay.
I do this
mentioned at the beginning basically
mentioned at the beginning basically
solves it from an RL side and it be a
solves it from an RL side and it be a
hardware problem. What does RL
hardware problem. What does RL
accomplish and what is there to do
accomplish and what is there to do
afterward in terms of hardware? Well, I
afterward in terms of hardware? Well, I
mean you can fly the drones around in
mean you can fly the drones around in
simulation. you can randomize them in
simulation. you can randomize them in
simulation. Uh the actual learned
simulation. Uh the actual learned
control works very well. Obviously, you
control works very well. Obviously, you
then have to actually put it on a drone
then have to actually put it on a drone
and if it's a custom drone, then you
and if it's a custom drone, then you
have to build said custom drone and have
have to build said custom drone and have
the sensors be sane and make sure that
the sensors be sane and make sure that
whatever the heck that drone is is at
whatever the heck that drone is is at
least somewhat matched by something in
least somewhat matched by something in
the randomized training scenario.
Without puffer lib, you have to both do
Without puffer lib, you have to both do
that and figure out how like the crazy
that and figure out how like the crazy
cursed RL that doesn't make sense can be
cursed RL that doesn't make sense can be
made to work.
It is interesting to me
It is interesting to me
the gold reward is getting higher the
the gold reward is getting higher the
more I make the bigger I make this
more I make the bigger I make this
buffer
Now, it could just be that this is
Now, it could just be that this is
getting filled up with easy levels,
getting filled up with easy levels,
right?
right?
That would be a problem with doing it
That would be a problem with doing it
this way.
this way.
Filling up with easy levels.
Filling up with easy levels.
Yeah,
cuz now unlike before, right, we have
cuz now unlike before, right, we have
a pretty solid buffer.
The PF is maybe a little higher than
The PF is maybe a little higher than
before, but not really.
I wouldn't be surprised at some way that
I wouldn't be surprised at some way that
it's something about the way I'm doing
it's something about the way I'm doing
this.
Why are there some shillers of Isaac and
Why are there some shillers of Isaac and
on Twitter?
Well, the thing is like if you're doing
Well, the thing is like if you're doing
humanoids, for instance, you're not
humanoids, for instance, you're not
going to go write your own humanoid sim.
going to go write your own humanoid sim.
The problem is those packages are these
The problem is those packages are these
really big bulky physics sims that are
really big bulky physics sims that are
super slow when you have like relatively
super slow when you have like relatively
simple form factors. Um,
simple form factors. Um,
but like the vast majority of
but like the vast majority of
roboticists are not simulation
roboticists are not simulation
engineers. Like they're not going to go
engineers. Like they're not going to go
spend their time working on how can I
spend their time working on how can I
make a better sim, right? They spend
make a better sim, right? They spend
their time like fiddling with control
their time like fiddling with control
algorithms and training and setups and
algorithms and training and setups and
stuff like that.
Do you see puffer training robotics over
Do you see puffer training robotics over
the next few years? Well, I mean,
the next few years? Well, I mean,
robotics is really just one application
robotics is really just one application
area for us, right? And it's not even
area for us, right? And it's not even
necessarily the biggest one. Um, we're
necessarily the biggest one. Um, we're
going to have our drone project. I think
going to have our drone project. I think
that we should be able to get something
that we should be able to get something
pretty cool out of that. So, you see
pretty cool out of that. So, you see
like, yeah, you can do this stuff. You
like, yeah, you can do this stuff. You
can do it in fast simulation. You can do
can do it in fast simulation. You can do
it very well. probably going to have and
it very well. probably going to have and
we've already had one person working on
we've already had one person working on
like a six degree of arm uh sim. See if
like a six degree of arm uh sim. See if
we can get that to work and throw that
we can get that to work and throw that
on real hardware and depending on how
on real hardware and depending on how
those go. Then we'll see what else we do
those go. Then we'll see what else we do
in robotics, right?
in robotics, right?
Like one of many problems that we can
Like one of many problems that we can
solve though.
Something weird happened because
Something weird happened because
Ward in the buffer kept going up.
The performance did not
performance stalled.
I guess this is the main the main
I guess this is the main the main
difficulty is how do we actually
difficulty is how do we actually
make sure
make sure
we're getting good data in this buffer
we're getting good data in this buffer
that's like diverse and interesting.
that's like diverse and interesting.
Okay.
Yeah, that's the key problem here.
cuz like we know that we can learn.
I mean initially what happens if I just
what happens if I just
say we kick out four samples
that do for
We'll see how this compares.
We'll see how this compares.
If there's a substantial difference,
If there's a substantial difference,
then we'll know it's the bias in the
then we'll know it's the bias in the
data. If not, then we'll have to think
data. If not, then we'll have to think
about this more.
It's so weird to me how this will get
It's so weird to me how this will get
you like a relatively decent policy so
you like a relatively decent policy so
quickly,
quickly,
not even using any RL. Hell.
It's tough because this feels like
It's tough because this feels like
incredibly powerful, but it has weak
incredibly powerful, but it has weak
spots and they're different weak spots
spots and they're different weak spots
from what I'm used to with um EPO and
from what I'm used to with um EPO and
such.
Very different weak spots.
Very different weak spots.
This appears to be doing like
This appears to be doing like
substantially worse than before.
substantially worse than before.
We'll see by the end of training.
Fact does something on our hard M though
Fact does something on our hard M though
is crazy.
Not a super at all.
If this has to be full episode, it still
If this has to be full episode, it still
wouldn't work though because you could
wouldn't work though because you could
still save like the easiest levels for
still save like the easiest levels for
instance.
instance.
But actually prioritizing your data just
But actually prioritizing your data just
off of reward
off of reward
pretty tough.
I mean, we there's a massively less
I mean, we there's a massively less
sample efficient thing we can do. Kind
sample efficient thing we can do. Kind
of defeats the point, but is a good
of defeats the point, but is a good
experiment to run.
experiment to run.
Going through CS30 2 CS231N lectures and
Going through CS30 2 CS231N lectures and
problem sets as suggested.
problem sets as suggested.
Thoughts on what Justin is building? You
Thoughts on what Justin is building? You
know, I actually I meant to go visit
know, I actually I meant to go visit
because I just heard that he's actually
because I just heard that he's actually
an SF.
an SF.
Um,
Um,
I meant to go visit because I haven't
I meant to go visit because I haven't
talked to him in years.
talked to him in years.
I was uh I interned with him as a high
I was uh I interned with him as a high
school student actually. It was like the
school student actually. It was like the
first bit of research that I did,
but I haven't talked with him in a long
but I haven't talked with him in a long
time.
So, if this fails completely, this will
So, if this fails completely, this will
be interesting.
I expect this to basically just save the
I expect this to basically just save the
easy levels.
easy levels.
Basically give itself a very bad
Basically give itself a very bad
curriculum.
curriculum.
Like if you study for a math test by
Like if you study for a math test by
only solving the uh the first few
only solving the uh the first few
problems in the the section
that would be consistent with um
that would be consistent with um
a lower score
a lower score
like a lower but still reasonable for
well there's one thing that we can
well there's one thing that we can
definitely try
definitely try
um that we would expect to be like
um that we would expect to be like
substantially less sample efficient
substantially less sample efficient
which again kind of defeats the point.
It still won't
and might not be want to use testing
and might not be want to use testing
with this.
This is lower.
Get something that's like more
Get something that's like more
monolithic, I guess.
No, I'm surprised that triple triad
No, I'm surprised that triple triad
doesn't work. I'm really kind of
doesn't work. I'm really kind of
surprised with that. Like, why why
surprised with that. Like, why why
shouldn't triple triad work, right?
You looked at what building at World
You looked at what building at World
Labs.
Labs.
I have no idea what they're doing.
I have no idea what they're doing.
I really have no idea what they're
I really have no idea what they're
doing.
See like this failing is weird.
Something like this failing
at least is positive score.
at least is positive score.
Very poor.
Very poor.
Very very poor.
Very very poor.
Negative 0.
Negative 0.
Really?
Oh,
try this. But I doubt this.
It's kind of weird how I can't really
It's kind of weird how I can't really
predict what MSIS does stuff on versus
Can I not do connect four?
are connectable.
All right. Cuz it's kind of slow with
All right. Cuz it's kind of slow with
the opponent.
I don't want to just fiddle with this
I don't want to just fiddle with this
forever. I'm trying to think how I can
forever. I'm trying to think how I can
leverage this.
Getting
a value function RL giving you a value
a value function RL giving you a value
function is so powerful because
you get like reasonable estimates
you get like reasonable estimates
of segments even when there's not reward
of segments even when there's not reward
there like you get get state estimation
there like you get get state estimation
out of it. the tough thing. So this this
out of it. the tough thing. So this this
does not have any state estimation
does not have any state estimation
basically.
But then the problem is when you have
But then the problem is when you have
when you have a value function
like that gets stale.
like that gets stale.
And also if it has errors in it,
And also if it has errors in it,
tough to build a data set out of
I am kind of confused. Why?
How does this not work?
I tried this.
Other way around.
Maybe this does work and it's just very
Maybe this does work and it's just very
data inefficient. Here
data inefficient. Here
I like
I like do this
This is like a straight up best event,
This is like a straight up best event,
right?
get stuck already. Like 18.
Guess there's just super little variance
Guess there's just super little variance
in the data then of that, huh?
Entropy is super low.
Oh no.
problem is this is now completely off of
problem is this is now completely off of
the original objective of this ample
the original objective of this ample
efficiency.
Not even faster wall clock even though
Not even faster wall clock even though
fan super
fan super
even though like high SPS
that is in some sense a very easily
that is in some sense a very easily
scalable method.
Best of n.
What if we try to do like something
What if we try to do like something
offline?
problems when you go to offline, you end
problems when you go to offline, you end
up with the same bloody problems off
up with the same bloody problems off
policy has, don't you?
policy has, don't you?
Yeah. Because then you end up with like
Yeah. Because then you end up with like
Yeah. Literally all the same problems.
What are the issues with off policy?
What are the issues with off policy?
Well, it's the methods get very
Well, it's the methods get very
complicated very quickly because
complicated very quickly because
the base algorithm in on policy of
the base algorithm in on policy of
policy gradients, you don't need to bolt
policy gradients, you don't need to bolt
that much stuff onto it before you have
that much stuff onto it before you have
a soda algorithm essentially. Like if
a soda algorithm essentially. Like if
you just take on pol like just policy
you just take on pol like just policy
gradients and you bolt on advantage
gradients and you bolt on advantage
estimation and clipping, you already
estimation and clipping, you already
have PO pretty much. Whereas if you take
have PO pretty much. Whereas if you take
DQN, it just fails horribly at
DQN, it just fails horribly at
everything and you have to bolt on like
everything and you have to bolt on like
six or seven different tricks to even
six or seven different tricks to even
get it to be comparable to PO. Well,
I'm also trying to figure out how
How's a Q function even for you from
function? behavioral
cloning.
cloning.
What problem do you big enough data set
What problem do you big enough data set
you don't have any problems?
you don't have any problems?
Of course, I tried to do a peace meal,
Of course, I tried to do a peace meal,
so I didn't I hit problems.
so I didn't I hit problems.
big enough data set
have any problems cloning
with sparity problems online
with sparity problems online
major Of
course, you can't reuse
course, you can't reuse
data
data
to be expert quality data.
There any other way we can go about
There any other way we can go about
this?
I don't want to type this rock. I'm
I don't want to type this rock. I'm
trying to think what else I can come up
trying to think what else I can come up
with that'll like give me a good answer.
who I spoke with at
who I spoke with at
at Nurups.
This the on is this off policy?
Oh, okay. So, this is off policy. The
Oh, okay. So, this is off policy. The
VMPO
VMPO
separate
Okay, so this is the on policy. I didn't
Okay, so this is the on policy. I didn't
realize this. There are two versions.
realize this. There are two versions.
There's an off policy version and
There's an off policy version and
there's an on policy.
So, this is probably
So, this is probably
what I was directed to.
Super math heavy paper.
Very very different out potentially
Very very different out potentially
here.
here.
Do they have a pseudo code block?
I can understand this at least a little
I can understand this at least a little
bit.
Yeah.
Okay, this is an M application.
like more experiments.
What would you say scal would you say
What would you say scal would you say
scalability is one of the drawbacks of
scalability is one of the drawbacks of
off policy?
No, it's just
No, it's just
look basically the only actually good
look basically the only actually good
off policy research it's like a small
off policy research it's like a small
handful of deep mind papers behind
handful of deep mind papers behind
closed doors that don't have a ton of
closed doors that don't have a ton of
context
context
like all the random p like all the
like all the random p like all the
random robotics papers using SACE for a
random robotics papers using SACE for a
million steps on the same three tasks
million steps on the same three tasks
like you can basically just ignore that
does bother me here that they only do
does bother me here that they only do
control task.
Why did they do
they do this?
Okay, actually this will be a better
Okay, actually this will be a better
look at
run from full state.
Is this
Is this
I think they don't have comparisons but
I think they don't have comparisons but
I think we together
task name equals do
Seven.
Seven.
Wait, what?
Same problem.
Same problem.
Any luck on material science? Uh, I've
Any luck on material science? Uh, I've
talked to a couple people. I need to go
talked to a couple people. I need to go
back to doing it. I'm doing core
back to doing it. I'm doing core
research at the moment, though. I'm
research at the moment, though. I'm
taking a break from doing
taking a break from doing
like building more applications to do a
like building more applications to do a
little bit of research.
to run from
to run from
more experiments in here or this I have
more experiments in here or this I have
to work
Wait, what?
SACE and PO are on top of each other
SACE and PO are on top of each other
here, right?
M no up here 25 mil different steps
what the heck is this
build
build
the different architecture True. Well,
oh, we do have MO on here.
They have MO as the best baseline
They have MO as the best baseline
for this stuff.
I probably do have to actually
I probably do have to actually
understand this thing
though.
I think is there anything I can take
I think is there anything I can take
from segment though?
At the very least
can say for certain that we're nowhere
can say for certain that we're nowhere
near optimal with our current algorithm.
The fact
you can say that for certain tasks
you can say that for certain tasks
in a very simple like data collection
in a very simple like data collection
strategy
strategy
with imitation learning
with imitation learning
works very well. also a fact. Also say
works very well. also a fact. Also say
it does shockingly well on some hard
it does shockingly well on some hard
problems, including at least better
problems, including at least better
initially than
initially than
uh our best online RL method. Also a
uh our best online RL method. Also a
fact.
They ever in
they wrote this followup.
they wrote this followup.
Why would they write this followup?
They tell us
it's interesting that they wrote this
it's interesting that they wrote this
paper.
They just want to adapt this original to
They just want to adapt this original to
high data regime.
That the idea
top K advantages.
Yes, this is actually
Yes, this is actually
this is in ours as well. Okay.
Fact that they wrote the original off
Fact that they wrote the original off
policy paper then they wrote a follow-up
policy paper then they wrote a follow-up
on policy is kind of telling in some
on policy is kind of telling in some
sense isn't it
sense isn't it
like why would they do That
Pass smart.
LM
I didn't they
I didn't they
is slow
is slow
like why they didn't even give
like why they didn't even give
themselves reasonable MO baselines right
themselves reasonable MO baselines right
like look they just treat
like look they just treat
where is it
where is it
okay look how crazy this is right this
okay look how crazy this is right this
is what I say about science in this
is what I say about science in this
space just being weird
space just being weird
why wouldn't you just run mo
like why wouldn't you do a learning
like why wouldn't you do a learning
curve for MO.
curve for MO.
It's literally your algorithm.
It's literally your algorithm.
These are both deep minds similar like
These are both deep minds similar like
shared authorship.
shared authorship.
So they just have this line at 40
So they just have this line at 40
million steps. What they say here,
million steps. What they say here,
right, is that MO is better than SVG,
right, is that MO is better than SVG,
better than uh where is it?
better than uh where is it?
They say this is better,
They say this is better,
but they don't actually do a comparison
but they don't actually do a comparison
curve. And this thing is a tiny number
curve. And this thing is a tiny number
of samples,
of samples,
40 million. And then here they do better
40 million. And then here they do better
than mo once they have two billion
than mo once they have two billion
environment steps.
So according to their own experiments
So according to their own experiments
and here's like 600 million
and here's like 600 million
according to their own experiments. This
according to their own experiments. This
thing is like massively more sample
thing is like massively more sample
efficient at least up to this level of
efficient at least up to this level of
PF.
PF.
But they still wrote this algorithm and
But they still wrote this algorithm and
have this paper.
So presumably like there's got to be a
So presumably like there's got to be a
big difference in computation applied
big difference in computation applied
somehow.
somehow.
I have to understand the algorithms more
I have to understand the algorithms more
for that. So definitely going to look at
for that. So definitely going to look at
these two.
The other thing that I think is
The other thing that I think is
interesting, right?
This is as far as I am aware.
This is like the last core algorithm
This is like the last core algorithm
paper.
MP and BMP. Yeah.
this thing also include algorithm
this thing also include algorithm
changes. I mean uh network changes
Did they use a transform
or network?
I think I trust this way more.
The other one.
already senior on this.
When you go to this paper here, right,
it's only was this two years later?
This is just a ResNet.
They didn't keep the architecture
They didn't keep the architecture
for this
actually. Do we have curves?
Wait, this makes no bloody sense, right?
Yeah. Yeah. Yeah. This thing makes no
Yeah. Yeah. Yeah. This thing makes no
sense at all. Look at this.
sense at all. Look at this.
They're doing billions of steps to solve
They're doing billions of steps to solve
Breakout.
Yeah, I don't trust this as far as I can
Yeah, I don't trust this as far as I can
throw it.
Multitask stuff is interesting,
but then that's um not a heavily
but then that's um not a heavily
contested benchmark.
contested benchmark.
Basically, just keep mind doing that.
Basically, just keep mind doing that.
Okay, so they did adapt this thing
Okay, so they did adapt this thing
probably some high throughput thing as
probably some high throughput thing as
well.
But like the results kind of suck.
Unless I'm missing something here,
Unless I'm missing something here,
right?
right?
But don't these results just kind of
But don't these results just kind of
suck?
like bill like fancy architecture,
like bill like fancy architecture,
billions of frames or freaking Atari
billions of frames or freaking Atari
like Breakout. Five billion frames.
Yeah. And then they're baselining versus
Yeah. And then they're baselining versus
like PO at three million steps. Like,
like PO at three million steps. Like,
oh, it's down here. It's three million
oh, it's down here. It's three million
steps.
steps.
Honestly, the interesting one is
the MO being higher in sample F on this
ulser versus SACE being very close to PO
ulser versus SACE being very close to PO
on some of these
I don't think what we take from
I think we have to start looking at
I think we have to start looking at
the details of this paper
about this.
I saw some random,
dude. What are you doing?
Absolutely deranged programmer.
very complicated paper.
This all of it.
Yeah, this is just a network
Yeah, this is just a network
just a network
buffer. Okay, so at least this is the
buffer. Okay, so at least this is the
entire implementation.
Fire in
There's also this update critic.
Me see if I can understand at least a
Me see if I can understand at least a
little bit of this
policy evaluation update. Critic
lost.
lost.
This is down here.
The forward pass.
The forward pass.
The categorical
take log probed
two.
discounting. So this is the onestep
boss event. Why
did
This is fine. This is fine. This is mini
This is fine. This is fine. This is mini
batch right
batch right
this loss
credits.
Then you get
Then you get
dual function.
Just compute.
Do multiple iterations on this.
There's the
There's the
word pass.
a totally crazy algorithm.
Obviously, there are ton of details to
Obviously, there are ton of details to
this
I have to write Martin an email, I
I have to write Martin an email, I
think, before I go too crazy into this.
think, before I go too crazy into this.
I'm trying to think what's the most
I'm trying to think what's the most
productive way for me to go about things
productive way for me to go about things
right now. Um,
right now. Um,
I have some initial results.
I have some initial results.
They're not good enough to turn into a
They're not good enough to turn into a
new algorithm just off the bat. They are
new algorithm just off the bat. They are
enough to convince me though that there
enough to convince me though that there
are substantial holes in the current
are substantial holes in the current
algorithm like qualitative holes in the
algorithm like qualitative holes in the
current algorithm.
Neural MMO experiment probably being the
Neural MMO experiment probably being the
most convincing bad
most convincing bad
mostly around reward sparity. I think
mostly around reward sparity. I think
sample reviews
I'm going to have to email Martin on
I'm going to have to email Martin on
this and I'm going to have to do some
this and I'm going to have to do some
thinking
thinking
as to like do I want to invest in doing
as to like do I want to invest in doing
a full MO implementation.
a full MO implementation.
I think there are more things I don't
I think there are more things I don't
understand. The other thing I really
understand. The other thing I really
don't understand
don't understand
here's the question to people actually
here's the question to people actually
here's the question for the audience if
here's the question for the audience if
anybody has any idea off policy is
anybody has any idea off policy is
supposed to be able to is supposed to
supposed to be able to is supposed to
enable sample reuse right
enable sample reuse right
because you're no longer the problem in
because you're no longer the problem in
on policy right is you're getting data
on policy right is you're getting data
from your policy
from your policy
um but then you're actually evaluating
um but then you're actually evaluating
your data with a value function and when
your data with a value function and when
the data is old the value function is
the data is old the value function is
stale fail. Um, you're not actually
stale fail. Um, you're not actually
going to be able to like data that came
going to be able to like data that came
from a previous policy is not going to
from a previous policy is not going to
have a good value estimate and you're
have a good value estimate and you're
not going to be able to compute
not going to be able to compute
advantages and the whole thing falls
advantages and the whole thing falls
apart.
apart.
The thing I don't understand is how
The thing I don't understand is how
uh how does off policy fix this if when
uh how does off policy fix this if when
you have a Q function
you have a Q function
right like okay q function gives you the
right like okay q function gives you the
value for each uh each action you could
value for each uh each action you could
take right so for that one step that
take right so for that one step that
makes sense but the problem is you're
makes sense but the problem is you're
still training it against this
still training it against this
discounted return of what happens if uh
discounted return of what happens if uh
I take this action and then follow my
I take this action and then follow my
own policy or whatever. And if I follow
own policy or whatever. And if I follow
my own policy from there, then like,
my own policy from there, then like,
you're still going to get off policy,
you're still going to get off policy,
aren't you? I'm probably not phrasing
aren't you? I'm probably not phrasing
that great, but that's the thing I don't
that great, but that's the thing I don't
understand. Like, does this even fully
understand. Like, does this even fully
make sense? Um, cuz I talked to to Pablo
make sense? Um, cuz I talked to to Pablo
at RLC. He said like, "Yeah, we can't
at RLC. He said like, "Yeah, we can't
even really do more than eight update
even really do more than eight update
epochs on our data with off policy.
epochs on our data with off policy.
So, that seems really weird to me.
3:00 3:12.
3:00 3:12.
Uh, I have a separate project that I
Uh, I have a separate project that I
could work on now if we're stuck on
could work on now if we're stuck on
this. Unless anybody has any ideas
this. Unless anybody has any ideas
there.
there.
I have a separate thing that I can work
I have a separate thing that I can work
on.
on.
I'm going to go I'll be right back and
I'm going to go I'll be right back and
think about this for a minute and then
think about this for a minute and then
we'll see.
Let me get another.
Let me get another.
Oh, I'm still good here.
Okay,
Okay,
so
so
let me think a little bit more about
let me think a little bit more about
this problem before I just move to
this problem before I just move to
something else because I'm kind of in
something else because I'm kind of in
the right mind space for this at the
the right mind space for this at the
moment.
You're going to collect
policy pi
date.
This gives you action.
B.
You
kind of value as well, right? It gives
kind of value as well, right? It gives
you a value.
I'm going to collect a bunch of data.
And I also have the actions.
Words.
Okay. So, we have some segment here.
What I'm just doing now,
I'm saying this segment is good.
If we have
it just has high summed rewards.
All right. So this is what we have now.
All right. So this is what we have now.
This is one thing
This is one thing
we could do.
Did we just add the value?
Could add the value of the final state
Could add the value of the final state
to this.
Not easily though, right? Because we
Not easily though, right? Because we
have to recomputee it.
Theoretically though, at the very least,
Theoretically though, at the very least,
we could do plus
we could do plus
a
a
do V
quality of the segment. Of
course, the problem with doing this is
course, the problem with doing this is
you have to recomputee it every time you
you have to recomputee it every time you
update the bloody network.
But theoretically, we could do something
But theoretically, we could do something
like this.
And then this could fix the problem
And then this could fix the problem
of not being able to rank experience by
of not being able to rank experience by
reward alone.
What does a Q function give you for
What does a Q function give you for
this?
this?
Delay the network. You can,
Delay the network. You can,
but um
but um
what the heck does a Q function even
what the heck does a Q function even
give you?
sample transitions or whatever or
sample transitions or whatever or
segments
this does still not just get stale.
Yes, bet. Like generally what does um
Yes, bet. Like generally what does um
how precisely
how precisely
does a Q function let you go off policy?
does a Q function let you go off policy?
I'm not actually seeing it when I'm
I'm not actually seeing it when I'm
looking at it closely.
Suppose you sample a batch of
Suppose you sample a batch of
experience.
Is it just that it's recursively
Is it just that it's recursively
computed?
I keep thinking of this in terms of
I keep thinking of this in terms of
segments and I forget that a lot of the
segments and I forget that a lot of the
time people are like Q function off off
time people are like Q function off off
policy you're literally just looking at
policy you're literally just looking at
at like state action state or reward or
at like state action state or reward or
whatever.
If this actually makes sense to me
If this actually makes sense to me
that this would make a fix problem.
Yeah. So this is correct because this is
Yeah. So this is correct because this is
QST star which is optimal policy. Okay.
How does this work when you do not have
How does this work when you do not have
QST Are
It's a very different case.
It's a very different case.
The tabular case you can actually prove
The tabular case you can actually prove
that it will converge.
I suppose it's state action state pair
I suppose it's state action state pair
like it's triples or whatever.
It's kind of a world modeling objective
It's kind of a world modeling objective
in some sense, isn't it?
But it's kind of hidden in here, you
But it's kind of hidden in here, you
know,
because you take the difference
because you take the difference
you take the difference of these Q
you take the difference of these Q
functions.
You literally just get a value estimate
You literally just get a value estimate
out of it
versus the idea. I suppose
versus the idea. I suppose
the thing that stays fixed and is policy
the thing that stays fixed and is policy
independent, right? The idea is that if
independent, right? The idea is that if
you're in a state
you're in a state
and you take an action in that state,
and you take an action in that state,
then at least with the same probability,
then at least with the same probability,
you will see the next state, right? Like
you will see the next state, right? Like
that that transition stays the same.
Yes, it's off policy.
But the key actual idea is that the
But the key actual idea is that the
transition like the transitions don't
transition like the transitions don't
change.
change.
That's the thing that you're exploiting.
That's the thing that you're exploiting.
It is not dependent on the policy.
Of course, it's hard to freaking see
Of course, it's hard to freaking see
that and all this math that's like
that and all this math that's like
wrapped up with other stuff,
wrapped up with other stuff,
but I don't think there's anything else
but I don't think there's anything else
magic here.
I think that's the key idea of it.
Let me just see if there's any other
Let me just see if there's any other
framing of it in here.
One.
Okay. So
Okay. So
this is a pure iterative form in a
this is a pure iterative form in a
tabular case, right?
tabular case, right?
Step size Reward at next time step
Step size Reward at next time step
discount factor.
discount factor.
The Q val the maximum uh the val the
The Q val the maximum uh the val the
take the action such if you take the
take the action such if you take the
action such that the Q value is
action such that the Q value is
maximized. What is that value?
maximized. What is that value?
What this piece is and this is at the
What this piece is and this is at the
next state minus the previous state.
So wait,
do you this state
do you this state
take this action gets you to state t +
take this action gets you to state t +
one
maximum action?
Oh wait, wait. There Q directly
Oh wait, wait. There Q directly
approximates Qstar which is the optimal
approximates Qstar which is the optimal
action value function independent of the
action value function independent of the
policy being followed
directly approximate to star which is
directly approximate to star which is
the optimal option
the optimal option
is true in the
is true in the
neural net case
determines which state action didn't
all pairs update
at the guarantee.
Well, clearly you can break it in the um
Well, clearly you can break it in the um
the neural net case, right?
Because here you're only ever updating
Because here you're only ever updating
you're maintaining an estimate for each
you're maintaining an estimate for each
state in action, right? each transition.
state in action, right? each transition.
So like at the very least you can have a
So like at the very least you can have a
continual learning fail in the function
continual learning fail in the function
approximation case, right? If you sample
approximation case, right? If you sample
if you don't sample evenly in your data
one policy that this is not a neural net
one policy that this is not a neural net
bed it's literally a table
bed it's literally a table
the tabular learning case
the tabular learning case
the original this is the original predqn
the original this is the original predqn
algorithm. This is just this is standard
algorithm. This is just this is standard
Q-learning.
And essentially what I'm trying to do
And essentially what I'm trying to do
for the folks who just joined, right, is
for the folks who just joined, right, is
I'm trying to figure out how strong of a
I'm trying to figure out how strong of a
guarantee off policy algorithms actually
guarantee off policy algorithms actually
give you that you can continue to reuse
give you that you can continue to reuse
whatever data you want without breaking
whatever data you want without breaking
it.
I suppose in some sense though it is a
I suppose in some sense though it is a
recursive
like whenever you go to update your Q
like whenever you go to update your Q
function
function
you're not going to have a stale value
you're not going to have a stale value
because like you're calling your value
because like you're calling your value
function again. So you're getting fresh
function again. So you're getting fresh
value data.
value data.
The problem is also it's based on a
The problem is also it's based on a
one-step bootstrap at the same time,
one-step bootstrap at the same time,
right?
So, doesn't this break as soon as you
So, doesn't this break as soon as you
try to do more than one step?
Cuz I think that this makes sense in
Cuz I think that this makes sense in
this like onestep case, right?
this like onestep case, right?
Nothing is ever stale
Nothing is ever stale
because you're evaluating your Q
because you're evaluating your Q
function on this is
function on this is
uh state action reward state action.
uh state action reward state action.
It's SARSA, right?
You have SARSA data like you have state
You have SARSA data like you have state
action reward state action data.
So nothing can ever get stale here.
Now the problem is that like as soon as
Now the problem is that like as soon as
you want to do a multi-step bootstrap
you want to do a multi-step bootstrap
right
right
then you are actually off of your
then you are actually off of your
function.
I'm pretty damn sure at least
I'm pretty damn sure at least
now I was looking at this on the plane.
now I was looking at this on the plane.
There is
There is
there is something that handles this
there is something that handles this
later on. Let me find it.
Yeah. So here
endstep off policy. Let's start with
endstep off policy. Let's start with
this
endstep offs policy.
Okay. So there is an important sampling
Okay. So there is an important sampling
thing that you can do here.
Does this actually fully correct for it
Does this actually fully correct for it
though?
I suppose it does, doesn't it?
Wait,
Wait,
hang on. Hang on. No, there's a there's
hang on. Hang on. No, there's a there's
a gotcha here. Hold on.
Why is the stale one.
Uh yeah, this can blow up.
Can it not? Yeah, this can totally blow
Can it not? Yeah, this can totally blow
up.
Yeah. So, the thing is right this Yes.
Yeah. So, the thing is right this Yes.
Okay. So, here's the issue.
Okay. So, here's the issue.
You can still technically get some
You can still technically get some
information
information
out of uh an old sequence that's with a
out of uh an old sequence that's with a
very different policy.
But like it's not actually
suppose the only case in which it's
suppose the only case in which it's
actually useful is if it's like expert
actually useful is if it's like expert
data, right?
Makes sense if it's expert data.
It does.
So yeah, I mean in that case then right
So yeah, I mean in that case then right
data that's collected with a clumsy
data that's collected with a clumsy
policy
like a segment that's collected from a
like a segment that's collected from a
clumsy policy is simply not going to be
clumsy policy is simply not going to be
useful for anything other than a world
useful for anything other than a world
modeling objective.
But if you have a good segment, let's
But if you have a good segment, let's
say you get lucky even
then you can still kind of use it.
then you can still kind of use it.
If you have expert data,
I think this important sampling ratio
I think this important sampling ratio
actually helps you a lot.
Yes.
solved end policy.
Yeah, that's I did that before today.
Yeah, that's I did that before today.
Bet it's something slightly different.
Bet it's something slightly different.
Um it mostly works if you do that even
Um it mostly works if you do that even
the mo even if you do like the most
the mo even if you do like the most
naive possible way. It's not perfect
naive possible way. It's not perfect
surprisingly
surprisingly
like the behavioral cloning objective is
like the behavioral cloning objective is
surprisingly not just perfect
surprisingly not just perfect
but it does like it does train something
but it does like it does train something
in pretty much all cases.
I mean regardless though, right? Like
I don't think the same convergence
I don't think the same convergence
guarantees hold.
because it's a neural net, right? So, if
because it's a neural net, right? So, if
you if you think of a neural net like a
you if you think of a neural net like a
water balloon
water balloon
or like a you know, whatever, like you
or like a you know, whatever, like you
push it in one area and it's going to
push it in one area and it's going to
blub out in another area.
So like the reason that
So like the reason that
if you train it on like bad historical
if you train it on like bad historical
data, it's going to be bad.
Should it be better? Should it be
Should it be better? Should it be
strictly better to train on more data
strictly better to train on more data
than less data?
Let's say the data set is sufficiently
Let's say the data set is sufficiently
large.
I think the answer in theory is yes, but
I think the answer in theory is yes, but
in practice there are a lot of other
in practice there are a lot of other
factors.
There's no label. The rewards are always
There's no label. The rewards are always
correct. It's ground truth data. RL's
correct. It's ground truth data. RL's
ground truth data always.
I'm asking in the RL case specifically.
You assume the rewards are always
You assume the rewards are always
perfectly correct.
perfectly correct.
You know, it's it's more important in
You know, it's it's more important in
the in this case though, right? It's
the in this case though, right? It's
like the RL case is different here.
So I suppose if you take the supervised
So I suppose if you take the supervised
case then the answer is no. So the
case then the answer is no. So the
answer here
answer here
probably also no for sufficiently
probably also no for sufficiently
degenerate data which you could very
degenerate data which you could very
easily end up getting.
Okay,
we have this endstep off policy. Cool.
And let's see what this is.
Is this actually a clean algorithm?
Is this actually a clean algorithm?
Hang on.
clean as in it's not rainbow. Like Rambo
clean as in it's not rainbow. Like Rambo
has a bunch of crap bolted onto
Why is this
Why is this
bursting?
Hang on. That's sketchy as hell, isn't
Hang on. That's sketchy as hell, isn't
it?
Wait, I
Okay. So, this actually doesn't work.
Okay. So, this actually doesn't work.
Yeah, this does not work. This is
Yeah, this does not work. This is
continuous control specific.
continuous control specific.
That's a good thing that I found that
That's a good thing that I found that
doesn't remotely work out of the box.
doesn't remotely work out of the box.
Okay.
have to read a bunch of this map at some
have to read a bunch of this map at some
point. This definitely is not an out of
point. This definitely is not an out of
the box. It's a good thing that I found
the box. It's a good thing that I found
that ablation.
on this.
It is
We get rid of dueling doesn't do
We get rid of dueling doesn't do
anything.
Does a little bit something
Does a little bit something
get rid of double
double is funny that
up.
A
lot of these hurt.
It's actually funny. The distributional
It's actually funny. The distributional
one gets the closest.
one gets the closest.
You just took a couple of these tricks.
Prioritize replay plus
Prioritize replay plus
probably get most of it.
Move dual.
Only four tricks.
Only four tricks.
Cut out these dumb ones.
How many of these do I have in our
How many of these do I have in our
current thing?
current thing?
We have priority. We have multistep.
What's noisy?
distribution is what um Spencer was
distribution is what um Spencer was
doing. What is noisy?
The castic network layers for
The castic network layers for
exploration. That's horrifying.
But we kind of have most of these now,
But we kind of have most of these now,
don't we?
How do you do um
How do you do um
what's the continuous sample look like
what's the continuous sample look like
for a Q function?
for a Q function?
Is it bucketed?
I'd like to try
It's funny though that they you get rid
It's funny though that they you get rid
of DDQN and it still works.
It's actually cleaner, isn't it?
4:00.
4:00.
Okay. Well,
I think it makes sense for me to start
I think it makes sense for me to start
messing around with off Paul tomorrow.
messing around with off Paul tomorrow.
Um
definitely something being able to keep
definitely something being able to keep
data around.
Likewise,
Likewise,
I haven't been able to get any form of
I haven't been able to get any form of
um pure imitation
um pure imitation
like best of end data.
like best of end data.
be remotely as close
be remotely as close
on polic as like on policy RL.
Suppose the question then should be
if I replace the on policy formulation
if I replace the on policy formulation
with an off policy formulation
with an off policy formulation
something way closer that should be
something way closer that should be
theoretically even almost directly
theoretically even almost directly
comparable
comparable
what happens
what like kind of haven't done that
what like kind of haven't done that
sensors done HL gaus for the uh the
sensors done HL gaus for the uh the
value function specifically But that's
value function specifically But that's
kind of a different thing.
You don't need HL Gaus on Hang on. What
You don't need HL Gaus on Hang on. What
is What is the distributional? Let me
is What is the distributional? Let me
look up this last thing.
I see
Oh, I think I get it. Yeah, because you
Oh, I think I get it. Yeah, because you
get a quadratic loss with uh standard
get a quadratic loss with uh standard
DQN, right?
DQN, right?
You actually want something like HL
You actually want something like HL
Gaus.
Gaus.
Let's see.
Yeah, this one
G for this
test. Okay,
message Spencer on this real quick.
Yeah, funny.
Yeah, funny.
I mentioned it for this action thing.
I mentioned it for this action thing.
All
All
right,
right,
that's pretty much a good view. Let me
that's pretty much a good view. Let me
let me find Spencer.
Okay. So, we have a few extra hours.
Okay. So, we have a few extra hours.
I think what we can do is we can start
I think what we can do is we can start
on the other new project.
There are two projects that I came away
There are two projects that I came away
from RLC with. Right. One is actually
from RLC with. Right. One is actually
figuring out the sample efficiency
figuring out the sample efficiency
thing. You see I've been doing imitation
thing. You see I've been doing imitation
when a try off policy. There are lots of
when a try off policy. There are lots of
things to mess around with there, right?
things to mess around with there, right?
Um, the other thing is getting an
Um, the other thing is getting an
actually clean test suite of
actually clean test suite of
environments for really determining
environments for really determining
which of this works and doesn't.
So there's this paper which is
So there's this paper which is
a well motivated but well motivated but
a well motivated but well motivated but
with a clumsy execution in my mind.
So what they try to do is they try to
So what they try to do is they try to
come up with environments that measure
come up with environments that measure
agent capabilities like Pokemon on this
agent capabilities like Pokemon on this
like well what will forever be in my
like well what will forever be in my
mind the Pokemon chart because that's
mind the Pokemon chart because that's
where I first saw it. Um, but these
where I first saw it. Um, but these
different axes,
different axes,
like I don't think these are actually
like I don't think these are actually
the axes
the axes
that you want.
Make a dock for
Oh, perfect. Spencer's running sweeps.
for puffer bench. Let's think of some
for puffer bench. Let's think of some
things here. Um,
let's just type some keywords in. So,
let's just type some keywords in. So,
they they talk about like
they they talk about like
generalization,
generalization,
exploration,
exploration,
and more
and more
ways.
credit
credit
I don't think this is a good basis
I don't think this is a good basis
because in my mind right
what's this so this is
what's this so this is
robustness noise right
just take these out immediately
Credit assignment require memory?
It does, doesn't it?
Well, it doesn't require the agent to
Well, it doesn't require the agent to
have memory. It requires the algorithm
have memory. It requires the algorithm
to have like a multi-step
to have like a multi-step
thing to it.
But in our case, right, actual
But in our case, right, actual
intelligence,
intelligence,
credit assignment requires memory
credit assignment requires memory
fundamentally.
memory.
Credit assignment as a problem is
Credit assignment as a problem is
actually not a component vector, is it?
Like credit assignment is a higher level
Like credit assignment is a higher level
topic in some sense, right?
or is it fundamental?
How would you set up an RL problem that
How would you set up an RL problem that
does not require credit assignment?
Are you limited to like one-step
Are you limited to like one-step
problems?
Credit assignment is like a blanket term
Credit assignment is like a blanket term
now.
and let me answer one message about
and let me answer one message about
this.
These are bad. Okay, let me sort of
These are bad. Okay, let me sort of
explain what I'm thinking here.
explain what I'm thinking here.
Uh the problem is that these are bad
Uh the problem is that these are bad
basis vector, right? This was like if I
basis vector, right? This was like if I
were instead of giving you uh if I'm
were instead of giving you uh if I'm
trying to cover a space and in this case
trying to cover a space and in this case
the space we're trying to cover is like
the space we're trying to cover is like
difficulty in RL and instead of giving
difficulty in RL and instead of giving
you like X Y Z I give you like 2X + Z
you like X Y Z I give you like 2X + Z
and like X - Y + 3 Z and then like 3 Z
and like X - Y + 3 Z and then like 3 Z
minus Y or something that's even
minus Y or something that's even
independent like yeah probably is has to
independent like yeah probably is has to
be. But yeah, it's just like a really
be. But yeah, it's just like a really
crappy basis.
We're trying to think like what are the
We're trying to think like what are the
actual I like in order for something to
actual I like in order for something to
be able basis vector we have to be able
be able basis vector we have to be able
to test it on its own cleanly somehow
I suppose one of the things that messes
I suppose one of the things that messes
with this, right, is there's a
with this, right, is there's a
difference between the algorithm being
difference between the algorithm being
able to do credit assignment and uh the
able to do credit assignment and uh the
agent being able to do credit
agent being able to do credit
assignment, right? Because they have
assignment, right? Because they have
RNN boot DQN. Where is this?
RNN boot DQN. Where is this?
Is this
charts
credit assignment? Okay, so this is D
credit assignment? Okay, so this is D
this is tiny but this is DQN having
this is tiny but this is DQN having
positive score in all the credit
positive score in all the credit
assignment tasks. So basically what they
assignment tasks. So basically what they
they have this umbrella length task
they have this umbrella length task
where it's like you you have to make a
where it's like you you have to make a
decision and then you only get the
decision and then you only get the
reward a long time after that. So an
reward a long time after that. So an
algorithm can do credit assignment even
algorithm can do credit assignment even
if the agent has no idea.
if the agent has no idea.
The agent is taught to make the right
The agent is taught to make the right
decision based on the context but it it
decision based on the context but it it
itself cannot do credit assignment which
itself cannot do credit assignment which
means it can't do in context learning or
means it can't do in context learning or
anything either.
Maybe I looking at this the correct way.
Maybe I looking at this the correct way.
Instead, think about hard problems that
Instead, think about hard problems that
we solve and what makes them hard.
is counting horizon.
What if we wanted to build
like what are the actual hard problems
like what are the actual hard problems
about?
I kind of want to just like
I kind of want to just like
can I instead think of like very simple
can I instead think of like very simple
games that has stuff that's not in
games that has stuff that's not in
Puffer Lab.
What if we do like a recipe game?
What if we do like a recipe game?
All right.
Each reset has several rounds.
Each reset has several rounds.
Round presents some ingredients.
Round
Round
and
target item.
Um,
a damn
that does what's this thing test?
that does what's this thing test?
This thing tests
tests in context
tests in context
learning for sure.
Is that any different though?
Or you could have all the ingredients.
Yeah, you could have all the
Yeah, you could have all the
ingredients, right?
Okay, I think that this one is decent.
This is like a cool little nifty game
This is like a cool little nifty game
and I come up with a few others like
and I come up with a few others like
this.
This is this is like this in context
This is this is like this in context
learning type thing.
kind of metaly and context learny
kind of metaly and context learny
whatever.
Is there something I can do to test like
Is there something I can do to test like
really long games?
Because progressive games have like
Because progressive games have like
there's several challenges to them,
there's several challenges to them,
right?
Do I want to test pure memory though?
puffer be applied
puffer be applied
basically anything that you know to make
basically anything that you know to make
it better. Uh we focus on fast
it better. Uh we focus on fast
environments. So like all our defaults
environments. So like all our defaults
are optimized around environments being
are optimized around environments being
fast. Obviously, we have ways to make a
fast. Obviously, we have ways to make a
lot of environments fast, but yeah, it
lot of environments fast, but yeah, it
can be a lot applied to a lot of stuff
can be a lot applied to a lot of stuff
like our implementation is just
like our implementation is just
generally better. It's not exactly PO.
generally better. It's not exactly PO.
It's a better algorithm.
I like this recipe game.
Generalization,
Generalization,
exploration,
exploration,
memory,
Okay, there's control as a problem
sweet test scenarios pretty much.
binding peptide generation.
I don't know what the simulator used is
I don't know what the simulator used is
for that. If there if that's a simulator
for that. If there if that's a simulator
in a ye a loop, then um
in a ye a loop, then um
I mean like molecular simulation is
I mean like molecular simulation is
probably the thing I'm most interested
probably the thing I'm most interested
in overall in RL. It's just hard.
Really is the most interesting thing out
Really is the most interesting thing out
there.
What is something like So bon
It's hard to actually come up with very
It's hard to actually come up with very
good test agents, right? Takes a lot of
good test agents, right? Takes a lot of
thought.
Oh, literally that concept merger game
Oh, literally that concept merger game
does this, doesn't it?
There's not going to be a mathematical
There's not going to be a mathematical
categorization like that.
Like you could do that, but that's not
Like you could do that, but that's not
actually the important one.
Okay, an alternative version of this
Okay, an alternative version of this
would be like
I don't know if this one has enough
I don't know if this one has enough
structure.
Yeah, I don't know if that one has
Yeah, I don't know if that one has
enough structure. The recipe game sounds
enough structure. The recipe game sounds
better to me.
Are these not the same game?
Oh yeah, these literally are the same
Oh yeah, these literally are the same
game, aren't they?
game, aren't they?
Yeah, these literally are two different
Yeah, these literally are two different
just flavors of the same game.
That's exploration in some sense, isn't
That's exploration in some sense, isn't
it?
That's exploration. That's in context
That's exploration. That's in context
learning
that actually tests a lot of good stuff.
that actually tests a lot of good stuff.
A bunch of related ideas.
There's precise control.
Precise control even really a thing?
load down to you.
There's also the really long horizon
There's also the really long horizon
stuff. How do we do that?
Like a lot of these tasks are kind of
Like a lot of these tasks are kind of
hard just because
hard just because
they change a little bit over time.
they change a little bit over time.
Horizons are really long and you
Horizons are really long and you
introduce new types of stuff over long
introduce new types of stuff over long
horizons. That's true of neural MMO.
What are some other environments that I
What are some other environments that I
liked that I've done before? Right.
liked that I've done before? Right.
So, I've done stuff like passcodes, like
So, I've done stuff like passcodes, like
guess a passcodes, like memory related.
That's an exploration problem as well in
That's an exploration problem as well in
some sense.
How is this different from recipe game?
How is this different from recipe game?
Recipe game has an additional element to
Recipe game has an additional element to
it.
Password guessing game is pure
Password guessing game is pure
exploration is interesting.
not pure exploration. It's also memory.
There is really no credit assignment
There is really no credit assignment
though, which is kind of interesting.
The two things missing.
The two things missing.
What other things are missing? Really?
What other things are missing? Really?
Like the length and progression of a
Like the length and progression of a
game.
Hang on. No, you can actually get you
Hang on. No, you can actually get you
can get both of these in this. You can
can get both of these in this. You can
actually get both of these.
actually get both of these.
always say Tetris looks nice. It does.
always say Tetris looks nice. It does.
Tetris looks very nice.
Length and progression we can get just
Length and progression we can get just
by making these very long, right?
by making these very long, right?
The recipe game in particular.
Okay, let's say that we have a game
Let's say that we have a game in which
Let's say that we have a game in which
we have
we have like a bunch of ingredients,
we have like a bunch of ingredients,
right?
right?
Let's say it's like a decent number of
Let's say it's like a decent number of
them like this.
Like suppose we extend this, right?
Like suppose we extend this, right?
Let's say that we start off with five
Let's say that we start off with five
ingredients, okay?
ingredients, okay?
And
And
we need to make a two slot recipe.
You pick two of these, fill the slots,
You pick two of these, fill the slots,
and you get out a result.
and you get out a result.
And the recipes don't change. So they're
And the recipes don't change. So they're
already set. You just don't know what
already set. You just don't know what
they are.
All right.
And then you know we do some of these
And then you know we do some of these
you end up getting
always do two slot recipes more than two
always do two slot recipes more than two
slot recipes.
And it could be that
you end up being able to use
you end up being able to use
products that you've made before. So the
products that you've made before. So the
initial set of recipes can use anything
initial set of recipes can use anything
in here and then whenever you make a
in here and then whenever you make a
product it can also use that
And the game can progress.
And the game can progress.
At a certain point, you unlock new
At a certain point, you unlock new
ingredients to use.
ingredients to use.
Now, there are a whole bunch of new
Now, there are a whole bunch of new
recipes that can also use these.
Keep going with this.
What
What
What qualities does this game have?
What qualities does this game have?
Well, obviously it requires memory.
It requires exploration.
The combination of both of these over
The combination of both of these over
several rounds is what you would call in
several rounds is what you would call in
context learning. Right?
context learning. Right?
So, it does require in context learning.
There is a concrete strategy from game
There is a concrete strategy from game
to game that you can learn.
It has the potential to scale to very
It has the potential to scale to very
long horizons.
long horizons.
It's a progressive game where you
It's a progressive game where you
actually have to play it for a while to
actually have to play it for a while to
see all the content, which is one of the
see all the content, which is one of the
key things that's difficult because like
key things that's difficult because like
you have to play through the early game
you have to play through the early game
to get to the end game.
You get dramatically less experience at
You get dramatically less experience at
the end game unless you get good at
the end game unless you get good at
getting there quickly.
It doesn't have any control aspect.
Main thing missing is like some sort of
Main thing missing is like some sort of
control aspect.
Now I suppose the thing that this
Now I suppose the thing that this
doesn't do, right? This gives you like a
doesn't do, right? This gives you like a
good total task.
The thing that this doesn't do is
The thing that this doesn't do is
separate them into individual tests.
Like it's possible we end up with a
Like it's possible we end up with a
really good environment for research.
Nice thing about it is you only have to
Nice thing about it is you only have to
run one hyperparameters. Well,
but disentangling these is very
but disentangling these is very
different.
Memory is pretty easy to test on a
or no because it's still credit
or no because it's still credit
assignment.
How do you test memory without credit
How do you test memory without credit
assignment in RL? Right.
up with a good joint test is actually
there a reason that you want to separate
there a reason that you want to separate
these
can't come up with a memory independent
can't come up with a memory independent
of credit assignment task.
of credit assignment task.
They're basically
They're basically
completely coingled.
Okay.
What about exploration?
What does exploration mean?
Again, I don't think it has meaning on
Again, I don't think it has meaning on
its own. It has to be
its own. It has to be
It has to mean learning an exploration
It has to mean learning an exploration
strategy more so than the algorithm
strategy more so than the algorithm
having one. Right?
The recipe game counts as that.
We could come up with a purer test.
Oh,
Oh,
Emory
could do
Red
passive.
passive.
What variation is that bet?
Oh,
so that would mean like the algorithm
so that would mean like the algorithm
itself that would be uh exploration
itself that would be uh exploration
encoded at the algorithm level.
encoded at the algorithm level.
The RL one though that's for memory
The RL one though that's for memory
independent
Let me think.
I guess there's like a question of what
I guess there's like a question of what
it means for what it means for an
it means for what it means for an
algorithm to be good at exploration
algorithm to be good at exploration
versus
Okay.
Oh, wait. Yeah, you can do this, right?
Oh, wait. Yeah, you can do this, right?
Because
Oh, yeah. You just do like the umbrella
Oh, yeah. You just do like the umbrella
problem, right? Yeah. It's like, okay,
problem, right? Yeah. It's like, okay,
you just do the memory problem, but you
you just do the memory problem, but you
can observe your choice.
Right.
Wait, actually, what did they do for
Wait, actually, what did they do for
pure memory?
Memory cards.
A long teammates.
Oh, is it because for the final action
Oh, is it because for the final action
you immediately get
I see you just flip it. So, it's like
I see you just flip it. So, it's like
you get presented the thing at the
you get presented the thing at the
start.
You have to remember it the full way,
You have to remember it the full way,
but you get rewarded instantly.
And then credit assignment. Your action
And then credit assignment. Your action
doesn't matter at the end. Okay. So, we
doesn't matter at the end. Okay. So, we
can actually do this. Yeah. Yeah, we can
can actually do this. Yeah. Yeah, we can
actually do this.
actually do this.
So for so for memory
So for so for memory
all you need to do is um
first
final action sides
take
credit
first action.
Okay, so you can actually perfectly
Okay, so you can actually perfectly
isolate memory and credit assignment.
isolate memory and credit assignment.
Okay, that's fine.
I don't know if there are any of these
I don't know if there are any of these
others that you can actually isolate so
others that you can actually isolate so
cleanly.
Honestly, you can very easily come up
Honestly, you can very easily come up
with a task that requires exploration
with a task that requires exploration
and
and
like exploration has to be learned.
like exploration has to be learned.
The password game requires exploration.
The password game requires exploration.
The um recipe game requires exploration.
Password game also requires memory.
Well, I suppose it depends, right?
You could make a version of it that
I guess there are like a few different
I guess there are like a few different
versions of this people talk about like
versions of this people talk about like
what about like just the big empty room
what about like just the big empty room
thing.
Yeah, it's difficult to come up with a
Yeah, it's difficult to come up with a
good version of that, right?
good version of that, right?
What's like an actual reasonable
What's like an actual reasonable
exploration task?
I guess there's there's the learn an
I guess there's there's the learn an
exploration strategy and uh there's the
exploration strategy and uh there's the
does your learning algorithm have an
does your learning algorithm have an
exploration strategy in it regardless of
exploration strategy in it regardless of
reward I
Tough to think what to what extent I
Tough to think what to what extent I
care about these. I guess
RC. today.
I could frame a lot of these as lock
I could frame a lot of these as lock
games, couldn't I?
Think about this.
Depending if I can like phrase these all
Depending if I can like phrase these all
is like passcode guessing games somehow.
This would actually be cool because I
This would actually be cool because I
could potentially test them all at the
could potentially test them all at the
same time, right?
Generalization.
Generalization.
General
I'm trying to think what the way that
I'm trying to think what the way that
you structure something like right
just have the bar
You can observe just have a sequence
You can observe just have a sequence
that you can observe.
First action decides yeah given many
Let me see how this would work. Right?
So if we have something like this
have like
have like
tape
tape
like your clue
your final action decides which one is
your final action decides which one is
given on the first like shade this this
given on the first like shade this this
is essentially
take this action
This is shown on the first
you get any results from IIL. Uh we got
you get any results from IIL. Uh we got
some crazy results but not enough to
some crazy results but not enough to
like go sub it in just right away. So we
like go sub it in just right away. So we
have it working like surprisingly well
have it working like surprisingly well
on some hard environments. It does
on some hard environments. It does
really well on the first chunk of neural
really well on the first chunk of neural
MMO 3. It instas solves uh cartpole and
MMO 3. It instas solves uh cartpole and
pong, but then like it does very poorly.
pong, but then like it does very poorly.
It does better than random on most
It does better than random on most
stuff, but like it does very poorly on
stuff, but like it does very poorly on
breakout and triple triad and a bunch of
breakout and triple triad and a bunch of
other tasks. So,
other tasks. So,
I think that um we definitely have
I think that um we definitely have
learned from this that we need to be
learned from this that we need to be
able to reuse samples, especially the
able to reuse samples, especially the
high value ones,
high value ones,
and that the objective is pretty good.
and that the objective is pretty good.
But it's going to take some time to
But it's going to take some time to
figure out how to integrate that nicely.
figure out how to integrate that nicely.
I'm taking a little break at the moment
I'm taking a little break at the moment
and I'm working on
and I'm working on
um some benchmark environments.
Okay, so the clue can change on each
Okay, so the clue can change on each
time step.
Loop can change on each time stamp.
Passcode is random. Password doesn't
Passcode is random. Password doesn't
change.
that in context learning.
How's in context learning different from
How's in context learning different from
memory?
An application of memory, right?
You pushed the code somewhere so I can
You pushed the code somewhere so I can
check that out. Uh the slightly old
check that out. Uh the slightly old
version is on I believe the imitate
version is on I believe the imitate
branch
branch
and I will push this stuff today as
and I will push this stuff today as
well.
just maintain my train of thought a
just maintain my train of thought a
little bit here. Uh, this segment of the
little bit here. Uh, this segment of the
stream is probably fairly boring, but
stream is probably fairly boring, but
it's very important because I'm like I'm
it's very important because I'm like I'm
trying to think about uh simple test
trying to think about uh simple test
environments for specific things.
environments for specific things.
I'd like to have some more of these.
I'd like to have some more of these.
basically an a puffer ocean sanity uh V2
basically an a puffer ocean sanity uh V2
Hill.
It's in puffer lib.
It's in puffer lib.
It's just a branch. Imitate branch.
I'll push all the new stuff. I added in
I'll push all the new stuff. I added in
like an expert model so that you can
like an expert model so that you can
like load in an expert if you want and
like load in an expert if you want and
like test imitation on that to see that
like test imitation on that to see that
mostly works.
You hear?
problem.
problem.
Yeah,
Yeah,
funny
just solved everything. I don't know.
Um, yeah, that's kind of obnoxious to
Um, yeah, that's kind of obnoxious to
do. Bet.
Well, one, it's pedantic, and two, I
Well, one, it's pedantic, and two, I
guarantee you a bunch of people aren't
guarantee you a bunch of people aren't
going to want to like
going to want to like
have to prepare an oral defense of their
have to prepare an oral defense of their
code on like live on the I am.
That is mostly how an oral defense
That is mostly how an oral defense
works.
help me figure out how to do these
help me figure out how to do these
environments instead.
Um,
there are several tricky things here,
there are several tricky things here,
right?
Not quite exploration of the same bloody
Not quite exploration of the same bloody
sequence of actions is always correct.
Well, it's not quite
the memory and credit assignment ones
the memory and credit assignment ones
are solid.
are solid.
I think the sparsity one is good.
I think the sparsity one is good.
Why rust and the devs are fighting on X?
Why rust and the devs are fighting on X?
They have nothing better to do.
can go grab some of the C devs and they
can go grab some of the C devs and they
can come work on this if they know what
can come work on this if they know what
they're
The pros of Rust are that you feel
The pros of Rust are that you feel
smart. Cons are that you're not. The
smart. Cons are that you're not. The
pros of C are that uh you are smart. The
pros of C are that uh you are smart. The
cons are you don't feel smart. Take your
cons are you don't feel smart. Take your
pick.
Totally not bait.
the exploration task. Oh,
Yeah, I'm trying to think if there's a
Yeah, I'm trying to think if there's a
better way of doing any of this stuff.
See this is still not memory
See this is still not memory
actually. Your final action decides
actually. Your final action decides
reward. Which one to take is given on
reward. Which one to take is given on
the first step.
the first step.
You still have to do credit assignment,
You still have to do credit assignment,
don't you?
Or association.
You still have to do association.
Am I wrong? Bet.
Am I wrong? Bet.
The credit assignment one works. Your
The credit assignment one works. Your
first action decides reward which is
first action decides reward which is
then given many steps later.
The credit assignment task is correct.
The credit assignment task is correct.
The memory task seems wrong to me.
Why the no op? What do you mean?
So the memory game is supposed to be I
So the memory game is supposed to be I
tell you what like what action to take
tell you what like what action to take
on the last step and I tell you it on
on the last step and I tell you it on
the first step and nothing else matters.
the first step and nothing else matters.
The problem though is that you still
The problem though is that you still
have to understand the instruction. So
have to understand the instruction. So
like if you don't understand the
like if you don't understand the
instruction that's not a pure test of
instruction that's not a pure test of
memory. So it doesn't work. tabularasa.
Yeah, we're using RL Well,
minimal tasks I think are more useful
minimal tasks I think are more useful
for diagnosing like specific
for diagnosing like specific
architecture deficiencies. It's like I
architecture deficiencies. It's like I
have a memory one that's linked to
have a memory one that's linked to
credit assignment that's kind of decent
credit assignment that's kind of decent
cuz it's like a fundamental check.
cuz it's like a fundamental check.
I don't know if the other ones are even
I don't know if the other ones are even
good.
I kind of like this recipe game
reset has several rounds.
reset has several rounds.
You can increase sparsity by just
You can increase sparsity by just
changing the number of ingredients.
The only other thing we haven't done is
The only other thing we haven't done is
like control, I guess, right?
You don't want to introduce a control
You don't want to introduce a control
problem to this.
problem to this.
Doesn't this do everything but control?
Doesn't this do everything but control?
Actually, let's see. Is there anything
Actually, let's see. Is there anything
that is in any of these M that is not
that is in any of these M that is not
control that is not covered by this? So,
control that is not covered by this? So,
what does neural MMO require you to do?
what does neural MMO require you to do?
You have to explore. There's sparcity.
You have to explore. There's sparcity.
You have to learn to fight agents.
You have to learn to fight agents.
They're like associations that go over
They're like associations that go over
time. I suppose
the associations over time part.
Is this it?
They tell you the recipe.
Yeah, it's different though.
Oh, I suppose. Yeah, it doesn't have any
Oh, I suppose. Yeah, it doesn't have any
of the multi-agent in it yet, right?
So this is not
Near MMO has some of these actions are
Near MMO has some of these actions are
poorly extended.
poorly extended.
I suppose the recipes get longer.
Never get that long though, do they?
It's kind of interesting that like
It's kind of interesting that like
making a pure task for this is hard.
Well, of course it's hard, right? We
Well, of course it's hard, right? We
wouldn't need all these tasks
wouldn't need all these tasks
otherwise.
It's completely missing control.
Like honestly, outside of these little
Like honestly, outside of these little
probe environments, I'm actually
probe environments, I'm actually
reconsidering the idea,
reconsidering the idea,
like isn't literally just having these
like isn't literally just having these
different little RL environments better?
different little RL environments better?
These different game environments?
These different game environments?
I still think the recipe environment is
I still think the recipe environment is
on its own pretty reasonable.
like in context learning as well.
Good.
Okay. Well, I like the recipe game. I
Okay. Well, I like the recipe game. I
like the idea of sprucing up some of the
like the idea of sprucing up some of the
individual ocean like sparse like test
individual ocean like sparse like test
environments.
environments.
I have a decent number of ideas here.
I have a decent number of ideas here.
Enough that I could start implementing
Enough that I could start implementing
stuff.
stuff.
It is it's only it's 5:30 still.
It is it's only it's 5:30 still.
Let me go check couple quick things off
Let me go check couple quick things off
to the side.
Okay. Um,
Okay. Um,
want to start on these.
I want to try
also. Do I want to have
separate C environments forities
kind of tired for today. I've thought a
kind of tired for today. I've thought a
lot about a lot of different things
lot about a lot of different things
and done a lot of thinking about all
and done a lot of thinking about all
this stuff.
this stuff.
Um
a quick look at what we have in ocean
a quick look at what we have in ocean
right now.
right now.
Oh, also commit this right.
All right.
Bandit memory multi- aent password
Bandit memory multi- aent password
spaces
spaces
all the environments. Could
modernize uh these up for sure.
More
heat.
There is an alchemy benchmark.
Oh, wait. There is this is a thing.
Well, I mean, this is a good idea
Well, I mean, this is a good idea
implemented in the stupidest possible
implemented in the stupidest possible
way.
Deep mind.
Do this in freaking unity. You got to be
Do this in freaking unity. You got to be
kidding me.
Yep.
Cool.
All right. Well, there's literally a
All right. Well, there's literally a
basis for this thing. How many citations
basis for this thing. How many citations
this paper have? 43.
That's so funny. Wait, Nurup's 2025.
That's so funny. Wait, Nurup's 2025.
This is 2021.
Did they then submit this? Oh, no. This
Did they then submit this? Oh, no. This
is 2021. Okay.
is 2021. Okay.
It's It has the date wrong.
Why is it in the proceedings 2025?
Why is it in the proceedings 2025?
No, it says papers 2021. Okay, it's just
No, it says papers 2021. Okay, it's just
mislabeled. It's not V2. It's just
mislabeled. It's not V2. It's just
mislabeled. Well, of course, nobody ever
mislabeled. Well, of course, nobody ever
used it, right? Like literally nobody
used it, right? Like literally nobody
used it cuz they implemented it in the
used it cuz they implemented it in the
dumbest possible way. Like imagine like
dumbest possible way. Like imagine like
imagine having a good idea and then
imagine having a good idea and then
implementing it as a Unity
implementing it as a Unity
environment. Like are you insane?
I can still take the name Alchemy for
I can still take the name Alchemy for
this.
I find it very funny though that like I
I find it very funny though that like I
come up with the name alchemy for this.
come up with the name alchemy for this.
I Google alchemy RL just to see and then
I Google alchemy RL just to see and then
somebody has a very very similar idea
somebody has a very very similar idea
under the same name already just not
under the same name already just not
executed well.
It's It's more just they don't have good
It's It's more just they don't have good
um I think they have engineers and they
um I think they have engineers and they
have scientists, but like they don't
have scientists, but like they don't
actually think about what the other
actually think about what the other
person needs at all.
annexating the current. Yeah,
it's just kind of stupid if you think
it's just kind of stupid if you think
about it, right?
Next time I'm trying to do is something
Next time I'm trying to do is something
stupid
stupid
genie except for puffer. I'm making
genie except for puffer. I'm making
actions the output.
actions the output.
What
actions space the output?
Yeah, if you're going to use RL to learn
Yeah, if you're going to use RL to learn
a world model, that's a terrible idea.
a world model, that's a terrible idea.
Um, what you would do is you would do
Um, what you would do is you would do
that at the algorithm level. If you want
that at the algorithm level. If you want
to do a world modeling objective, you do
to do a world modeling objective, you do
that at the algorithm level. And you can
that at the algorithm level. And you can
do it on pixels or not pixels.
do it on pixels or not pixels.
World models are not constrained to just
World models are not constrained to just
pixels. They can work on different obs
pixels. They can work on different obs
types.
Ideally though know a lot like sometimes
Ideally though know a lot like sometimes
you do world model objectives not in
you do world model objectives not in
pixel space or in obsp space you do it
pixel space or in obsp space you do it
in embedding space so that you don't
in embedding space so that you don't
have to deal with the reconstruction as
probably
probably
well I mean it depends like if you come
well I mean it depends like if you come
up with some sort of world model thing
up with some sort of world model thing
that's actually generally useful on the
that's actually generally useful on the
algorithm side then yes but that's hard
algorithm side then yes but that's hard
to do that's a very high bar
and just think of like some basic stuff
and just think of like some basic stuff
I'm going to want in here So
I'm going to want in here So
you have
Um
want recipe. What do we want? I want to
want recipe. What do we want? I want to
do this.
make the recipe big array.
No, we can't make the recipes because we
No, we can't make the recipes because we
array because we have to check them,
array because we have to check them,
right?
Check them.
Make a big ND array. Don't you
You'd make the recipes would be a big ND
You'd make the recipes would be a big ND
array.
want to restrict it to two ingredients.
Hello.
Well, we would just do like
Well, we would just do like
We're not going to have a huge number of
We're not going to have a huge number of
different ones, right?
Just do 2D.
Just do 2D.
probably interesting enough, right?
Okay,
this
we have to generate all the recipes,
we have to generate all the recipes,
right?
going to have to come up with a
going to have to come up with a
generation algorithm. I'm sure
this
What's the Maybe we just do this top to
What's the Maybe we just do this top to
bottom like let's start on the
bottom like let's start on the
generation algorithm.
Okay. So,
Okay. So,
know what level we're in.
know what level we're in.
know how many
products There.
do products per level or something as
do products per level or something as
well.
well.
But like it's going to be something to
But like it's going to be something to
the effect of
All
right. And so you just pick two
right. And so you just pick two
ingredients
the product. We have to think through
the product. We have to think through
the dynamics quite a bit of this.
the dynamics quite a bit of this.
Hello. How's it going?
Hello. How's it going?
Hey, Spencer. Yeah, it's um
Hey, Spencer. Yeah, it's um
it's a dedicated test environment for
it's a dedicated test environment for
some things that's kind of complicated,
some things that's kind of complicated,
but I don't know, not that bad.
but I don't know, not that bad.
There's actually kind of precedent for
There's actually kind of precedent for
it as well. There's an old Deep Mind
it as well. There's an old Deep Mind
paper that's kind of similar. I came up
paper that's kind of similar. I came up
with the idea, then I Googled to see if
with the idea, then I Googled to see if
anybody had done it, and there's like
anybody had done it, and there's like
something similar, but of course it's
something similar, but of course it's
like this one. That one's like a 100,000
like this one. That one's like a 100,000
times slower than it should be.
I got some pretty cool results though
I got some pretty cool results though
throughout the day and I'm about to end
throughout the day and I'm about to end
stream because it's sick. So, I'm going
stream because it's sick. So, I'm going
to go get dinner. Um, but yeah, Spencer,
to go get dinner. Um, but yeah, Spencer,
I got a lot of uh interesting results.
I got a lot of uh interesting results.
Cracked out HL G for more
Cracked out HL G for more
eight.
eight.
Um,
maybe we try both.
maybe we try both.
Did it push VF? If it pushed clipping to
Did it push VF? If it pushed clipping to
like really low, then don't I don't want
like really low, then don't I don't want
that one.
that one.
Clipping really low usually means like
Clipping really low usually means like
cracked out, unstable, like completely
cracked out, unstable, like completely
unreproducible.
I want to do like off policy learning
I want to do like off policy learning
stuff tomorrow.
What about the clipping? Oh, yeah.
What about the clipping? Oh, yeah.
You're right. That might be a problem.
You're right. That might be a problem.
Like not clipping the loss somehow.
Yeah.
Yeah.
Well, try the slightly more conservative
Well, try the slightly more conservative
one. Um, we can try both to be fair. I
one. Um, we can try both to be fair. I
just want to run it on the uh the 5090
just want to run it on the uh the 5090
bin max value bins.
bin max value bins.
Yeah, a different kind type of clipping
Yeah, a different kind type of clipping
though. That's that's prediction space
though. That's that's prediction space
clipping. I mean lost base clipping.
clipping. I mean lost base clipping.
Okay.
have that up here one. Cool.
have that up here one. Cool.
Um,
Um,
yeah, I'm going to want to try to play
yeah, I'm going to want to try to play
with that tomorrow. I'm pretty tired at
with that tomorrow. I'm pretty tired at
the moment. So, stuff I did today for
the moment. So, stuff I did today for
record Spencer. Um, so pure imitation
record Spencer. Um, so pure imitation
learning with no reinforcement learning
learning with no reinforcement learning
can get like 67% on tower climb.
can get like 67% on tower climb.
like the same number of steps and
like the same number of steps and
everything.
It also solves the first chunk of neural
It also solves the first chunk of neural
MMO faster than our current RL and it
MMO faster than our current RL and it
solves cart pull instantly and pulling
solves cart pull instantly and pulling
instant.
instant.
It fails hard on a lot of other ends,
It fails hard on a lot of other ends,
even simple ones, though. So, I don't
even simple ones, though. So, I don't
think necessarily this is going to be
think necessarily this is going to be
like, oh yeah, we immediately just use
like, oh yeah, we immediately just use
this. But I think this is pretty strong
this. But I think this is pretty strong
evidence that if we get an off policy
evidence that if we get an off policy
thing or like an auxiliary thing working
thing or like an auxiliary thing working
that uh we can definitely use past data
that uh we can definitely use past data
in a way that's very effective.
It's pretty nutty.
I also found some good references.
I also found some good references.
I have one question to email uh to send
I have one question to email uh to send
via email to someone but um
via email to someone but um
yeah
yeah
suffice to say it's I mean research
suffice to say it's I mean research
progress is always slow in the sense
progress is always slow in the sense
that like no nothing just like works out
that like no nothing just like works out
of the box on everything yet but we got
of the box on everything yet but we got
some cool information from today. It is
some cool information from today. It is
very tiring. I also did a lot of work
very tiring. I also did a lot of work
looking through identity tasks and like
looking through identity tasks and like
B Suite stuff and like how we can
B Suite stuff and like how we can
isolate different tasks and I got some
isolate different tasks and I got some
ideas for that as well as one idea for
ideas for that as well as one idea for
this like combined environment that
this like combined environment that
would be a very good benchmark for uh I
would be a very good benchmark for uh I
think it'll just end up being a very
think it'll just end up being a very
good benchmark overall more than
good benchmark overall more than
anything.
Spencer, I don't know. It's probably
Spencer, I don't know. It's probably
just learning. I don't know if he has
just learning. I don't know if he has
done as action masking or stuff. It
done as action masking or stuff. It
could just be learning invalid actions.
What did this say?
This thing gets to be quadratic is the
This thing gets to be quadratic is the
problem.
All
right. Well, I got to go for dinner,
right. Well, I got to go for dinner,
guys. Um, I'm on a pretty funny schedule
guys. Um, I'm on a pretty funny schedule
these days. Be back in the morning for
these days. Be back in the morning for
tuning into the uh the research stream.
tuning into the uh the research stream.
Tomorrow we will be doing off policy,
Tomorrow we will be doing off policy,
possibly looking at the environment if I
possibly looking at the environment if I
get bored of that. Um, but yeah, lots of
get bored of that. Um, but yeah, lots of
work to done. But if you're interested
work to done. But if you're interested
in my stuff, generally
in my stuff, generally
everything I do is at puffer.ai. I
everything I do is at puffer.ai. I
Somebody please fix these graphics. They
Somebody please fix these graphics. They
should be cars. Um, but yeah, all my
should be cars. Um, but yeah, all my
stuff's at puffer.ai.
stuff's at puffer.ai.
Go the repo. It's free. Helps me out.
Go the repo. It's free. Helps me out.
You want to get involved with research,
You want to get involved with research,
join the Discord. If you want to build
join the Discord. If you want to build
environments, join the Discord. Other
environments, join the Discord. Other
than that, all my stuff is at X here.
