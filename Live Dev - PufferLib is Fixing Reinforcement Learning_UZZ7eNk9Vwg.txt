Kind: captions
Language: en
We are back
We are back
live
with more modelbased RL
with more modelbased RL
stuff. I left off implementing a hacky
stuff. I left off implementing a hacky
version of
version of
M0 and if I
M0 and if I
recall needed to train the word reward
recall needed to train the word reward
model needed to fix the action
model needed to fix the action
encoder. Let's get to it.
thing. Oh yeah, this needs to
thing. Oh yeah, this needs to
be Hang on.
I see. So the value is going to be
I see. So the value is going to be
different maybe.
Why is stream not
show? There we go. Hey
show? There we go. Hey
folks, back to getting this implemented.
I'm not sure how Muse will do versus
I'm not sure how Muse will do versus
Dreamer to be honest. We need to see
Dreamer to be honest. We need to see
uh view comparisons on
uh view comparisons on
stuff. I would like to see if I can at
stuff. I would like to see if I can at
least get it to do something though.
Uh, okay. So, these are
Uh, okay. So, these are
misaligned. That's sketchy.
State hidden.
Okay, so these are now
Okay, so these are now
aligned. Um, we're going to want to
aligned. Um, we're going to want to
predict one ahead like before. So, this
predict one ahead like before. So, this
is going to
be this is going to be star
More
loss. Define the loss.
See how this
does.
does.
Okay, this does
run. Cool. This does actually run.
run. Cool. This does actually run.
Um, now we just need to figure out
Um, now we just need to figure out
the encoder, I believe.
So, do we just have to add a layer? Is
So, do we just have to add a layer? Is
that all we have to do?
I guess that this should
be
single
broad. Okay. And this is going to we
broad. Okay. And this is going to we
have to add like a
projection. Yeah, we have to add a
projection. Yeah, we have to add a
project.
This
is there's a better way to do this.
I don't think so.
So we have this action
So we have this action
projection and then instead of this
projection and then instead of this
concatenate
concatenate
here this needs embed
here this needs embed
equals
equals
coder and then
concat then we do
concat then we do
this. So you have this action project.
Now this is the hidden
state. State.hidden has to equal this
embedding. And then forward
here. We do m zero.
something like thisish, I
something like thisish, I
think. And then you shouldn't have to
think. And then you shouldn't have to
do this on the rest of them, maybe.
Think that's how it
works. Be tenser. Not left.
It should be one hot actions.
Okay, so this will almost be at the
Okay, so this will almost be at the
point where it could
point where it could
work. Uh, this is going to be way too
work. Uh, this is going to be way too
slow is the only issue.
I think what we want to do
is do we replicate
trial? I think we
trial? I think we
replicate across trials,
right? So, we only do one set of forward
right? So, we only do one set of forward
passes. We don't do a double
passes. We don't do a double
loop. I think I can do that. And I think
loop. I think I can do that. And I think
it'll fit in memory. Maybe.
Does the LSTM cell not accept batched
dimensions? It probably doesn't, right?
Dropbox running. Only weird thing is SPS
Dropbox running. Only weird thing is SPS
recording is erratic. If you're on dev,
recording is erratic. If you're on dev,
that's normal. We uh there's some screwy
that's normal. We uh there's some screwy
stuff in in
that. If you see the new profiling
that. If you see the new profiling
though, it's really cool. I just there's
though, it's really cool. I just there's
some things to
some things to
fix. I'm currently attempting to uh
fix. I'm currently attempting to uh
implement like this
implement like this
um not quite Monte Carlo research but
um not quite Monte Carlo research but
this searchbased edition that uses a
this searchbased edition that uses a
world model. So I'm going to see how
world model. So I'm going to see how
much I can make how much progress I make
much I can make how much progress I make
on
on
this. Saturday is a good day to just
this. Saturday is a good day to just
crank dev
Okay, so this did
something. So
now we have to do
We should reshape
We should reshape
these. We only
need rewards into action.
probably how that repeat works,
right? No.
Is this like a gather? There.
Oh, cuz you're doing it completely wrong
Oh, cuz you're doing it completely wrong
is why.
So we have the rewards here which are
So we have the rewards here which are
all zero right now. Lovely or the
all zero right now. Lovely or the
indices are
zero. This is
zero. This is
idxis first of all.
I think this is correct.
No. Okay, we'll check this. But I think
No. Okay, we'll check this. But I think
this is the roughly correct
this is the roughly correct
thing. So now we have 2,000 actions or
thing. So now we have 2,000 actions or
whatever.
So this is like the full vectorzed
version. And what now we have to
version. And what now we have to
return the actions instead of the logics
return the actions instead of the logics
I
guess. So we just do like
Something like this.
Something like
this. Okay. So, this is at 800K somehow.
This is like still reasonably
fast. Now we have to look at like how I
fast. Now we have to look at like how I
screwed it up
obviously. Um but in general this should
obviously. Um but in general this should
make
make
sense. Reward loss is
sense. Reward loss is
zero. Reward loss start at zero.
I didn't log
it. Okay, cool. There is a reward loss
it. Okay, cool. There is a reward loss
now.
So, I think then we just have to get
So, I think then we just have to get
this
this
correct and we should actually be good.
Um, I'm trying to think if there's
Um, I'm trying to think if there's
anything else I've missed. I don't think
anything else I've missed. I don't think
there is,
though. I guess we just like
Let it at least train a couple
Let it at least train a couple
steps. Okay,
so
so
shape. So now we've got this massive
shape. So now we've got this massive
hidden
hidden
state. It's repeated a whole bunch.
So if I go for 0 0
here and if I go for
here and if I go for
zero the one zero here. Yeah, they
zero the one zero here. Yeah, they
should give me the same thing. And then
should give me the same thing. And then
if I go for
if I go for
like 01. It should be
like 01. It should be
different.
different.
Yes. Okay. So, I think I have this
Yes. Okay. So, I think I have this
correct in the
view. So,
view. So,
[Music]
then one
then one
hot tape.
uh these should not all be zeros, right?
State.action. Oh, I think it's
State.action. Oh, I think it's
because there's probably something's
because there's probably something's
free with the sampling, right? Okay. So,
free with the sampling, right? Okay. So,
let's do it this
let's do it this
way. Because if all the actions are
zero. Yeah, there is variance in the
zero. Yeah, there is variance in the
rewards here, right?
And there
is 01 and two. There is variance in the
is 01 and two. There is variance in the
actions a little
bit. Okay, this is all zero.
That doesn't look correct.
Oh, the rewards are the same exactly in
Oh, the rewards are the same exactly in
each. Okay. Yeah, we definitely have
each. Okay. Yeah, we definitely have
something screwy then.
Because the action getting appended
here.
here.
Action. This should be move
Action. This should be move
zero action, right?
Yeah, we should not be getting all the
Yeah, we should not be getting all the
same
action. Okay, so this gives us separate
action. Okay, so this gives us separate
indices, right?
So if I go forward one
Stop. Try
Stop. Try
something. If we go forward one
something. If we go forward one
step and we only have n= 1, what
step and we only have n= 1, what
happens?
Okay, I think that the
Okay, I think that the
uh the sanity here is going to be that
uh the sanity here is going to be that
this should be the same as the original
this should be the same as the original
algorithm pretty much,
right? We'll have to see what the exact
right? We'll have to see what the exact
differences are, but this should recover
differences are, but this should recover
original algorithm PF because now this
original algorithm PF because now this
is just going
is just going
through
through
and you sample logits.
There's only one to
select. This is the sanity right here.
select. This is the sanity right here.
This will be the sanity.
and the way that we make sure that this
and the way that we make sure that this
is the
is the
same, we just comment all of this crap,
same, we just comment all of this crap,
right?
No, I guess we can't set the action as
No, I guess we can't set the action as
well.
Okay, so this is the one that recovers
Okay, so this is the one that recovers
perf
Okay, this is also fine
here. So now the question is going to be
How is this here
different? This hidden
different? This hidden
state. Okay, wait. This is a different
state. Okay, wait. This is a different
hidden state. Hang on.
This is a different
hidden. This is state.hidden.
How exactly did we do
How exactly did we do
this? Embedding concatenate with one
hot encoder on
observations and then concat and
observations and then concat and
project. All right.
This should be identical to
This should be identical to
before. So what I should be able to do
before. So what I should be able to do
then this should equal
hidden. This should exactly equal the
hidden. This should exactly equal the
hidden state.
Okay, so this is correct up to
here and
here and
then
then
cell is called on this and then the mu0
cell is called on this and then the mu0
lstm state.
which is just
agency,
agency,
right? That's
correct. You call decode actions on the
correct. You call decode actions on the
hidden
hidden
state. It's also
state. It's also
correct. So, this does look identical to
correct. So, this does look identical to
me. Okay. So, something
me. Okay. So, something
Possibly something else is
Possibly something else is
screwing. Let me just get a a random
screwing. Let me just get a a random
sample that's not from the
star. Identical, right?
star. Identical, right?
So we got the hidden state
correct and then you get move zero
correct and then you get move zero
hidden objects values. Yes.
This take is the wrong one. Yeah, this
This take is the wrong one. Yeah, this
take has got to
take has got to
be
wrong. I think this is it.
Hey, welcome.
That's not
it. Still not that. All right, this is
it. Still not that. All right, this is
just me screwing up
just me screwing up
indexing. What are you doing? I'm trying
indexing. What are you doing? I'm trying
to get sort of a form of mu0 into puffer
to get sort of a form of mu0 into puffer
lib so we can test some like search
lib so we can test some like search
basedbased RL.
Is this
it? Yeah, that's still not
it. What kind of applications is
it. What kind of applications is
tailored to Mu
tailored to Mu
Zero?
Zero?
Um, well, that's the cool thing with Mu
Um, well, that's the cool thing with Mu
Zero, right? It's not like alpha zero
Zero, right? It's not like alpha zero
where you need a you need a uh like a
where you need a you need a uh like a
state set and get function for your end.
state set and get function for your end.
It can kind of just learn it for
It can kind of just learn it for
whatever. So it's a pretty way clean way
whatever. So it's a pretty way clean way
of doing model based RL with search
of doing model based RL with search
without needing to have like a state set
without needing to have like a state set
like you can have with go or
chess. I'm just blanking on how to do
chess. I'm just blanking on how to do
this really simple indexing operation
this really simple indexing operation
and then we should have something
Why is this like so
weird? Isn't it just like this of
star? No.
Not this either.
Tensor processing libraries were a
Tensor processing libraries were a
mistake. I
mistake. I
swear. So much time spent doing
indexing. This is like so unbelievably
indexing. This is like so unbelievably
stupid.
It's still freaking
wrong. Oh, here it is.
Okay, we're going to just use this for
Okay, we're going to just use this for
now and then we'll figure out a better
now and then we'll figure out a better
way of doing
it. Come
on. Okay. Dot
squeeze. So if I've done this right,
squeeze. So if I've done this right,
this should match the original
curve roughly.
Yeah. How's this still not match the
Yeah. How's this still not match the
original curve?
original curve?
It's like better than before, but it's
It's like better than before, but it's
still nowhere near as good as the
still nowhere near as good as the
original. Is this not identical?
Okay, let's try this.
This should reproduce,
right? Okay. So, this is the same error
right? Okay. So, this is the same error
as before. This is the same curve. So,
as before. This is the same curve. So,
let's see why that is the
let's see why that is the
case. So,
return logits and
values. Action gets to be
stateaction instead of getting
stateaction instead of getting
sampled. You just sampled it before,
sampled. You just sampled it before,
right?
And then if I comment
this and I do
this and I do
action like so it's a
fix. Yeah. Okay. So this is just I've
fix. Yeah. Okay. So this is just I've
made a mess somehow. I'm getting
made a mess somehow. I'm getting
confused
confused
because otherwise this would be totally
because otherwise this would be totally
fine. Action
fine. Action
state.action. This
state.action. This
[Music]
[Music]
is sample logs.
That's very weird. Very very weird.
Yeah.
Yeah.
So state
action. These are the same logics, are
action. These are the same logics, are
they not?
Okay, this seems to
work now. Can I do this?
No, I cannot do this for some
reason. Different
curve. Is
it? And this is working, but
it? And this is working, but
3 250. Maybe this just
variance. Sounds good.
So this is cell on hidden and LSTM
So this is cell on hidden and LSTM
state.
I guess you can technically do it.
I guess you can technically do it.
You can technically do it this way.
Can I just
like comment this? comment
this. Okay, that's the good one. That's
this. Okay, that's the good one. That's
the good
the good
curve
now. You can do this just like
Okay, this is just too much to think
Okay, this is just too much to think
about. This is breaking my freaking
about. This is breaking my freaking
brain because I haven't like I've done
brain because I haven't like I've done
this in a really messy way and I no
this in a really messy way and I no
longer know what the I'm doing. So,
longer know what the I'm doing. So,
let me just clean this up and make this
let me just clean this up and make this
actually all make sense.
actually all make sense.
Um, yeah, there's just too many shitty
Um, yeah, there's just too many shitty
tensor operations to follow.
Okay.
Okay.
So, the first thing I need to figure out
So, the first thing I need to figure out
is the way that these actions
is the way that these actions
work.
work.
Like you
pass action at last step, right?
And this is going to get the
reward. So for the first step, these
reward. So for the first step, these
should all
should all
match because you have the same actions
match because you have the same actions
from last step.
from last step.
This is really just getting you your
This is really just getting you your
initial encoding state,
initial encoding state,
right? And then from there it can
change.
change.
Okay, so that makes slightly more
sense. Can you not feed multiple? I
sense. Can you not feed multiple? I
think the LSTM does require me to do it
think the LSTM does require me to do it
this way,
unfortunately. Here's your action
unfortunately. Here's your action
projection. Here's your cell. Here's
projection. Here's your cell. Here's
your
your
decode. logits and
values. M0 actions is the one on the
values. M0 actions is the one on the
first step
first step
taken. Yes.
Okay. So, I it's this indexing operation
Okay. So, I it's this indexing operation
right here, which is a freaking mess,
right here, which is a freaking mess,
which is just confusing the hell out of
which is just confusing the hell out of
me, I'm pretty
me, I'm pretty
sure. So, I just have to find a better
sure. So, I just have to find a better
way to do this operation, and then
way to do this operation, and then
everything will get a lot easier.
I don't know why I bashed it this way.
I don't know why I bashed it this way.
This was kind of stupid.
want to take one of these each, right?
want to take one of these each, right?
One of these for each
equals
two. God damn it. Yeah, you can't do
two. God damn it. Yeah, you can't do
this
this
because LSTM cell just sucks.
So obnoxious. Okay.
Nope, not this.
there. So
such
If this freaking works.
Can we not just like
Can we not just like
do one extra forward pass and
do one extra forward pass and
not deal with this horshit? We totally
not deal with this horshit? We totally
can, right?
this. We really just need this actions.
going to drive me
going to drive me
insane. Freaking thing.
You know, I start to wonder what the
You know, I start to wonder what the
hell the point is of all these fancy ass
hell the point is of all these fancy ass
frameworks when it's like I'd almost
frameworks when it's like I'd almost
rather just be writing CUDA at this
rather just be writing CUDA at this
point.
point.
Almost. It would be really easy
Almost. It would be really easy
actually. Like CUDA with a couple basic
actually. Like CUDA with a couple basic
utils would be way easier than this.
Okay, this should be the fast one,
right? This doesn't optimize anywhere as
right? This doesn't optimize anywhere as
near as quickly as the other one.
Okay. State action
Okay. State action
gets move zero
gets move zero
actions values
logits moo zero hidden get state.hidden
How are these freaking
How are these freaking
different? There's no way.
Hidden's different now. I thought we
Hidden's different now. I thought we
checked that like extensively.
Nope. These don't freaking match either.
Nope. These don't freaking match either.
Okay. How do these not
Okay. How do these not
match? How do these not match?
You take mu zero
actions. You
actions. You
append m zero hidden. Mero kitten comes
append m zero hidden. Mero kitten comes
from
state.hidden, right? And then this is
state.hidden, right? And then this is
supposed to be right here. Go with one.
So right here, this is supposed to match
So right here, this is supposed to match
a rig
hidden. Okay, so this
matches. This matches, right? No, this
matches. This matches, right? No, this
doesn't match. What the
doesn't match. What the
hell? Move zero LSTM state
maybe. Ah, you dummy.
right here.
Okay. Mu Z hidden and hidden now
match. Does this give a S
curve? It should give a S curve.
Nope. Still
curve.
Identical objects.
LSTMH LSTMC get updated from
LSTMH LSTMC get updated from
here. Where's hidden get updated? Hidden
here. Where's hidden get updated? Hidden
should get updated in here, right?
State.hidden gets updated
State.hidden gets updated
there. That's everything.
Seems like this should be
Seems like this should be
fine, I guess. Wait, log
fine, I guess. Wait, log
prop. Maybe it's
prop. Maybe it's
this. No, but log prop. Oh, cuz this
this. No, but log prop. Oh, cuz this
returns log prop of the action. This is
returns log prop of the action. This is
why.
Log. Yeah. Yeah, this has got to be it.
So you can return logits but not log
So you can return logits but not log
problem.
Okay, there. That's the fast curve.
Okay, there. That's the fast curve.
Good. Works as expected. Finally. Now,
Good. Works as expected. Finally. Now,
next next is going to
next next is going to
be first we do this.
So 360 at 13
mil. They even change anything.
Okay, that
Okay, that
matches. So this end doesn't matter
matches. So this end doesn't matter
apparently I like at
apparently I like at
all. I guess it's just really nicely
all. I guess it's just really nicely
batched for very small
networks. And now this is where uh
networks. And now this is where uh
things can change a bit.
Okay. So, this is a little far a little
Okay. So, this is a little far a little
bit behind but not far behind.
Now, how many steps do they look
Now, how many steps do they look
ahead in
this How
many steps ahead does this
many steps ahead does this
look? Unroll for K equals five
steps. So then this is
valid. So I guess we just need to like
valid. So I guess we just need to like
scale this
maybe. Hope we don't run out of
memory. See what this does.
So, we're going to definitely have to
So, we're going to definitely have to
try this on a different end.
fully collapsed entropy as well. So,
fully collapsed entropy as well. So,
this really shouldn't be doing much over
this really shouldn't be doing much over
um
um
Yeah, it's
Yeah, it's
fine. It's generally fine.
Let's go to something like
go. Yeah. So, go is a good one here.
Okay. So this is
like baseline
thing. Really doesn't do very much,
thing. Really doesn't do very much,
right?
Let's just do
Let's just do
this and see
uh see what it does based on Yes.
Can you guide what this is? Uh, this is
Can you guide what this is? Uh, this is
me attempting to get some sort of model
me attempting to get some sort of model
based RL into buffer lib. So I have this
based RL into buffer lib. So I have this
like moo zeroesque search thing where
like moo zeroesque search thing where
for every action
for every action
um or for every like trajectory
um or for every like trajectory
It tries 32 times to generate
It tries 32 times to generate
alternative trajectories and it tries to
alternative trajectories and it tries to
see which one is the best before it
see which one is the best before it
takes an
takes an
action. So far, I have not gotten this
action. So far, I have not gotten this
to really do anything
though. And this definitely doesn't help
though. And this definitely doesn't help
here. This is like not
here. This is like not
stable. This is just
stable. This is just
stuck. Okay.
There's one other experiment I want to
There's one other experiment I want to
run on
this. Just
Now that's a fully random search.
Yeah, this doesn't seem to help here
Yeah, this doesn't seem to help here
either. You're doing programming. Well,
yeah. This whole channel is
yeah. This whole channel is
reinforcement learning research done
reinforcement learning research done
live.
Okay. So, this doesn't help. Go back to
breakout. I mean, it is possible I've
breakout. I mean, it is possible I've
just done something screwy here.
and the reward loss is very
low.
Um, yeah, but that doesn't
Um, yeah, but that doesn't
seem okay. That definitely suggests
seem okay. That definitely suggests
there's an error here.
ID. This is just
Neo. Okay, I'm actually suspecting I
Neo. Okay, I'm actually suspecting I
just have something wrong based on this.
just have something wrong based on this.
I'm not going to abandon this just
I'm not going to abandon this just
because I did it
because I did it
wrong. Um, let me
see. I mess up the action logic or
see. I mess up the action logic or
something.
something.
Sample
Sample
logs, decode actions, hidden
That actually seems like it should be
That actually seems like it should be
fine.
fine.
Um, I do wonder if I messed up the
Um, I do wonder if I messed up the
reward
loss cuz that would also do
it. Hang on. I could technically do
this. I've seen all this for the first
this. I've seen all this for the first
time in my life. So well yeah you will
time in my life. So well yeah you will
be a bit confused. Um the whole idea of
be a bit confused. Um the whole idea of
this channel
this channel
here I am a researcher. I do
here I am a researcher. I do
reinforcement learning research
reinforcement learning research
full-time and uh I build a whole bunch
full-time and uh I build a whole bunch
of open source
of open source
software and I do all of my development
software and I do all of my development
live. So people come by who use the
live. So people come by who use the
library, they ask questions, people get
library, they ask questions, people get
involved in dev. I review code
involved in dev. I review code
contributions live sometimes.
This the goal here generally is to just
This the goal here generally is to just
advance this branch of
science. Well, that was stable for
science. Well, that was stable for
longer and then blew
up. Weird. Very, very weird.
How does this compare to like the
How does this compare to like the
dreamer
style? I don't want to abandon this just
style? I don't want to abandon this just
yet, though. The movie the Miro style
yet, though. The movie the Miro style
thing actually seems pretty good. If we
thing actually seems pretty good. If we
can get it to
work, the fact that they train only on
work, the fact that they train only on
the world model seems really jank,
the world model seems really jank,
right?
MO zero trains on real data and uses
MO zero trains on real data and uses
more compute to get better real data
more compute to get better real data
using the world model to like plan
using the world model to like plan
forward. It also gives you test time
forward. It also gives you test time
compute scaling for
free. Dreamer uses a much larger
free. Dreamer uses a much larger
model. The forward pass or the data
model. The forward pass or the data
collection step is very short.
And then it just spins training for a
And then it just spins training for a
long
time. I This is live research,
right? I mean, I have a PhD in this
right? I mean, I have a PhD in this
stuff. I've been doing this like half of
stuff. I've been doing this like half of
my life.
though the math side is actually my
though the math side is actually my
weakest part here. Um, I'm far better on
weakest part here. Um, I'm far better on
the engineering side.
I
mean, which of these makes more sense or
mean, which of these makes more sense or
do they both make sense?
So in one of these you hallucinate a ton
So in one of these you hallucinate a ton
of
of
trajectories and then train on them
trajectories and then train on them
directly and in the other you
directly and in the other you
hallucinate a bunch of
hallucinate a bunch of
trajectories. You pick which one you
trajectories. You pick which one you
think is the best and then you take the
think is the best and then you take the
action according to that trajectory.
action according to that trajectory.
Right?
Move zero seems like it should be way
Move zero seems like it should be way
more likely to work quickly. I would
more likely to work quickly. I would
think you're not really changing the
think you're not really changing the
learning dynamics a ton. You're really
learning dynamics a ton. You're really
just you're strictly using a world model
just you're strictly using a world model
to get better data with the simplest
to get better data with the simplest
formulation of this.
So you're scaling compute in order to
So you're scaling compute in order to
get better training
get better training
data. That
data. That
seems that seems like this should be
seems that seems like this should be
easier to get to work. So I don't think
easier to get to work. So I don't think
I should even think about trying to go
I should even think about trying to go
to dreamer today. I think I should just
to dreamer today. I think I should just
try to get
this for each game in
this for each game in
Atari. 20 billion frames we used a
Atari. 20 billion frames we used a
lot GPU for
lot GPU for
selfplay equivalent to two weeks of
selfplay equivalent to two weeks of
training on one
training on one
GPU. It's a lot of freaking
training. They did 50 simulations per
training. They did 50 simulations per
move.
Is there an
end or I would expect to need to
end or I would expect to need to
plan? No, it shouldn't matter. I should
plan? No, it shouldn't matter. I should
just be able to use this on breakout and
just be able to use this on breakout and
get better perf.
Honestly, I I should be able if this
Honestly, I I should be able if this
works, it should at least be able to
works, it should at least be able to
match on that, I would
think, cuz the policy is stochastic,
think, cuz the policy is stochastic,
right? So, this would at least give you
right? So, this would at least give you
the effect of
the effect of
determinizing the policy. Yeah. Okay.
If I tell it rand to take random
If I tell it rand to take random
actions, it blows up
immediately. I don't know why I'm having
immediately. I don't know why I'm having
well I mean it's mostly intended for
well I mean it's mostly intended for
technical audience but this is I mean
technical audience but this is I mean
this is scientific progress happening
this is scientific progress happening
live. If this thing works, then we will
live. If this thing works, then we will
immediately have uh a way of scaling all
immediately have uh a way of scaling all
of our AI training with compute way more
of our AI training with compute way more
effectively than we have now um and
effectively than we have now um and
applicable to way more
problems. So I usually on Saturdays I do
problems. So I usually on Saturdays I do
like deep dives into crazy rabbit holes
like deep dives into crazy rabbit holes
and like I try to like look at something
and like I try to like look at something
out there and
out there and
uh see what I can get working in like
uh see what I can get working in like
the span of a day.
the span of a day.
It's kind of just meant to see like what
It's kind of just meant to see like what
are promising directions that we could
are promising directions that we could
take. Oh, if you want to see something
take. Oh, if you want to see something
cool that you will actually understand.
cool that you will actually understand.
So on puffer.ai, we have all our demos
So on puffer.ai, we have all our demos
which are mostly just a bunch of games,
which are mostly just a bunch of games,
right? And uh these games are being
right? And uh these games are being
played by neural nets that we trained
played by neural nets that we trained
live in your
live in your
browser. And you can hold shift and you
browser. And you can hold shift and you
can take over and you can try to play,
can take over and you can try to play,
but you're going to not be as good as
but you're going to not be as good as
the agent for most of these. You know,
the agent for most of these. You know,
we've got like this race thing. This is
we've got like this race thing. This is
like a superhuman racing
like a superhuman racing
AI. See how quickly it responds to
AI. See how quickly it responds to
everything. And this is trained purely
everything. And this is trained purely
just by playing the game. This plays the
just by playing the game. This plays the
game a whole bunch and it figures out
game a whole bunch and it figures out
how to play
it. Same thing with like multi-agent
it. Same thing with like multi-agent
snake. We have lots of cool demos of
snake. We have lots of cool demos of
this stuff. So, what we're trying to do
this stuff. So, what we're trying to do
here is actually kind of easy to
here is actually kind of easy to
explain. Um, the how of it is a bunch of
explain. Um, the how of it is a bunch of
math and a bunch of
engineering. It seems weird to me that
engineering. It seems weird to me that
this would break
training, right?
cuz this is just saying take
cuz this is just saying take
random try from random actions. This is
random try from random actions. This is
all this is saying
right? If I do like this
It's kind of a Q
function. Kind of
It's not going to train either. Had a
It's not going to train either. Had a
question regarding classification. If I
question regarding classification. If I
have a bunch of features and a bunch of
have a bunch of features and a bunch of
data points but just a binary
data points but just a binary
classification, you think reinforcement
classification, you think reinforcement
learning is a good No, reinforcement
learning is a good No, reinforcement
learning is never a good way to train if
learning is never a good way to train if
you have a supervised learning signal.
you have a supervised learning signal.
Uh reinforcement learning is for when
Uh reinforcement learning is for when
you have usually an interactive process
you have usually an interactive process
with unlabeled data. So you're
with unlabeled data. So you're
interacting with some sim and then you
interacting with some sim and then you
basically just know every so often
basically just know every so often
whether you did well or not. So you have
whether you did well or not. So you have
a very weak learning signal. That's what
a very weak learning signal. That's what
reinforcement uh learning is used for.
reinforcement uh learning is used for.
If you try to like throw reinforcement
If you try to like throw reinforcement
learning on classification, you can
learning on classification, you can
actually technically set it up, but it's
actually technically set it up, but it's
like it's way less effective than just
like it's way less effective than just
doing supervised learning because like
doing supervised learning because like
reinforcement learning as a whole is a
reinforcement learning as a whole is a
method of techniques for dealing with
method of techniques for dealing with
when you have very low learning signal,
when you have very low learning signal,
usually in like a non-stationary
usually in like a non-stationary
interactive process. If you have a
interactive process. If you have a
strong learning signal in a stationary
strong learning signal in a stationary
process, which is what classification
process, which is what classification
is, you just use
is, you just use
it. There's no reason not to.
it. There's no reason not to.
Okay, so this blows up freaking
Okay, so this blows up freaking
infinitely, huh?
And then if I just do like this
instead. Gotcha. Yep.
So the cool thing with RL, right, and
So the cool thing with RL, right, and
like the best way to understand it is
like the best way to understand it is
this thing plays a game. It starts off
this thing plays a game. It starts off
by mashing buttons randomly and it just
by mashing buttons randomly and it just
gets better at it and it just gets to be
gets better at it and it just gets to be
super
super
human. Like one of the cool things about
human. Like one of the cool things about
RL, you can just completely mess up the
RL, you can just completely mess up the
graphics of a game to a point that no
graphics of a game to a point that no
human will ever be able to play it. And
human will ever be able to play it. And
the RL won't even notice. It'll just
the RL won't even notice. It'll just
learn to play the game exactly like it
learn to play the game exactly like it
would with the good
graphics. It's pretty
cool. Okay, so this trains it is when
cool. Okay, so this trains it is when
you increase the horizon that it stops
you increase the horizon that it stops
working or it doesn't really stop
working or it doesn't really stop
working is the thing in this case,
working is the thing in this case,
right? It just doesn't do any
right? It just doesn't do any
better. And then on like let go, it
better. And then on like let go, it
doesn't really do anything. But then
doesn't really do anything. But then
that our implementation of that is kind
that our implementation of that is kind
of sketchy.
I also want to learn all this from
I also want to learn all this from
scratch. How can
I CS231N is the best like starter deep
I CS231N is the best like starter deep
learning class? I would say
learning class? I would say
um it's a graduate course, so there are
um it's a graduate course, so there are
some prerexs. They're considered basic
some prerexs. They're considered basic
prerexs for a course of this type. Um
prerexs for a course of this type. Um
but yeah, you need like multivariate
but yeah, you need like multivariate
differential calculus, linear algebra,
differential calculus, linear algebra,
decent programming
background. Like this stuff is
background. Like this stuff is
incredibly cool. And we actually we have
incredibly cool. And we actually we have
people who came in to Puffer Lib like a
people who came in to Puffer Lib like a
year ago with no programming background,
year ago with no programming background,
not really any relevant math, and have
not really any relevant math, and have
done pretty darn well. But it takes
done pretty darn well. But it takes
work, right?
Like I started doing this when I was
Like I started doing this when I was
14 and pretty much just never
stopped. Okay, so this still
runs with this
But the entropy is like super
But the entropy is like super
crashed. So I mean this just runs
crashed. So I mean this just runs
because it's not actually doing any
because it's not actually doing any
search. I would
search. I would
imagine if I just set this to one like
imagine if I just set this to one like
this. I need to understand why this
this. I need to understand why this
breaks it because this is just like it
breaks it because this is just like it
should be able to evaluate which action
should be able to evaluate which action
puts it in a better state pretty easily.
If it doesn't, then I probably have
If it doesn't, then I probably have
something set up
wrong. Yeah. And then it explodes. Okay.
I mean, there really only two objectives
I mean, there really only two objectives
added to
added to
this. I
this. I
added these two
losses. So there's a world model
objective right up
there and this is
there and this is
just world model on
state.hidden. Wait world
state.hidden. Wait world
model of
model of
state.hidden. Does that make
state.hidden. Does that make
sense or did I mess it up?
Do you come live every day at this time?
Do you come live every day at this time?
Uh, I stream like 40 to 60 hours a
Uh, I stream like 40 to 60 hours a
week. There's not like a fixed super
week. There's not like a fixed super
fixed schedule, but I'm like I'm usually
fixed schedule, but I'm like I'm usually
on because it's 40 to 60 hours a week.
This bothers me. This really bothers
me. I think that this
me. I think that this
one Wait, neither of these are correct,
one Wait, neither of these are correct,
right?
right?
Yeah, I think I just have both of these
wrong. So if both of these are like a
wrong. So if both of these are like a
bit off, then neither of them are going
bit off, then neither of them are going
to work. Okay, so then this reward
model. Yeah, we need to just go rename
model. Yeah, we need to just go rename
things so that they're not dumb. I think
things so that they're not dumb. I think
that's what needs to happen right
now. So this should be
Instead of
Instead of
statehidden, this
statehidden, this
is state.obs embed is
hidden. And this is state.hidden.
Okay. So, this is the same as
before.
Um, and now we're
Um, and now we're
putting obs
into model state.hidden hidden and then
into model state.hidden hidden and then
state wait world model of state.hidden
state wait world model of state.hidden
hidden as to predict obsa. Is that
hidden as to predict obsa. Is that
right? I think this is correct.
Right. The state hidden should be the
Right. The state hidden should be the
output of the
output of the
LSTM and we want that to train to
LSTM and we want that to train to
predict
predict
the embedding
input. Hang on though. The world model
input. Hang on though. The world model
doesn't it need
Does it not need the
action? It needs the action, man. What
action? It needs the action, man. What
are you doing?
right now I have it set up so that the
right now I have it set up so that the
input to the
input to the
network
network
takes actions. Right.
Hang on. Maybe I can make this
simpler. If I just have like my standard
simpler. If I just have like my standard
network here, which is your
network here, which is your
LSTM, right?
LSTM, right?
S0
one two
one and then this is like A1,
one and then this is like A1,
B1, 2,
B2,
B2,
two. Then from
here, I just need something that
here, I just need something that
predicts the input here.
which is the next
which is the next
observation. So you really don't need
observation. So you really don't need
you kind of just need the action there.
you kind of just need the action there.
Can I just have like a world model thing
Can I just have like a world model thing
that takes in like
that takes in like
Oops. Can I just like do something like
Oops. Can I just like do something like
this where I take in
this where I take in
like and let's call this
like and let's call this
E1. So I take like E1
You don't even need to take in E1,
You don't even need to take in E1,
right? You can take the output
right? You can take the output
vector. Take literally the output
vector. Take literally the output
vector. So this is
vector. So this is
actually this S1 is actually technically
actually this S1 is actually technically
H1 and then this is C1.
H1 and then this is C1.
So then I can take in
H1
H1
A1 and I can
predict E2. I
guess isn't this
guess isn't this
easier because then you don't have to
easier because then you don't have to
change anything about the base net at
change anything about the base net at
all. I think this would be
all. I think this would be
easier and then the world model should
easier and then the world model should
actually have context for what I'm
actually have context for what I'm
trying to
trying to
do. Okay, let's try it that
way. And then this can also predict
way. And then this can also predict
reward, right? This needs reward and
reward, right? This needs reward and
value, but it can do that
value, but it can do that
then. Okay, we can do
then. Okay, we can do
this. Let me get a let me go grab a cup
this. Let me get a let me go grab a cup
of tea real quick so I keep myself awake
of tea real quick so I keep myself awake
here. This is a thinking
here. This is a thinking
day. Smoke coming out of my ears. Give
day. Smoke coming out of my ears. Give
me a minute. I'll be right back.
Okay, I
Okay, I
think I think we can get this to
work. I think we can get this to
work. I think we can get this to
work. Say, man.
Um, I think it was the fact that I just
Um, I think it was the fact that I just
didn't have this thing conditioned
didn't have this thing conditioned
correctly.
So what we're going to
So what we're going to
do going to make the world model a
do going to make the world model a
little
little
differently and try
again. So this is going to be hidden
again. So this is going to be hidden
size
size
plus selfnum actions going to hidden
plus selfnum actions going to hidden
size like so. Okay, we no longer need
size like so. Okay, we no longer need
action project.
action project.
We no longer need
We no longer need
this. Just need like this.
this. Just need like this.
Okay. Encode
Okay. Encode
observations. This get state
obs. We still need this for
obs. We still need this for
uh it doesn't need this one hot actions
uh it doesn't need this one hot actions
anymore. Doesn't need this.
needs
needs
this.
this.
So, we embed and we save this to here.
So, we embed and we save this to here.
Okay. Also, I need to not get dehydrated
Okay. Also, I need to not get dehydrated
doing this. Had a lot of
doing this. Had a lot of
coffee. Let me do this before I get my
coffee. Let me do this before I get my
tea.
Okay, we
do the LSTM rapper now.
You don't need any of this,
right? So, this actions actually gets
right? So, this actions actually gets
added to the hidden
added to the hidden
state. Obset doesn't get used at all
here. No, state.hidden doesn't exist
here. No, state.hidden doesn't exist
yet.
yet.
Um yeah, state.hidden does not exist
Um yeah, state.hidden does not exist
yet. We go like
yet. We go like
this. Now we've run the policy
this. Now we've run the policy
once.
Now we have the hidden
target. Doesn't super matter actually.
M0 hidden equals.
state.Action
repeat. You don't need
repeat. You don't need
this previous action
this previous action
anymore, right?
You need to know actions at
You need to know actions at
all? I don't think you need to know
all? I don't think you need to know
actions for training,
actions for training,
right? For training the world
right? For training the world
model. So I don't think you need to know
model. So I don't think you need to know
this at all.
Okay, you do need to sample some logs.
So the first thing you need to do is
So the first thing you need to do is
actually sample
actually sample
logits which is up
logits which is up
here, right?
here, right?
And
then you can one hat you can one hot
then you can one hat you can one hot
those
those
actions. And then you can
do
input right here right
input right here right
m
hidden and then this is self.world World
hidden and then this is self.world World
model
model
policy. World
policy. World
model of
hidden. And then this is
hidden. And then this is
like world model
obs and this is m0
obs and this is m0
hidden. Then you
hidden. Then you
get logits and
values from decoding the actions.
and then what do you do? You
and then what do you do? You
decode. You get the rewards from the
decode. You get the rewards from the
hidden
state with this
state with this
action and the rewards you append the
action and the rewards you append the
actions.
Okay, so it's something like this. I
Okay, so it's something like this. I
think it's something a lot closer to
think it's something a lot closer to
this. And what are we doing? Forward for
training. OBS embed is equal to
training. OBS embed is equal to
hidden. Uh, this is not supposed to be
hidden. Uh, this is not supposed to be
here. It's
here. It's
actually Yeah. Well, it doesn't super
actually Yeah. Well, it doesn't super
matter,
but that hidden goes there.
but that hidden goes there.
We got like three hours to get something
We got like three hours to get something
to work
to work
here. I'm going do the best I
here. I'm going do the best I
can. I think we should be able to do
it. Okay. So, there's no
it. Okay. So, there's no
statehidden. It should just be hidden.
This is in the world model I
This is in the world model I
think
think
no those model
obs here is get like a nice
obs here is get like a nice
self-contained world model module that
self-contained world model module that
takes in uh hidden state and action
takes in uh hidden state and action
embedding and produces something that is
embedding and produces something that is
going to be learned to be similar to an
going to be learned to be similar to an
observ ation
observ ation
embedding. What are we still screwing
embedding. What are we still screwing
up? Policy. Okay, cool. This is in clean
puffl which is right
puffl which is right
here. Uh now this world
model. We can just do
model. We can just do
policy world model forward statehidden
policy world model forward statehidden
and actions, right?
And then what we can do is we can take
And then what we can do is we can take
this
thing. We can do this
thing. We can do this
m0
m0
hidden. And then we can just add this
hidden. And then we can just add this
function on
function on
here which
here which
takes
takes
hidden
actions.
Okay. And then this takes
Okay. And then this takes
hidden. This is self actions. This is
hidden. This is self actions. This is
self world model. Return world model
self world model. Return world model
ops.
batch. Uh, I thought there was an
batch. Uh, I thought there was an
action. Oh, it's not batch action.
action. Oh, it's not batch action.
It's state.action.
action
imp
batch.actions colon
batch.actions colon
negative1. That seems screwy to me. I
negative1. That seems screwy to me. I
think this is just batch.s
Okay. So,
Okay. So,
[Music]
[Music]
expected some small mismatch shenanigans
expected some small mismatch shenanigans
here.
Um, presumably these are different
Um, presumably these are different
shapes or something.
Oh, so batch actions
is we should just return hidden after
is we should just return hidden after
the transpose then,
the transpose then,
right? Yeah, we should just do
that like this.
Maybe maybe just do this.
Okay, this is
Okay, this is
correct. And uh it's just not dim equals
correct. And uh it's just not dim equals
1 anymore because it's a
1 anymore because it's a
sequence. It should probably just be
sequence. It should probably just be
dim=1.
Right. Oh, am I
Right. Oh, am I
have I have Neptune on on all these?
have I have Neptune on on all these?
Let's just do that. That'll be
Let's just do that. That'll be
faster. Oh, that's so much
better. Take me a few seconds per run.
Good. And state ops embed is
Good. And state ops embed is
the the
the the
target. Next
state. Ah, okay.
state. Ah, okay.
state
targets. You know, you really don't even
targets. You know, you really don't even
need this here at
need this here at
all. You can kind of
just OBS embed
just OBS embed
goes after the LSTM, right?
goes after the LSTM, right?
You can just do it like right
You can just do it like right
here
dashtt
state right there before you get to the
state right there before you get to the
lstm. That looks good.
Next state
Next state
credits, next state
credits, next state
card. That's pretty
card. That's pretty
good. Now, this is
good. Now, this is
uh concat on
uh concat on
wrong dimension, I
believe.
believe.
So, let me think about how this works.
Obs one colon
Obs one colon
Yeah, like this. This should be the
Yeah, like this. This should be the
correct
loss. And this is your world
loss. And this is your world
loss. Mean squared error. World
loss. Mean squared error. World
loss
hidden
reds. You have your main squid
reds. You have your main squid
error. And then similar for
error. And then similar for
this I suppose we'll quickly find out
this I suppose we'll quickly find out
whether this is
correct. Okay. So these are also
uh these are also messed
up. State hidden shape is 12864
up. State hidden shape is 12864
128. So you actually don't want to need
128. So you actually don't want to need
this transpose at
this transpose at
all.
all.
Huh? Yeah. You just think we just do
this. Okay. So now we have 128x 64 by
this. Okay. So now we have 128x 64 by
1 and batch rewards is
1 and batch rewards is
perfect.
Squeeze and this is
Squeeze and this is
of minus one.
of minus one.
And then this
And then this
is so these should be
is so these should be
better better rewards than ts
Not the perfect result we want to
see. Does something
though. Next we will look
at we'll look at
the actual world model here
code. I should be able to set this to
code. I should be able to set this to
like zero. Right?
like zero. Right?
Well, I need to refactor this so that
Well, I need to refactor this so that
this is the first iteration basically.
Is there a way that we
could put this all one
could put this all one
loop? Is there a way that that could
The thing is you don't really need to
The thing is you don't really need to
call
The world
model. Let's verify something real
model. Let's verify something real
quick.
quick.
If I just do
this, I just do
this, I just do
like
stateactions actions.
Just
do actions log
do actions log
props
logs something like
this is this
train state log
from oops and this is
Yeah. So this is the ultra fast train
Yeah. So this is the ultra fast train
curve
curve
here. So I guess if we were to look at
here. So I guess if we were to look at
it this way, right?
It's kind of like doing
this. And this still this doesn't break
this. And this still this doesn't break
anything, I'm sure.
anything, I'm sure.
But and then the stuff below needs to
But and then the stuff below needs to
refine these
refine these
predictions. So this is still good.
Now the one annoying thing is that
Now the one annoying thing is that
um you do have to expand
um you do have to expand
this to size N I guess.
So this is just going one ahead and
So this is just going one ahead and
looking at the
looking at the
value, right?
I think I implemented this a little
I think I implemented this a little
differently from Uzero actually,
right? I did this the more like PO
right? I did this the more like PO
compatibleish
way. I mean it is training here, right?
It is actually training.
You got your reward loss and you got
You got your reward loss and you got
your world loss.
Hang on. I might have set this up wrong
Hang on. I might have set this up wrong
still because
Yeah. Okay. I think I did set this up
Yeah. Okay. I think I did set this up
wrong. Hang on. I think I skipped one
wrong. Hang on. I think I skipped one
time step.
logic
logic
repeat. Oh no, wait. So we take these
repeat. Oh no, wait. So we take these
logits, we repeat these a bunch of
logits, we repeat these a bunch of
times, we sample different
times, we sample different
actions.
actions.
Okay, we put these through the world
Okay, we put these through the world
model forwards.
And then we put it through the cell of
And then we put it through the cell of
the next time
the next time
step m0 lstm state which
gets this is the wrong
h this is the wrong age. It needs to
be just do
that. Okay. It's not like rocket ship
that. Okay. It's not like rocket ship
takeoff.
Next time
step. Oh yeah, but I think we're still
step. Oh yeah, but I think we're still
trolling here.
trolling here.
So world model
So world model
forward mu0 LSTM
forward mu0 LSTM
state mu0 hidden
C
C
Pretty sure this
Pretty sure this
works.
works.
So you update your M0 LSTM
state. Okay, that makes it rain faster.
state. Okay, that makes it rain faster.
That's way better,
That's way better,
right? That's not like perfect yet, but
right? That's not like perfect yet, but
that's way
that's way
better. Way way way better. Actually,
better. Way way way better. Actually,
that might be
matching. Yeah. No, this is actually on
matching. Yeah. No, this is actually on
par now. Cool.
Um, you get your new LSTM
Um, you get your new LSTM
state. You decode
state. You decode
actions
actions
from m hidden.
from m hidden.
Oh, this is the old
hidden. Oh, this isn't getting used
hidden. Oh, this isn't getting used
anyways, I don't
anyways, I don't
think. Yeah, this isn't getting
think. Yeah, this isn't getting
used anyways. So, when we go to multiple
used anyways. So, when we go to multiple
steps, this will
steps, this will
break. So, decode actions
of zero.
Hang
on. What's this
hidden? Okay. World model
hidden? Okay. World model
ops. M0 LSTM
ops. M0 LSTM
state. This is M0 LSTM state.
This is m
This is m
zero
hidden. Move zero
hidden. Move zero
hidden. Then moo zero
hidden. Then moo zero
hidden. Then you get your reward model
hidden. Then you get your reward model
on the hidden. You update your rewards.
on the hidden. You update your rewards.
Let's make sure we didn't break
Let's make sure we didn't break
anything.
We're going to get this today. We're
We're going to get this today. We're
going to get
going to get
it. This got to work,
it. This got to work,
right? Okay. So, this
right? Okay. So, this
is How the did we just break it?
is How the did we just break it?
Oh, we didn't. Hang
Oh, we didn't. Hang
on. Did
we? No, we did break
we? No, we did break
it.
it.
Let's go back to where it was
working. This was the one that was
working. This was the one that was
working. We have to be very very careful
working. We have to be very very careful
with
with
this. Very very careful.
And this is a good
one. Now this is your LSTM state.
one. Now this is your LSTM state.
What if I just change this to
age? Okay. So, this breaks if I change
age? Okay. So, this breaks if I change
that to eight, right?
Yes, definitely
Yes, definitely
breaks. I change this to
age zero logic
rewards. That's weird.
So this goes, hey, if we sample a bunch
So this goes, hey, if we sample a bunch
of
of
logs and we run them through the world
model, it is correct to pass here
model, it is correct to pass here
though. So I think the thing this is m
logs. Does this get used
though? This doesn't get
used
values. The reward gets used I
guess. So how is this
break? Oh, I guess it's because the
break? Oh, I guess it's because the
value here is going to change,
right? Yeah. So the value Okay. Okay. So
right? Yeah. So the value Okay. Okay. So
that is going to
change. So we sample
change. So we sample
logics from
here. What the hell happened to our
here. What the hell happened to our
reward model? Right.
We're totally ignoring the reward that
We're totally ignoring the reward that
we get at this step.
This needs to
This needs to
go on the hidden state here,
right? This needs to go here.
This is fast. Good.
This still
This still
works. So
careful. Okay. So, this is the good
careful. Okay. So, this is the good
one. So, we're getting there.
Now you run the world model on the
Now you run the world model on the
hidden state and actions. You get the
hidden state and actions. You get the
new mu0 LSTM
state. This
state. This
decode should be on
decode should be on
H. Apparently this breaks though.
H. Apparently this breaks though.
Let's see if this breaks
it.
totally breaks
it. Like it still trains,
but nowhere near as
well. So issue here, right, is you're
well. So issue here, right, is you're
adding on
You're adding on this value bootstrap, I
You're adding on this value bootstrap, I
guess.
Man, the thing is that this estimate
Man, the thing is that this estimate
should not be unstable.
Yeah, the thing is that this estimate
Yeah, the thing is that this estimate
should not be
unstable. If this estimate is
unstable then um the world model has to
unstable then um the world model has to
be bad, right?
How's the world model being
learned? I guess that's the question.
learned? I guess that's the question.
How's the world model getting learned?
How
about this?
learned but not very fast.
This is like just still so way
This is like just still so way
off. And we know that like if we just
off. And we know that like if we just
put in the value that we actually get,
put in the value that we actually get,
then it's correct. So then the only
then it's correct. So then the only
errors in here
The only error here has to come from
The only error here has to come from
errors
in the world model, right? Because mu0
in the world model, right? Because mu0
lstm state
Oh, wait. Hang on. Yeah, it has to come
Oh, wait. Hang on. Yeah, it has to come
through errors
in in the dynamics of Nep's next
step. So
step. So
obnoxious. This was 305 and it's
obnoxious. This was 305 and it's
probably like
probably like
300.
300.
But it takes forever to evaluate now.
But it takes forever to evaluate now.
It's running through this whole freaking
search. 250. It's even
lower. That's
lower. That's
tricky. That's
tricky. That's
tricky because see, we should be able to
tricky because see, we should be able to
like crank this. The goal is that you
like crank this. The goal is that you
should be able to like crank this
should be able to like crank this
here and like now it should be able to
here and like now it should be able to
do this like multi-step modeling.
Is this better than before?
This actually seems decent, doesn't
This actually seems decent, doesn't
it? Hang
it? Hang
on. Huh.
So,
So,
um I guess just compared to the
um I guess just compared to the
original, there's a a
original, there's a a
penalty when you use one time step
penalty when you use one time step
because you may as well at that point
because you may as well at that point
just use the true value estimate and not
just use the true value estimate and not
have the world model drift or whatever.
have the world model drift or whatever.
But if you roll ahead four like a few
But if you roll ahead four like a few
time steps, you actually get benefit.
I mean, compared to the default, I don't
I mean, compared to the default, I don't
know if this is getting benefit compared
know if this is getting benefit compared
to it's probably not going to get
to it's probably not going to get
benefit compared to the
benefit compared to the
original the original, but I mean, this
original the original, but I mean, this
is progress at
least. Definitely progress.
So they say that they use five steps
So they say that they use five steps
look ahead for
look ahead for
everything. We could try more step look
everything. We could try more step look
ahead or we could try um bigger batch or
ahead or we could try um bigger batch or
both.
So 800 at 28 mil,
right? Yeah. So this is like pretty well
right? Yeah. So this is like pretty well
on par with
here. So potentially
Potentially this can do
something. I think if the benefit is
something. I think if the benefit is
there's probably benefit from going
there's probably benefit from going
deeper. Let's try
deeper. Let's try
this steps.
I think that we have a correct
I think that we have a correct
implementation though now. I think we do
implementation though now. I think we do
have a correct
implementation. We can probably scale
implementation. We can probably scale
the world model up as
well. We're just going to we're going to
well. We're just going to we're going to
try a few separate experiments.
So this is
deeper. Because we don't have frame skip
deeper. Because we don't have frame skip
in ours.
It's pretty well on par with the other
curves. We are actually using the
curves. We are actually using the
actions,
actions,
right? Log
right? Log
prop use your Yeah, we are giving it the
prop use your Yeah, we are giving it the
actions.
Okay. Still behind our on
Okay. Still behind our on
policy version.
It's This is not like magically better.
It's This is not like magically better.
It's decent, but going deeper just makes
It's decent, but going deeper just makes
it slower. It doesn't really make it
it slower. It doesn't really make it
better. At least on
better. At least on
this. It's interesting.
Okay, we could go wider as well,
Okay, we could go wider as well,
right? And go
right? And go
128. So wider.
We'll see what it is. MS.
Uh, this is too
slow. Yeah, this is too slow to even
slow. Yeah, this is too slow to even
run. This is probably like a
run. This is probably like a
scaling bottleneck 64.
Yep, that'll be better. Like 200k or
Yep, that'll be better. Like 200k or
something. That at least we can run the
something. That at least we can run the
experiment.
It seems like there might be It just
It seems like there might be It just
seems like it's roughly on par
really. It's about it.
It's like not getting a lot out of this
model. Well, there are a lot of things
model. Well, there are a lot of things
we can uh oblate though with this to be
fair. Okay, like let's say we did
fair. Okay, like let's say we did
32. We scale up the world model a
bunch. Yeah, it's stuck. Okay, it's like
bunch. Yeah, it's stuck. Okay, it's like
about the same.
scale up the world model a little bit.
scale up the world model a little bit.
You're supposed to do well with scaling
Okay, it's just redoing the optimizer.
Yeah, the reward loss is also still
Yeah, the reward loss is also still
high,
high,
right? And that should be very easy to
right? And that should be very easy to
predict.
Is that
anything? Or is this the same curve?
pretty much the same freaking curve.
World model loss is
lower. Reward loss is still high. Maybe
lower. Reward loss is still high. Maybe
that's next. Okay. You know, this was
that's next. Okay. You know, this was
kind of interesting. This is
like feels smooth.
There's no way the reward loss should be
There's no way the reward loss should be
that high, though.
We still to
We still to
650. No, we are
not. That's only about on
par. It's just like on par.
par. It's just like on par.
Not better
Not better
yet. Hang on. We are still dependent on
yet. Hang on. We are still dependent on
this reward model, right?
So we're at like
So we're at like
014 right now on the reward scale for
014 right now on the reward scale for
this. See about
this. See about
this. You are very dependent on good
this. You are very dependent on good
world model and good reward function.
So, so if this isn't better then the
So, so if this isn't better then the
question will Why not? Because it's like
question will Why not? Because it's like
it's predicting the
future, but it doesn't look like the
future, but it doesn't look like the
reward loss has gotten better, which is
reward loss has gotten better, which is
suspicious to me.
suspicious to me.
The reward loss in particular should be
The reward loss in particular should be
very easy to predict actually,
right? Yeah, the reward loss should be
right? Yeah, the reward loss should be
like very very easy to predict. Okay.
Um, in that
case, MSSE
case, MSSE
over next reward predictions and next
over next reward predictions and next
reward targets.
Did I offset it by one by
mistake? What happens if I do this?
I don't think I
I don't think I
did. I don't think I did. But we'll try
did. I don't think I did. But we'll try
this.
Let me think about
this. I'm almost sure it's the other
this. I'm almost sure it's the other
way.
cuz you get the reward at every step.
I mean, this doesn't seem to care which
I mean, this doesn't seem to care which
one you give
it. It doesn't like do better or worse.
it. It doesn't like do better or worse.
It's the same
It's the same
graph, which is also bizarre because it
graph, which is also bizarre because it
should matter.
I think this is
I think this is
correct any more than the other
correct any more than the other
one. It's very weird that it doesn't
one. It's very weird that it doesn't
seem to matter.
Like if the world model can predict
You know, you're probably now getting
You know, you're probably now getting
just screwed by your sampling scheme
just screwed by your sampling scheme
here, right? Because you're not doing
here, right? Because you're not doing
MCTS. You're doing something
MCTS. You're doing something
stupid. It's like a really dumb
stupid. It's like a really dumb
approximation. It shouldn't be a
max.
Should
Should
it? Or maybe it should be a max. Uh-uh.
It's kind of just rolling out a bunch of
It's kind of just rolling out a bunch of
these This
Maybe you have to like bump up the
Maybe you have to like bump up the
entropy a ton with this
method. That might be a decent baseline.
Let's go look at the MCTS scheme that
Let's go look at the MCTS scheme that
they use in the
meanwhile this
meanwhile this
paper. Um,
Okay, so MCTs with upper confidence
Okay, so MCTs with upper confidence
bounds. Every node of the search tree is
bounds. Every node of the search tree is
associated with an internal state
associated with an internal state
S. For each action, there's an edge SA
S. For each action, there's an edge SA
that stores a set of
that stores a set of
statistics respectively representing
statistics respectively representing
recent counts and policy P mean value Q
recent counts and policy P mean value Q
reward and state
transition action is selected according
transition action is selected according
to stored statistics by maximizing over
to stored statistics by maximizing over
a probabilistic upper confidence bound.
a probabilistic upper confidence bound.
Oh
jeez.
Yesh. That's
Yesh. That's
something. That is something.
There are a couple other things we can
There are a couple other things we can
try before we go nuts with
try before we go nuts with
this because I think that's going to be
this because I think that's going to be
I mean this is like the type of thing
I mean this is like the type of thing
where they probably spent a lot of
where they probably spent a lot of
effort getting a fast GPU implementation
effort getting a fast GPU implementation
of
of
this and there's like yeah there are a
this and there's like yeah there are a
lot of different things going going on
lot of different things going going on
in
here. This algorithm is very powerful
here. This algorithm is very powerful
though. This algorithm does feel very
though. This algorithm does feel very
powerful. Okay. So this uh we are going
powerful. Okay. So this uh we are going
to let this finish because what I want
to let this finish because what I want
to do is I want to compare this to 0.05
to do is I want to compare this to 0.05
entropy without MCTS. So basically like
entropy without MCTS. So basically like
if I make the policy more stochastic
if I make the policy more stochastic
does MCTS do better than
does MCTS do better than
um or not MCTS whatever search thing I'm
um or not MCTS whatever search thing I'm
doing does it do better than the
baseline and in the meantime I also want
baseline and in the meantime I also want
to see look this reanalyze
thing m0 revisits its past time steps
thing m0 revisits its past time steps
and re-executes it search using the
and re-executes it search using the
latest model parameters
Oh, that's hard.
Yeah, that's hard. Okay, that's that's
Yeah, that's hard. Okay, that's that's
hard to do with the on policy
hard to do with the on policy
uh formulation that we have.
They do have an equal tension up boot
They do have an equal tension up boot
strap.
It does kind of make you wonder.
How much you needed?
MCTS. Okay.
But it should definitely be doing better
But it should definitely be doing better
on a sample's axis, right?
Even with only six simulations per move,
Even with only six simulations per move,
at Museio learned an effective policy
at Museio learned an effective policy
and improved
rapidly. So we should actually like we
rapidly. So we should actually like we
should very well see
We have
We have
this. What if I just go to
this. What if I just go to
mu0? This should be the fast
one. Let's do this.
It's just so much faster. It's crazy.
Yeah, looks like this thing is still
Yeah, looks like this thing is still
just going to be better,
huh? Oh, it actually just matches like
huh? Oh, it actually just matches like
close to perfectly.
Am I somehow just not doing anything
Am I somehow just not doing anything
with
this? I wonder if we're like somehow
this? I wonder if we're like somehow
just
just
wrong in here. Let's look cuz we should
wrong in here. Let's look cuz we should
definitely see with 32 Sims. We should
definitely see with 32 Sims. We should
see something, right?
Do
Do
rewards plus
value argmax.
Okay.
So, yes. So these indices are supposed
So, yes. So these indices are supposed
to
to
be the
be the
best roll out according to the policy
right the returns
Max is
Max is
five, which is completely impossible,
right? Yeah. Five is completely
right? Yeah. Five is completely
impossible.
Hang on. The reward
function. Let's let me There are a few
function. Let's let me There are a few
more things that just like don't add up
more things that just like don't add up
here, I
think. So, data
think. So, data
policy reward model of state.hidden,
right? State hidden is hidden.
right? State hidden is hidden.
Okay, that should be differentiably
Okay, that should be differentiably
optimizing through the whole
optimizing through the whole
net. That should be a very good
prediction. And then I mean there's no
prediction. And then I mean there's no
reason not to optimize the world model
reason not to optimize the world model
through the net, right? Unless there
is. If we do this
And while we run that, I need to really
And while we run that, I need to really
think about this reward loss and what
think about this reward loss and what
that could possibly
that could possibly
be be wrong.
Wait, is this the wrong state? Hang
Wait, is this the wrong state? Hang
on. This is state hidden.
No, this state hidden should
No, this state hidden should
go into the
go into the
reward loss, right?
state.
Hidden reward reward model goes on to
Hidden reward reward model goes on to
state.
plus equal
plus equal
rewards reward
rewards reward
model on the hidden
model on the hidden
state. It's moo zero
state. It's moo zero
hidden which is the output hidden
state. This doesn't get updated.
Okay, this doesn't get updated. So, this
Okay, this doesn't get updated. So, this
is completely
screwed. Uh, this is worse as well
screwed. Uh, this is worse as well
without the
detaching the objective. Okay, so
So, we'll see what this
does. The two things to look at should
does. The two things to look at should
be the world loss and the reward loss.
Do we think this is correct
yet? Move zero
yet? Move zero
hidden. So this is now the next hidden
hidden. So this is now the next hidden
state coming out of the cell. This gives
state coming out of the cell. This gives
us reward prediction. We predict
us reward prediction. We predict
reward. We
sample. We append
actions that came from U0
logits. Okay.
I think we've corrected it, but now it
I think we've corrected it, but now it
doesn't
work. Reward loss is about the same as
work. Reward loss is about the same as
before.
Um it gets applied to hidden
Um it gets applied to hidden
state lstm equals
hidden
hidden
forward state
forward state
hidden set
there. I do not like this reward loss. I
there. I do not like this reward loss. I
really
don't. This feels like there's something
don't. This feels like there's something
very wrong with
it. But it's got to
it. But it's got to
be state hidden is what you get as the
be state hidden is what you get as the
output.
this forward pass up
this forward pass up
here. And
then targets just the reward
then targets just the reward
batch.rewards,
batch.rewards,
right? And do you want the reward that
right? And do you want the reward that
is you want to cut the first reward off,
is you want to cut the first reward off,
right?
Seems right to
me. It's not like it isn't training. It
me. It's not like it isn't training. It
is.
is.
But, you know, the entropy is very low
But, you know, the entropy is very low
now. I'm noticing
I could do random action and then follow
I could do random action and then follow
policy.
It's got to be such a low variation in
It's got to be such a low variation in
the
the
[Music]
Yeah. We could do epsilon technically.
Okay. It's not like this search doesn't
Okay. It's not like this search doesn't
work. It does. It's just not
work. It does. It's just not
efficient. And it's probably because of
efficient. And it's probably because of
this
It's like a quick epsilon grad.
Right. This is going to mess up log
Right. This is going to mess up log
problem.
No, it isn't. No, it isn't. It's fine.
The reward loss is still the thing that
The reward loss is still the thing that
bothers me. It's like way too high.
We're learning something here. I don't
We're learning something here. I don't
know if this curve is any
know if this curve is any
steeper. This is with epsilon gradient
on. Seems like it probably isn't.
What would be like a quick alternative
What would be like a quick alternative
sampling scheme I could use here?
I could do mean reward under
I could do mean reward under
policy instead of
policy instead of
best. Maybe that's more
robust cuz this is worse,
robust cuz this is worse,
right? This is like way worse,
right? Yeah, this is worse. Okay, but
right? Yeah, this is worse. Okay, but
really bad.
Wait.
Wait.
Returns
shape 32.
How do we group this
nicely? I mean, I can write a really
nicely? I mean, I can write a really
stupid loop, right?
I can't freaking do
I can't freaking do
this and find another thing I can do
this and find another thing I can do
instead. This is just going to be like
instead. This is just going to be like
it's this is just not easy to write
it's this is just not easy to write
with torch tensors at all. Like you just
with torch tensors at all. Like you just
you need to write a freaking CUDA kernel
you need to write a freaking CUDA kernel
for this. It's like so much harder to
for this. It's like so much harder to
write forch than it is to write CUDA
write forch than it is to write CUDA
ironically for this. Like you need to
ironically for this. Like you need to
just be able to write arbitrary loops
just be able to write arbitrary loops
and stuff.
It just work as the way it's
written. Really seems like it should at
written. Really seems like it should at
least do something the way it's written.
Try it again on go, but I don't really
Try it again on go, but I don't really
trust that item as much.
Art map should be totally fine. There's
Art map should be totally fine. There's
like a basic version of it. It should be
like a basic version of it. It should be
totally
totally
fine. I guess I haven't tried going
fine. I guess I haven't tried going
deeper
deeper
since since I fixed
since since I fixed
it. So, I can technically just see if
it. So, I can technically just see if
this does anything. I doubt
this does anything. I doubt
it, but
maybe cuz like five is a
maybe cuz like five is a
tiny number of steps to look ahead and
tiny number of steps to look ahead and
break
out. It could be that there's just
out. It could be that there's just
nothing that looking ahead five does for
nothing that looking ahead five does for
you to be fair.
This reward loss is pissing me off. I
This reward loss is pissing me off. I
don't understand how it's not like
don't understand how it's not like
Perfect.
the world loss is like really low,
the world loss is like really low,
right? Like this thing should
right? Like this thing should
have a decent grasp on the next OBS.
Well, to be
Well, to be
fair, yeah, in this end they're not like
fair, yeah, in this end they're not like
that would kind of make sense, I guess.
It should kind of tell me something that
It should kind of tell me something that
so far I like I exactly
so far I like I exactly
match PO, right? That should like be
match PO, right? That should like be
weird,
right? I mean, that could just mean that
right? I mean, that could just mean that
you're not really limited by picking bad
you're not really limited by picking bad
actions in the training here,
right? But that wouldn't make sense.
right? But that wouldn't make sense.
You should be limited by picking bad
You should be limited by picking bad
actions.
Yeah, this looks like just on par with
Yeah, this looks like just on par with
the
the
best non search
best non search
curve, right? 360 at 13 mil. This is
curve, right? 360 at 13 mil. This is
just like on par, right?
Yeah, it's right over
there. I mean, I could do like fewer
there. I mean, I could do like fewer
ends maybe.
Maybe that's the
trick. We like down sample M's by a
trick. We like down sample M's by a
factor of 32.
That could work.
I mean like what if Yeah. What if we
I mean like what if Yeah. What if we
just have so many parallel ends that we
just have so many parallel ends that we
just have a perfect estimate of
just have a perfect estimate of
everything for this end,
everything for this end,
right? Cuz actually like this run isn't
right? Cuz actually like this run isn't
bad.
Like it seems like this does
Like it seems like this does
something, but it could just be that we
something, but it could just be that we
already have perfect estimates of
already have perfect estimates of
everything
here, I say. And then this like takes a
here, I say. And then this like takes a
little
dip. Okay, I want to try I'll try
dip. Okay, I want to try I'll try
something.
64 total
ends. I might have been a little bit too
ends. I might have been a little bit too
low. We'll see.
So the hope with that was
So the hope with that was
like you need to have a decent number of
like you need to have a decent number of
parallel computations
parallel computations
um in order to use your hardware
um in order to use your hardware
efficiently, but like the MCTS kind of
efficiently, but like the MCTS kind of
does
Well, this doesn't seem to be
Well, this doesn't seem to be
particularly fast at all, now does it?
If we assume optimal batch is 32k for
If we assume optimal batch is 32k for
very small nets.
Yeah, I think I probably went too
Yeah, I think I probably went too
low on
this cuz this is going to take forever.
That should be at least 4x
faster. Let's try
this. Oh, you know what? I I think the
this. Oh, you know what? I I think the
real alpha would
real alpha would
be So, you divide
be So, you divide
this by eight, right?
this by eight, right?
And then what if you just divide this by
And then what if you just divide this by
eight as
well
65 same mini batch size do we think or
65 same mini batch size do we think or
do we go way smaller mini
do we go way smaller mini
bat we can't go way smaller because the
bat we can't go way smaller because the
BPT horizon maybe All
right. Okay. So, this frames
right. Okay. So, this frames
at
at
100k steps per second.
which is quite slow.
Can we
Can we
get
get
better training out of this?
Okay. So, this seems like it does
Okay. So, this seems like it does
something
maybe. But then the question is going to
maybe. But then the question is going to
be if we just don't use
be if we just don't use
MCTS, we get the same
thing. Yeah. So this is going to be
thing. Yeah. So this is going to be
dramatically
more if it continues on this trend.
I suspect what's going to happen is the
I suspect what's going to happen is the
on policy is just going to work just as
on policy is just going to work just as
well.
You kind of have to be limited by like
You kind of have to be limited by like
very small batches or something. I
think also this isn't performing as well
think also this isn't performing as well
as it should.
Seems like this is just
unstable. Yeah, this is just messed up.
So, I don't know what happened that got
So, I don't know what happened that got
it stuck here. It's
weird. Can't even do
this. Let me think if I can pick a
this. Let me think if I can pick a
different problem where I would know for
different problem where I would know for
sure.
The thing is like we kind of just get
The thing is like we kind of just get
perfect gradient information because we
perfect gradient information because we
have these massive
have these massive
batches. We really
do. And then we have advantage filtering
do. And then we have advantage filtering
that throws away the stuff we don't need
that throws away the stuff we don't need
anyways, right?
So like if you're training with a tiny
So like if you're training with a tiny
batch of data Hey babe.
kind of recovered. That seems like
kind of recovered. That seems like
optimization screw
optimization screw
up. It was on pace to be
up. It was on pace to be
faster. I guess I could try
faster. I guess I could try
um the normal PO on
um the normal PO on
this without
search. Just see like the initial curve
search. Just see like the initial curve
maybe.
This looks like it's just
better. Yeah, this is just like straight
better. Yeah, this is just like straight
up better, right?
So yeah, this is not even like a thing
where I guess I could try to solve it
where I guess I could try to solve it
with like tiny
with like tiny
bash. That could be like a
bash. That could be like a
decent decent thing to
do cuz then you definitely at the mercy
of
Yeah. This is probably just going to be
Yeah. This is probably just going to be
unstable because of normal normalization
though. 4096
size 64. That still seems
size 64. That still seems
big.
64. That's still
64. That's still
big. No, that's
big. No, that's
correct. So we do
like 64 m for 64
steps. See if this even trains with PO.
Yeah, I forgot about
Yeah, I forgot about
that. There's literally a min
that. There's literally a min
requirement.
How's this still an
How's this still an
issue? Agents
Ah, you need Yeah, you need 32 total
Ah, you need Yeah, you need 32 total
M's, which means like
I just want to see if this does
I just want to see if this does
anything.
Hey,
Hey,
bet. Trying to get Model Base to do
bet. Trying to get Model Base to do
something. So far, it's not doing
something. So far, it's not doing
anything. like it's working, but it's
anything. like it's working, but it's
not any better than
not any better than
um Model
3. Well, have fun with
3. Well, have fun with
that. Steps, threads per block.
Super
obnoxious eight.
obnoxious eight.
Wait time 8 64
Wait time 8 64
* 8 is 5. Yeah. So, this is literally
* 8 is 5. Yeah. So, this is literally
you can only have eight threads, I
you can only have eight threads, I
believe. It's so stupid, but we're going
believe. It's so stupid, but we're going
to try it for the sake of
it. Have to run.
Well, I'm not sure here. Um, if
Well, I'm not sure here. Um, if
basically we just have so much freaking
basically we just have so much freaking
data that there's just like no point in
data that there's just like no point in
trying to get better data because we
trying to get better data because we
just have all the data we could possibly
just have all the data we could possibly
need. I don't know.
What I'm doing now is I'm just reducing
What I'm doing now is I'm just reducing
it to a ludicrously small back and
it to a ludicrously small back and
seeing
Oh, this is cash. I'm dumb.
The thing that's weird to me, right, is
The thing that's weird to me, right, is
they use this on go, which is like a
they use this on go, which is like a
ludicrously fast environment if you
ludicrously fast environment if you
implement it. Well, so like they they
implement it. Well, so like they they
didn't care about the number of steps in
didn't care about the number of steps in
which they solve go, right? They were
which they solve go, right? They were
just trying to solve it. You would think
just trying to solve it. You would think
that this would do
something. The
something. The
That's not doing
Thought it'd fix that
bug.
Thought I didn't fix that bug.
Uh, this
Uh, this
is is this just like
40? No.
40? No.
Dummy
Dummy
one
one
60.
60.
Yes. Dummy.
bugs, man. I can't even test
What? I wasn't dividing. Yeah, I
What? I wasn't dividing. Yeah, I
know. It doesn't work because of shitty
know. It doesn't work because of shitty
bugs.
But I can't fix it here because
But I can't fix it here because
it's
it's
like so
obnoxious. You can do this, but I think
obnoxious. You can do this, but I think
that this just like is going to be
that this just like is going to be
invalid experiment results.
We'll see. I guess.
It's
It's
447. I don't want to try like the learn
447. I don't want to try like the learn
and hallucinated version.
This algorithm makes sense. I would
This algorithm makes sense. I would
think that it should
think that it should
work. It doesn't not work either, I
work. It doesn't not work either, I
guess.
Let's go back to the version that
Let's go back to the version that
actually makes
sense. Let's go back to the version that
sense. Let's go back to the version that
makes sense.
This is how it's supposed to look,
right? 360 at like 13 or whatever mil.
right? 360 at like 13 or whatever mil.
What if I do
this? So look one extra step
this? So look one extra step
ahead. The world model is trained one
ahead. The world model is trained one
step ahead.
You still train at a mostly reasonable
speed and you stop
speed and you stop
learning. You no longer learn remotely
learning. You no longer learn remotely
quickly. If I increase this to five or
quickly. If I increase this to five or
10, you recover the original curves.
10, you recover the original curves.
That seems
screwy. So you get plus
rewards, the reward that you expect to
get and then you get the
value in the next state.
There's definitely something going on
There's definitely something going on
here that like you recover the original
here that like you recover the original
performance of the algorithm if you
performance of the algorithm if you
increase the roll
increase the roll
out. There's definitely something going
out. There's definitely something going
on with
on with
that. Let me think about what that
that. Let me think about what that
means.
So why would perf
So why would perf
degrade versus the baseline is the
thing. So we're checking the reward that
thing. So we're checking the reward that
we expect to get, right?
Oh, hang
Oh, hang
on. Don't we
need No, these are the correct
actions. But this reward model is wrong,
actions. But this reward model is wrong,
isn't
isn't
it? Hang on.
it? Hang on.
How can you have a reward model based on
How can you have a reward model based on
the hidden state like
this? This needs to be the world model
this? This needs to be the world model
here needs to be
predicting the reward, doesn't
it? Because it has the hidden state.
it? Because it has the hidden state.
and it has the
action. So, let me try
that. Let me try that.
Okay. So now this is the thing that's
Okay. So now this is the thing that's
going to predict your rewards,
going to predict your rewards,
right? Let's see how this
right? Let's see how this
goes. So your world model
So you sample your logits, right? You
So you sample your logits, right? You
append and then this is going to give
append and then this is going to give
you okay. So now you actually have
you okay. So now you actually have
correctly conditioned
reward. This could have been the issue
reward. This could have been the issue
to be fair. This totally could have been
to be fair. This totally could have been
it.
So then this is neck
rewards. What do we think about this?
Okay, so this now
runs. See if that makes a
runs. See if that makes a
difference. Now the reward model is
difference. Now the reward model is
correctly action conditioned.
still nowhere near in Perf,
right? The reward loss is not better
either.
Totally should
be hidden is world model shared with
be hidden is world model shared with
your world model you do
right fine.
So this gives you your next
So this gives you your next
observation and the reward for this
observation and the reward for this
observation. And then you can
compute the value in your next state is
compute the value in your next state is
the idea,
right? Yeah, you compute the value in
right? Yeah, you compute the value in
next
state. How does the reward loss not get
state. How does the reward loss not get
better from being action conditioned?
better from being action conditioned?
That seems to me like it's just not
That seems to me like it's just not
predicting the right reward,
predicting the right reward,
right? I'm pretty sure it is,
right? I'm pretty sure it is,
but that loss is assessed. That loss is
sketchy. Zero
actions supposed to be predicting the
actions supposed to be predicting the
next reward.
next reward.
And then this thing has crashed
And then this thing has crashed
totally. It's supposed to be predicting
totally. It's supposed to be predicting
the next
reward. One reward in advance. Yep.
reward. One reward in advance. Yep.
Well, you roll it out so we can do that
Well, you roll it out so we can do that
repeatedly.
It's got to definitely be next
reward. No, you're not predicting 128
reward. No, you're not predicting 128
rewards. You're just predicting right
rewards. You're just predicting right
now one, but usually like five or 10
now one, but usually like five or 10
rewards.
Maybe this one you should back
Maybe this one you should back
propagate.
kind of
kind of
hard do Yeah.
Wait, you
Wait, you
dummy. Isn't
dummy. Isn't
this forward
train? That shouldn't none of that
train? That shouldn't none of that
matters.
Yeah, none of that mattered at all.
It should not be hard to predict the
It should not be hard to predict the
next reward. It should be very very very
next reward. It should be very very very
easy to predict the next reward.
So like 0159 is just random
So like 0159 is just random
uninitialized
uninitialized
garbage and it
Inputs targets inputs targets. It's
Inputs targets inputs targets. It's
correct.
Is there any chance I'm computing the
Is there any chance I'm computing the
actions
wrong? That would totally screw it up
wrong? That would totally screw it up
though. Like it wouldn't be partially
though. Like it wouldn't be partially
fine.
Something so wrong
here. Like it does optimize and then it
here. Like it does optimize and then it
goes back up I guess.
optimize particularly well
though. Not at
though. Not at
all. Right.
What should be a very easy
loss? Was this computed?
rewards come
rewards come
from. Well, you don't need to detach
from. Well, you don't need to detach
Jack Chitten here because this is this
Jack Chitten here because this is this
is the forward
pass. This is all detached
pass. This is all detached
anyways. And then the forward
anyways. And then the forward
here, mu0 hidden is what this gets
here, mu0 hidden is what this gets
applied to. So view zero hidden is the
applied to. So view zero hidden is the
initial LSTM state that comes out of the
initial LSTM state that comes out of the
policy and actions as
well. Hidden state coming out of the
well. Hidden state coming out of the
policy into the actions
That's like a random reward prediction
That's like a random reward prediction
right
right
there. I'm pretty
there. I'm pretty
sure maybe not to be fair. Maybe it's
sure maybe not to be fair. Maybe it's
substantially better. It probably is
substantially better. It probably is
substantially better is the thing
substantially better is the thing
because the number of rewards goes up a
ton. Yeah, you know, actually that is
ton. Yeah, you know, actually that is
possibly decent. Okay.
I'm going to run this again with
I'm going to run this again with
five. We haven't done this in a
while. I think about this.
Have your world
model. What about
values? Value is action agnostic,
right? Yeah.
could technically just do it over values
could technically just do it over values
or just do it over
rewards. Probably be just over rewards,
rewards. Probably be just over rewards,
right?
The fact though that it didn't decrease
The fact though that it didn't decrease
the reward loss at
the reward loss at
all. I guess maybe it's just a tiny
all. I guess maybe it's just a tiny
action decoder, so it kind of knows what
action decoder, so it kind of knows what
it's going to do
it's going to do
anyways. That wouldn't be
anyways. That wouldn't be
crazy. Be pretty
reasonable. What happened to episode
reasonable. What happened to episode
return here?
return here?
not doing
well. There's a reward network that's
well. There's a reward network that's
separate.
separate.
Yeah, it's just there's a separate goal
Yeah, it's just there's a separate goal
of just predict next reward.
Start. I could technically do it just on
Start. I could technically do it just on
the value function, right?
Not really. You kind of do need
reward. Okay, so now this is worse than
reward. Okay, so now this is worse than
before.
with the corrected action model. That's
with the corrected action model. That's
worse than before.
Let me see if there's anything I can do
Let me see if there's anything I can do
to bring this more in line with how mu
to bring this more in line with how mu
zero
works. Um, so I think that they don't
works. Um, so I think that they don't
send it through the
send it through the
model, right? I don't think that they
model, right? I don't think that they
have a target on
have a target on
the the world model itself.
I think theirs is like a separate
I think theirs is like a separate
network,
network,
right? That just
right? That just
outputs state and reward or whatever.
outputs state and reward or whatever.
State action
reward. So what would that do if I just
reward. So what would that do if I just
had a separate thing doing state action
had a separate thing doing state action
and reward?
would just take action as input.
We'd probably use the LSTM state or
We'd probably use the LSTM state or
something as the
input. We could try that. I'd be down.
input. We could try that. I'd be down.
It's 5:16. I've got an
hour. Maybe that makes it more
hour. Maybe that makes it more
stable. Let me I want to commit all
stable. Let me I want to commit all
this.
So we make the world model a
separate separate network, right?
No, not at
all. This needs the LSTM thing,
all. This needs the LSTM thing,
doesn't
it? Should be easier than the other
it? Should be easier than the other
ones.
It kind of should be a GRU,
right? Does it
matter? I don't think it matters
matter? I don't think it matters
actually. Does it?
I think we can just reuse this
I think we can just reuse this
probably. I'll have to screw with the
probably. I'll have to screw with the
GRU.
Hell's wrong with
this. So is it
just Okay, so this is the cell, right?
just Okay, so this is the cell, right?
It's got the cell and then it needs
It's got the cell and then it needs
input.
Action embed.
And then world model
And then world model
forward. It's actually going to
forward. It's actually going to
take actions and then hidden
take actions and then hidden
state and then it
will self to action embed actions.
Get
in and go
This works.
state and
state and
then hidden state gets to
then hidden state gets to
be. So action equals action head of the
be. So action equals action head of the
hidden reward head. It returns
hidden reward head. It returns
actions and
actions and
rewards and state.
Okay. So, let's think about
this. We can just put this in the LSTM
this. We can just put this in the LSTM
one, right?
This takes
This takes
actions and state and returns actions,
actions and state and returns actions,
rewards and state which is actually it
rewards and state which is actually it
should be logits rewards and state.
But it should
But it should
return
pockets
right. So this is world model
logs.
logs.
Okay. And then this has to
Okay. And then this has to
take zero
take zero
actions and do a zero
hidden and then the hidden should
be should it be the hidden state of the
be should it be the hidden state of the
model? Should it be the
embedding? I kind of like making it
embedding? I kind of like making it
the we'll give
it 0
equals obsed
U
C.
C.
Okay. Sample your
Okay. Sample your
logits. You have your logits, your
logits. You have your logits, your
rewards, and your state.
That's
rewards.
rewards.
Escape. Rolling
forwards.
forwards.
Then
Then
hidden. No longer need this
hidden. No longer need this
because this is
because this is
done. This is done.
You do need values, don't
You do need values, don't
you? I think you do need
values. Yeah.
state. That's pretty close.
state. That's pretty close.
Now in clean
puff, get out of here,
bot.
bot.
Okay, so we need we actually do need to
Okay, so we need we actually do need to
define this
define this
thing like this but for the world model.
thing like this but for the world model.
So easiest way to do
So easiest way to do
that buffer
that buffer
GRU it I I realize it's not going to
GRU it I I realize it's not going to
make a
difference. This is
difference. This is
forward. We have forward
train. Shit's completely fine.
ships completely
fine. Encode observations. I'm realizing
fine. Encode observations. I'm realizing
we can literally get rid of all this now
we can literally get rid of all this now
because the new API. That's going to be
because the new API. That's going to be
nice.
nice.
Um, yeah, there's no encode
Um, yeah, there's no encode
observations,
observations,
right? You just give
right? You just give
it the actions or whatever the
hell. I guess it's action m
You need any of this
Maybe that's
enough. Now hit in
I mean, does it freaking
I mean, does it freaking
matter? I just skip all this
You don't need to decode actions
You don't need to decode actions
here. You just
do. Okay, that works for
state.
Um,
okay. I think it's something simple like
okay. I think it's something simple like
this. We're going to have to debug this
this. We're going to have to debug this
into
into
existence. May code.
Stop. Neptune needs to not take
Stop. Neptune needs to not take
forever to
initialize. So you do need opposite
initialize. So you do need opposite
betting now or what?
betting now or what?
You need obsetting just to
You need obsetting just to
initialize uh just to initialize I
initialize uh just to initialize I
think. Okay.
cannot address what model
stayed. Hey there. How's it going? We're
stayed. Hey there. How's it going? We're
doing all right. We're deep in
doing all right. We're deep in
uh modelbased RL territory here.
Is this the correct initial state? I
Is this the correct initial state? I
don't think it
is. We need to get
is. We need to get
this state equals.
this state equals.
Yeah,
that's
okay.
54.
54.
Probably some shade
Uh that is not supposed to
happen. State does state get
transposed? State get transposed.
I didn't think state gets
I didn't think state gets
transposed. No, it doesn't. So, you give
transposed. No, it doesn't. So, you give
this
thing time and then batch, which is what
thing time and then batch, which is what
I gave it.
Why? Why is
Why? Why is
this
object expected hidden of zero size
object expected hidden of zero size
one?
one?
Oh yeah.
Oh yeah.
Um, hang on.
In this state, LSTMC isn't going to do
In this state, LSTMC isn't going to do
either, right? This needs to be
zeroed. Is this even getting trained
zeroed. Is this even getting trained
correctly? Hang
on. Yeah. I mean technically it is
right but
um doesn't have a cell state to start
with. I guess it just starts with zero
with. I guess it just starts with zero
for cell Okay.
Why is this the shape of this stupid
thing? I guess it's like
and do
and do
this and then it'll
be like this.
be like this.
Maybe that ought to be right shape.
sake. How about just zeros
like How about that?
Drive me
crazy. Five.
Too many values to
unpack. So this is going to return
unpack. So this is going to return
logits. Uh we don't need logits, right?
logits. Uh we don't need logits, right?
We
need We just
need We just
need What do we need for this? I think
need What do we need for this? I think
we literally just
we literally just
need
state target reward target state,
right? Well, we need value target.
How's the policy get trained then? Wait,
How's the policy get trained then? Wait,
what?
Okay. So
now we can
now we can
do
do
finally next state
finally next state
presence it's world model
presence it's world model
state
one
state or loss
We also have the freaking world model
We also have the freaking world model
policy laws, don't
we? I guess we want to like behavioral
we? I guess we want to like behavioral
clone this thing or something. What does
clone this thing or something. What does
it train the logits on this too?
It's the
It's the
objective policy.
So we
just we have the Diane loss that we were
just we have the Diane loss that we were
using.
I have to get these indexes All right.
H C.
Which one of these are you freaking
using? There's no more obs,
right? You just don't do this anymore.
right? You just don't do this anymore.
So can I do
I don't think you need
um I think you can just use
um I think you can just use
I'll just use
I'll just use
this. You put
the it gets the observation
in starting
in starting
from the state
from the state
which gets an OBS
which gets an OBS
embedding. So it gets the first
embedding. So it gets the first
OBS and then it's going to
produce. Does this thing have a policy
produce. Does this thing have a policy
on it?
It has to,
right? So, I guess that's like the
right? So, I guess that's like the
previous action or whatever.
Now this is the action that's going to
Now this is the action that's going to
produce the next
produce the next
state. So you took this action with this
state. So you took this action with this
ops.
ops.
Okay. And then the reward this makes
Okay. And then the reward this makes
sense. This is
sense. This is
offset. Uh the value
function is is I think that this
is this
The actions
The actions
also just like this
Come on.
Value
loss. New value.
Okay, better.
Okay, we're now actually maybe getting
Okay, we're now actually maybe getting
somewhere. Um, I don't think we're doing
somewhere. Um, I don't think we're doing
anything remotely
sane in the Ford Passover here, right?
Oh, no. We
Oh, no. We
are. So, we just sample. We just roll
are. So, we just sample. We just roll
out through the world
out through the world
model. And this kind of does stuff, I
model. And this kind of does stuff, I
guess,
right? And um we probably want to train
right? And um we probably want to train
this thing with
this thing with
like the same
value loss, I
value loss, I
guess. How do we want to do
this? I mean, you have this like extra
this? I mean, you have this like extra
thing now.
Yeah. So, I kind of see that this would
Yeah. So, I kind of see that this would
be way simpler.
Um, this would be like way
simpler if your policy weren't recurrent
simpler if your policy weren't recurrent
because then you use the state to
because then you use the state to
represent the world model, but it has to
represent the world model, but it has to
be separate because of that.
be separate because of that.
Okay. I have a crazy idea and I'm not
Okay. I have a crazy idea and I'm not
qualified to build it. Sure, go ahead.
It's crazy. This does actually kind of
train reward
loss. So that's going to be squared
loss. So that's going to be squared
error with the actual reward. That's
easy. I don't advocate that particular
easy. I don't advocate that particular
slogan bet.
slogan bet.
But I I would just say to learn the damn
You could very easily do something that
You could very easily do something that
pretends to do that
pretends to do that
um with the language models and it would
um with the language models and it would
just be like bollocks. It would just all
just be like bollocks. It would just all
be smok and
mirrors. It would have like no
mirrors. It would have like no
predictive power of
anything even remotely possible. No, not
anything even remotely possible. No, not
at all.
It's kind of like um there's like a lot
It's kind of like um there's like a lot
of adjacent language model stuff that's
of adjacent language model stuff that's
like really flashy looking but just
like really flashy looking but just
doesn't make any sense if you think
doesn't make any sense if you think
about it for a few Thanks.
Man, so we just have to fix these losses
Man, so we just have to fix these losses
now, huh?
Probably this doesn't need to be clipped
Probably this doesn't need to be clipped
to be
honest. But then what do we do with
this? Does this just need um
I got rid of the encoding piece, which
I got rid of the encoding piece, which
is good.
supplementary figure
2, which is not in
here. Where's the non version of
here. Where's the non version of
this? I don't know why they publish so
this? I don't know why they publish so
much crap in nature. It's like their
much crap in nature. It's like their
format just
sucks. They have this on it's on archive
sucks. They have this on it's on archive
and now you can actually read
and now you can actually read
it. Yeah. Here you go. The nature paper
it. Yeah. Here you go. The nature paper
format is just
No, I think it's the opposite. I think
No, I think it's the opposite. I think
there's a strict page
there's a strict page
limit, isn't
it? The hell is this?
Huh? The
Oh, this is their bucketed thing.
Oh, this is their bucketed thing.
We don't have to care that
We don't have to care that
much. Okay. Squared error
loss then.
We can make the world model
We can make the world model
bigger. I don't think
bigger. I don't think
that's could be the issue. I don't
know. This thing is supposed to scale.
It's pretty cheap to train this as
It's pretty cheap to train this as
well. The inference cost that'll get you
Isn't the reward loss higher than
before? Hang on. This gets H is the OBS
before? Hang on. This gets H is the OBS
embedding and then
C there's no
C there's no
C because it's zeros right on first
Well, hang on. How's
this? This is training to predict next
reward,
right?
right?
Yes, the value function is training.
I think I got to go for
I think I got to go for
dinner. Value function is training to
do. So you do the value function needs
do. So you do the value function needs
to be trained to predict the next value
to be trained to predict the next value
I believe.
Try
that. I don't think that'll do anything,
that. I don't think that'll do anything,
but we'll see.
Okay, we will continue on
Okay, we will continue on
this. We will continue on this
this. We will continue on this
afterwards. Uh I Let me go double check
afterwards. Uh I Let me go double check
see if dinner's if it's dinner time.
Okay, I will be back after dinner. Um,
Okay, I will be back after dinner. Um,
thanks for tuning in, folks. There will
thanks for tuning in, folks. There will
be hopefully a little bit more world
be hopefully a little bit more world
modeled stuff later tonight. I'd really
modeled stuff later tonight. I'd really
like to get this model working. Um, I
like to get this model working. Um, I
just I was going to invest a day in it
just I was going to invest a day in it
for now and then we'll come back later.
for now and then we'll come back later.
But uh all my stuff's at
puffer.ai. You can check out all open
puffer.ai. You can check out all open
source code. If you want to help me out
source code. If you want to help me out
for free, start the GitHub. If you want
for free, start the GitHub. If you want
to get involved with development, join
to get involved with development, join
the
the
Discord. Some of our best contributors
Discord. Some of our best contributors
came in with zero RL experience. And if
came in with zero RL experience. And if
you want more RL content, you can follow
you want more RL content, you can follow
me on X. Post all sorts of things there.
me on X. Post all sorts of things there.
Uh other than that, thank you and see
Uh other than that, thank you and see
you

Kind: captions
Language: en
We are back
We are back
live
with more modelbased RL
with more modelbased RL
stuff. I left off implementing a hacky
stuff. I left off implementing a hacky
version of
version of
M0 and if I
M0 and if I
recall needed to train the word reward
recall needed to train the word reward
model needed to fix the action
model needed to fix the action
encoder. Let's get to it.
thing. Oh yeah, this needs to
thing. Oh yeah, this needs to
be Hang on.
I see. So the value is going to be
I see. So the value is going to be
different maybe.
Why is stream not
show? There we go. Hey
show? There we go. Hey
folks, back to getting this implemented.
I'm not sure how Muse will do versus
I'm not sure how Muse will do versus
Dreamer to be honest. We need to see
Dreamer to be honest. We need to see
uh view comparisons on
uh view comparisons on
stuff. I would like to see if I can at
stuff. I would like to see if I can at
least get it to do something though.
Uh, okay. So, these are
Uh, okay. So, these are
misaligned. That's sketchy.
State hidden.
Okay, so these are now
Okay, so these are now
aligned. Um, we're going to want to
aligned. Um, we're going to want to
predict one ahead like before. So, this
predict one ahead like before. So, this
is going to
be this is going to be star
More
loss. Define the loss.
See how this
does.
does.
Okay, this does
run. Cool. This does actually run.
run. Cool. This does actually run.
Um, now we just need to figure out
Um, now we just need to figure out
the encoder, I believe.
So, do we just have to add a layer? Is
So, do we just have to add a layer? Is
that all we have to do?
I guess that this should
be
single
broad. Okay. And this is going to we
broad. Okay. And this is going to we
have to add like a
projection. Yeah, we have to add a
projection. Yeah, we have to add a
project.
This
is there's a better way to do this.
I don't think so.
So we have this action
So we have this action
projection and then instead of this
projection and then instead of this
concatenate
concatenate
here this needs embed
here this needs embed
equals
equals
coder and then
concat then we do
concat then we do
this. So you have this action project.
Now this is the hidden
state. State.hidden has to equal this
embedding. And then forward
here. We do m zero.
something like thisish, I
something like thisish, I
think. And then you shouldn't have to
think. And then you shouldn't have to
do this on the rest of them, maybe.
Think that's how it
works. Be tenser. Not left.
It should be one hot actions.
Okay, so this will almost be at the
Okay, so this will almost be at the
point where it could
point where it could
work. Uh, this is going to be way too
work. Uh, this is going to be way too
slow is the only issue.
I think what we want to do
is do we replicate
trial? I think we
trial? I think we
replicate across trials,
right? So, we only do one set of forward
right? So, we only do one set of forward
passes. We don't do a double
passes. We don't do a double
loop. I think I can do that. And I think
loop. I think I can do that. And I think
it'll fit in memory. Maybe.
Does the LSTM cell not accept batched
dimensions? It probably doesn't, right?
Dropbox running. Only weird thing is SPS
Dropbox running. Only weird thing is SPS
recording is erratic. If you're on dev,
recording is erratic. If you're on dev,
that's normal. We uh there's some screwy
that's normal. We uh there's some screwy
stuff in in
that. If you see the new profiling
that. If you see the new profiling
though, it's really cool. I just there's
though, it's really cool. I just there's
some things to
some things to
fix. I'm currently attempting to uh
fix. I'm currently attempting to uh
implement like this
implement like this
um not quite Monte Carlo research but
um not quite Monte Carlo research but
this searchbased edition that uses a
this searchbased edition that uses a
world model. So I'm going to see how
world model. So I'm going to see how
much I can make how much progress I make
much I can make how much progress I make
on
on
this. Saturday is a good day to just
this. Saturday is a good day to just
crank dev
Okay, so this did
something. So
now we have to do
We should reshape
We should reshape
these. We only
need rewards into action.
probably how that repeat works,
right? No.
Is this like a gather? There.
Oh, cuz you're doing it completely wrong
Oh, cuz you're doing it completely wrong
is why.
So we have the rewards here which are
So we have the rewards here which are
all zero right now. Lovely or the
all zero right now. Lovely or the
indices are
zero. This is
zero. This is
idxis first of all.
I think this is correct.
No. Okay, we'll check this. But I think
No. Okay, we'll check this. But I think
this is the roughly correct
this is the roughly correct
thing. So now we have 2,000 actions or
thing. So now we have 2,000 actions or
whatever.
So this is like the full vectorzed
version. And what now we have to
version. And what now we have to
return the actions instead of the logics
return the actions instead of the logics
I
guess. So we just do like
Something like this.
Something like
this. Okay. So, this is at 800K somehow.
This is like still reasonably
fast. Now we have to look at like how I
fast. Now we have to look at like how I
screwed it up
obviously. Um but in general this should
obviously. Um but in general this should
make
make
sense. Reward loss is
sense. Reward loss is
zero. Reward loss start at zero.
I didn't log
it. Okay, cool. There is a reward loss
it. Okay, cool. There is a reward loss
now.
So, I think then we just have to get
So, I think then we just have to get
this
this
correct and we should actually be good.
Um, I'm trying to think if there's
Um, I'm trying to think if there's
anything else I've missed. I don't think
anything else I've missed. I don't think
there is,
though. I guess we just like
Let it at least train a couple
Let it at least train a couple
steps. Okay,
so
so
shape. So now we've got this massive
shape. So now we've got this massive
hidden
hidden
state. It's repeated a whole bunch.
So if I go for 0 0
here and if I go for
here and if I go for
zero the one zero here. Yeah, they
zero the one zero here. Yeah, they
should give me the same thing. And then
should give me the same thing. And then
if I go for
if I go for
like 01. It should be
like 01. It should be
different.
different.
Yes. Okay. So, I think I have this
Yes. Okay. So, I think I have this
correct in the
view. So,
view. So,
[Music]
then one
then one
hot tape.
uh these should not all be zeros, right?
State.action. Oh, I think it's
State.action. Oh, I think it's
because there's probably something's
because there's probably something's
free with the sampling, right? Okay. So,
free with the sampling, right? Okay. So,
let's do it this
let's do it this
way. Because if all the actions are
zero. Yeah, there is variance in the
zero. Yeah, there is variance in the
rewards here, right?
And there
is 01 and two. There is variance in the
is 01 and two. There is variance in the
actions a little
bit. Okay, this is all zero.
That doesn't look correct.
Oh, the rewards are the same exactly in
Oh, the rewards are the same exactly in
each. Okay. Yeah, we definitely have
each. Okay. Yeah, we definitely have
something screwy then.
Because the action getting appended
here.
here.
Action. This should be move
Action. This should be move
zero action, right?
Yeah, we should not be getting all the
Yeah, we should not be getting all the
same
action. Okay, so this gives us separate
action. Okay, so this gives us separate
indices, right?
So if I go forward one
Stop. Try
Stop. Try
something. If we go forward one
something. If we go forward one
step and we only have n= 1, what
step and we only have n= 1, what
happens?
Okay, I think that the
Okay, I think that the
uh the sanity here is going to be that
uh the sanity here is going to be that
this should be the same as the original
this should be the same as the original
algorithm pretty much,
right? We'll have to see what the exact
right? We'll have to see what the exact
differences are, but this should recover
differences are, but this should recover
original algorithm PF because now this
original algorithm PF because now this
is just going
is just going
through
through
and you sample logits.
There's only one to
select. This is the sanity right here.
select. This is the sanity right here.
This will be the sanity.
and the way that we make sure that this
and the way that we make sure that this
is the
is the
same, we just comment all of this crap,
same, we just comment all of this crap,
right?
No, I guess we can't set the action as
No, I guess we can't set the action as
well.
Okay, so this is the one that recovers
Okay, so this is the one that recovers
perf
Okay, this is also fine
here. So now the question is going to be
How is this here
different? This hidden
different? This hidden
state. Okay, wait. This is a different
state. Okay, wait. This is a different
hidden state. Hang on.
This is a different
hidden. This is state.hidden.
How exactly did we do
How exactly did we do
this? Embedding concatenate with one
hot encoder on
observations and then concat and
observations and then concat and
project. All right.
This should be identical to
This should be identical to
before. So what I should be able to do
before. So what I should be able to do
then this should equal
hidden. This should exactly equal the
hidden. This should exactly equal the
hidden state.
Okay, so this is correct up to
here and
here and
then
then
cell is called on this and then the mu0
cell is called on this and then the mu0
lstm state.
which is just
agency,
agency,
right? That's
correct. You call decode actions on the
correct. You call decode actions on the
hidden
hidden
state. It's also
state. It's also
correct. So, this does look identical to
correct. So, this does look identical to
me. Okay. So, something
me. Okay. So, something
Possibly something else is
Possibly something else is
screwing. Let me just get a a random
screwing. Let me just get a a random
sample that's not from the
star. Identical, right?
star. Identical, right?
So we got the hidden state
correct and then you get move zero
correct and then you get move zero
hidden objects values. Yes.
This take is the wrong one. Yeah, this
This take is the wrong one. Yeah, this
take has got to
take has got to
be
wrong. I think this is it.
Hey, welcome.
That's not
it. Still not that. All right, this is
it. Still not that. All right, this is
just me screwing up
just me screwing up
indexing. What are you doing? I'm trying
indexing. What are you doing? I'm trying
to get sort of a form of mu0 into puffer
to get sort of a form of mu0 into puffer
lib so we can test some like search
lib so we can test some like search
basedbased RL.
Is this
it? Yeah, that's still not
it. What kind of applications is
it. What kind of applications is
tailored to Mu
tailored to Mu
Zero?
Zero?
Um, well, that's the cool thing with Mu
Um, well, that's the cool thing with Mu
Zero, right? It's not like alpha zero
Zero, right? It's not like alpha zero
where you need a you need a uh like a
where you need a you need a uh like a
state set and get function for your end.
state set and get function for your end.
It can kind of just learn it for
It can kind of just learn it for
whatever. So it's a pretty way clean way
whatever. So it's a pretty way clean way
of doing model based RL with search
of doing model based RL with search
without needing to have like a state set
without needing to have like a state set
like you can have with go or
chess. I'm just blanking on how to do
chess. I'm just blanking on how to do
this really simple indexing operation
this really simple indexing operation
and then we should have something
Why is this like so
weird? Isn't it just like this of
star? No.
Not this either.
Tensor processing libraries were a
Tensor processing libraries were a
mistake. I
mistake. I
swear. So much time spent doing
indexing. This is like so unbelievably
indexing. This is like so unbelievably
stupid.
It's still freaking
wrong. Oh, here it is.
Okay, we're going to just use this for
Okay, we're going to just use this for
now and then we'll figure out a better
now and then we'll figure out a better
way of doing
it. Come
on. Okay. Dot
squeeze. So if I've done this right,
squeeze. So if I've done this right,
this should match the original
curve roughly.
Yeah. How's this still not match the
Yeah. How's this still not match the
original curve?
original curve?
It's like better than before, but it's
It's like better than before, but it's
still nowhere near as good as the
still nowhere near as good as the
original. Is this not identical?
Okay, let's try this.
This should reproduce,
right? Okay. So, this is the same error
right? Okay. So, this is the same error
as before. This is the same curve. So,
as before. This is the same curve. So,
let's see why that is the
let's see why that is the
case. So,
return logits and
values. Action gets to be
stateaction instead of getting
stateaction instead of getting
sampled. You just sampled it before,
sampled. You just sampled it before,
right?
And then if I comment
this and I do
this and I do
action like so it's a
fix. Yeah. Okay. So this is just I've
fix. Yeah. Okay. So this is just I've
made a mess somehow. I'm getting
made a mess somehow. I'm getting
confused
confused
because otherwise this would be totally
because otherwise this would be totally
fine. Action
fine. Action
state.action. This
state.action. This
[Music]
[Music]
is sample logs.
That's very weird. Very very weird.
Yeah.
Yeah.
So state
action. These are the same logics, are
action. These are the same logics, are
they not?
Okay, this seems to
work now. Can I do this?
No, I cannot do this for some
reason. Different
curve. Is
it? And this is working, but
it? And this is working, but
3 250. Maybe this just
variance. Sounds good.
So this is cell on hidden and LSTM
So this is cell on hidden and LSTM
state.
I guess you can technically do it.
I guess you can technically do it.
You can technically do it this way.
Can I just
like comment this? comment
this. Okay, that's the good one. That's
this. Okay, that's the good one. That's
the good
the good
curve
now. You can do this just like
Okay, this is just too much to think
Okay, this is just too much to think
about. This is breaking my freaking
about. This is breaking my freaking
brain because I haven't like I've done
brain because I haven't like I've done
this in a really messy way and I no
this in a really messy way and I no
longer know what the I'm doing. So,
longer know what the I'm doing. So,
let me just clean this up and make this
let me just clean this up and make this
actually all make sense.
actually all make sense.
Um, yeah, there's just too many shitty
Um, yeah, there's just too many shitty
tensor operations to follow.
Okay.
Okay.
So, the first thing I need to figure out
So, the first thing I need to figure out
is the way that these actions
is the way that these actions
work.
work.
Like you
pass action at last step, right?
And this is going to get the
reward. So for the first step, these
reward. So for the first step, these
should all
should all
match because you have the same actions
match because you have the same actions
from last step.
from last step.
This is really just getting you your
This is really just getting you your
initial encoding state,
initial encoding state,
right? And then from there it can
change.
change.
Okay, so that makes slightly more
sense. Can you not feed multiple? I
sense. Can you not feed multiple? I
think the LSTM does require me to do it
think the LSTM does require me to do it
this way,
unfortunately. Here's your action
unfortunately. Here's your action
projection. Here's your cell. Here's
projection. Here's your cell. Here's
your
your
decode. logits and
values. M0 actions is the one on the
values. M0 actions is the one on the
first step
first step
taken. Yes.
Okay. So, I it's this indexing operation
Okay. So, I it's this indexing operation
right here, which is a freaking mess,
right here, which is a freaking mess,
which is just confusing the hell out of
which is just confusing the hell out of
me, I'm pretty
me, I'm pretty
sure. So, I just have to find a better
sure. So, I just have to find a better
way to do this operation, and then
way to do this operation, and then
everything will get a lot easier.
I don't know why I bashed it this way.
I don't know why I bashed it this way.
This was kind of stupid.
want to take one of these each, right?
want to take one of these each, right?
One of these for each
equals
two. God damn it. Yeah, you can't do
two. God damn it. Yeah, you can't do
this
this
because LSTM cell just sucks.
So obnoxious. Okay.
Nope, not this.
there. So
such
If this freaking works.
Can we not just like
Can we not just like
do one extra forward pass and
do one extra forward pass and
not deal with this horshit? We totally
not deal with this horshit? We totally
can, right?
this. We really just need this actions.
going to drive me
going to drive me
insane. Freaking thing.
You know, I start to wonder what the
You know, I start to wonder what the
hell the point is of all these fancy ass
hell the point is of all these fancy ass
frameworks when it's like I'd almost
frameworks when it's like I'd almost
rather just be writing CUDA at this
rather just be writing CUDA at this
point.
point.
Almost. It would be really easy
Almost. It would be really easy
actually. Like CUDA with a couple basic
actually. Like CUDA with a couple basic
utils would be way easier than this.
Okay, this should be the fast one,
right? This doesn't optimize anywhere as
right? This doesn't optimize anywhere as
near as quickly as the other one.
Okay. State action
Okay. State action
gets move zero
gets move zero
actions values
logits moo zero hidden get state.hidden
How are these freaking
How are these freaking
different? There's no way.
Hidden's different now. I thought we
Hidden's different now. I thought we
checked that like extensively.
Nope. These don't freaking match either.
Nope. These don't freaking match either.
Okay. How do these not
Okay. How do these not
match? How do these not match?
You take mu zero
actions. You
actions. You
append m zero hidden. Mero kitten comes
append m zero hidden. Mero kitten comes
from
state.hidden, right? And then this is
state.hidden, right? And then this is
supposed to be right here. Go with one.
So right here, this is supposed to match
So right here, this is supposed to match
a rig
hidden. Okay, so this
matches. This matches, right? No, this
matches. This matches, right? No, this
doesn't match. What the
doesn't match. What the
hell? Move zero LSTM state
maybe. Ah, you dummy.
right here.
Okay. Mu Z hidden and hidden now
match. Does this give a S
curve? It should give a S curve.
Nope. Still
curve.
Identical objects.
LSTMH LSTMC get updated from
LSTMH LSTMC get updated from
here. Where's hidden get updated? Hidden
here. Where's hidden get updated? Hidden
should get updated in here, right?
State.hidden gets updated
State.hidden gets updated
there. That's everything.
Seems like this should be
Seems like this should be
fine, I guess. Wait, log
fine, I guess. Wait, log
prop. Maybe it's
prop. Maybe it's
this. No, but log prop. Oh, cuz this
this. No, but log prop. Oh, cuz this
returns log prop of the action. This is
returns log prop of the action. This is
why.
Log. Yeah. Yeah, this has got to be it.
So you can return logits but not log
So you can return logits but not log
problem.
Okay, there. That's the fast curve.
Okay, there. That's the fast curve.
Good. Works as expected. Finally. Now,
Good. Works as expected. Finally. Now,
next next is going to
next next is going to
be first we do this.
So 360 at 13
mil. They even change anything.
Okay, that
Okay, that
matches. So this end doesn't matter
matches. So this end doesn't matter
apparently I like at
apparently I like at
all. I guess it's just really nicely
all. I guess it's just really nicely
batched for very small
networks. And now this is where uh
networks. And now this is where uh
things can change a bit.
Okay. So, this is a little far a little
Okay. So, this is a little far a little
bit behind but not far behind.
Now, how many steps do they look
Now, how many steps do they look
ahead in
this How
many steps ahead does this
many steps ahead does this
look? Unroll for K equals five
steps. So then this is
valid. So I guess we just need to like
valid. So I guess we just need to like
scale this
maybe. Hope we don't run out of
memory. See what this does.
So, we're going to definitely have to
So, we're going to definitely have to
try this on a different end.
fully collapsed entropy as well. So,
fully collapsed entropy as well. So,
this really shouldn't be doing much over
this really shouldn't be doing much over
um
um
Yeah, it's
Yeah, it's
fine. It's generally fine.
Let's go to something like
go. Yeah. So, go is a good one here.
Okay. So this is
like baseline
thing. Really doesn't do very much,
thing. Really doesn't do very much,
right?
Let's just do
Let's just do
this and see
uh see what it does based on Yes.
Can you guide what this is? Uh, this is
Can you guide what this is? Uh, this is
me attempting to get some sort of model
me attempting to get some sort of model
based RL into buffer lib. So I have this
based RL into buffer lib. So I have this
like moo zeroesque search thing where
like moo zeroesque search thing where
for every action
for every action
um or for every like trajectory
um or for every like trajectory
It tries 32 times to generate
It tries 32 times to generate
alternative trajectories and it tries to
alternative trajectories and it tries to
see which one is the best before it
see which one is the best before it
takes an
takes an
action. So far, I have not gotten this
action. So far, I have not gotten this
to really do anything
though. And this definitely doesn't help
though. And this definitely doesn't help
here. This is like not
here. This is like not
stable. This is just
stable. This is just
stuck. Okay.
There's one other experiment I want to
There's one other experiment I want to
run on
this. Just
Now that's a fully random search.
Yeah, this doesn't seem to help here
Yeah, this doesn't seem to help here
either. You're doing programming. Well,
yeah. This whole channel is
yeah. This whole channel is
reinforcement learning research done
reinforcement learning research done
live.
Okay. So, this doesn't help. Go back to
breakout. I mean, it is possible I've
breakout. I mean, it is possible I've
just done something screwy here.
and the reward loss is very
low.
Um, yeah, but that doesn't
Um, yeah, but that doesn't
seem okay. That definitely suggests
seem okay. That definitely suggests
there's an error here.
ID. This is just
Neo. Okay, I'm actually suspecting I
Neo. Okay, I'm actually suspecting I
just have something wrong based on this.
just have something wrong based on this.
I'm not going to abandon this just
I'm not going to abandon this just
because I did it
because I did it
wrong. Um, let me
see. I mess up the action logic or
see. I mess up the action logic or
something.
something.
Sample
Sample
logs, decode actions, hidden
That actually seems like it should be
That actually seems like it should be
fine.
fine.
Um, I do wonder if I messed up the
Um, I do wonder if I messed up the
reward
loss cuz that would also do
it. Hang on. I could technically do
this. I've seen all this for the first
this. I've seen all this for the first
time in my life. So well yeah you will
time in my life. So well yeah you will
be a bit confused. Um the whole idea of
be a bit confused. Um the whole idea of
this channel
this channel
here I am a researcher. I do
here I am a researcher. I do
reinforcement learning research
reinforcement learning research
full-time and uh I build a whole bunch
full-time and uh I build a whole bunch
of open source
of open source
software and I do all of my development
software and I do all of my development
live. So people come by who use the
live. So people come by who use the
library, they ask questions, people get
library, they ask questions, people get
involved in dev. I review code
involved in dev. I review code
contributions live sometimes.
This the goal here generally is to just
This the goal here generally is to just
advance this branch of
science. Well, that was stable for
science. Well, that was stable for
longer and then blew
up. Weird. Very, very weird.
How does this compare to like the
How does this compare to like the
dreamer
style? I don't want to abandon this just
style? I don't want to abandon this just
yet, though. The movie the Miro style
yet, though. The movie the Miro style
thing actually seems pretty good. If we
thing actually seems pretty good. If we
can get it to
work, the fact that they train only on
work, the fact that they train only on
the world model seems really jank,
the world model seems really jank,
right?
MO zero trains on real data and uses
MO zero trains on real data and uses
more compute to get better real data
more compute to get better real data
using the world model to like plan
using the world model to like plan
forward. It also gives you test time
forward. It also gives you test time
compute scaling for
free. Dreamer uses a much larger
free. Dreamer uses a much larger
model. The forward pass or the data
model. The forward pass or the data
collection step is very short.
And then it just spins training for a
And then it just spins training for a
long
time. I This is live research,
right? I mean, I have a PhD in this
right? I mean, I have a PhD in this
stuff. I've been doing this like half of
stuff. I've been doing this like half of
my life.
though the math side is actually my
though the math side is actually my
weakest part here. Um, I'm far better on
weakest part here. Um, I'm far better on
the engineering side.
I
mean, which of these makes more sense or
mean, which of these makes more sense or
do they both make sense?
So in one of these you hallucinate a ton
So in one of these you hallucinate a ton
of
of
trajectories and then train on them
trajectories and then train on them
directly and in the other you
directly and in the other you
hallucinate a bunch of
hallucinate a bunch of
trajectories. You pick which one you
trajectories. You pick which one you
think is the best and then you take the
think is the best and then you take the
action according to that trajectory.
action according to that trajectory.
Right?
Move zero seems like it should be way
Move zero seems like it should be way
more likely to work quickly. I would
more likely to work quickly. I would
think you're not really changing the
think you're not really changing the
learning dynamics a ton. You're really
learning dynamics a ton. You're really
just you're strictly using a world model
just you're strictly using a world model
to get better data with the simplest
to get better data with the simplest
formulation of this.
So you're scaling compute in order to
So you're scaling compute in order to
get better training
get better training
data. That
data. That
seems that seems like this should be
seems that seems like this should be
easier to get to work. So I don't think
easier to get to work. So I don't think
I should even think about trying to go
I should even think about trying to go
to dreamer today. I think I should just
to dreamer today. I think I should just
try to get
this for each game in
this for each game in
Atari. 20 billion frames we used a
Atari. 20 billion frames we used a
lot GPU for
lot GPU for
selfplay equivalent to two weeks of
selfplay equivalent to two weeks of
training on one
training on one
GPU. It's a lot of freaking
training. They did 50 simulations per
training. They did 50 simulations per
move.
Is there an
end or I would expect to need to
end or I would expect to need to
plan? No, it shouldn't matter. I should
plan? No, it shouldn't matter. I should
just be able to use this on breakout and
just be able to use this on breakout and
get better perf.
Honestly, I I should be able if this
Honestly, I I should be able if this
works, it should at least be able to
works, it should at least be able to
match on that, I would
think, cuz the policy is stochastic,
think, cuz the policy is stochastic,
right? So, this would at least give you
right? So, this would at least give you
the effect of
the effect of
determinizing the policy. Yeah. Okay.
If I tell it rand to take random
If I tell it rand to take random
actions, it blows up
immediately. I don't know why I'm having
immediately. I don't know why I'm having
well I mean it's mostly intended for
well I mean it's mostly intended for
technical audience but this is I mean
technical audience but this is I mean
this is scientific progress happening
this is scientific progress happening
live. If this thing works, then we will
live. If this thing works, then we will
immediately have uh a way of scaling all
immediately have uh a way of scaling all
of our AI training with compute way more
of our AI training with compute way more
effectively than we have now um and
effectively than we have now um and
applicable to way more
problems. So I usually on Saturdays I do
problems. So I usually on Saturdays I do
like deep dives into crazy rabbit holes
like deep dives into crazy rabbit holes
and like I try to like look at something
and like I try to like look at something
out there and
out there and
uh see what I can get working in like
uh see what I can get working in like
the span of a day.
the span of a day.
It's kind of just meant to see like what
It's kind of just meant to see like what
are promising directions that we could
are promising directions that we could
take. Oh, if you want to see something
take. Oh, if you want to see something
cool that you will actually understand.
cool that you will actually understand.
So on puffer.ai, we have all our demos
So on puffer.ai, we have all our demos
which are mostly just a bunch of games,
which are mostly just a bunch of games,
right? And uh these games are being
right? And uh these games are being
played by neural nets that we trained
played by neural nets that we trained
live in your
live in your
browser. And you can hold shift and you
browser. And you can hold shift and you
can take over and you can try to play,
can take over and you can try to play,
but you're going to not be as good as
but you're going to not be as good as
the agent for most of these. You know,
the agent for most of these. You know,
we've got like this race thing. This is
we've got like this race thing. This is
like a superhuman racing
like a superhuman racing
AI. See how quickly it responds to
AI. See how quickly it responds to
everything. And this is trained purely
everything. And this is trained purely
just by playing the game. This plays the
just by playing the game. This plays the
game a whole bunch and it figures out
game a whole bunch and it figures out
how to play
it. Same thing with like multi-agent
it. Same thing with like multi-agent
snake. We have lots of cool demos of
snake. We have lots of cool demos of
this stuff. So, what we're trying to do
this stuff. So, what we're trying to do
here is actually kind of easy to
here is actually kind of easy to
explain. Um, the how of it is a bunch of
explain. Um, the how of it is a bunch of
math and a bunch of
engineering. It seems weird to me that
engineering. It seems weird to me that
this would break
training, right?
cuz this is just saying take
cuz this is just saying take
random try from random actions. This is
random try from random actions. This is
all this is saying
right? If I do like this
It's kind of a Q
function. Kind of
It's not going to train either. Had a
It's not going to train either. Had a
question regarding classification. If I
question regarding classification. If I
have a bunch of features and a bunch of
have a bunch of features and a bunch of
data points but just a binary
data points but just a binary
classification, you think reinforcement
classification, you think reinforcement
learning is a good No, reinforcement
learning is a good No, reinforcement
learning is never a good way to train if
learning is never a good way to train if
you have a supervised learning signal.
you have a supervised learning signal.
Uh reinforcement learning is for when
Uh reinforcement learning is for when
you have usually an interactive process
you have usually an interactive process
with unlabeled data. So you're
with unlabeled data. So you're
interacting with some sim and then you
interacting with some sim and then you
basically just know every so often
basically just know every so often
whether you did well or not. So you have
whether you did well or not. So you have
a very weak learning signal. That's what
a very weak learning signal. That's what
reinforcement uh learning is used for.
reinforcement uh learning is used for.
If you try to like throw reinforcement
If you try to like throw reinforcement
learning on classification, you can
learning on classification, you can
actually technically set it up, but it's
actually technically set it up, but it's
like it's way less effective than just
like it's way less effective than just
doing supervised learning because like
doing supervised learning because like
reinforcement learning as a whole is a
reinforcement learning as a whole is a
method of techniques for dealing with
method of techniques for dealing with
when you have very low learning signal,
when you have very low learning signal,
usually in like a non-stationary
usually in like a non-stationary
interactive process. If you have a
interactive process. If you have a
strong learning signal in a stationary
strong learning signal in a stationary
process, which is what classification
process, which is what classification
is, you just use
is, you just use
it. There's no reason not to.
it. There's no reason not to.
Okay, so this blows up freaking
Okay, so this blows up freaking
infinitely, huh?
And then if I just do like this
instead. Gotcha. Yep.
So the cool thing with RL, right, and
So the cool thing with RL, right, and
like the best way to understand it is
like the best way to understand it is
this thing plays a game. It starts off
this thing plays a game. It starts off
by mashing buttons randomly and it just
by mashing buttons randomly and it just
gets better at it and it just gets to be
gets better at it and it just gets to be
super
super
human. Like one of the cool things about
human. Like one of the cool things about
RL, you can just completely mess up the
RL, you can just completely mess up the
graphics of a game to a point that no
graphics of a game to a point that no
human will ever be able to play it. And
human will ever be able to play it. And
the RL won't even notice. It'll just
the RL won't even notice. It'll just
learn to play the game exactly like it
learn to play the game exactly like it
would with the good
graphics. It's pretty
cool. Okay, so this trains it is when
cool. Okay, so this trains it is when
you increase the horizon that it stops
you increase the horizon that it stops
working or it doesn't really stop
working or it doesn't really stop
working is the thing in this case,
working is the thing in this case,
right? It just doesn't do any
right? It just doesn't do any
better. And then on like let go, it
better. And then on like let go, it
doesn't really do anything. But then
doesn't really do anything. But then
that our implementation of that is kind
that our implementation of that is kind
of sketchy.
I also want to learn all this from
I also want to learn all this from
scratch. How can
I CS231N is the best like starter deep
I CS231N is the best like starter deep
learning class? I would say
learning class? I would say
um it's a graduate course, so there are
um it's a graduate course, so there are
some prerexs. They're considered basic
some prerexs. They're considered basic
prerexs for a course of this type. Um
prerexs for a course of this type. Um
but yeah, you need like multivariate
but yeah, you need like multivariate
differential calculus, linear algebra,
differential calculus, linear algebra,
decent programming
background. Like this stuff is
background. Like this stuff is
incredibly cool. And we actually we have
incredibly cool. And we actually we have
people who came in to Puffer Lib like a
people who came in to Puffer Lib like a
year ago with no programming background,
year ago with no programming background,
not really any relevant math, and have
not really any relevant math, and have
done pretty darn well. But it takes
done pretty darn well. But it takes
work, right?
Like I started doing this when I was
Like I started doing this when I was
14 and pretty much just never
stopped. Okay, so this still
runs with this
But the entropy is like super
But the entropy is like super
crashed. So I mean this just runs
crashed. So I mean this just runs
because it's not actually doing any
because it's not actually doing any
search. I would
search. I would
imagine if I just set this to one like
imagine if I just set this to one like
this. I need to understand why this
this. I need to understand why this
breaks it because this is just like it
breaks it because this is just like it
should be able to evaluate which action
should be able to evaluate which action
puts it in a better state pretty easily.
If it doesn't, then I probably have
If it doesn't, then I probably have
something set up
wrong. Yeah. And then it explodes. Okay.
I mean, there really only two objectives
I mean, there really only two objectives
added to
added to
this. I
this. I
added these two
losses. So there's a world model
objective right up
there and this is
there and this is
just world model on
state.hidden. Wait world
state.hidden. Wait world
model of
model of
state.hidden. Does that make
state.hidden. Does that make
sense or did I mess it up?
Do you come live every day at this time?
Do you come live every day at this time?
Uh, I stream like 40 to 60 hours a
Uh, I stream like 40 to 60 hours a
week. There's not like a fixed super
week. There's not like a fixed super
fixed schedule, but I'm like I'm usually
fixed schedule, but I'm like I'm usually
on because it's 40 to 60 hours a week.
This bothers me. This really bothers
me. I think that this
me. I think that this
one Wait, neither of these are correct,
one Wait, neither of these are correct,
right?
right?
Yeah, I think I just have both of these
wrong. So if both of these are like a
wrong. So if both of these are like a
bit off, then neither of them are going
bit off, then neither of them are going
to work. Okay, so then this reward
model. Yeah, we need to just go rename
model. Yeah, we need to just go rename
things so that they're not dumb. I think
things so that they're not dumb. I think
that's what needs to happen right
now. So this should be
Instead of
Instead of
statehidden, this
statehidden, this
is state.obs embed is
hidden. And this is state.hidden.
Okay. So, this is the same as
before.
Um, and now we're
Um, and now we're
putting obs
into model state.hidden hidden and then
into model state.hidden hidden and then
state wait world model of state.hidden
state wait world model of state.hidden
hidden as to predict obsa. Is that
hidden as to predict obsa. Is that
right? I think this is correct.
Right. The state hidden should be the
Right. The state hidden should be the
output of the
output of the
LSTM and we want that to train to
LSTM and we want that to train to
predict
predict
the embedding
input. Hang on though. The world model
input. Hang on though. The world model
doesn't it need
Does it not need the
action? It needs the action, man. What
action? It needs the action, man. What
are you doing?
right now I have it set up so that the
right now I have it set up so that the
input to the
input to the
network
network
takes actions. Right.
Hang on. Maybe I can make this
simpler. If I just have like my standard
simpler. If I just have like my standard
network here, which is your
network here, which is your
LSTM, right?
LSTM, right?
S0
one two
one and then this is like A1,
one and then this is like A1,
B1, 2,
B2,
B2,
two. Then from
here, I just need something that
here, I just need something that
predicts the input here.
which is the next
which is the next
observation. So you really don't need
observation. So you really don't need
you kind of just need the action there.
you kind of just need the action there.
Can I just have like a world model thing
Can I just have like a world model thing
that takes in like
that takes in like
Oops. Can I just like do something like
Oops. Can I just like do something like
this where I take in
this where I take in
like and let's call this
like and let's call this
E1. So I take like E1
You don't even need to take in E1,
You don't even need to take in E1,
right? You can take the output
right? You can take the output
vector. Take literally the output
vector. Take literally the output
vector. So this is
vector. So this is
actually this S1 is actually technically
actually this S1 is actually technically
H1 and then this is C1.
H1 and then this is C1.
So then I can take in
H1
H1
A1 and I can
predict E2. I
guess isn't this
guess isn't this
easier because then you don't have to
easier because then you don't have to
change anything about the base net at
change anything about the base net at
all. I think this would be
all. I think this would be
easier and then the world model should
easier and then the world model should
actually have context for what I'm
actually have context for what I'm
trying to
trying to
do. Okay, let's try it that
way. And then this can also predict
way. And then this can also predict
reward, right? This needs reward and
reward, right? This needs reward and
value, but it can do that
value, but it can do that
then. Okay, we can do
then. Okay, we can do
this. Let me get a let me go grab a cup
this. Let me get a let me go grab a cup
of tea real quick so I keep myself awake
of tea real quick so I keep myself awake
here. This is a thinking
here. This is a thinking
day. Smoke coming out of my ears. Give
day. Smoke coming out of my ears. Give
me a minute. I'll be right back.
Okay, I
Okay, I
think I think we can get this to
work. I think we can get this to
work. I think we can get this to
work. Say, man.
Um, I think it was the fact that I just
Um, I think it was the fact that I just
didn't have this thing conditioned
didn't have this thing conditioned
correctly.
So what we're going to
So what we're going to
do going to make the world model a
do going to make the world model a
little
little
differently and try
again. So this is going to be hidden
again. So this is going to be hidden
size
size
plus selfnum actions going to hidden
plus selfnum actions going to hidden
size like so. Okay, we no longer need
size like so. Okay, we no longer need
action project.
action project.
We no longer need
We no longer need
this. Just need like this.
this. Just need like this.
Okay. Encode
Okay. Encode
observations. This get state
obs. We still need this for
obs. We still need this for
uh it doesn't need this one hot actions
uh it doesn't need this one hot actions
anymore. Doesn't need this.
needs
needs
this.
this.
So, we embed and we save this to here.
So, we embed and we save this to here.
Okay. Also, I need to not get dehydrated
Okay. Also, I need to not get dehydrated
doing this. Had a lot of
doing this. Had a lot of
coffee. Let me do this before I get my
coffee. Let me do this before I get my
tea.
Okay, we
do the LSTM rapper now.
You don't need any of this,
right? So, this actions actually gets
right? So, this actions actually gets
added to the hidden
added to the hidden
state. Obset doesn't get used at all
here. No, state.hidden doesn't exist
here. No, state.hidden doesn't exist
yet.
yet.
Um yeah, state.hidden does not exist
Um yeah, state.hidden does not exist
yet. We go like
yet. We go like
this. Now we've run the policy
this. Now we've run the policy
once.
Now we have the hidden
target. Doesn't super matter actually.
M0 hidden equals.
state.Action
repeat. You don't need
repeat. You don't need
this previous action
this previous action
anymore, right?
You need to know actions at
You need to know actions at
all? I don't think you need to know
all? I don't think you need to know
actions for training,
actions for training,
right? For training the world
right? For training the world
model. So I don't think you need to know
model. So I don't think you need to know
this at all.
Okay, you do need to sample some logs.
So the first thing you need to do is
So the first thing you need to do is
actually sample
actually sample
logits which is up
logits which is up
here, right?
here, right?
And
then you can one hat you can one hot
then you can one hat you can one hot
those
those
actions. And then you can
do
input right here right
input right here right
m
hidden and then this is self.world World
hidden and then this is self.world World
model
model
policy. World
policy. World
model of
hidden. And then this is
hidden. And then this is
like world model
obs and this is m0
obs and this is m0
hidden. Then you
hidden. Then you
get logits and
values from decoding the actions.
and then what do you do? You
and then what do you do? You
decode. You get the rewards from the
decode. You get the rewards from the
hidden
state with this
state with this
action and the rewards you append the
action and the rewards you append the
actions.
Okay, so it's something like this. I
Okay, so it's something like this. I
think it's something a lot closer to
think it's something a lot closer to
this. And what are we doing? Forward for
training. OBS embed is equal to
training. OBS embed is equal to
hidden. Uh, this is not supposed to be
hidden. Uh, this is not supposed to be
here. It's
here. It's
actually Yeah. Well, it doesn't super
actually Yeah. Well, it doesn't super
matter,
but that hidden goes there.
but that hidden goes there.
We got like three hours to get something
We got like three hours to get something
to work
to work
here. I'm going do the best I
here. I'm going do the best I
can. I think we should be able to do
it. Okay. So, there's no
it. Okay. So, there's no
statehidden. It should just be hidden.
This is in the world model I
This is in the world model I
think
think
no those model
obs here is get like a nice
obs here is get like a nice
self-contained world model module that
self-contained world model module that
takes in uh hidden state and action
takes in uh hidden state and action
embedding and produces something that is
embedding and produces something that is
going to be learned to be similar to an
going to be learned to be similar to an
observ ation
observ ation
embedding. What are we still screwing
embedding. What are we still screwing
up? Policy. Okay, cool. This is in clean
puffl which is right
puffl which is right
here. Uh now this world
model. We can just do
model. We can just do
policy world model forward statehidden
policy world model forward statehidden
and actions, right?
And then what we can do is we can take
And then what we can do is we can take
this
thing. We can do this
thing. We can do this
m0
m0
hidden. And then we can just add this
hidden. And then we can just add this
function on
function on
here which
here which
takes
takes
hidden
actions.
Okay. And then this takes
Okay. And then this takes
hidden. This is self actions. This is
hidden. This is self actions. This is
self world model. Return world model
self world model. Return world model
ops.
batch. Uh, I thought there was an
batch. Uh, I thought there was an
action. Oh, it's not batch action.
action. Oh, it's not batch action.
It's state.action.
action
imp
batch.actions colon
batch.actions colon
negative1. That seems screwy to me. I
negative1. That seems screwy to me. I
think this is just batch.s
Okay. So,
Okay. So,
[Music]
[Music]
expected some small mismatch shenanigans
expected some small mismatch shenanigans
here.
Um, presumably these are different
Um, presumably these are different
shapes or something.
Oh, so batch actions
is we should just return hidden after
is we should just return hidden after
the transpose then,
the transpose then,
right? Yeah, we should just do
that like this.
Maybe maybe just do this.
Okay, this is
Okay, this is
correct. And uh it's just not dim equals
correct. And uh it's just not dim equals
1 anymore because it's a
1 anymore because it's a
sequence. It should probably just be
sequence. It should probably just be
dim=1.
Right. Oh, am I
Right. Oh, am I
have I have Neptune on on all these?
have I have Neptune on on all these?
Let's just do that. That'll be
Let's just do that. That'll be
faster. Oh, that's so much
better. Take me a few seconds per run.
Good. And state ops embed is
Good. And state ops embed is
the the
the the
target. Next
state. Ah, okay.
state. Ah, okay.
state
targets. You know, you really don't even
targets. You know, you really don't even
need this here at
need this here at
all. You can kind of
just OBS embed
just OBS embed
goes after the LSTM, right?
goes after the LSTM, right?
You can just do it like right
You can just do it like right
here
dashtt
state right there before you get to the
state right there before you get to the
lstm. That looks good.
Next state
Next state
credits, next state
credits, next state
card. That's pretty
card. That's pretty
good. Now, this is
good. Now, this is
uh concat on
uh concat on
wrong dimension, I
believe.
believe.
So, let me think about how this works.
Obs one colon
Obs one colon
Yeah, like this. This should be the
Yeah, like this. This should be the
correct
loss. And this is your world
loss. And this is your world
loss. Mean squared error. World
loss. Mean squared error. World
loss
hidden
reds. You have your main squid
reds. You have your main squid
error. And then similar for
error. And then similar for
this I suppose we'll quickly find out
this I suppose we'll quickly find out
whether this is
correct. Okay. So these are also
uh these are also messed
up. State hidden shape is 12864
up. State hidden shape is 12864
128. So you actually don't want to need
128. So you actually don't want to need
this transpose at
this transpose at
all.
all.
Huh? Yeah. You just think we just do
this. Okay. So now we have 128x 64 by
this. Okay. So now we have 128x 64 by
1 and batch rewards is
1 and batch rewards is
perfect.
Squeeze and this is
Squeeze and this is
of minus one.
of minus one.
And then this
And then this
is so these should be
is so these should be
better better rewards than ts
Not the perfect result we want to
see. Does something
though. Next we will look
at we'll look at
the actual world model here
code. I should be able to set this to
code. I should be able to set this to
like zero. Right?
like zero. Right?
Well, I need to refactor this so that
Well, I need to refactor this so that
this is the first iteration basically.
Is there a way that we
could put this all one
could put this all one
loop? Is there a way that that could
The thing is you don't really need to
The thing is you don't really need to
call
The world
model. Let's verify something real
model. Let's verify something real
quick.
quick.
If I just do
this, I just do
this, I just do
like
stateactions actions.
Just
do actions log
do actions log
props
logs something like
this is this
train state log
from oops and this is
Yeah. So this is the ultra fast train
Yeah. So this is the ultra fast train
curve
curve
here. So I guess if we were to look at
here. So I guess if we were to look at
it this way, right?
It's kind of like doing
this. And this still this doesn't break
this. And this still this doesn't break
anything, I'm sure.
anything, I'm sure.
But and then the stuff below needs to
But and then the stuff below needs to
refine these
refine these
predictions. So this is still good.
Now the one annoying thing is that
Now the one annoying thing is that
um you do have to expand
um you do have to expand
this to size N I guess.
So this is just going one ahead and
So this is just going one ahead and
looking at the
looking at the
value, right?
I think I implemented this a little
I think I implemented this a little
differently from Uzero actually,
right? I did this the more like PO
right? I did this the more like PO
compatibleish
way. I mean it is training here, right?
It is actually training.
You got your reward loss and you got
You got your reward loss and you got
your world loss.
Hang on. I might have set this up wrong
Hang on. I might have set this up wrong
still because
Yeah. Okay. I think I did set this up
Yeah. Okay. I think I did set this up
wrong. Hang on. I think I skipped one
wrong. Hang on. I think I skipped one
time step.
logic
logic
repeat. Oh no, wait. So we take these
repeat. Oh no, wait. So we take these
logits, we repeat these a bunch of
logits, we repeat these a bunch of
times, we sample different
times, we sample different
actions.
actions.
Okay, we put these through the world
Okay, we put these through the world
model forwards.
And then we put it through the cell of
And then we put it through the cell of
the next time
the next time
step m0 lstm state which
gets this is the wrong
h this is the wrong age. It needs to
be just do
that. Okay. It's not like rocket ship
that. Okay. It's not like rocket ship
takeoff.
Next time
step. Oh yeah, but I think we're still
step. Oh yeah, but I think we're still
trolling here.
trolling here.
So world model
So world model
forward mu0 LSTM
forward mu0 LSTM
state mu0 hidden
C
C
Pretty sure this
Pretty sure this
works.
works.
So you update your M0 LSTM
state. Okay, that makes it rain faster.
state. Okay, that makes it rain faster.
That's way better,
That's way better,
right? That's not like perfect yet, but
right? That's not like perfect yet, but
that's way
that's way
better. Way way way better. Actually,
better. Way way way better. Actually,
that might be
matching. Yeah. No, this is actually on
matching. Yeah. No, this is actually on
par now. Cool.
Um, you get your new LSTM
Um, you get your new LSTM
state. You decode
state. You decode
actions
actions
from m hidden.
from m hidden.
Oh, this is the old
hidden. Oh, this isn't getting used
hidden. Oh, this isn't getting used
anyways, I don't
anyways, I don't
think. Yeah, this isn't getting
think. Yeah, this isn't getting
used anyways. So, when we go to multiple
used anyways. So, when we go to multiple
steps, this will
steps, this will
break. So, decode actions
of zero.
Hang
on. What's this
hidden? Okay. World model
hidden? Okay. World model
ops. M0 LSTM
ops. M0 LSTM
state. This is M0 LSTM state.
This is m
This is m
zero
hidden. Move zero
hidden. Move zero
hidden. Then moo zero
hidden. Then moo zero
hidden. Then you get your reward model
hidden. Then you get your reward model
on the hidden. You update your rewards.
on the hidden. You update your rewards.
Let's make sure we didn't break
Let's make sure we didn't break
anything.
We're going to get this today. We're
We're going to get this today. We're
going to get
going to get
it. This got to work,
it. This got to work,
right? Okay. So, this
right? Okay. So, this
is How the did we just break it?
is How the did we just break it?
Oh, we didn't. Hang
Oh, we didn't. Hang
on. Did
we? No, we did break
we? No, we did break
it.
it.
Let's go back to where it was
working. This was the one that was
working. This was the one that was
working. We have to be very very careful
working. We have to be very very careful
with
with
this. Very very careful.
And this is a good
one. Now this is your LSTM state.
one. Now this is your LSTM state.
What if I just change this to
age? Okay. So, this breaks if I change
age? Okay. So, this breaks if I change
that to eight, right?
Yes, definitely
Yes, definitely
breaks. I change this to
age zero logic
rewards. That's weird.
So this goes, hey, if we sample a bunch
So this goes, hey, if we sample a bunch
of
of
logs and we run them through the world
model, it is correct to pass here
model, it is correct to pass here
though. So I think the thing this is m
logs. Does this get used
though? This doesn't get
used
values. The reward gets used I
guess. So how is this
break? Oh, I guess it's because the
break? Oh, I guess it's because the
value here is going to change,
right? Yeah. So the value Okay. Okay. So
right? Yeah. So the value Okay. Okay. So
that is going to
change. So we sample
change. So we sample
logics from
here. What the hell happened to our
here. What the hell happened to our
reward model? Right.
We're totally ignoring the reward that
We're totally ignoring the reward that
we get at this step.
This needs to
This needs to
go on the hidden state here,
right? This needs to go here.
This is fast. Good.
This still
This still
works. So
careful. Okay. So, this is the good
careful. Okay. So, this is the good
one. So, we're getting there.
Now you run the world model on the
Now you run the world model on the
hidden state and actions. You get the
hidden state and actions. You get the
new mu0 LSTM
state. This
state. This
decode should be on
decode should be on
H. Apparently this breaks though.
H. Apparently this breaks though.
Let's see if this breaks
it.
totally breaks
it. Like it still trains,
but nowhere near as
well. So issue here, right, is you're
well. So issue here, right, is you're
adding on
You're adding on this value bootstrap, I
You're adding on this value bootstrap, I
guess.
Man, the thing is that this estimate
Man, the thing is that this estimate
should not be unstable.
Yeah, the thing is that this estimate
Yeah, the thing is that this estimate
should not be
unstable. If this estimate is
unstable then um the world model has to
unstable then um the world model has to
be bad, right?
How's the world model being
learned? I guess that's the question.
learned? I guess that's the question.
How's the world model getting learned?
How
about this?
learned but not very fast.
This is like just still so way
This is like just still so way
off. And we know that like if we just
off. And we know that like if we just
put in the value that we actually get,
put in the value that we actually get,
then it's correct. So then the only
then it's correct. So then the only
errors in here
The only error here has to come from
The only error here has to come from
errors
in the world model, right? Because mu0
in the world model, right? Because mu0
lstm state
Oh, wait. Hang on. Yeah, it has to come
Oh, wait. Hang on. Yeah, it has to come
through errors
in in the dynamics of Nep's next
step. So
step. So
obnoxious. This was 305 and it's
obnoxious. This was 305 and it's
probably like
probably like
300.
300.
But it takes forever to evaluate now.
But it takes forever to evaluate now.
It's running through this whole freaking
search. 250. It's even
lower. That's
lower. That's
tricky. That's
tricky. That's
tricky because see, we should be able to
tricky because see, we should be able to
like crank this. The goal is that you
like crank this. The goal is that you
should be able to like crank this
should be able to like crank this
here and like now it should be able to
here and like now it should be able to
do this like multi-step modeling.
Is this better than before?
This actually seems decent, doesn't
This actually seems decent, doesn't
it? Hang
it? Hang
on. Huh.
So,
So,
um I guess just compared to the
um I guess just compared to the
original, there's a a
original, there's a a
penalty when you use one time step
penalty when you use one time step
because you may as well at that point
because you may as well at that point
just use the true value estimate and not
just use the true value estimate and not
have the world model drift or whatever.
have the world model drift or whatever.
But if you roll ahead four like a few
But if you roll ahead four like a few
time steps, you actually get benefit.
I mean, compared to the default, I don't
I mean, compared to the default, I don't
know if this is getting benefit compared
know if this is getting benefit compared
to it's probably not going to get
to it's probably not going to get
benefit compared to the
benefit compared to the
original the original, but I mean, this
original the original, but I mean, this
is progress at
least. Definitely progress.
So they say that they use five steps
So they say that they use five steps
look ahead for
look ahead for
everything. We could try more step look
everything. We could try more step look
ahead or we could try um bigger batch or
ahead or we could try um bigger batch or
both.
So 800 at 28 mil,
right? Yeah. So this is like pretty well
right? Yeah. So this is like pretty well
on par with
here. So potentially
Potentially this can do
something. I think if the benefit is
something. I think if the benefit is
there's probably benefit from going
there's probably benefit from going
deeper. Let's try
deeper. Let's try
this steps.
I think that we have a correct
I think that we have a correct
implementation though now. I think we do
implementation though now. I think we do
have a correct
implementation. We can probably scale
implementation. We can probably scale
the world model up as
well. We're just going to we're going to
well. We're just going to we're going to
try a few separate experiments.
So this is
deeper. Because we don't have frame skip
deeper. Because we don't have frame skip
in ours.
It's pretty well on par with the other
curves. We are actually using the
curves. We are actually using the
actions,
actions,
right? Log
right? Log
prop use your Yeah, we are giving it the
prop use your Yeah, we are giving it the
actions.
Okay. Still behind our on
Okay. Still behind our on
policy version.
It's This is not like magically better.
It's This is not like magically better.
It's decent, but going deeper just makes
It's decent, but going deeper just makes
it slower. It doesn't really make it
it slower. It doesn't really make it
better. At least on
better. At least on
this. It's interesting.
Okay, we could go wider as well,
Okay, we could go wider as well,
right? And go
right? And go
128. So wider.
We'll see what it is. MS.
Uh, this is too
slow. Yeah, this is too slow to even
slow. Yeah, this is too slow to even
run. This is probably like a
run. This is probably like a
scaling bottleneck 64.
Yep, that'll be better. Like 200k or
Yep, that'll be better. Like 200k or
something. That at least we can run the
something. That at least we can run the
experiment.
It seems like there might be It just
It seems like there might be It just
seems like it's roughly on par
really. It's about it.
It's like not getting a lot out of this
model. Well, there are a lot of things
model. Well, there are a lot of things
we can uh oblate though with this to be
fair. Okay, like let's say we did
fair. Okay, like let's say we did
32. We scale up the world model a
bunch. Yeah, it's stuck. Okay, it's like
bunch. Yeah, it's stuck. Okay, it's like
about the same.
scale up the world model a little bit.
scale up the world model a little bit.
You're supposed to do well with scaling
Okay, it's just redoing the optimizer.
Yeah, the reward loss is also still
Yeah, the reward loss is also still
high,
high,
right? And that should be very easy to
right? And that should be very easy to
predict.
Is that
anything? Or is this the same curve?
pretty much the same freaking curve.
World model loss is
lower. Reward loss is still high. Maybe
lower. Reward loss is still high. Maybe
that's next. Okay. You know, this was
that's next. Okay. You know, this was
kind of interesting. This is
like feels smooth.
There's no way the reward loss should be
There's no way the reward loss should be
that high, though.
We still to
We still to
650. No, we are
not. That's only about on
par. It's just like on par.
par. It's just like on par.
Not better
Not better
yet. Hang on. We are still dependent on
yet. Hang on. We are still dependent on
this reward model, right?
So we're at like
So we're at like
014 right now on the reward scale for
014 right now on the reward scale for
this. See about
this. See about
this. You are very dependent on good
this. You are very dependent on good
world model and good reward function.
So, so if this isn't better then the
So, so if this isn't better then the
question will Why not? Because it's like
question will Why not? Because it's like
it's predicting the
future, but it doesn't look like the
future, but it doesn't look like the
reward loss has gotten better, which is
reward loss has gotten better, which is
suspicious to me.
suspicious to me.
The reward loss in particular should be
The reward loss in particular should be
very easy to predict actually,
right? Yeah, the reward loss should be
right? Yeah, the reward loss should be
like very very easy to predict. Okay.
Um, in that
case, MSSE
case, MSSE
over next reward predictions and next
over next reward predictions and next
reward targets.
Did I offset it by one by
mistake? What happens if I do this?
I don't think I
I don't think I
did. I don't think I did. But we'll try
did. I don't think I did. But we'll try
this.
Let me think about
this. I'm almost sure it's the other
this. I'm almost sure it's the other
way.
cuz you get the reward at every step.
I mean, this doesn't seem to care which
I mean, this doesn't seem to care which
one you give
it. It doesn't like do better or worse.
it. It doesn't like do better or worse.
It's the same
It's the same
graph, which is also bizarre because it
graph, which is also bizarre because it
should matter.
I think this is
I think this is
correct any more than the other
correct any more than the other
one. It's very weird that it doesn't
one. It's very weird that it doesn't
seem to matter.
Like if the world model can predict
You know, you're probably now getting
You know, you're probably now getting
just screwed by your sampling scheme
just screwed by your sampling scheme
here, right? Because you're not doing
here, right? Because you're not doing
MCTS. You're doing something
MCTS. You're doing something
stupid. It's like a really dumb
stupid. It's like a really dumb
approximation. It shouldn't be a
max.
Should
Should
it? Or maybe it should be a max. Uh-uh.
It's kind of just rolling out a bunch of
It's kind of just rolling out a bunch of
these This
Maybe you have to like bump up the
Maybe you have to like bump up the
entropy a ton with this
method. That might be a decent baseline.
Let's go look at the MCTS scheme that
Let's go look at the MCTS scheme that
they use in the
meanwhile this
meanwhile this
paper. Um,
Okay, so MCTs with upper confidence
Okay, so MCTs with upper confidence
bounds. Every node of the search tree is
bounds. Every node of the search tree is
associated with an internal state
associated with an internal state
S. For each action, there's an edge SA
S. For each action, there's an edge SA
that stores a set of
that stores a set of
statistics respectively representing
statistics respectively representing
recent counts and policy P mean value Q
recent counts and policy P mean value Q
reward and state
transition action is selected according
transition action is selected according
to stored statistics by maximizing over
to stored statistics by maximizing over
a probabilistic upper confidence bound.
a probabilistic upper confidence bound.
Oh
jeez.
Yesh. That's
Yesh. That's
something. That is something.
There are a couple other things we can
There are a couple other things we can
try before we go nuts with
try before we go nuts with
this because I think that's going to be
this because I think that's going to be
I mean this is like the type of thing
I mean this is like the type of thing
where they probably spent a lot of
where they probably spent a lot of
effort getting a fast GPU implementation
effort getting a fast GPU implementation
of
of
this and there's like yeah there are a
this and there's like yeah there are a
lot of different things going going on
lot of different things going going on
in
here. This algorithm is very powerful
here. This algorithm is very powerful
though. This algorithm does feel very
though. This algorithm does feel very
powerful. Okay. So this uh we are going
powerful. Okay. So this uh we are going
to let this finish because what I want
to let this finish because what I want
to do is I want to compare this to 0.05
to do is I want to compare this to 0.05
entropy without MCTS. So basically like
entropy without MCTS. So basically like
if I make the policy more stochastic
if I make the policy more stochastic
does MCTS do better than
does MCTS do better than
um or not MCTS whatever search thing I'm
um or not MCTS whatever search thing I'm
doing does it do better than the
baseline and in the meantime I also want
baseline and in the meantime I also want
to see look this reanalyze
thing m0 revisits its past time steps
thing m0 revisits its past time steps
and re-executes it search using the
and re-executes it search using the
latest model parameters
Oh, that's hard.
Yeah, that's hard. Okay, that's that's
Yeah, that's hard. Okay, that's that's
hard to do with the on policy
hard to do with the on policy
uh formulation that we have.
They do have an equal tension up boot
They do have an equal tension up boot
strap.
It does kind of make you wonder.
How much you needed?
MCTS. Okay.
But it should definitely be doing better
But it should definitely be doing better
on a sample's axis, right?
Even with only six simulations per move,
Even with only six simulations per move,
at Museio learned an effective policy
at Museio learned an effective policy
and improved
rapidly. So we should actually like we
rapidly. So we should actually like we
should very well see
We have
We have
this. What if I just go to
this. What if I just go to
mu0? This should be the fast
one. Let's do this.
It's just so much faster. It's crazy.
Yeah, looks like this thing is still
Yeah, looks like this thing is still
just going to be better,
huh? Oh, it actually just matches like
huh? Oh, it actually just matches like
close to perfectly.
Am I somehow just not doing anything
Am I somehow just not doing anything
with
this? I wonder if we're like somehow
this? I wonder if we're like somehow
just
just
wrong in here. Let's look cuz we should
wrong in here. Let's look cuz we should
definitely see with 32 Sims. We should
definitely see with 32 Sims. We should
see something, right?
Do
Do
rewards plus
value argmax.
Okay.
So, yes. So these indices are supposed
So, yes. So these indices are supposed
to
to
be the
be the
best roll out according to the policy
right the returns
Max is
Max is
five, which is completely impossible,
right? Yeah. Five is completely
right? Yeah. Five is completely
impossible.
Hang on. The reward
function. Let's let me There are a few
function. Let's let me There are a few
more things that just like don't add up
more things that just like don't add up
here, I
think. So, data
think. So, data
policy reward model of state.hidden,
right? State hidden is hidden.
right? State hidden is hidden.
Okay, that should be differentiably
Okay, that should be differentiably
optimizing through the whole
optimizing through the whole
net. That should be a very good
prediction. And then I mean there's no
prediction. And then I mean there's no
reason not to optimize the world model
reason not to optimize the world model
through the net, right? Unless there
is. If we do this
And while we run that, I need to really
And while we run that, I need to really
think about this reward loss and what
think about this reward loss and what
that could possibly
that could possibly
be be wrong.
Wait, is this the wrong state? Hang
Wait, is this the wrong state? Hang
on. This is state hidden.
No, this state hidden should
No, this state hidden should
go into the
go into the
reward loss, right?
state.
Hidden reward reward model goes on to
Hidden reward reward model goes on to
state.
plus equal
plus equal
rewards reward
rewards reward
model on the hidden
model on the hidden
state. It's moo zero
state. It's moo zero
hidden which is the output hidden
state. This doesn't get updated.
Okay, this doesn't get updated. So, this
Okay, this doesn't get updated. So, this
is completely
screwed. Uh, this is worse as well
screwed. Uh, this is worse as well
without the
detaching the objective. Okay, so
So, we'll see what this
does. The two things to look at should
does. The two things to look at should
be the world loss and the reward loss.
Do we think this is correct
yet? Move zero
yet? Move zero
hidden. So this is now the next hidden
hidden. So this is now the next hidden
state coming out of the cell. This gives
state coming out of the cell. This gives
us reward prediction. We predict
us reward prediction. We predict
reward. We
sample. We append
actions that came from U0
logits. Okay.
I think we've corrected it, but now it
I think we've corrected it, but now it
doesn't
work. Reward loss is about the same as
work. Reward loss is about the same as
before.
Um it gets applied to hidden
Um it gets applied to hidden
state lstm equals
hidden
hidden
forward state
forward state
hidden set
there. I do not like this reward loss. I
there. I do not like this reward loss. I
really
don't. This feels like there's something
don't. This feels like there's something
very wrong with
it. But it's got to
it. But it's got to
be state hidden is what you get as the
be state hidden is what you get as the
output.
this forward pass up
this forward pass up
here. And
then targets just the reward
then targets just the reward
batch.rewards,
batch.rewards,
right? And do you want the reward that
right? And do you want the reward that
is you want to cut the first reward off,
is you want to cut the first reward off,
right?
Seems right to
me. It's not like it isn't training. It
me. It's not like it isn't training. It
is.
is.
But, you know, the entropy is very low
But, you know, the entropy is very low
now. I'm noticing
I could do random action and then follow
I could do random action and then follow
policy.
It's got to be such a low variation in
It's got to be such a low variation in
the
the
[Music]
Yeah. We could do epsilon technically.
Okay. It's not like this search doesn't
Okay. It's not like this search doesn't
work. It does. It's just not
work. It does. It's just not
efficient. And it's probably because of
efficient. And it's probably because of
this
It's like a quick epsilon grad.
Right. This is going to mess up log
Right. This is going to mess up log
problem.
No, it isn't. No, it isn't. It's fine.
The reward loss is still the thing that
The reward loss is still the thing that
bothers me. It's like way too high.
We're learning something here. I don't
We're learning something here. I don't
know if this curve is any
know if this curve is any
steeper. This is with epsilon gradient
on. Seems like it probably isn't.
What would be like a quick alternative
What would be like a quick alternative
sampling scheme I could use here?
I could do mean reward under
I could do mean reward under
policy instead of
policy instead of
best. Maybe that's more
robust cuz this is worse,
robust cuz this is worse,
right? This is like way worse,
right? Yeah, this is worse. Okay, but
right? Yeah, this is worse. Okay, but
really bad.
Wait.
Wait.
Returns
shape 32.
How do we group this
nicely? I mean, I can write a really
nicely? I mean, I can write a really
stupid loop, right?
I can't freaking do
I can't freaking do
this and find another thing I can do
this and find another thing I can do
instead. This is just going to be like
instead. This is just going to be like
it's this is just not easy to write
it's this is just not easy to write
with torch tensors at all. Like you just
with torch tensors at all. Like you just
you need to write a freaking CUDA kernel
you need to write a freaking CUDA kernel
for this. It's like so much harder to
for this. It's like so much harder to
write forch than it is to write CUDA
write forch than it is to write CUDA
ironically for this. Like you need to
ironically for this. Like you need to
just be able to write arbitrary loops
just be able to write arbitrary loops
and stuff.
It just work as the way it's
written. Really seems like it should at
written. Really seems like it should at
least do something the way it's written.
Try it again on go, but I don't really
Try it again on go, but I don't really
trust that item as much.
Art map should be totally fine. There's
Art map should be totally fine. There's
like a basic version of it. It should be
like a basic version of it. It should be
totally
totally
fine. I guess I haven't tried going
fine. I guess I haven't tried going
deeper
deeper
since since I fixed
since since I fixed
it. So, I can technically just see if
it. So, I can technically just see if
this does anything. I doubt
this does anything. I doubt
it, but
maybe cuz like five is a
maybe cuz like five is a
tiny number of steps to look ahead and
tiny number of steps to look ahead and
break
out. It could be that there's just
out. It could be that there's just
nothing that looking ahead five does for
nothing that looking ahead five does for
you to be fair.
This reward loss is pissing me off. I
This reward loss is pissing me off. I
don't understand how it's not like
don't understand how it's not like
Perfect.
the world loss is like really low,
the world loss is like really low,
right? Like this thing should
right? Like this thing should
have a decent grasp on the next OBS.
Well, to be
Well, to be
fair, yeah, in this end they're not like
fair, yeah, in this end they're not like
that would kind of make sense, I guess.
It should kind of tell me something that
It should kind of tell me something that
so far I like I exactly
so far I like I exactly
match PO, right? That should like be
match PO, right? That should like be
weird,
right? I mean, that could just mean that
right? I mean, that could just mean that
you're not really limited by picking bad
you're not really limited by picking bad
actions in the training here,
right? But that wouldn't make sense.
right? But that wouldn't make sense.
You should be limited by picking bad
You should be limited by picking bad
actions.
Yeah, this looks like just on par with
Yeah, this looks like just on par with
the
the
best non search
best non search
curve, right? 360 at 13 mil. This is
curve, right? 360 at 13 mil. This is
just like on par, right?
Yeah, it's right over
there. I mean, I could do like fewer
there. I mean, I could do like fewer
ends maybe.
Maybe that's the
trick. We like down sample M's by a
trick. We like down sample M's by a
factor of 32.
That could work.
I mean like what if Yeah. What if we
I mean like what if Yeah. What if we
just have so many parallel ends that we
just have so many parallel ends that we
just have a perfect estimate of
just have a perfect estimate of
everything for this end,
everything for this end,
right? Cuz actually like this run isn't
right? Cuz actually like this run isn't
bad.
Like it seems like this does
Like it seems like this does
something, but it could just be that we
something, but it could just be that we
already have perfect estimates of
already have perfect estimates of
everything
here, I say. And then this like takes a
here, I say. And then this like takes a
little
dip. Okay, I want to try I'll try
dip. Okay, I want to try I'll try
something.
64 total
ends. I might have been a little bit too
ends. I might have been a little bit too
low. We'll see.
So the hope with that was
So the hope with that was
like you need to have a decent number of
like you need to have a decent number of
parallel computations
parallel computations
um in order to use your hardware
um in order to use your hardware
efficiently, but like the MCTS kind of
efficiently, but like the MCTS kind of
does
Well, this doesn't seem to be
Well, this doesn't seem to be
particularly fast at all, now does it?
If we assume optimal batch is 32k for
If we assume optimal batch is 32k for
very small nets.
Yeah, I think I probably went too
Yeah, I think I probably went too
low on
this cuz this is going to take forever.
That should be at least 4x
faster. Let's try
this. Oh, you know what? I I think the
this. Oh, you know what? I I think the
real alpha would
real alpha would
be So, you divide
be So, you divide
this by eight, right?
this by eight, right?
And then what if you just divide this by
And then what if you just divide this by
eight as
well
65 same mini batch size do we think or
65 same mini batch size do we think or
do we go way smaller mini
do we go way smaller mini
bat we can't go way smaller because the
bat we can't go way smaller because the
BPT horizon maybe All
right. Okay. So, this frames
right. Okay. So, this frames
at
at
100k steps per second.
which is quite slow.
Can we
Can we
get
get
better training out of this?
Okay. So, this seems like it does
Okay. So, this seems like it does
something
maybe. But then the question is going to
maybe. But then the question is going to
be if we just don't use
be if we just don't use
MCTS, we get the same
thing. Yeah. So this is going to be
thing. Yeah. So this is going to be
dramatically
more if it continues on this trend.
I suspect what's going to happen is the
I suspect what's going to happen is the
on policy is just going to work just as
on policy is just going to work just as
well.
You kind of have to be limited by like
You kind of have to be limited by like
very small batches or something. I
think also this isn't performing as well
think also this isn't performing as well
as it should.
Seems like this is just
unstable. Yeah, this is just messed up.
So, I don't know what happened that got
So, I don't know what happened that got
it stuck here. It's
weird. Can't even do
this. Let me think if I can pick a
this. Let me think if I can pick a
different problem where I would know for
different problem where I would know for
sure.
The thing is like we kind of just get
The thing is like we kind of just get
perfect gradient information because we
perfect gradient information because we
have these massive
have these massive
batches. We really
do. And then we have advantage filtering
do. And then we have advantage filtering
that throws away the stuff we don't need
that throws away the stuff we don't need
anyways, right?
So like if you're training with a tiny
So like if you're training with a tiny
batch of data Hey babe.
kind of recovered. That seems like
kind of recovered. That seems like
optimization screw
optimization screw
up. It was on pace to be
up. It was on pace to be
faster. I guess I could try
faster. I guess I could try
um the normal PO on
um the normal PO on
this without
search. Just see like the initial curve
search. Just see like the initial curve
maybe.
This looks like it's just
better. Yeah, this is just like straight
better. Yeah, this is just like straight
up better, right?
So yeah, this is not even like a thing
where I guess I could try to solve it
where I guess I could try to solve it
with like tiny
with like tiny
bash. That could be like a
bash. That could be like a
decent decent thing to
do cuz then you definitely at the mercy
of
Yeah. This is probably just going to be
Yeah. This is probably just going to be
unstable because of normal normalization
though. 4096
size 64. That still seems
size 64. That still seems
big.
64. That's still
64. That's still
big. No, that's
big. No, that's
correct. So we do
like 64 m for 64
steps. See if this even trains with PO.
Yeah, I forgot about
Yeah, I forgot about
that. There's literally a min
that. There's literally a min
requirement.
How's this still an
How's this still an
issue? Agents
Ah, you need Yeah, you need 32 total
Ah, you need Yeah, you need 32 total
M's, which means like
I just want to see if this does
I just want to see if this does
anything.
Hey,
Hey,
bet. Trying to get Model Base to do
bet. Trying to get Model Base to do
something. So far, it's not doing
something. So far, it's not doing
anything. like it's working, but it's
anything. like it's working, but it's
not any better than
not any better than
um Model
3. Well, have fun with
3. Well, have fun with
that. Steps, threads per block.
Super
obnoxious eight.
obnoxious eight.
Wait time 8 64
Wait time 8 64
* 8 is 5. Yeah. So, this is literally
* 8 is 5. Yeah. So, this is literally
you can only have eight threads, I
you can only have eight threads, I
believe. It's so stupid, but we're going
believe. It's so stupid, but we're going
to try it for the sake of
it. Have to run.
Well, I'm not sure here. Um, if
Well, I'm not sure here. Um, if
basically we just have so much freaking
basically we just have so much freaking
data that there's just like no point in
data that there's just like no point in
trying to get better data because we
trying to get better data because we
just have all the data we could possibly
just have all the data we could possibly
need. I don't know.
What I'm doing now is I'm just reducing
What I'm doing now is I'm just reducing
it to a ludicrously small back and
it to a ludicrously small back and
seeing
Oh, this is cash. I'm dumb.
The thing that's weird to me, right, is
The thing that's weird to me, right, is
they use this on go, which is like a
they use this on go, which is like a
ludicrously fast environment if you
ludicrously fast environment if you
implement it. Well, so like they they
implement it. Well, so like they they
didn't care about the number of steps in
didn't care about the number of steps in
which they solve go, right? They were
which they solve go, right? They were
just trying to solve it. You would think
just trying to solve it. You would think
that this would do
something. The
something. The
That's not doing
Thought it'd fix that
bug.
Thought I didn't fix that bug.
Uh, this
Uh, this
is is this just like
40? No.
40? No.
Dummy
Dummy
one
one
60.
60.
Yes. Dummy.
bugs, man. I can't even test
What? I wasn't dividing. Yeah, I
What? I wasn't dividing. Yeah, I
know. It doesn't work because of shitty
know. It doesn't work because of shitty
bugs.
But I can't fix it here because
But I can't fix it here because
it's
it's
like so
obnoxious. You can do this, but I think
obnoxious. You can do this, but I think
that this just like is going to be
that this just like is going to be
invalid experiment results.
We'll see. I guess.
It's
It's
447. I don't want to try like the learn
447. I don't want to try like the learn
and hallucinated version.
This algorithm makes sense. I would
This algorithm makes sense. I would
think that it should
think that it should
work. It doesn't not work either, I
work. It doesn't not work either, I
guess.
Let's go back to the version that
Let's go back to the version that
actually makes
sense. Let's go back to the version that
sense. Let's go back to the version that
makes sense.
This is how it's supposed to look,
right? 360 at like 13 or whatever mil.
right? 360 at like 13 or whatever mil.
What if I do
this? So look one extra step
this? So look one extra step
ahead. The world model is trained one
ahead. The world model is trained one
step ahead.
You still train at a mostly reasonable
speed and you stop
speed and you stop
learning. You no longer learn remotely
learning. You no longer learn remotely
quickly. If I increase this to five or
quickly. If I increase this to five or
10, you recover the original curves.
10, you recover the original curves.
That seems
screwy. So you get plus
rewards, the reward that you expect to
get and then you get the
value in the next state.
There's definitely something going on
There's definitely something going on
here that like you recover the original
here that like you recover the original
performance of the algorithm if you
performance of the algorithm if you
increase the roll
increase the roll
out. There's definitely something going
out. There's definitely something going
on with
on with
that. Let me think about what that
that. Let me think about what that
means.
So why would perf
So why would perf
degrade versus the baseline is the
thing. So we're checking the reward that
thing. So we're checking the reward that
we expect to get, right?
Oh, hang
Oh, hang
on. Don't we
need No, these are the correct
actions. But this reward model is wrong,
actions. But this reward model is wrong,
isn't
isn't
it? Hang on.
it? Hang on.
How can you have a reward model based on
How can you have a reward model based on
the hidden state like
this? This needs to be the world model
this? This needs to be the world model
here needs to be
predicting the reward, doesn't
it? Because it has the hidden state.
it? Because it has the hidden state.
and it has the
action. So, let me try
that. Let me try that.
Okay. So now this is the thing that's
Okay. So now this is the thing that's
going to predict your rewards,
going to predict your rewards,
right? Let's see how this
right? Let's see how this
goes. So your world model
So you sample your logits, right? You
So you sample your logits, right? You
append and then this is going to give
append and then this is going to give
you okay. So now you actually have
you okay. So now you actually have
correctly conditioned
reward. This could have been the issue
reward. This could have been the issue
to be fair. This totally could have been
to be fair. This totally could have been
it.
So then this is neck
rewards. What do we think about this?
Okay, so this now
runs. See if that makes a
runs. See if that makes a
difference. Now the reward model is
difference. Now the reward model is
correctly action conditioned.
still nowhere near in Perf,
right? The reward loss is not better
either.
Totally should
be hidden is world model shared with
be hidden is world model shared with
your world model you do
right fine.
So this gives you your next
So this gives you your next
observation and the reward for this
observation and the reward for this
observation. And then you can
compute the value in your next state is
compute the value in your next state is
the idea,
right? Yeah, you compute the value in
right? Yeah, you compute the value in
next
state. How does the reward loss not get
state. How does the reward loss not get
better from being action conditioned?
better from being action conditioned?
That seems to me like it's just not
That seems to me like it's just not
predicting the right reward,
predicting the right reward,
right? I'm pretty sure it is,
right? I'm pretty sure it is,
but that loss is assessed. That loss is
sketchy. Zero
actions supposed to be predicting the
actions supposed to be predicting the
next reward.
next reward.
And then this thing has crashed
And then this thing has crashed
totally. It's supposed to be predicting
totally. It's supposed to be predicting
the next
reward. One reward in advance. Yep.
reward. One reward in advance. Yep.
Well, you roll it out so we can do that
Well, you roll it out so we can do that
repeatedly.
It's got to definitely be next
reward. No, you're not predicting 128
reward. No, you're not predicting 128
rewards. You're just predicting right
rewards. You're just predicting right
now one, but usually like five or 10
now one, but usually like five or 10
rewards.
Maybe this one you should back
Maybe this one you should back
propagate.
kind of
kind of
hard do Yeah.
Wait, you
Wait, you
dummy. Isn't
dummy. Isn't
this forward
train? That shouldn't none of that
train? That shouldn't none of that
matters.
Yeah, none of that mattered at all.
It should not be hard to predict the
It should not be hard to predict the
next reward. It should be very very very
next reward. It should be very very very
easy to predict the next reward.
So like 0159 is just random
So like 0159 is just random
uninitialized
uninitialized
garbage and it
Inputs targets inputs targets. It's
Inputs targets inputs targets. It's
correct.
Is there any chance I'm computing the
Is there any chance I'm computing the
actions
wrong? That would totally screw it up
wrong? That would totally screw it up
though. Like it wouldn't be partially
though. Like it wouldn't be partially
fine.
Something so wrong
here. Like it does optimize and then it
here. Like it does optimize and then it
goes back up I guess.
optimize particularly well
though. Not at
though. Not at
all. Right.
What should be a very easy
loss? Was this computed?
rewards come
rewards come
from. Well, you don't need to detach
from. Well, you don't need to detach
Jack Chitten here because this is this
Jack Chitten here because this is this
is the forward
pass. This is all detached
pass. This is all detached
anyways. And then the forward
anyways. And then the forward
here, mu0 hidden is what this gets
here, mu0 hidden is what this gets
applied to. So view zero hidden is the
applied to. So view zero hidden is the
initial LSTM state that comes out of the
initial LSTM state that comes out of the
policy and actions as
well. Hidden state coming out of the
well. Hidden state coming out of the
policy into the actions
That's like a random reward prediction
That's like a random reward prediction
right
right
there. I'm pretty
there. I'm pretty
sure maybe not to be fair. Maybe it's
sure maybe not to be fair. Maybe it's
substantially better. It probably is
substantially better. It probably is
substantially better is the thing
substantially better is the thing
because the number of rewards goes up a
ton. Yeah, you know, actually that is
ton. Yeah, you know, actually that is
possibly decent. Okay.
I'm going to run this again with
I'm going to run this again with
five. We haven't done this in a
while. I think about this.
Have your world
model. What about
values? Value is action agnostic,
right? Yeah.
could technically just do it over values
could technically just do it over values
or just do it over
rewards. Probably be just over rewards,
rewards. Probably be just over rewards,
right?
The fact though that it didn't decrease
The fact though that it didn't decrease
the reward loss at
the reward loss at
all. I guess maybe it's just a tiny
all. I guess maybe it's just a tiny
action decoder, so it kind of knows what
action decoder, so it kind of knows what
it's going to do
it's going to do
anyways. That wouldn't be
anyways. That wouldn't be
crazy. Be pretty
reasonable. What happened to episode
reasonable. What happened to episode
return here?
return here?
not doing
well. There's a reward network that's
well. There's a reward network that's
separate.
separate.
Yeah, it's just there's a separate goal
Yeah, it's just there's a separate goal
of just predict next reward.
Start. I could technically do it just on
Start. I could technically do it just on
the value function, right?
Not really. You kind of do need
reward. Okay, so now this is worse than
reward. Okay, so now this is worse than
before.
with the corrected action model. That's
with the corrected action model. That's
worse than before.
Let me see if there's anything I can do
Let me see if there's anything I can do
to bring this more in line with how mu
to bring this more in line with how mu
zero
works. Um, so I think that they don't
works. Um, so I think that they don't
send it through the
send it through the
model, right? I don't think that they
model, right? I don't think that they
have a target on
have a target on
the the world model itself.
I think theirs is like a separate
I think theirs is like a separate
network,
network,
right? That just
right? That just
outputs state and reward or whatever.
outputs state and reward or whatever.
State action
reward. So what would that do if I just
reward. So what would that do if I just
had a separate thing doing state action
had a separate thing doing state action
and reward?
would just take action as input.
We'd probably use the LSTM state or
We'd probably use the LSTM state or
something as the
input. We could try that. I'd be down.
input. We could try that. I'd be down.
It's 5:16. I've got an
hour. Maybe that makes it more
hour. Maybe that makes it more
stable. Let me I want to commit all
stable. Let me I want to commit all
this.
So we make the world model a
separate separate network, right?
No, not at
all. This needs the LSTM thing,
all. This needs the LSTM thing,
doesn't
it? Should be easier than the other
it? Should be easier than the other
ones.
It kind of should be a GRU,
right? Does it
matter? I don't think it matters
matter? I don't think it matters
actually. Does it?
I think we can just reuse this
I think we can just reuse this
probably. I'll have to screw with the
probably. I'll have to screw with the
GRU.
Hell's wrong with
this. So is it
just Okay, so this is the cell, right?
just Okay, so this is the cell, right?
It's got the cell and then it needs
It's got the cell and then it needs
input.
Action embed.
And then world model
And then world model
forward. It's actually going to
forward. It's actually going to
take actions and then hidden
take actions and then hidden
state and then it
will self to action embed actions.
Get
in and go
This works.
state and
state and
then hidden state gets to
then hidden state gets to
be. So action equals action head of the
be. So action equals action head of the
hidden reward head. It returns
hidden reward head. It returns
actions and
actions and
rewards and state.
Okay. So, let's think about
this. We can just put this in the LSTM
this. We can just put this in the LSTM
one, right?
This takes
This takes
actions and state and returns actions,
actions and state and returns actions,
rewards and state which is actually it
rewards and state which is actually it
should be logits rewards and state.
But it should
But it should
return
pockets
right. So this is world model
logs.
logs.
Okay. And then this has to
Okay. And then this has to
take zero
take zero
actions and do a zero
hidden and then the hidden should
be should it be the hidden state of the
be should it be the hidden state of the
model? Should it be the
embedding? I kind of like making it
embedding? I kind of like making it
the we'll give
it 0
equals obsed
U
C.
C.
Okay. Sample your
Okay. Sample your
logits. You have your logits, your
logits. You have your logits, your
rewards, and your state.
That's
rewards.
rewards.
Escape. Rolling
forwards.
forwards.
Then
Then
hidden. No longer need this
hidden. No longer need this
because this is
because this is
done. This is done.
You do need values, don't
You do need values, don't
you? I think you do need
values. Yeah.
state. That's pretty close.
state. That's pretty close.
Now in clean
puff, get out of here,
bot.
bot.
Okay, so we need we actually do need to
Okay, so we need we actually do need to
define this
define this
thing like this but for the world model.
thing like this but for the world model.
So easiest way to do
So easiest way to do
that buffer
that buffer
GRU it I I realize it's not going to
GRU it I I realize it's not going to
make a
difference. This is
difference. This is
forward. We have forward
train. Shit's completely fine.
ships completely
fine. Encode observations. I'm realizing
fine. Encode observations. I'm realizing
we can literally get rid of all this now
we can literally get rid of all this now
because the new API. That's going to be
because the new API. That's going to be
nice.
nice.
Um, yeah, there's no encode
Um, yeah, there's no encode
observations,
observations,
right? You just give
right? You just give
it the actions or whatever the
hell. I guess it's action m
You need any of this
Maybe that's
enough. Now hit in
I mean, does it freaking
I mean, does it freaking
matter? I just skip all this
You don't need to decode actions
You don't need to decode actions
here. You just
do. Okay, that works for
state.
Um,
okay. I think it's something simple like
okay. I think it's something simple like
this. We're going to have to debug this
this. We're going to have to debug this
into
into
existence. May code.
Stop. Neptune needs to not take
Stop. Neptune needs to not take
forever to
initialize. So you do need opposite
initialize. So you do need opposite
betting now or what?
betting now or what?
You need obsetting just to
You need obsetting just to
initialize uh just to initialize I
initialize uh just to initialize I
think. Okay.
cannot address what model
stayed. Hey there. How's it going? We're
stayed. Hey there. How's it going? We're
doing all right. We're deep in
doing all right. We're deep in
uh modelbased RL territory here.
Is this the correct initial state? I
Is this the correct initial state? I
don't think it
is. We need to get
is. We need to get
this state equals.
this state equals.
Yeah,
that's
okay.
54.
54.
Probably some shade
Uh that is not supposed to
happen. State does state get
transposed? State get transposed.
I didn't think state gets
I didn't think state gets
transposed. No, it doesn't. So, you give
transposed. No, it doesn't. So, you give
this
thing time and then batch, which is what
thing time and then batch, which is what
I gave it.
Why? Why is
Why? Why is
this
object expected hidden of zero size
object expected hidden of zero size
one?
one?
Oh yeah.
Oh yeah.
Um, hang on.
In this state, LSTMC isn't going to do
In this state, LSTMC isn't going to do
either, right? This needs to be
zeroed. Is this even getting trained
zeroed. Is this even getting trained
correctly? Hang
on. Yeah. I mean technically it is
right but
um doesn't have a cell state to start
with. I guess it just starts with zero
with. I guess it just starts with zero
for cell Okay.
Why is this the shape of this stupid
thing? I guess it's like
and do
and do
this and then it'll
be like this.
be like this.
Maybe that ought to be right shape.
sake. How about just zeros
like How about that?
Drive me
crazy. Five.
Too many values to
unpack. So this is going to return
unpack. So this is going to return
logits. Uh we don't need logits, right?
logits. Uh we don't need logits, right?
We
need We just
need We just
need What do we need for this? I think
need What do we need for this? I think
we literally just
we literally just
need
state target reward target state,
right? Well, we need value target.
How's the policy get trained then? Wait,
How's the policy get trained then? Wait,
what?
Okay. So
now we can
now we can
do
do
finally next state
finally next state
presence it's world model
presence it's world model
state
one
state or loss
We also have the freaking world model
We also have the freaking world model
policy laws, don't
we? I guess we want to like behavioral
we? I guess we want to like behavioral
clone this thing or something. What does
clone this thing or something. What does
it train the logits on this too?
It's the
It's the
objective policy.
So we
just we have the Diane loss that we were
just we have the Diane loss that we were
using.
I have to get these indexes All right.
H C.
Which one of these are you freaking
using? There's no more obs,
right? You just don't do this anymore.
right? You just don't do this anymore.
So can I do
I don't think you need
um I think you can just use
um I think you can just use
I'll just use
I'll just use
this. You put
the it gets the observation
in starting
in starting
from the state
from the state
which gets an OBS
which gets an OBS
embedding. So it gets the first
embedding. So it gets the first
OBS and then it's going to
produce. Does this thing have a policy
produce. Does this thing have a policy
on it?
It has to,
right? So, I guess that's like the
right? So, I guess that's like the
previous action or whatever.
Now this is the action that's going to
Now this is the action that's going to
produce the next
produce the next
state. So you took this action with this
state. So you took this action with this
ops.
ops.
Okay. And then the reward this makes
Okay. And then the reward this makes
sense. This is
sense. This is
offset. Uh the value
function is is I think that this
is this
The actions
The actions
also just like this
Come on.
Value
loss. New value.
Okay, better.
Okay, we're now actually maybe getting
Okay, we're now actually maybe getting
somewhere. Um, I don't think we're doing
somewhere. Um, I don't think we're doing
anything remotely
sane in the Ford Passover here, right?
Oh, no. We
Oh, no. We
are. So, we just sample. We just roll
are. So, we just sample. We just roll
out through the world
out through the world
model. And this kind of does stuff, I
model. And this kind of does stuff, I
guess,
right? And um we probably want to train
right? And um we probably want to train
this thing with
this thing with
like the same
value loss, I
value loss, I
guess. How do we want to do
this? I mean, you have this like extra
this? I mean, you have this like extra
thing now.
Yeah. So, I kind of see that this would
Yeah. So, I kind of see that this would
be way simpler.
Um, this would be like way
simpler if your policy weren't recurrent
simpler if your policy weren't recurrent
because then you use the state to
because then you use the state to
represent the world model, but it has to
represent the world model, but it has to
be separate because of that.
be separate because of that.
Okay. I have a crazy idea and I'm not
Okay. I have a crazy idea and I'm not
qualified to build it. Sure, go ahead.
It's crazy. This does actually kind of
train reward
loss. So that's going to be squared
loss. So that's going to be squared
error with the actual reward. That's
easy. I don't advocate that particular
easy. I don't advocate that particular
slogan bet.
slogan bet.
But I I would just say to learn the damn
You could very easily do something that
You could very easily do something that
pretends to do that
pretends to do that
um with the language models and it would
um with the language models and it would
just be like bollocks. It would just all
just be like bollocks. It would just all
be smok and
mirrors. It would have like no
mirrors. It would have like no
predictive power of
anything even remotely possible. No, not
anything even remotely possible. No, not
at all.
It's kind of like um there's like a lot
It's kind of like um there's like a lot
of adjacent language model stuff that's
of adjacent language model stuff that's
like really flashy looking but just
like really flashy looking but just
doesn't make any sense if you think
doesn't make any sense if you think
about it for a few Thanks.
Man, so we just have to fix these losses
Man, so we just have to fix these losses
now, huh?
Probably this doesn't need to be clipped
Probably this doesn't need to be clipped
to be
honest. But then what do we do with
this? Does this just need um
I got rid of the encoding piece, which
I got rid of the encoding piece, which
is good.
supplementary figure
2, which is not in
here. Where's the non version of
here. Where's the non version of
this? I don't know why they publish so
this? I don't know why they publish so
much crap in nature. It's like their
much crap in nature. It's like their
format just
sucks. They have this on it's on archive
sucks. They have this on it's on archive
and now you can actually read
and now you can actually read
it. Yeah. Here you go. The nature paper
it. Yeah. Here you go. The nature paper
format is just
No, I think it's the opposite. I think
No, I think it's the opposite. I think
there's a strict page
there's a strict page
limit, isn't
it? The hell is this?
Huh? The
Oh, this is their bucketed thing.
Oh, this is their bucketed thing.
We don't have to care that
We don't have to care that
much. Okay. Squared error
loss then.
We can make the world model
We can make the world model
bigger. I don't think
bigger. I don't think
that's could be the issue. I don't
know. This thing is supposed to scale.
It's pretty cheap to train this as
It's pretty cheap to train this as
well. The inference cost that'll get you
Isn't the reward loss higher than
before? Hang on. This gets H is the OBS
before? Hang on. This gets H is the OBS
embedding and then
C there's no
C there's no
C because it's zeros right on first
Well, hang on. How's
this? This is training to predict next
reward,
right?
right?
Yes, the value function is training.
I think I got to go for
I think I got to go for
dinner. Value function is training to
do. So you do the value function needs
do. So you do the value function needs
to be trained to predict the next value
to be trained to predict the next value
I believe.
Try
that. I don't think that'll do anything,
that. I don't think that'll do anything,
but we'll see.
Okay, we will continue on
Okay, we will continue on
this. We will continue on this
this. We will continue on this
afterwards. Uh I Let me go double check
afterwards. Uh I Let me go double check
see if dinner's if it's dinner time.
Okay, I will be back after dinner. Um,
Okay, I will be back after dinner. Um,
thanks for tuning in, folks. There will
thanks for tuning in, folks. There will
be hopefully a little bit more world
be hopefully a little bit more world
modeled stuff later tonight. I'd really
modeled stuff later tonight. I'd really
like to get this model working. Um, I
like to get this model working. Um, I
just I was going to invest a day in it
just I was going to invest a day in it
for now and then we'll come back later.
for now and then we'll come back later.
But uh all my stuff's at
puffer.ai. You can check out all open
puffer.ai. You can check out all open
source code. If you want to help me out
source code. If you want to help me out
for free, start the GitHub. If you want
for free, start the GitHub. If you want
to get involved with development, join
to get involved with development, join
the
the
Discord. Some of our best contributors
Discord. Some of our best contributors
came in with zero RL experience. And if
came in with zero RL experience. And if
you want more RL content, you can follow
you want more RL content, you can follow
me on X. Post all sorts of things there.
me on X. Post all sorts of things there.
Uh other than that, thank you and see
Uh other than that, thank you and see
you
