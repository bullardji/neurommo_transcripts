Kind: captions
Language: en
okay we are
okay we are
live
live
morning uh we have some stuff to do
today so I think that we're just going
today so I think that we're just going
to optimize the heck out of training
to optimize the heck out of training
today
today
in
puff we're going to check on our
puff we're going to check on our
experiments we ran overnight first
experiments we ran overnight first
though we got to check on those
there's one thing I want to check on
there's one thing I want to check on
first which is just the uh this run
first which is just the uh this run
versus the previous
versus the previous
run I want to check on the sample
run I want to check on the sample
complexity just so that we know whether
complexity just so that we know whether
our new algorithm is performing as well
our new algorithm is performing as well
as uh as we think it should be able to
okay so this is the one that we had
okay so this is the one that we had
where it's like really really good
right this gamma Lambda and then if we
right this gamma Lambda and then if we
look here lambda's all over the place
look here lambda's all over the place
gamma's whatever because it doesn't
gamma's whatever because it doesn't
matter in this uh this version
but what we want to look at
but what we want to look at
is the
charts so here we have
why are all of these experiments running
why are all of these experiments running
so
long shouldn't be
right hang on are these
right hang on are these
all these are all at like
100k
and so very very weird
and so very very weird
there or 100
Mil these ones
we also have clearly we have 50 mil
we also have clearly we have 50 mil
solves for this one as
solves for this one as
well right this is a 50 mil solve right
here now it does look like we have a few
here now it does look like we have a few
somewhat uh faster solves over here mind
somewhat uh faster solves over here mind
you haven't finished the other sweep
you haven't finished the other sweep
yet it's weird that they're all out to
yet it's weird that they're all out to
100 Mil
100 Mil
though
though
right they shouldn't all be up to 100
Mil I don't know why they would be
what end this is breakout so this is
what end this is breakout so this is
this is our completed sweep over
this is our completed sweep over
breakout and you can see we have solves
breakout and you can see we have solves
in a minute and a half and almost solved
in a minute and a half and almost solved
and this is under two
minutes so this one is very
minutes so this one is very
good this is just PPO
good this is just PPO
J but it's weird how the total time
J but it's weird how the total time
steps is locked to 100 million up top
right like I don't know how it's this
right like I don't know how it's this
fast at 100
Mil wa a cost graph didn't
Mil wa a cost graph didn't
we yeah we do have a cost graph so
kind of want to see this one
21 212
see this one is reasonable this one is
see this one is reasonable this one is
not 100
Mil maybe this one yeah it's this one
Mil maybe this one yeah it's this one
here
but I don't know why there's so many
but I don't know why there's so many
samples all the way up
samples all the way up
here it's doing something clearly
then we
have I mean we have pretty darn good
have I mean we have pretty darn good
solves with the other algorithm as well
solves with the other algorithm as well
the new algorithm
I suppose the question is going to be
I suppose the question is going to be
[Music]
[Music]
whether the uh the new
whether the uh the new
one is just not dialed into the level of
one is just not dialed into the level of
the other
the other
experiments or if there's a
experiments or if there's a
gap we'll have to look
at I it's very close at this point
it is very
close well we'll let the sweep run
close well we'll let the sweep run
before we uh pass final judgment
before we uh pass final judgment
here so I want to do something a little
here so I want to do something a little
different today I want to just work on
different today I want to just work on
some optimization
for at least a little
bit so this is a uh clean up a faster
bit so this is a uh clean up a faster
version of clean ARL that's like six
version of clean ARL that's like six
times faster I think we're five times
times faster I think we're five times
faster than this already but there's
faster than this already but there's
some things in here that we haven't done
let's
let's
see there are a few new things
see there are a few new things
here Vincent
well this is
well this is
obvious reason for this stream series I
obvious reason for this stream series I
just stream my Dev dayto day on Puffer C
just stream my Dev dayto day on Puffer C
my goal is to fix reinforcement learning
my goal is to fix reinforcement learning
I want to make everything fast and
I want to make everything fast and
stable uh We've made a ton of progress
stable uh We've made a ton of progress
on this as well so if you just go to
on this as well so if you just go to
puffer doai you can see we have a dozen
puffer doai you can see we have a dozen
different environments here from very
different environments here from very
simple stuff like breakout to very
simple stuff like breakout to very
complex like this mini
complex like this mini
MMO and uh all of these things that
MMO and uh all of these things that
you're seeing are agents just playing
you're seeing are agents just playing
the game trained with reinforcement
the game trained with reinforcement
learning some of them being superum
so we are trying to make a really fast
so we are trying to make a really fast
environment and really good tools to
environment and really good tools to
make it easy to do reinforcement
make it easy to do reinforcement
learning very
fast so at the moment I think we're
fast so at the moment I think we're
going to be doing today some uh
going to be doing today some uh
optimization on the training side of the
optimization on the training side of the
current Library it's already very fast
current Library it's already very fast
but I want to see if there's any more
but I want to see if there's any more
performance that we can eek out of it
interesting that's
interesting that's
silly so let's see if they found
silly so let's see if they found
anything useful in lean RL
anything useful in lean RL
here let me load the chat as well so I
here let me load the chat as well so I
can deal with that if I need to
I think when I talked with Vincent I
I think when I talked with Vincent I
don't think he had I don't I think he
don't think he had I don't I think he
understood shared memory correctly
understood shared memory correctly
um let me see so I want to create a CPU
um let me see so I want to create a CPU
tensor in P content of this tensor needs
tensor in P content of this tensor needs
to be placed in
to be placed in
memory we distinguish two types of
memory we distinguish two types of
memory the RAM and the swap
memory the RAM and the swap
Bas together these make a virtual
memory okay but you should never be
memory okay but you should never be
doing that right
doing that right
you should never be swapping to dis
you should never be swapping to dis
anyways regular CPU tensor is pageable
anyways regular CPU tensor is pageable
which means it's divided in blocks
which means it's divided in blocks
called pages that can live anywhere in
called pages that can live anywhere in
the virtual memory both in Ram or on dis
the virtual memory both in Ram or on dis
but they're going to stay on
but they're going to stay on
RAM memory seems larger okay when
RAM memory seems larger okay when
program accesses a page that's not Ram
program accesses a page that's not Ram
page fult
page fult
occurs and Os brings this back into RAM
occurs and Os brings this back into RAM
okay this should this is all irrelevant
okay this should this is all irrelevant
this should never happen in RL
a pinned memory is a type of memory that
a pinned memory is a type of memory that
cannot be swapped out to disc okay good
cannot be swapped out to disc okay good
so this is just telling the not to swap
so this is just telling the not to swap
the
memory it's kind of weird to me that it
memory it's kind of weird to me that it
would swap out the memory in the first
would swap out the memory in the first
place when you still have plenty of RAM
memory is Page block the device can
memory is Page block the device can
access the memory directly in the main
access the memory directly in the main
memory the memory is pageable all pages
memory the memory is pageable all pages
will have to be brought to the main
will have to be brought to the main
memory before being sent to the
memory before being sent to the
GPU Fuda sends pageable data from CPU to
GPU Fuda sends pageable data from CPU to
GPU it must create it must first create
GPU it must create it must first create
a page lock copy of that data before
a page lock copy of that data before
making the
transfer okay so this is interesting so
transfer okay so this is interesting so
when Cuda sends pageable data from CPU
when Cuda sends pageable data from CPU
to GPU it must first create a page
to GPU it must first create a page
locked copy of that
data asynchronous versus synchronous
data asynchronous versus synchronous
when executing a copy from host device
when executing a copy from host device
tool coolkit to the Cuda toolkit offers
tool coolkit to the Cuda toolkit offers
modalities to do these operations
modalities to do these operations
synchronistically or
synchronistically or
asynchronously with respect to the host
asynchronously with respect to the host
in practice when call two PCH always
in practice when call two PCH always
makes a call to Cuda me copy
ASN if nonblocking is false Auda stream
ASN if nonblocking is false Auda stream
synchronized we called after each and
synchronized we called after each and
every Cuda M Copy Asing
okay if nonblocking is true then no
okay if nonblocking is true then no
synchronization is triggered and the
synchronization is triggered and the
main thread on the host is not
main thread on the host is not
blocked so multiple tensors can be sent
blocked so multiple tensors can be sent
to the device on mously as the thread
to the device on mously as the thread
does not have to wait for one transfer
does not have to wait for one transfer
to be completed to initiate the other so
to be completed to initiate the other so
this is
this is
important in general the transfer a the
important in general the transfer a the
transfer is blocking on the device side
transfer is blocking on the device side
even if it isn't on the host
even if it isn't on the host
side the copy on the device cannot occur
side the copy on the device cannot occur
while another operation is executed
while another operation is executed
however in some Advanced scenarios a
however in some Advanced scenarios a
copy internal execution can be done
copy internal execution can be done
simultaneously on the GPU
simultaneously on the GPU
side three requirements must be met to
side three requirements must be met to
enable this the device must have at
enable this the device must have at
least one free direct memory access
least one free direct memory access
engine modern GPU architector have more
engine modern GPU architector have more
than
than
one the transfer must be done on a
one the transfer must be done on a
separate non deault Cuda stream in
separate non deault Cuda stream in
pytorch Cuda streams can be handles
pytorch Cuda streams can be handles
using
using
stream The Source data must be pinned in
stream The Source data must be pinned in
pinned memory demonstrate this by
pinned memory demonstrate this by
running profiles in the following
running profiles in the following
scripts so okay there's some good stuff
scripts so okay there's some good stuff
here so you have a
here so you have a
stream you
stream you
have pinned and non- pinned
tensors and then we have a Cuda
tensor this is just a perf test
and
and
then oh I didn't know that these nice
then oh I didn't know that these nice
profilers were there
full we will probably be using this
full we will probably be using this
later today in profiling puffer lip
using a pinned memory tensor doesn't
using a pinned memory tensor doesn't
change the trace much both operations
change the trace much both operations
are still executed consecutively can we
are still executed consecutively can we
see this let's go to review
see this let's go to review
mode I also want to work in RL and do
mode I also want to work in RL and do
research in it myself but I don't have a
research in it myself but I don't have a
good uh good laptop with GPU for
good uh good laptop with GPU for
training only have C Core i7 suggest me
training only have C Core i7 suggest me
what I should do some of puffer's
what I should do some of puffer's
environments are so fast that uh you can
environments are so fast that uh you can
kind of do training on our environments
kind of do training on our environments
on CPU faster than the majority of
on CPU faster than the majority of
researchers can do on GPU so if you just
researchers can do on GPU so if you just
want to like get into it a little bit
want to like get into it a little bit
and train some stuff and not have it
and train some stuff and not have it
take forever uh you can probably still
take forever uh you can probably still
train a few of our environments in maybe
train a few of our environments in maybe
it takes 5 10 minutes like our stuff is
it takes 5 10 minutes like our stuff is
really fast you probably can still do
really fast you probably can still do
training at like 100,000 steps per
training at like 100,000 steps per
second for breakout or
calm for context on a modern core each
calm for context on a modern core each
of these environments runs a million
of these environments runs a million
steps per second we use the GPU to get
steps per second we use the GPU to get
300,000 to a million step per second
300,000 to a million step per second
training but for the smaller models you
training but for the smaller models you
can often still train at 100K just on a
can often still train at 100K just on a
like even on just a
core and give that a try
I also have a quick start guide on the
I also have a quick start guide on the
website that I highly highly recommend I
website that I highly highly recommend I
mean I literally wrote it for people who
mean I literally wrote it for people who
want an easy way to get into RL so let's
want an easy way to get into RL so let's
go to blog quick start guide It's like a
go to blog quick start guide It's like a
20 minute read links you to some other
resources can we see what this image
is okay cool so now we can see this
is okay cool so now we can see this
um and I actually should go back to here
um and I actually should go back to here
so you can see it
properly okay at 10
properly okay at 10
2 to copy and then we can see that this
2 to copy and then we can see that this
little Gap here this is the overhead of
little Gap here this is the overhead of
whatever the P torch binding is and this
whatever the P torch binding is and this
is the whole cud M Copy
is the whole cud M Copy
okay this is the first
okay this is the first
one
one
microc and here's the
synchronize
okay sending a pageable tensor to GPU on
okay sending a pageable tensor to GPU on
a separate stream
a separate stream
is also a blocking
operation streamed equals
operation streamed equals
true only pinned tensor copies only
true only pinned tensor copies only
pined tensor copies to GPU on a separate
pined tensor copies to GPU on a separate
stream overlap with another Cuda kernel
stream overlap with another Cuda kernel
executed on the
mainstream okay
on a separate stream though so how do
on a separate stream though so how do
you use this stream
you use this stream
thing with to torch Cuda stream s ah you
thing with to torch Cuda stream s ah you
have to manually manage these things
have to manually manage these things
that's
annoying okay but it's not that hard to
annoying okay but it's not that hard to
deal with
where did we we
go high torch perspective pin memory
go high torch perspective pin memory
High torch offers the possibility to
High torch offers the possibility to
create and send tensors to page lock
create and send tensors to page lock
memory through the pin memory
memory through the pin memory
method and Constructor arguments CPU
method and Constructor arguments CPU
tensors on a machine where food is
tensors on a machine where food is
initialized can be pinned cast to pin
initialized can be pinned cast to pin
memory though the pin through the pin me
memory though the pin through the pin me
memory method in memory is blocking on
memory method in memory is blocking on
the main thread of the host it waits for
the main thread of the host it waits for
the testor to be copied to page lock yes
the testor to be copied to page lock yes
you should never be doing pin memory in
you should never be doing pin memory in
a loop or something anyways that's dumb
a loop or something anyways that's dumb
new tensors can be created in pin memory
new tensors can be created in pin memory
with functions like zeros on and other
with functions like zeros on and other
Constructors let's check the speed of
Constructors let's check the speed of
sending pin of pinning memory and
sending pin of pinning memory and
sending tensor to
Cuda pageable tensor pin memory. two
Cuda pageable tensor pin memory. two
device yeah so this is stupid if you're
device yeah so this is stupid if you're
doing this you don't know what you're
doing this you don't know what you're
doing uh pageable tensor do2 device the
doing uh pageable tensor do2 device the
Baseline and then you save 20% uh if it
Baseline and then you save 20% uh if it
is pinned
cool so contrary just a somewhat common
cool so contrary just a somewhat common
believe calling pin memory on a pageable
believe calling pin memory on a pageable
tensor before casting the GPU should not
tensor before casting the GPU should not
bring any significant speed up and
bring any significant speed up and
should be slower okay yeah if you're
should be slower okay yeah if you're
doing this you just don't understand
doing this you just don't understand
your Hardware yo how's it going
your Hardware yo how's it going
Spencer we're going to perf optimize the
Spencer we're going to perf optimize the
crap out of puffer
today we're going to see if we can hit 2
today we're going to see if we can hit 2
million train SPS on
something I just haven't done this in
something I just haven't done this in
many months so I figure I'd take a day
many months so I figure I'd take a day
to do
to do
it non blocking is true
okay to act account accurately of the
okay to act account accurately of the
benefit of using nonblocking we'll
benefit of using nonblocking we'll
design a slightly more complex
design a slightly more complex
experiment since we want to assess how
experiment since we want to assess how
fast it is to send multiple
fast it is to send multiple
tensors to GPU with and without calling
tensors to GPU with and without calling
nonblocking
okay 2 okay so it's not magic you know
okay 2 okay so it's not magic you know
there's like 20 25% overhead sometimes
there's like 20 25% overhead sometimes
with
synchronization to get a better sense of
synchronization to get a better sense of
what is happening here oh feeling pretty
what is happening here oh feeling pretty
confident on the portability of GPU
confident on the portability of GPU
drive only got one more thing to clarify
drive only got one more thing to clarify
with how the data set gets loaded and
with how the data set gets loaded and
should be
should be
clear great I also I went to Brennan's
clear great I also I went to Brennan's
defense yesterday Brennan is the uh the
defense yesterday Brennan is the uh the
author of
author of
Madrona um we'll see if he has ideas on
Madrona um we'll see if he has ideas on
GPU drive for the madona version but I
GPU drive for the madona version but I
think regardless that our CPU version is
think regardless that our CPU version is
just going to be awesome to have um
just going to be awesome to have um
because regardless right it will make
because regardless right it will make
the project faster F even if they get it
the project faster F even if they get it
to work in madona ours will run on web
to work in madona ours will run on web
which theirs will not right ours will be
which theirs will not right ours will be
easier to use because it won't require
easier to use because it won't require
all the C++ Shenanigans and this is also
all the C++ Shenanigans and this is also
an opportunity for us to figure out
an opportunity for us to figure out
variable agent stuff in C uh which will
variable agent stuff in C uh which will
then let us make a whole new class of
then let us make a whole new class of
environments and also I think that in
environments and also I think that in
the process of doing this you're
the process of doing this you're
probably going to have to do stuff like
probably going to have to do stuff like
go look at mad drona's uh bounding
go look at mad drona's uh bounding
volume hierarchy thing which is
volume hierarchy thing which is
how they handle uh lots of entities in
how they handle uh lots of entities in
3D and that will be an amazing piece to
3D and that will be an amazing piece to
have uh for puffer in see so I think we
have uh for puffer in see so I think we
can just use some of those things in
can just use some of those things in
like portm to see and then we'll have
like portm to see and then we'll have
way way way more flexibility with
way way way more flexibility with
working with all sorts of 3D
working with all sorts of 3D
environments in the future so I think
environments in the future so I think
that you're like if this is the type of
that you're like if this is the type of
stuff you're interested in yeah this is
stuff you're interested in yeah this is
really going to be at the frontier of
really going to be at the frontier of
how do we make uh fast M's work I got to
how do we make uh fast M's work I got to
chat with Brandon actually on what he's
chat with Brandon actually on what he's
planning to do going forward cuz he's
planning to do going forward cuz he's
got some cool
tack some really exciting ways to expand
tack some really exciting ways to expand
the M oh yeah lots of
the M oh yeah lots of
them
definitely it's kind of funny I realized
definitely it's kind of funny I realized
watching brenan's defense y yesterday
watching brenan's defense y yesterday
that like the stuff he's doing is way
that like the stuff he's doing is way
way way more sophisticated than what I'm
way way more sophisticated than what I'm
doing with puffer lip right um you know
doing with puffer lip right um you know
he's written like a Cuda game engine
he's written like a Cuda game engine
basically uh
basically uh
but the stuff that we're doing is like
but the stuff that we're doing is like
so much more flexible and it's still
so much more flexible and it's still
fast for so many different domains and
fast for so many different domains and
it's so much easier to do
right so I don't know there are
right so I don't know there are
definitely things that we cannot do uh
definitely things that we cannot do uh
that he can do with like his uh with his
that he can do with like his uh with his
approach that I want to think about like
approach that I want to think about like
we've never tried software rendering for
we've never tried software rendering for
instance um I don't know how fast we
instance um I don't know how fast we
could get like pixel rendering done if
could get like pixel rendering done if
we wanted
we wanted
to though technically to be fair we
to though technically to be fair we
could take uh I think we could just use
could take uh I think we could just use
their batch renderer so we could because
their batch renderer so we could because
it like renders from it renders from a
it like renders from it renders from a
data format that's very similar to what
data format that's very similar to what
we have so Tech technically we could
we have so Tech technically we could
still write the whole Sim on CPU and
still write the whole Sim on CPU and
then just render on
then just render on
GPU I think that would still
work I don't
know you can have every trajectory in
know you can have every trajectory in
the end be a controllable agent if you
the end be a controllable agent if you
could port a map of La or something then
could port a map of La or something then
Rand r l set goals of a [ __ ] ton of cars
Rand r l set goals of a [ __ ] ton of cars
yeah no you could definitely I mean if
yeah no you could definitely I mean if
you want to make you're basically making
you want to make you're basically making
high per GTA at that
point it' be sick yeah we'd be happy to
point it' be sick yeah we'd be happy to
have that that'd be cool that would be
have that that'd be cool that would be
awesome you just have to evaluate how
awesome you just have to evaluate how
hard individual things are right and
hard individual things are right and
what Corners to cut and stuff like
that profile
memory what are we looking at
memory what are we looking at
here welcome YouTube
here welcome YouTube
folks today is going to be optimizing
folks today is going to be optimizing
the crap out of puffer label all day
the crap out of puffer label all day
we're going to try to hit two million
we're going to try to hit two million
steps per second training on some of our
steps per second training on some of our
environments that's going to be the goal
environments that's going to be the goal
I don't know if it's even possible but
I don't know if it's even possible but
we're going to try it
we're going to try it
uh I'm currently going through this blog
uh I'm currently going through this blog
post from Vincent on larl which is an
post from Vincent on larl which is an
optimized version of clean RL that's
optimized version of clean RL that's
like five or six times faster puffer Li
like five or six times faster puffer Li
is already five times faster than this
is already five times faster than this
but I don't think we're using all the
but I don't think we're using all the
same optimizations so there might still
same optimizations so there might still
be stuff in here first start with board
be stuff in here first start with board
look into Bing volume hierarchy stuff
look into Bing volume hierarchy stuff
it's necessary for a bunch of 3D
it's necessary for a bunch of 3D
collisions yeah they have stuff for
collisions yeah they have stuff for
collisions in there now I don't know if
collisions in there now I don't know if
the way that they do it is actually
the way that they do it is actually
going to be the best way to do it right
going to be the best way to do it right
there are a lot of different ways that
there are a lot of different ways that
you can do that type of stuff um so
you can do that type of stuff um so
you'll have to look at what if the way
you'll have to look at what if the way
that they have it make
sense let's start with uh let's see the
sense let's start with uh let's see the
call stack with a regular. two device
first call to device profile memory copy
first call to device profile memory copy
to device
tensors this says
so there's a whole bunch of garbage here
so there's a whole bunch of garbage here
two copy empty
strided I don't know what any of that
strided I don't know what any of that
is I still don't know if it's more
is I still don't know if it's more
efficient to try to emulate the physics
efficient to try to emulate the physics
with a good model to train an RL based
with a good model to train an RL based
model
model
wait I still don't know if it's more if
wait I still don't know if it's more if
to try to emulate the physics with a
to try to emulate the physics with a
good model to train an RL based model
good model to train an RL based model
they have really low compute fluid
they have really low compute fluid
models two minute papers talked about
models two minute papers talked about
this years ago I don't know about any of
this years ago I don't know about any of
that
stuff non-blocking
stuff non-blocking
version call to
version call to
device profile
device profile
memory and now I don't know what all
memory and now I don't know what all
these functions are though empty
these functions are though empty
strided to copy copy
like what what the heck
right better when using non-blocking
right better when using non-blocking
true as all transfers are initiated
true as all transfers are initiated
simultaneously so this is 23 Ms 31 Ms of
simultaneously so this is 23 Ms 31 Ms of
okay synergies now that we've made the
okay synergies now that we've made the
point that data transfers of tensor
point that data transfers of tensor
already in pin memory the GPU is faster
already in pin memory the GPU is faster
and that we know that do the transfers
and that we know that do the transfers
asynchronously is faster than
asynchronously is faster than
synchronously we can Benchmark the
synchronously we can Benchmark the
combination of these
approaches first let's write a couple of
approaches first let's write a couple of
new functions that we'll call pin memory
new functions that we'll call pin memory
and two device on each
and two device on each
tenser oh yeah also one thing I've been
tenser oh yeah also one thing I've been
wondering guys just for this stream does
wondering guys just for this stream does
the mic sound substantially better if I
the mic sound substantially better if I
move it
move it
closer like if I do this is it is this
closer like if I do this is it is this
substantially better
now that we've made the point the data
now that we've made the point the data
transfers okay
so I'll look into this I think I thought
so I'll look into this I think I thought
of a way to update on every simulation
of a way to update on every simulation
run just by running multi- agency
run just by running multi- agency
framework to change circuit parameters
framework to change circuit parameters
and get rid of bad Sims quickly that'd
and get rid of bad Sims quickly that'd
be cool yeah anything you can do to make
be cool yeah anything you can do to make
Sim faster is usually really helpful in
Sim faster is usually really helpful in
RL
we can Benchmark the combination of
we can Benchmark the combination of
these approaches okay so pin plus
async pin memory are more pronounced for
async pin memory are more pronounced for
somewhat large batches of large
somewhat large batches of large
tensors okay so this is a very very
tensors okay so this is a very very
optimistic benchmark
okay so it can be two or three times
okay so it can be two or three times
faster in a very optimistic case
here favorite RL Benchmark right
here favorite RL Benchmark right
now uh Benchmark all the benchmarks
now uh Benchmark all the benchmarks
suck all the benchmarks really suck at
suck all the benchmarks really suck at
the moment what we're doing is we are
the moment what we're doing is we are
just kind of using our environments in
just kind of using our environments in
puffer lib uh as they are in their
puffer lib uh as they are in their
current form so puffer lib we're not
current form so puffer lib we're not
providing RM is like a stable Benchmark
providing RM is like a stable Benchmark
right like they will keep we will keep
right like they will keep we will keep
updating them and we will keep fixing
updating them and we will keep fixing
bugs and we're not doing historical
bugs and we're not doing historical
versioning and all sorts of shenanigans
versioning and all sorts of shenanigans
the idea is that at any point in time if
the idea is that at any point in time if
you have two methods right that you want
you have two methods right that you want
to compare you just train them both on
to compare you just train them both on
our environments and you see what you
our environments and you see what you
get and the training is so fast that you
get and the training is so fast that you
can do that you know you can do that and
can do that you know you can do that and
we don't really need to have like an old
we don't really need to have like an old
historical copy
historical copy
um but yeah puffer lib RM are what we're
um but yeah puffer lib RM are what we're
using we will be doing like some uh
using we will be doing like some uh
external runs I'd say on like Atari and
external runs I'd say on like Atari and
proen and stuff but we're not going to
proen and stuff but we're not going to
do optimization like we're not going to
do optimization like we're not going to
do hyperparameter sweeps I don't think
do hyperparameter sweeps I don't think
anymore over original atar your proen
anymore over original atar your proen
it's just you're wasting
it's just you're wasting
compute it's so bloody
slow until now we've operated under the
slow until now we've operated under the
assumption that asynchronous copies from
assumption that asynchronous copies from
the CPU to the GPU are safe
what this is generally true because Cuda
what this is generally true because Cuda
automatically handles synchronization to
automatically handles synchronization to
ensure that data is being access is
ensure that data is being access is
valid at read time whenever the tensor
valid at read time whenever the tensor
is imp pageable memory in other cases we
is imp pageable memory in other cases we
cannot make the same assumption when a
cannot make the same assumption when a
tensor is placed in pin memory mutating
tensor is placed in pin memory mutating
the original
the original
copy after calling the host to device
copy after calling the host to device
transfer May corrupt the data received
transfer May corrupt the data received
on GPU
when a transfer is achieved in the
when a transfer is achieved in the
opposite
opposite
direction there's no guarantee that the
direction there's no guarantee that the
data read on GPU is
data read on GPU is
valid without explicit
synchronization
what in these scenarios the transfers
what in these scenarios the transfers
offer no assurance that the copy will be
offer no assurance that the copy will be
complete at the time of data
complete at the time of data
access consequently the data on the host
access consequently the data on the host
might be incomplete or incorrect
might be incomplete or incorrect
let's demonstrate this with a pin to
let's demonstrate this with a pin to
memory so if you
do you create a pin memory
tensor you send the
tensor to
Cuda it doesn't
complete then you corrupt the original
complete then you corrupt the original
tensor
that is incredibly
stupid this thing should have a flag set
stupid this thing should have a flag set
that tells
that tells
you that it's being used or something
you that it's being used or something
that's
weird using a pageable tensor always
works all right this is
Vincent instead of writing a blog post
Vincent instead of writing a blog post
on this fix it in pie torch
on this fix it in pie torch
please it should not
happen let demonstrate that Cuda CPU
happen let demonstrate that Cuda CPU
also fails to produce reliable outputs
also fails to produce reliable outputs
without synchronization now it's so
without synchronization now it's so
weird when you have like some big like
weird when you have like some big like
python uh front-end framework you kind
python uh front-end framework you kind
of expect everything to work and not
of expect everything to work and not
fail in weird ways like
fail in weird ways like
this is the type of stuff that's allowed
this is the type of stuff that's allowed
to happen if I'm working in C right
to happen if I'm working in C right
because I expect that type of stuff to
because I expect that type of stuff to
happen and I'm not going to like like
happen and I'm not going to like like
this is the type of thing that you would
this is the type of thing that you would
get stuck on for hours because you
get stuck on for hours because you
wouldn't expect this to be possible in
wouldn't expect this to be possible in
something where you check it way earlier
something where you check it way earlier
if this were like a C++ run end or
if this were like a C++ run end or
something you just check it way earlier
something you just check it way earlier
and assume that something could be
screwy Cuda to CPU also fails to produce
screwy Cuda to CPU also fails to produce
reliable
outputs asynchronous copies to device
outputs asynchronous copies to device
are safe without explicit
are safe without explicit
synchronization only when the target is
synchronization only when the target is
a c enabled device and the original
a c enabled device and the original
tensor is imp pageable
tensor is imp pageable
memory
memory
great in general non blocking is true
great in general non blocking is true
will be good
will be good
throughout provide good
throughput if tensor is already in
throughput if tensor is already in
pinned memory transfer can be
pinned memory transfer can be
accelerated sending it to pinned memory
accelerated sending it to pinned memory
manually from python main thread is a
manually from python main thread is a
blocking operation on the host and hence
blocking operation on the host and hence
will annihilate much of the benefit of
will annihilate much of the benefit of
using nonblock
using nonblock
trip one might now legitimately ask what
trip one might now legitimately ask what
use there is for pinned pin memory
use there is for pinned pin memory
method in the following section we will
method in the following section we will
explain
how this can be used to accelerate the
how this can be used to accelerate the
data transfer even
more High torch notoriously
more High torch notoriously
notoriously provides a data loader class
notoriously provides a data loader class
whose Constructor accepts a pin memory
whose Constructor accepts a pin memory
argument considering our previous
argument considering our previous
discussion on pin memory might render
discussion on pin memory might render
how data loader manages to accelerate
how data loader manages to accelerate
data transfers memory pting is
data transfers memory pting is
inherently blocking the key lies in the
inherently blocking the key lies in the
data loaders use of separate thread to
data loaders use of separate thread to
handle the data from pageable pinned
handle the data from pageable pinned
memory thus preventing any blockage in
memory thus preventing any blockage in
the main thread to illustrate this we
the main thread to illustrate this we
will use tensor dict
will use tensor dict
primitive the homon from the
whole I didn't realize there was
whole I didn't realize there was
a adjective form of
homonym
homonym
homonymous homonymous that's very weird
homonymous homonymous that's very weird
all right I didn't know that I learned a
all right I didn't know that I learned a
new form of a word today and invoking
new form of a word today and invoking
two the default behavior is to send
two the default behavior is to send
tensors to the device
tensors to the device
asynchronously followed by a single car
asynchronously followed by a single car
call to torch device synchronize
call to torch device synchronize
afterwards I just is this actually a
afterwards I just is this actually a
word this is bothering
word this is bothering
me huh that
me huh that
is I didn't realize there was an
is I didn't realize there was an
adjective form of that that's funny uh
adjective form of that that's funny uh
invoking
invoking
two default behaviors to send tensor to
two default behaviors to send tensor to
device asynchronously followed by a
device asynchronously followed by a
single call to torch device synchronize
afterwards
afterwards
okay 10.2 includes a non-blocking pin
okay 10.2 includes a non-blocking pin
option which initiates multiple threads
option which initiates multiple threads
to
to
execute pin memory before proceeding
execute pin memory before proceeding
with thatu
Device yeah we would not do that
though oh that is kind of interesting to
though oh that is kind of interesting to
be
be
fair what are you linking
me that was funny
see the question is at which at which of
see the question is at which at which of
these layers can you do work quickly
these layers can you do work quickly
enough like what is the furthest layer
enough like what is the furthest layer
back that you can do work quickly enough
back that you can do work quickly enough
to build massive things solo efficiently
to build massive things solo efficiently
and I think the line is right about here
and I think the line is right about here
with maybe a little bit of assembly
good RL agent for
assembly
H actually to be fair it might it might
H actually to be fair it might it might
be easier than for normal code
agents non blocking pin
agents non blocking pin
okay so this is all tensor dick
okay so this is all tensor dick
stuff again this is optimal case for
stuff again this is optimal case for
tensor
dict no one siiz fit all
dict no one siiz fit all
solution need to have number of
solution need to have number of
available cores memory C
device okay
device okay
cool nice blog post so
don't Sensers without two if you can
don't Sensers without two if you can
instantiate them avoid pinning memory
instantiate them avoid pinning memory
unless you thoroughly tested well that's
unless you thoroughly tested well that's
just the don't pin and then do two avoid
just the don't pin and then do two avoid
calling tensor do item in between Cuda
calling tensor do item in between Cuda
operations this triggers Cuda
operations this triggers Cuda
synchronization and blocks your code D
synchronization and blocks your code D
logging after all code has been
logging after all code has been
completed see how to find sync points
completed see how to find sync points
here okay
here okay
cool avoid frequent calls to eval or
cool avoid frequent calls to eval or
train in eager mode we don't do
train in eager mode we don't do
that avoid calls to avoid calling args
that avoid calls to avoid calling args
do attribute often in the
code wait
what avoid calling ARS do attribute
what avoid calling ARS do attribute
often in the code especially with
Hydra what
calling ar. attribute especially with
Hydra is python config are like python
Hydra is python config are like python
config libraries so bad that that is
config libraries so bad that that is
slow that's OB that's like actually
slow that's OB that's like actually
disgusting Inplay operations are not
disgusting Inplay operations are not
preferable to regular
ones don't load your code if not
ones don't load your code if not
absolutely
necessary
interesting very
nice now that's only one of the uh the
nice now that's only one of the uh the
things here there's also
compile cographs
capturing the operations executed on a
capturing the operations executed on a
Cuda device using device buffers and
Cuda device using device buffers and
replaying the same operations
replaying the same operations
later the buffers are updated in place
later the buffers are updated in place
new results can be
new results can be
generated here's how a typical kaph
generated here's how a typical kaph
pipeline
appears
interesting with torch. ca. graph
look for calls to Tor compile and toog
look for calls to Tor compile and toog
graph module within theore torch compile
scripts
scripts
so pytorch
so pytorch
implementation then lenar
implementation then lenar
implementation then compil does nothing
implementation then compil does nothing
but then compile with cphs doubles
it did he get this to work with
lstms
lstms
aha I don't think he
aha I don't think he
did is there an lstm in
here there's not an lstm in here he
here there's not an lstm in here he
didn't get it working with
didn't get it working with
lstms that is
painful um I might be able to get the
painful um I might be able to get the
forward pass working with
lstms let's see what uh what he did
lstms let's see what uh what he did
here
soode statistics here's your
soode statistics here's your
agent it's j
torch compiler cud graph marks step
begin torch compiler cograph marks that
begin torch compiler cograph marks that
again and he doesn't Mark the end so
again and he doesn't Mark the end so
assumedly it's the bottom
it just marks that begin there's no
end Cuda graph module
end Cuda graph module
here so Mark step again roll
out Mark step again
interesting now the question I'm going
interesting now the question I'm going
to have is how much time does this add
to have is how much time does this add
to your launch
right well we will start with this
okay
and we forgot to
do Dev branch has a couple things we're
do Dev branch has a couple things we're
still experimenting
with I'm just going to get this to run
with I'm just going to get this to run
and then I got to go do a thing real
and then I got to go do a thing real
quick
quick
and then uh I will be
and then uh I will be
back in a few minutes and we will
back in a few minutes and we will
continue
continue
here okay so current version is 900k
train update
train update
EPO uh this mini batch size is too small
EPO uh this mini batch size is too small
though I mean too big
okay apparently not cool so we will
okay apparently not cool so we will
start with there that's our starting
start with there that's our starting
point and uh I will be back in a few and
point and uh I will be back in a few and
we will optimize one
sec
e
e
e
e
e
e
e
e
e
e
e
e e
okay
so first thing I want to figure out I
so first thing I want to figure out I
thought that this thing could train at
thought that this thing could train at
1.2 million though maybe that was just
1.2 million though maybe that was just
breakout I think this one got up to
breakout I think this one got up to
about a
about a
million with optimal settings let me
million with optimal settings let me
see so
see so
multipro
multipro
thing oh yeah this is a very
thing oh yeah this is a very
different environment isn't it that we
different environment isn't it that we
have here I don't see why it would be
have here I don't see why it would be
different
different
though this is 4096 the same
end we could do pong or breakout instead
end we could do pong or breakout instead
but I think I think this is probably
fine should probably be fine
okay let's just let's profile this
one we'll profile this one
this tells you all the pie torch
this tells you all the pie torch
operations
this looks like what we want for
this looks like what we want for
profiling
right e
it's all right
right
here do
with
profile oops
where did this width profile come from
where did this width profile come from
is this just torch. profile
yeah there we
go e
where's this freaking activi going come
where's this freaking activi going come
from now
so we do this
where's this Trace
where's this Trace
Handler export Chrome
Trace
okay
e e
what
let's see if we actually can get some
let's see if we actually can get some
graphs out of this thing
uh this is going to be too long
uh this is going to be too long
obviously 10
obviously 10
mil should be
good and let's see if this
profiles okay it's printing stuff but
profiles okay it's printing stuff but
it's all messed up
it's printing stuff and it's crushing my
it's printing stuff and it's crushing my
perk to the point that I don't even
perk to the point that I don't even
trust what it's profiling anymore
this thing is saying that there's 3x
this thing is saying that there's 3x
time spent in CPU versus Cuda but it's
time spent in CPU versus Cuda but it's
also crushing my perf with the profiling
also crushing my perf with the profiling
overhead so who
knows there a problem with a lot of
knows there a problem with a lot of
these profilers
we can see here
we can see here
the actual backward passes look like
the actual backward passes look like
they don't take up any time at
all all right so we have
how do we get some stats out of this
how do we get some stats out of this
thing
uh so this literally just gives you
uh so this literally just gives you
device synchronized which doesn't
help maybe we have traces though
we do have traces
cool export Chrome
cool export Chrome
trace and then you can
trace and then you can
[Music]
examine so apparently we can take these
examine so apparently we can take these
files
let's take like Trace eight give it a
let's take like Trace eight give it a
little time to warm up Trace
12 I just drag this in
here this
cool so we can actually look at
cool so we can actually look at
these I'm going to
do
this profiler step 10
how's this UI work
I do not know how this UI works
should be this thing
right okay I I actually don't know what
right okay I I actually don't know what
the hell's going on with this thing
how do you zoom on this
thing ah very intuitive alt plus Mouse
thing ah very intuitive alt plus Mouse
wheel very
intuitive but we can see the uh the
intuitive but we can see the uh the
profiles here I
think we'll make sure we get
the full
thing and these all are very
similar you just get
oh okay so
this is all idle in here for some
this is all idle in here for some
reason what the heck do we do with this
why do we have idle
why do we have idle
like big idle
blocks ah so here's the launches the
blocks ah so here's the launches the
kernel
kernel
launches so we can
launches so we can
see the actual the launches here are a
see the actual the launches here are a
big chunk of
big chunk of
perf okay
A10
multinomial I can see the multinomial
multinomial I can see the multinomial
here is
expensive this doesn't really give me
expensive this doesn't really give me
any information though on what to what
any information though on what to what
to uh pursue now does
it let me try something
it let me try something
else I had a different profile I could
try that doesn't give me much
try that doesn't give me much
information at
information at
all for
are you
are you
profiling I don't think I have torch 26
profiling I don't think I have torch 26
right
now oh no I am I have forch 26 I haven't
now oh no I am I have forch 26 I haven't
seen any difference At All by the way I
seen any difference At All by the way I
haven't seen any difference in per
okay so there we have a
profile yeah this is way lighter
weight sampling multi
discreet yeah that seems like a bug with
discreet yeah that seems like a bug with
torch or something
so we have 49% time spent in forward
13% Miss 10% Miss cure
I think what we should do here that will
I think what we should do here that will
be much more useful than
be much more useful than
this is to just expand on our
this is to just expand on our
built-in our built-in profiling
up it is nice to have the summary
well I can always just expand it for now
well I can always just expand it for now
right and then I can format this cleanly
right and then I can format this cleanly
later
if you could add that's what I'm doing
if you could add that's what I'm doing
right now
I don't know why I have this done with
I don't know why I have this done with
yields like this this is kind of insane
we'll rewrite a few things in
here so this should run immediately
here so this should run immediately
right
yeah that runs
cool let's do this
see if I'll
copy uh training should be entirely on
copy uh training should be entirely on
device but let me just run this to just
device but let me just run this to just
get an initial test
okay there we can see copy is at
okay there we can see copy is at
1% but it's not
synchronized e
man hey
man hey
welcome when was that reward clamping
welcome when was that reward clamping
added fairly recently it was needed for
added fairly recently it was needed for
breakout like breakout just didn't work
breakout like breakout just didn't work
without it so we just added it generally
without it so we just added it generally
to the dev branch
okay so that takes a little bit off of
M store is also copy isn't
it oh this is all copying
this is all
copying I think what we get to do is we
copying I think what we get to do is we
do this then we do this goes at the
bottom now we get a different story
right now we get a different story
okay and as for
train
MK sort training
data e
oh
yeah there's the
overhead so what can we do about it for
this
is yeah these indices are
weird I think before we optimize we
weird I think before we optimize we
should make sure we actually have all
should make sure we actually have all
the profile if we want
what other profile I don't I don't
what other profile I don't I don't
necessarily know we need more metrics
necessarily know we need more metrics
but I want to make sure everything's
but I want to make sure everything's
classified correctly and every Line's
classified correctly and every Line's
profiled
in place update should be
faster this is an inplace
update should be using a c to tensor to
update should be using a c to tensor to
index that's
true
for
e e
sample
Logics it's fine
and we're going to have to clean all
and we're going to have to clean all
this [ __ ] up up a whole bunch I'm sure
I think I put too much under eval copy
I think I put too much under eval copy
didn't
I hang on this is eval forward
ah hold on this is uh quite significant
ah hold on this is uh quite significant
right 28
29 oh
yeah
e e
uhoh 11k
SPS how's that happen
isn't there a thing where like indexing
isn't there a thing where like indexing
with Cuda tensor is
with Cuda tensor is
slow e
oh that's actually probably a big one
oh that's actually probably a big one
right here is this
right here is this
indexing
for e
yeah this definitely needs to be uh
yeah this definitely needs to be uh
special
special
cased so returning these
cased so returning these
indices is going to make it
slower for
slower for
sure now I would like to figure out
sure now I would like to figure out
first the indexing problem
first the indexing problem
so we we definitely have clear
so we we definitely have clear
actionable stuff that we can improve on
actionable stuff that we can improve on
this
this
today
today
definitely in fact I think this should
definitely in fact I think this should
be quite major
the Cuda tensor
right it's a
list e
and this is slow some reason this is
slow very slow
this is still slow but less
slow and then this is
fast
uhhuh well let's see where this thing
uhhuh well let's see where this thing
gets
used it gets used it does get put in
used it gets used it does get put in
store
so if I just do let's just do
so if I just do let's just do
um no
um no
Cuda n
ID
H okay so this is just going to
be we're not using e3b we're not going
be we're not using e3b we're not going
to worry about that this is going to be
to worry about that this is going to be
Cuda by
D Cuda n ID and then use it
here does that do
anything that immediately speeds us up
anything that immediately speeds us up
to over a million SPS 1.1
to over a million SPS 1.1
million so this does
something cool we are definitely
something cool we are definitely
definitely going to be able to make this
definitely going to be able to make this
this Library much
this Library much
faster I know people are starting to
faster I know people are starting to
creep up to puffer and performance a
creep up to puffer and performance a
little bit no puffer will be
fastest in fact I think that's a good
fastest in fact I think that's a good
enough result to uh to tweet to get some
enough result to uh to tweet to get some
people to join to stream to watch this
people to join to stream to watch this
stuff because this will be fun
I'm going to go grab a drink at least
I'm going to go grab a drink at least
before Bo I twe that one
sec
e e
okay I'm very happy with that but I
okay I'm very happy with that but I
don't
don't
understand what it is that screwing this
understand what it is that screwing this
up
up
elsewhere so if I put this
oops let's use P30 we're not using
oops let's use P30 we're not using
p3o if I put this in here it's slow
p3o if I put this in here it's slow
right
yeah
yeah
holy yeah that's very slow
okay Ah that's it right there it gets
okay Ah that's it right there it gets
get used for short Keys
get used for short Keys
only
only
so yeah this is taking a lot of little
so yeah this is taking a lot of little
slices of a a Cuda tensor so then this
slices of a a Cuda tensor so then this
one actually shouldn't be the Cuda
one we
one we
mask
mask
numpy values mean. CPU
okay so what is our
okay so what is our
new our new overhead after
that still at 20% copy
that still at 20% copy
overhead in
overhead in
eal not good
okay next thing I can do
here man that's going to be tricky
here man that's going to be tricky
actually think
indexing is going to be fundamentally
indexing is going to be fundamentally
slower than
slicing e
well let's at least first see how much
well let's at least first see how much
we're losing because I think we are
we're losing because I think we are
losing a lot but I we should probably
losing a lot but I we should probably
confirm that to explain what's going on
confirm that to explain what's going on
here these nids are a list and anytime
here these nids are a list and anytime
you index with a list you have to copy
you index with a list you have to copy
so there's redundant copy going on here
so there's redundant copy going on here
because of this um but I think we can
because of this um but I think we can
get around it if we are a little
clever I think so at
least yeah so just to start
with for
there's life
tenser maybe you don't need it we can
tenser maybe you don't need it we can
just
do hi there I've been doing research in
do hi there I've been doing research in
Aral for about 5 years now and I just
Aral for about 5 years now and I just
recently came across puffer lip I
recently came across puffer lip I
respect all your work on trying to
respect all your work on trying to
improve the experience with
improve the experience with
experimenting with ar thank you yeah we
experimenting with ar thank you yeah we
are going to make RL way way way faster
are going to make RL way way way faster
and easier um I think this year you're
and easier um I think this year you're
going to see it just become way easier
going to see it just become way easier
to get RL working on new problems we
to get RL working on new problems we
have a whole bunch of active open source
have a whole bunch of active open source
efforts on this right now if you're
efforts on this right now if you're
interested in getting involved join the
interested in getting involved join the
Discord we have a lot of open projects
Discord we have a lot of open projects
right
right
now research side engineering side lots
now research side engineering side lots
of cool
stuff right now I'm trying to optimize
stuff right now I'm trying to optimize
our training loop I found some uh some
our training loop I found some uh some
overhead so I think we're going to get
overhead so I think we're going to get
this to be at least a good 20% faster
this to be at least a good 20% faster
today maybe
more slice
more slice
object right
object right
so that's an
issue e
that got us
5% I was expecting more oh because of
5% I was expecting more oh because of
this
mask
e e
what are up today what are you up to
what are up today what are you up to
today I'm per optimizing puffer lip
today I'm per optimizing puffer lip
we're going to make training faster
uh there's overhead right now in the
uh there's overhead right now in the
training Loop that's caused by some of
training Loop that's caused by some of
the fancy indexing we have to
the fancy indexing we have to
do but I think we can optimize it in
do but I think we can optimize it in
many
cases I'm just playing around at the
cases I'm just playing around at the
moment to see how much overhead there
moment to see how much overhead there
actually is I've already found a good a
actually is I've already found a good a
good 20% though that we can definitely
good 20% though that we can definitely
fix
still all this copy overhead really
there should not be this much let me see
this is already on
Cuda right we're going to add a thing to
Cuda right we're going to add a thing to
profile so it's easier for me to uh to
profile so it's easier for me to uh to
see what's going on here we're going to
see what's going on here we're going to
just add
custom just add custom at the bottom
okay and then now I can actually profile
okay and then now I can actually profile
stuff as I
stuff as I
please we get 15% of time and
copy
copy
with file.
with file.
custom uh all this mess of logging
custom uh all this mess of logging
you're seeing is obviously we're not
you're seeing is obviously we're not
going to leave the code looking like
this this is just me trying to quickly
this this is just me trying to quickly
profile some
stuff okay so custom here is at
stuff okay so custom here is at
zero let me just
zero let me just
sleep just make sure that this is
sleep just make sure that this is
actually what it
actually what it
[Music]
[Music]
is that might have been too much Fleep
is that might have been too much Fleep
yeah okay so custom here has all the
yeah okay so custom here has all the
time if I do that so yes this logging is
time if I do that so yes this logging is
working uh when you add slices like we
working uh when you add slices like we
have here it's not any
have here it's not any
overhead so I need to check
overhead so I need to check
for copy eval copy
here so with
let's just do this I want to see how
let's just do this I want to see how
much overhead this
much overhead this
is profile that
custom okay so this custom is only
custom okay so this custom is only
taking uh 2%
taking uh 2%
compute so there is overhead here
compute so there is overhead here
somewhere that I do not know where it is
I it's got to be in here
right okay custom is about 7% it's about
right okay custom is about 7% it's about
half of the copy time 8% of the cop uh
half of the copy time 8% of the cop uh
half of the copy time a little more
this door function
is so where's the other half come
from so far I can't find anywhere else
from so far I can't find anywhere else
that's even
1% of the uh the total copy
here eval forward
here eval forward
right eval
copy maybe it's the fact that there's
oops maybe it's the fact that there's no
oops maybe it's the fact that there's no
synchronize on
synchronize on
here could that be it
I do
this okay so this is 2% right here
still doesn't add
up this is 2% right here
3%
3%
okay so this
okay so this
assignment this assignment actually does
assignment this assignment actually does
cost
something it's a slice
that's hard
that's hard
the issue here right is that
the issue here right is that
um if you always get the nids in
um if you always get the nids in
order then this is
order then this is
fine if you don't there's a
problem if you don't there's a problem
so we are at the point where uh fancy
so we are at the point where uh fancy
indexing is one of if not the main
indexing is one of if not the main
bottlenecks so we know now we have about
bottlenecks so we know now we have about
5% if we can figure out lftm
5% if we can figure out lftm
State Management we can get about 5%
State Management we can get about 5%
performance
performance
back um
back um
um and then with eval copy
um and then with eval copy
here this is about 8% performance in
here this is about 8% performance in
store is there anything we can do to
store is there anything we can do to
optimize store let's
see not blocking equals
true let me make sure that the thing I'm
true let me make sure that the thing I'm
planning on doing here is safe I think
planning on doing here is safe I think
it is
mutating the original
mutating the original
copy when it transfers from GPU to CPU
there's no guarantee that the data read
there's no guarantee that the data read
read on GPU is
valid
valid
so to Cuda and then this
is Cuda to
is Cuda to
CP also fails to produce reliable output
CP also fails to produce reliable output
without synchronization
okay so you actually have to synchronize
okay so you actually have to synchronize
here as well left
so I think what we do
so I think what we do
here me make sure I have all of
these actually let's make sure that uh
these actually let's make sure that uh
the thing is the bottleneck that I think
the thing is the bottleneck that I think
it is
right let me make sure that this is
actually just do profile
uh I did already it was a piece of
uh I did already it was a piece of
garbage that Trace profile [ __ ] sucks
garbage that Trace profile [ __ ] sucks
man it's so
heavy okay 5% leaking here
so I had an idea for this I think we can
so I had an idea for this I think we can
do I think we can async all these
do I think we can async all these
transfers if we're
careful do nonblocking equals true but
careful do nonblocking equals true but
then you have to be
careful do
what does torch RL
what does torch RL
guy uh he does he just batches it kind
guy uh he does he just batches it kind
of does what this is I'm doing
of does what this is I'm doing
here kind of does what I'm doing
here actions of indices so these are
here actions of indices so these are
already on the right device
why do I take them
all off of the device I don't actually
all off of the device I don't actually
know we should look at
that we're going to do this
iteratively yeah but mind you so there
iteratively yeah but mind you so there
are some tricks we can use in Lan RL the
are some tricks we can use in Lan RL the
main one is we're not compiling it all
main one is we're not compiling it all
so they are three fast three times
so they are three fast three times
faster than clean RL without compiling
faster than clean RL without compiling
we are 30 times faster than clean our
we are 30 times faster than clean our
all out from
Pine but there are still some tricks
here okay cool so now we have to do
here okay cool so now we have to do
torch. c.
torch. c.
synchronize
obs for
all right so Ops value mean
does this do
anything why does CPU not take uh
doesn't dot CPU it should
doesn't dot CPU it should
take that should take non-blocking
right yeah there is for some reason you
right yeah there is for some reason you
can't do non-blocking on CPU but you can
can't do non-blocking on CPU but you can
do it on the device
you should still have a CPU that's an
alias e
okay so that actually does help we just
okay so that actually does help we just
got back
got back
uh a couple percent from that as
think pretty small optimization but it's
think pretty small optimization but it's
that it is an
optimization back to gift making good
optimization back to gift making good
luck see you
how much are we losing from this sort oh
how much are we losing from this sort oh
yeah we're losing uh or this extend
yeah we're losing uh or this extend
we're losing a lot okay
really just from this one as one
really just from this one as one
operation
okay let's figure out this piece of code
okay let's figure out this piece of code
here
okay that's
zero okay so this is 4% right here
zero okay so this is 4% right here
right 5%
still
5% the
5% the
pend m key start plus I and
stuff
stuff
huh that's funny but yeah that's that's
huh that's funny but yeah that's that's
totally
legit Python's just really
legit Python's just really
slow um let me think how to do something
slow um let me think how to do something
about
this e
nire
maybe you can't pre-allocate the swort
maybe you can't pre-allocate the swort
casys
well you can pre-allocate storage for
well you can pre-allocate storage for
them but you don't know hang
them but you don't know hang
on it's not it's just python being crap
on it's not it's just python being crap
is all it is
along I think we can do it as a numpy
along I think we can do it as a numpy
race we can allocate the storage but I
race we can allocate the storage but I
don't it's not the storage that's the
don't it's not the storage that's the
issue it's the uh the iteration
issue it's the uh the iteration
in setting
numpy AR raise is also
numpy AR raise is also
slow let me try let me see one other
slow let me try let me see one other
thing here before I go into this too
thing here before I go into this too
much so there because there's a second
much so there because there's a second
problem
okay yeah that's
okay yeah that's
it 177% yeah we can get rid of
that that's crazy
you know this was very very efficient
you know this was very very efficient
when puff Li was training at 100,000
when puff Li was training at 100,000
steps per second but not so much when
steps per second but not so much when
you're training at a
million
holy
for e
do self. sort
keys so you collect data the order you
keys so you collect data the order you
collect data in is not predictable um
collect data in is not predictable um
because we have a synchron
because we have a synchron
simulation so this is used to tell you
simulation so this is used to tell you
uh essentially to figure out how to sort
uh essentially to figure out how to sort
all the
all the
data so that you can train on it in the
data so that you can train on it in the
correct
order oh hold on that's doesn't the way
order oh hold on that's doesn't the way
I did this doesn't make sense does it
I did this doesn't make sense does it
pointer on
end maybe it
does yeah know this is
good this
good this
works and then sort
there it goes
sort key. getet
item and actually we can do
indices e
uh oh how'd that
uh oh how'd that
happen 31's at the
bottom there we go
but there should not be multiple zeros
but there should not be multiple zeros
like that
either now there should not be multiple
either now there should not be multiple
zeros e
oh is it sorting the rows
oh is it sorting the rows
separately yes probably sorting these
separately yes probably sorting these
separately
array art
sort that's wrong
maybe this
there it
is so this is a mass
oh but this is
oh but this is
correct it is correct it's a mess but
correct it is correct it's a mess but
it's correct so we will see
we'll probably want to do this so we can
we'll probably want to do this so we can
eliminate the
eliminate the
cast but let's
at that broke terminal
okay that is
okay that is
slower that is slower
sort if I just do this let me see if
sort if I just do this let me see if
this is
faster oh
faster oh
yeah so it's all coming from this one
yeah so it's all coming from this one
sort line which is great because we just
sort line which is great because we just
have to figure out how to make this
fast for
yeah so this doesn't
work so I need to sort by multiple
work so I need to sort by multiple
columns
yeah this is slow h
no four
columns sort rows
let's try
this e
well this is freaking wrong so that
well this is freaking wrong so that
doesn't help me now does it
this is fiddly but there's 30% perf on
this is fiddly but there's 30% perf on
the table here
yeah that's bizarre for
got be a better way of doing this
right e
oh okay hold on
yeah hold on
there it is
but how is it that uh range SW
Keys is that right
Keys is that right
on seems
right okay this is one thing that we've
gotten e
yeah okay so this is
correct I'm a little confused as to how
correct I'm a little confused as to how
this
this
works
works
because I think make
because I think make
sure you sort by
H
H
sort
sort
sort sort Keys star
sort sort Keys star
one sort Keys star
two that's really weird no this
two that's really weird no this
isn't short
by sort first by surname then by name
oh so it is actually the interface to
oh so it is actually the interface to
this is just backwards okay I'm not
this is just backwards okay I'm not
crazy it's just that the interface to
crazy it's just that the interface to
this thing is backwards cool
this thing is backwards cool
um yeah that's that's totally fine
so w
1.4
mil
beautiful 1.4 million just like that
do you think Raab will be sufficient of
do you think Raab will be sufficient of
course why wouldn't it
course why wouldn't it
be Ra's
great have you seen their pie game
great have you seen their pie game
interface it's literally just um I mean
interface it's literally just um I mean
you can decide whether you want to do
you can decide whether you want to do
like a snazzy 3D one or not but this is
like a snazzy 3D one or not but this is
their P game interface oh this isn't
their P game interface oh this isn't
even their P game interface their P game
even their P game interface their P game
interface is too
interface is too
where is
it well so we're up here like 30%
already there's still a fair bit of
already there's still a fair bit of
overhead I
overhead I
seeing and we haven't even touched
seeing and we haven't even touched
compile yet
use a restro real quick I'll be back in
use a restro real quick I'll be back in
a minute and then we'll keep looking for
a minute and then we'll keep looking for
more stuff to
fix e
what happened to my uh my screen
what happened to my uh my screen
let move this on me
we're sitting at half of time in
forward it's a
lot at least a good chunk of that is
lot at least a good chunk of that is
copy overhead
though let's see
let me see if I can figure out how much
let me see if I can figure out how much
of this
of this
is and then profile.
is and then profile.
custom we want to get rid of
custom we want to get rid of
this want to
this want to
this make sure this is the same
done
okay so custom is still tiny
here okay this is 5% right here
so yeah this is 5% compute on doing
so yeah this is 5% compute on doing
nothing I think
nothing I think
6%
6%
compute I can only
assume if it's 6% compute doing
nothing CU this is the only other stuff
right oh no this
is really that's the 6% hang
on how much is right here
2% torch.
2% torch.
tensor to config DOD
device that's got to be it right there
device that's got to be it right there
right that's 2% is moving the OBS onto
right that's 2% is moving the OBS onto
the
GPU that's actually pretty
GPU that's actually pretty
reasonable that's pretty reasonable
got the you forgot the synchronized
got the you forgot the synchronized
call that's not going to work put that
call that's not going to work put that
there that there okay
about 4% right
about 4% right
now leaking just on this lstm
assignment it's
annoying it's very annoying
I mean that is a single copy
I mean that is a single copy
right like this is with uh slicing
why is that so
why is that so
much
right I mean that's not nothing that's
right I mean that's not nothing that's
like
I'm trying to think if there's anything
I'm trying to think if there's anything
I can remotely do to fix
I can remotely do to fix
that I don't think so not without
that I don't think so not without
breaking the
breaking the
uh the acing setup I have
apparently somebody reset
that breaking
anything but at least we're only leaking
anything but at least we're only leaking
4% now not
4% now not
seven okay so that is 4% of it what
seven okay so that is 4% of it what
about the rest of this
copy there's still like stuff we're not
copy there's still like stuff we're not
getting
right eval copy
oops
okay here's
okay here's
something storing is now store is now
2% so where's the rest of it coming from
I miss something here
so
11%
okay if I do this does it close the gap
[Music]
[Music]
kind of I think there's a very small
kind of I think there's a very small
optimization we can
do very small optimization we can do
do very small optimization we can do
right here
for e
we're going to clean this all up after
we're going to clean this all up after
we just need to make sure we get the
perf okay so now this way more closely
perf okay so now this way more closely
tracks
this this tens serve
mask e
there we
there we
go
okay so you profile eil
copy EV forward
you profile Evo
copy now we
have we still have
15%
overhead for
z% okay
0% it's got to come from somewhere
0% it's got to come from somewhere
doesn't
it says 4% compute
it says 4% compute
leaking and I'm being told that none of
leaking and I'm being told that none of
it's from here
can't possibly be from
this
no
no
ah this is where the computer is huh
okay how is this possibly where the
okay how is this possibly where the
computer is
that's so funny that's 3% compute saved
that's so funny that's 3% compute saved
literally just by uh replacing some with
literally just by uh replacing some with
a numpy sum
cool well we're at 1.5 million steps per
cool well we're at 1.5 million steps per
second which I think is pretty damn
second which I think is pretty damn
cool that's up uh more than 50% since we
started that's a heck of a thing to be
doing let's run this
now 0% in
now 0% in
Miss already got to 1.5 what did I
Miss already got to 1.5 what did I
miss just a lot of fiddly debugging um
miss just a lot of fiddly debugging um
main things are there's some stuff that
main things are there's some stuff that
we're doing with indexing there are some
we're doing with indexing there are some
asynchronous device transfers the really
asynchronous device transfers the really
big one was um the way that we were
big one was um the way that we were
sorting training data was slow and then
sorting training data was slow and then
I just found like a random
I just found like a random
sum increased speed here like crazy yeah
sum increased speed here like crazy yeah
I just haven't done this in in many
I just haven't done this in in many
months since like puffer was way slower
months since like puffer was way slower
and every time you get faster right like
and every time you get faster right like
new stuff ends up being the bottleneck
new stuff ends up being the bottleneck
so stuff that didn't matter to optimize
so stuff that didn't matter to optimize
before ma matters
now moving sort to nump yeah well it was
now moving sort to nump yeah well it was
just in it was a python list before
just in it was a python list before
which it doesn't matter when it's a
which it doesn't matter when it's a
100,00 things it matters when it's a
100,00 things it matters when it's a
million
things for a second
literally like uh some just to give you
literally like uh some just to give you
an idea of how much the how fast we're
an idea of how much the how fast we're
going here I just replaced uh sum of
going here I just replaced uh sum of
duns with duns do sum and I just got a
duns with duns do sum and I just got a
3% speed
3% speed
up I guess that Some Loops uh in Python
up I guess that Some Loops uh in Python
and nump Loops in
C so you know we're at the point where
C so you know we're at the point where
really small stuff matters a lot
really small stuff matters a lot
because python will um like python
because python will um like python
actually caps out at a surprisingly low
actually caps out at a surprisingly low
number of iterations per second like if
number of iterations per second like if
you write a really simple program in
you write a really simple program in
Python uh you're going to be capped at
Python uh you're going to be capped at
like low tens of millions of iterations
like low tens of millions of iterations
per second Python's just really
slow so you know tens of millions if you
slow so you know tens of millions if you
get like 10 million steps per second
get like 10 million steps per second
right if you have one of those
right if you have one of those
operations in a million step per second
operations in a million step per second
program that's 5%
overhead and we'll fix that later don't
overhead and we'll fix that later don't
worry about that one
worry about that one
um crane has a MK as
well compute J
well compute J
flatten
batch profile
do well I don't know if I want to look
do well I don't know if I want to look
at this one just yet let me think what
at this one just yet let me think what
else there is to look at
here
here
copy what do you think about tiny grad
copy what do you think about tiny grad
cool project you know very cool
cool project you know very cool
project he's right pie torch is very
project he's right pie torch is very
bloated
bloated
um they're not doing in eager mode which
um they're not doing in eager mode which
is kind of
is kind of
annoying it's like all it's like all
annoying it's like all it's like all
graph compiled so it's closer to Jax or
graph compiled so it's closer to Jax or
something uh without Jax is like God
something uh without Jax is like God
awful Syntax for the most part it seems
awful Syntax for the most part it seems
but yeah it seems fine e
probably the next thing to look at is
probably the next thing to look at is
going to be if there's anything in
going to be if there's anything in
forward that shouldn't be
forward that shouldn't be
there now forward is just
there now forward is just
forward so 52% of the time is in here
huh oh no there's more stuff in forward
huh oh no there's more stuff in forward
hold
on custom
how much do this
take
8% 8% is sampling
logics for
all right let me see if I see anything
all right let me see if I see anything
weird in here that looks like it could
weird in here that looks like it could
be made faster
g.
multinomial yeah but these list
multinomial yeah but these list
comprehension Or List comprehensions
comprehension Or List comprehensions
over like one thing or like a few
things hang on I just heard let me just
things hang on I just heard let me just
see what the hell that
was hello
just the TV
what's this line take
what's this line take
up Pro the
heck oh yeah
it's
it's
only that's
4% so about half of that came from the
multinomial
e e
when a sample index is drawn for a row
when a sample index is drawn for a row
it cannot be drawn
it cannot be drawn
again or that row light what
okay is that 1% faster no it's not it's
okay is that 1% faster no it's not it's
the
the
same I was wondering if this would be
same I was wondering if this would be
slow with uh replacement or
slow with uh replacement or
whatever out
equals for
Bing vile you hierarchy
Bing vile you hierarchy
seems well check if you actually need to
seems well check if you actually need to
do it first before you just naively Port
do it first before you just naively Port
stuff right like does the thingy do does
stuff right like does the thingy do does
the algorithm or whatever does the the
the algorithm or whatever does the the
way he's constructing it makes sense is
way he's constructing it makes sense is
there a simpler thing that you can do CU
there a simpler thing that you can do CU
everything's not on crazy GPU
land oh also you don't even have to
land oh also you don't even have to
implement that thing first right like
implement that thing first right like
you can get the whole thing working end
you can get the whole thing working end
to end just like checking every object
to end just like checking every object
right so it's quadratic and then you can
right so it's quadratic and then you can
like add optimizations from there that's
like add optimizations from there that's
probably smarter
yeah exactly so just
yeah exactly so just
like you know go like do the simple
like you know go like do the simple
thing first and I got to help you with
thing first and I got to help you with
the data structures as well but I'm
the data structures as well but I'm
doing this at the
doing this at the
moment I mean I figure if I can spend a
moment I mean I figure if I can spend a
day and get the thing to be more than
day and get the thing to be more than
50% faster that's probably going to be
50% faster that's probably going to be
pretty darn good
hang on
categorical any guesses from what I've
categorical any guesses from what I've
done before on timing to get something
done before on timing to get something
decent with your DS
help I mean whenever you need help I can
help do you have like an understanding
help do you have like an understanding
of the GPU Drive architecture and
of the GPU Drive architecture and
everything already
change
what's this multinomial
do I'm kind of
confused e
how long it'll take me to Port High
how long it'll take me to Port High
Level I think I understand
Level I think I understand
details still reading through I mean it
details still reading through I mean it
depends how like how much of the stuff
depends how like how much of the stuff
you add right like the level editor is
you add right like the level editor is
going to take a while a lot of things
going to take a while a lot of things
are going to take a while but like I
are going to take a while but like I
don't know I I think I could probably
don't know I I think I could probably
knock something like basic together in a
knock something like basic together in a
few days that kind of looks like a
few days that kind of looks like a
driving Sim
driving Sim
um it like it really depends how much of
um it like it really depends how much of
the stuff that you add that is in there
so
elements given tens are based
elements given tens are based
on yeah I I don't I remember this being
on yeah I I don't I remember this being
PR to puffer lib is like a perf
PR to puffer lib is like a perf
enhancement but I'm looking at this
enhancement but I'm looking at this
thing and it's not actually doing
thing and it's not actually doing
multiple uh like multi- discreete
multiple uh like multi- discreete
actions at the same
actions at the same
time so something's weird with this
first input being
first input being
tensor tensor to sample
from well here it's sampling
indices like it's the it's you're not
indices like it's the it's you're not
sampling from something right you have a
sampling from something right you have a
distribution and then you're picking the
distribution and then you're picking the
index that's the sample
logit to
problems ah so this
problems ah so this
uses
multinomial so we just skip this and we
multinomial so we just skip this and we
go use multinomial directly okay so then
go use multinomial directly okay so then
this is fine
logic. log
probob oh no wait hold on is
disre
e e
getting something that render the way Mo
getting something that render the way Mo
set should be relatively easy
yeah it's interesting that there's 8% in
yeah it's interesting that there's 8% in
here
there's like 8% in
here this thing just need more
ends
ends
okay yeah that uh
okay yeah that uh
H interesting
I mean I can go like
16 the training forward
pass I'm I'm trying to figure something
pass I'm I'm trying to figure something
out 32
here's what I'm trying to figure out
here's what I'm trying to figure out
forward
forward
path of the train routine is
14%
14%
okay and in my case it's
54% so what's different
stuff to go through the
network just make sure it's not like all
network just make sure it's not like all
of a sudden
exploding no actually it's lower than
before okay so there's for some reason
before okay so there's for some reason
there's this
there's this
massive this massive
Gap I'm trying to figure out what it
Gap I'm trying to figure out what it
would be
use decode
action unless the train profiler is
action unless the train profiler is
wrong
maybe the train profiler is wrong let's
maybe the train profiler is wrong let's
see about
that
no train profiler is consistent
incode decode sample is
14%
e e
maybe there is something about the bat
size because this goes in as
2048 this goes into as a
sequence this might be an lstm thing
sequence this might be an lstm thing
hang on
reading through stuff this weekend just
reading through stuff this weekend just
ping me yep pretty much
T so without the lstm here tiny little
T so without the lstm here tiny little
model trains at 3.2 million steps per
second and then look at that the uh the
second and then look at that the uh the
forward time is way
forward time is way
closer to the forward
closer to the forward
time in uh yeah the forward time is way
closer and now is there scaling here if
closer and now is there scaling here if
I do
I do
812 do we do
21% there's a little bit of scaling
21% there's a little bit of scaling
above 8192 it looks like
okay and then
32k so there's definitely something
32k so there's definitely something
going on with uh with the RNN here
51% forward
time goes up even a little more with
time goes up even a little more with
more MS
we see this right though the uh so
we see this right though the uh so
forward takes three times longer than
forward takes three times longer than
train with the
train with the
lstm without the lstm they're roughly
lstm without the lstm they're roughly
even
I'll be right back give me a minute and
I'll be right back give me a minute and
we'll figure this
out
e e
all
right I'm trying to think now what could
right I'm trying to think now what could
what could this
what could this
be
be
right just the forward
pass eval forward
now we do have this stuff in the
middle now there's no reason that this
middle now there's no reason that this
can't go higher
can't go higher
right if we do
right if we do
this and put this up
here
copy and then if I do that I can delete
copy and then if I do that I can delete
this line right which also means I can
this line right which also means I can
delete this line
delete this line
so now we don't have the synchronizes in
there not much
really let make sure I don't have stray
decorators no that is correct okay um
[Music]
that's got to be a synchronized bug
that's got to be a synchronized bug
right
yeah this a synchronized bug
yeah this a synchronized bug
holy okay so all the
holy okay so all the
time is in this
time is in this
function right
here uh also you're a [ __ ]
here uh also you're a [ __ ]
you put your lstm stuff completely
wrong but now we figured out that it
wrong but now we figured out that it
is profile so
with
custom there we go
50% half of my time is going into the
50% half of my time is going into the
policy en
policy en
code only 14% is going into it in
training is there something screwy
going it is an INT 8
but it should be in eight for both of
but it should be in eight for both of
these
right let's make sure
y
eight so that is not
it that's so weird
it's very
weird okay I have an
idea let's just profile Thea lstm
idea let's just profile Thea lstm
call for
well something double counted
well something double counted
right yeah double
counted okay
so 40%
so 40%
45% 6% in
45% 6% in
here and that's including the other Ford
here and that's including the other Ford
call so yeah all the time it's just
call so yeah all the time it's just
happening in the lstm
empty
sup doc hey how's it
sup doc hey how's it
going we are making some big games and
going we are making some big games and
optimization here
I wonder if this thing has this thing
I wonder if this thing has this thing
doesn't contain a cell does
it if we look at the
lstm
forward okay hold on look there's
forward okay hold on look there's
something here
something here
so if it is a packed sequence you do
so if it is a packed sequence you do
some stuff otherwise
do I do
train. no I don't I don't even bother
train. no I don't I don't even bother
with that stuff
stuff e
it should be freaking faster in forward
it should be freaking faster in forward
than backward mode
that doesn't fix
that doesn't fix
it same Speed without
uh without no grat on this okay
it's very weird the way that this is
so if you don't pass it it just makes
so if you don't pass it it just makes
zeros for
you e
jeez by George you're not fun to debug
jeez by George you're not fun to debug
through are
you
e
e e
I'm trying to think how we can determine
I'm trying to think how we can determine
why this is happening because this is
why this is happening because this is
definitely the biggest spot for
definitely the biggest spot for
perf at the moment is I don't know why
perf at the moment is I don't know why
the lstm is 3x slower at least three
the lstm is 3x slower at least three
times slower in inference versus
times slower in inference versus
training
hang on maybe there is some
hang on maybe there is some
preprocessing that's done that's like
preprocessing that's done that's like
really
slow uh let me
slow uh let me
see here is our lstm
right and where's the to call to this so
right and where's the to call to this so
when you call forward on
this here it is update flat
weights I don't know why that's
weights I don't know why that's
called okay not giving it a packed
called okay not giving it a packed
sequence input dim
sequence input dim
okay is
batched we gave it bashed
input this doesn't get
input this doesn't get
called is
bashed we get this this is
bashed we get this this is
fine per mute hidden
sorted
indices wait
indices wait
each permute indices with sorted
each permute indices with sorted
indices sorted indices is none unsorted
indices sorted indices is none unsorted
indices is
none
none
huh what
oh okay so this just gets
oh okay so this just gets
called by permute
called by permute
hidden where's permute
hidden yeah they are always none that's
hidden yeah they are always none that's
why I was wondering why they were there
def permute
def permute
hidden apply
permutation index
permutation index
select dim
permutation okay
I think I actually have to go to the
I think I actually have to go to the
freaking torch profiler again how
freaking torch profiler again how
obnoxious this thing is
you see if there's a simpler one since I
you see if there's a simpler one since I
really just need to time this one
really just need to time this one
function
oops
e e
this is good
try this
model inference
okay so
okay so
here we get the at 10
lstm is for inference
usually very
usually very
high which one's
weird qnn
RNN
100 so what is A10
lstm doing
2nn RNN is slower
it's
yeah you can see it here it's this I
yeah you can see it here it's this I
don't know what this function is
don't know what this function is
though native P torch
though native P torch
implementation which is a fallback
well we do have C and this is getting
well we do have C and this is getting
called
called
still this is very weird so this is
still this is very weird so this is
still getting
still getting
called here
oh hold on
misec this is the first call so this
misec this is the first call so this
takes
takes
longer and now it's at 300
longer and now it's at 300
and now it's at 300 still
and now it's at 300 still
okay
400 this one went
fast this
fast this
one that might be the last batch or
one that might be the last batch or
something this one and then
something this one and then
650 with one exception here right these
650 with one exception here right these
are at
are at
650 it's much lower here
why do I
see I
see for
was that 16
doesn't strictly choose one back
end so this is the high level
end so this is the high level
one so okay this is overhead then
is optimized for longer
sequences now see the thing is
sequences now see the thing is
it's we're still getting the same
it's we're still getting the same
time I think that there's some reshape
time I think that there's some reshape
operation or some [ __ ] happening in
operation or some [ __ ] happening in
the lstm implementation
yeah e
where do we get the
uh source
where's A10
lstm
lstm
[Music]
Vulcan
e e
State output
is this it
oh you are giving it way more data
oh you are giving it way more data
actually on the hidden State aren't
actually on the hidden State aren't
you that makes sense
you that makes sense
maybe it is 3x lower because you're
maybe it is 3x lower because you're
giving
giving
it instead of just input you're giving
it instead of just input you're giving
it input and then two different hidden
it input and then two different hidden
States
okay but then the setup cost should be
okay but then the setup cost should be
3x not 6X right is it 3x or
6X it's more than
6X it's more than
3x e
I think there is a functional ldm right
no there isn't it just made that [ __ ]
no there isn't it just made that [ __ ]
up dang it
I think how we can reduce that
overhead batch first drop
out then you input this
it's very difficult to know where
it's very difficult to know where
uh where this is coming
uh where this is coming
from no
arage be
lstm e
RNN re okay so there is a
RNN re okay so there is a
separate function here
h
e
e
e e
maybe we can compile it I don't know
commented that come
on and com these as
well
e e
see what the do with these
cographs cograph
modol
e
e e
oh e
doesn't do anything
right I don't know if it's work working
right I don't know if it's work working
correctly I think this is separate
correctly I think this is separate
though from the lstm
problem e
tring think what we can do about
tring think what we can do about
this lstm going
crazy
for e
this is
tough mean we need lstms
Gru only has one
Gru only has one
state does Gru have hold
on yeah only has one
state that would be faster I would
think
for e
I have this thing set up for lstms is
I have this thing set up for lstms is
the
problem okay here's an
idea well no even if I pass none it's
idea well no even if I pass none it's
still not going to work right
still not going to work right
now the issue
is You' think this would be made up for
is You' think this would be made up for
though what the heck does A10 lstm even
do
e e
I don't know what the overhead
I don't know what the overhead
is the problem
I'm going to think about this
I'm going to think about this
for a bit
um might want to go get some
um might want to go get some
lunch this is a solid progress we are
lunch this is a solid progress we are
already up more than 50%
so I'm going to get some food I'll be
so I'm going to get some food I'll be
back and then I think we'll probably
back and then I think we'll probably
mess with torch compile a little bit
mess with torch compile a little bit
first just to see if that evens out the
first just to see if that evens out the
numbers cuz it's like overhead anyways
numbers cuz it's like overhead anyways
so maybe maybe compile bypasses it we'll
so maybe maybe compile bypasses it we'll
see I'm going to go do
see I'm going to go do
that um and I will be back all right

Kind: captions
Language: en
okay we are
okay we are
live
live
morning uh we have some stuff to do
today so I think that we're just going
today so I think that we're just going
to optimize the heck out of training
to optimize the heck out of training
today
today
in
puff we're going to check on our
puff we're going to check on our
experiments we ran overnight first
experiments we ran overnight first
though we got to check on those
there's one thing I want to check on
there's one thing I want to check on
first which is just the uh this run
first which is just the uh this run
versus the previous
versus the previous
run I want to check on the sample
run I want to check on the sample
complexity just so that we know whether
complexity just so that we know whether
our new algorithm is performing as well
our new algorithm is performing as well
as uh as we think it should be able to
okay so this is the one that we had
okay so this is the one that we had
where it's like really really good
right this gamma Lambda and then if we
right this gamma Lambda and then if we
look here lambda's all over the place
look here lambda's all over the place
gamma's whatever because it doesn't
gamma's whatever because it doesn't
matter in this uh this version
but what we want to look at
but what we want to look at
is the
charts so here we have
why are all of these experiments running
why are all of these experiments running
so
long shouldn't be
right hang on are these
right hang on are these
all these are all at like
100k
and so very very weird
and so very very weird
there or 100
Mil these ones
we also have clearly we have 50 mil
we also have clearly we have 50 mil
solves for this one as
solves for this one as
well right this is a 50 mil solve right
here now it does look like we have a few
here now it does look like we have a few
somewhat uh faster solves over here mind
somewhat uh faster solves over here mind
you haven't finished the other sweep
you haven't finished the other sweep
yet it's weird that they're all out to
yet it's weird that they're all out to
100 Mil
100 Mil
though
though
right they shouldn't all be up to 100
Mil I don't know why they would be
what end this is breakout so this is
what end this is breakout so this is
this is our completed sweep over
this is our completed sweep over
breakout and you can see we have solves
breakout and you can see we have solves
in a minute and a half and almost solved
in a minute and a half and almost solved
and this is under two
minutes so this one is very
minutes so this one is very
good this is just PPO
good this is just PPO
J but it's weird how the total time
J but it's weird how the total time
steps is locked to 100 million up top
right like I don't know how it's this
right like I don't know how it's this
fast at 100
Mil wa a cost graph didn't
Mil wa a cost graph didn't
we yeah we do have a cost graph so
kind of want to see this one
21 212
see this one is reasonable this one is
see this one is reasonable this one is
not 100
Mil maybe this one yeah it's this one
Mil maybe this one yeah it's this one
here
but I don't know why there's so many
but I don't know why there's so many
samples all the way up
samples all the way up
here it's doing something clearly
then we
have I mean we have pretty darn good
have I mean we have pretty darn good
solves with the other algorithm as well
solves with the other algorithm as well
the new algorithm
I suppose the question is going to be
I suppose the question is going to be
[Music]
[Music]
whether the uh the new
whether the uh the new
one is just not dialed into the level of
one is just not dialed into the level of
the other
the other
experiments or if there's a
experiments or if there's a
gap we'll have to look
at I it's very close at this point
it is very
close well we'll let the sweep run
close well we'll let the sweep run
before we uh pass final judgment
before we uh pass final judgment
here so I want to do something a little
here so I want to do something a little
different today I want to just work on
different today I want to just work on
some optimization
for at least a little
bit so this is a uh clean up a faster
bit so this is a uh clean up a faster
version of clean ARL that's like six
version of clean ARL that's like six
times faster I think we're five times
times faster I think we're five times
faster than this already but there's
faster than this already but there's
some things in here that we haven't done
let's
let's
see there are a few new things
see there are a few new things
here Vincent
well this is
well this is
obvious reason for this stream series I
obvious reason for this stream series I
just stream my Dev dayto day on Puffer C
just stream my Dev dayto day on Puffer C
my goal is to fix reinforcement learning
my goal is to fix reinforcement learning
I want to make everything fast and
I want to make everything fast and
stable uh We've made a ton of progress
stable uh We've made a ton of progress
on this as well so if you just go to
on this as well so if you just go to
puffer doai you can see we have a dozen
puffer doai you can see we have a dozen
different environments here from very
different environments here from very
simple stuff like breakout to very
simple stuff like breakout to very
complex like this mini
complex like this mini
MMO and uh all of these things that
MMO and uh all of these things that
you're seeing are agents just playing
you're seeing are agents just playing
the game trained with reinforcement
the game trained with reinforcement
learning some of them being superum
so we are trying to make a really fast
so we are trying to make a really fast
environment and really good tools to
environment and really good tools to
make it easy to do reinforcement
make it easy to do reinforcement
learning very
fast so at the moment I think we're
fast so at the moment I think we're
going to be doing today some uh
going to be doing today some uh
optimization on the training side of the
optimization on the training side of the
current Library it's already very fast
current Library it's already very fast
but I want to see if there's any more
but I want to see if there's any more
performance that we can eek out of it
interesting that's
interesting that's
silly so let's see if they found
silly so let's see if they found
anything useful in lean RL
anything useful in lean RL
here let me load the chat as well so I
here let me load the chat as well so I
can deal with that if I need to
I think when I talked with Vincent I
I think when I talked with Vincent I
don't think he had I don't I think he
don't think he had I don't I think he
understood shared memory correctly
understood shared memory correctly
um let me see so I want to create a CPU
um let me see so I want to create a CPU
tensor in P content of this tensor needs
tensor in P content of this tensor needs
to be placed in
to be placed in
memory we distinguish two types of
memory we distinguish two types of
memory the RAM and the swap
memory the RAM and the swap
Bas together these make a virtual
memory okay but you should never be
memory okay but you should never be
doing that right
doing that right
you should never be swapping to dis
you should never be swapping to dis
anyways regular CPU tensor is pageable
anyways regular CPU tensor is pageable
which means it's divided in blocks
which means it's divided in blocks
called pages that can live anywhere in
called pages that can live anywhere in
the virtual memory both in Ram or on dis
the virtual memory both in Ram or on dis
but they're going to stay on
but they're going to stay on
RAM memory seems larger okay when
RAM memory seems larger okay when
program accesses a page that's not Ram
program accesses a page that's not Ram
page fult
page fult
occurs and Os brings this back into RAM
occurs and Os brings this back into RAM
okay this should this is all irrelevant
okay this should this is all irrelevant
this should never happen in RL
a pinned memory is a type of memory that
a pinned memory is a type of memory that
cannot be swapped out to disc okay good
cannot be swapped out to disc okay good
so this is just telling the not to swap
so this is just telling the not to swap
the
memory it's kind of weird to me that it
memory it's kind of weird to me that it
would swap out the memory in the first
would swap out the memory in the first
place when you still have plenty of RAM
memory is Page block the device can
memory is Page block the device can
access the memory directly in the main
access the memory directly in the main
memory the memory is pageable all pages
memory the memory is pageable all pages
will have to be brought to the main
will have to be brought to the main
memory before being sent to the
memory before being sent to the
GPU Fuda sends pageable data from CPU to
GPU Fuda sends pageable data from CPU to
GPU it must create it must first create
GPU it must create it must first create
a page lock copy of that data before
a page lock copy of that data before
making the
transfer okay so this is interesting so
transfer okay so this is interesting so
when Cuda sends pageable data from CPU
when Cuda sends pageable data from CPU
to GPU it must first create a page
to GPU it must first create a page
locked copy of that
data asynchronous versus synchronous
data asynchronous versus synchronous
when executing a copy from host device
when executing a copy from host device
tool coolkit to the Cuda toolkit offers
tool coolkit to the Cuda toolkit offers
modalities to do these operations
modalities to do these operations
synchronistically or
synchronistically or
asynchronously with respect to the host
asynchronously with respect to the host
in practice when call two PCH always
in practice when call two PCH always
makes a call to Cuda me copy
ASN if nonblocking is false Auda stream
ASN if nonblocking is false Auda stream
synchronized we called after each and
synchronized we called after each and
every Cuda M Copy Asing
okay if nonblocking is true then no
okay if nonblocking is true then no
synchronization is triggered and the
synchronization is triggered and the
main thread on the host is not
main thread on the host is not
blocked so multiple tensors can be sent
blocked so multiple tensors can be sent
to the device on mously as the thread
to the device on mously as the thread
does not have to wait for one transfer
does not have to wait for one transfer
to be completed to initiate the other so
to be completed to initiate the other so
this is
this is
important in general the transfer a the
important in general the transfer a the
transfer is blocking on the device side
transfer is blocking on the device side
even if it isn't on the host
even if it isn't on the host
side the copy on the device cannot occur
side the copy on the device cannot occur
while another operation is executed
while another operation is executed
however in some Advanced scenarios a
however in some Advanced scenarios a
copy internal execution can be done
copy internal execution can be done
simultaneously on the GPU
simultaneously on the GPU
side three requirements must be met to
side three requirements must be met to
enable this the device must have at
enable this the device must have at
least one free direct memory access
least one free direct memory access
engine modern GPU architector have more
engine modern GPU architector have more
than
than
one the transfer must be done on a
one the transfer must be done on a
separate non deault Cuda stream in
separate non deault Cuda stream in
pytorch Cuda streams can be handles
pytorch Cuda streams can be handles
using
using
stream The Source data must be pinned in
stream The Source data must be pinned in
pinned memory demonstrate this by
pinned memory demonstrate this by
running profiles in the following
running profiles in the following
scripts so okay there's some good stuff
scripts so okay there's some good stuff
here so you have a
here so you have a
stream you
stream you
have pinned and non- pinned
tensors and then we have a Cuda
tensor this is just a perf test
and
and
then oh I didn't know that these nice
then oh I didn't know that these nice
profilers were there
full we will probably be using this
full we will probably be using this
later today in profiling puffer lip
using a pinned memory tensor doesn't
using a pinned memory tensor doesn't
change the trace much both operations
change the trace much both operations
are still executed consecutively can we
are still executed consecutively can we
see this let's go to review
see this let's go to review
mode I also want to work in RL and do
mode I also want to work in RL and do
research in it myself but I don't have a
research in it myself but I don't have a
good uh good laptop with GPU for
good uh good laptop with GPU for
training only have C Core i7 suggest me
training only have C Core i7 suggest me
what I should do some of puffer's
what I should do some of puffer's
environments are so fast that uh you can
environments are so fast that uh you can
kind of do training on our environments
kind of do training on our environments
on CPU faster than the majority of
on CPU faster than the majority of
researchers can do on GPU so if you just
researchers can do on GPU so if you just
want to like get into it a little bit
want to like get into it a little bit
and train some stuff and not have it
and train some stuff and not have it
take forever uh you can probably still
take forever uh you can probably still
train a few of our environments in maybe
train a few of our environments in maybe
it takes 5 10 minutes like our stuff is
it takes 5 10 minutes like our stuff is
really fast you probably can still do
really fast you probably can still do
training at like 100,000 steps per
training at like 100,000 steps per
second for breakout or
calm for context on a modern core each
calm for context on a modern core each
of these environments runs a million
of these environments runs a million
steps per second we use the GPU to get
steps per second we use the GPU to get
300,000 to a million step per second
300,000 to a million step per second
training but for the smaller models you
training but for the smaller models you
can often still train at 100K just on a
can often still train at 100K just on a
like even on just a
core and give that a try
I also have a quick start guide on the
I also have a quick start guide on the
website that I highly highly recommend I
website that I highly highly recommend I
mean I literally wrote it for people who
mean I literally wrote it for people who
want an easy way to get into RL so let's
want an easy way to get into RL so let's
go to blog quick start guide It's like a
go to blog quick start guide It's like a
20 minute read links you to some other
resources can we see what this image
is okay cool so now we can see this
is okay cool so now we can see this
um and I actually should go back to here
um and I actually should go back to here
so you can see it
properly okay at 10
properly okay at 10
2 to copy and then we can see that this
2 to copy and then we can see that this
little Gap here this is the overhead of
little Gap here this is the overhead of
whatever the P torch binding is and this
whatever the P torch binding is and this
is the whole cud M Copy
is the whole cud M Copy
okay this is the first
okay this is the first
one
one
microc and here's the
synchronize
okay sending a pageable tensor to GPU on
okay sending a pageable tensor to GPU on
a separate stream
a separate stream
is also a blocking
operation streamed equals
operation streamed equals
true only pinned tensor copies only
true only pinned tensor copies only
pined tensor copies to GPU on a separate
pined tensor copies to GPU on a separate
stream overlap with another Cuda kernel
stream overlap with another Cuda kernel
executed on the
mainstream okay
on a separate stream though so how do
on a separate stream though so how do
you use this stream
you use this stream
thing with to torch Cuda stream s ah you
thing with to torch Cuda stream s ah you
have to manually manage these things
have to manually manage these things
that's
annoying okay but it's not that hard to
annoying okay but it's not that hard to
deal with
where did we we
go high torch perspective pin memory
go high torch perspective pin memory
High torch offers the possibility to
High torch offers the possibility to
create and send tensors to page lock
create and send tensors to page lock
memory through the pin memory
memory through the pin memory
method and Constructor arguments CPU
method and Constructor arguments CPU
tensors on a machine where food is
tensors on a machine where food is
initialized can be pinned cast to pin
initialized can be pinned cast to pin
memory though the pin through the pin me
memory though the pin through the pin me
memory method in memory is blocking on
memory method in memory is blocking on
the main thread of the host it waits for
the main thread of the host it waits for
the testor to be copied to page lock yes
the testor to be copied to page lock yes
you should never be doing pin memory in
you should never be doing pin memory in
a loop or something anyways that's dumb
a loop or something anyways that's dumb
new tensors can be created in pin memory
new tensors can be created in pin memory
with functions like zeros on and other
with functions like zeros on and other
Constructors let's check the speed of
Constructors let's check the speed of
sending pin of pinning memory and
sending pin of pinning memory and
sending tensor to
Cuda pageable tensor pin memory. two
Cuda pageable tensor pin memory. two
device yeah so this is stupid if you're
device yeah so this is stupid if you're
doing this you don't know what you're
doing this you don't know what you're
doing uh pageable tensor do2 device the
doing uh pageable tensor do2 device the
Baseline and then you save 20% uh if it
Baseline and then you save 20% uh if it
is pinned
cool so contrary just a somewhat common
cool so contrary just a somewhat common
believe calling pin memory on a pageable
believe calling pin memory on a pageable
tensor before casting the GPU should not
tensor before casting the GPU should not
bring any significant speed up and
bring any significant speed up and
should be slower okay yeah if you're
should be slower okay yeah if you're
doing this you just don't understand
doing this you just don't understand
your Hardware yo how's it going
your Hardware yo how's it going
Spencer we're going to perf optimize the
Spencer we're going to perf optimize the
crap out of puffer
today we're going to see if we can hit 2
today we're going to see if we can hit 2
million train SPS on
something I just haven't done this in
something I just haven't done this in
many months so I figure I'd take a day
many months so I figure I'd take a day
to do
to do
it non blocking is true
okay to act account accurately of the
okay to act account accurately of the
benefit of using nonblocking we'll
benefit of using nonblocking we'll
design a slightly more complex
design a slightly more complex
experiment since we want to assess how
experiment since we want to assess how
fast it is to send multiple
fast it is to send multiple
tensors to GPU with and without calling
tensors to GPU with and without calling
nonblocking
okay 2 okay so it's not magic you know
okay 2 okay so it's not magic you know
there's like 20 25% overhead sometimes
there's like 20 25% overhead sometimes
with
synchronization to get a better sense of
synchronization to get a better sense of
what is happening here oh feeling pretty
what is happening here oh feeling pretty
confident on the portability of GPU
confident on the portability of GPU
drive only got one more thing to clarify
drive only got one more thing to clarify
with how the data set gets loaded and
with how the data set gets loaded and
should be
should be
clear great I also I went to Brennan's
clear great I also I went to Brennan's
defense yesterday Brennan is the uh the
defense yesterday Brennan is the uh the
author of
author of
Madrona um we'll see if he has ideas on
Madrona um we'll see if he has ideas on
GPU drive for the madona version but I
GPU drive for the madona version but I
think regardless that our CPU version is
think regardless that our CPU version is
just going to be awesome to have um
just going to be awesome to have um
because regardless right it will make
because regardless right it will make
the project faster F even if they get it
the project faster F even if they get it
to work in madona ours will run on web
to work in madona ours will run on web
which theirs will not right ours will be
which theirs will not right ours will be
easier to use because it won't require
easier to use because it won't require
all the C++ Shenanigans and this is also
all the C++ Shenanigans and this is also
an opportunity for us to figure out
an opportunity for us to figure out
variable agent stuff in C uh which will
variable agent stuff in C uh which will
then let us make a whole new class of
then let us make a whole new class of
environments and also I think that in
environments and also I think that in
the process of doing this you're
the process of doing this you're
probably going to have to do stuff like
probably going to have to do stuff like
go look at mad drona's uh bounding
go look at mad drona's uh bounding
volume hierarchy thing which is
volume hierarchy thing which is
how they handle uh lots of entities in
how they handle uh lots of entities in
3D and that will be an amazing piece to
3D and that will be an amazing piece to
have uh for puffer in see so I think we
have uh for puffer in see so I think we
can just use some of those things in
can just use some of those things in
like portm to see and then we'll have
like portm to see and then we'll have
way way way more flexibility with
way way way more flexibility with
working with all sorts of 3D
working with all sorts of 3D
environments in the future so I think
environments in the future so I think
that you're like if this is the type of
that you're like if this is the type of
stuff you're interested in yeah this is
stuff you're interested in yeah this is
really going to be at the frontier of
really going to be at the frontier of
how do we make uh fast M's work I got to
how do we make uh fast M's work I got to
chat with Brandon actually on what he's
chat with Brandon actually on what he's
planning to do going forward cuz he's
planning to do going forward cuz he's
got some cool
tack some really exciting ways to expand
tack some really exciting ways to expand
the M oh yeah lots of
the M oh yeah lots of
them
definitely it's kind of funny I realized
definitely it's kind of funny I realized
watching brenan's defense y yesterday
watching brenan's defense y yesterday
that like the stuff he's doing is way
that like the stuff he's doing is way
way way more sophisticated than what I'm
way way more sophisticated than what I'm
doing with puffer lip right um you know
doing with puffer lip right um you know
he's written like a Cuda game engine
he's written like a Cuda game engine
basically uh
basically uh
but the stuff that we're doing is like
but the stuff that we're doing is like
so much more flexible and it's still
so much more flexible and it's still
fast for so many different domains and
fast for so many different domains and
it's so much easier to do
right so I don't know there are
right so I don't know there are
definitely things that we cannot do uh
definitely things that we cannot do uh
that he can do with like his uh with his
that he can do with like his uh with his
approach that I want to think about like
approach that I want to think about like
we've never tried software rendering for
we've never tried software rendering for
instance um I don't know how fast we
instance um I don't know how fast we
could get like pixel rendering done if
could get like pixel rendering done if
we wanted
we wanted
to though technically to be fair we
to though technically to be fair we
could take uh I think we could just use
could take uh I think we could just use
their batch renderer so we could because
their batch renderer so we could because
it like renders from it renders from a
it like renders from it renders from a
data format that's very similar to what
data format that's very similar to what
we have so Tech technically we could
we have so Tech technically we could
still write the whole Sim on CPU and
still write the whole Sim on CPU and
then just render on
then just render on
GPU I think that would still
work I don't
know you can have every trajectory in
know you can have every trajectory in
the end be a controllable agent if you
the end be a controllable agent if you
could port a map of La or something then
could port a map of La or something then
Rand r l set goals of a [ __ ] ton of cars
Rand r l set goals of a [ __ ] ton of cars
yeah no you could definitely I mean if
yeah no you could definitely I mean if
you want to make you're basically making
you want to make you're basically making
high per GTA at that
point it' be sick yeah we'd be happy to
point it' be sick yeah we'd be happy to
have that that'd be cool that would be
have that that'd be cool that would be
awesome you just have to evaluate how
awesome you just have to evaluate how
hard individual things are right and
hard individual things are right and
what Corners to cut and stuff like
that profile
memory what are we looking at
memory what are we looking at
here welcome YouTube
here welcome YouTube
folks today is going to be optimizing
folks today is going to be optimizing
the crap out of puffer label all day
the crap out of puffer label all day
we're going to try to hit two million
we're going to try to hit two million
steps per second training on some of our
steps per second training on some of our
environments that's going to be the goal
environments that's going to be the goal
I don't know if it's even possible but
I don't know if it's even possible but
we're going to try it
we're going to try it
uh I'm currently going through this blog
uh I'm currently going through this blog
post from Vincent on larl which is an
post from Vincent on larl which is an
optimized version of clean RL that's
optimized version of clean RL that's
like five or six times faster puffer Li
like five or six times faster puffer Li
is already five times faster than this
is already five times faster than this
but I don't think we're using all the
but I don't think we're using all the
same optimizations so there might still
same optimizations so there might still
be stuff in here first start with board
be stuff in here first start with board
look into Bing volume hierarchy stuff
look into Bing volume hierarchy stuff
it's necessary for a bunch of 3D
it's necessary for a bunch of 3D
collisions yeah they have stuff for
collisions yeah they have stuff for
collisions in there now I don't know if
collisions in there now I don't know if
the way that they do it is actually
the way that they do it is actually
going to be the best way to do it right
going to be the best way to do it right
there are a lot of different ways that
there are a lot of different ways that
you can do that type of stuff um so
you can do that type of stuff um so
you'll have to look at what if the way
you'll have to look at what if the way
that they have it make
sense let's start with uh let's see the
sense let's start with uh let's see the
call stack with a regular. two device
first call to device profile memory copy
first call to device profile memory copy
to device
tensors this says
so there's a whole bunch of garbage here
so there's a whole bunch of garbage here
two copy empty
strided I don't know what any of that
strided I don't know what any of that
is I still don't know if it's more
is I still don't know if it's more
efficient to try to emulate the physics
efficient to try to emulate the physics
with a good model to train an RL based
with a good model to train an RL based
model
model
wait I still don't know if it's more if
wait I still don't know if it's more if
to try to emulate the physics with a
to try to emulate the physics with a
good model to train an RL based model
good model to train an RL based model
they have really low compute fluid
they have really low compute fluid
models two minute papers talked about
models two minute papers talked about
this years ago I don't know about any of
this years ago I don't know about any of
that
stuff non-blocking
stuff non-blocking
version call to
version call to
device profile
device profile
memory and now I don't know what all
memory and now I don't know what all
these functions are though empty
these functions are though empty
strided to copy copy
like what what the heck
right better when using non-blocking
right better when using non-blocking
true as all transfers are initiated
true as all transfers are initiated
simultaneously so this is 23 Ms 31 Ms of
simultaneously so this is 23 Ms 31 Ms of
okay synergies now that we've made the
okay synergies now that we've made the
point that data transfers of tensor
point that data transfers of tensor
already in pin memory the GPU is faster
already in pin memory the GPU is faster
and that we know that do the transfers
and that we know that do the transfers
asynchronously is faster than
asynchronously is faster than
synchronously we can Benchmark the
synchronously we can Benchmark the
combination of these
approaches first let's write a couple of
approaches first let's write a couple of
new functions that we'll call pin memory
new functions that we'll call pin memory
and two device on each
and two device on each
tenser oh yeah also one thing I've been
tenser oh yeah also one thing I've been
wondering guys just for this stream does
wondering guys just for this stream does
the mic sound substantially better if I
the mic sound substantially better if I
move it
move it
closer like if I do this is it is this
closer like if I do this is it is this
substantially better
now that we've made the point the data
now that we've made the point the data
transfers okay
so I'll look into this I think I thought
so I'll look into this I think I thought
of a way to update on every simulation
of a way to update on every simulation
run just by running multi- agency
run just by running multi- agency
framework to change circuit parameters
framework to change circuit parameters
and get rid of bad Sims quickly that'd
and get rid of bad Sims quickly that'd
be cool yeah anything you can do to make
be cool yeah anything you can do to make
Sim faster is usually really helpful in
Sim faster is usually really helpful in
RL
we can Benchmark the combination of
we can Benchmark the combination of
these approaches okay so pin plus
async pin memory are more pronounced for
async pin memory are more pronounced for
somewhat large batches of large
somewhat large batches of large
tensors okay so this is a very very
tensors okay so this is a very very
optimistic benchmark
okay so it can be two or three times
okay so it can be two or three times
faster in a very optimistic case
here favorite RL Benchmark right
here favorite RL Benchmark right
now uh Benchmark all the benchmarks
now uh Benchmark all the benchmarks
suck all the benchmarks really suck at
suck all the benchmarks really suck at
the moment what we're doing is we are
the moment what we're doing is we are
just kind of using our environments in
just kind of using our environments in
puffer lib uh as they are in their
puffer lib uh as they are in their
current form so puffer lib we're not
current form so puffer lib we're not
providing RM is like a stable Benchmark
providing RM is like a stable Benchmark
right like they will keep we will keep
right like they will keep we will keep
updating them and we will keep fixing
updating them and we will keep fixing
bugs and we're not doing historical
bugs and we're not doing historical
versioning and all sorts of shenanigans
versioning and all sorts of shenanigans
the idea is that at any point in time if
the idea is that at any point in time if
you have two methods right that you want
you have two methods right that you want
to compare you just train them both on
to compare you just train them both on
our environments and you see what you
our environments and you see what you
get and the training is so fast that you
get and the training is so fast that you
can do that you know you can do that and
can do that you know you can do that and
we don't really need to have like an old
we don't really need to have like an old
historical copy
historical copy
um but yeah puffer lib RM are what we're
um but yeah puffer lib RM are what we're
using we will be doing like some uh
using we will be doing like some uh
external runs I'd say on like Atari and
external runs I'd say on like Atari and
proen and stuff but we're not going to
proen and stuff but we're not going to
do optimization like we're not going to
do optimization like we're not going to
do hyperparameter sweeps I don't think
do hyperparameter sweeps I don't think
anymore over original atar your proen
anymore over original atar your proen
it's just you're wasting
it's just you're wasting
compute it's so bloody
slow until now we've operated under the
slow until now we've operated under the
assumption that asynchronous copies from
assumption that asynchronous copies from
the CPU to the GPU are safe
what this is generally true because Cuda
what this is generally true because Cuda
automatically handles synchronization to
automatically handles synchronization to
ensure that data is being access is
ensure that data is being access is
valid at read time whenever the tensor
valid at read time whenever the tensor
is imp pageable memory in other cases we
is imp pageable memory in other cases we
cannot make the same assumption when a
cannot make the same assumption when a
tensor is placed in pin memory mutating
tensor is placed in pin memory mutating
the original
the original
copy after calling the host to device
copy after calling the host to device
transfer May corrupt the data received
transfer May corrupt the data received
on GPU
when a transfer is achieved in the
when a transfer is achieved in the
opposite
opposite
direction there's no guarantee that the
direction there's no guarantee that the
data read on GPU is
data read on GPU is
valid without explicit
synchronization
what in these scenarios the transfers
what in these scenarios the transfers
offer no assurance that the copy will be
offer no assurance that the copy will be
complete at the time of data
complete at the time of data
access consequently the data on the host
access consequently the data on the host
might be incomplete or incorrect
might be incomplete or incorrect
let's demonstrate this with a pin to
let's demonstrate this with a pin to
memory so if you
do you create a pin memory
tensor you send the
tensor to
Cuda it doesn't
complete then you corrupt the original
complete then you corrupt the original
tensor
that is incredibly
stupid this thing should have a flag set
stupid this thing should have a flag set
that tells
that tells
you that it's being used or something
you that it's being used or something
that's
weird using a pageable tensor always
works all right this is
Vincent instead of writing a blog post
Vincent instead of writing a blog post
on this fix it in pie torch
on this fix it in pie torch
please it should not
happen let demonstrate that Cuda CPU
happen let demonstrate that Cuda CPU
also fails to produce reliable outputs
also fails to produce reliable outputs
without synchronization now it's so
without synchronization now it's so
weird when you have like some big like
weird when you have like some big like
python uh front-end framework you kind
python uh front-end framework you kind
of expect everything to work and not
of expect everything to work and not
fail in weird ways like
fail in weird ways like
this is the type of stuff that's allowed
this is the type of stuff that's allowed
to happen if I'm working in C right
to happen if I'm working in C right
because I expect that type of stuff to
because I expect that type of stuff to
happen and I'm not going to like like
happen and I'm not going to like like
this is the type of thing that you would
this is the type of thing that you would
get stuck on for hours because you
get stuck on for hours because you
wouldn't expect this to be possible in
wouldn't expect this to be possible in
something where you check it way earlier
something where you check it way earlier
if this were like a C++ run end or
if this were like a C++ run end or
something you just check it way earlier
something you just check it way earlier
and assume that something could be
screwy Cuda to CPU also fails to produce
screwy Cuda to CPU also fails to produce
reliable
outputs asynchronous copies to device
outputs asynchronous copies to device
are safe without explicit
are safe without explicit
synchronization only when the target is
synchronization only when the target is
a c enabled device and the original
a c enabled device and the original
tensor is imp pageable
tensor is imp pageable
memory
memory
great in general non blocking is true
great in general non blocking is true
will be good
will be good
throughout provide good
throughput if tensor is already in
throughput if tensor is already in
pinned memory transfer can be
pinned memory transfer can be
accelerated sending it to pinned memory
accelerated sending it to pinned memory
manually from python main thread is a
manually from python main thread is a
blocking operation on the host and hence
blocking operation on the host and hence
will annihilate much of the benefit of
will annihilate much of the benefit of
using nonblock
using nonblock
trip one might now legitimately ask what
trip one might now legitimately ask what
use there is for pinned pin memory
use there is for pinned pin memory
method in the following section we will
method in the following section we will
explain
how this can be used to accelerate the
how this can be used to accelerate the
data transfer even
more High torch notoriously
more High torch notoriously
notoriously provides a data loader class
notoriously provides a data loader class
whose Constructor accepts a pin memory
whose Constructor accepts a pin memory
argument considering our previous
argument considering our previous
discussion on pin memory might render
discussion on pin memory might render
how data loader manages to accelerate
how data loader manages to accelerate
data transfers memory pting is
data transfers memory pting is
inherently blocking the key lies in the
inherently blocking the key lies in the
data loaders use of separate thread to
data loaders use of separate thread to
handle the data from pageable pinned
handle the data from pageable pinned
memory thus preventing any blockage in
memory thus preventing any blockage in
the main thread to illustrate this we
the main thread to illustrate this we
will use tensor dict
will use tensor dict
primitive the homon from the
whole I didn't realize there was
whole I didn't realize there was
a adjective form of
homonym
homonym
homonymous homonymous that's very weird
homonymous homonymous that's very weird
all right I didn't know that I learned a
all right I didn't know that I learned a
new form of a word today and invoking
new form of a word today and invoking
two the default behavior is to send
two the default behavior is to send
tensors to the device
tensors to the device
asynchronously followed by a single car
asynchronously followed by a single car
call to torch device synchronize
call to torch device synchronize
afterwards I just is this actually a
afterwards I just is this actually a
word this is bothering
word this is bothering
me huh that
me huh that
is I didn't realize there was an
is I didn't realize there was an
adjective form of that that's funny uh
adjective form of that that's funny uh
invoking
invoking
two default behaviors to send tensor to
two default behaviors to send tensor to
device asynchronously followed by a
device asynchronously followed by a
single call to torch device synchronize
afterwards
afterwards
okay 10.2 includes a non-blocking pin
okay 10.2 includes a non-blocking pin
option which initiates multiple threads
option which initiates multiple threads
to
to
execute pin memory before proceeding
execute pin memory before proceeding
with thatu
Device yeah we would not do that
though oh that is kind of interesting to
though oh that is kind of interesting to
be
be
fair what are you linking
me that was funny
see the question is at which at which of
see the question is at which at which of
these layers can you do work quickly
these layers can you do work quickly
enough like what is the furthest layer
enough like what is the furthest layer
back that you can do work quickly enough
back that you can do work quickly enough
to build massive things solo efficiently
to build massive things solo efficiently
and I think the line is right about here
and I think the line is right about here
with maybe a little bit of assembly
good RL agent for
assembly
H actually to be fair it might it might
H actually to be fair it might it might
be easier than for normal code
agents non blocking pin
agents non blocking pin
okay so this is all tensor dick
okay so this is all tensor dick
stuff again this is optimal case for
stuff again this is optimal case for
tensor
dict no one siiz fit all
dict no one siiz fit all
solution need to have number of
solution need to have number of
available cores memory C
device okay
device okay
cool nice blog post so
don't Sensers without two if you can
don't Sensers without two if you can
instantiate them avoid pinning memory
instantiate them avoid pinning memory
unless you thoroughly tested well that's
unless you thoroughly tested well that's
just the don't pin and then do two avoid
just the don't pin and then do two avoid
calling tensor do item in between Cuda
calling tensor do item in between Cuda
operations this triggers Cuda
operations this triggers Cuda
synchronization and blocks your code D
synchronization and blocks your code D
logging after all code has been
logging after all code has been
completed see how to find sync points
completed see how to find sync points
here okay
here okay
cool avoid frequent calls to eval or
cool avoid frequent calls to eval or
train in eager mode we don't do
train in eager mode we don't do
that avoid calls to avoid calling args
that avoid calls to avoid calling args
do attribute often in the
code wait
what avoid calling ARS do attribute
what avoid calling ARS do attribute
often in the code especially with
Hydra what
calling ar. attribute especially with
Hydra is python config are like python
Hydra is python config are like python
config libraries so bad that that is
config libraries so bad that that is
slow that's OB that's like actually
slow that's OB that's like actually
disgusting Inplay operations are not
disgusting Inplay operations are not
preferable to regular
ones don't load your code if not
ones don't load your code if not
absolutely
necessary
interesting very
nice now that's only one of the uh the
nice now that's only one of the uh the
things here there's also
compile cographs
capturing the operations executed on a
capturing the operations executed on a
Cuda device using device buffers and
Cuda device using device buffers and
replaying the same operations
replaying the same operations
later the buffers are updated in place
later the buffers are updated in place
new results can be
new results can be
generated here's how a typical kaph
generated here's how a typical kaph
pipeline
appears
interesting with torch. ca. graph
look for calls to Tor compile and toog
look for calls to Tor compile and toog
graph module within theore torch compile
scripts
scripts
so pytorch
so pytorch
implementation then lenar
implementation then lenar
implementation then compil does nothing
implementation then compil does nothing
but then compile with cphs doubles
it did he get this to work with
lstms
lstms
aha I don't think he
aha I don't think he
did is there an lstm in
here there's not an lstm in here he
here there's not an lstm in here he
didn't get it working with
didn't get it working with
lstms that is
painful um I might be able to get the
painful um I might be able to get the
forward pass working with
lstms let's see what uh what he did
lstms let's see what uh what he did
here
soode statistics here's your
soode statistics here's your
agent it's j
torch compiler cud graph marks step
begin torch compiler cograph marks that
begin torch compiler cograph marks that
again and he doesn't Mark the end so
again and he doesn't Mark the end so
assumedly it's the bottom
it just marks that begin there's no
end Cuda graph module
end Cuda graph module
here so Mark step again roll
out Mark step again
interesting now the question I'm going
interesting now the question I'm going
to have is how much time does this add
to have is how much time does this add
to your launch
right well we will start with this
okay
and we forgot to
do Dev branch has a couple things we're
do Dev branch has a couple things we're
still experimenting
with I'm just going to get this to run
with I'm just going to get this to run
and then I got to go do a thing real
and then I got to go do a thing real
quick
quick
and then uh I will be
and then uh I will be
back in a few minutes and we will
back in a few minutes and we will
continue
continue
here okay so current version is 900k
train update
train update
EPO uh this mini batch size is too small
EPO uh this mini batch size is too small
though I mean too big
okay apparently not cool so we will
okay apparently not cool so we will
start with there that's our starting
start with there that's our starting
point and uh I will be back in a few and
point and uh I will be back in a few and
we will optimize one
sec
e
e
e
e
e
e
e
e
e
e
e
e e
okay
so first thing I want to figure out I
so first thing I want to figure out I
thought that this thing could train at
thought that this thing could train at
1.2 million though maybe that was just
1.2 million though maybe that was just
breakout I think this one got up to
breakout I think this one got up to
about a
about a
million with optimal settings let me
million with optimal settings let me
see so
see so
multipro
multipro
thing oh yeah this is a very
thing oh yeah this is a very
different environment isn't it that we
different environment isn't it that we
have here I don't see why it would be
have here I don't see why it would be
different
different
though this is 4096 the same
end we could do pong or breakout instead
end we could do pong or breakout instead
but I think I think this is probably
fine should probably be fine
okay let's just let's profile this
one we'll profile this one
this tells you all the pie torch
this tells you all the pie torch
operations
this looks like what we want for
this looks like what we want for
profiling
right e
it's all right
right
here do
with
profile oops
where did this width profile come from
where did this width profile come from
is this just torch. profile
yeah there we
go e
where's this freaking activi going come
where's this freaking activi going come
from now
so we do this
where's this Trace
where's this Trace
Handler export Chrome
Trace
okay
e e
what
let's see if we actually can get some
let's see if we actually can get some
graphs out of this thing
uh this is going to be too long
uh this is going to be too long
obviously 10
obviously 10
mil should be
good and let's see if this
profiles okay it's printing stuff but
profiles okay it's printing stuff but
it's all messed up
it's printing stuff and it's crushing my
it's printing stuff and it's crushing my
perk to the point that I don't even
perk to the point that I don't even
trust what it's profiling anymore
this thing is saying that there's 3x
this thing is saying that there's 3x
time spent in CPU versus Cuda but it's
time spent in CPU versus Cuda but it's
also crushing my perf with the profiling
also crushing my perf with the profiling
overhead so who
knows there a problem with a lot of
knows there a problem with a lot of
these profilers
we can see here
we can see here
the actual backward passes look like
the actual backward passes look like
they don't take up any time at
all all right so we have
how do we get some stats out of this
how do we get some stats out of this
thing
uh so this literally just gives you
uh so this literally just gives you
device synchronized which doesn't
help maybe we have traces though
we do have traces
cool export Chrome
cool export Chrome
trace and then you can
trace and then you can
[Music]
examine so apparently we can take these
examine so apparently we can take these
files
let's take like Trace eight give it a
let's take like Trace eight give it a
little time to warm up Trace
12 I just drag this in
here this
cool so we can actually look at
cool so we can actually look at
these I'm going to
do
this profiler step 10
how's this UI work
I do not know how this UI works
should be this thing
right okay I I actually don't know what
right okay I I actually don't know what
the hell's going on with this thing
how do you zoom on this
thing ah very intuitive alt plus Mouse
thing ah very intuitive alt plus Mouse
wheel very
intuitive but we can see the uh the
intuitive but we can see the uh the
profiles here I
think we'll make sure we get
the full
thing and these all are very
similar you just get
oh okay so
this is all idle in here for some
this is all idle in here for some
reason what the heck do we do with this
why do we have idle
why do we have idle
like big idle
blocks ah so here's the launches the
blocks ah so here's the launches the
kernel
kernel
launches so we can
launches so we can
see the actual the launches here are a
see the actual the launches here are a
big chunk of
big chunk of
perf okay
A10
multinomial I can see the multinomial
multinomial I can see the multinomial
here is
expensive this doesn't really give me
expensive this doesn't really give me
any information though on what to what
any information though on what to what
to uh pursue now does
it let me try something
it let me try something
else I had a different profile I could
try that doesn't give me much
try that doesn't give me much
information at
information at
all for
are you
are you
profiling I don't think I have torch 26
profiling I don't think I have torch 26
right
now oh no I am I have forch 26 I haven't
now oh no I am I have forch 26 I haven't
seen any difference At All by the way I
seen any difference At All by the way I
haven't seen any difference in per
okay so there we have a
profile yeah this is way lighter
weight sampling multi
discreet yeah that seems like a bug with
discreet yeah that seems like a bug with
torch or something
so we have 49% time spent in forward
13% Miss 10% Miss cure
I think what we should do here that will
I think what we should do here that will
be much more useful than
be much more useful than
this is to just expand on our
this is to just expand on our
built-in our built-in profiling
up it is nice to have the summary
well I can always just expand it for now
well I can always just expand it for now
right and then I can format this cleanly
right and then I can format this cleanly
later
if you could add that's what I'm doing
if you could add that's what I'm doing
right now
I don't know why I have this done with
I don't know why I have this done with
yields like this this is kind of insane
we'll rewrite a few things in
here so this should run immediately
here so this should run immediately
right
yeah that runs
cool let's do this
see if I'll
copy uh training should be entirely on
copy uh training should be entirely on
device but let me just run this to just
device but let me just run this to just
get an initial test
okay there we can see copy is at
okay there we can see copy is at
1% but it's not
synchronized e
man hey
man hey
welcome when was that reward clamping
welcome when was that reward clamping
added fairly recently it was needed for
added fairly recently it was needed for
breakout like breakout just didn't work
breakout like breakout just didn't work
without it so we just added it generally
without it so we just added it generally
to the dev branch
okay so that takes a little bit off of
M store is also copy isn't
it oh this is all copying
this is all
copying I think what we get to do is we
copying I think what we get to do is we
do this then we do this goes at the
bottom now we get a different story
right now we get a different story
okay and as for
train
MK sort training
data e
oh
yeah there's the
overhead so what can we do about it for
this
is yeah these indices are
weird I think before we optimize we
weird I think before we optimize we
should make sure we actually have all
should make sure we actually have all
the profile if we want
what other profile I don't I don't
what other profile I don't I don't
necessarily know we need more metrics
necessarily know we need more metrics
but I want to make sure everything's
but I want to make sure everything's
classified correctly and every Line's
classified correctly and every Line's
profiled
in place update should be
faster this is an inplace
update should be using a c to tensor to
update should be using a c to tensor to
index that's
true
for
e e
sample
Logics it's fine
and we're going to have to clean all
and we're going to have to clean all
this [ __ ] up up a whole bunch I'm sure
I think I put too much under eval copy
I think I put too much under eval copy
didn't
I hang on this is eval forward
ah hold on this is uh quite significant
ah hold on this is uh quite significant
right 28
29 oh
yeah
e e
uhoh 11k
SPS how's that happen
isn't there a thing where like indexing
isn't there a thing where like indexing
with Cuda tensor is
with Cuda tensor is
slow e
oh that's actually probably a big one
oh that's actually probably a big one
right here is this
right here is this
indexing
for e
yeah this definitely needs to be uh
yeah this definitely needs to be uh
special
special
cased so returning these
cased so returning these
indices is going to make it
slower for
slower for
sure now I would like to figure out
sure now I would like to figure out
first the indexing problem
first the indexing problem
so we we definitely have clear
so we we definitely have clear
actionable stuff that we can improve on
actionable stuff that we can improve on
this
this
today
today
definitely in fact I think this should
definitely in fact I think this should
be quite major
the Cuda tensor
right it's a
list e
and this is slow some reason this is
slow very slow
this is still slow but less
slow and then this is
fast
uhhuh well let's see where this thing
uhhuh well let's see where this thing
gets
used it gets used it does get put in
used it gets used it does get put in
store
so if I just do let's just do
so if I just do let's just do
um no
um no
Cuda n
ID
H okay so this is just going to
be we're not using e3b we're not going
be we're not using e3b we're not going
to worry about that this is going to be
to worry about that this is going to be
Cuda by
D Cuda n ID and then use it
here does that do
anything that immediately speeds us up
anything that immediately speeds us up
to over a million SPS 1.1
to over a million SPS 1.1
million so this does
something cool we are definitely
something cool we are definitely
definitely going to be able to make this
definitely going to be able to make this
this Library much
this Library much
faster I know people are starting to
faster I know people are starting to
creep up to puffer and performance a
creep up to puffer and performance a
little bit no puffer will be
fastest in fact I think that's a good
fastest in fact I think that's a good
enough result to uh to tweet to get some
enough result to uh to tweet to get some
people to join to stream to watch this
people to join to stream to watch this
stuff because this will be fun
I'm going to go grab a drink at least
I'm going to go grab a drink at least
before Bo I twe that one
sec
e e
okay I'm very happy with that but I
okay I'm very happy with that but I
don't
don't
understand what it is that screwing this
understand what it is that screwing this
up
up
elsewhere so if I put this
oops let's use P30 we're not using
oops let's use P30 we're not using
p3o if I put this in here it's slow
p3o if I put this in here it's slow
right
yeah
yeah
holy yeah that's very slow
okay Ah that's it right there it gets
okay Ah that's it right there it gets
get used for short Keys
get used for short Keys
only
only
so yeah this is taking a lot of little
so yeah this is taking a lot of little
slices of a a Cuda tensor so then this
slices of a a Cuda tensor so then this
one actually shouldn't be the Cuda
one we
one we
mask
mask
numpy values mean. CPU
okay so what is our
okay so what is our
new our new overhead after
that still at 20% copy
that still at 20% copy
overhead in
overhead in
eal not good
okay next thing I can do
here man that's going to be tricky
here man that's going to be tricky
actually think
indexing is going to be fundamentally
indexing is going to be fundamentally
slower than
slicing e
well let's at least first see how much
well let's at least first see how much
we're losing because I think we are
we're losing because I think we are
losing a lot but I we should probably
losing a lot but I we should probably
confirm that to explain what's going on
confirm that to explain what's going on
here these nids are a list and anytime
here these nids are a list and anytime
you index with a list you have to copy
you index with a list you have to copy
so there's redundant copy going on here
so there's redundant copy going on here
because of this um but I think we can
because of this um but I think we can
get around it if we are a little
clever I think so at
least yeah so just to start
with for
there's life
tenser maybe you don't need it we can
tenser maybe you don't need it we can
just
do hi there I've been doing research in
do hi there I've been doing research in
Aral for about 5 years now and I just
Aral for about 5 years now and I just
recently came across puffer lip I
recently came across puffer lip I
respect all your work on trying to
respect all your work on trying to
improve the experience with
improve the experience with
experimenting with ar thank you yeah we
experimenting with ar thank you yeah we
are going to make RL way way way faster
are going to make RL way way way faster
and easier um I think this year you're
and easier um I think this year you're
going to see it just become way easier
going to see it just become way easier
to get RL working on new problems we
to get RL working on new problems we
have a whole bunch of active open source
have a whole bunch of active open source
efforts on this right now if you're
efforts on this right now if you're
interested in getting involved join the
interested in getting involved join the
Discord we have a lot of open projects
Discord we have a lot of open projects
right
right
now research side engineering side lots
now research side engineering side lots
of cool
stuff right now I'm trying to optimize
stuff right now I'm trying to optimize
our training loop I found some uh some
our training loop I found some uh some
overhead so I think we're going to get
overhead so I think we're going to get
this to be at least a good 20% faster
this to be at least a good 20% faster
today maybe
more slice
more slice
object right
object right
so that's an
issue e
that got us
5% I was expecting more oh because of
5% I was expecting more oh because of
this
mask
e e
what are up today what are you up to
what are up today what are you up to
today I'm per optimizing puffer lip
today I'm per optimizing puffer lip
we're going to make training faster
uh there's overhead right now in the
uh there's overhead right now in the
training Loop that's caused by some of
training Loop that's caused by some of
the fancy indexing we have to
the fancy indexing we have to
do but I think we can optimize it in
do but I think we can optimize it in
many
cases I'm just playing around at the
cases I'm just playing around at the
moment to see how much overhead there
moment to see how much overhead there
actually is I've already found a good a
actually is I've already found a good a
good 20% though that we can definitely
good 20% though that we can definitely
fix
still all this copy overhead really
there should not be this much let me see
this is already on
Cuda right we're going to add a thing to
Cuda right we're going to add a thing to
profile so it's easier for me to uh to
profile so it's easier for me to uh to
see what's going on here we're going to
see what's going on here we're going to
just add
custom just add custom at the bottom
okay and then now I can actually profile
okay and then now I can actually profile
stuff as I
stuff as I
please we get 15% of time and
copy
copy
with file.
with file.
custom uh all this mess of logging
custom uh all this mess of logging
you're seeing is obviously we're not
you're seeing is obviously we're not
going to leave the code looking like
this this is just me trying to quickly
this this is just me trying to quickly
profile some
stuff okay so custom here is at
stuff okay so custom here is at
zero let me just
zero let me just
sleep just make sure that this is
sleep just make sure that this is
actually what it
actually what it
[Music]
[Music]
is that might have been too much Fleep
is that might have been too much Fleep
yeah okay so custom here has all the
yeah okay so custom here has all the
time if I do that so yes this logging is
time if I do that so yes this logging is
working uh when you add slices like we
working uh when you add slices like we
have here it's not any
have here it's not any
overhead so I need to check
overhead so I need to check
for copy eval copy
here so with
let's just do this I want to see how
let's just do this I want to see how
much overhead this
much overhead this
is profile that
custom okay so this custom is only
custom okay so this custom is only
taking uh 2%
taking uh 2%
compute so there is overhead here
compute so there is overhead here
somewhere that I do not know where it is
I it's got to be in here
right okay custom is about 7% it's about
right okay custom is about 7% it's about
half of the copy time 8% of the cop uh
half of the copy time 8% of the cop uh
half of the copy time a little more
this door function
is so where's the other half come
from so far I can't find anywhere else
from so far I can't find anywhere else
that's even
1% of the uh the total copy
here eval forward
here eval forward
right eval
copy maybe it's the fact that there's
oops maybe it's the fact that there's no
oops maybe it's the fact that there's no
synchronize on
synchronize on
here could that be it
I do
this okay so this is 2% right here
still doesn't add
up this is 2% right here
3%
3%
okay so this
okay so this
assignment this assignment actually does
assignment this assignment actually does
cost
something it's a slice
that's hard
that's hard
the issue here right is that
the issue here right is that
um if you always get the nids in
um if you always get the nids in
order then this is
order then this is
fine if you don't there's a
problem if you don't there's a problem
so we are at the point where uh fancy
so we are at the point where uh fancy
indexing is one of if not the main
indexing is one of if not the main
bottlenecks so we know now we have about
bottlenecks so we know now we have about
5% if we can figure out lftm
5% if we can figure out lftm
State Management we can get about 5%
State Management we can get about 5%
performance
performance
back um
back um
um and then with eval copy
um and then with eval copy
here this is about 8% performance in
here this is about 8% performance in
store is there anything we can do to
store is there anything we can do to
optimize store let's
see not blocking equals
true let me make sure that the thing I'm
true let me make sure that the thing I'm
planning on doing here is safe I think
planning on doing here is safe I think
it is
mutating the original
mutating the original
copy when it transfers from GPU to CPU
there's no guarantee that the data read
there's no guarantee that the data read
read on GPU is
valid
valid
so to Cuda and then this
is Cuda to
is Cuda to
CP also fails to produce reliable output
CP also fails to produce reliable output
without synchronization
okay so you actually have to synchronize
okay so you actually have to synchronize
here as well left
so I think what we do
so I think what we do
here me make sure I have all of
these actually let's make sure that uh
these actually let's make sure that uh
the thing is the bottleneck that I think
the thing is the bottleneck that I think
it is
right let me make sure that this is
actually just do profile
uh I did already it was a piece of
uh I did already it was a piece of
garbage that Trace profile [ __ ] sucks
garbage that Trace profile [ __ ] sucks
man it's so
heavy okay 5% leaking here
so I had an idea for this I think we can
so I had an idea for this I think we can
do I think we can async all these
do I think we can async all these
transfers if we're
careful do nonblocking equals true but
careful do nonblocking equals true but
then you have to be
careful do
what does torch RL
what does torch RL
guy uh he does he just batches it kind
guy uh he does he just batches it kind
of does what this is I'm doing
of does what this is I'm doing
here kind of does what I'm doing
here actions of indices so these are
here actions of indices so these are
already on the right device
why do I take them
all off of the device I don't actually
all off of the device I don't actually
know we should look at
that we're going to do this
iteratively yeah but mind you so there
iteratively yeah but mind you so there
are some tricks we can use in Lan RL the
are some tricks we can use in Lan RL the
main one is we're not compiling it all
main one is we're not compiling it all
so they are three fast three times
so they are three fast three times
faster than clean RL without compiling
faster than clean RL without compiling
we are 30 times faster than clean our
we are 30 times faster than clean our
all out from
Pine but there are still some tricks
here okay cool so now we have to do
here okay cool so now we have to do
torch. c.
torch. c.
synchronize
obs for
all right so Ops value mean
does this do
anything why does CPU not take uh
doesn't dot CPU it should
doesn't dot CPU it should
take that should take non-blocking
right yeah there is for some reason you
right yeah there is for some reason you
can't do non-blocking on CPU but you can
can't do non-blocking on CPU but you can
do it on the device
you should still have a CPU that's an
alias e
okay so that actually does help we just
okay so that actually does help we just
got back
got back
uh a couple percent from that as
think pretty small optimization but it's
think pretty small optimization but it's
that it is an
optimization back to gift making good
optimization back to gift making good
luck see you
how much are we losing from this sort oh
how much are we losing from this sort oh
yeah we're losing uh or this extend
yeah we're losing uh or this extend
we're losing a lot okay
really just from this one as one
really just from this one as one
operation
okay let's figure out this piece of code
okay let's figure out this piece of code
here
okay that's
zero okay so this is 4% right here
zero okay so this is 4% right here
right 5%
still
5% the
5% the
pend m key start plus I and
stuff
stuff
huh that's funny but yeah that's that's
huh that's funny but yeah that's that's
totally
legit Python's just really
legit Python's just really
slow um let me think how to do something
slow um let me think how to do something
about
this e
nire
maybe you can't pre-allocate the swort
maybe you can't pre-allocate the swort
casys
well you can pre-allocate storage for
well you can pre-allocate storage for
them but you don't know hang
them but you don't know hang
on it's not it's just python being crap
on it's not it's just python being crap
is all it is
along I think we can do it as a numpy
along I think we can do it as a numpy
race we can allocate the storage but I
race we can allocate the storage but I
don't it's not the storage that's the
don't it's not the storage that's the
issue it's the uh the iteration
issue it's the uh the iteration
in setting
numpy AR raise is also
numpy AR raise is also
slow let me try let me see one other
slow let me try let me see one other
thing here before I go into this too
thing here before I go into this too
much so there because there's a second
much so there because there's a second
problem
okay yeah that's
okay yeah that's
it 177% yeah we can get rid of
that that's crazy
you know this was very very efficient
you know this was very very efficient
when puff Li was training at 100,000
when puff Li was training at 100,000
steps per second but not so much when
steps per second but not so much when
you're training at a
million
holy
for e
do self. sort
keys so you collect data the order you
keys so you collect data the order you
collect data in is not predictable um
collect data in is not predictable um
because we have a synchron
because we have a synchron
simulation so this is used to tell you
simulation so this is used to tell you
uh essentially to figure out how to sort
uh essentially to figure out how to sort
all the
all the
data so that you can train on it in the
data so that you can train on it in the
correct
order oh hold on that's doesn't the way
order oh hold on that's doesn't the way
I did this doesn't make sense does it
I did this doesn't make sense does it
pointer on
end maybe it
does yeah know this is
good this
good this
works and then sort
there it goes
sort key. getet
item and actually we can do
indices e
uh oh how'd that
uh oh how'd that
happen 31's at the
bottom there we go
but there should not be multiple zeros
but there should not be multiple zeros
like that
either now there should not be multiple
either now there should not be multiple
zeros e
oh is it sorting the rows
oh is it sorting the rows
separately yes probably sorting these
separately yes probably sorting these
separately
array art
sort that's wrong
maybe this
there it
is so this is a mass
oh but this is
oh but this is
correct it is correct it's a mess but
correct it is correct it's a mess but
it's correct so we will see
we'll probably want to do this so we can
we'll probably want to do this so we can
eliminate the
eliminate the
cast but let's
at that broke terminal
okay that is
okay that is
slower that is slower
sort if I just do this let me see if
sort if I just do this let me see if
this is
faster oh
faster oh
yeah so it's all coming from this one
yeah so it's all coming from this one
sort line which is great because we just
sort line which is great because we just
have to figure out how to make this
fast for
yeah so this doesn't
work so I need to sort by multiple
work so I need to sort by multiple
columns
yeah this is slow h
no four
columns sort rows
let's try
this e
well this is freaking wrong so that
well this is freaking wrong so that
doesn't help me now does it
this is fiddly but there's 30% perf on
this is fiddly but there's 30% perf on
the table here
yeah that's bizarre for
got be a better way of doing this
right e
oh okay hold on
yeah hold on
there it is
but how is it that uh range SW
Keys is that right
Keys is that right
on seems
right okay this is one thing that we've
gotten e
yeah okay so this is
correct I'm a little confused as to how
correct I'm a little confused as to how
this
this
works
works
because I think make
because I think make
sure you sort by
H
H
sort
sort
sort sort Keys star
sort sort Keys star
one sort Keys star
two that's really weird no this
two that's really weird no this
isn't short
by sort first by surname then by name
oh so it is actually the interface to
oh so it is actually the interface to
this is just backwards okay I'm not
this is just backwards okay I'm not
crazy it's just that the interface to
crazy it's just that the interface to
this thing is backwards cool
this thing is backwards cool
um yeah that's that's totally fine
so w
1.4
mil
beautiful 1.4 million just like that
do you think Raab will be sufficient of
do you think Raab will be sufficient of
course why wouldn't it
course why wouldn't it
be Ra's
great have you seen their pie game
great have you seen their pie game
interface it's literally just um I mean
interface it's literally just um I mean
you can decide whether you want to do
you can decide whether you want to do
like a snazzy 3D one or not but this is
like a snazzy 3D one or not but this is
their P game interface oh this isn't
their P game interface oh this isn't
even their P game interface their P game
even their P game interface their P game
interface is too
interface is too
where is
it well so we're up here like 30%
already there's still a fair bit of
already there's still a fair bit of
overhead I
overhead I
seeing and we haven't even touched
seeing and we haven't even touched
compile yet
use a restro real quick I'll be back in
use a restro real quick I'll be back in
a minute and then we'll keep looking for
a minute and then we'll keep looking for
more stuff to
fix e
what happened to my uh my screen
what happened to my uh my screen
let move this on me
we're sitting at half of time in
forward it's a
lot at least a good chunk of that is
lot at least a good chunk of that is
copy overhead
though let's see
let me see if I can figure out how much
let me see if I can figure out how much
of this
of this
is and then profile.
is and then profile.
custom we want to get rid of
custom we want to get rid of
this want to
this want to
this make sure this is the same
done
okay so custom is still tiny
here okay this is 5% right here
so yeah this is 5% compute on doing
so yeah this is 5% compute on doing
nothing I think
nothing I think
6%
6%
compute I can only
assume if it's 6% compute doing
nothing CU this is the only other stuff
right oh no this
is really that's the 6% hang
on how much is right here
2% torch.
2% torch.
tensor to config DOD
device that's got to be it right there
device that's got to be it right there
right that's 2% is moving the OBS onto
right that's 2% is moving the OBS onto
the
GPU that's actually pretty
GPU that's actually pretty
reasonable that's pretty reasonable
got the you forgot the synchronized
got the you forgot the synchronized
call that's not going to work put that
call that's not going to work put that
there that there okay
about 4% right
about 4% right
now leaking just on this lstm
assignment it's
annoying it's very annoying
I mean that is a single copy
I mean that is a single copy
right like this is with uh slicing
why is that so
why is that so
much
right I mean that's not nothing that's
right I mean that's not nothing that's
like
I'm trying to think if there's anything
I'm trying to think if there's anything
I can remotely do to fix
I can remotely do to fix
that I don't think so not without
that I don't think so not without
breaking the
breaking the
uh the acing setup I have
apparently somebody reset
that breaking
anything but at least we're only leaking
anything but at least we're only leaking
4% now not
4% now not
seven okay so that is 4% of it what
seven okay so that is 4% of it what
about the rest of this
copy there's still like stuff we're not
copy there's still like stuff we're not
getting
right eval copy
oops
okay here's
okay here's
something storing is now store is now
2% so where's the rest of it coming from
I miss something here
so
11%
okay if I do this does it close the gap
[Music]
[Music]
kind of I think there's a very small
kind of I think there's a very small
optimization we can
do very small optimization we can do
do very small optimization we can do
right here
for e
we're going to clean this all up after
we're going to clean this all up after
we just need to make sure we get the
perf okay so now this way more closely
perf okay so now this way more closely
tracks
this this tens serve
mask e
there we
there we
go
okay so you profile eil
copy EV forward
you profile Evo
copy now we
have we still have
15%
overhead for
z% okay
0% it's got to come from somewhere
0% it's got to come from somewhere
doesn't
it says 4% compute
it says 4% compute
leaking and I'm being told that none of
leaking and I'm being told that none of
it's from here
can't possibly be from
this
no
no
ah this is where the computer is huh
okay how is this possibly where the
okay how is this possibly where the
computer is
that's so funny that's 3% compute saved
that's so funny that's 3% compute saved
literally just by uh replacing some with
literally just by uh replacing some with
a numpy sum
cool well we're at 1.5 million steps per
cool well we're at 1.5 million steps per
second which I think is pretty damn
second which I think is pretty damn
cool that's up uh more than 50% since we
started that's a heck of a thing to be
doing let's run this
now 0% in
now 0% in
Miss already got to 1.5 what did I
Miss already got to 1.5 what did I
miss just a lot of fiddly debugging um
miss just a lot of fiddly debugging um
main things are there's some stuff that
main things are there's some stuff that
we're doing with indexing there are some
we're doing with indexing there are some
asynchronous device transfers the really
asynchronous device transfers the really
big one was um the way that we were
big one was um the way that we were
sorting training data was slow and then
sorting training data was slow and then
I just found like a random
I just found like a random
sum increased speed here like crazy yeah
sum increased speed here like crazy yeah
I just haven't done this in in many
I just haven't done this in in many
months since like puffer was way slower
months since like puffer was way slower
and every time you get faster right like
and every time you get faster right like
new stuff ends up being the bottleneck
new stuff ends up being the bottleneck
so stuff that didn't matter to optimize
so stuff that didn't matter to optimize
before ma matters
now moving sort to nump yeah well it was
now moving sort to nump yeah well it was
just in it was a python list before
just in it was a python list before
which it doesn't matter when it's a
which it doesn't matter when it's a
100,00 things it matters when it's a
100,00 things it matters when it's a
million
things for a second
literally like uh some just to give you
literally like uh some just to give you
an idea of how much the how fast we're
an idea of how much the how fast we're
going here I just replaced uh sum of
going here I just replaced uh sum of
duns with duns do sum and I just got a
duns with duns do sum and I just got a
3% speed
3% speed
up I guess that Some Loops uh in Python
up I guess that Some Loops uh in Python
and nump Loops in
C so you know we're at the point where
C so you know we're at the point where
really small stuff matters a lot
really small stuff matters a lot
because python will um like python
because python will um like python
actually caps out at a surprisingly low
actually caps out at a surprisingly low
number of iterations per second like if
number of iterations per second like if
you write a really simple program in
you write a really simple program in
Python uh you're going to be capped at
Python uh you're going to be capped at
like low tens of millions of iterations
like low tens of millions of iterations
per second Python's just really
slow so you know tens of millions if you
slow so you know tens of millions if you
get like 10 million steps per second
get like 10 million steps per second
right if you have one of those
right if you have one of those
operations in a million step per second
operations in a million step per second
program that's 5%
overhead and we'll fix that later don't
overhead and we'll fix that later don't
worry about that one
worry about that one
um crane has a MK as
well compute J
well compute J
flatten
batch profile
do well I don't know if I want to look
do well I don't know if I want to look
at this one just yet let me think what
at this one just yet let me think what
else there is to look at
here
here
copy what do you think about tiny grad
copy what do you think about tiny grad
cool project you know very cool
cool project you know very cool
project he's right pie torch is very
project he's right pie torch is very
bloated
bloated
um they're not doing in eager mode which
um they're not doing in eager mode which
is kind of
is kind of
annoying it's like all it's like all
annoying it's like all it's like all
graph compiled so it's closer to Jax or
graph compiled so it's closer to Jax or
something uh without Jax is like God
something uh without Jax is like God
awful Syntax for the most part it seems
awful Syntax for the most part it seems
but yeah it seems fine e
probably the next thing to look at is
probably the next thing to look at is
going to be if there's anything in
going to be if there's anything in
forward that shouldn't be
forward that shouldn't be
there now forward is just
there now forward is just
forward so 52% of the time is in here
huh oh no there's more stuff in forward
huh oh no there's more stuff in forward
hold
on custom
how much do this
take
8% 8% is sampling
logics for
all right let me see if I see anything
all right let me see if I see anything
weird in here that looks like it could
weird in here that looks like it could
be made faster
g.
multinomial yeah but these list
multinomial yeah but these list
comprehension Or List comprehensions
comprehension Or List comprehensions
over like one thing or like a few
things hang on I just heard let me just
things hang on I just heard let me just
see what the hell that
was hello
just the TV
what's this line take
what's this line take
up Pro the
heck oh yeah
it's
it's
only that's
4% so about half of that came from the
multinomial
e e
when a sample index is drawn for a row
when a sample index is drawn for a row
it cannot be drawn
it cannot be drawn
again or that row light what
okay is that 1% faster no it's not it's
okay is that 1% faster no it's not it's
the
the
same I was wondering if this would be
same I was wondering if this would be
slow with uh replacement or
slow with uh replacement or
whatever out
equals for
Bing vile you hierarchy
Bing vile you hierarchy
seems well check if you actually need to
seems well check if you actually need to
do it first before you just naively Port
do it first before you just naively Port
stuff right like does the thingy do does
stuff right like does the thingy do does
the algorithm or whatever does the the
the algorithm or whatever does the the
way he's constructing it makes sense is
way he's constructing it makes sense is
there a simpler thing that you can do CU
there a simpler thing that you can do CU
everything's not on crazy GPU
land oh also you don't even have to
land oh also you don't even have to
implement that thing first right like
implement that thing first right like
you can get the whole thing working end
you can get the whole thing working end
to end just like checking every object
to end just like checking every object
right so it's quadratic and then you can
right so it's quadratic and then you can
like add optimizations from there that's
like add optimizations from there that's
probably smarter
yeah exactly so just
yeah exactly so just
like you know go like do the simple
like you know go like do the simple
thing first and I got to help you with
thing first and I got to help you with
the data structures as well but I'm
the data structures as well but I'm
doing this at the
doing this at the
moment I mean I figure if I can spend a
moment I mean I figure if I can spend a
day and get the thing to be more than
day and get the thing to be more than
50% faster that's probably going to be
50% faster that's probably going to be
pretty darn good
hang on
categorical any guesses from what I've
categorical any guesses from what I've
done before on timing to get something
done before on timing to get something
decent with your DS
help I mean whenever you need help I can
help do you have like an understanding
help do you have like an understanding
of the GPU Drive architecture and
of the GPU Drive architecture and
everything already
change
what's this multinomial
do I'm kind of
confused e
how long it'll take me to Port High
how long it'll take me to Port High
Level I think I understand
Level I think I understand
details still reading through I mean it
details still reading through I mean it
depends how like how much of the stuff
depends how like how much of the stuff
you add right like the level editor is
you add right like the level editor is
going to take a while a lot of things
going to take a while a lot of things
are going to take a while but like I
are going to take a while but like I
don't know I I think I could probably
don't know I I think I could probably
knock something like basic together in a
knock something like basic together in a
few days that kind of looks like a
few days that kind of looks like a
driving Sim
driving Sim
um it like it really depends how much of
um it like it really depends how much of
the stuff that you add that is in there
so
elements given tens are based
elements given tens are based
on yeah I I don't I remember this being
on yeah I I don't I remember this being
PR to puffer lib is like a perf
PR to puffer lib is like a perf
enhancement but I'm looking at this
enhancement but I'm looking at this
thing and it's not actually doing
thing and it's not actually doing
multiple uh like multi- discreete
multiple uh like multi- discreete
actions at the same
actions at the same
time so something's weird with this
first input being
first input being
tensor tensor to sample
from well here it's sampling
indices like it's the it's you're not
indices like it's the it's you're not
sampling from something right you have a
sampling from something right you have a
distribution and then you're picking the
distribution and then you're picking the
index that's the sample
logit to
problems ah so this
problems ah so this
uses
multinomial so we just skip this and we
multinomial so we just skip this and we
go use multinomial directly okay so then
go use multinomial directly okay so then
this is fine
logic. log
probob oh no wait hold on is
disre
e e
getting something that render the way Mo
getting something that render the way Mo
set should be relatively easy
yeah it's interesting that there's 8% in
yeah it's interesting that there's 8% in
here
there's like 8% in
here this thing just need more
ends
ends
okay yeah that uh
okay yeah that uh
H interesting
I mean I can go like
16 the training forward
pass I'm I'm trying to figure something
pass I'm I'm trying to figure something
out 32
here's what I'm trying to figure out
here's what I'm trying to figure out
forward
forward
path of the train routine is
14%
14%
okay and in my case it's
54% so what's different
stuff to go through the
network just make sure it's not like all
network just make sure it's not like all
of a sudden
exploding no actually it's lower than
before okay so there's for some reason
before okay so there's for some reason
there's this
there's this
massive this massive
Gap I'm trying to figure out what it
Gap I'm trying to figure out what it
would be
use decode
action unless the train profiler is
action unless the train profiler is
wrong
maybe the train profiler is wrong let's
maybe the train profiler is wrong let's
see about
that
no train profiler is consistent
incode decode sample is
14%
e e
maybe there is something about the bat
size because this goes in as
2048 this goes into as a
sequence this might be an lstm thing
sequence this might be an lstm thing
hang on
reading through stuff this weekend just
reading through stuff this weekend just
ping me yep pretty much
T so without the lstm here tiny little
T so without the lstm here tiny little
model trains at 3.2 million steps per
second and then look at that the uh the
second and then look at that the uh the
forward time is way
forward time is way
closer to the forward
closer to the forward
time in uh yeah the forward time is way
closer and now is there scaling here if
closer and now is there scaling here if
I do
I do
812 do we do
21% there's a little bit of scaling
21% there's a little bit of scaling
above 8192 it looks like
okay and then
32k so there's definitely something
32k so there's definitely something
going on with uh with the RNN here
51% forward
time goes up even a little more with
time goes up even a little more with
more MS
we see this right though the uh so
we see this right though the uh so
forward takes three times longer than
forward takes three times longer than
train with the
train with the
lstm without the lstm they're roughly
lstm without the lstm they're roughly
even
I'll be right back give me a minute and
I'll be right back give me a minute and
we'll figure this
out
e e
all
right I'm trying to think now what could
right I'm trying to think now what could
what could this
what could this
be
be
right just the forward
pass eval forward
now we do have this stuff in the
middle now there's no reason that this
middle now there's no reason that this
can't go higher
can't go higher
right if we do
right if we do
this and put this up
here
copy and then if I do that I can delete
copy and then if I do that I can delete
this line right which also means I can
this line right which also means I can
delete this line
delete this line
so now we don't have the synchronizes in
there not much
really let make sure I don't have stray
decorators no that is correct okay um
[Music]
that's got to be a synchronized bug
that's got to be a synchronized bug
right
yeah this a synchronized bug
yeah this a synchronized bug
holy okay so all the
holy okay so all the
time is in this
time is in this
function right
here uh also you're a [ __ ]
here uh also you're a [ __ ]
you put your lstm stuff completely
wrong but now we figured out that it
wrong but now we figured out that it
is profile so
with
custom there we go
50% half of my time is going into the
50% half of my time is going into the
policy en
policy en
code only 14% is going into it in
training is there something screwy
going it is an INT 8
but it should be in eight for both of
but it should be in eight for both of
these
right let's make sure
y
eight so that is not
it that's so weird
it's very
weird okay I have an
idea let's just profile Thea lstm
idea let's just profile Thea lstm
call for
well something double counted
well something double counted
right yeah double
counted okay
so 40%
so 40%
45% 6% in
45% 6% in
here and that's including the other Ford
here and that's including the other Ford
call so yeah all the time it's just
call so yeah all the time it's just
happening in the lstm
empty
sup doc hey how's it
sup doc hey how's it
going we are making some big games and
going we are making some big games and
optimization here
I wonder if this thing has this thing
I wonder if this thing has this thing
doesn't contain a cell does
it if we look at the
lstm
forward okay hold on look there's
forward okay hold on look there's
something here
something here
so if it is a packed sequence you do
so if it is a packed sequence you do
some stuff otherwise
do I do
train. no I don't I don't even bother
train. no I don't I don't even bother
with that stuff
stuff e
it should be freaking faster in forward
it should be freaking faster in forward
than backward mode
that doesn't fix
that doesn't fix
it same Speed without
uh without no grat on this okay
it's very weird the way that this is
so if you don't pass it it just makes
so if you don't pass it it just makes
zeros for
you e
jeez by George you're not fun to debug
jeez by George you're not fun to debug
through are
you
e
e e
I'm trying to think how we can determine
I'm trying to think how we can determine
why this is happening because this is
why this is happening because this is
definitely the biggest spot for
definitely the biggest spot for
perf at the moment is I don't know why
perf at the moment is I don't know why
the lstm is 3x slower at least three
the lstm is 3x slower at least three
times slower in inference versus
times slower in inference versus
training
hang on maybe there is some
hang on maybe there is some
preprocessing that's done that's like
preprocessing that's done that's like
really
slow uh let me
slow uh let me
see here is our lstm
right and where's the to call to this so
right and where's the to call to this so
when you call forward on
this here it is update flat
weights I don't know why that's
weights I don't know why that's
called okay not giving it a packed
called okay not giving it a packed
sequence input dim
sequence input dim
okay is
batched we gave it bashed
input this doesn't get
input this doesn't get
called is
bashed we get this this is
bashed we get this this is
fine per mute hidden
sorted
indices wait
indices wait
each permute indices with sorted
each permute indices with sorted
indices sorted indices is none unsorted
indices sorted indices is none unsorted
indices is
none
none
huh what
oh okay so this just gets
oh okay so this just gets
called by permute
called by permute
hidden where's permute
hidden yeah they are always none that's
hidden yeah they are always none that's
why I was wondering why they were there
def permute
def permute
hidden apply
permutation index
permutation index
select dim
permutation okay
I think I actually have to go to the
I think I actually have to go to the
freaking torch profiler again how
freaking torch profiler again how
obnoxious this thing is
you see if there's a simpler one since I
you see if there's a simpler one since I
really just need to time this one
really just need to time this one
function
oops
e e
this is good
try this
model inference
okay so
okay so
here we get the at 10
lstm is for inference
usually very
usually very
high which one's
weird qnn
RNN
100 so what is A10
lstm doing
2nn RNN is slower
it's
yeah you can see it here it's this I
yeah you can see it here it's this I
don't know what this function is
don't know what this function is
though native P torch
though native P torch
implementation which is a fallback
well we do have C and this is getting
well we do have C and this is getting
called
called
still this is very weird so this is
still this is very weird so this is
still getting
still getting
called here
oh hold on
misec this is the first call so this
misec this is the first call so this
takes
takes
longer and now it's at 300
longer and now it's at 300
and now it's at 300 still
and now it's at 300 still
okay
400 this one went
fast this
fast this
one that might be the last batch or
one that might be the last batch or
something this one and then
something this one and then
650 with one exception here right these
650 with one exception here right these
are at
are at
650 it's much lower here
why do I
see I
see for
was that 16
doesn't strictly choose one back
end so this is the high level
end so this is the high level
one so okay this is overhead then
is optimized for longer
sequences now see the thing is
sequences now see the thing is
it's we're still getting the same
it's we're still getting the same
time I think that there's some reshape
time I think that there's some reshape
operation or some [ __ ] happening in
operation or some [ __ ] happening in
the lstm implementation
yeah e
where do we get the
uh source
where's A10
lstm
lstm
[Music]
Vulcan
e e
State output
is this it
oh you are giving it way more data
oh you are giving it way more data
actually on the hidden State aren't
actually on the hidden State aren't
you that makes sense
you that makes sense
maybe it is 3x lower because you're
maybe it is 3x lower because you're
giving
giving
it instead of just input you're giving
it instead of just input you're giving
it input and then two different hidden
it input and then two different hidden
States
okay but then the setup cost should be
okay but then the setup cost should be
3x not 6X right is it 3x or
6X it's more than
6X it's more than
3x e
I think there is a functional ldm right
no there isn't it just made that [ __ ]
no there isn't it just made that [ __ ]
up dang it
I think how we can reduce that
overhead batch first drop
out then you input this
it's very difficult to know where
it's very difficult to know where
uh where this is coming
uh where this is coming
from no
arage be
lstm e
RNN re okay so there is a
RNN re okay so there is a
separate function here
h
e
e
e e
maybe we can compile it I don't know
commented that come
on and com these as
well
e e
see what the do with these
cographs cograph
modol
e
e e
oh e
doesn't do anything
right I don't know if it's work working
right I don't know if it's work working
correctly I think this is separate
correctly I think this is separate
though from the lstm
problem e
tring think what we can do about
tring think what we can do about
this lstm going
crazy
for e
this is
tough mean we need lstms
Gru only has one
Gru only has one
state does Gru have hold
on yeah only has one
state that would be faster I would
think
for e
I have this thing set up for lstms is
I have this thing set up for lstms is
the
problem okay here's an
idea well no even if I pass none it's
idea well no even if I pass none it's
still not going to work right
still not going to work right
now the issue
is You' think this would be made up for
is You' think this would be made up for
though what the heck does A10 lstm even
do
e e
I don't know what the overhead
I don't know what the overhead
is the problem
I'm going to think about this
I'm going to think about this
for a bit
um might want to go get some
um might want to go get some
lunch this is a solid progress we are
lunch this is a solid progress we are
already up more than 50%
so I'm going to get some food I'll be
so I'm going to get some food I'll be
back and then I think we'll probably
back and then I think we'll probably
mess with torch compile a little bit
mess with torch compile a little bit
first just to see if that evens out the
first just to see if that evens out the
numbers cuz it's like overhead anyways
numbers cuz it's like overhead anyways
so maybe maybe compile bypasses it we'll
so maybe maybe compile bypasses it we'll
see I'm going to go do
see I'm going to go do
that um and I will be back all right
