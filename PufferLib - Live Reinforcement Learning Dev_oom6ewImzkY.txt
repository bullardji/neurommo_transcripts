Kind: captions
Language: en
Good morning,
Good morning,
folks. Back
folks. Back
live. An hour late
live. An hour late
today. I uh got dinner last night. I
today. I uh got dinner last night. I
worked on some of this stuff for a few
worked on some of this stuff for a few
hours. Did a little bit of exercise and
hours. Did a little bit of exercise and
then I just crashed and I slept
then I just crashed and I slept
for at least a good 10 hours.
Um but hey, I'm
Um but hey, I'm
back. We're going to get stuff going
back. We're going to get stuff going
here.
here.
I think that we have some fiddly bits to
I think that we have some fiddly bits to
work on today. Uh I've got to really see
work on today. Uh I've got to really see
with these experiments if we can find a
with these experiments if we can find a
repro for what's been going
repro for what's been going
on with this. And
on with this. And
then outside of that, we got the tiny
then outside of that, we got the tiny
boxes to finish setting up. I think my
boxes to finish setting up. I think my
package should be here today. Let me um
package should be here today. Let me um
let me go check on where the cables for
let me go check on where the cables for
those are.
should have been here
yesterday. Okay, now arriving today.
yesterday. Okay, now arriving today.
Previously expected yesterday. So, they
Previously expected yesterday. So, they
should be
here. Cool. So, when those get here, we
here. Cool. So, when those get here, we
will be able to do stuff. um with the
will be able to do stuff. um with the
tiny boxes. In the
tiny boxes. In the
meantime, let's look at this that from
today.
See that is Hey man, how's it
going? Just put this under the camera.
going? Just put this under the camera.
All right. So, this is the sweep. Here's
All right. So, this is the sweep. Here's
the thing that's kind of
the thing that's kind of
weird.
Um, so I
Um, so I
ran the old sweep the exact same as the
ran the old sweep the exact same as the
old code,
right? And where is it? Let me put these
right? And where is it? Let me put these
side by side so you can see how weird
side by side so you can see how weird
this is. This bug that I've been dealing
this is. This bug that I've been dealing
with here. I'm going to put it's easier
with here. I'm going to put it's easier
to see it on this score chart.
Okay. So, this is the goated
run and then this is the not so good
run and then this is the not so good
run. Uh, this is literally running the
run. Uh, this is literally running the
same code. Okay, I went and I pulled the
same code. Okay, I went and I pulled the
branch that this one on the right ran on
branch that this one on the right ran on
and I reran it. And you can see that
and I reran it. And you can see that
qualitatively it's like pretty
qualitatively it's like pretty
similar. The one on the right has a few
similar. The one on the right has a few
runs that go on a little
runs that go on a little
longer, but you even have like the like
longer, but you even have like the like
there's one run that does better than
there's one run that does better than
the others by like a decent margin.
the others by like a decent margin.
You've got very similar curve
You've got very similar curve
shapes, but there's a perf gap, right?
shapes, but there's a perf gap, right?
This one gets up to 0.9ish and then this
This one gets up to 0.9ish and then this
one gets up to n like seven or
one gets up to n like seven or
something.
So, what I was thinking we could
do figure
do figure
out where this orange one
out where this orange one
is. And I'm going to look at the optimal
is. And I'm going to look at the optimal
parameters that this thing has come up
parameters that this thing has come up
with.
Let's just filter. Save some
time. So this is
What the
heck? Laggy piece of [ __ ] interface. Let
heck? Laggy piece of [ __ ] interface. Let
me tell
you, they advertise this as being like
you, they advertise this as being like
the high perf alternative to wand. I
the high perf alternative to wand. I
swear nobody knows how to write software
swear nobody knows how to write software
anymore.
anymore.
and it is better than wi, but it's like
and it is better than wi, but it's like
it's still
it's still
not
not
acceptable. I think it's to be fair that
acceptable. I think it's to be fair that
they've been working on like a new
interface like they they haven't been
interface like they they haven't been
working on
this. It shouldn't just rot.
So we got to
find
find
obnoxious. Can can this interface start
obnoxious. Can can this interface start
working please?
like hey
like hey
captain maybe Monday I should be able to
captain maybe Monday I should be able to
work on impulse bolding without seeing
work on impulse bolding without seeing
very
very
good I'm working on getting everything
good I'm working on getting everything
for this release man but like the thing
for this release man but like the thing
is reproducing this weep has proven to
is reproducing this weep has proven to
be very very difficult
Yeah, we're having a difficulty
Yeah, we're having a difficulty
reproducing the uh the grid sweep was
reproducing the uh the grid sweep was
our original benchmark.
Hey, this UI is just screwing with me
Hey, this UI is just screwing with me
because
like, hey, this doesn't make
sense why this is not showing up.
There it
is. Now you can see the good runs. You
is. Now you can see the good runs. You
can see the difference here.
I grab the hyper prams from this run.
I grab the hyper prams from this run.
And the idea here is like this is the
And the idea here is like this is the
best run
best run
uh from what should have been the
uh from what should have been the
identical code to the
identical code to the
original. So I want to see if it's just
original. So I want to see if it's just
finding dramatically different hypers.
finding dramatically different hypers.
Uh if when I actually run and repro this
Uh if when I actually run and repro this
it's different or like what is going on
it's different or like what is going on
here?
All right. So, we have
this. Let's
grab What's going on with my machine?
grab What's going on with my machine?
Why is
Why is
it Why do I have all these cores at at
it Why do I have all these cores at at
100%?
100%?
It says I have something training
It says I have something training
locally.
freaking ghost processes. I
swear every single
time. All right.
Yeah, this is the right one. Puffer
Yeah, this is the right one. Puffer
train. Puffer
grid. I'm going to try this.
In the meantime, let's actually take a
In the meantime, let's actually take a
look at what these prams
are. Got a lower atom beta 1
I
clip gamma and lambda are
reasonable. Learning rates may be a
reasonable. Learning rates may be a
little high.
First of all, it's Blow.
See if I missed any
params. Don't believe I did.
It's very weird because the thing is
It's very weird because the thing is
like this sweep if I take these
like this sweep if I take these
parameters it repros. I have them up
parameters it repros. I have them up
here. I've done it several
here. I've done it several
times. But like you can consistently
times. But like you can consistently
reproduce this graph with these hypers.
reproduce this graph with these hypers.
Well, you can't find these hyperp with
Well, you can't find these hyperp with
either the new sweep code or the old
either the new sweep code or the old
sweep code.
Interestingly, the best run I believe
Interestingly, the best run I believe
it's right here. So, this is the best
it's right here. So, this is the best
run is only at like
run is only at like
92ish after 200 million
steps. That's only 2% off of
steps. That's only 2% off of
of this run
here. Like it's actually pretty close.
here. Like it's actually pretty close.
So I wonder if it's straight up
like if I make this thing run for just
like if I make this thing run for just
longer,
longer,
right? If I made it do
400, I might try that next.
Because yeah, 92 is the best that we
Because yeah, 92 is the best that we
get.
The only thing I can think of is like
The only thing I can think of is like
some sort of timing difference, right?
cuz basically like none of these runs
cuz basically like none of these runs
have done I haven't been able to get it
have done I haven't been able to get it
to do as long of runs in a while if that
to do as long of runs in a while if that
makes sense.
Oh, and these prams are not even stable.
Trying to think like what would cause
was this level of discrepancy,
right? Like a reporting interval thing.
The thing the the other thing though
The thing the the other thing though
that's weird is like the sweeps take
that's weird is like the sweeps take
about the same amount of time.
Wait, if the sweeps take about the same
Wait, if the sweeps take about the same
amount of
amount of
time, this one's still using fewer time
time, this one's still using fewer time
steps, isn't
steps, isn't
it? I didn't think of that. This one
it? I didn't think of that. This one
should be taking longer, shouldn't it?
This one should be taking longer.
This is 22
hours. We can ignore the last like 20 or
hours. We can ignore the last like 20 or
so points.
Okay, if you cut out the last points,
Okay, if you cut out the last points,
then it is longer
then it is longer
actually cuz these are just like the
actually cuz these are just like the
final. This one's 20 more runs than this
final. This one's 20 more runs than this
one. 22 more
one. 22 more
runs. The times do match
runs. The times do match
up. I was wondering if it's like a
up. I was wondering if it's like a
timing based thing, right? We're
timing based thing, right? We're
like hyperparam suggestions are in part
like hyperparam suggestions are in part
based on how long runs
take I mean we can see the total time
take I mean we can see the total time
step discrepancy here
right like we get some runs up at 600
right like we get some runs up at 600
mil and that just doesn't happen over
here. I mean, I've checked this
here. I mean, I've checked this
particular thing a million times,
particular thing a million times,
though.
though.
Like, it's pretty easy to just go to one
Like, it's pretty easy to just go to one
of these runs
This is 500 or 50 mil to 600 mil 100 mil
This is 500 or 50 mil to 600 mil 100 mil
mean with an auto scale log
normal is exactly how I have it now.
Yeah. So, this doesn't do I mean, this
Yeah. So, this doesn't do I mean, this
actually got unlucky. I think it crashed
actually got unlucky. I think it crashed
even a little bit
worse. That's real
bad. What am I not seeing here? Like,
bad. What am I not seeing here? Like,
what am I not seeing?
There's got to be Something.
Dude, there's nobody reporting
like stuff, right?
I thought that I'd um I checked this
I thought that I'd um I checked this
though. Can I have a run where I did
that? I did.
I had a run.
Maybe I should just do that just to be
Maybe I should just do that just to be
absolutely sure.
Maybe I should just set up a
Maybe I should just set up a
run, a longer run, even match like the
run, a longer run, even match like the
torch version or
torch version or
whatever. The thing is I actually I
whatever. The thing is I actually I
don't remember what torch version this
don't remember what torch version this
was
was
on. Like wouldn't this have probably
on. Like wouldn't this have probably
already have been on
128? Yeah, it would have been on 128.
This is much more similar.
I mean this is interesting
here. This is fixed mini batch.
I mean, this tells me it is like pretty
I mean, this tells me it is like pretty
insistent on this being the best it can
insistent on this being the best it can
do, even though I
know. Hang on.
100 almost all these are on uh
100 almost all these are on uh
h low mini batch
h low mini batch
size. Okay. So we have this this
plot and like it doesn't look like it's
plot and like it doesn't look like it's
particularly difficult for this thing to
particularly difficult for this thing to
get
above above 0.8
either. Yeah, we actually we have 93%ish
in 200 mil.
in 200 mil.
So what params?
Where are my runs?
It's doing some like jank shenanigans,
It's doing some like jank shenanigans,
but this interface kind of sucks.
the learning
rate. Yeah, we've got the right like
rate. Yeah, we've got the right like
learning
rateish 996.
a gamma.
learning a little low,
but should be able to get 0.9 with any
but should be able to get 0.9 with any
of
of
these lambdas. Lambda's really tolerant,
these lambdas. Lambda's really tolerant,
right?
value function
coefficient two.
There should be a little bit of
jank.
This if I check our value function
This if I check our value function
coefficients on this
Oh yeah. I mean it's it's exploring the
Oh yeah. I mean it's it's exploring the
higher ones for
sure. Radiant
norm. That was good.
entropy in a reasonable
entropy in a reasonable
range. Horizon.
compare the distributions of these
compare the distributions of these
directly.
Now, shift it a little bit, but we've
Now, shift it a little bit, but we've
got
got
overlap. I'm just looking for
overlap. I'm just looking for
overlap. Okay, gamma is not going high
overlap. Okay, gamma is not going high
enough
enough
somehow. That is interesting. Gamma is
somehow. That is interesting. Gamma is
not going high enough.
Need to rerun this with um fixed
Need to rerun this with um fixed
vectorzation param.
The rest of these look
fine. That's a bit high.
better. Okay. Well,
better. Okay. Well,
so the one piece of
so the one piece of
information I can potentially get from
information I can potentially get from
this
Oh no, we have ve num ms equal
8. This is fine then.
Yeah, this is
fine. The only discrepancy I see is it
fine. The only discrepancy I see is it
just not exploring high enough gamma.
Does this thing just get the high gamma
immediately? I'm going to have to run
immediately? I'm going to have to run
for uh for breakfast in a minute. Then
for uh for breakfast in a minute. Then
I'll be back doing more experiment
I'll be back doing more experiment
analysis, more environment work,
analysis, more environment work,
probably doing some server stuff in the
probably doing some server stuff in the
background, lots of things. But since we
background, lots of things. But since we
have a fair few folks on YouTube, uh
have a fair few folks on YouTube, uh
before I do
before I do
that, all the stuff's open source at
that, all the stuff's open source at
puffer.ai, you can help me for free just
puffer.ai, you can help me for free just
by starting the repo. And you can get
by starting the repo. And you can get
involved with development today just by
involved with development today just by
joining the uh the
joining the uh the
Discord. All right, I'll do my last bits
Discord. All right, I'll do my last bits
of analysis here.
Um, I want to see gamma as a function
Um, I want to see gamma as a function
of how many experiments you
run. Okay. So, like
run. Okay. So, like
gamma. Oh, that's bizarre.
It just snaps to max
gamma. If I cut this
out, is it that the intermediate runs
So there are a whole bunch of runs not
So there are a whole bunch of runs not
at the optimal batch size that push this
at the optimal batch size that push this
gamma
gamma
up to the
top. Huh.
Okay. And let's just be sure I didn't
Okay. And let's just be sure I didn't
miss them here.
auto scale.
Right. This is the same
What about in our um our original
run? This have a gamma problem. No, this
run? This have a gamma problem. No, this
doesn't have a gamma problem.
doesn't have a gamma problem.
This still doesn't repro
though. Another good comparison to
do, but this one will wait for after
do, but this one will wait for after
breakfast. So I'll check I'll check the
breakfast. So I'll check I'll check the
distributions of uh swept parameters
distributions of uh swept parameters
after breakfast for
after breakfast for
um the intended reproduction run and
um the intended reproduction run and
then this run to see directly if there's
then this run to see directly if there's
anything between those two. All right.
anything between those two. All right.
Well, thanks for tuning in folks. I will
Well, thanks for tuning in folks. I will
be back in 30 40 minutes.

Kind: captions
Language: en
Good morning,
Good morning,
folks. Back
folks. Back
live. An hour late
live. An hour late
today. I uh got dinner last night. I
today. I uh got dinner last night. I
worked on some of this stuff for a few
worked on some of this stuff for a few
hours. Did a little bit of exercise and
hours. Did a little bit of exercise and
then I just crashed and I slept
then I just crashed and I slept
for at least a good 10 hours.
Um but hey, I'm
Um but hey, I'm
back. We're going to get stuff going
back. We're going to get stuff going
here.
here.
I think that we have some fiddly bits to
I think that we have some fiddly bits to
work on today. Uh I've got to really see
work on today. Uh I've got to really see
with these experiments if we can find a
with these experiments if we can find a
repro for what's been going
repro for what's been going
on with this. And
on with this. And
then outside of that, we got the tiny
then outside of that, we got the tiny
boxes to finish setting up. I think my
boxes to finish setting up. I think my
package should be here today. Let me um
package should be here today. Let me um
let me go check on where the cables for
let me go check on where the cables for
those are.
should have been here
yesterday. Okay, now arriving today.
yesterday. Okay, now arriving today.
Previously expected yesterday. So, they
Previously expected yesterday. So, they
should be
here. Cool. So, when those get here, we
here. Cool. So, when those get here, we
will be able to do stuff. um with the
will be able to do stuff. um with the
tiny boxes. In the
tiny boxes. In the
meantime, let's look at this that from
today.
See that is Hey man, how's it
going? Just put this under the camera.
going? Just put this under the camera.
All right. So, this is the sweep. Here's
All right. So, this is the sweep. Here's
the thing that's kind of
the thing that's kind of
weird.
Um, so I
Um, so I
ran the old sweep the exact same as the
ran the old sweep the exact same as the
old code,
right? And where is it? Let me put these
right? And where is it? Let me put these
side by side so you can see how weird
side by side so you can see how weird
this is. This bug that I've been dealing
this is. This bug that I've been dealing
with here. I'm going to put it's easier
with here. I'm going to put it's easier
to see it on this score chart.
Okay. So, this is the goated
run and then this is the not so good
run and then this is the not so good
run. Uh, this is literally running the
run. Uh, this is literally running the
same code. Okay, I went and I pulled the
same code. Okay, I went and I pulled the
branch that this one on the right ran on
branch that this one on the right ran on
and I reran it. And you can see that
and I reran it. And you can see that
qualitatively it's like pretty
qualitatively it's like pretty
similar. The one on the right has a few
similar. The one on the right has a few
runs that go on a little
runs that go on a little
longer, but you even have like the like
longer, but you even have like the like
there's one run that does better than
there's one run that does better than
the others by like a decent margin.
the others by like a decent margin.
You've got very similar curve
You've got very similar curve
shapes, but there's a perf gap, right?
shapes, but there's a perf gap, right?
This one gets up to 0.9ish and then this
This one gets up to 0.9ish and then this
one gets up to n like seven or
one gets up to n like seven or
something.
So, what I was thinking we could
do figure
do figure
out where this orange one
out where this orange one
is. And I'm going to look at the optimal
is. And I'm going to look at the optimal
parameters that this thing has come up
parameters that this thing has come up
with.
Let's just filter. Save some
time. So this is
What the
heck? Laggy piece of [ __ ] interface. Let
heck? Laggy piece of [ __ ] interface. Let
me tell
you, they advertise this as being like
you, they advertise this as being like
the high perf alternative to wand. I
the high perf alternative to wand. I
swear nobody knows how to write software
swear nobody knows how to write software
anymore.
anymore.
and it is better than wi, but it's like
and it is better than wi, but it's like
it's still
it's still
not
not
acceptable. I think it's to be fair that
acceptable. I think it's to be fair that
they've been working on like a new
interface like they they haven't been
interface like they they haven't been
working on
this. It shouldn't just rot.
So we got to
find
find
obnoxious. Can can this interface start
obnoxious. Can can this interface start
working please?
like hey
like hey
captain maybe Monday I should be able to
captain maybe Monday I should be able to
work on impulse bolding without seeing
work on impulse bolding without seeing
very
very
good I'm working on getting everything
good I'm working on getting everything
for this release man but like the thing
for this release man but like the thing
is reproducing this weep has proven to
is reproducing this weep has proven to
be very very difficult
Yeah, we're having a difficulty
Yeah, we're having a difficulty
reproducing the uh the grid sweep was
reproducing the uh the grid sweep was
our original benchmark.
Hey, this UI is just screwing with me
Hey, this UI is just screwing with me
because
like, hey, this doesn't make
sense why this is not showing up.
There it
is. Now you can see the good runs. You
is. Now you can see the good runs. You
can see the difference here.
I grab the hyper prams from this run.
I grab the hyper prams from this run.
And the idea here is like this is the
And the idea here is like this is the
best run
best run
uh from what should have been the
uh from what should have been the
identical code to the
identical code to the
original. So I want to see if it's just
original. So I want to see if it's just
finding dramatically different hypers.
finding dramatically different hypers.
Uh if when I actually run and repro this
Uh if when I actually run and repro this
it's different or like what is going on
it's different or like what is going on
here?
All right. So, we have
this. Let's
grab What's going on with my machine?
grab What's going on with my machine?
Why is
Why is
it Why do I have all these cores at at
it Why do I have all these cores at at
100%?
100%?
It says I have something training
It says I have something training
locally.
freaking ghost processes. I
swear every single
time. All right.
Yeah, this is the right one. Puffer
Yeah, this is the right one. Puffer
train. Puffer
grid. I'm going to try this.
In the meantime, let's actually take a
In the meantime, let's actually take a
look at what these prams
are. Got a lower atom beta 1
I
clip gamma and lambda are
reasonable. Learning rates may be a
reasonable. Learning rates may be a
little high.
First of all, it's Blow.
See if I missed any
params. Don't believe I did.
It's very weird because the thing is
It's very weird because the thing is
like this sweep if I take these
like this sweep if I take these
parameters it repros. I have them up
parameters it repros. I have them up
here. I've done it several
here. I've done it several
times. But like you can consistently
times. But like you can consistently
reproduce this graph with these hypers.
reproduce this graph with these hypers.
Well, you can't find these hyperp with
Well, you can't find these hyperp with
either the new sweep code or the old
either the new sweep code or the old
sweep code.
Interestingly, the best run I believe
Interestingly, the best run I believe
it's right here. So, this is the best
it's right here. So, this is the best
run is only at like
run is only at like
92ish after 200 million
steps. That's only 2% off of
steps. That's only 2% off of
of this run
here. Like it's actually pretty close.
here. Like it's actually pretty close.
So I wonder if it's straight up
like if I make this thing run for just
like if I make this thing run for just
longer,
longer,
right? If I made it do
400, I might try that next.
Because yeah, 92 is the best that we
Because yeah, 92 is the best that we
get.
The only thing I can think of is like
The only thing I can think of is like
some sort of timing difference, right?
cuz basically like none of these runs
cuz basically like none of these runs
have done I haven't been able to get it
have done I haven't been able to get it
to do as long of runs in a while if that
to do as long of runs in a while if that
makes sense.
Oh, and these prams are not even stable.
Trying to think like what would cause
was this level of discrepancy,
right? Like a reporting interval thing.
The thing the the other thing though
The thing the the other thing though
that's weird is like the sweeps take
that's weird is like the sweeps take
about the same amount of time.
Wait, if the sweeps take about the same
Wait, if the sweeps take about the same
amount of
amount of
time, this one's still using fewer time
time, this one's still using fewer time
steps, isn't
steps, isn't
it? I didn't think of that. This one
it? I didn't think of that. This one
should be taking longer, shouldn't it?
This one should be taking longer.
This is 22
hours. We can ignore the last like 20 or
hours. We can ignore the last like 20 or
so points.
Okay, if you cut out the last points,
Okay, if you cut out the last points,
then it is longer
then it is longer
actually cuz these are just like the
actually cuz these are just like the
final. This one's 20 more runs than this
final. This one's 20 more runs than this
one. 22 more
one. 22 more
runs. The times do match
runs. The times do match
up. I was wondering if it's like a
up. I was wondering if it's like a
timing based thing, right? We're
timing based thing, right? We're
like hyperparam suggestions are in part
like hyperparam suggestions are in part
based on how long runs
take I mean we can see the total time
take I mean we can see the total time
step discrepancy here
right like we get some runs up at 600
right like we get some runs up at 600
mil and that just doesn't happen over
here. I mean, I've checked this
here. I mean, I've checked this
particular thing a million times,
particular thing a million times,
though.
though.
Like, it's pretty easy to just go to one
Like, it's pretty easy to just go to one
of these runs
This is 500 or 50 mil to 600 mil 100 mil
This is 500 or 50 mil to 600 mil 100 mil
mean with an auto scale log
normal is exactly how I have it now.
Yeah. So, this doesn't do I mean, this
Yeah. So, this doesn't do I mean, this
actually got unlucky. I think it crashed
actually got unlucky. I think it crashed
even a little bit
worse. That's real
bad. What am I not seeing here? Like,
bad. What am I not seeing here? Like,
what am I not seeing?
There's got to be Something.
Dude, there's nobody reporting
like stuff, right?
I thought that I'd um I checked this
I thought that I'd um I checked this
though. Can I have a run where I did
that? I did.
I had a run.
Maybe I should just do that just to be
Maybe I should just do that just to be
absolutely sure.
Maybe I should just set up a
Maybe I should just set up a
run, a longer run, even match like the
run, a longer run, even match like the
torch version or
torch version or
whatever. The thing is I actually I
whatever. The thing is I actually I
don't remember what torch version this
don't remember what torch version this
was
was
on. Like wouldn't this have probably
on. Like wouldn't this have probably
already have been on
128? Yeah, it would have been on 128.
This is much more similar.
I mean this is interesting
here. This is fixed mini batch.
I mean, this tells me it is like pretty
I mean, this tells me it is like pretty
insistent on this being the best it can
insistent on this being the best it can
do, even though I
know. Hang on.
100 almost all these are on uh
100 almost all these are on uh
h low mini batch
h low mini batch
size. Okay. So we have this this
plot and like it doesn't look like it's
plot and like it doesn't look like it's
particularly difficult for this thing to
particularly difficult for this thing to
get
above above 0.8
either. Yeah, we actually we have 93%ish
in 200 mil.
in 200 mil.
So what params?
Where are my runs?
It's doing some like jank shenanigans,
It's doing some like jank shenanigans,
but this interface kind of sucks.
the learning
rate. Yeah, we've got the right like
rate. Yeah, we've got the right like
learning
rateish 996.
a gamma.
learning a little low,
but should be able to get 0.9 with any
but should be able to get 0.9 with any
of
of
these lambdas. Lambda's really tolerant,
these lambdas. Lambda's really tolerant,
right?
value function
coefficient two.
There should be a little bit of
jank.
This if I check our value function
This if I check our value function
coefficients on this
Oh yeah. I mean it's it's exploring the
Oh yeah. I mean it's it's exploring the
higher ones for
sure. Radiant
norm. That was good.
entropy in a reasonable
entropy in a reasonable
range. Horizon.
compare the distributions of these
compare the distributions of these
directly.
Now, shift it a little bit, but we've
Now, shift it a little bit, but we've
got
got
overlap. I'm just looking for
overlap. I'm just looking for
overlap. Okay, gamma is not going high
overlap. Okay, gamma is not going high
enough
enough
somehow. That is interesting. Gamma is
somehow. That is interesting. Gamma is
not going high enough.
Need to rerun this with um fixed
Need to rerun this with um fixed
vectorzation param.
The rest of these look
fine. That's a bit high.
better. Okay. Well,
better. Okay. Well,
so the one piece of
so the one piece of
information I can potentially get from
information I can potentially get from
this
Oh no, we have ve num ms equal
8. This is fine then.
Yeah, this is
fine. The only discrepancy I see is it
fine. The only discrepancy I see is it
just not exploring high enough gamma.
Does this thing just get the high gamma
immediately? I'm going to have to run
immediately? I'm going to have to run
for uh for breakfast in a minute. Then
for uh for breakfast in a minute. Then
I'll be back doing more experiment
I'll be back doing more experiment
analysis, more environment work,
analysis, more environment work,
probably doing some server stuff in the
probably doing some server stuff in the
background, lots of things. But since we
background, lots of things. But since we
have a fair few folks on YouTube, uh
have a fair few folks on YouTube, uh
before I do
before I do
that, all the stuff's open source at
that, all the stuff's open source at
puffer.ai, you can help me for free just
puffer.ai, you can help me for free just
by starting the repo. And you can get
by starting the repo. And you can get
involved with development today just by
involved with development today just by
joining the uh the
joining the uh the
Discord. All right, I'll do my last bits
Discord. All right, I'll do my last bits
of analysis here.
Um, I want to see gamma as a function
Um, I want to see gamma as a function
of how many experiments you
run. Okay. So, like
run. Okay. So, like
gamma. Oh, that's bizarre.
It just snaps to max
gamma. If I cut this
out, is it that the intermediate runs
So there are a whole bunch of runs not
So there are a whole bunch of runs not
at the optimal batch size that push this
at the optimal batch size that push this
gamma
gamma
up to the
top. Huh.
Okay. And let's just be sure I didn't
Okay. And let's just be sure I didn't
miss them here.
auto scale.
Right. This is the same
What about in our um our original
run? This have a gamma problem. No, this
run? This have a gamma problem. No, this
doesn't have a gamma problem.
doesn't have a gamma problem.
This still doesn't repro
though. Another good comparison to
do, but this one will wait for after
do, but this one will wait for after
breakfast. So I'll check I'll check the
breakfast. So I'll check I'll check the
distributions of uh swept parameters
distributions of uh swept parameters
after breakfast for
after breakfast for
um the intended reproduction run and
um the intended reproduction run and
then this run to see directly if there's
then this run to see directly if there's
anything between those two. All right.
anything between those two. All right.
Well, thanks for tuning in folks. I will
Well, thanks for tuning in folks. I will
be back in 30 40 minutes.
