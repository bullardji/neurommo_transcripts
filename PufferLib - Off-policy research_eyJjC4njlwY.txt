Kind: captions
Language: en
Okay,
Okay,
be live here.
be live here.
Hi.
A whole bunch of research I've been
A whole bunch of research I've been
looking at.
I want to go back today to um
want to go back to this
heck.
heck.
It'd
be fine.
Okay. So this was the imitation result
Okay. So this was the imitation result
from before.
It's kind of funny how this was uh
It's kind of funny how this was uh
immediately.
It's like so much more promising than
It's like so much more promising than
the offpaul, isn't it?
It also did better than nothing on
It also did better than nothing on
Breakout.
Okay. So, I think what we're going to do
Okay. So, I think what we're going to do
is we're going to train an advantage
is we're going to train an advantage
function
function
and we're going to wait the loss by the
and we're going to wait the loss by the
advantage function.
advantage function.
Let's see what that does for us.
Welcome, Adrian.
Welcome, Adrian.
Gold stands for good.
Gold stands for good.
Like the actual good ones.
Like the actual good ones.
Um,
honestly, we can probably take a lot of
honestly, we can probably take a lot of
this stuff, can't we?
Yeah, really. I just need the um
need this entire block.
Okay. So we can do this on
Okay. So we can do this on
old values
words.
Fine.
And
And
believe we already messed with Yeah. So,
believe we already messed with Yeah. So,
we already have the gold
we already have the gold
data.
We can start with this, right?
Ah,
Gold is equal to advantage filtering.
Gold is equal to advantage filtering.
No, I'm doing imitation stuff. So, it's
No, I'm doing imitation stuff. So, it's
an experimental. I should have probably
an experimental. I should have probably
clarified. I'm kind of tired today. Um,
clarified. I'm kind of tired today. Um,
so what I'm doing at the moment, it's
so what I'm doing at the moment, it's
it's imitation learning without an
it's imitation learning without an
expert.
expert.
The idea is, you know, you collect some
The idea is, you know, you collect some
data, you sort it to see what data is
data, you sort it to see what data is
actually good, and then you just do
actually good, and then you just do
imitation learning on the good samples
imitation learning on the good samples
with some waiting.
with some waiting.
I've been going through over the last
I've been going through over the last
couple days like a whole bunch of math.
couple days like a whole bunch of math.
And it seems like there's a very close
And it seems like there's a very close
equivalence between behavioral cloning
equivalence between behavioral cloning
and policy gradient methods. It's just
and policy gradient methods. It's just
that the derivation that you get out of
that the derivation that you get out of
policy gradients assumes that your data
policy gradients assumes that your data
is on policy. Behavioral cloning
is on policy. Behavioral cloning
doesn't. But they seem like um just
doesn't. But they seem like um just
different weightings of the same
different weightings of the same
objective when you actually write it
objective when you actually write it
out. So I'm basically I'm trying to see
out. So I'm basically I'm trying to see
if we can break the uh the need for the
if we can break the uh the need for the
on policy assumption.
Yeah. So, I mean, this has been the main
Yeah. So, I mean, this has been the main
thing I've been trying to figure out um
thing I've been trying to figure out um
over the past several days is like how
over the past several days is like how
do we drop the need for the on policy
do we drop the need for the on policy
assumption? Interesting
that this works perfectly.
math blogs if any. Well, if it works,
math blogs if any. Well, if it works,
there will be one, right?
there will be one, right?
I've seen that this is like a
I've seen that this is like a
qualitatively different type of
qualitatively different type of
algorithm already. There's something
algorithm already. There's something
here, but I know that I know that that
here, but I know that I know that that
we can get something out of this. The
we can get something out of this. The
question is just how because I've seen
question is just how because I've seen
some I've seen some pretty crazy results
some I've seen some pretty crazy results
already.
But they're holes.
Okay,
Okay,
at least it's more stable here.
There are a few different ways I can
There are a few different ways I can
incorporate this, right?
incorporate this, right?
Let's get the prioritized um sampling
Let's get the prioritized um sampling
into this.
Let's get the prioritized sampling.
Okay. So, you get the priority and you
Okay. So, you get the priority and you
get an index.
You have this absolute advantage here.
like Yes,
need to move to a harder task. I think.
Yeah, this will be a better um starting
Yeah, this will be a better um starting
point.
I don't know why it is, but like
I don't know why it is, but like
breakout seems the minimal task where if
breakout seems the minimal task where if
it works on breakout, it probably works
it works on breakout, it probably works
on a lot of stuff.
on a lot of stuff.
If you try stuff on Pong, it works on
If you try stuff on Pong, it works on
Pong. It doesn't necessarily work on a
Pong. It doesn't necessarily work on a
lot of other stuff,
right? So, this is
right? So, this is
our 20 whatever it is for
do our prioritize sampling.
I do wrong.
Okay. So, it's not a huge difference.
Okay. So, it's not a huge difference.
Interesting.
Go back to cartpole for a little bit.
Definitely do something.
Okay. So,
Okay. So,
yeah, this batch size is freaking tiny,
yeah, this batch size is freaking tiny,
right?
even have to be.
even have to be.
This doesn't even have to be
This doesn't even have to be
like this, right?
We have a whole bunch of samples now,
We have a whole bunch of samples now,
right?
right?
5 mil. This should get slow.
Yeah. So now this is super slow.
this to one.
That gets you
prioritize sampling.
Possibly not very good. Prioritize
Possibly not very good. Prioritize
sampling.
I awaits.
Where's A
I need to come up with some values for
I need to come up with some values for
this, right?
Oh, so this MB pryo actually doesn't
Oh, so this MB pryo actually doesn't
even come in at all, right?
even come in at all, right?
It's just the index.
It's just the index.
And just get rid of this for now.
Okay.
This is still not waiting them yet
This is still not waiting them yet
though, right?
Hey, Dash.
Weird exponential waiting, huh?
they use for beta
4.05
You're recomputing as well every time.
You're recomputing as well every time.
No.
No.
What if we just did top K?
Why not prioritize replay? You're
Why not prioritize replay? You're
literally looking at prioritize replay
literally looking at prioritize replay
right here.
It's hard to reason about. So, I'm
It's hard to reason about. So, I'm
testing a few other things.
You just do top K.
Oh, wait. Should it not be absolute
Oh, wait. Should it not be absolute
value?
Maybe it shouldn't be absolute value.
Maybe it shouldn't be absolute value.
Hang on. Yeah. Yeah. Yeah. It shouldn't
Hang on. Yeah. Yeah. Yeah. It shouldn't
be absolute value.
This is why we need to just have a top K
This is why we need to just have a top K
baseline.
used it with TD error.
used it with TD error.
Yeah, because we don't have um
what you what you
what you what you
the one in Rainbow is like a two or what
the one in Rainbow is like a two or what
is it? Three-step bootstrap.
is it? Three-step bootstrap.
I've been doing a whole bunch of looking
I've been doing a whole bunch of looking
at the math and the conclusion that I've
at the math and the conclusion that I've
come to basically is I think that a lot
come to basically is I think that a lot
of these off policy methods kind of just
of these off policy methods kind of just
suck. Like the update that they do just
suck. Like the update that they do just
sucks. Um but and the reason that it
sucks. Um but and the reason that it
sucks is because they have like a very
sucks is because they have like a very
short bootstrap interval and they get
short bootstrap interval and they get
around it just because since it's on
around it just because since it's on
policy they can just crank more and more
policy they can just crank more and more
compute into it
compute into it
because like at least as far as I can
because like at least as far as I can
see how on earth are you supposed to get
see how on earth are you supposed to get
a good how on earth are you supposed to
a good how on earth are you supposed to
get good updates out of like a
get good updates out of like a
three-step bootstrap
and the three-step bootstrap is already
and the three-step bootstrap is already
cheating because that's already off
cheating because that's already off
policy in a way that the algorithm like
policy in a way that the algorithm like
in a way that EQN can't even tolerate.
in a way that EQN can't even tolerate.
Like the only thing that actually should
Like the only thing that actually should
work is um the onestep bootstrap.
work is um the onestep bootstrap.
Hey, convert
Hey, convert
automate homestead
automate homestead
found another. What do you do for work
found another. What do you do for work
that you're getting all the uh this
that you're getting all the uh this
land? That is awesome. Land is very
land? That is awesome. Land is very
good. I miss my uh my family's farm.
good. I miss my uh my family's farm.
I've been here, you know, in in Palo
I've been here, you know, in in Palo
Alto for a bit.
Alto for a bit.
What's a three-step bootstrap? All
What's a three-step bootstrap? All
right, you guys can tell me if this is
right, you guys can tell me if this is
crazy or not, but I've been looking
crazy or not, but I've been looking
through tons and tons of math.
So, pretty much
So, pretty much
all the off policies stuff does some
all the off policies stuff does some
form of
form of
this endstep bootstrap right here. This
this endstep bootstrap right here. This
multi-step learning
multi-step learning
Okay,
Okay,
but the thing is they set n equals like
but the thing is they set n equals like
three.
three.
So instead of it being
So instead of it being
a literal one-step bootstrap where it's
a literal one-step bootstrap where it's
just one state like one reward gamma
just one state like one reward gamma
maxus Q whatever um
maxus Q whatever um
you know you go forward a few steps
you know you go forward a few steps
and now this data
this data did not actually come from the
this data did not actually come from the
policy. So technically you should
policy. So technically you should
already have to import and sample this
but yeah
but yeah
for a truck from continental coolep
for a truck from continental coolep
learning is stops after eight. Yeah. So
learning is stops after eight. Yeah. So
that's the problem, right? The reason
that's the problem, right? The reason
that it stops after 8 is because it
that it stops after 8 is because it
actually does break it breaks the
actually does break it breaks the
assumption of the algorithm, right?
assumption of the algorithm, right?
The reason DQN works for O policy is
The reason DQN works for O policy is
that it's a one-step bootstrap.
If you do a multi-step bootstrap,
when you do like an endstep bootstrap
when you do like an endstep bootstrap
like this,
like this,
you have to you technically are supposed
you have to you technically are supposed
to apply um important sampling to the
to apply um important sampling to the
data to correct,
data to correct,
right? And here it's just okay that you
right? And here it's just okay that you
don't do that, but whatever. It's only
don't do that, but whatever. It's only
three steps, so you don't get that far
three steps, so you don't get that far
off. But then if you actually go look at
off. But then if you actually go look at
important sampling like
if you go look at retrace
if you go look at retrace
this is actually very very similar to
this is actually very very similar to
what I came up with for puffer advantage
what I came up with for puffer advantage
for the on policy case. Okay. And you
for the on policy case. Okay. And you
have like I said you have to go through
have like I said you have to go through
all this freaking math. But the idea
all this freaking math. But the idea
here is that you weight samples by the
here is that you weight samples by the
ratio of the probability of taking the
ratio of the probability of taking the
action with the policy that you have now
action with the policy that you have now
uh to the probability of taking that
uh to the probability of taking that
action with the policy under which you
action with the policy under which you
collected the data.
collected the data.
So what this means is if these two
So what this means is if these two
things diverge a bunch, what's going to
things diverge a bunch, what's going to
happen
happen
is you're not actually going to be able
is you're not actually going to be able
to make use of uh of off policy data
to make use of uh of off policy data
because it's going to downweight
because it's going to downweight
everything.
Like it's a correction in the sense that
Like it's a correction in the sense that
you don't trust the data that's off
you don't trust the data that's off
policy very much.
Monte. No.
Okay. Interesting.
Okay. Interesting.
Um
get these baselines.
Yeah, this is actually a pretty weak
Yeah, this is actually a pretty weak
result
result
for something this fancy.
I do think search is the right way to go
I do think search is the right way to go
though.
like at least search is a reasonable way
like at least search is a reasonable way
to go for scaling compute I should say
to go for scaling compute I should say
when you have a slow environment
when you have a slow environment
specifically
specifically
how do you approximate the denominator
how do you approximate the denominator
oh of uh important sampling you don't
oh of uh important sampling you don't
you just you store it because you have
you just you store it because you have
the you can get the probability of
the you can get the probability of
sampling this action right under the old
sampling this action right under the old
policy and then you're just using that
policy and then you're just using that
whenever you use the new sample
whenever you use the new sample
strong correlation between being fancy
strong correlation between being fancy
and being fake. Yeah.
I mean, the problem with uh the offpaul
I mean, the problem with uh the offpaul
methods as well is you kind of get it
methods as well is you kind of get it
masked. So, this is the best thing that
masked. So, this is the best thing that
I found, which is just like this is
I found, which is just like this is
basically Rainbow V2 in the sense that
basically Rainbow V2 in the sense that
it's what happens if we strap more
it's what happens if we strap more
tricks, we just add more tricks onto it.
tricks, we just add more tricks onto it.
And like they get a pretty dang good
And like they get a pretty dang good
result. Um,
result. Um,
I actually like this paper a lot more
I actually like this paper a lot more
than the other ones. And it's like said,
than the other ones. And it's like said,
"Oh, yeah, this is way faster as well
"Oh, yeah, this is way faster as well
than the other offpaul methods." But you
than the other offpaul methods." But you
do the math and you're training Atari at
do the math and you're training Atari at
like 2,000 steps per second.
And the thing that I haven't seen from
And the thing that I haven't seen from
off policy yet,
so I think that there are worse
so I think that there are worse
algorithms that are being made better by
algorithms that are being made better by
being able to leverage more compute on
being able to leverage more compute on
the same data. That's not what I want. I
the same data. That's not what I want. I
want to have like an equivalent
want to have like an equivalent
algorithm that does just as well in the
algorithm that does just as well in the
high data regime,
high data regime,
but where you can crank up compute if
but where you can crank up compute if
you don't have as much data. You need to
you don't have as much data. You need to
have this like smooth scaling between
have this like smooth scaling between
these two.
these two.
Optimizing purely for low data regime is
Optimizing purely for low data regime is
kind of just dumb.
Okay. So, this is a top K baseline.
H, this gets stuck.
That's kind of Honey
fast TD3. Yeah, I saw that.
fast TD3. Yeah, I saw that.
I hadn't looked at the details
lab.
Okay. So, this is pretty close to what I
Okay. So, this is pretty close to what I
was wanting to look at here.
large batch training is good
distribute if Yeah. So, this is what I
distribute if Yeah. So, this is what I
was looking at yesterday. And one of the
was looking at yesterday. And one of the
annoying things is you immediately have
annoying things is you immediately have
to do um
Okay.
Okay.
Double Q.
Where's the algorithm?
Where's the algorithm?
Where's the uh the algorithm
Where's the uh the algorithm
description?
description?
They miss it.
NDPJ
critic.
critic.
>> Yeah, it could be that this is the way
>> Yeah, it could be that this is the way
to go. I'm not sure yet.
Well, that's some good evidence to have
Well, that's some good evidence to have
here, though.
here, though.
I'll keep on this for a little bit.
I should actually do this up here,
I should actually do this up here,
right?
Oh, wait. Can you even compute?
Oh, wait. Can you even compute?
You can compute advantage on this,
You can compute advantage on this,
right?
Yeah, you can.
Enter.
How are you?
Okay. So, we do something like this. I
Okay. So, we do something like this. I
think
I don't know if I like the idea of
I don't know if I like the idea of
throwing away data based on
throwing away data based on
Vantage or not yet.
think like this. Maybe
still worse.
Well, we can kind of break up this
Well, we can kind of break up this
problem a little bit, right?
I go to 20 million
here. Let's see. Um,
here. Let's see. Um,
this is going to be the full 20
full 20 mil. And then
full 20 mil. And then
uh if we do
uh if we do
the 32K
before
eight batches.
eight batches.
Okay. So that's not bad.
Okay.
Okay.
150. But basically, no data should ever
150. But basically, no data should ever
get kicked out here, right?
And no data should ever
And no data should ever
uh ever get kicked out.
IL segments
8192 segments.
That's totally wrong. Right.
Hatch size.
Oh yeah, this is just a mess up here,
Oh yeah, this is just a mess up here,
right?
Um, something's
This is just me having been sloppy.
Okay, so no data is getting kicked out
Okay, so no data is getting kicked out
anymore.
How do you start research?
How do you start research?
Uh, if you're interested in getting
Uh, if you're interested in getting
started on this type of stuff,
started on this type of stuff,
my quick start guide. I made a guide for
my quick start guide. I made a guide for
people. It's the most common question I
people. It's the most common question I
get asked.
So here's starter guide for programming
So here's starter guide for programming
in ML and then for RL research
in ML and then for RL research
specifically
specifically
research and dev.
research and dev.
The ML article also has some of my best
The ML article also has some of my best
advice for uh research in general.
So
I break here
I break here
self
and no worries.
[Music]
This is probably horribly slow.
probably horribly horribly slow.
So, can I just train more now?
It kind of It was unstable though,
It kind of It was unstable though,
right?
right?
Why should it be unstable?
Why should it be unstable?
Why can I not just do this and have it
Why can I not just do this and have it
work?
Okay.
And just keep going up like this.
What's wrong?
Oh, you know what it is?
Good
know if they're going to release the RLC
know if they're going to release the RLC
talks. No idea. I don't know if they're
talks. No idea. I don't know if they're
recorded, but mine is up on YouTube.
I recorded mine separately.
What happens here?
Okay, so this definitely shouldn't be
Okay, so this definitely shouldn't be
happening, right?
You get your best indices.
Oh, well, you're not optimizing a value
Oh, well, you're not optimizing a value
function, dummy. So, that would do it.
Should
be stable.
better.
Okay. So, you actually do get
You do get Yes.
No positive advantage left.
got to clip this somehow, right?
H okay.
We have some advantages. Yes.
in some todos and evaluate
in some todos and evaluate
where you only stack observations
where you only stack observations
from the same end and don't use mask.
from the same end and don't use mask.
Wait, what do you mean you only stack
Wait, what do you mean you only stack
observations
observations
the same end? So that todo is for
the same end? So that todo is for
something different.
something different.
That todo is for multi- aent like
That todo is for multi- aent like
masking or if you have um like if you
masking or if you have um like if you
have dead agents basically.
Truncations you could easily handle.
Truncations you could easily handle.
I don't think any of our new ocean
I don't think any of our new ocean
environments use truncations.
environments use truncations.
You could handle it for classic ones
You could handle it for classic ones
though. I haven't bothered because like
though. I haven't bothered because like
we just don't bother with uh in our new
we just don't bother with uh in our new
environments making a distinction.
The main thing I'm trying to figure out
The main thing I'm trying to figure out
at the moment is how to bridge this like
at the moment is how to bridge this like
gulf between um
gulf between um
like sample efficiency and compute
like sample efficiency and compute
efficiency.
When you get a done from an end, you
When you get a done from an end, you
keep stacking up. So that doesn't So
keep stacking up. So that doesn't So
that's actually handled correctly. If
that's actually handled correctly. If
you get a done from an environment, it
you get a done from an environment, it
gets reset. But like you uh you cut the
gets reset. But like you uh you cut the
bootstrap like you don't you don't keep
bootstrap like you don't you don't keep
increased like incrementing values in
increased like incrementing values in
your advantage function across done
your advantage function across done
boundaries. That's already handled
boundaries. That's already handled
correctly.
downside of downside of computer. What
downside of downside of computer. What
benefit of replay buffer?
benefit of replay buffer?
We're trying to figure out uh if we can
We're trying to figure out uh if we can
have a replay buffer without having to
have a replay buffer without having to
use really janky off policy algorithms
use really janky off policy algorithms
that don't actually work in the high
that don't actually work in the high
data regime.
and just do it this way, right?
Okay. So, now we're no longer going to
Okay. So, now we're no longer going to
collapse on bad advantages, I suppose.
Oh, we're definitely collapsing.
How's that possible? Oh,
Okay. I mean, so like you'll pretty darn
Okay. I mean, so like you'll pretty darn
quickly here get to the point where
quickly here get to the point where
this thing is saying that there's just
this thing is saying that there's just
like almost no advantage left in the
like almost no advantage left in the
data,
which seems weird.
because we're keeping all of the data
because we're keeping all of the data
around, right?
You know, let's do um losses.
You know, let's do um losses.
Let's put this in here. We can kind of
Let's put this in here. We can kind of
see without having it spam us
see without having it spam us
at what point this thing messes up.
We start off high advantage.
We start off high advantage.
We're learning well.
We're learning well.
Then we get down to very low advantage
Then we get down to very low advantage
and we regress. Yes.
code is kind of a mess but the uh the
code is kind of a mess but the uh the
algorithm here unless I have a major
algorithm here unless I have a major
mistake
mistake
should be pretty simple.
should be pretty simple.
So we have all our rewards,
So we have all our rewards,
observations, actions, values,
observations, actions, values,
terminals, right? We have everything
and we compute our advantage function.
We sort it descending
and then we keep the uh the best ones,
and then we keep the uh the best ones,
right?
And this is set up such that we really
And this is set up such that we really
should not ever be throwing data away
should not ever be throwing data away
either.
And we run a whole bunch of mini batches
And we run a whole bunch of mini batches
which we compute
which we compute
advantage estimate.
The advantage estimate, right?
We take our top K samples.
We take our top K samples.
We just do IIL.
Right.
We literally would just do
We literally would just do
mitation learning.
mitation learning.
Same exact loss and everything.
at this
A lot.
This this
no something of this form I think should
no something of this form I think should
work.
Advantages should be close to zero.
Advantages should be close to zero.
Right? So when you've learned the task,
Right? So when you've learned the task,
the advantages should be close to zero.
They shouldn't be before you've learned
They shouldn't be before you've learned
the task, though.
And you definitely should not regress
and like start doing worse when you're
and like start doing worse when you're
literally doing batch learning.
about the number of samples surviving
about the number of samples surviving
Bing.
Bing.
Uh that one is fair.
What our losses
Why is it such a big number?
Oh,
for now we'll just hard code.
Huh?
Yeah, this thing should be locked to um
be like locked.
Okay.
There isn't a Q function though.
There isn't a Q function though.
It's just an advantage estimate
which is dependent on the current
which is dependent on the current
policy.
policy.
So if it shouldn't be able to get better
So if it shouldn't be able to get better
than the current policy, All right.
Having
to fit the value function.
now. This actually this this should
now. This actually this this should
totally work. There's got to be
totally work. There's got to be
something screwy here.
Yeah. So, what I'm doing here, right, is
Yeah. So, what I'm doing here, right, is
I have I have fresh data being collected
I have I have fresh data being collected
for now. I have it set up so that
for now. I have it set up so that
effectively you're storing all of it. So
effectively you're storing all of it. So
you have access to all of the data
you have access to all of the data
and then I'm just computing
and then I'm just computing
advantage
advantage
um fresh advantage for everything.
um fresh advantage for everything.
Well, as fresh as you can get it. And
Well, as fresh as you can get it. And
I'm just training on the I'm just doing
I'm just training on the I'm just doing
behavioral cloning on like the top 32k.
behavioral cloning on like the top 32k.
So the idea is that you get all the
So the idea is that you get all the
information you can out of each of the
information you can out of each of the
samples.
I guess it is also possible.
Well, the problem is you actually can't
Well, the problem is you actually can't
get a fresh advantage estimate, right? I
get a fresh advantage estimate, right? I
think that's the problem.
Yeah, you can't get a fresh advantage
Yeah, you can't get a fresh advantage
estimate without rerunning it on the
estimate without rerunning it on the
whole batch. Like you have to rerun the
whole batch. Like you have to rerun the
value function on the whole batch to do
value function on the whole batch to do
That
I can probably just code that up. It'll
I can probably just code that up. It'll
be slow.
We do something like this.
We do something like this.
We do this full forward pass.
This will at least let me verify if
This will at least let me verify if
what's happening is uh what I Think.
Oh,
Move this out here
slightly less often.
dude are going to be out of memory. Come
dude are going to be out of memory. Come
on.
This stupid thing breaking.
That's totally freaking wrong. How do I
That's totally freaking wrong. How do I
do this?
Still No.
Okay,
Much better.
Still unstable. interesting enough.
Still unstable. interesting enough.
Um, but we have positive advantage now
Um, but we have positive advantage now
at least.
Yeah, we actually have like good uh
Yeah, we actually have like good uh
reasonable data now. So, we can figure
reasonable data now. So, we can figure
out why this thing is unstable.
Oh, this is going to run the same
Oh, this is going to run the same
freaking batch 128 times, right?
freaking batch 128 times, right?
Oops.
Yeah, this is totally going to run the
Yeah, this is totally going to run the
uh
uh
the same batch.
I don't see how the heck this is
I don't see how the heck this is
possibly still unstable.
It's even worse. Really?
Really?
I just want to integrate a value
I just want to integrate a value
function with this thing. Why is this so
function with this thing. Why is this so
difficult?
I suppose I could do this on um
I suppose I could do this on um
on raw rewards first, right?
Let's do it this way.
That's going to be super overfit and
That's going to be super overfit and
that's still better.
that's still better.
So yeah, there's something very screwy
So yeah, there's something very screwy
then about this advantage based
then about this advantage based
estimate. Like this is literally just
estimate. Like this is literally just
overfitting a big a batch of rewards.
Okay.
Let's just make absolutely sure we're
Let's just make absolutely sure we're
not kicking out any data that we want.
not kicking out any data that we want.
Now we are keeping data by reward.
Get your new value function
Get your new value function
like
This doesn't do enough, right?
Ability.
You just train based on reward. This
You just train based on reward. This
works
thumb advantage.
very low positive advantage already.
mean the policy thinks it should be
mean the policy thinks it should be
doing very well.
It does, right?
Policy thinks it should be doing very
Policy thinks it should be doing very
well.
Shouldn't
be not for the purpose of sorting it.
Think that would make a difference. Um
this is literally data selection because
this is literally data selection because
we're only using advantage
we're only using advantage
to select data.
to select data.
So I mean literally if I just do look
So I mean literally if I just do look
how easy this problem is, right?
If I just change this
to gold rewards.
Literally just training on a mini batch
Literally just training on a mini batch
of the highest reward segments.
which barely even makes sense cuz you're
which barely even makes sense cuz you're
like you're going to giga overfit
that actually solved before something
that actually solved before something
changed.
changed.
[Music]
like a cheat.
You can do it.
this not solve a second ago.
this not solve a second ago.
I could have sworn that we had this
I could have sworn that we had this
solving
We should actually then make sure that
We should actually then make sure that
we can get this to solve right.
we can get this to solve right.
Otherwise,
Okay.
Good. So, um, sanity has returned,
Good. So, um, sanity has returned,
right?
So all this is saying is if you sample
So all this is saying is if you sample
proportional to summed reward
proportional to summed reward
you solve the task.
I do it this way.
unstable.
I think we absolutely do need the
I think we absolutely do need the
advantage function here as well, right?
advantage function here as well, right?
Like the only thing that we really need
Like the only thing that we really need
to know is is this segment better than
to know is is this segment better than
the current policy or Not
this. so hard about that.
this. so hard about that.
Be easy.
Just read through this whole thing
Just read through this whole thing
carefully.
carefully.
See if I've done something dumb.
So you're just keeping this means you're
So you're just keeping this means you're
keeping all the data.
keeping all the data.
The batch is like 20 million. So fun.
You're updating your value.
You're updating your value.
Fine.
Get the value of everything in the
Get the value of everything in the
buffer.
Get your advantage.
Values, rewards, terminals, right?
Values, rewards, terminals, right?
Above right
is here.
Done. This
Done. This
same sample as before.
your value here.
again.
Now this should work
Now this should work
totally work
suppose I don't understand why the value
suppose I don't understand why the value
function
Is it the retrace thing biting me?
Is it the retrace thing biting me?
I'd highly doubt that, but
It's still unstable.
How do you learn the actual value
How do you learn the actual value
function?
function?
No, it's not implicit. It's right here.
The clipped value loss, right?
This is better.
This is better.
[Music]
doesn't if I just do this on breakout it
doesn't if I just do this on breakout it
doesn't work right
okay well light problem with that
can't have a 20 million buffer size.
to try to match the um
to try to match the um
original sample efficiency, right?
You on.
Okay,
Okay,
that is uh that is actually something.
Is it amazing yet? No.
It's something though.
Ryan, we are doing behavioral cloning
Ryan, we are doing behavioral cloning
without expert data.
Okay.
Pretty much that. Yeah. I'm trying to
Pretty much that. Yeah. I'm trying to
fit. The thing that's tough is I I you
fit. The thing that's tough is I I you
have to fit the advantage function to
have to fit the advantage function to
that to make it make sense.
that to make it make sense.
And I'm having trouble getting that to
And I'm having trouble getting that to
play nicely.
I should probably just go steal the uh
I should probably just go steal the uh
the advantage waiting, right?
Well, I suppose I haven't tried actually
Well, I suppose I haven't tried actually
waiting it by the advantage yet, right?
waiting it by the advantage yet, right?
I've just done
You actually don't necessarily though
You actually don't necessarily though
want to get rid of
want to get rid of
of segments in the buffer when they're
of segments in the buffer when they're
at low advantage, right?
behavioral
cloning. Yeah.
Work. Uh I don't think that that works.
Work. Uh I don't think that that works.
So I did a whole bunch of looking at the
So I did a whole bunch of looking at the
math and um
math and um
important sampling is just really
important sampling is just really
stupid.
like it pretty much just it just
like it pretty much just it just
prevents you from learning on data
prevents you from learning on data
because it's off policy. It doesn't
because it's off policy. It doesn't
actually help you learn on the data. It
actually help you learn on the data. It
just like takes it out of the uh the
just like takes it out of the uh the
advantage pretty much.
Yeah, but that's not what we want. I
Yeah, but that's not what we want. I
don't want to wait on policy data
don't want to wait on policy data
higher. I want to wait better data
higher. I want to wait better data
higher.
Exactly. I'm pretty much like I've been
Exactly. I'm pretty much like I've been
looking through so much of um the
looking through so much of um the
offpaul literature
offpaul literature
and
and
it seems to me like these are just
it seems to me like these are just
stupid worse algorithms that are always
stupid worse algorithms that are always
going to be worse than the high data
going to be worse than the high data
regime and it's just that they spend a
regime and it's just that they spend a
stupid amount of compute and eventually
stupid amount of compute and eventually
it gets better than on policy and then
it gets better than on policy and then
they keep going and you know they can
they keep going and you know they can
keep training on their stale data.
keep training on their stale data.
This seems like it's why um onpaul has
This seems like it's why um onpaul has
been competitive in the first place is
been competitive in the first place is
because the off pal the off-paul
because the off pal the off-paul
algorithms are just fundamentally worse.
Like they do this really stupid like
Like they do this really stupid like
three-step bootstrap or whatever.
three-step bootstrap or whatever.
That's the main thing. And then even if
That's the main thing. And then even if
you do something like retrace, right,
you do something like retrace, right,
it like kind of collapses to just being
it like kind of collapses to just being
on policy learning anyways. just like
on policy learning anyways. just like
with weird settings.
Low data regime is more relevant to I
Low data regime is more relevant to I
disagree.
disagree.
No, I completely disagree there.
No, I completely disagree there.
Industries do not give a if you
Industries do not give a if you
solve their problem by getting more data
solve their problem by getting more data
or um if you solve their problem with a
or um if you solve their problem with a
better algorithm.
better algorithm.
And fundamentally having more data will
And fundamentally having more data will
let you learn better with spending less
let you learn better with spending less
compute
until he addresses sample efficiency. I
until he addresses sample efficiency. I
don't think it does fundamentally
don't think it does fundamentally
address the sample efficiency and we
address the sample efficiency and we
have a I mean your author on the
have a I mean your author on the
published paper or like the main
published paper or like the main
candidate algorithm for that kind of
candidate algorithm for that kind of
crap.
Like I fundamentally don't even know if
Like I fundamentally don't even know if
modelbased RL makes any sense.
You're literally training. You're
You're literally training. You're
literally training to like poke all the
literally training to like poke all the
holes in your learned model.
Definitely need data reuse.
What to do with this?
In expectation, changing sampling
In expectation, changing sampling
frequency should be the same as
frequency should be the same as
is changing the waiting of the data.
all do we do with
I don't like so the thing that I don't
I don't like so the thing that I don't
like here Right.
You're going to sample your good data
You're going to sample your good data
more often and thereby you're going to
more often and thereby you're going to
overfit it and then you're going to kick
overfit it and then you're going to kick
it out of your buffer.
it out of your buffer.
Wait, high reward trajectories are. Yes,
Wait, high reward trajectories are. Yes,
that actually uh well, it's the same
that actually uh well, it's the same
thing, right? You can either like sample
thing, right? You can either like sample
the high reward trajectories more or you
the high reward trajectories more or you
can weight them. Either of these work at
can weight them. Either of these work at
least for the simple environments. Just
least for the simple environments. Just
doing behavioral cloning on high reward
doing behavioral cloning on high reward
segments works for simple tasks. The
segments works for simple tasks. The
problem right is um
problem right is um
tasks where you don't have enough
tasks where you don't have enough
information just in that one segment.
information just in that one segment.
You you need to like bootstrap uh an
You you need to like bootstrap uh an
advantage estimate of some type. Unless
advantage estimate of some type. Unless
you're going to wait all the way to the
you're going to wait all the way to the
end of the episode, which technically
end of the episode, which technically
you could kind of do with a lot of
you could kind of do with a lot of
environments, but um it's not ideal.
environments, but um it's not ideal.
Definitely not ideal.
Yeah. So, policy gradients does that.
Yeah. So, policy gradients does that.
And this is the thing that I'm trying to
And this is the thing that I'm trying to
figure out. Um, if you actually just
figure out. Um, if you actually just
write out the math for behavioral
write out the math for behavioral
cloning and policy gradient, it looks
cloning and policy gradient, it looks
really really similar. But like the
really really similar. But like the
derivation of policy gradients makes the
derivation of policy gradients makes the
on policy assumption. Um, behavioral
on policy assumption. Um, behavioral
cloning looks very similar, but doesn't
cloning looks very similar, but doesn't
make that assumption. But like we know
make that assumption. But like we know
that you can't just run
that you can't just run
like policy gradient methods with
like policy gradient methods with
totally off policy data and have it
totally off policy data and have it
work.
work.
So, I'm trying to figure that chunk of
So, I'm trying to figure that chunk of
it out pretty much.
Maybe you don't want an advantage
Maybe you don't want an advantage
function, right?
function, right?
Hang on.
What if you just train?
What if you just train?
What if you just train like a
What if you just train like a
bootstrapping value function?
And then you keep the segments
And then you keep the segments
that have the highest re like some
that have the highest re like some
reward plus value.
reward plus value.
Isn't this kind of similar to what um
Isn't this kind of similar to what um
I think I might have just recovered this
I think I might have just recovered this
other paper here.
other paper here.
This thing
This thing
they weigh it here. But yeah, this is
they weigh it here. But yeah, this is
reward.
reward.
Probably this is like return or
Probably this is like return or
something. Value.
something. Value.
No, this is advantage still.
No, this is advantage still.
But what if we just do value estimate?
I'm going to reduce the increasing
I'm going to reduce the increasing
learning rate or clipping rat. No.
learning rate or clipping rat. No.
Um,
Um,
well, not in our case, right, Ryan?
well, not in our case, right, Ryan?
because we like we have a much more
because we like we have a much more
aggressively optimized baseline.
aggressively optimized baseline.
So, like if you actually beat our
So, like if you actually beat our
baseline on a bunch of M's, you should
baseline on a bunch of M's, you should
be pretty confident that it actually
be pretty confident that it actually
works.
I think you do just want a value
I think you do just want a value
function, not an advantage function,
function, not an advantage function,
right?
Or do you want both?
Well, hang on. You would still want to
Well, hang on. You would still want to
sample your data according to advantage,
sample your data according to advantage,
right? You just wouldn't want to kick it
right? You just wouldn't want to kick it
out of the buffer.
I think that's right.
I think that's right.
Yeah. So, why does this not like insta
Yeah. So, why does this not like insta
solve everything?
solve everything?
Go to here.
We can mess with the sampling pram on
We can mess with the sampling pram on
advantage here.
prioritizing data definitely policy.
prioritizing data definitely policy.
Uh you're not doing policy gradient,
Uh you're not doing policy gradient,
you're just doing behavioral cloning.
you're just doing behavioral cloning.
So there are two different things here,
So there are two different things here,
right? One is how do you select what
right? One is how do you select what
data to keep in your buffer? You
data to keep in your buffer? You
probably don't want that to be advantage
probably don't want that to be advantage
based because then if you learn a piece
based because then if you learn a piece
of data, you throw it away. You want to
of data, you throw it away. You want to
keep it around so you're building a good
keep it around so you're building a good
data set.
data set.
But then how do you actually sample from
But then how do you actually sample from
that buffer? You probably just want to
that buffer? You probably just want to
sample like high advantage trajectory
sample like high advantage trajectory
segments, right? Because those are the
segments, right? Because those are the
ones that you can get the most
ones that you can get the most
information out of. But you don't want
information out of. But you don't want
to kick them out of the buffer based on
to kick them out of the buffer based on
advantage. Because if you kick them out
advantage. Because if you kick them out
based on advantage, right, once you've
based on advantage, right, once you've
learned them, that's it. If you kick
learned them, that's it. If you kick
them out based on low value, so which is
them out based on low value, so which is
uh summed reward over like some
uh summed reward over like some
discounted reward or whatever plus the
discounted reward or whatever plus the
final value estimate, then you'll you
final value estimate, then you'll you
will not kick out like good segments.
I guess your value function can still be
I guess your value function can still be
wrong. So if you get too many super
wrong. So if you get too many super
overoptimistic ones, you can kick out
overoptimistic ones, you can kick out
other ones potentially.
Don't have to do that just yet, though.
Don't have to do that just yet, though.
We can kind of just chill on this for
We can kind of just chill on this for
now. Um,
let's do
make this a mean first.
one over beta.
Exactly.
Wait, you need roll outs for advantage?
Wait, you need roll outs for advantage?
What do you mean you need rollouts for
What do you mean you need rollouts for
advantage? You need segments and you
advantage? You need segments and you
have segments. You can refresh both of
have segments. You can refresh both of
these estimates, right?
Okay, so the sampling scheme actually
Okay, so the sampling scheme actually
definitely matters a ton.
It's over 200 score on Breakout with
It's over 200 score on Breakout with
literally just behavioral cloning.
Can we find something that solves
Can we find something that solves
breakout
parameter they had was
parameter they had was
One over 105 says 20.
Yeah, that does by comparison.
Now I suppose the next question right
Now I suppose the next question right
is do you actually want to weight the
is do you actually want to weight the
entire segment the Mhm.
You know, maybe you're right, Ryan.
You know, maybe you're right, Ryan.
Maybe the next step is literally to just
Maybe the next step is literally to just
say, "Screw it. We're doing on policy
say, "Screw it. We're doing on policy
algorithms with horribly off policy
algorithms with horribly off policy
data, but we're only going to use the
data, but we're only going to use the
best data, and we're going to hope it
best data, and we're going to hope it
reduces to behavioral cloning,
reduces to behavioral cloning,
right?
right?
I'll be right back.
on policy RL with horribly off policy
on policy RL with horribly off policy
data of everybody calling me an idiot.
data of everybody calling me an idiot.
I'm sure unless I actually make it work.
I'm sure unless I actually make it work.
I think the odds are one in five.
I think the odds are one in five.
One in five odds of having this actually
One in five odds of having this actually
work I'll say
work I'll say
maybe one in four being a little too
maybe one in four being a little too
harsh
kind of a believer in that approach.
kind of a believer in that approach.
So like
So like
the thing that I came across right
the thing that I came across right
the behavioral cloning objective
the behavioral cloning objective
and the uh the policy gradient objective
and the uh the policy gradient objective
are not like fundamentally different.
are not like fundamentally different.
They're just kind of like different rate
They're just kind of like different rate
different um weightings of the same like
different um weightings of the same like
log props.
But I don't see why we can't just try
But I don't see why we can't just try
this
this
and just basically say screw it, right?
and just basically say screw it, right?
And then the idea is maybe on policy
is like you're just trying to grab
is like you're just trying to grab
you're just trying to grab rewards from
you're just trying to grab rewards from
the data you have which is going to be
the data you have which is going to be
whatever the quality of the current
whatever the quality of the current
policy is.
And like obviously if you go reuse past
And like obviously if you go reuse past
old data
old data
that like you're better than
that like you're better than
then that's not going to help, right?
then that's not going to help, right?
That's going to mess you up. But if you
That's going to mess you up. But if you
sample it according to advantage,
doesn't this kind of make sense?
doesn't this kind of make sense?
Because like I've gone through so much
Because like I've gone through so much
literature in the past few days on
literature in the past few days on
OffPaul and it just seems like a shitow.
OffPaul and it just seems like a shitow.
It really just seems like a whole bunch
It really just seems like a whole bunch
of hacks tacked onto a really bad
of hacks tacked onto a really bad
algorithm
algorithm
and then you just spin up a ton of
and then you just spin up a ton of
compute
compute
cuz like base Q-learning works. I
cuz like base Q-learning works. I
implemented it in puffer. works and like
implemented it in puffer. works and like
technically right if you just run it for
technically right if you just run it for
long enough on off policy data it should
long enough on off policy data it should
do something
I think that basically they've just kind
I think that basically they've just kind
of
of
packed a bunch of things onto that and
packed a bunch of things onto that and
then run it for a really long time
try this next I got to commit this
But my point is they're the same thing.
But my point is they're the same thing.
Like they're different waitings of the
Like they're different waitings of the
same objective, right? It's like
same objective, right? It's like
literally just a a log prob.
literally just a a log prob.
It's just like grad log prop or whatever
It's just like grad log prop or whatever
on both.
for the critic. Yeah, I think it's not
for the critic. Yeah, I think it's not
entirely insane.
And I could be wrong, but I think it's
And I could be wrong, but I think it's
not entirely
not entirely
insane.
Where's our PO update thing?
Have to go grab a bunch of old code.
Now we just do
Wouldn't it be funny if this actually
Wouldn't it be funny if this actually
ends up being the thing that works?
Like you have this whole other class of
Like you have this whole other class of
off policy algorithms and it's like
off policy algorithms and it's like
yeah,
yeah,
you kind of just don't do that.
The likelihood of this being the case
The likelihood of this being the case
has gone up in my mind after talking to
has gone up in my mind after talking to
a bunch of people
a bunch of people
and like actually digging through a
and like actually digging through a
bunch of the math reasonably well. I'd
bunch of the math reasonably well. I'd
say
be legitimately mad. Dude, I'm going to
be legitimately mad. Dude, I'm going to
be ecstatic if this works because it
be ecstatic if this works because it
means that we'll have a super simple um
means that we'll have a super simple um
puffer implementation that just solves
puffer implementation that just solves
freaking everything.
I don't have to spend the rest of my
I don't have to spend the rest of my
life doing core RL research, right? If
life doing core RL research, right? If
we just solve the field. I can find
we just solve the field. I can find
other problems to work on.
Cool applications of it as well.
Isn't this it up here?
Isn't this it up here?
All right. So, this is kind of it up
All right. So, this is kind of it up
there. No,
there. No,
like
log
and then you don't need the PO objective
and then you don't need the PO objective
either, right?
Yeah. You don't even need the clipping
Yeah. You don't even need the clipping
of uh PO.
You literally just need then the
uh because the clipping is to keep you
uh because the clipping is to keep you
on policy. We're not doing that.
We'll keep the gradient clip, right?
Oh, literally it's advantage, right?
Oh, literally it's advantage, right?
Where's
on how the heck are you optimizing
on how the heck are you optimizing
advantage?
advantage?
Wait, wait. advantage.
Wait, wait. advantage.
ABS
ABS
some
how is this thing even differentiable
how is this thing even differentiable
again?
MB
Let's
get back hacked.
get back hacked.
Very good. Hurry it up so we can
Very good. Hurry it up so we can
actually get some cool stuff in.
Is this actually differentiable?
Is this actually differentiable?
Confused.
Yeah, I'm not even differentiable,
Yeah, I'm not even differentiable,
right? Wait, how the heck are we
right? Wait, how the heck are we
this stupid? Like, where's this loss
this stupid? Like, where's this loss
even coming from?
advantage times ratio.
Oh, wait. Log ratio.
But isn't this one
But isn't this one
MB?
MB?
Oh, no. Because this is a constant,
Oh, no. Because this is a constant,
right?
right?
So, it's
So, it's
it's just new log probs times advantage.
it's just new log probs times advantage.
Edge
there.
Okay, so that actually is a thing.
Okay, so that actually is a thing.
And now can I put the full loss back?
Okay. So, it doesn't do anything
Okay. So, it doesn't do anything
meaningful.
It is possible I've kind of just broken
It is possible I've kind of just broken
things too much though at this point.
I'm kind of tempted to like start clean
I'm kind of tempted to like start clean
with this,
right?
Yeah, I think we're going to start clean
Yeah, I think we're going to start clean
with this.
with this.
What I'm gonna do
And now I'm just going to copy in the uh
And now I'm just going to copy in the uh
additional bits I suppose. was
Yeah, I think easiest thing is to just
Yeah, I think easiest thing is to just
keep the uh the code that I have like
keep the uh the code that I have like
the gold
the gold
data
and then just put this
and then just put this
more cleanly back into
like this
and then
basically don't change eval stuff at
basically don't change eval stuff at
all.
And all I need to do
uh
uh
this sort of a thing right
at the top of train
Let's say that for now we're just going
Let's say that for now we're just going
to store
by reward.
by reward.
Keep it simple.
We'll just dodo
store by turn plus val.
Right. So now we have
Right. So now we have
we have our gold data.
Then we'll basically do
Then we'll basically do
the exact thing that we have here
the exact thing that we have here
um just with this different data.
um just with this different data.
All right. So
All right. So
I need the defaults.
Yeah. So this is normal PO right?
So this is like our normal training
and then
we literally just swap in
old
Okay, so this is pretty much
Okay, so this is pretty much
everything.
Pretty much everything in here.
Uh, this needs to be set
Okay.
So um
that looks pretty stable. Now obviously
that looks pretty stable. Now obviously
it's not good but uh this should be
it's not good but uh this should be
crashing everything
crashing everything
according to the onpaul theory. So this
according to the onpaul theory. So this
is actually I'm happy with that result.
That solves part pole
That solves part pole
unstable
unstable
does solve it.
does solve it.
Okay. So
well now we have questions, right?
This
break before this, huh?
break before this, huh?
Okay.
Fine.
wrong with this.
wrong with this.
Oh,
you know
what?
This happened.
See?
Fine.
This is it.
That's for now.
Okay.
Okay.
Time for cart pull.
Nothing on breakout.
Nothing on breakout.
Huh?
Huh?
beard.
Well, there's probably like um
Well, there's probably like um
I'm probably not dealing with the empty
I'm probably not dealing with the empty
buffer correctly, right?
go to like
millionish.
Okay.
Not stable. Interesting enough.
Not stable. Interesting enough.
Plus though.
makes it a lot simpler if we just sample
makes it a lot simpler if we just sample
based on reward for a bit, right?
Can we literally just do
Also, this self segment
Also, this self segment
not terrible. All
right.
Ah,
Ah,
okay. Still not stable, but
they did something.
Funny cuz we know the clipping term is
Funny cuz we know the clipping term is
like super important when you're
like super important when you're
normally doing PO
do
You want the log.
This is insta solve territory, right?
end up with a
It crashes,
starts learning.
enough.
Negative.
Negative.
Why is it negative advantage?
Oh, because you want to maximize
Oh, because you want to maximize
advantage, right?
No, hang on.
No, hang on.
Used.
Are you able to explain the intuition
Are you able to explain the intuition
for why you wait geometric lambda?
Um,
Um,
so it comes from I believe it comes from
so it comes from I believe it comes from
eligibility traces.
eligibility traces.
Hang on.
There's this thing.
Is it? Oh, here it is. Yeah. 287.
Yeah. Okay. So
I honestly I don't know whether this is
I honestly I don't know whether this is
like the book was revised after J. I I
like the book was revised after J. I I
don't think so. I think that this is
don't think so. I think that this is
older. Um
older. Um
but you do get the
but you do get the
lambda return formula here.
So, if you want the full math of it,
So, if you want the full math of it,
it's here.
it's here.
If you want the quick version, it's just
If you want the quick version, it's just
it's an exponentially smoothed average
it's an exponentially smoothed average
of exponentially smoothed averages.
What's the exact shape of advantages?
What's the exact shape of advantages?
Um, it's going to have the same shape as
Um, it's going to have the same shape as
the number. It's one advantage per
the number. It's one advantage per
observation, right? So if you have
observation, right? So if you have
trajectory segments like 512 segments of
trajectory segments like 512 segments of
length 64, it's going to be 512 x 64
length 64, it's going to be 512 x 64
a ge looks like directly that to me.
a ge looks like directly that to me.
Kevin,
I'm trying to figure out if we can get,
I'm trying to figure out if we can get,
and this is driving me crazy, but I'm
and this is driving me crazy, but I'm
trying to figure out if we can get
trying to figure out if we can get
um
um
uh on policy learning to work with off
uh on policy learning to work with off
policy data because
policy data because
and it like it actually surprisingly
and it like it actually surprisingly
kind of does in a few different
kind of does in a few different
settings.
going to do a whole bunch of math lately
going to do a whole bunch of math lately
that's convinced me that there should be
that's convinced me that there should be
a version of this that works
log props
I just do
I just do
implicit cross entropy
You just do explicit cross entropy,
You just do explicit cross entropy,
right?
Completely off policy is the goal.
Completely off policy is the goal.
Um the goal here
The goal here
The goal here
is that we maintain a high quality
is that we maintain a high quality
buffer of data
buffer of data
and if we exclusively use that to train
and if we exclusively use that to train
on,
on,
we should be able to bypass the on
we should be able to bypass the on
policy assumption because
policy assumption because
uh the math for behavioral cloning looks
uh the math for behavioral cloning looks
a lot like just a different waiting of
a lot like just a different waiting of
on policy. We're kind of trying to push
on policy. We're kind of trying to push
on policy learning towards instead being
on policy learning towards instead being
behavioral cloning of uh whatever data
behavioral cloning of uh whatever data
you have that's currently higher quality
you have that's currently higher quality
than your average than like average
than your average than like average
policy quality. Okay.
not using. So important sampling is the
not using. So important sampling is the
opposite of what you want. If you
opposite of what you want. If you
actually look at important sampling, it
actually look at important sampling, it
doesn't let you really use off policy
doesn't let you really use off policy
data.
data.
It kind of just throws away off policy
It kind of just throws away off policy
data.
Speaking of which, thanks for the
Speaking of which, thanks for the
reminder. I did forget to uh
reminder. I did forget to uh
disable the important sampling.
disable the important sampling.
I mean, I'm sure it's just getting worse
I mean, I'm sure it's just getting worse
and worse. Oh.
Okay.
Okay.
Did learn for a moment.
Okay. So, you do have now the imitation
Okay. So, you do have now the imitation
loss, right?
and
it's weighted by this
this one.
Now it doesn't learn anymore.
Words do sum
Words do sum
just sorting segments by reward.
And then
thumb.
You're trying to get full-fledged PO.
You're trying to get full-fledged PO.
Well, it's not really going to be PO
Well, it's not really going to be PO
anymore, right? because like the
anymore, right? because like the
clipping term is for keeping your data
clipping term is for keeping your data
on policy. So you're going to delete
on policy. So you're going to delete
that. So you basically are just keeping
that. So you basically are just keeping
the advantage function
reinforced looks exactly like behavioral
reinforced looks exactly like behavioral
cloning weighted by return right and
cloning weighted by return right and
then po if you ignore the clipping the
then po if you ignore the clipping the
other thing is it's um it's weighted by
other thing is it's um it's weighted by
an advantage function right
and like there are algorithms that are
and like there are algorithms that are
based on um like behavioral cloning
based on um like behavioral cloning
weighted by advantage. The question is,
weighted by advantage. The question is,
if we filter our data carefully, can we
if we filter our data carefully, can we
get it to work
get it to work
like without expert data?
like without expert data?
Do we just like full bootstrap?
I've seen like flashes of brilliance
I've seen like flashes of brilliance
from um like this type of stuff already.
from um like this type of stuff already.
It's like definitely not insane.
It's like definitely not insane.
It's just like is there a good version
It's just like is there a good version
of this that I can figure out in a
of this that I can figure out in a
reasonable amount of time?
So why the hell is that E+?
You probably don't want X on that, huh?
You probably don't want X on that, huh?
That probably breaks freaking
That probably breaks freaking
everything.
iteration.
iteration.
Yeah, that's the goal.
Yeah, that's the goal.
Is that what it is called? Expert. Well,
Is that what it is called? Expert. Well,
except the fact that like you're
except the fact that like you're
starting with nothing, right?
Wait, that's definitely not
[Music]
Was
it one at every step?
Huh?
Huh?
Funny.
Literally just least number of falls.
kind of why sampling
needed.
Think the value function would fix this.
Weirdo. I haven't been able to get like
Weirdo. I haven't been able to get like
the simplest thing I tried is the thing
the simplest thing I tried is the thing
that I think has worked the best so far,
that I think has worked the best so far,
but it has blind spots.
but it has blind spots.
Yeah, it's similar to that, right? I'm
Yeah, it's similar to that, right? I'm
trying to do something pretty similar to
See the sampling thing now.
Okay,
keeping around the highest reward
keeping around the highest reward
segments.
What is is it the freaking advantage
What is is it the freaking advantage
waiting that's messing me up?
Okay, so actually the advantage waiting
Okay, so actually the advantage waiting
is messing it up.
If you just grab your higher reward
If you just grab your higher reward
data, it works.
What's the overall goal of this then?
What's the overall goal of this then?
Well, I mean, what I would like to be
Well, I mean, what I would like to be
able to do
able to do
is I would like to be able to
I'd like to be able to get the same PF
I'd like to be able to get the same PF
as my normal on policy learning, but I'd
as my normal on policy learning, but I'd
like to also be able to use old data and
like to also be able to use old data and
thereby crank up compute and train more.
I suppose the bit that's unknown, right,
I suppose the bit that's unknown, right,
is like
is like
well, you don't know this should work.
well, you don't know this should work.
If you're taking your if you're like if
If you're taking your if you're like if
advantage weighted regression, it's
advantage weighted regression, it's
it's basically the exact same objective
it's basically the exact same objective
is on policy learning, right?
basically the same objective.
It's like a negative
It's like a negative
mind flip.
Not just like a dumb sign flip mistake,
Not just like a dumb sign flip mistake,
right?
For some reason,
waiting by the advantage like this.
Kreg
Kreg
PO doesn't have KL rag in the clipped
PO doesn't have KL rag in the clipped
form.
Welcome.
I guess the question is why like every
I guess the question is why like every
single advantage weighted form of this
single advantage weighted form of this
I've tried has not really worked.
Oh yeah.
Why can't I wait this by?
I wonder if it's the normalization being
I wonder if it's the normalization being
screwy
or
um
um
I don't know what the point of it would
I don't know what the point of it would
be.
be.
have kale rag.
have kale rag.
You need it.
You need it.
You're not trying to uh there's no on
You're not trying to uh there's no on
policy thing to do
this form.
this form.
It wouldn't be this norm, right?
It wouldn't be this norm, right?
And this is going to mess up.
And this is going to mess up.
It still got J.
I mean, that did something for a second.
I mean, that did something for a second.
Too big, but um I wonder
I wonder if it's just the uh the norm on
I wonder if it's just the uh the norm on
it.
It's a trivial policy.
You don't want mean advantage at all.
You don't want mean advantage at all.
Can't subtract the mean.
Divide
by maximum.
Yeah, that's not I mean that's not
Yeah, that's not I mean that's not
crashed totally at least for a little
crashed totally at least for a little
bit, but that's not uh stable at all.
Super obnoxious like
literally if you just do this form of
literally if you just do this form of
it.
it.
Yeah, this form is just IIL on good uh
Yeah, this form is just IIL on good uh
on high reward segments, right?
And this works.
Why do we still need an off policy
Why do we still need an off policy
buffer? Uh two reasons. The goal is one
buffer? Uh two reasons. The goal is one
so that we can actually do sample
so that we can actually do sample
efficiency if we want to. we add that
efficiency if we want to. we add that
capability to puffer and uh two is
capability to puffer and uh two is
that uh exploration is really hard when
that uh exploration is really hard when
stuff is really sparse when you don't
stuff is really sparse when you don't
keep around data. Hey Spencer, what's
keep around data. Hey Spencer, what's
up?
I was off policy land hard. Taking top K
I was off policy land hard. Taking top K
leads to no stochasticity.
leads to no stochasticity.
Yeah, I've tried top K a whole bunch and
Yeah, I've tried top K a whole bunch and
it never works.
it never works.
Switch to sack. Now,
Switch to sack. Now,
I I've been looking at a lot of the off
I I've been looking at a lot of the off
policy lit and I I really think that
policy lit and I I really think that
it's just the case that the algorithms
it's just the case that the algorithms
all suck. They let you reuse samples,
all suck. They let you reuse samples,
but they suck. So, like
but they suck. So, like
I at least I could be wrong, but you
I at least I could be wrong, but you
know, here's an experiment for anybody
know, here's an experiment for anybody
who wants to. If you have some off
who wants to. If you have some off
policy implementation that you have, try
policy implementation that you have, try
to get it to match wall clock time with
to get it to match wall clock time with
puffer on anything.
puffer on anything.
Because as far as I've seen, it seems to
Because as far as I've seen, it seems to
me like
me like
it seems to me like it's just a bad
it seems to me like it's just a bad
algorithm that you crank compute
algorithm that you crank compute
through. But you wouldn't want that,
through. But you wouldn't want that,
right? You want the same algorithm that
right? You want the same algorithm that
does in the case where you have a lot of
does in the case where you have a lot of
data, it's fast. In the case where you
data, it's fast. In the case where you
don't have a lot of data, you can spend
don't have a lot of data, you can spend
more compute to still learn.
like switching to S like to this SACE
like switching to S like to this SACE
doesn't do really anything for Yes.
I've tried several things, Spencer.
We doing absolute advantage or we could
We doing absolute advantage or we could
try just um
No. So, right now we're just doing we
No. So, right now we're just doing we
were having so much pro like trouble
were having so much pro like trouble
with advantage that right now this is
with advantage that right now this is
just reward. The thing that does
just reward. The thing that does
consistently work um
consistently work um
is just doing imitation on high reward
is just doing imitation on high reward
segments
segments
like that actually works.
Now the thing is it's probably not going
Now the thing is it's probably not going
to ever be as efficient as waiting by an
to ever be as efficient as waiting by an
advantage estimate because
advantage estimate because
when you just do high reward segments,
when you just do high reward segments,
you're treating the whole segment as
you're treating the whole segment as
good.
positive, negative reward. Now, I think
positive, negative reward. Now, I think
right now we're just doing high positive
right now we're just doing high positive
reward.
I mean, for breakout, it's um
I mean, for breakout, it's um
negative wouldn't even do anything. That
negative wouldn't even do anything. That
wouldn't make a That doesn't even give
wouldn't make a That doesn't even give
you anything.
I'd like to know why waiting by
I'd like to know why waiting by
advantage breaks everything.
Shouldn't it be
if I do
if I do
isn't it positive advantage times mean?
isn't it positive advantage times mean?
Yeah, because it's it should be positive
Yeah, because it's it should be positive
advantage times mean with no norm.
Oh,
okay.
Still not super stable, but um
Still not super stable, but um
that's something, right?
Learn the value network with the
Learn the value network with the
standard algorithm
standard algorithm
and use that. Wait, you learn the value
and use that. Wait, you learn the value
network with the standard algorithm. We
network with the standard algorithm. We
do have the standard, right?
do have the standard, right?
The standard value net is just going to
The standard value net is just going to
predict returns. It's the exact thing we
predict returns. It's the exact thing we
have here. It's a clipped value function
have here. It's a clipped value function
loss.
loss.
Good value network to compute advantage
Good value network to compute advantage
here.
You mean learn?
What does it mean to learn the value
What does it mean to learn the value
network with the standard algorithm? I
network with the standard algorithm? I
think that's what I'm doing.
I had a negative sign because it's it's
I had a negative sign because it's it's
grad of it's negative log props or
grad of it's negative log props or
whatever. I had a negative here because
whatever. I had a negative here because
they changed the objective.
So I mean this is kind of something. Is
So I mean this is kind of something. Is
this consistent?
Yeah, that's consistent. Okay.
Yeah, that's consistent. Okay.
Shouldn't it be negative though?
Shouldn't it be negative though?
I don't think so. Right. For imitation
I don't think so. Right. For imitation
learning, it's advantage times the uh
learning, it's advantage times the uh
behavioral cloning objective. Right. I
behavioral cloning objective. Right. I
think it's not negative advantage times
think it's not negative advantage times
ratio. I think it's supposed to be the
ratio. I think it's supposed to be the
actual losses advantage times negative
actual losses advantage times negative
ratio.
Isn't it grad of like negative log prop
Isn't it grad of like negative log prop
or whatever?
The clipping definitely doesn't make Get
The clipping definitely doesn't make Get
rid of this.
final value network. Yeah.
Right. So that's why I didn't filter
Right. So that's why I didn't filter
based on advantage. I filtered based on
based on advantage. I filtered based on
reward. Right.
reward. Right.
We're up top.
I suppose that is the next thing would
I suppose that is the next thing would
be to
sample based on advantage.
Okay. Similar to before.
So we can sample based on
So we can sample based on
based on this.
What happens if I do cart pull?
kind of janky.
kind of janky.
Yes.
Oh, you know we do actually remember.
Oh, you know we do actually remember.
Hang on.
Bandages get stale,
liked your post on learning programming
liked your post on learning programming
in ML. Thanks. Yeah, no worries.
Really has a lot of my best
Really has a lot of my best
best advice.
Okay. So, this is storing more data than
Okay. So, this is storing more data than
before and doing worse. So, this is 100%
before and doing worse. So, this is 100%
a sampling problem, right?
It's actually better on breakout, which
It's actually better on breakout, which
is funny. despite the sampling problem.
is funny. despite the sampling problem.
I think you just need more data.
It's actually kind of like okay.
Okay. So
Okay. So
the problem with sampling, right?
the problem with sampling, right?
Problem with sampling is
there are a couple things.
Ideally, you want to sample based on
Ideally, you want to sample based on
the advantage, right?
the advantage, right?
That's like information gain.
But advantage
But advantage
advantage can get stale.
You hate AI coding.
Get off an iteration speed. It doesn't
Get off an iteration speed. It doesn't
make it faster. It makes it slower.
Like if you're actually a good
Like if you're actually a good
programmer, it's kind of the same thing
programmer, it's kind of the same thing
as like asking a good mathematically
as like asking a good mathematically
inclined researcher why they don't just
inclined researcher why they don't just
outsource all their math to uh to an an
outsource all their math to uh to an an
LLM to make them be able to do their
LLM to make them be able to do their
math faster, right? It's cuz the LLMs
math faster, right? It's cuz the LLMs
are really stupid.
There's like not a single thing I've
There's like not a single thing I've
done today that an LM would have made
done today that an LM would have made
faster.
faster.
Actually, the the one thing that it does
Actually, the the one thing that it does
make faster is what you see here, which
make faster is what you see here, which
is the single line really stupid
is the single line really stupid
autocompletes that saves me typing
autocompletes that saves me typing
that
Let's just for the heck of it, right?
Let's just update
Let's just update
the value function, right?
Let's just update the value function.
I've coding learned helplessness.
Yeah, I'm very happy just ignoring the
Yeah, I'm very happy just ignoring the
LLM people and telling them they're dumb
LLM people and telling them they're dumb
and like, you know, I'm here. I'm I'll
and like, you know, I'm here. I'm I'll
be waiting on the day that like people
be waiting on the day that like people
are actually writing like good useful
are actually writing like good useful
libraries and and like advancing
libraries and and like advancing
research and science with these things.
research and science with these things.
But for now, it's to me it mostly just
But for now, it's to me it mostly just
seems like a lot of people who are lazy
seems like a lot of people who are lazy
and dumb and decided to not invest time
and dumb and decided to not invest time
in actually learning how things work
in actually learning how things work
feeling smuggly superior because they
feeling smuggly superior because they
can now type in random sentences in
can now type in random sentences in
English and think that they understand
English and think that they understand
how things work and are useful. They're
how things work and are useful. They're
not. They're still stupid and lazy.
like
like
literally just like rldled you into
literally just like rldled you into
feeling like you can do
modify it in place.
modify it in place.
uh it does modify it in place and then I
uh it does modify it in place and then I
added the return afterwards.
So because like people expect functions
So because like people expect functions
to return things I added the return
to return things I added the return
afterwards but yes it does modify it in
afterwards but yes it does modify it in
place
and it has to because otherwise you have
and it has to because otherwise you have
to allocate a fresh tensor which is
to allocate a fresh tensor which is
slower. So you can actually
slower. So you can actually
pre-allocate, which to be fair, we're
pre-allocate, which to be fair, we're
not really doing nicely in the way we
not really doing nicely in the way we
should be doing it here. But
should be doing it here. But
I wrote it that way so I at least I
I wrote it that way so I at least I
could optimize it. And it does make a
could optimize it. And it does make a
difference when you when these
difference when you when these
advantages uh the advantage tensors get
advantages uh the advantage tensors get
too big.
too big.
All right. So,
All right. So,
this should be
good. That's a good slice.
Ah, yeah. It's segments, isn't it?
Ah, yeah. It's segments, isn't it?
Slightly obnoxious.
right adding a little noise to the
right adding a little noise to the
states and actions.
Uh why would you do that?
That would be like an overfitting
That would be like an overfitting
regularization thing, right?
regularization thing, right?
I can't think of why else you would do
I can't think of why else you would do
that.
Okay.
I mean that's nice and stable at least,
I mean that's nice and stable at least,
right?
Maybe less stale data to directly copy.
Maybe less stale data to directly copy.
Less stale?
Less stale?
What do you mean less stale data to
What do you mean less stale data to
directly copy?
directly copy?
Well, here now I know for sure the value
Well, here now I know for sure the value
is not stale.
Oh, but actually no, I did this wrong,
Oh, but actually no, I did this wrong,
right?
right?
Belf gold values.
Can you just update the priority?
Can you just update the priority?
What do you mean update the priority in
What do you mean update the priority in
the tree? So the reason that it gets
the tree? So the reason that it gets
stale
stale
um is like you're storing a cache like
um is like you're storing a cache like
you're caching a value estimate so that
you're caching a value estimate so that
what you can compute advantages,
what you can compute advantages,
right? Because otherwise you have to do
right? Because otherwise you have to do
the full forward pass every time you
the full forward pass every time you
want to compute advantages.
want to compute advantages.
Um,
Um,
and the problem with that is if you're
and the problem with that is if you're
sampling based on advantage, like let's
sampling based on advantage, like let's
say you optimize one data point and
say you optimize one data point and
advantage goes to zero, you're not going
advantage goes to zero, you're not going
to sample it again, even though like
to sample it again, even though like
you're going to forget that data point
you're going to forget that data point
later on for sure.
later on for sure.
But because you haven't updated the
But because you haven't updated the
value, you can't see that.
value, you can't see that.
In practice, it seems like this didn't
In practice, it seems like this didn't
make a big difference. We get pretty
make a big difference. We get pretty
much the same results as before.
much the same results as before.
Insignificantly higher, I'd say.
But this was definitely an issue before.
But this was definitely an issue before.
So, we're at 172
and break out.
Uh there's still more to do here,
Uh there's still more to do here,
though.
though.
We're storing segments based on
storing segments based on reward is kind
storing segments based on reward is kind
of just not optimal, right?
Wouldn't be used. Well, because you you
Wouldn't be used. Well, because you you
have to update the value estimate,
have to update the value estimate,
right? Because if you're not going to
right? Because if you're not going to
sample it, if you're computing estimate
sample it, if you're computing estimate
uh advantage based on the stale value,
uh advantage based on the stale value,
then you're never going to use it. You
then you're never going to use it. You
have to update the value so that when
have to update the value so that when
you update the advantage estimate, you
you update the advantage estimate, you
actually get something recent.
I'll find some hack to get around this
I'll find some hack to get around this
at some point so I don't have to do this
at some point so I don't have to do this
full pass over the buffer, right? But
full pass over the buffer, right? But
this is for dev and can do this now for
this is for dev and can do this now for
dev
172.
172.
Let's
do a cart pull.
do a cart pull.
Actually, not sure if this does cart
Actually, not sure if this does cart
pull because of the way the uh reward
pull because of the way the uh reward
buffer works.
Uh, it does. It is just slow.
Uh, it does. It is just slow.
Interesting.
Interesting.
All right. So, like what portions of
All right. So, like what portions of
this
What portions of the current approach
What portions of the current approach
should make this slower than uh normal
should make this slower than uh normal
PO
advantage waiting applied to samples
advantage waiting applied to samples
elementwise.
elementwise.
That's basically the same loss.
That is basically the same loss.
That is basically the same loss.
I think there's some variance in it.
I think there's some variance in it.
Right.
is it?
is it?
You want to just store all the states?
You want to just store all the states?
Can't store all the states.
What do you want?
What do you want?
You want your data buffer to have
You want your data buffer to have
essentially
essentially
the expert demonstrations, right?
What you want is you want your data
What you want is you want your data
buffer to have the expert
buffer to have the expert
demonstrations.
That would be the high reward. Like
That would be the high reward. Like
that's the high value ones, not the high
that's the high value ones, not the high
advantage ones.
advantage ones.
Right?
So what we have to do then we have to
So what we have to do then we have to
train a separate value function right
wait
returns
now. What are we what are we training
now. What are we what are we training
this thing against?
this thing against?
Value function
advantages
advantages
plus value
New value minus value plus advantage I
New value minus value plus advantage I
guess.
guess.
Yes. So your value function is
better off. You're doing off Paul. No
better off. You're doing off Paul. No
expertise
expertise
be better.
be better.
Uh we have we have prioritized
Uh we have we have prioritized
experience replay.
experience replay.
It's it's um technically it's behavioral
It's it's um technically it's behavioral
cloning. The objective is behavioral
cloning. The objective is behavioral
cloning, not off policy. Uh not off
cloning, not off policy. Uh not off
policy today
policy today
based on some stuff I found.
based on some stuff I found.
And like we actually have it learning
And like we actually have it learning
some stuff which is kind of crazy.
some stuff which is kind of crazy.
Like literally this is a behavioral
Like literally this is a behavioral
cloning objective, right?
You're literally doing cross entropy
You're literally doing cross entropy
between logits and actions. You're
between logits and actions. You're
waiting by advantage.
Hang on. Am I wrong? Is the value
Hang on. Am I wrong? Is the value
function.
function.
But what is the value function learning
But what is the value function learning
to predict in?
Is it learning to predict discounted
Is it learning to predict discounted
returns or is it learning to predict? It
returns or is it learning to predict? It
is learning to predict just discounted
is learning to predict just discounted
returns, right?
Yeah.
So in that case, hang on.
So in that case, hang on.
In that case, I know what to do, right?
We update the values for everything.
We update the values for everything.
Yes.
know what to do. We move this up.
Okay. So you're updating your all values
Okay. So you're updating your all values
estimate,
estimate,
right? And then we do prayer
Plus.
So this is return plus value
Bad 189.
This is how you'd want to store it,
This is how you'd want to store it,
right?
you'd want to use um this metric.
you'd want to use um this metric.
Okay. Okay. And then the only the only
Okay. Okay. And then the only the only
thing here now is
uh the advantage based sampling I
uh the advantage based sampling I
believe right
believe right
and then how we get this correct.
Okay. I mean this works pretty much like
Okay. I mean this works pretty much like
it did before.
So now we are sorting by
So now we are sorting by
or now we are prioritizing by advantage.
I think we also did.
Let's see if this does anything.
Wait it by priority, you know.
Oh yeah.
Oh yeah.
Okay.
Okay.
over 200 now, right?
Uh, advantages are not normalized.
Uh, advantages are not normalized.
I think normalizing advantages breaks it
I think normalizing advantages breaks it
though, right?
Yeah. So, normalizing advantages
breaks it.
So, we're going to need something else
So, we're going to need something else
in place of that
in place of that
cuz that's like a major major thing.
cuz that's like a major major thing.
But being able to get over 200 on
But being able to get over 200 on
breakout already,
breakout already,
pretty damn good.
pretty damn good.
Uh, we can get rid of this log ratio
Uh, we can get rid of this log ratio
stuff, right?
Don't need any of this.
There. No more tails.
Mhm.
Mhm.
Uh, of course there is like the question
it's technically possible that we're
it's technically possible that we're
just like using the off the on Paul data
just like using the off the on Paul data
anyways
anyways
and that's why it's stable.
Wouldn't think so though.
sampling by
sampling by
I probably should be doing absolute
I probably should be doing absolute
value of advantage. I'm just doing
value of advantage. I'm just doing
advantage now if I recall correctly.
advantage now if I recall correctly.
Some sort of bounce positive and
Some sort of bounce positive and
negative.
negative.
I think it's all positive. Check.
I think it's all positive. Check.
Positive data is better than negative
Positive data is better than negative
data, right?
I think for behavioral cloning you
I think for behavioral cloning you
actually you do want positive data
actually you do want positive data
because you want like it's not that
because you want like it's not that
informative to have a data set of
informative to have a data set of
negatives.
negatives.
You really just learn from the positive
You really just learn from the positive
examples.
Oh, you know what? It's currently
Oh, you know what? It's currently
sampling by absolute value. You're
sampling by absolute value. You're
right.
I mean, it's technically
be surprised if this is
be surprised if this is
that different. We'll see.
All right. I don't think that's a
All right. I don't think that's a
statistically different. I think that's
statistically different. I think that's
just in the noise.
So, okay. I guess then the question is
So, okay. I guess then the question is
why
why
why are we not as good as
why are we not as good as
our main
Hang on. We also have tuned parameters
Hang on. We also have tuned parameters
for a totally different algorithm now,
for a totally different algorithm now,
right?
These two co-ops for example
hopefully odd. It's um
hopefully odd. It's um
it's less than one.
That's prioritized experience replay.
That's prioritized experience replay.
There's nothing custom there.
Okay, that's the same as before,
Okay, that's the same as before,
but um off Paul correction is totally
but um off Paul correction is totally
bad for this. So, I think this is good.
bad for this. So, I think this is good.
And then
like we should be able to increase
like we should be able to increase
update epochs. Now, I don't know if this
update epochs. Now, I don't know if this
actually works, but we should be able to
actually works, but we should be able to
do like update epoch equals 4,
do like update epoch equals 4,
right? Like, shouldn't this work Now,
And it crashes. Oh, it nanned out
And it crashes. Oh, it nanned out
though.
It's possible we just have to sweep this
It's possible we just have to sweep this
thing.
We have some sanities we can do, right?
we can do as a sanity check here, right?
What if we run this?
I mean, okay, this is um
I mean, okay, this is um
this says that this is better our off uh
this says that this is better our off uh
our off data our off data version is
our off data our off data version is
better than our onpaul data version.
better than our onpaul data version.
So, that could just be a matter of
So, that could just be a matter of
tuning.
We've changed a whole bunch of stuff.
use our restroom real quick and then I'm
use our restroom real quick and then I'm
going to see if I can set up um
going to see if I can set up um
I'm going to do a few more manual
I'm going to do a few more manual
experiments, I think, and then try to
experiments, I think, and then try to
see if I can set up some some form of
see if I can set up some some form of
sweep.
sweep.
I'll be right back. This potentially
I'll be right back. This potentially
pretty uh pretty novel though.
How cool would it be if this worked?
How cool would it be if this worked?
I mean, there's some tests we should be
I mean, there's some tests we should be
able to do.
It should get, if you have your sampling
It should get, if you have your sampling
set up correctly, it should get strictly
set up correctly, it should get strictly
better with um a bigger buffer size,
better with um a bigger buffer size,
right?
Make sure we can get this to work.
So, this one should do worse, right?
So, uh, this kind of stalled out.
So, uh, this kind of stalled out.
It only has a million samples to train
It only has a million samples to train
on.
on.
Okay.
Ignoring
the inefficiency of this implementation.
the inefficiency of this implementation.
If this does not do better, it is a
If this does not do better, it is a
sampling issue.
Not bad. 267.
And then this also should let me run
And then this also should let me run
more updates in turn.
Of course, there's a lot of noise in
Of course, there's a lot of noise in
these, right? So, if it doesn't do
these, right? So, if it doesn't do
strictly better, um, if it crashes, we
strictly better, um, if it crashes, we
know there's a bug
from variety.
Okay. So, like this just didn't sample
Okay. So, like this just didn't sample
well.
We'll go back to 8 mil for now.
Do two update epox
changes anything.
Which plugin? Uh, this is Super Maven
44. Okay, so this didn't do any better
44. Okay, so this didn't do any better
with more updates.
Fix that.
Oh, you know why? It's because I'm a
Oh, you know why? It's because I'm a
dummy. Literally not changing anything.
or at least for that test.
Okay. So, not really
Okay. So, not really
not really any better, right?
not really any better, right?
just with these this setup.
We get to get rid of a whole bunch of
We get to get rid of a whole bunch of
the um
the um
the hypers though, don't we?
Coefficient
Good number of parameters here
Good number of parameters here
be changed.
It actually does kind of concern me,
It actually does kind of concern me,
right, that we don't do.
right, that we don't do.
We should do better with more update
We should do better with more update
epochs in this, shouldn't we?
At least we should be able to get to
At least we should be able to get to
like
like
some no form of sample reuse.
This at least needs to be stable, right?
Nan's out. So this is like learning rate
Nan's out. So this is like learning rate
pars and such I suppose.
pars and such I suppose.
Um
Um
we can tune those. I am a little
we can tune those. I am a little
curious.
Behavioral cloning is supposed to scale
those. I already have these.
those. I already have these.
Uh, this is supposed to scale better,
Uh, this is supposed to scale better,
isn't it?
I don't see how it possibly could
I don't see how it possibly could
because
because
Oh, maybe.
Of course, that's going to change
Of course, that's going to change
optimal learning, right? And a bunch of
optimal learning, right? And a bunch of
other crap
and not really a distinct advantage,
and not really a distinct advantage,
huh?
huh?
Okay.
Well, let's just do um
just do the easy to sweep ones, right?
We'll leave these alone.
Okay.
I would like to have seen um I think
I would like to have seen um I think
like a slightly higher number
like a slightly higher number
in order to be reasonably uh confident.
I mean, but this is kind of a crazy
I mean, but this is kind of a crazy
thing to do.
So
I suppose the next thing is to just
I suppose the next thing is to just
think about if there are any fundamental
think about if there are any fundamental
things that make this more or less
things that make this more or less
powerful.
I mean the pros here right the thing
I mean the pros here right the thing
that this has going for it is data reuse
that this has going for it is data reuse
right that's the key
and it's data use at the level of the
and it's data use at the level of the
trajectory segment
compared to the PO objective.
I mean it's
the thing is the behavioral cloning
the thing is the behavioral cloning
objective is so close to the online
objective is so close to the online
objective that it should learn better
objective that it should learn better
with online data, right?
Can we think of it that way?
It makes sense, right? Because you get
It makes sense, right? Because you get
you get like a direct gradient
based off of your actions, rewards.
The other form of this I suppose would
The other form of this I suppose would
be to do explicit
on policy and behavioral clothing,
on policy and behavioral clothing,
right?
That would be the next thing to try.
That would be the next thing to try.
would be to make it explicit, right?
would be to make it explicit, right?
the separate phases.
I mean that still pairs relatively
I mean that still pairs relatively
nicely, right?
So this for this first piece is
So this for this first piece is
literally
literally
it's the same right.
The only difference is they put the
The only difference is they put the
advantage in here
versus behavioral cloning. It's one,
versus behavioral cloning. It's one,
right?
That's the only difference.
So in this setup, you're not assuming
So in this setup, you're not assuming
that the data is better than the current
that the data is better than the current
policy, right?
policy, right?
In the online RL setup, you're not
In the online RL setup, you're not
assuming that the data is better than
assuming that the data is better than
the current policy. In fact,
the current policy. In fact,
it's it is data from the current policy.
it's it is data from the current policy.
The some of it's better than average,
The some of it's better than average,
some of it's worse than average, and a
some of it's worse than average, and a
lot of it's pretty average
in the behavioral cloning setup.
in the behavioral cloning setup.
you're assuming that the data is just
you're assuming that the data is just
better than the current policy.
Of course, like the naive,
Of course, like the naive,
let's say we train a um
let's say we train a um
a good advantage function, right?
The typical behavioral cloning setup, we
The typical behavioral cloning setup, we
could use maybe the top x% of the data.
could use maybe the top x% of the data.
Whereas here, you get to use all the
Whereas here, you get to use all the
data.
In the typical behavioral cloning setup,
In the typical behavioral cloning setup,
you don't weight it by advantage, though
you don't weight it by advantage, though
you can weight it by advantage as well.
you can weight it by advantage as well.
Be found as long as you're careful about
Be found as long as you're careful about
it.
We also know that behavioral cloning is
We also know that behavioral cloning is
a pretty strong baseline, right?
When DDPG Okay.
I think this is actually not like a
I think this is actually not like a
sample efficiency thing, right?
Yeah, this is not like a sample
Yeah, this is not like a sample
efficiency thing. This is actually kind
efficiency thing. This is actually kind
of more like puffer style. just happens
of more like puffer style. just happens
to be um
actually the fact that they report all
actually the fact that they report all
their stuff in uh in seconds actually
their stuff in uh in seconds actually
pretty much guarantees that this is
pretty much guarantees that this is
or a puffer style thing. So looking at
or a puffer style thing. So looking at
this for sample F is probably not great
this for sample F is probably not great
unless they have a comparison.
Yeah, this is I think they just took
Yeah, this is I think they just took
some random offpaul algorithm, right?
some random offpaul algorithm, right?
They kind of did some similar stuff to
They kind of did some similar stuff to
what Puffer does to make it fast.
Yeah, that's totally what they did.
Yeah, that's totally what they did.
Okay. So, I guess like
this paper is still one of my favorites,
but it's so tough to judge things like
but it's so tough to judge things like
this because
I mean they have the wall time,
I mean they have the wall time,
but they cut samples so you don't
but they cut samples so you don't
actually know like
Like, can this do better than puffer
Like, can this do better than puffer
given the same amount of wall clock
given the same amount of wall clock
on like one of our tasks?
like a few thousand steps per second. If
like a few thousand steps per second. If
I recall
I mean, this is a good paper. It's just
I mean, this is a good paper. It's just
um
tough to compare here, All right.
and honestly the biggest improvement was
and honestly the biggest improvement was
just swapping the uh the architecture.
just swapping the uh the architecture.
Funny enough,
we could pretty easily do architecture
we could pretty easily do architecture
stuff.
stuff.
I'll look at sample efficiency.
The thing that's like obnoxious I think
The thing that's like obnoxious I think
is just that um
the sample efficiency setup
at least as it's normally defined as a
at least as it's normally defined as a
hardware inefficient setup.
Guess it doesn't have to be, right?
Just like train reasonable batch sizes.
Just like train reasonable batch sizes.
Don't have a ton of parallel ends,
Don't have a ton of parallel ends,
but then you use more compute on the
but then you use more compute on the
train pass anyways.
train pass anyways.
Fire replay buffer.
Fire replay buffer.
It's actually kind of feasible.
It's actually kind of
Five.
H.
It's going to be very difficult to beat
It's going to be very difficult to beat
like our optimized on policy setup with
like our optimized on policy setup with
this. Frankly, it could happen. I'd be
this. Frankly, it could happen. I'd be
surprised.
surprised.
I would like to see how well this does.
Wonder if it was mistaken to try to do
Wonder if it was mistaken to try to do
um
um
the on policy
set up.
set up.
Well, actually, you know what?
Well, actually, you know what?
The other thing that just is tough,
The other thing that just is tough,
um,
it's fully an artifact of the LSTM,
it's fully an artifact of the LSTM,
right?
Yeah. Because we have an LSTM, right?
Yeah. Because we have an LSTM, right?
You get off policy. You mess up your
You get off policy. You mess up your
LSTM state. Like, regardless of the
LSTM state. Like, regardless of the
algorithm you use, you're going to mess
algorithm you use, you're going to mess
up your LSTM state a bunch.
That's not great.
It's actually kind of a motivation for
It's actually kind of a motivation for
transformerbased architecture, isn't it?
Efficient at smaller batches.
possible.
Let's get messed up with that. Not
Let's get messed up with that. Not
really. Right.
really
is just deploy
is just deploy
but it actually probably the LSTM
but it actually probably the LSTM
architecture is probably driving me
architecture is probably driving me
towards on Paul more than I normally
towards on Paul more than I normally
would be.
would be.
I think that the
well the key finding here right
well the key finding here right
and this was the key finding from a
and this was the key finding from a
couple of days ago at this point but um
couple of days ago at this point but um
even doing a super super simple
even doing a super super simple
behavioral cloning thing with just the
behavioral cloning thing with just the
top whatever percent of your data even
top whatever percent of your data even
just by reward.
just by reward.
uh that already gives you qualitatively
uh that already gives you qualitatively
and in some very important cases better
and in some very important cases better
behavior than doing soda on policy
behavior than doing soda on policy
learning.
learning.
Very difficult to get it to solve like
Very difficult to get it to solve like
full tasks.
full tasks.
Sufficient motivation to include it
Sufficient motivation to include it
though in some capacity.
I think the formulation I came up with
I think the formulation I came up with
today is quite good for the uh the
today is quite good for the uh the
buffer.
Right. The buffer being
Right. The buffer being
stored by uh reward plus terminal value
stored by uh reward plus terminal value
instead of by advantage.
instead of by advantage.
That's really how you would do it if you
That's really how you would do it if you
wanted to build up like a good data set
wanted to build up like a good data set
over time, right?
I think we'll try tomorrow.
I think we'll try tomorrow.
We'll try to do it in phases.
We'll try to do
We'll try to do
our current on policy
our current on policy
uh with this extra
uh with this extra
like additional gold data set and we'll
like additional gold data set and we'll
do it like as an additional step.
We'll compare that.
We'll compare that.
That should be a lot easier to get to
That should be a lot easier to get to
work.
Unless this thing just happens to crush
Unless this thing just happens to crush
it, which I doubt.
it, which I doubt.
This break
This break
been on 23 runs for a while, hasn't it?
How' I know
the heck is this?
Somehow managed to screw something up
Somehow managed to screw something up
there.
want it to stay
8:16.
start on this now or no.
I suppose I could start on it now.
Kind of want to just do have all day
Kind of want to just do have all day
tomorrow to do it though.
for a bit.
I mean, that should have to work, right?
I mean, that should have to work, right?
And I think that the mistake I made last
And I think that the mistake I made last
time was I didn't train the uh
time was I didn't train the uh
I didn't train the value function
I didn't train the value function
when I tried this.
That does leave us with an annoyingly
That does leave us with an annoyingly
complicated algorithm, I guess. But
complicated algorithm, I guess. But
I think we'll be able to strip off
I think we'll be able to strip off
things we don't need over time from
things we don't need over time from
there.
Okay, I think that's what I'm going to
Okay, I think that's what I'm going to
do. I think I'm going to call it early.
do. I think I'm going to call it early.
I'm going to get myself an early dinner.
I'm going to get myself an early dinner.
Um,
I'm going to come back tomorrow
and we'll have a full day to just crank
and we'll have a full day to just crank
this stuff out.
this stuff out.
And hopefully by the end of tomorrow I
And hopefully by the end of tomorrow I
have at least a usable version of
have at least a usable version of
uh, behavioral cloning integration.
uh, behavioral cloning integration.
Cut off that
Cut off that
working later.
Well,
I suppose I'm biased towards thinking
I suppose I'm biased towards thinking
that there is a very simple method that
that there is a very simple method that
should just work. I shouldn't just have
should just work. I shouldn't just have
to keep bolting stuff on
to keep bolting stuff on
to the algorithm.
to the algorithm.
The behavioral cloning one.
It's not that bad. I suppose
I was hoping we would get like one
I was hoping we would get like one
algorithm, right?
That could do uh that could do both.
Actually, I think that this will be the
Actually, I think that this will be the
last question I'll have for the stream
last question I'll have for the stream
if um if anybody has any ideas today. So
if um if anybody has any ideas today. So
the setup here right is that you have a
the setup here right is that you have a
whole section of researchers who have
whole section of researchers who have
been aggressively optimizing for sample
been aggressively optimizing for sample
efficiency working on off policy
efficiency working on off policy
algorithms and to them like compute is
algorithms and to them like compute is
free right like compute is just free so
free right like compute is just free so
they've been like writing these
they've been like writing these
ridiculous not that great algorithms but
ridiculous not that great algorithms but
that let you infinitely reuse data Um
that let you infinitely reuse data Um
and therefore like by doing that you can
and therefore like by doing that you can
get pretty good. And then there's a
get pretty good. And then there's a
bunch of on policy research some of
bunch of on policy research some of
which is misguidedly on sample
which is misguidedly on sample
efficiency but a lot of it's just like
efficiency but a lot of it's just like
can we get the thing to learn fast
can we get the thing to learn fast
that's what we do in puffer
that's what we do in puffer
and this works very very well.
and this works very very well.
The problem is, at least as far as I've
The problem is, at least as far as I've
seen, it doesn't seem to me that you can
seen, it doesn't seem to me that you can
just take uh one of the best off policy
just take uh one of the best off policy
algorithms and just like run it fast on
algorithms and just like run it fast on
uh a ton of fresh data and have it do as
uh a ton of fresh data and have it do as
well as our on policy. And likewise, you
well as our on policy. And likewise, you
know, we can optimize our stuff for
know, we can optimize our stuff for
sample efficiency a bit, but we can't
sample efficiency a bit, but we can't
just go match the sample efficiency of
just go match the sample efficiency of
algorithms that reuse data. So how do we
algorithms that reuse data. So how do we
bridge this gap without doing something
bridge this gap without doing something
horrendous?
Right?
Right?
That is the key research question for
That is the key research question for
right now.
I keep finding skeletons in the closet
I keep finding skeletons in the closet
like freaking everywhere with this stuff
like freaking everywhere with this stuff
as well.
There's no code for this thing, is
There's no code for this thing, is
there?
there?
There is code for this thing.
I mean, I guess one thing I could do
I could take some time.
I could actually assign this to uh one
I could actually assign this to uh one
of my
stupid link.
Why can't I copy a link? This
Well, this is super simple.
So, um maybe what I should do is I
So, um maybe what I should do is I
should take
should take
I should take this
and like run this on
and like run this on
one of my buffer environments.
Like a relatively simple script.
Like a relatively simple script.
I didn't know that this had code
I didn't know that this had code
released.
Maybe we just hack this, right?
So what I would do is I'd take this
So what I would do is I'd take this
thing
to reoptimize hypers as well is the
to reoptimize hypers as well is the
problem
problem
I was trying to figure out right like if
I was trying to figure out right like if
I take this thing
I take this thing
And I run it at like puffer batch sizes.
If that still does something.
It's actually kind of tricky. Okay.
and actually come to think of it like
the thing I was thinking about doing
the thing I was thinking about doing
tomorrow. Does that even make sense?
like yes technically it le it allows you
like yes technically it le it allows you
to leverage some off policy
to leverage some off policy
data through behavioral cloning and I
data through behavioral cloning and I
think it would improve our on policy
think it would improve our on policy
stuff in a few cases
stuff in a few cases
um
don't think that gets you sample f
don't think that gets you sample f
though right
maybe it does
I guess the idea would be that you
you do most of the steps on
you do most of the steps on
the behavioral cloning objective.
A lot of work to do, frankly.
Okay. Well, that's interesting at least.
It actually be really tough to like do
It actually be really tough to like do
apples to apples of like any of you
apples to apples of like any of you
like let's say that we get BTR code
like let's say that we get BTR code
basically have to get it into puffer
basically have to get it into puffer
the whole thing.
Hope to not screw it up.
Hope to not screw it up.
There a safer way I can like test this.
I mean, yeah, right. I would just set
I mean, yeah, right. I would just set
the batch sizes to something crazy.
the batch sizes to something crazy.
Use um puffer networks.
See how it how quick it is and how well
See how it how quick it is and how well
it does.
There's not going to be an optimized
There's not going to be an optimized
implication.
Well, actually, that's kind of okay,
Well, actually, that's kind of okay,
right? I'll run it for if I run it for
right? I'll run it for if I run it for
the same number of steps,
the same number of steps,
I assume that I can optimize it to that
I assume that I can optimize it to that
point. It's good.
That might be a more practical thing to
That might be a more practical thing to
do tomorrow.
Regardless, I'm going to need a few more
Regardless, I'm going to need a few more
days to spend on um on this research
days to spend on um on this research
before I really get something. I think
uh I think it would also be interesting
uh I think it would also be interesting
to see if I threw this on neural MMO
to see if I threw this on neural MMO
if it does the same thing that the
if it does the same thing that the
behavioral cloning objective did,
behavioral cloning objective did,
which was um
which was um
to like start leveling up very very
to like start leveling up very very
early on.
I do think though that this is like
a weird in between.
literally half of the perf from
not half. See?
not half. See?
Yeah. Half of the Perf
Yeah. Half of the Perf
is just the network architecture.
Bunch of other tricks.
Lower.
Well,
it is Friday. I'm tired. Um,
it is Friday. I'm tired. Um,
been doing crazy research all week.
been doing crazy research all week.
Huh.
I want to get this run to be at least
I want to get this run to be at least
stable. Actually, would be good to have
stable. Actually, would be good to have
this thing actually tuned correctly.
this thing actually tuned correctly.
Get us like some interesting results.
Get us like some interesting results.
All right, though. I'm going to just
All right, though. I'm going to just
call it here and go get my dinner. So,
call it here and go get my dinner. So,
thank you folks for tuning in.
thank you folks for tuning in.
Um,
Um,
I will be back likely first thing in the
I will be back likely first thing in the
morning.
morning.
Interested in the research generally?
Interested in the research generally?
Buffer.ai for all the things or the
Buffer.ai for all the things or the
GitHub to help me out for free. Discord
GitHub to help me out for free. Discord
if you want to get involved. Amen.

Kind: captions
Language: en
Okay,
Okay,
be live here.
be live here.
Hi.
A whole bunch of research I've been
A whole bunch of research I've been
looking at.
I want to go back today to um
want to go back to this
heck.
heck.
It'd
be fine.
Okay. So this was the imitation result
Okay. So this was the imitation result
from before.
It's kind of funny how this was uh
It's kind of funny how this was uh
immediately.
It's like so much more promising than
It's like so much more promising than
the offpaul, isn't it?
It also did better than nothing on
It also did better than nothing on
Breakout.
Okay. So, I think what we're going to do
Okay. So, I think what we're going to do
is we're going to train an advantage
is we're going to train an advantage
function
function
and we're going to wait the loss by the
and we're going to wait the loss by the
advantage function.
advantage function.
Let's see what that does for us.
Welcome, Adrian.
Welcome, Adrian.
Gold stands for good.
Gold stands for good.
Like the actual good ones.
Like the actual good ones.
Um,
honestly, we can probably take a lot of
honestly, we can probably take a lot of
this stuff, can't we?
Yeah, really. I just need the um
need this entire block.
Okay. So we can do this on
Okay. So we can do this on
old values
words.
Fine.
And
And
believe we already messed with Yeah. So,
believe we already messed with Yeah. So,
we already have the gold
we already have the gold
data.
We can start with this, right?
Ah,
Gold is equal to advantage filtering.
Gold is equal to advantage filtering.
No, I'm doing imitation stuff. So, it's
No, I'm doing imitation stuff. So, it's
an experimental. I should have probably
an experimental. I should have probably
clarified. I'm kind of tired today. Um,
clarified. I'm kind of tired today. Um,
so what I'm doing at the moment, it's
so what I'm doing at the moment, it's
it's imitation learning without an
it's imitation learning without an
expert.
expert.
The idea is, you know, you collect some
The idea is, you know, you collect some
data, you sort it to see what data is
data, you sort it to see what data is
actually good, and then you just do
actually good, and then you just do
imitation learning on the good samples
imitation learning on the good samples
with some waiting.
with some waiting.
I've been going through over the last
I've been going through over the last
couple days like a whole bunch of math.
couple days like a whole bunch of math.
And it seems like there's a very close
And it seems like there's a very close
equivalence between behavioral cloning
equivalence between behavioral cloning
and policy gradient methods. It's just
and policy gradient methods. It's just
that the derivation that you get out of
that the derivation that you get out of
policy gradients assumes that your data
policy gradients assumes that your data
is on policy. Behavioral cloning
is on policy. Behavioral cloning
doesn't. But they seem like um just
doesn't. But they seem like um just
different weightings of the same
different weightings of the same
objective when you actually write it
objective when you actually write it
out. So I'm basically I'm trying to see
out. So I'm basically I'm trying to see
if we can break the uh the need for the
if we can break the uh the need for the
on policy assumption.
Yeah. So, I mean, this has been the main
Yeah. So, I mean, this has been the main
thing I've been trying to figure out um
thing I've been trying to figure out um
over the past several days is like how
over the past several days is like how
do we drop the need for the on policy
do we drop the need for the on policy
assumption? Interesting
that this works perfectly.
math blogs if any. Well, if it works,
math blogs if any. Well, if it works,
there will be one, right?
there will be one, right?
I've seen that this is like a
I've seen that this is like a
qualitatively different type of
qualitatively different type of
algorithm already. There's something
algorithm already. There's something
here, but I know that I know that that
here, but I know that I know that that
we can get something out of this. The
we can get something out of this. The
question is just how because I've seen
question is just how because I've seen
some I've seen some pretty crazy results
some I've seen some pretty crazy results
already.
But they're holes.
Okay,
Okay,
at least it's more stable here.
There are a few different ways I can
There are a few different ways I can
incorporate this, right?
incorporate this, right?
Let's get the prioritized um sampling
Let's get the prioritized um sampling
into this.
Let's get the prioritized sampling.
Okay. So, you get the priority and you
Okay. So, you get the priority and you
get an index.
You have this absolute advantage here.
like Yes,
need to move to a harder task. I think.
Yeah, this will be a better um starting
Yeah, this will be a better um starting
point.
I don't know why it is, but like
I don't know why it is, but like
breakout seems the minimal task where if
breakout seems the minimal task where if
it works on breakout, it probably works
it works on breakout, it probably works
on a lot of stuff.
on a lot of stuff.
If you try stuff on Pong, it works on
If you try stuff on Pong, it works on
Pong. It doesn't necessarily work on a
Pong. It doesn't necessarily work on a
lot of other stuff,
right? So, this is
right? So, this is
our 20 whatever it is for
do our prioritize sampling.
I do wrong.
Okay. So, it's not a huge difference.
Okay. So, it's not a huge difference.
Interesting.
Go back to cartpole for a little bit.
Definitely do something.
Okay. So,
Okay. So,
yeah, this batch size is freaking tiny,
yeah, this batch size is freaking tiny,
right?
even have to be.
even have to be.
This doesn't even have to be
This doesn't even have to be
like this, right?
We have a whole bunch of samples now,
We have a whole bunch of samples now,
right?
right?
5 mil. This should get slow.
Yeah. So now this is super slow.
this to one.
That gets you
prioritize sampling.
Possibly not very good. Prioritize
Possibly not very good. Prioritize
sampling.
I awaits.
Where's A
I need to come up with some values for
I need to come up with some values for
this, right?
Oh, so this MB pryo actually doesn't
Oh, so this MB pryo actually doesn't
even come in at all, right?
even come in at all, right?
It's just the index.
It's just the index.
And just get rid of this for now.
Okay.
This is still not waiting them yet
This is still not waiting them yet
though, right?
Hey, Dash.
Weird exponential waiting, huh?
they use for beta
4.05
You're recomputing as well every time.
You're recomputing as well every time.
No.
No.
What if we just did top K?
Why not prioritize replay? You're
Why not prioritize replay? You're
literally looking at prioritize replay
literally looking at prioritize replay
right here.
It's hard to reason about. So, I'm
It's hard to reason about. So, I'm
testing a few other things.
You just do top K.
Oh, wait. Should it not be absolute
Oh, wait. Should it not be absolute
value?
Maybe it shouldn't be absolute value.
Maybe it shouldn't be absolute value.
Hang on. Yeah. Yeah. Yeah. It shouldn't
Hang on. Yeah. Yeah. Yeah. It shouldn't
be absolute value.
This is why we need to just have a top K
This is why we need to just have a top K
baseline.
used it with TD error.
used it with TD error.
Yeah, because we don't have um
what you what you
what you what you
the one in Rainbow is like a two or what
the one in Rainbow is like a two or what
is it? Three-step bootstrap.
is it? Three-step bootstrap.
I've been doing a whole bunch of looking
I've been doing a whole bunch of looking
at the math and the conclusion that I've
at the math and the conclusion that I've
come to basically is I think that a lot
come to basically is I think that a lot
of these off policy methods kind of just
of these off policy methods kind of just
suck. Like the update that they do just
suck. Like the update that they do just
sucks. Um but and the reason that it
sucks. Um but and the reason that it
sucks is because they have like a very
sucks is because they have like a very
short bootstrap interval and they get
short bootstrap interval and they get
around it just because since it's on
around it just because since it's on
policy they can just crank more and more
policy they can just crank more and more
compute into it
compute into it
because like at least as far as I can
because like at least as far as I can
see how on earth are you supposed to get
see how on earth are you supposed to get
a good how on earth are you supposed to
a good how on earth are you supposed to
get good updates out of like a
get good updates out of like a
three-step bootstrap
and the three-step bootstrap is already
and the three-step bootstrap is already
cheating because that's already off
cheating because that's already off
policy in a way that the algorithm like
policy in a way that the algorithm like
in a way that EQN can't even tolerate.
in a way that EQN can't even tolerate.
Like the only thing that actually should
Like the only thing that actually should
work is um the onestep bootstrap.
work is um the onestep bootstrap.
Hey, convert
Hey, convert
automate homestead
automate homestead
found another. What do you do for work
found another. What do you do for work
that you're getting all the uh this
that you're getting all the uh this
land? That is awesome. Land is very
land? That is awesome. Land is very
good. I miss my uh my family's farm.
good. I miss my uh my family's farm.
I've been here, you know, in in Palo
I've been here, you know, in in Palo
Alto for a bit.
Alto for a bit.
What's a three-step bootstrap? All
What's a three-step bootstrap? All
right, you guys can tell me if this is
right, you guys can tell me if this is
crazy or not, but I've been looking
crazy or not, but I've been looking
through tons and tons of math.
So, pretty much
So, pretty much
all the off policies stuff does some
all the off policies stuff does some
form of
form of
this endstep bootstrap right here. This
this endstep bootstrap right here. This
multi-step learning
multi-step learning
Okay,
Okay,
but the thing is they set n equals like
but the thing is they set n equals like
three.
three.
So instead of it being
So instead of it being
a literal one-step bootstrap where it's
a literal one-step bootstrap where it's
just one state like one reward gamma
just one state like one reward gamma
maxus Q whatever um
maxus Q whatever um
you know you go forward a few steps
you know you go forward a few steps
and now this data
this data did not actually come from the
this data did not actually come from the
policy. So technically you should
policy. So technically you should
already have to import and sample this
but yeah
but yeah
for a truck from continental coolep
for a truck from continental coolep
learning is stops after eight. Yeah. So
learning is stops after eight. Yeah. So
that's the problem, right? The reason
that's the problem, right? The reason
that it stops after 8 is because it
that it stops after 8 is because it
actually does break it breaks the
actually does break it breaks the
assumption of the algorithm, right?
assumption of the algorithm, right?
The reason DQN works for O policy is
The reason DQN works for O policy is
that it's a one-step bootstrap.
If you do a multi-step bootstrap,
when you do like an endstep bootstrap
when you do like an endstep bootstrap
like this,
like this,
you have to you technically are supposed
you have to you technically are supposed
to apply um important sampling to the
to apply um important sampling to the
data to correct,
data to correct,
right? And here it's just okay that you
right? And here it's just okay that you
don't do that, but whatever. It's only
don't do that, but whatever. It's only
three steps, so you don't get that far
three steps, so you don't get that far
off. But then if you actually go look at
off. But then if you actually go look at
important sampling like
if you go look at retrace
if you go look at retrace
this is actually very very similar to
this is actually very very similar to
what I came up with for puffer advantage
what I came up with for puffer advantage
for the on policy case. Okay. And you
for the on policy case. Okay. And you
have like I said you have to go through
have like I said you have to go through
all this freaking math. But the idea
all this freaking math. But the idea
here is that you weight samples by the
here is that you weight samples by the
ratio of the probability of taking the
ratio of the probability of taking the
action with the policy that you have now
action with the policy that you have now
uh to the probability of taking that
uh to the probability of taking that
action with the policy under which you
action with the policy under which you
collected the data.
collected the data.
So what this means is if these two
So what this means is if these two
things diverge a bunch, what's going to
things diverge a bunch, what's going to
happen
happen
is you're not actually going to be able
is you're not actually going to be able
to make use of uh of off policy data
to make use of uh of off policy data
because it's going to downweight
because it's going to downweight
everything.
Like it's a correction in the sense that
Like it's a correction in the sense that
you don't trust the data that's off
you don't trust the data that's off
policy very much.
Monte. No.
Okay. Interesting.
Okay. Interesting.
Um
get these baselines.
Yeah, this is actually a pretty weak
Yeah, this is actually a pretty weak
result
result
for something this fancy.
I do think search is the right way to go
I do think search is the right way to go
though.
like at least search is a reasonable way
like at least search is a reasonable way
to go for scaling compute I should say
to go for scaling compute I should say
when you have a slow environment
when you have a slow environment
specifically
specifically
how do you approximate the denominator
how do you approximate the denominator
oh of uh important sampling you don't
oh of uh important sampling you don't
you just you store it because you have
you just you store it because you have
the you can get the probability of
the you can get the probability of
sampling this action right under the old
sampling this action right under the old
policy and then you're just using that
policy and then you're just using that
whenever you use the new sample
whenever you use the new sample
strong correlation between being fancy
strong correlation between being fancy
and being fake. Yeah.
I mean, the problem with uh the offpaul
I mean, the problem with uh the offpaul
methods as well is you kind of get it
methods as well is you kind of get it
masked. So, this is the best thing that
masked. So, this is the best thing that
I found, which is just like this is
I found, which is just like this is
basically Rainbow V2 in the sense that
basically Rainbow V2 in the sense that
it's what happens if we strap more
it's what happens if we strap more
tricks, we just add more tricks onto it.
tricks, we just add more tricks onto it.
And like they get a pretty dang good
And like they get a pretty dang good
result. Um,
result. Um,
I actually like this paper a lot more
I actually like this paper a lot more
than the other ones. And it's like said,
than the other ones. And it's like said,
"Oh, yeah, this is way faster as well
"Oh, yeah, this is way faster as well
than the other offpaul methods." But you
than the other offpaul methods." But you
do the math and you're training Atari at
do the math and you're training Atari at
like 2,000 steps per second.
And the thing that I haven't seen from
And the thing that I haven't seen from
off policy yet,
so I think that there are worse
so I think that there are worse
algorithms that are being made better by
algorithms that are being made better by
being able to leverage more compute on
being able to leverage more compute on
the same data. That's not what I want. I
the same data. That's not what I want. I
want to have like an equivalent
want to have like an equivalent
algorithm that does just as well in the
algorithm that does just as well in the
high data regime,
high data regime,
but where you can crank up compute if
but where you can crank up compute if
you don't have as much data. You need to
you don't have as much data. You need to
have this like smooth scaling between
have this like smooth scaling between
these two.
these two.
Optimizing purely for low data regime is
Optimizing purely for low data regime is
kind of just dumb.
Okay. So, this is a top K baseline.
H, this gets stuck.
That's kind of Honey
fast TD3. Yeah, I saw that.
fast TD3. Yeah, I saw that.
I hadn't looked at the details
lab.
Okay. So, this is pretty close to what I
Okay. So, this is pretty close to what I
was wanting to look at here.
large batch training is good
distribute if Yeah. So, this is what I
distribute if Yeah. So, this is what I
was looking at yesterday. And one of the
was looking at yesterday. And one of the
annoying things is you immediately have
annoying things is you immediately have
to do um
Okay.
Okay.
Double Q.
Where's the algorithm?
Where's the algorithm?
Where's the uh the algorithm
Where's the uh the algorithm
description?
description?
They miss it.
NDPJ
critic.
critic.
>> Yeah, it could be that this is the way
>> Yeah, it could be that this is the way
to go. I'm not sure yet.
Well, that's some good evidence to have
Well, that's some good evidence to have
here, though.
here, though.
I'll keep on this for a little bit.
I should actually do this up here,
I should actually do this up here,
right?
Oh, wait. Can you even compute?
Oh, wait. Can you even compute?
You can compute advantage on this,
You can compute advantage on this,
right?
Yeah, you can.
Enter.
How are you?
Okay. So, we do something like this. I
Okay. So, we do something like this. I
think
I don't know if I like the idea of
I don't know if I like the idea of
throwing away data based on
throwing away data based on
Vantage or not yet.
think like this. Maybe
still worse.
Well, we can kind of break up this
Well, we can kind of break up this
problem a little bit, right?
I go to 20 million
here. Let's see. Um,
here. Let's see. Um,
this is going to be the full 20
full 20 mil. And then
full 20 mil. And then
uh if we do
uh if we do
the 32K
before
eight batches.
eight batches.
Okay. So that's not bad.
Okay.
Okay.
150. But basically, no data should ever
150. But basically, no data should ever
get kicked out here, right?
And no data should ever
And no data should ever
uh ever get kicked out.
IL segments
8192 segments.
That's totally wrong. Right.
Hatch size.
Oh yeah, this is just a mess up here,
Oh yeah, this is just a mess up here,
right?
Um, something's
This is just me having been sloppy.
Okay, so no data is getting kicked out
Okay, so no data is getting kicked out
anymore.
How do you start research?
How do you start research?
Uh, if you're interested in getting
Uh, if you're interested in getting
started on this type of stuff,
started on this type of stuff,
my quick start guide. I made a guide for
my quick start guide. I made a guide for
people. It's the most common question I
people. It's the most common question I
get asked.
So here's starter guide for programming
So here's starter guide for programming
in ML and then for RL research
in ML and then for RL research
specifically
specifically
research and dev.
research and dev.
The ML article also has some of my best
The ML article also has some of my best
advice for uh research in general.
So
I break here
I break here
self
and no worries.
[Music]
This is probably horribly slow.
probably horribly horribly slow.
So, can I just train more now?
It kind of It was unstable though,
It kind of It was unstable though,
right?
right?
Why should it be unstable?
Why should it be unstable?
Why can I not just do this and have it
Why can I not just do this and have it
work?
Okay.
And just keep going up like this.
What's wrong?
Oh, you know what it is?
Good
know if they're going to release the RLC
know if they're going to release the RLC
talks. No idea. I don't know if they're
talks. No idea. I don't know if they're
recorded, but mine is up on YouTube.
I recorded mine separately.
What happens here?
Okay, so this definitely shouldn't be
Okay, so this definitely shouldn't be
happening, right?
You get your best indices.
Oh, well, you're not optimizing a value
Oh, well, you're not optimizing a value
function, dummy. So, that would do it.
Should
be stable.
better.
Okay. So, you actually do get
You do get Yes.
No positive advantage left.
got to clip this somehow, right?
H okay.
We have some advantages. Yes.
in some todos and evaluate
in some todos and evaluate
where you only stack observations
where you only stack observations
from the same end and don't use mask.
from the same end and don't use mask.
Wait, what do you mean you only stack
Wait, what do you mean you only stack
observations
observations
the same end? So that todo is for
the same end? So that todo is for
something different.
something different.
That todo is for multi- aent like
That todo is for multi- aent like
masking or if you have um like if you
masking or if you have um like if you
have dead agents basically.
Truncations you could easily handle.
Truncations you could easily handle.
I don't think any of our new ocean
I don't think any of our new ocean
environments use truncations.
environments use truncations.
You could handle it for classic ones
You could handle it for classic ones
though. I haven't bothered because like
though. I haven't bothered because like
we just don't bother with uh in our new
we just don't bother with uh in our new
environments making a distinction.
The main thing I'm trying to figure out
The main thing I'm trying to figure out
at the moment is how to bridge this like
at the moment is how to bridge this like
gulf between um
gulf between um
like sample efficiency and compute
like sample efficiency and compute
efficiency.
When you get a done from an end, you
When you get a done from an end, you
keep stacking up. So that doesn't So
keep stacking up. So that doesn't So
that's actually handled correctly. If
that's actually handled correctly. If
you get a done from an environment, it
you get a done from an environment, it
gets reset. But like you uh you cut the
gets reset. But like you uh you cut the
bootstrap like you don't you don't keep
bootstrap like you don't you don't keep
increased like incrementing values in
increased like incrementing values in
your advantage function across done
your advantage function across done
boundaries. That's already handled
boundaries. That's already handled
correctly.
downside of downside of computer. What
downside of downside of computer. What
benefit of replay buffer?
benefit of replay buffer?
We're trying to figure out uh if we can
We're trying to figure out uh if we can
have a replay buffer without having to
have a replay buffer without having to
use really janky off policy algorithms
use really janky off policy algorithms
that don't actually work in the high
that don't actually work in the high
data regime.
and just do it this way, right?
Okay. So, now we're no longer going to
Okay. So, now we're no longer going to
collapse on bad advantages, I suppose.
Oh, we're definitely collapsing.
How's that possible? Oh,
Okay. I mean, so like you'll pretty darn
Okay. I mean, so like you'll pretty darn
quickly here get to the point where
quickly here get to the point where
this thing is saying that there's just
this thing is saying that there's just
like almost no advantage left in the
like almost no advantage left in the
data,
which seems weird.
because we're keeping all of the data
because we're keeping all of the data
around, right?
You know, let's do um losses.
You know, let's do um losses.
Let's put this in here. We can kind of
Let's put this in here. We can kind of
see without having it spam us
see without having it spam us
at what point this thing messes up.
We start off high advantage.
We start off high advantage.
We're learning well.
We're learning well.
Then we get down to very low advantage
Then we get down to very low advantage
and we regress. Yes.
code is kind of a mess but the uh the
code is kind of a mess but the uh the
algorithm here unless I have a major
algorithm here unless I have a major
mistake
mistake
should be pretty simple.
should be pretty simple.
So we have all our rewards,
So we have all our rewards,
observations, actions, values,
observations, actions, values,
terminals, right? We have everything
and we compute our advantage function.
We sort it descending
and then we keep the uh the best ones,
and then we keep the uh the best ones,
right?
And this is set up such that we really
And this is set up such that we really
should not ever be throwing data away
should not ever be throwing data away
either.
And we run a whole bunch of mini batches
And we run a whole bunch of mini batches
which we compute
which we compute
advantage estimate.
The advantage estimate, right?
We take our top K samples.
We take our top K samples.
We just do IIL.
Right.
We literally would just do
We literally would just do
mitation learning.
mitation learning.
Same exact loss and everything.
at this
A lot.
This this
no something of this form I think should
no something of this form I think should
work.
Advantages should be close to zero.
Advantages should be close to zero.
Right? So when you've learned the task,
Right? So when you've learned the task,
the advantages should be close to zero.
They shouldn't be before you've learned
They shouldn't be before you've learned
the task, though.
And you definitely should not regress
and like start doing worse when you're
and like start doing worse when you're
literally doing batch learning.
about the number of samples surviving
about the number of samples surviving
Bing.
Bing.
Uh that one is fair.
What our losses
Why is it such a big number?
Oh,
for now we'll just hard code.
Huh?
Yeah, this thing should be locked to um
be like locked.
Okay.
There isn't a Q function though.
There isn't a Q function though.
It's just an advantage estimate
which is dependent on the current
which is dependent on the current
policy.
policy.
So if it shouldn't be able to get better
So if it shouldn't be able to get better
than the current policy, All right.
Having
to fit the value function.
now. This actually this this should
now. This actually this this should
totally work. There's got to be
totally work. There's got to be
something screwy here.
Yeah. So, what I'm doing here, right, is
Yeah. So, what I'm doing here, right, is
I have I have fresh data being collected
I have I have fresh data being collected
for now. I have it set up so that
for now. I have it set up so that
effectively you're storing all of it. So
effectively you're storing all of it. So
you have access to all of the data
you have access to all of the data
and then I'm just computing
and then I'm just computing
advantage
advantage
um fresh advantage for everything.
um fresh advantage for everything.
Well, as fresh as you can get it. And
Well, as fresh as you can get it. And
I'm just training on the I'm just doing
I'm just training on the I'm just doing
behavioral cloning on like the top 32k.
behavioral cloning on like the top 32k.
So the idea is that you get all the
So the idea is that you get all the
information you can out of each of the
information you can out of each of the
samples.
I guess it is also possible.
Well, the problem is you actually can't
Well, the problem is you actually can't
get a fresh advantage estimate, right? I
get a fresh advantage estimate, right? I
think that's the problem.
Yeah, you can't get a fresh advantage
Yeah, you can't get a fresh advantage
estimate without rerunning it on the
estimate without rerunning it on the
whole batch. Like you have to rerun the
whole batch. Like you have to rerun the
value function on the whole batch to do
value function on the whole batch to do
That
I can probably just code that up. It'll
I can probably just code that up. It'll
be slow.
We do something like this.
We do something like this.
We do this full forward pass.
This will at least let me verify if
This will at least let me verify if
what's happening is uh what I Think.
Oh,
Move this out here
slightly less often.
dude are going to be out of memory. Come
dude are going to be out of memory. Come
on.
This stupid thing breaking.
That's totally freaking wrong. How do I
That's totally freaking wrong. How do I
do this?
Still No.
Okay,
Much better.
Still unstable. interesting enough.
Still unstable. interesting enough.
Um, but we have positive advantage now
Um, but we have positive advantage now
at least.
Yeah, we actually have like good uh
Yeah, we actually have like good uh
reasonable data now. So, we can figure
reasonable data now. So, we can figure
out why this thing is unstable.
Oh, this is going to run the same
Oh, this is going to run the same
freaking batch 128 times, right?
freaking batch 128 times, right?
Oops.
Yeah, this is totally going to run the
Yeah, this is totally going to run the
uh
uh
the same batch.
I don't see how the heck this is
I don't see how the heck this is
possibly still unstable.
It's even worse. Really?
Really?
I just want to integrate a value
I just want to integrate a value
function with this thing. Why is this so
function with this thing. Why is this so
difficult?
I suppose I could do this on um
I suppose I could do this on um
on raw rewards first, right?
Let's do it this way.
That's going to be super overfit and
That's going to be super overfit and
that's still better.
that's still better.
So yeah, there's something very screwy
So yeah, there's something very screwy
then about this advantage based
then about this advantage based
estimate. Like this is literally just
estimate. Like this is literally just
overfitting a big a batch of rewards.
Okay.
Let's just make absolutely sure we're
Let's just make absolutely sure we're
not kicking out any data that we want.
not kicking out any data that we want.
Now we are keeping data by reward.
Get your new value function
Get your new value function
like
This doesn't do enough, right?
Ability.
You just train based on reward. This
You just train based on reward. This
works
thumb advantage.
very low positive advantage already.
mean the policy thinks it should be
mean the policy thinks it should be
doing very well.
It does, right?
Policy thinks it should be doing very
Policy thinks it should be doing very
well.
Shouldn't
be not for the purpose of sorting it.
Think that would make a difference. Um
this is literally data selection because
this is literally data selection because
we're only using advantage
we're only using advantage
to select data.
to select data.
So I mean literally if I just do look
So I mean literally if I just do look
how easy this problem is, right?
If I just change this
to gold rewards.
Literally just training on a mini batch
Literally just training on a mini batch
of the highest reward segments.
which barely even makes sense cuz you're
which barely even makes sense cuz you're
like you're going to giga overfit
that actually solved before something
that actually solved before something
changed.
changed.
[Music]
like a cheat.
You can do it.
this not solve a second ago.
this not solve a second ago.
I could have sworn that we had this
I could have sworn that we had this
solving
We should actually then make sure that
We should actually then make sure that
we can get this to solve right.
we can get this to solve right.
Otherwise,
Okay.
Good. So, um, sanity has returned,
Good. So, um, sanity has returned,
right?
So all this is saying is if you sample
So all this is saying is if you sample
proportional to summed reward
proportional to summed reward
you solve the task.
I do it this way.
unstable.
I think we absolutely do need the
I think we absolutely do need the
advantage function here as well, right?
advantage function here as well, right?
Like the only thing that we really need
Like the only thing that we really need
to know is is this segment better than
to know is is this segment better than
the current policy or Not
this. so hard about that.
this. so hard about that.
Be easy.
Just read through this whole thing
Just read through this whole thing
carefully.
carefully.
See if I've done something dumb.
So you're just keeping this means you're
So you're just keeping this means you're
keeping all the data.
keeping all the data.
The batch is like 20 million. So fun.
You're updating your value.
You're updating your value.
Fine.
Get the value of everything in the
Get the value of everything in the
buffer.
Get your advantage.
Values, rewards, terminals, right?
Values, rewards, terminals, right?
Above right
is here.
Done. This
Done. This
same sample as before.
your value here.
again.
Now this should work
Now this should work
totally work
suppose I don't understand why the value
suppose I don't understand why the value
function
Is it the retrace thing biting me?
Is it the retrace thing biting me?
I'd highly doubt that, but
It's still unstable.
How do you learn the actual value
How do you learn the actual value
function?
function?
No, it's not implicit. It's right here.
The clipped value loss, right?
This is better.
This is better.
[Music]
doesn't if I just do this on breakout it
doesn't if I just do this on breakout it
doesn't work right
okay well light problem with that
can't have a 20 million buffer size.
to try to match the um
to try to match the um
original sample efficiency, right?
You on.
Okay,
Okay,
that is uh that is actually something.
Is it amazing yet? No.
It's something though.
Ryan, we are doing behavioral cloning
Ryan, we are doing behavioral cloning
without expert data.
Okay.
Pretty much that. Yeah. I'm trying to
Pretty much that. Yeah. I'm trying to
fit. The thing that's tough is I I you
fit. The thing that's tough is I I you
have to fit the advantage function to
have to fit the advantage function to
that to make it make sense.
that to make it make sense.
And I'm having trouble getting that to
And I'm having trouble getting that to
play nicely.
I should probably just go steal the uh
I should probably just go steal the uh
the advantage waiting, right?
Well, I suppose I haven't tried actually
Well, I suppose I haven't tried actually
waiting it by the advantage yet, right?
waiting it by the advantage yet, right?
I've just done
You actually don't necessarily though
You actually don't necessarily though
want to get rid of
want to get rid of
of segments in the buffer when they're
of segments in the buffer when they're
at low advantage, right?
behavioral
cloning. Yeah.
Work. Uh I don't think that that works.
Work. Uh I don't think that that works.
So I did a whole bunch of looking at the
So I did a whole bunch of looking at the
math and um
math and um
important sampling is just really
important sampling is just really
stupid.
like it pretty much just it just
like it pretty much just it just
prevents you from learning on data
prevents you from learning on data
because it's off policy. It doesn't
because it's off policy. It doesn't
actually help you learn on the data. It
actually help you learn on the data. It
just like takes it out of the uh the
just like takes it out of the uh the
advantage pretty much.
Yeah, but that's not what we want. I
Yeah, but that's not what we want. I
don't want to wait on policy data
don't want to wait on policy data
higher. I want to wait better data
higher. I want to wait better data
higher.
Exactly. I'm pretty much like I've been
Exactly. I'm pretty much like I've been
looking through so much of um the
looking through so much of um the
offpaul literature
offpaul literature
and
and
it seems to me like these are just
it seems to me like these are just
stupid worse algorithms that are always
stupid worse algorithms that are always
going to be worse than the high data
going to be worse than the high data
regime and it's just that they spend a
regime and it's just that they spend a
stupid amount of compute and eventually
stupid amount of compute and eventually
it gets better than on policy and then
it gets better than on policy and then
they keep going and you know they can
they keep going and you know they can
keep training on their stale data.
keep training on their stale data.
This seems like it's why um onpaul has
This seems like it's why um onpaul has
been competitive in the first place is
been competitive in the first place is
because the off pal the off-paul
because the off pal the off-paul
algorithms are just fundamentally worse.
Like they do this really stupid like
Like they do this really stupid like
three-step bootstrap or whatever.
three-step bootstrap or whatever.
That's the main thing. And then even if
That's the main thing. And then even if
you do something like retrace, right,
you do something like retrace, right,
it like kind of collapses to just being
it like kind of collapses to just being
on policy learning anyways. just like
on policy learning anyways. just like
with weird settings.
Low data regime is more relevant to I
Low data regime is more relevant to I
disagree.
disagree.
No, I completely disagree there.
No, I completely disagree there.
Industries do not give a if you
Industries do not give a if you
solve their problem by getting more data
solve their problem by getting more data
or um if you solve their problem with a
or um if you solve their problem with a
better algorithm.
better algorithm.
And fundamentally having more data will
And fundamentally having more data will
let you learn better with spending less
let you learn better with spending less
compute
until he addresses sample efficiency. I
until he addresses sample efficiency. I
don't think it does fundamentally
don't think it does fundamentally
address the sample efficiency and we
address the sample efficiency and we
have a I mean your author on the
have a I mean your author on the
published paper or like the main
published paper or like the main
candidate algorithm for that kind of
candidate algorithm for that kind of
crap.
Like I fundamentally don't even know if
Like I fundamentally don't even know if
modelbased RL makes any sense.
You're literally training. You're
You're literally training. You're
literally training to like poke all the
literally training to like poke all the
holes in your learned model.
Definitely need data reuse.
What to do with this?
In expectation, changing sampling
In expectation, changing sampling
frequency should be the same as
frequency should be the same as
is changing the waiting of the data.
all do we do with
I don't like so the thing that I don't
I don't like so the thing that I don't
like here Right.
You're going to sample your good data
You're going to sample your good data
more often and thereby you're going to
more often and thereby you're going to
overfit it and then you're going to kick
overfit it and then you're going to kick
it out of your buffer.
it out of your buffer.
Wait, high reward trajectories are. Yes,
Wait, high reward trajectories are. Yes,
that actually uh well, it's the same
that actually uh well, it's the same
thing, right? You can either like sample
thing, right? You can either like sample
the high reward trajectories more or you
the high reward trajectories more or you
can weight them. Either of these work at
can weight them. Either of these work at
least for the simple environments. Just
least for the simple environments. Just
doing behavioral cloning on high reward
doing behavioral cloning on high reward
segments works for simple tasks. The
segments works for simple tasks. The
problem right is um
problem right is um
tasks where you don't have enough
tasks where you don't have enough
information just in that one segment.
information just in that one segment.
You you need to like bootstrap uh an
You you need to like bootstrap uh an
advantage estimate of some type. Unless
advantage estimate of some type. Unless
you're going to wait all the way to the
you're going to wait all the way to the
end of the episode, which technically
end of the episode, which technically
you could kind of do with a lot of
you could kind of do with a lot of
environments, but um it's not ideal.
environments, but um it's not ideal.
Definitely not ideal.
Yeah. So, policy gradients does that.
Yeah. So, policy gradients does that.
And this is the thing that I'm trying to
And this is the thing that I'm trying to
figure out. Um, if you actually just
figure out. Um, if you actually just
write out the math for behavioral
write out the math for behavioral
cloning and policy gradient, it looks
cloning and policy gradient, it looks
really really similar. But like the
really really similar. But like the
derivation of policy gradients makes the
derivation of policy gradients makes the
on policy assumption. Um, behavioral
on policy assumption. Um, behavioral
cloning looks very similar, but doesn't
cloning looks very similar, but doesn't
make that assumption. But like we know
make that assumption. But like we know
that you can't just run
that you can't just run
like policy gradient methods with
like policy gradient methods with
totally off policy data and have it
totally off policy data and have it
work.
work.
So, I'm trying to figure that chunk of
So, I'm trying to figure that chunk of
it out pretty much.
Maybe you don't want an advantage
Maybe you don't want an advantage
function, right?
function, right?
Hang on.
What if you just train?
What if you just train?
What if you just train like a
What if you just train like a
bootstrapping value function?
And then you keep the segments
And then you keep the segments
that have the highest re like some
that have the highest re like some
reward plus value.
reward plus value.
Isn't this kind of similar to what um
Isn't this kind of similar to what um
I think I might have just recovered this
I think I might have just recovered this
other paper here.
other paper here.
This thing
This thing
they weigh it here. But yeah, this is
they weigh it here. But yeah, this is
reward.
reward.
Probably this is like return or
Probably this is like return or
something. Value.
something. Value.
No, this is advantage still.
No, this is advantage still.
But what if we just do value estimate?
I'm going to reduce the increasing
I'm going to reduce the increasing
learning rate or clipping rat. No.
learning rate or clipping rat. No.
Um,
Um,
well, not in our case, right, Ryan?
well, not in our case, right, Ryan?
because we like we have a much more
because we like we have a much more
aggressively optimized baseline.
aggressively optimized baseline.
So, like if you actually beat our
So, like if you actually beat our
baseline on a bunch of M's, you should
baseline on a bunch of M's, you should
be pretty confident that it actually
be pretty confident that it actually
works.
I think you do just want a value
I think you do just want a value
function, not an advantage function,
function, not an advantage function,
right?
Or do you want both?
Well, hang on. You would still want to
Well, hang on. You would still want to
sample your data according to advantage,
sample your data according to advantage,
right? You just wouldn't want to kick it
right? You just wouldn't want to kick it
out of the buffer.
I think that's right.
I think that's right.
Yeah. So, why does this not like insta
Yeah. So, why does this not like insta
solve everything?
solve everything?
Go to here.
We can mess with the sampling pram on
We can mess with the sampling pram on
advantage here.
prioritizing data definitely policy.
prioritizing data definitely policy.
Uh you're not doing policy gradient,
Uh you're not doing policy gradient,
you're just doing behavioral cloning.
you're just doing behavioral cloning.
So there are two different things here,
So there are two different things here,
right? One is how do you select what
right? One is how do you select what
data to keep in your buffer? You
data to keep in your buffer? You
probably don't want that to be advantage
probably don't want that to be advantage
based because then if you learn a piece
based because then if you learn a piece
of data, you throw it away. You want to
of data, you throw it away. You want to
keep it around so you're building a good
keep it around so you're building a good
data set.
data set.
But then how do you actually sample from
But then how do you actually sample from
that buffer? You probably just want to
that buffer? You probably just want to
sample like high advantage trajectory
sample like high advantage trajectory
segments, right? Because those are the
segments, right? Because those are the
ones that you can get the most
ones that you can get the most
information out of. But you don't want
information out of. But you don't want
to kick them out of the buffer based on
to kick them out of the buffer based on
advantage. Because if you kick them out
advantage. Because if you kick them out
based on advantage, right, once you've
based on advantage, right, once you've
learned them, that's it. If you kick
learned them, that's it. If you kick
them out based on low value, so which is
them out based on low value, so which is
uh summed reward over like some
uh summed reward over like some
discounted reward or whatever plus the
discounted reward or whatever plus the
final value estimate, then you'll you
final value estimate, then you'll you
will not kick out like good segments.
I guess your value function can still be
I guess your value function can still be
wrong. So if you get too many super
wrong. So if you get too many super
overoptimistic ones, you can kick out
overoptimistic ones, you can kick out
other ones potentially.
Don't have to do that just yet, though.
Don't have to do that just yet, though.
We can kind of just chill on this for
We can kind of just chill on this for
now. Um,
let's do
make this a mean first.
one over beta.
Exactly.
Wait, you need roll outs for advantage?
Wait, you need roll outs for advantage?
What do you mean you need rollouts for
What do you mean you need rollouts for
advantage? You need segments and you
advantage? You need segments and you
have segments. You can refresh both of
have segments. You can refresh both of
these estimates, right?
Okay, so the sampling scheme actually
Okay, so the sampling scheme actually
definitely matters a ton.
It's over 200 score on Breakout with
It's over 200 score on Breakout with
literally just behavioral cloning.
Can we find something that solves
Can we find something that solves
breakout
parameter they had was
parameter they had was
One over 105 says 20.
Yeah, that does by comparison.
Now I suppose the next question right
Now I suppose the next question right
is do you actually want to weight the
is do you actually want to weight the
entire segment the Mhm.
You know, maybe you're right, Ryan.
You know, maybe you're right, Ryan.
Maybe the next step is literally to just
Maybe the next step is literally to just
say, "Screw it. We're doing on policy
say, "Screw it. We're doing on policy
algorithms with horribly off policy
algorithms with horribly off policy
data, but we're only going to use the
data, but we're only going to use the
best data, and we're going to hope it
best data, and we're going to hope it
reduces to behavioral cloning,
reduces to behavioral cloning,
right?
right?
I'll be right back.
on policy RL with horribly off policy
on policy RL with horribly off policy
data of everybody calling me an idiot.
data of everybody calling me an idiot.
I'm sure unless I actually make it work.
I'm sure unless I actually make it work.
I think the odds are one in five.
I think the odds are one in five.
One in five odds of having this actually
One in five odds of having this actually
work I'll say
work I'll say
maybe one in four being a little too
maybe one in four being a little too
harsh
kind of a believer in that approach.
kind of a believer in that approach.
So like
So like
the thing that I came across right
the thing that I came across right
the behavioral cloning objective
the behavioral cloning objective
and the uh the policy gradient objective
and the uh the policy gradient objective
are not like fundamentally different.
are not like fundamentally different.
They're just kind of like different rate
They're just kind of like different rate
different um weightings of the same like
different um weightings of the same like
log props.
But I don't see why we can't just try
But I don't see why we can't just try
this
this
and just basically say screw it, right?
and just basically say screw it, right?
And then the idea is maybe on policy
is like you're just trying to grab
is like you're just trying to grab
you're just trying to grab rewards from
you're just trying to grab rewards from
the data you have which is going to be
the data you have which is going to be
whatever the quality of the current
whatever the quality of the current
policy is.
And like obviously if you go reuse past
And like obviously if you go reuse past
old data
old data
that like you're better than
that like you're better than
then that's not going to help, right?
then that's not going to help, right?
That's going to mess you up. But if you
That's going to mess you up. But if you
sample it according to advantage,
doesn't this kind of make sense?
doesn't this kind of make sense?
Because like I've gone through so much
Because like I've gone through so much
literature in the past few days on
literature in the past few days on
OffPaul and it just seems like a shitow.
OffPaul and it just seems like a shitow.
It really just seems like a whole bunch
It really just seems like a whole bunch
of hacks tacked onto a really bad
of hacks tacked onto a really bad
algorithm
algorithm
and then you just spin up a ton of
and then you just spin up a ton of
compute
compute
cuz like base Q-learning works. I
cuz like base Q-learning works. I
implemented it in puffer. works and like
implemented it in puffer. works and like
technically right if you just run it for
technically right if you just run it for
long enough on off policy data it should
long enough on off policy data it should
do something
I think that basically they've just kind
I think that basically they've just kind
of
of
packed a bunch of things onto that and
packed a bunch of things onto that and
then run it for a really long time
try this next I got to commit this
But my point is they're the same thing.
But my point is they're the same thing.
Like they're different waitings of the
Like they're different waitings of the
same objective, right? It's like
same objective, right? It's like
literally just a a log prob.
literally just a a log prob.
It's just like grad log prop or whatever
It's just like grad log prop or whatever
on both.
for the critic. Yeah, I think it's not
for the critic. Yeah, I think it's not
entirely insane.
And I could be wrong, but I think it's
And I could be wrong, but I think it's
not entirely
not entirely
insane.
Where's our PO update thing?
Have to go grab a bunch of old code.
Now we just do
Wouldn't it be funny if this actually
Wouldn't it be funny if this actually
ends up being the thing that works?
Like you have this whole other class of
Like you have this whole other class of
off policy algorithms and it's like
off policy algorithms and it's like
yeah,
yeah,
you kind of just don't do that.
The likelihood of this being the case
The likelihood of this being the case
has gone up in my mind after talking to
has gone up in my mind after talking to
a bunch of people
a bunch of people
and like actually digging through a
and like actually digging through a
bunch of the math reasonably well. I'd
bunch of the math reasonably well. I'd
say
be legitimately mad. Dude, I'm going to
be legitimately mad. Dude, I'm going to
be ecstatic if this works because it
be ecstatic if this works because it
means that we'll have a super simple um
means that we'll have a super simple um
puffer implementation that just solves
puffer implementation that just solves
freaking everything.
I don't have to spend the rest of my
I don't have to spend the rest of my
life doing core RL research, right? If
life doing core RL research, right? If
we just solve the field. I can find
we just solve the field. I can find
other problems to work on.
Cool applications of it as well.
Isn't this it up here?
Isn't this it up here?
All right. So, this is kind of it up
All right. So, this is kind of it up
there. No,
there. No,
like
log
and then you don't need the PO objective
and then you don't need the PO objective
either, right?
Yeah. You don't even need the clipping
Yeah. You don't even need the clipping
of uh PO.
You literally just need then the
uh because the clipping is to keep you
uh because the clipping is to keep you
on policy. We're not doing that.
We'll keep the gradient clip, right?
Oh, literally it's advantage, right?
Oh, literally it's advantage, right?
Where's
on how the heck are you optimizing
on how the heck are you optimizing
advantage?
advantage?
Wait, wait. advantage.
Wait, wait. advantage.
ABS
ABS
some
how is this thing even differentiable
how is this thing even differentiable
again?
MB
Let's
get back hacked.
get back hacked.
Very good. Hurry it up so we can
Very good. Hurry it up so we can
actually get some cool stuff in.
Is this actually differentiable?
Is this actually differentiable?
Confused.
Yeah, I'm not even differentiable,
Yeah, I'm not even differentiable,
right? Wait, how the heck are we
right? Wait, how the heck are we
this stupid? Like, where's this loss
this stupid? Like, where's this loss
even coming from?
advantage times ratio.
Oh, wait. Log ratio.
But isn't this one
But isn't this one
MB?
MB?
Oh, no. Because this is a constant,
Oh, no. Because this is a constant,
right?
right?
So, it's
So, it's
it's just new log probs times advantage.
it's just new log probs times advantage.
Edge
there.
Okay, so that actually is a thing.
Okay, so that actually is a thing.
And now can I put the full loss back?
Okay. So, it doesn't do anything
Okay. So, it doesn't do anything
meaningful.
It is possible I've kind of just broken
It is possible I've kind of just broken
things too much though at this point.
I'm kind of tempted to like start clean
I'm kind of tempted to like start clean
with this,
right?
Yeah, I think we're going to start clean
Yeah, I think we're going to start clean
with this.
with this.
What I'm gonna do
And now I'm just going to copy in the uh
And now I'm just going to copy in the uh
additional bits I suppose. was
Yeah, I think easiest thing is to just
Yeah, I think easiest thing is to just
keep the uh the code that I have like
keep the uh the code that I have like
the gold
the gold
data
and then just put this
and then just put this
more cleanly back into
like this
and then
basically don't change eval stuff at
basically don't change eval stuff at
all.
And all I need to do
uh
uh
this sort of a thing right
at the top of train
Let's say that for now we're just going
Let's say that for now we're just going
to store
by reward.
by reward.
Keep it simple.
We'll just dodo
store by turn plus val.
Right. So now we have
Right. So now we have
we have our gold data.
Then we'll basically do
Then we'll basically do
the exact thing that we have here
the exact thing that we have here
um just with this different data.
um just with this different data.
All right. So
All right. So
I need the defaults.
Yeah. So this is normal PO right?
So this is like our normal training
and then
we literally just swap in
old
Okay, so this is pretty much
Okay, so this is pretty much
everything.
Pretty much everything in here.
Uh, this needs to be set
Okay.
So um
that looks pretty stable. Now obviously
that looks pretty stable. Now obviously
it's not good but uh this should be
it's not good but uh this should be
crashing everything
crashing everything
according to the onpaul theory. So this
according to the onpaul theory. So this
is actually I'm happy with that result.
That solves part pole
That solves part pole
unstable
unstable
does solve it.
does solve it.
Okay. So
well now we have questions, right?
This
break before this, huh?
break before this, huh?
Okay.
Fine.
wrong with this.
wrong with this.
Oh,
you know
what?
This happened.
See?
Fine.
This is it.
That's for now.
Okay.
Okay.
Time for cart pull.
Nothing on breakout.
Nothing on breakout.
Huh?
Huh?
beard.
Well, there's probably like um
Well, there's probably like um
I'm probably not dealing with the empty
I'm probably not dealing with the empty
buffer correctly, right?
go to like
millionish.
Okay.
Not stable. Interesting enough.
Not stable. Interesting enough.
Plus though.
makes it a lot simpler if we just sample
makes it a lot simpler if we just sample
based on reward for a bit, right?
Can we literally just do
Also, this self segment
Also, this self segment
not terrible. All
right.
Ah,
Ah,
okay. Still not stable, but
they did something.
Funny cuz we know the clipping term is
Funny cuz we know the clipping term is
like super important when you're
like super important when you're
normally doing PO
do
You want the log.
This is insta solve territory, right?
end up with a
It crashes,
starts learning.
enough.
Negative.
Negative.
Why is it negative advantage?
Oh, because you want to maximize
Oh, because you want to maximize
advantage, right?
No, hang on.
No, hang on.
Used.
Are you able to explain the intuition
Are you able to explain the intuition
for why you wait geometric lambda?
Um,
Um,
so it comes from I believe it comes from
so it comes from I believe it comes from
eligibility traces.
eligibility traces.
Hang on.
There's this thing.
Is it? Oh, here it is. Yeah. 287.
Yeah. Okay. So
I honestly I don't know whether this is
I honestly I don't know whether this is
like the book was revised after J. I I
like the book was revised after J. I I
don't think so. I think that this is
don't think so. I think that this is
older. Um
older. Um
but you do get the
but you do get the
lambda return formula here.
So, if you want the full math of it,
So, if you want the full math of it,
it's here.
it's here.
If you want the quick version, it's just
If you want the quick version, it's just
it's an exponentially smoothed average
it's an exponentially smoothed average
of exponentially smoothed averages.
What's the exact shape of advantages?
What's the exact shape of advantages?
Um, it's going to have the same shape as
Um, it's going to have the same shape as
the number. It's one advantage per
the number. It's one advantage per
observation, right? So if you have
observation, right? So if you have
trajectory segments like 512 segments of
trajectory segments like 512 segments of
length 64, it's going to be 512 x 64
length 64, it's going to be 512 x 64
a ge looks like directly that to me.
a ge looks like directly that to me.
Kevin,
I'm trying to figure out if we can get,
I'm trying to figure out if we can get,
and this is driving me crazy, but I'm
and this is driving me crazy, but I'm
trying to figure out if we can get
trying to figure out if we can get
um
um
uh on policy learning to work with off
uh on policy learning to work with off
policy data because
policy data because
and it like it actually surprisingly
and it like it actually surprisingly
kind of does in a few different
kind of does in a few different
settings.
going to do a whole bunch of math lately
going to do a whole bunch of math lately
that's convinced me that there should be
that's convinced me that there should be
a version of this that works
log props
I just do
I just do
implicit cross entropy
You just do explicit cross entropy,
You just do explicit cross entropy,
right?
Completely off policy is the goal.
Completely off policy is the goal.
Um the goal here
The goal here
The goal here
is that we maintain a high quality
is that we maintain a high quality
buffer of data
buffer of data
and if we exclusively use that to train
and if we exclusively use that to train
on,
on,
we should be able to bypass the on
we should be able to bypass the on
policy assumption because
policy assumption because
uh the math for behavioral cloning looks
uh the math for behavioral cloning looks
a lot like just a different waiting of
a lot like just a different waiting of
on policy. We're kind of trying to push
on policy. We're kind of trying to push
on policy learning towards instead being
on policy learning towards instead being
behavioral cloning of uh whatever data
behavioral cloning of uh whatever data
you have that's currently higher quality
you have that's currently higher quality
than your average than like average
than your average than like average
policy quality. Okay.
not using. So important sampling is the
not using. So important sampling is the
opposite of what you want. If you
opposite of what you want. If you
actually look at important sampling, it
actually look at important sampling, it
doesn't let you really use off policy
doesn't let you really use off policy
data.
data.
It kind of just throws away off policy
It kind of just throws away off policy
data.
Speaking of which, thanks for the
Speaking of which, thanks for the
reminder. I did forget to uh
reminder. I did forget to uh
disable the important sampling.
disable the important sampling.
I mean, I'm sure it's just getting worse
I mean, I'm sure it's just getting worse
and worse. Oh.
Okay.
Okay.
Did learn for a moment.
Okay. So, you do have now the imitation
Okay. So, you do have now the imitation
loss, right?
and
it's weighted by this
this one.
Now it doesn't learn anymore.
Words do sum
Words do sum
just sorting segments by reward.
And then
thumb.
You're trying to get full-fledged PO.
You're trying to get full-fledged PO.
Well, it's not really going to be PO
Well, it's not really going to be PO
anymore, right? because like the
anymore, right? because like the
clipping term is for keeping your data
clipping term is for keeping your data
on policy. So you're going to delete
on policy. So you're going to delete
that. So you basically are just keeping
that. So you basically are just keeping
the advantage function
reinforced looks exactly like behavioral
reinforced looks exactly like behavioral
cloning weighted by return right and
cloning weighted by return right and
then po if you ignore the clipping the
then po if you ignore the clipping the
other thing is it's um it's weighted by
other thing is it's um it's weighted by
an advantage function right
and like there are algorithms that are
and like there are algorithms that are
based on um like behavioral cloning
based on um like behavioral cloning
weighted by advantage. The question is,
weighted by advantage. The question is,
if we filter our data carefully, can we
if we filter our data carefully, can we
get it to work
get it to work
like without expert data?
like without expert data?
Do we just like full bootstrap?
I've seen like flashes of brilliance
I've seen like flashes of brilliance
from um like this type of stuff already.
from um like this type of stuff already.
It's like definitely not insane.
It's like definitely not insane.
It's just like is there a good version
It's just like is there a good version
of this that I can figure out in a
of this that I can figure out in a
reasonable amount of time?
So why the hell is that E+?
You probably don't want X on that, huh?
You probably don't want X on that, huh?
That probably breaks freaking
That probably breaks freaking
everything.
iteration.
iteration.
Yeah, that's the goal.
Yeah, that's the goal.
Is that what it is called? Expert. Well,
Is that what it is called? Expert. Well,
except the fact that like you're
except the fact that like you're
starting with nothing, right?
Wait, that's definitely not
[Music]
Was
it one at every step?
Huh?
Huh?
Funny.
Literally just least number of falls.
kind of why sampling
needed.
Think the value function would fix this.
Weirdo. I haven't been able to get like
Weirdo. I haven't been able to get like
the simplest thing I tried is the thing
the simplest thing I tried is the thing
that I think has worked the best so far,
that I think has worked the best so far,
but it has blind spots.
but it has blind spots.
Yeah, it's similar to that, right? I'm
Yeah, it's similar to that, right? I'm
trying to do something pretty similar to
See the sampling thing now.
Okay,
keeping around the highest reward
keeping around the highest reward
segments.
What is is it the freaking advantage
What is is it the freaking advantage
waiting that's messing me up?
Okay, so actually the advantage waiting
Okay, so actually the advantage waiting
is messing it up.
If you just grab your higher reward
If you just grab your higher reward
data, it works.
What's the overall goal of this then?
What's the overall goal of this then?
Well, I mean, what I would like to be
Well, I mean, what I would like to be
able to do
able to do
is I would like to be able to
I'd like to be able to get the same PF
I'd like to be able to get the same PF
as my normal on policy learning, but I'd
as my normal on policy learning, but I'd
like to also be able to use old data and
like to also be able to use old data and
thereby crank up compute and train more.
I suppose the bit that's unknown, right,
I suppose the bit that's unknown, right,
is like
is like
well, you don't know this should work.
well, you don't know this should work.
If you're taking your if you're like if
If you're taking your if you're like if
advantage weighted regression, it's
advantage weighted regression, it's
it's basically the exact same objective
it's basically the exact same objective
is on policy learning, right?
basically the same objective.
It's like a negative
It's like a negative
mind flip.
Not just like a dumb sign flip mistake,
Not just like a dumb sign flip mistake,
right?
For some reason,
waiting by the advantage like this.
Kreg
Kreg
PO doesn't have KL rag in the clipped
PO doesn't have KL rag in the clipped
form.
Welcome.
I guess the question is why like every
I guess the question is why like every
single advantage weighted form of this
single advantage weighted form of this
I've tried has not really worked.
Oh yeah.
Why can't I wait this by?
I wonder if it's the normalization being
I wonder if it's the normalization being
screwy
or
um
um
I don't know what the point of it would
I don't know what the point of it would
be.
be.
have kale rag.
have kale rag.
You need it.
You need it.
You're not trying to uh there's no on
You're not trying to uh there's no on
policy thing to do
this form.
this form.
It wouldn't be this norm, right?
It wouldn't be this norm, right?
And this is going to mess up.
And this is going to mess up.
It still got J.
I mean, that did something for a second.
I mean, that did something for a second.
Too big, but um I wonder
I wonder if it's just the uh the norm on
I wonder if it's just the uh the norm on
it.
It's a trivial policy.
You don't want mean advantage at all.
You don't want mean advantage at all.
Can't subtract the mean.
Divide
by maximum.
Yeah, that's not I mean that's not
Yeah, that's not I mean that's not
crashed totally at least for a little
crashed totally at least for a little
bit, but that's not uh stable at all.
Super obnoxious like
literally if you just do this form of
literally if you just do this form of
it.
it.
Yeah, this form is just IIL on good uh
Yeah, this form is just IIL on good uh
on high reward segments, right?
And this works.
Why do we still need an off policy
Why do we still need an off policy
buffer? Uh two reasons. The goal is one
buffer? Uh two reasons. The goal is one
so that we can actually do sample
so that we can actually do sample
efficiency if we want to. we add that
efficiency if we want to. we add that
capability to puffer and uh two is
capability to puffer and uh two is
that uh exploration is really hard when
that uh exploration is really hard when
stuff is really sparse when you don't
stuff is really sparse when you don't
keep around data. Hey Spencer, what's
keep around data. Hey Spencer, what's
up?
I was off policy land hard. Taking top K
I was off policy land hard. Taking top K
leads to no stochasticity.
leads to no stochasticity.
Yeah, I've tried top K a whole bunch and
Yeah, I've tried top K a whole bunch and
it never works.
it never works.
Switch to sack. Now,
Switch to sack. Now,
I I've been looking at a lot of the off
I I've been looking at a lot of the off
policy lit and I I really think that
policy lit and I I really think that
it's just the case that the algorithms
it's just the case that the algorithms
all suck. They let you reuse samples,
all suck. They let you reuse samples,
but they suck. So, like
but they suck. So, like
I at least I could be wrong, but you
I at least I could be wrong, but you
know, here's an experiment for anybody
know, here's an experiment for anybody
who wants to. If you have some off
who wants to. If you have some off
policy implementation that you have, try
policy implementation that you have, try
to get it to match wall clock time with
to get it to match wall clock time with
puffer on anything.
puffer on anything.
Because as far as I've seen, it seems to
Because as far as I've seen, it seems to
me like
me like
it seems to me like it's just a bad
it seems to me like it's just a bad
algorithm that you crank compute
algorithm that you crank compute
through. But you wouldn't want that,
through. But you wouldn't want that,
right? You want the same algorithm that
right? You want the same algorithm that
does in the case where you have a lot of
does in the case where you have a lot of
data, it's fast. In the case where you
data, it's fast. In the case where you
don't have a lot of data, you can spend
don't have a lot of data, you can spend
more compute to still learn.
like switching to S like to this SACE
like switching to S like to this SACE
doesn't do really anything for Yes.
I've tried several things, Spencer.
We doing absolute advantage or we could
We doing absolute advantage or we could
try just um
No. So, right now we're just doing we
No. So, right now we're just doing we
were having so much pro like trouble
were having so much pro like trouble
with advantage that right now this is
with advantage that right now this is
just reward. The thing that does
just reward. The thing that does
consistently work um
consistently work um
is just doing imitation on high reward
is just doing imitation on high reward
segments
segments
like that actually works.
Now the thing is it's probably not going
Now the thing is it's probably not going
to ever be as efficient as waiting by an
to ever be as efficient as waiting by an
advantage estimate because
advantage estimate because
when you just do high reward segments,
when you just do high reward segments,
you're treating the whole segment as
you're treating the whole segment as
good.
positive, negative reward. Now, I think
positive, negative reward. Now, I think
right now we're just doing high positive
right now we're just doing high positive
reward.
I mean, for breakout, it's um
I mean, for breakout, it's um
negative wouldn't even do anything. That
negative wouldn't even do anything. That
wouldn't make a That doesn't even give
wouldn't make a That doesn't even give
you anything.
I'd like to know why waiting by
I'd like to know why waiting by
advantage breaks everything.
Shouldn't it be
if I do
if I do
isn't it positive advantage times mean?
isn't it positive advantage times mean?
Yeah, because it's it should be positive
Yeah, because it's it should be positive
advantage times mean with no norm.
Oh,
okay.
Still not super stable, but um
Still not super stable, but um
that's something, right?
Learn the value network with the
Learn the value network with the
standard algorithm
standard algorithm
and use that. Wait, you learn the value
and use that. Wait, you learn the value
network with the standard algorithm. We
network with the standard algorithm. We
do have the standard, right?
do have the standard, right?
The standard value net is just going to
The standard value net is just going to
predict returns. It's the exact thing we
predict returns. It's the exact thing we
have here. It's a clipped value function
have here. It's a clipped value function
loss.
loss.
Good value network to compute advantage
Good value network to compute advantage
here.
You mean learn?
What does it mean to learn the value
What does it mean to learn the value
network with the standard algorithm? I
network with the standard algorithm? I
think that's what I'm doing.
I had a negative sign because it's it's
I had a negative sign because it's it's
grad of it's negative log props or
grad of it's negative log props or
whatever. I had a negative here because
whatever. I had a negative here because
they changed the objective.
So I mean this is kind of something. Is
So I mean this is kind of something. Is
this consistent?
Yeah, that's consistent. Okay.
Yeah, that's consistent. Okay.
Shouldn't it be negative though?
Shouldn't it be negative though?
I don't think so. Right. For imitation
I don't think so. Right. For imitation
learning, it's advantage times the uh
learning, it's advantage times the uh
behavioral cloning objective. Right. I
behavioral cloning objective. Right. I
think it's not negative advantage times
think it's not negative advantage times
ratio. I think it's supposed to be the
ratio. I think it's supposed to be the
actual losses advantage times negative
actual losses advantage times negative
ratio.
Isn't it grad of like negative log prop
Isn't it grad of like negative log prop
or whatever?
The clipping definitely doesn't make Get
The clipping definitely doesn't make Get
rid of this.
final value network. Yeah.
Right. So that's why I didn't filter
Right. So that's why I didn't filter
based on advantage. I filtered based on
based on advantage. I filtered based on
reward. Right.
reward. Right.
We're up top.
I suppose that is the next thing would
I suppose that is the next thing would
be to
sample based on advantage.
Okay. Similar to before.
So we can sample based on
So we can sample based on
based on this.
What happens if I do cart pull?
kind of janky.
kind of janky.
Yes.
Oh, you know we do actually remember.
Oh, you know we do actually remember.
Hang on.
Bandages get stale,
liked your post on learning programming
liked your post on learning programming
in ML. Thanks. Yeah, no worries.
Really has a lot of my best
Really has a lot of my best
best advice.
Okay. So, this is storing more data than
Okay. So, this is storing more data than
before and doing worse. So, this is 100%
before and doing worse. So, this is 100%
a sampling problem, right?
It's actually better on breakout, which
It's actually better on breakout, which
is funny. despite the sampling problem.
is funny. despite the sampling problem.
I think you just need more data.
It's actually kind of like okay.
Okay. So
Okay. So
the problem with sampling, right?
the problem with sampling, right?
Problem with sampling is
there are a couple things.
Ideally, you want to sample based on
Ideally, you want to sample based on
the advantage, right?
the advantage, right?
That's like information gain.
But advantage
But advantage
advantage can get stale.
You hate AI coding.
Get off an iteration speed. It doesn't
Get off an iteration speed. It doesn't
make it faster. It makes it slower.
Like if you're actually a good
Like if you're actually a good
programmer, it's kind of the same thing
programmer, it's kind of the same thing
as like asking a good mathematically
as like asking a good mathematically
inclined researcher why they don't just
inclined researcher why they don't just
outsource all their math to uh to an an
outsource all their math to uh to an an
LLM to make them be able to do their
LLM to make them be able to do their
math faster, right? It's cuz the LLMs
math faster, right? It's cuz the LLMs
are really stupid.
There's like not a single thing I've
There's like not a single thing I've
done today that an LM would have made
done today that an LM would have made
faster.
faster.
Actually, the the one thing that it does
Actually, the the one thing that it does
make faster is what you see here, which
make faster is what you see here, which
is the single line really stupid
is the single line really stupid
autocompletes that saves me typing
autocompletes that saves me typing
that
Let's just for the heck of it, right?
Let's just update
Let's just update
the value function, right?
Let's just update the value function.
I've coding learned helplessness.
Yeah, I'm very happy just ignoring the
Yeah, I'm very happy just ignoring the
LLM people and telling them they're dumb
LLM people and telling them they're dumb
and like, you know, I'm here. I'm I'll
and like, you know, I'm here. I'm I'll
be waiting on the day that like people
be waiting on the day that like people
are actually writing like good useful
are actually writing like good useful
libraries and and like advancing
libraries and and like advancing
research and science with these things.
research and science with these things.
But for now, it's to me it mostly just
But for now, it's to me it mostly just
seems like a lot of people who are lazy
seems like a lot of people who are lazy
and dumb and decided to not invest time
and dumb and decided to not invest time
in actually learning how things work
in actually learning how things work
feeling smuggly superior because they
feeling smuggly superior because they
can now type in random sentences in
can now type in random sentences in
English and think that they understand
English and think that they understand
how things work and are useful. They're
how things work and are useful. They're
not. They're still stupid and lazy.
like
like
literally just like rldled you into
literally just like rldled you into
feeling like you can do
modify it in place.
modify it in place.
uh it does modify it in place and then I
uh it does modify it in place and then I
added the return afterwards.
So because like people expect functions
So because like people expect functions
to return things I added the return
to return things I added the return
afterwards but yes it does modify it in
afterwards but yes it does modify it in
place
and it has to because otherwise you have
and it has to because otherwise you have
to allocate a fresh tensor which is
to allocate a fresh tensor which is
slower. So you can actually
slower. So you can actually
pre-allocate, which to be fair, we're
pre-allocate, which to be fair, we're
not really doing nicely in the way we
not really doing nicely in the way we
should be doing it here. But
should be doing it here. But
I wrote it that way so I at least I
I wrote it that way so I at least I
could optimize it. And it does make a
could optimize it. And it does make a
difference when you when these
difference when you when these
advantages uh the advantage tensors get
advantages uh the advantage tensors get
too big.
too big.
All right. So,
All right. So,
this should be
good. That's a good slice.
Ah, yeah. It's segments, isn't it?
Ah, yeah. It's segments, isn't it?
Slightly obnoxious.
right adding a little noise to the
right adding a little noise to the
states and actions.
Uh why would you do that?
That would be like an overfitting
That would be like an overfitting
regularization thing, right?
regularization thing, right?
I can't think of why else you would do
I can't think of why else you would do
that.
Okay.
I mean that's nice and stable at least,
I mean that's nice and stable at least,
right?
Maybe less stale data to directly copy.
Maybe less stale data to directly copy.
Less stale?
Less stale?
What do you mean less stale data to
What do you mean less stale data to
directly copy?
directly copy?
Well, here now I know for sure the value
Well, here now I know for sure the value
is not stale.
Oh, but actually no, I did this wrong,
Oh, but actually no, I did this wrong,
right?
right?
Belf gold values.
Can you just update the priority?
Can you just update the priority?
What do you mean update the priority in
What do you mean update the priority in
the tree? So the reason that it gets
the tree? So the reason that it gets
stale
stale
um is like you're storing a cache like
um is like you're storing a cache like
you're caching a value estimate so that
you're caching a value estimate so that
what you can compute advantages,
what you can compute advantages,
right? Because otherwise you have to do
right? Because otherwise you have to do
the full forward pass every time you
the full forward pass every time you
want to compute advantages.
want to compute advantages.
Um,
Um,
and the problem with that is if you're
and the problem with that is if you're
sampling based on advantage, like let's
sampling based on advantage, like let's
say you optimize one data point and
say you optimize one data point and
advantage goes to zero, you're not going
advantage goes to zero, you're not going
to sample it again, even though like
to sample it again, even though like
you're going to forget that data point
you're going to forget that data point
later on for sure.
later on for sure.
But because you haven't updated the
But because you haven't updated the
value, you can't see that.
value, you can't see that.
In practice, it seems like this didn't
In practice, it seems like this didn't
make a big difference. We get pretty
make a big difference. We get pretty
much the same results as before.
much the same results as before.
Insignificantly higher, I'd say.
But this was definitely an issue before.
But this was definitely an issue before.
So, we're at 172
and break out.
Uh there's still more to do here,
Uh there's still more to do here,
though.
though.
We're storing segments based on
storing segments based on reward is kind
storing segments based on reward is kind
of just not optimal, right?
Wouldn't be used. Well, because you you
Wouldn't be used. Well, because you you
have to update the value estimate,
have to update the value estimate,
right? Because if you're not going to
right? Because if you're not going to
sample it, if you're computing estimate
sample it, if you're computing estimate
uh advantage based on the stale value,
uh advantage based on the stale value,
then you're never going to use it. You
then you're never going to use it. You
have to update the value so that when
have to update the value so that when
you update the advantage estimate, you
you update the advantage estimate, you
actually get something recent.
I'll find some hack to get around this
I'll find some hack to get around this
at some point so I don't have to do this
at some point so I don't have to do this
full pass over the buffer, right? But
full pass over the buffer, right? But
this is for dev and can do this now for
this is for dev and can do this now for
dev
172.
172.
Let's
do a cart pull.
do a cart pull.
Actually, not sure if this does cart
Actually, not sure if this does cart
pull because of the way the uh reward
pull because of the way the uh reward
buffer works.
Uh, it does. It is just slow.
Uh, it does. It is just slow.
Interesting.
Interesting.
All right. So, like what portions of
All right. So, like what portions of
this
What portions of the current approach
What portions of the current approach
should make this slower than uh normal
should make this slower than uh normal
PO
advantage waiting applied to samples
advantage waiting applied to samples
elementwise.
elementwise.
That's basically the same loss.
That is basically the same loss.
That is basically the same loss.
I think there's some variance in it.
I think there's some variance in it.
Right.
is it?
is it?
You want to just store all the states?
You want to just store all the states?
Can't store all the states.
What do you want?
What do you want?
You want your data buffer to have
You want your data buffer to have
essentially
essentially
the expert demonstrations, right?
What you want is you want your data
What you want is you want your data
buffer to have the expert
buffer to have the expert
demonstrations.
That would be the high reward. Like
That would be the high reward. Like
that's the high value ones, not the high
that's the high value ones, not the high
advantage ones.
advantage ones.
Right?
So what we have to do then we have to
So what we have to do then we have to
train a separate value function right
wait
returns
now. What are we what are we training
now. What are we what are we training
this thing against?
this thing against?
Value function
advantages
advantages
plus value
New value minus value plus advantage I
New value minus value plus advantage I
guess.
guess.
Yes. So your value function is
better off. You're doing off Paul. No
better off. You're doing off Paul. No
expertise
expertise
be better.
be better.
Uh we have we have prioritized
Uh we have we have prioritized
experience replay.
experience replay.
It's it's um technically it's behavioral
It's it's um technically it's behavioral
cloning. The objective is behavioral
cloning. The objective is behavioral
cloning, not off policy. Uh not off
cloning, not off policy. Uh not off
policy today
policy today
based on some stuff I found.
based on some stuff I found.
And like we actually have it learning
And like we actually have it learning
some stuff which is kind of crazy.
some stuff which is kind of crazy.
Like literally this is a behavioral
Like literally this is a behavioral
cloning objective, right?
You're literally doing cross entropy
You're literally doing cross entropy
between logits and actions. You're
between logits and actions. You're
waiting by advantage.
Hang on. Am I wrong? Is the value
Hang on. Am I wrong? Is the value
function.
function.
But what is the value function learning
But what is the value function learning
to predict in?
Is it learning to predict discounted
Is it learning to predict discounted
returns or is it learning to predict? It
returns or is it learning to predict? It
is learning to predict just discounted
is learning to predict just discounted
returns, right?
Yeah.
So in that case, hang on.
So in that case, hang on.
In that case, I know what to do, right?
We update the values for everything.
We update the values for everything.
Yes.
know what to do. We move this up.
Okay. So you're updating your all values
Okay. So you're updating your all values
estimate,
estimate,
right? And then we do prayer
Plus.
So this is return plus value
Bad 189.
This is how you'd want to store it,
This is how you'd want to store it,
right?
you'd want to use um this metric.
you'd want to use um this metric.
Okay. Okay. And then the only the only
Okay. Okay. And then the only the only
thing here now is
uh the advantage based sampling I
uh the advantage based sampling I
believe right
believe right
and then how we get this correct.
Okay. I mean this works pretty much like
Okay. I mean this works pretty much like
it did before.
So now we are sorting by
So now we are sorting by
or now we are prioritizing by advantage.
I think we also did.
Let's see if this does anything.
Wait it by priority, you know.
Oh yeah.
Oh yeah.
Okay.
Okay.
over 200 now, right?
Uh, advantages are not normalized.
Uh, advantages are not normalized.
I think normalizing advantages breaks it
I think normalizing advantages breaks it
though, right?
Yeah. So, normalizing advantages
breaks it.
So, we're going to need something else
So, we're going to need something else
in place of that
in place of that
cuz that's like a major major thing.
cuz that's like a major major thing.
But being able to get over 200 on
But being able to get over 200 on
breakout already,
breakout already,
pretty damn good.
pretty damn good.
Uh, we can get rid of this log ratio
Uh, we can get rid of this log ratio
stuff, right?
Don't need any of this.
There. No more tails.
Mhm.
Mhm.
Uh, of course there is like the question
it's technically possible that we're
it's technically possible that we're
just like using the off the on Paul data
just like using the off the on Paul data
anyways
anyways
and that's why it's stable.
Wouldn't think so though.
sampling by
sampling by
I probably should be doing absolute
I probably should be doing absolute
value of advantage. I'm just doing
value of advantage. I'm just doing
advantage now if I recall correctly.
advantage now if I recall correctly.
Some sort of bounce positive and
Some sort of bounce positive and
negative.
negative.
I think it's all positive. Check.
I think it's all positive. Check.
Positive data is better than negative
Positive data is better than negative
data, right?
I think for behavioral cloning you
I think for behavioral cloning you
actually you do want positive data
actually you do want positive data
because you want like it's not that
because you want like it's not that
informative to have a data set of
informative to have a data set of
negatives.
negatives.
You really just learn from the positive
You really just learn from the positive
examples.
Oh, you know what? It's currently
Oh, you know what? It's currently
sampling by absolute value. You're
sampling by absolute value. You're
right.
I mean, it's technically
be surprised if this is
be surprised if this is
that different. We'll see.
All right. I don't think that's a
All right. I don't think that's a
statistically different. I think that's
statistically different. I think that's
just in the noise.
So, okay. I guess then the question is
So, okay. I guess then the question is
why
why
why are we not as good as
why are we not as good as
our main
Hang on. We also have tuned parameters
Hang on. We also have tuned parameters
for a totally different algorithm now,
for a totally different algorithm now,
right?
These two co-ops for example
hopefully odd. It's um
hopefully odd. It's um
it's less than one.
That's prioritized experience replay.
That's prioritized experience replay.
There's nothing custom there.
Okay, that's the same as before,
Okay, that's the same as before,
but um off Paul correction is totally
but um off Paul correction is totally
bad for this. So, I think this is good.
bad for this. So, I think this is good.
And then
like we should be able to increase
like we should be able to increase
update epochs. Now, I don't know if this
update epochs. Now, I don't know if this
actually works, but we should be able to
actually works, but we should be able to
do like update epoch equals 4,
do like update epoch equals 4,
right? Like, shouldn't this work Now,
And it crashes. Oh, it nanned out
And it crashes. Oh, it nanned out
though.
It's possible we just have to sweep this
It's possible we just have to sweep this
thing.
We have some sanities we can do, right?
we can do as a sanity check here, right?
What if we run this?
I mean, okay, this is um
I mean, okay, this is um
this says that this is better our off uh
this says that this is better our off uh
our off data our off data version is
our off data our off data version is
better than our onpaul data version.
better than our onpaul data version.
So, that could just be a matter of
So, that could just be a matter of
tuning.
We've changed a whole bunch of stuff.
use our restroom real quick and then I'm
use our restroom real quick and then I'm
going to see if I can set up um
going to see if I can set up um
I'm going to do a few more manual
I'm going to do a few more manual
experiments, I think, and then try to
experiments, I think, and then try to
see if I can set up some some form of
see if I can set up some some form of
sweep.
sweep.
I'll be right back. This potentially
I'll be right back. This potentially
pretty uh pretty novel though.
How cool would it be if this worked?
How cool would it be if this worked?
I mean, there's some tests we should be
I mean, there's some tests we should be
able to do.
It should get, if you have your sampling
It should get, if you have your sampling
set up correctly, it should get strictly
set up correctly, it should get strictly
better with um a bigger buffer size,
better with um a bigger buffer size,
right?
Make sure we can get this to work.
So, this one should do worse, right?
So, uh, this kind of stalled out.
So, uh, this kind of stalled out.
It only has a million samples to train
It only has a million samples to train
on.
on.
Okay.
Ignoring
the inefficiency of this implementation.
the inefficiency of this implementation.
If this does not do better, it is a
If this does not do better, it is a
sampling issue.
Not bad. 267.
And then this also should let me run
And then this also should let me run
more updates in turn.
Of course, there's a lot of noise in
Of course, there's a lot of noise in
these, right? So, if it doesn't do
these, right? So, if it doesn't do
strictly better, um, if it crashes, we
strictly better, um, if it crashes, we
know there's a bug
from variety.
Okay. So, like this just didn't sample
Okay. So, like this just didn't sample
well.
We'll go back to 8 mil for now.
Do two update epox
changes anything.
Which plugin? Uh, this is Super Maven
44. Okay, so this didn't do any better
44. Okay, so this didn't do any better
with more updates.
Fix that.
Oh, you know why? It's because I'm a
Oh, you know why? It's because I'm a
dummy. Literally not changing anything.
or at least for that test.
Okay. So, not really
Okay. So, not really
not really any better, right?
not really any better, right?
just with these this setup.
We get to get rid of a whole bunch of
We get to get rid of a whole bunch of
the um
the um
the hypers though, don't we?
Coefficient
Good number of parameters here
Good number of parameters here
be changed.
It actually does kind of concern me,
It actually does kind of concern me,
right, that we don't do.
right, that we don't do.
We should do better with more update
We should do better with more update
epochs in this, shouldn't we?
At least we should be able to get to
At least we should be able to get to
like
like
some no form of sample reuse.
This at least needs to be stable, right?
Nan's out. So this is like learning rate
Nan's out. So this is like learning rate
pars and such I suppose.
pars and such I suppose.
Um
Um
we can tune those. I am a little
we can tune those. I am a little
curious.
Behavioral cloning is supposed to scale
those. I already have these.
those. I already have these.
Uh, this is supposed to scale better,
Uh, this is supposed to scale better,
isn't it?
I don't see how it possibly could
I don't see how it possibly could
because
because
Oh, maybe.
Of course, that's going to change
Of course, that's going to change
optimal learning, right? And a bunch of
optimal learning, right? And a bunch of
other crap
and not really a distinct advantage,
and not really a distinct advantage,
huh?
huh?
Okay.
Well, let's just do um
just do the easy to sweep ones, right?
We'll leave these alone.
Okay.
I would like to have seen um I think
I would like to have seen um I think
like a slightly higher number
like a slightly higher number
in order to be reasonably uh confident.
I mean, but this is kind of a crazy
I mean, but this is kind of a crazy
thing to do.
So
I suppose the next thing is to just
I suppose the next thing is to just
think about if there are any fundamental
think about if there are any fundamental
things that make this more or less
things that make this more or less
powerful.
I mean the pros here right the thing
I mean the pros here right the thing
that this has going for it is data reuse
that this has going for it is data reuse
right that's the key
and it's data use at the level of the
and it's data use at the level of the
trajectory segment
compared to the PO objective.
I mean it's
the thing is the behavioral cloning
the thing is the behavioral cloning
objective is so close to the online
objective is so close to the online
objective that it should learn better
objective that it should learn better
with online data, right?
Can we think of it that way?
It makes sense, right? Because you get
It makes sense, right? Because you get
you get like a direct gradient
based off of your actions, rewards.
The other form of this I suppose would
The other form of this I suppose would
be to do explicit
on policy and behavioral clothing,
on policy and behavioral clothing,
right?
That would be the next thing to try.
That would be the next thing to try.
would be to make it explicit, right?
would be to make it explicit, right?
the separate phases.
I mean that still pairs relatively
I mean that still pairs relatively
nicely, right?
So this for this first piece is
So this for this first piece is
literally
literally
it's the same right.
The only difference is they put the
The only difference is they put the
advantage in here
versus behavioral cloning. It's one,
versus behavioral cloning. It's one,
right?
That's the only difference.
So in this setup, you're not assuming
So in this setup, you're not assuming
that the data is better than the current
that the data is better than the current
policy, right?
policy, right?
In the online RL setup, you're not
In the online RL setup, you're not
assuming that the data is better than
assuming that the data is better than
the current policy. In fact,
the current policy. In fact,
it's it is data from the current policy.
it's it is data from the current policy.
The some of it's better than average,
The some of it's better than average,
some of it's worse than average, and a
some of it's worse than average, and a
lot of it's pretty average
in the behavioral cloning setup.
in the behavioral cloning setup.
you're assuming that the data is just
you're assuming that the data is just
better than the current policy.
Of course, like the naive,
Of course, like the naive,
let's say we train a um
let's say we train a um
a good advantage function, right?
The typical behavioral cloning setup, we
The typical behavioral cloning setup, we
could use maybe the top x% of the data.
could use maybe the top x% of the data.
Whereas here, you get to use all the
Whereas here, you get to use all the
data.
In the typical behavioral cloning setup,
In the typical behavioral cloning setup,
you don't weight it by advantage, though
you don't weight it by advantage, though
you can weight it by advantage as well.
you can weight it by advantage as well.
Be found as long as you're careful about
Be found as long as you're careful about
it.
We also know that behavioral cloning is
We also know that behavioral cloning is
a pretty strong baseline, right?
When DDPG Okay.
I think this is actually not like a
I think this is actually not like a
sample efficiency thing, right?
Yeah, this is not like a sample
Yeah, this is not like a sample
efficiency thing. This is actually kind
efficiency thing. This is actually kind
of more like puffer style. just happens
of more like puffer style. just happens
to be um
actually the fact that they report all
actually the fact that they report all
their stuff in uh in seconds actually
their stuff in uh in seconds actually
pretty much guarantees that this is
pretty much guarantees that this is
or a puffer style thing. So looking at
or a puffer style thing. So looking at
this for sample F is probably not great
this for sample F is probably not great
unless they have a comparison.
Yeah, this is I think they just took
Yeah, this is I think they just took
some random offpaul algorithm, right?
some random offpaul algorithm, right?
They kind of did some similar stuff to
They kind of did some similar stuff to
what Puffer does to make it fast.
Yeah, that's totally what they did.
Yeah, that's totally what they did.
Okay. So, I guess like
this paper is still one of my favorites,
but it's so tough to judge things like
but it's so tough to judge things like
this because
I mean they have the wall time,
I mean they have the wall time,
but they cut samples so you don't
but they cut samples so you don't
actually know like
Like, can this do better than puffer
Like, can this do better than puffer
given the same amount of wall clock
given the same amount of wall clock
on like one of our tasks?
like a few thousand steps per second. If
like a few thousand steps per second. If
I recall
I mean, this is a good paper. It's just
I mean, this is a good paper. It's just
um
tough to compare here, All right.
and honestly the biggest improvement was
and honestly the biggest improvement was
just swapping the uh the architecture.
just swapping the uh the architecture.
Funny enough,
we could pretty easily do architecture
we could pretty easily do architecture
stuff.
stuff.
I'll look at sample efficiency.
The thing that's like obnoxious I think
The thing that's like obnoxious I think
is just that um
the sample efficiency setup
at least as it's normally defined as a
at least as it's normally defined as a
hardware inefficient setup.
Guess it doesn't have to be, right?
Just like train reasonable batch sizes.
Just like train reasonable batch sizes.
Don't have a ton of parallel ends,
Don't have a ton of parallel ends,
but then you use more compute on the
but then you use more compute on the
train pass anyways.
train pass anyways.
Fire replay buffer.
Fire replay buffer.
It's actually kind of feasible.
It's actually kind of
Five.
H.
It's going to be very difficult to beat
It's going to be very difficult to beat
like our optimized on policy setup with
like our optimized on policy setup with
this. Frankly, it could happen. I'd be
this. Frankly, it could happen. I'd be
surprised.
surprised.
I would like to see how well this does.
Wonder if it was mistaken to try to do
Wonder if it was mistaken to try to do
um
um
the on policy
set up.
set up.
Well, actually, you know what?
Well, actually, you know what?
The other thing that just is tough,
The other thing that just is tough,
um,
it's fully an artifact of the LSTM,
it's fully an artifact of the LSTM,
right?
Yeah. Because we have an LSTM, right?
Yeah. Because we have an LSTM, right?
You get off policy. You mess up your
You get off policy. You mess up your
LSTM state. Like, regardless of the
LSTM state. Like, regardless of the
algorithm you use, you're going to mess
algorithm you use, you're going to mess
up your LSTM state a bunch.
That's not great.
It's actually kind of a motivation for
It's actually kind of a motivation for
transformerbased architecture, isn't it?
Efficient at smaller batches.
possible.
Let's get messed up with that. Not
Let's get messed up with that. Not
really. Right.
really
is just deploy
is just deploy
but it actually probably the LSTM
but it actually probably the LSTM
architecture is probably driving me
architecture is probably driving me
towards on Paul more than I normally
towards on Paul more than I normally
would be.
would be.
I think that the
well the key finding here right
well the key finding here right
and this was the key finding from a
and this was the key finding from a
couple of days ago at this point but um
couple of days ago at this point but um
even doing a super super simple
even doing a super super simple
behavioral cloning thing with just the
behavioral cloning thing with just the
top whatever percent of your data even
top whatever percent of your data even
just by reward.
just by reward.
uh that already gives you qualitatively
uh that already gives you qualitatively
and in some very important cases better
and in some very important cases better
behavior than doing soda on policy
behavior than doing soda on policy
learning.
learning.
Very difficult to get it to solve like
Very difficult to get it to solve like
full tasks.
full tasks.
Sufficient motivation to include it
Sufficient motivation to include it
though in some capacity.
I think the formulation I came up with
I think the formulation I came up with
today is quite good for the uh the
today is quite good for the uh the
buffer.
Right. The buffer being
Right. The buffer being
stored by uh reward plus terminal value
stored by uh reward plus terminal value
instead of by advantage.
instead of by advantage.
That's really how you would do it if you
That's really how you would do it if you
wanted to build up like a good data set
wanted to build up like a good data set
over time, right?
I think we'll try tomorrow.
I think we'll try tomorrow.
We'll try to do it in phases.
We'll try to do
We'll try to do
our current on policy
our current on policy
uh with this extra
uh with this extra
like additional gold data set and we'll
like additional gold data set and we'll
do it like as an additional step.
We'll compare that.
We'll compare that.
That should be a lot easier to get to
That should be a lot easier to get to
work.
Unless this thing just happens to crush
Unless this thing just happens to crush
it, which I doubt.
it, which I doubt.
This break
This break
been on 23 runs for a while, hasn't it?
How' I know
the heck is this?
Somehow managed to screw something up
Somehow managed to screw something up
there.
want it to stay
8:16.
start on this now or no.
I suppose I could start on it now.
Kind of want to just do have all day
Kind of want to just do have all day
tomorrow to do it though.
for a bit.
I mean, that should have to work, right?
I mean, that should have to work, right?
And I think that the mistake I made last
And I think that the mistake I made last
time was I didn't train the uh
time was I didn't train the uh
I didn't train the value function
I didn't train the value function
when I tried this.
That does leave us with an annoyingly
That does leave us with an annoyingly
complicated algorithm, I guess. But
complicated algorithm, I guess. But
I think we'll be able to strip off
I think we'll be able to strip off
things we don't need over time from
things we don't need over time from
there.
Okay, I think that's what I'm going to
Okay, I think that's what I'm going to
do. I think I'm going to call it early.
do. I think I'm going to call it early.
I'm going to get myself an early dinner.
I'm going to get myself an early dinner.
Um,
I'm going to come back tomorrow
and we'll have a full day to just crank
and we'll have a full day to just crank
this stuff out.
this stuff out.
And hopefully by the end of tomorrow I
And hopefully by the end of tomorrow I
have at least a usable version of
have at least a usable version of
uh, behavioral cloning integration.
uh, behavioral cloning integration.
Cut off that
Cut off that
working later.
Well,
I suppose I'm biased towards thinking
I suppose I'm biased towards thinking
that there is a very simple method that
that there is a very simple method that
should just work. I shouldn't just have
should just work. I shouldn't just have
to keep bolting stuff on
to keep bolting stuff on
to the algorithm.
to the algorithm.
The behavioral cloning one.
It's not that bad. I suppose
I was hoping we would get like one
I was hoping we would get like one
algorithm, right?
That could do uh that could do both.
Actually, I think that this will be the
Actually, I think that this will be the
last question I'll have for the stream
last question I'll have for the stream
if um if anybody has any ideas today. So
if um if anybody has any ideas today. So
the setup here right is that you have a
the setup here right is that you have a
whole section of researchers who have
whole section of researchers who have
been aggressively optimizing for sample
been aggressively optimizing for sample
efficiency working on off policy
efficiency working on off policy
algorithms and to them like compute is
algorithms and to them like compute is
free right like compute is just free so
free right like compute is just free so
they've been like writing these
they've been like writing these
ridiculous not that great algorithms but
ridiculous not that great algorithms but
that let you infinitely reuse data Um
that let you infinitely reuse data Um
and therefore like by doing that you can
and therefore like by doing that you can
get pretty good. And then there's a
get pretty good. And then there's a
bunch of on policy research some of
bunch of on policy research some of
which is misguidedly on sample
which is misguidedly on sample
efficiency but a lot of it's just like
efficiency but a lot of it's just like
can we get the thing to learn fast
can we get the thing to learn fast
that's what we do in puffer
that's what we do in puffer
and this works very very well.
and this works very very well.
The problem is, at least as far as I've
The problem is, at least as far as I've
seen, it doesn't seem to me that you can
seen, it doesn't seem to me that you can
just take uh one of the best off policy
just take uh one of the best off policy
algorithms and just like run it fast on
algorithms and just like run it fast on
uh a ton of fresh data and have it do as
uh a ton of fresh data and have it do as
well as our on policy. And likewise, you
well as our on policy. And likewise, you
know, we can optimize our stuff for
know, we can optimize our stuff for
sample efficiency a bit, but we can't
sample efficiency a bit, but we can't
just go match the sample efficiency of
just go match the sample efficiency of
algorithms that reuse data. So how do we
algorithms that reuse data. So how do we
bridge this gap without doing something
bridge this gap without doing something
horrendous?
Right?
Right?
That is the key research question for
That is the key research question for
right now.
I keep finding skeletons in the closet
I keep finding skeletons in the closet
like freaking everywhere with this stuff
like freaking everywhere with this stuff
as well.
There's no code for this thing, is
There's no code for this thing, is
there?
there?
There is code for this thing.
I mean, I guess one thing I could do
I could take some time.
I could actually assign this to uh one
I could actually assign this to uh one
of my
stupid link.
Why can't I copy a link? This
Well, this is super simple.
So, um maybe what I should do is I
So, um maybe what I should do is I
should take
should take
I should take this
and like run this on
and like run this on
one of my buffer environments.
Like a relatively simple script.
Like a relatively simple script.
I didn't know that this had code
I didn't know that this had code
released.
Maybe we just hack this, right?
So what I would do is I'd take this
So what I would do is I'd take this
thing
to reoptimize hypers as well is the
to reoptimize hypers as well is the
problem
problem
I was trying to figure out right like if
I was trying to figure out right like if
I take this thing
I take this thing
And I run it at like puffer batch sizes.
If that still does something.
It's actually kind of tricky. Okay.
and actually come to think of it like
the thing I was thinking about doing
the thing I was thinking about doing
tomorrow. Does that even make sense?
like yes technically it le it allows you
like yes technically it le it allows you
to leverage some off policy
to leverage some off policy
data through behavioral cloning and I
data through behavioral cloning and I
think it would improve our on policy
think it would improve our on policy
stuff in a few cases
stuff in a few cases
um
don't think that gets you sample f
don't think that gets you sample f
though right
maybe it does
I guess the idea would be that you
you do most of the steps on
you do most of the steps on
the behavioral cloning objective.
A lot of work to do, frankly.
Okay. Well, that's interesting at least.
It actually be really tough to like do
It actually be really tough to like do
apples to apples of like any of you
apples to apples of like any of you
like let's say that we get BTR code
like let's say that we get BTR code
basically have to get it into puffer
basically have to get it into puffer
the whole thing.
Hope to not screw it up.
Hope to not screw it up.
There a safer way I can like test this.
I mean, yeah, right. I would just set
I mean, yeah, right. I would just set
the batch sizes to something crazy.
the batch sizes to something crazy.
Use um puffer networks.
See how it how quick it is and how well
See how it how quick it is and how well
it does.
There's not going to be an optimized
There's not going to be an optimized
implication.
Well, actually, that's kind of okay,
Well, actually, that's kind of okay,
right? I'll run it for if I run it for
right? I'll run it for if I run it for
the same number of steps,
the same number of steps,
I assume that I can optimize it to that
I assume that I can optimize it to that
point. It's good.
That might be a more practical thing to
That might be a more practical thing to
do tomorrow.
Regardless, I'm going to need a few more
Regardless, I'm going to need a few more
days to spend on um on this research
days to spend on um on this research
before I really get something. I think
uh I think it would also be interesting
uh I think it would also be interesting
to see if I threw this on neural MMO
to see if I threw this on neural MMO
if it does the same thing that the
if it does the same thing that the
behavioral cloning objective did,
behavioral cloning objective did,
which was um
which was um
to like start leveling up very very
to like start leveling up very very
early on.
I do think though that this is like
a weird in between.
literally half of the perf from
not half. See?
not half. See?
Yeah. Half of the Perf
Yeah. Half of the Perf
is just the network architecture.
Bunch of other tricks.
Lower.
Well,
it is Friday. I'm tired. Um,
it is Friday. I'm tired. Um,
been doing crazy research all week.
been doing crazy research all week.
Huh.
I want to get this run to be at least
I want to get this run to be at least
stable. Actually, would be good to have
stable. Actually, would be good to have
this thing actually tuned correctly.
this thing actually tuned correctly.
Get us like some interesting results.
Get us like some interesting results.
All right, though. I'm going to just
All right, though. I'm going to just
call it here and go get my dinner. So,
call it here and go get my dinner. So,
thank you folks for tuning in.
thank you folks for tuning in.
Um,
Um,
I will be back likely first thing in the
I will be back likely first thing in the
morning.
morning.
Interested in the research generally?
Interested in the research generally?
Buffer.ai for all the things or the
Buffer.ai for all the things or the
GitHub to help me out for free. Discord
GitHub to help me out for free. Discord
if you want to get involved. Amen.
