Kind: captions
Language: en
God love Linux drivers
cool and also let's go check up on our
cool and also let's go check up on our
runs I think we should have only killed
runs I think we should have only killed
uh the ones that were running on this
uh the ones that were running on this
box by doing that
okay so here's Endo Ando is going
okay so here's Endo Ando is going
along we
along we
have breakout which
have breakout which
is pretty well set it looks
like what's wrong score
there we
are
lovely interested to know why
lovely interested to know why
uh this is kind of cool if this
is I red this from the right Branch
is I red this from the right Branch
didn't
I I will double check on the code here
I I will double check on the code here
but if this is how if this is actually
but if this is how if this is actually
the we're getting out of this this is
the we're getting out of this this is
amazing I think it probably
is me
is me
see oh look at that
this isn't going to show me
right but we will double check on that
right but we will double check on that
I'm pretty sure I know why that works as
I'm pretty sure I know why that works as
well I think we kind of just yeah I
well I think we kind of just yeah I
think we hit the jackpot with
this I think we have a very nice weep
this I think we have a very nice weep
algorithm and let me see score
algorithm and let me see score
oh yeah look at that
oh yeah look at that
48 this thing was doing pretty
48 this thing was doing pretty
well we even have a nice burito front on
well we even have a nice burito front on
it
perfect
howdy so sweeps are going well I wanted
howdy so sweeps are going well I wanted
I don't want to get distracted though
I don't want to get distracted though
for a bit I want to make sure I finish
for a bit I want to make sure I finish
this stuff
well this isn't going to
work e
I don't think there's any I don't think
I don't think there's any I don't think
this works I'll double check this
yeah you can't normalize across the
yeah you can't normalize across the
batch like this because Advantage is
batch like this because Advantage is
supposed to be over time so this isn't
supposed to be over time so this isn't
going to
work this is definitely not working here
so one step turn doesn't
help e
how's grock
how's grock
3 I haven't tried it for coding
3 I haven't tried it for coding
yet I wouldn't be surprised I don't
yet I wouldn't be surprised I don't
really use these models much for like
really use these models much for like
writing code I use them for docs and for
writing code I use them for docs and for
like research and Math More
hang on what's this adaptive Horizon
hang on what's this adaptive Horizon
policy
gradient Advantage is going to be the
sum 1/ HT
sum 1/ HT
okay
plus okay so it's an Adaptive Horizon
difference which is determined
difference which is determined
dynamically per
state greater than Epsilon
okay
okay
so that's
so that's
interesting that's very
interesting the problem here is I'm
interesting the problem here is I'm
going to guess these experiments suck
that's usually the
problem
problem
perplexity I don't know
perplexity I don't know
pin pin and de or whatever perplexity
pin pin and de or whatever perplexity
seems cool it's a cool
company the thing is I think that
company the thing is I think that
they're serving lower quality models for
they're serving lower quality models for
the most part they don't have they're
the most part they don't have they're
not serving giant models
not serving giant models
right it's more for just for
search okay so this is proposed
search okay so this is proposed
for a compl completely different reason
three
billion they don't really have good
billion they don't really have good
tasks on this but
tasks on this but
like
like
maybe uncensored
R1 I mean that's
R1 I mean that's
cool e
hm
still this is a parameter but this is
still this is a parameter but this is
like something at least
but then what do you train the value
but then what do you train the value
function to
predict
e e
learned Advantage estimation
yeah that's crazy freaking expensive
States it's not trained to predict
States it's not trained to predict
Advantage though it's training to
Advantage though it's training to
predict Monte Carlo return right oh no
predict Monte Carlo return right oh no
wait it's the difference between
so this is what I was looking at based
so this is what I was looking at based
on reward
variants you should basically be able
variants you should basically be able
the idea here is you basically should
the idea here is you basically should
just be able to fit these parameters to
um to the reward
distribution e
where's this
paper sources
no not this one
I don't know if this paper
exists
for e
there might be a way of doing this
there might be a way of doing this
efficiently
oh it's not
R&D yeah this
one e
not quite what we
wanted e
go back to generalized Advantage
go back to generalized Advantage
estimation
this thing is just so hard to get rid
this thing is just so hard to get rid
of I see we have some folks on YouTube
of I see we have some folks on YouTube
now so um what I'm currently doing is
now so um what I'm currently doing is
I'm seeing if there's any obvious way of
I'm seeing if there's any obvious way of
replacing generalized Advantage
replacing generalized Advantage
estimation in reinforcement learning uh
estimation in reinforcement learning uh
it's a really obnoxious requirement at
it's a really obnoxious requirement at
the moment because essentially this
the moment because essentially this
thing makes it so that uh po will not
thing makes it so that uh po will not
work on multiple environment without
work on multiple environment without
retuning at least a couple of the hyper
retuning at least a couple of the hyper
parameters so if we could get rid of
parameters so if we could get rid of
this thing with something that like
this thing with something that like
either fits its own parameters or
either fits its own parameters or
bypasses that requirement somehow it
bypasses that requirement somehow it
would make Po a lot more robust and
would make Po a lot more robust and
require less
tuning e
so Lambda equals 0 there a couple things
so Lambda equals 0 there a couple things
we need to do
here the thing is we know that this is
here the thing is we know that this is
with Lambda equals z this thing doesn't
with Lambda equals z this thing doesn't
work particularly well now does
it yeah this is Lambda equal Z
yeah based on the values of Lambda that
yeah based on the values of Lambda that
are common it seems
are common it seems
like you mostly
like you mostly
want the Lambda equals 1 Formula this
want the Lambda equals 1 Formula this
just interpolates between the two so
just interpolates between the two so
you'll mostly want this one but a little
you'll mostly want this one but a little
bit this
bit this
one um the problem is this one has the
one um the problem is this one has the
discount Factor on
it e
pretty much any heuristic I can come up
pretty much any heuristic I can come up
with
with
right uh there's going to be a way to
right uh there's going to be a way to
screw
with like any heuristic I come up with
with like any heuristic I come up with
based
based
on the distribution of rewards are
on the distribution of rewards are
similar I'm sure I could come up with
similar I'm sure I could come up with
something that work for like all the
something that work for like all the
arcade games where there's kind of just
arcade games where there's kind of just
one source of
reward but I don't think it's going to
reward but I don't think it's going to
be particularly
robust hang on
so this only
so this only
uses you're only using the next two
uses you're only using the next two
value functions here
how screwy would it be if you just lured
how screwy would it be if you just lured
them
instead hang on does that even do
instead hang on does that even do
anything for
me for
well you're training the value function
well you're training the value function
to
to
predict hang
predict hang
on let me look at something
yeah so you're training to predict the
yeah so you're training to predict the
discounted returns
right e
what I'm currently thinking here
so you can't try to have this thing just
so you can't try to have this thing just
predict like the sum of rewards from now
predict like the sum of rewards from now
into the future forever
into the future forever
right uh you have to have it be
right uh you have to have it be
discounted because otherwise that thing
discounted because otherwise that thing
explodes so you want it to be discounted
reasonably but that's only because
reasonably but that's only because
you're making a single prediction
you're making a single prediction
right what if instead of predicting
right what if instead of predicting
discounted return you were just to
discounted return you were just to
predict the reward several steps into
predict the reward several steps into
the future like at each step let's say
the future like at each step let's say
that you predict the reward 32 steps
that you predict the reward 32 steps
into the future
right e
and let me go through this with Gro
did they do
[Music]
[Music]
this just
planning
e
e
e e
I mean this is kind of a crazy
I mean this is kind of a crazy
method that I just cooked
method that I just cooked
up it could do something
though it's just reward minus value
right e
I wonder if anybody's tried this
ever e
um I don't think so
hold
on this
on this
maybe e
let me see if I understand
this so it took my method
well hang on this is
not I don't think you can train it to
predict
e e
uh no
paper this is me mucking around with
paper this is me mucking around with
generalized Advantage estimation and
generalized Advantage estimation and
bouncing stuff off an
llm I'm trying to see if we can do
llm I'm trying to see if we can do
something to replace the LMA and gam uh
something to replace the LMA and gam uh
Lambda and Gamma
parameters rewarded each future step
we could St this
okay hang on I think I I got
it likes to write so damn
much
e
e e
I think this does
it so you get your loss function like
it so you get your loss function like
this right
this right
and then your
and then your
advantage you
advantage you
sum reward minus
value which is equal to sum reward minus
value which is equal to sum reward minus
some value
right that's equal to some reward minus
right that's equal to some reward minus
some value so it actually it only
some value so it actually it only
matters that you get the magnitude of
matters that you get the magnitude of
the reward right over the course of your
the reward right over the course of your
prediction
that gives you a nice loss function
the value function is
inaccurate yeah it should be
averaged that's fine
value functions early in is noisy early
value functions early in is noisy early
in training anyways
okay well we have the
okay well we have the
idea we have the
idea I have no idea whether this works
idea I have no idea whether this works
or
or
not see it's five I could try to
not see it's five I could try to
implement something on this at least
I could try to implement
something how you going to predict words
something how you going to predict words
into the
into the
future
TD into the I'm just going to change the
TD into the I'm just going to change the
out the size of the value head like the
out the size of the value head like the
value head outputs one variable right
value head outputs one variable right
what if I just make it output 32
what if I just make it output 32
variables I mean if you think about it
variables I mean if you think about it
right a lot of the point of the point of
right a lot of the point of the point of
doing uh generalized Advantage
doing uh generalized Advantage
estimation where did it
go if we look at the paper
go if we look at the paper
here there's kind of some key
here there's kind of some key
Insight so generalized Advantage
Insight so generalized Advantage
estimation is a sort of an interpolation
estimation is a sort of an interpolation
between between these two formulas right
between between these two formulas right
using Lambda to interpolate it's not
using Lambda to interpolate it's not
just alert but it is an interpolation
just alert but it is an interpolation
is um this is the more important of the
is um this is the more important of the
two formulas because Lambda is almost
two formulas because Lambda is almost
always close to one right usually it's
always close to one right usually it's
going to be like 0. n at the
going to be like 0. n at the
lowest so what this is is this is just
lowest so what this is is this is just
training the value function to predict
training the value function to predict
discounted
discounted
returns um but this is because you only
returns um but this is because you only
have a value function for one step right
have a value function for one step right
this is assumed to be a single variable
this is assumed to be a single variable
so if instead if I just predict the
so if instead if I just predict the
value function outed each step into the
value function outed each step into the
future doesn't that just give me more
future doesn't that just give me more
information and I don't need this gamma
information and I don't need this gamma
parameter
anymore I think it should
I mean we can start implementing it a
I mean we can start implementing it a
little bit
right the problem with the TD methods
right the problem with the TD methods
right is having to have the uh the
right is having to have the uh the
tunable parameter same as J same as
tunable parameter same as J same as
everything
else so that means that you can't have
else so that means that you can't have
the same algorithm work on a ton of
the same algorithm work on a ton of
different environments like the same
different environments like the same
hyper parameters
TD error Target it is its own
TD error Target it is its own
bootstrap so it
bootstrap so it
validates how you validating your value
validates how you validating your value
head without supervision you do have
head without supervision you do have
supervision though
supervision though
right so I guess technically they're
right so I guess technically they're
going to be some samples in your batch
going to be some samples in your batch
where you don't have predictions out
where you don't have predictions out
that far
but
but
um you predict 30 step 32 time steps out
um you predict 30 step 32 time steps out
into the future you're going to collect
into the future you're going to collect
a bunch of data right so you're going to
a bunch of data right so you're going to
collect a bunch of rollouts and then
collect a bunch of rollouts and then
you're going to be able to use segments
you're going to be able to use segments
uh and you're going to be able to use
uh and you're going to be able to use
the rewards that you actually got to
the rewards that you actually got to
supervise that value loss which is the
supervise that value loss which is the
exact same thing that you do with
exact same thing that you do with
generalized Advantage estimation
anyways e
okay so you don't have advantages here
right so the first thing
value has to predict
reward uh let me make sure I get the
reward uh let me make sure I get the
indexing correct on this real quick
I think you have to shift it by one
I think you have to shift it by one
don't you
think you have to shift it by
one yeah t next
okay
e e
values T next C rewards minus C
values T next C rewards minus C
values okay so this
values okay so this
is this is just directly the next reward
right so I can leave this in
place advantages plus values
bandages plus
bandages plus
values yeah that ends up being the same
this is just one step prediction so this
this is just one step prediction so this
is going to stop
oh if you're going to collect samples to
oh if you're going to collect samples to
update that's okay you can do regression
update that's okay you can do regression
exactly each environment will have its
exactly each environment will have its
own distribution to
own distribution to
learn what do you mean each environment
learn what do you mean each environment
um well yes you have to yeah you have to
um well yes you have to yeah you have to
run the learning algorithm on each
run the learning algorithm on each
environment that's fine the thing is the
environment that's fine the thing is the
hyperparameter tuning right like right
hyperparameter tuning right like right
now you have to run a 100 experiments
now you have to run a 100 experiments
and the number of experiments you have
and the number of experiments you have
to run depends strongly on the number of
to run depends strongly on the number of
hyper parameters that you have to tune
hyper parameters that you have to tune
and their
and their
sensitivity so if we can remove Lambda
sensitivity so if we can remove Lambda
and Gamma which are two of the most
and Gamma which are two of the most
annoying ones then that's going to put
annoying ones then that's going to put
us in a pretty good spot
us in a pretty good spot
and then we'll basically just have to
and then we'll basically just have to
tune learning rate and uh most of the
tune learning rate and uh most of the
other ones are just scaling forams I
other ones are just scaling forams I
guess there's entropy as well but we'll
guess there's entropy as well but we'll
deal with that one
separately so obviously this isn't going
separately so obviously this isn't going
to work this is just a uh a really bad
to work this is just a uh a really bad
Baseline
Baseline
um with the one separate works
now let's do 32
values
size
32 do that
sensitivity comes from each end having a
sensitivity comes from each end having a
separate distribution
separate distribution
how are you going to adjust your reward
how are you going to adjust your reward
predictor without hyper parameters well
predictor without hyper parameters well
because it's predicting the reward at
because it's predicting the reward at
each step
each step
right it's not predicting returns so at
right it's not predicting returns so at
each step your model is going to predict
each step your model is going to predict
the reward at the next step the reward
the reward at the next step the reward
at the Second Step the third step out to
32 CU right now what you do is you
32 CU right now what you do is you
essentially predict like a
essentially predict like a
soft uh you predict like a soft
soft uh you predict like a soft
exponential decayed sum over those so
exponential decayed sum over those so
instead of predicting an exponentially
instead of predicting an exponentially
decayed sum you're just going to
decayed sum you're just going to
predict each one of them
individually
e e
I think we need to
do e
we don't need teen next at all do we
reward can be very
reward can be very
noisy well it's already hard to predict
noisy well it's already hard to predict
in G
in G
right like how do like these are
right like how do like these are
problems with J as
problems with J as
well be
skewed how it's going to work agent end
skewed how it's going to work agent end
Loop so you I mean this is a learning
Loop so you I mean this is a learning
time thing right so you collect your
time thing right so you collect your
rollouts it's it's on policy okay so you
rollouts it's it's on policy okay so you
collect your rollouts you're going to do
collect your rollouts you're going to do
po as normal but instead of using
po as normal but instead of using
J which is going to compute an
J which is going to compute an
exponentially uh decayed sum and then
exponentially uh decayed sum and then
it's going to compute a loss with that
it's going to compute a loss with that
and the single value prediction right
and the single value prediction right
the value being trained to predict the
the value being trained to predict the
exponentially decaying sum now instead
exponentially decaying sum now instead
you're training a value head that has 32
you're training a value head that has 32
outputs to predict the reward at each of
outputs to predict the reward at each of
the next 32 steps so the algorithm is
the next 32 steps so the algorithm is
almost the same it's just that you're
almost the same it's just that you're
predicting 32 separate rewards instead
predicting 32 separate rewards instead
of predicting an exponentially decayed
of predicting an exponentially decayed
uh exponentially decayed
Su and the cool thing about this as well
Su and the cool thing about this as well
is yes the value the individual values
is yes the value the individual values
are hard to predict but you only have to
are hard to predict but you only have to
get the SU right right you only have to
get the SU right right you only have to
get the sum over the next 32 steps
get the sum over the next 32 steps
approximately right and that'll be
approximately right and that'll be
what's used as the advantage now you
what's used as the advantage now you
know maybe that thing needs to be
know maybe that thing needs to be
decayed as well because your predictions
decayed as well because your predictions
into the future are less accurate or
into the future are less accurate or
something and then we'll have to think
something and then we'll have to think
of something for that but you know
of something for that but you know
initially it seems
reasonable right but all that it does is
reasonable right but all that it does is
gives you like what are those parameters
gives you like what are those parameters
doing right they're giving you an
doing right they're giving you an
effective Horizon to look
at all right like if you have a 0.9
at all right like if you have a 0.9
gamma then your effective Horizon is
gamma then your effective Horizon is
about
about
10 if it's 099 then it's about 100
here let me give you a very very strong
here let me give you a very very strong
motivation for trying to find an
motivation for trying to find an
alternative to uh to
J the proc gen Benchmark was introduced
J the proc gen Benchmark was introduced
uh with the assumption that you're not
uh with the assumption that you're not
supposed to tune the hyper parameters
supposed to tune the hyper parameters
per environment right you just have one
per environment right you just have one
set of hyper parameters and you see what
set of hyper parameters and you see what
generalizes and people came up with all
generalizes and people came up with all
sorts of fancy algorithms and none of
sorts of fancy algorithms and none of
them worked well these guys solved all
them worked well these guys solved all
of proin just by hyperparameter tuning
of proin just by hyperparameter tuning
per environment and it turns out that
per environment and it turns out that
the optimal gamma and Lambda vary like
the optimal gamma and Lambda vary like
dramatically across the different
dramatically across the different
environments so it's probably impossible
environments so it's probably impossible
for any algorithm that relies heavily on
for any algorithm that relies heavily on
that lamb Lambda and Gamma to generalize
that lamb Lambda and Gamma to generalize
across all these ends so as long as you
across all these ends so as long as you
have any method that uses these types of
have any method that uses these types of
parameters you're kind of just
parameters you're kind of just
screwed that's the
issue
e
e e
average reward
papers I mean are there any that are
decent like like reward averaging and
decent like like reward averaging and
any sort of like toast processing on
any sort of like toast processing on
that that doesn't do the same thing
I mean if you have references by all
I mean if you have references by all
means let me
know
e
e e
okay I think this one is good and then
okay I think this one is good and then
you do
you do
this and then you also have to check to
this and then you also have to check to
make
sure for
I think this might do
I think this might do
it so this gives
it so this gives
us continuous setting it's the same
us continuous setting it's the same
thing there's no reason they should be
thing there's no reason they should be
different for discreet or
continuous and the value head is
continuous and the value head is
completely independent of the action
completely independent of the action
head right or can
head right or can
be uh YouTube's not going to let you
be uh YouTube's not going to let you
post a link if you put the uh the paper
post a link if you put the uh the paper
name they'll all grab it
we can just do
we can just do
this perfect
right yeah post post the paper name
discounted is not an optimization
discounted is not an optimization
problem that's
problem that's
funny all
right Sutton
paper what
maximizing average
reward e
all
thank you yeah I'm a little suspicious
thank you yeah I'm a little suspicious
of the framing of this they're
of the framing of this they're
essentially saying that it
essentially saying that it
biases like having this discounted some
biases like having this discounted some
biases your optimization so there's no
biases your optimization so there's no
longer a solution or something but
longer a solution or something but
like you can't say it's
like you can't say it's
incompatible with high-dimensional
incompatible with high-dimensional
continuous control when this is
continuous control when this is
successfully used on high dimensional
successfully used on high dimensional
continuous control all over the
continuous control all over the
place um
so like dis prooof by counter example of
so like dis prooof by counter example of
the thing actually
working but the thing is like do they
working but the thing is like do they
actually have any alternative to this
I mean this is their alternative they
I mean this is their alternative they
just link a bunch of papers from the 9s
just link a bunch of papers from the 9s
with dynamic programming that doesn't do
with dynamic programming that doesn't do
jack for uh for
jack for uh for
this no you're good it's just like this
this no you're good it's just like this
is like you know this is your Sutton
is like you know this is your Sutton
paper where it's like ah yes
paper where it's like ah yes
theoretically the thing that everyone is
theoretically the thing that everyone is
doing is not well grounded okay what's
doing is not well grounded okay what's
your alternative links a bunch of papers
your alternative links a bunch of papers
from the '90s that are clearly not
from the '90s that are clearly not
applicable here
no experiment
paper I mean that's a Schmid hoing right
paper I mean that's a Schmid hoing right
there and it's not even Schmid
there and it's not even Schmid
hoer well yeah Su of course he's got
hoer well yeah Su of course he's got
lots of papers right
suton is like Godfather of RL but um the
suton is like Godfather of RL but um the
problem is this type of a this type of a
problem is this type of a this type of a
thing this is not particularly helpful
thing this is not particularly helpful
right like look literally like look at
right like look literally like look at
what this is right there are several
what this is right there are several
dynamic programming algorithms for
dynamic programming algorithms for
finding op optimal average reward
finding op optimal average reward
policies okay nobody's doing tabular RL
policies okay nobody's doing tabular RL
so that's completely uh that's
so that's completely uh that's
completely
redundant first RL algorithm to maximize
redundant first RL algorithm to maximize
undiscounted average uh reward I don't
undiscounted average uh reward I don't
think any of these
think any of these
Works Q learning this is before deep Q
Works Q learning this is before deep Q
learning this is
tabular as you are not able to find it's
tabular as you are not able to find it's
an active well that means it's an
an active well that means it's an
inactive research area right
inactive research area right
quite the
contrary I think we're just going to do
contrary I think we're just going to do
this this gives us
rewards this gave us
rewards this gave us
two for
I don't know so many papers about RL
I don't know so many papers about RL
without
without
gamas yeah I mean it's a pervasive issue
I think I need to compute a mask here as
I think I need to compute a mask here as
well don't
I
e e
L
Rewards e
okay so there's a little scon function
FS with this a little bit
go ahead and uh drop the paper
name
for
e e
the target for the bias and Advantage
the target for the bias and Advantage
are calculated without discount factors
the average return is subtracted from
the average return is subtracted from
the
reward does that actually work
h
okay this looks like
okay this looks like
something I'm going stick this in the
something I'm going stick this in the
rock
time to gr was dealing with
undergrad how does it get this wrong
undergrad how does it get this wrong
from the link that's incredibly stupid
apparently you can't paste
links I'll drop the PDF the PDF in next
if found it you just have to give it the
if found it you just have to give it the
name of the
paper it has browsing but it doesn't
paper it has browsing but it doesn't
have um link
have um link
following policies are always denoted pi
following policies are always denoted pi
and RL right yes except when they're
not
e
e e
how the heck there's no way that works
right I'd be incredibly surprised if
right I'd be incredibly surprised if
that
works I mean I will play with this I
works I mean I will play with this I
want to finish a little bit on this
want to finish a little bit on this
implementation because I got to go soon
implementation because I got to go soon
but
but
uh I will be very surprised if that one
uh I will be very surprised if that one
works works
well the thing is that like I assume
well the thing is that like I assume
that by default in RL here's the tough
that by default in RL here's the tough
thing I assume that the experiments of
thing I assume that the experiments of
Any Given paper even if it's like a nurs
Any Given paper even if it's like a nurs
oral I assume the experiments are wrong
oral I assume the experiments are wrong
because that's usually the experiments
because that's usually the experiments
are wrong like more often than not I
are wrong like more often than not I
think that more uh the majority of
think that more uh the majority of
papers are just
wrong that's the annoying
thing for
yep well this is the whole idea with
yep well this is the whole idea with
puffer I don't know how much you've seen
puffer I don't know how much you've seen
my other work but like why are why is
my other work but like why are why is
the majority of RL research wrong right
the majority of RL research wrong right
they don't run enough experiments and
they don't run enough experiments and
they don't run full hyper parameter
they don't run full hyper parameter
sweeps on their method and they write
sweeps on their method and they write
terrible code that's like irreproducible
terrible code that's like irreproducible
right so we have very simple and clean
right so we have very simple and clean
code and we have all these environments
code and we have all these environments
that run at a million steps per second
that run at a million steps per second
so now you can run thousands of
so now you can run thousands of
experiments on your one GPU that you can
experiments on your one GPU that you can
afford on your academic
afford on your academic
budget and you can do everything from
budget and you can do everything from
breakout at a million steps per second
breakout at a million steps per second
which is not that hard to make all the
which is not that hard to make all the
way up to neural MMO at a million steps
way up to neural MMO at a million steps
per second which is like one of the most
per second which is like one of the most
complex RL environments out there with
complex RL environments out there with
hundreds of agents and a dynamic economy
hundreds of agents and a dynamic economy
and all sorts of
stuff and this library is growing
stuff and this library is growing
constantly
you Advantage as well from this right
do you need this hang
on you don't need this do
on you don't need this do
you oh you need it for okay so we
you oh you need it for okay so we
do you do need this hang on
started a long time ago thank you very
started a long time ago thank you very
much I mean I just stream all my work so
much I mean I just stream all my work so
I'm kind of always doing this
stuff the thing I'm really excited about
stuff the thing I'm really excited about
I don't know if you've seen the hyper
I don't know if you've seen the hyper
parameter sweep stuff but um yeah I
parameter sweep stuff but um yeah I
think the hyper parameter sweep stuff is
think the hyper parameter sweep stuff is
really
promising we've got like yeah we're like
promising we've got like yeah we're like
automating we're really really
automating we're really really
automating the process
automating the process
of uh of hyperparameter search for
building stuff properly
yeah well it drives me nuts because
yeah well it drives me nuts because
nobody's done it like you look at all
nobody's done it like you look at all
these RL repositories and it's like the
these RL repositories and it's like the
most insane code you've ever seen
I say as I write this God awful
transpose
e e
so
Advantage should still be good this is
Advantage should still be good this is
the
same
e
e
e e
now you can train pong with that just
now you can train pong with that just
fine
let see that's obnoxious
keep getting
keep getting
baited thought I would have to get 20K
baited thought I would have to get 20K
of
of
compute o
well to be fair without puffer lib you
well to be fair without puffer lib you
would be kind of screwed everything else
would be kind of screwed everything else
is about 100 times slower
so we like
so we like
puffer puffer is very
fast
for
e
e
e
e e
oops it's not done it's rewards
empty for
B reward Block B math block
B reward Block B math block
right how'd that happen
then
for e
it's fine actually hold
on so miss says that uh my sithon is
on so miss says that uh my sithon is
slow no big deal I can fix
that e
the
heck well I mean that is uh
heck well I mean that is uh
doing
something
e e
is there uh there should be a group here
is there uh there should be a group here
right
yeah e
it's only slow because of my
it's only slow because of my
um my scon code being unoptimized we can
um my scon code being unoptimized we can
easily optimize
that I mean that's kind of crazy though
that I mean that's kind of crazy though
to just like this is literally the first
to just like this is literally the first
time I'm running it and it does
time I'm running it and it does
something
that's pretty cool well I'm going to let
that's pretty cool well I'm going to let
this run I'm going to go get dinner and
this run I'm going to go get dinner and
then uh I will probably no this is
then uh I will probably no this is
breakout um I will probably be back
breakout um I will probably be back
after dinner cuz this is kind of
after dinner cuz this is kind of
exciting I might just want to keep
exciting I might just want to keep
working on this tonight we'll see we'll
working on this tonight we'll see we'll
see how I feel after dinner but uh yeah
see how I feel after dinner but uh yeah
thanks folks for tuning in for now I'll
thanks folks for tuning in for now I'll
be back with more RL Dev soon and if
be back with more RL Dev soon and if
you're interested in following any of
you're interested in following any of
this work it's all open source just go
this work it's all open source just go
to puffer doai you can check out all the
to puffer doai you can check out all the
code please star it it really helps when
code please star it it really helps when
people star the
people star the
repo uh you can also join the Discord to
repo uh you can also join the Discord to
get involved or follow me on X for more
get involved or follow me on X for more
reinforcement learning content as well
reinforcement learning content as well
as notifications of when I'm live so uh
as notifications of when I'm live so uh
yeah thank you and hopefully we have
yeah thank you and hopefully we have
something here this was kind of just a
something here this was kind of just a
one-off we'll see uh I might keep
one-off we'll see uh I might keep
working on this

Kind: captions
Language: en
God love Linux drivers
cool and also let's go check up on our
cool and also let's go check up on our
runs I think we should have only killed
runs I think we should have only killed
uh the ones that were running on this
uh the ones that were running on this
box by doing that
okay so here's Endo Ando is going
okay so here's Endo Ando is going
along we
along we
have breakout which
have breakout which
is pretty well set it looks
like what's wrong score
there we
are
lovely interested to know why
lovely interested to know why
uh this is kind of cool if this
is I red this from the right Branch
is I red this from the right Branch
didn't
I I will double check on the code here
I I will double check on the code here
but if this is how if this is actually
but if this is how if this is actually
the we're getting out of this this is
the we're getting out of this this is
amazing I think it probably
is me
is me
see oh look at that
this isn't going to show me
right but we will double check on that
right but we will double check on that
I'm pretty sure I know why that works as
I'm pretty sure I know why that works as
well I think we kind of just yeah I
well I think we kind of just yeah I
think we hit the jackpot with
this I think we have a very nice weep
this I think we have a very nice weep
algorithm and let me see score
algorithm and let me see score
oh yeah look at that
oh yeah look at that
48 this thing was doing pretty
48 this thing was doing pretty
well we even have a nice burito front on
well we even have a nice burito front on
it
perfect
howdy so sweeps are going well I wanted
howdy so sweeps are going well I wanted
I don't want to get distracted though
I don't want to get distracted though
for a bit I want to make sure I finish
for a bit I want to make sure I finish
this stuff
well this isn't going to
work e
I don't think there's any I don't think
I don't think there's any I don't think
this works I'll double check this
yeah you can't normalize across the
yeah you can't normalize across the
batch like this because Advantage is
batch like this because Advantage is
supposed to be over time so this isn't
supposed to be over time so this isn't
going to
work this is definitely not working here
so one step turn doesn't
help e
how's grock
how's grock
3 I haven't tried it for coding
3 I haven't tried it for coding
yet I wouldn't be surprised I don't
yet I wouldn't be surprised I don't
really use these models much for like
really use these models much for like
writing code I use them for docs and for
writing code I use them for docs and for
like research and Math More
hang on what's this adaptive Horizon
hang on what's this adaptive Horizon
policy
gradient Advantage is going to be the
sum 1/ HT
sum 1/ HT
okay
plus okay so it's an Adaptive Horizon
difference which is determined
difference which is determined
dynamically per
state greater than Epsilon
okay
okay
so that's
so that's
interesting that's very
interesting the problem here is I'm
interesting the problem here is I'm
going to guess these experiments suck
that's usually the
problem
problem
perplexity I don't know
perplexity I don't know
pin pin and de or whatever perplexity
pin pin and de or whatever perplexity
seems cool it's a cool
company the thing is I think that
company the thing is I think that
they're serving lower quality models for
they're serving lower quality models for
the most part they don't have they're
the most part they don't have they're
not serving giant models
not serving giant models
right it's more for just for
search okay so this is proposed
search okay so this is proposed
for a compl completely different reason
three
billion they don't really have good
billion they don't really have good
tasks on this but
tasks on this but
like
like
maybe uncensored
R1 I mean that's
R1 I mean that's
cool e
hm
still this is a parameter but this is
still this is a parameter but this is
like something at least
but then what do you train the value
but then what do you train the value
function to
predict
e e
learned Advantage estimation
yeah that's crazy freaking expensive
States it's not trained to predict
States it's not trained to predict
Advantage though it's training to
Advantage though it's training to
predict Monte Carlo return right oh no
predict Monte Carlo return right oh no
wait it's the difference between
so this is what I was looking at based
so this is what I was looking at based
on reward
variants you should basically be able
variants you should basically be able
the idea here is you basically should
the idea here is you basically should
just be able to fit these parameters to
um to the reward
distribution e
where's this
paper sources
no not this one
I don't know if this paper
exists
for e
there might be a way of doing this
there might be a way of doing this
efficiently
oh it's not
R&D yeah this
one e
not quite what we
wanted e
go back to generalized Advantage
go back to generalized Advantage
estimation
this thing is just so hard to get rid
this thing is just so hard to get rid
of I see we have some folks on YouTube
of I see we have some folks on YouTube
now so um what I'm currently doing is
now so um what I'm currently doing is
I'm seeing if there's any obvious way of
I'm seeing if there's any obvious way of
replacing generalized Advantage
replacing generalized Advantage
estimation in reinforcement learning uh
estimation in reinforcement learning uh
it's a really obnoxious requirement at
it's a really obnoxious requirement at
the moment because essentially this
the moment because essentially this
thing makes it so that uh po will not
thing makes it so that uh po will not
work on multiple environment without
work on multiple environment without
retuning at least a couple of the hyper
retuning at least a couple of the hyper
parameters so if we could get rid of
parameters so if we could get rid of
this thing with something that like
this thing with something that like
either fits its own parameters or
either fits its own parameters or
bypasses that requirement somehow it
bypasses that requirement somehow it
would make Po a lot more robust and
would make Po a lot more robust and
require less
tuning e
so Lambda equals 0 there a couple things
so Lambda equals 0 there a couple things
we need to do
here the thing is we know that this is
here the thing is we know that this is
with Lambda equals z this thing doesn't
with Lambda equals z this thing doesn't
work particularly well now does
it yeah this is Lambda equal Z
yeah based on the values of Lambda that
yeah based on the values of Lambda that
are common it seems
are common it seems
like you mostly
like you mostly
want the Lambda equals 1 Formula this
want the Lambda equals 1 Formula this
just interpolates between the two so
just interpolates between the two so
you'll mostly want this one but a little
you'll mostly want this one but a little
bit this
bit this
one um the problem is this one has the
one um the problem is this one has the
discount Factor on
it e
pretty much any heuristic I can come up
pretty much any heuristic I can come up
with
with
right uh there's going to be a way to
right uh there's going to be a way to
screw
with like any heuristic I come up with
with like any heuristic I come up with
based
based
on the distribution of rewards are
on the distribution of rewards are
similar I'm sure I could come up with
similar I'm sure I could come up with
something that work for like all the
something that work for like all the
arcade games where there's kind of just
arcade games where there's kind of just
one source of
reward but I don't think it's going to
reward but I don't think it's going to
be particularly
robust hang on
so this only
so this only
uses you're only using the next two
uses you're only using the next two
value functions here
how screwy would it be if you just lured
how screwy would it be if you just lured
them
instead hang on does that even do
instead hang on does that even do
anything for
me for
well you're training the value function
well you're training the value function
to
to
predict hang
predict hang
on let me look at something
yeah so you're training to predict the
yeah so you're training to predict the
discounted returns
right e
what I'm currently thinking here
so you can't try to have this thing just
so you can't try to have this thing just
predict like the sum of rewards from now
predict like the sum of rewards from now
into the future forever
into the future forever
right uh you have to have it be
right uh you have to have it be
discounted because otherwise that thing
discounted because otherwise that thing
explodes so you want it to be discounted
reasonably but that's only because
reasonably but that's only because
you're making a single prediction
you're making a single prediction
right what if instead of predicting
right what if instead of predicting
discounted return you were just to
discounted return you were just to
predict the reward several steps into
predict the reward several steps into
the future like at each step let's say
the future like at each step let's say
that you predict the reward 32 steps
that you predict the reward 32 steps
into the future
right e
and let me go through this with Gro
did they do
[Music]
[Music]
this just
planning
e
e
e e
I mean this is kind of a crazy
I mean this is kind of a crazy
method that I just cooked
method that I just cooked
up it could do something
though it's just reward minus value
right e
I wonder if anybody's tried this
ever e
um I don't think so
hold
on this
on this
maybe e
let me see if I understand
this so it took my method
well hang on this is
not I don't think you can train it to
predict
e e
uh no
paper this is me mucking around with
paper this is me mucking around with
generalized Advantage estimation and
generalized Advantage estimation and
bouncing stuff off an
llm I'm trying to see if we can do
llm I'm trying to see if we can do
something to replace the LMA and gam uh
something to replace the LMA and gam uh
Lambda and Gamma
parameters rewarded each future step
we could St this
okay hang on I think I I got
it likes to write so damn
much
e
e e
I think this does
it so you get your loss function like
it so you get your loss function like
this right
this right
and then your
and then your
advantage you
advantage you
sum reward minus
value which is equal to sum reward minus
value which is equal to sum reward minus
some value
right that's equal to some reward minus
right that's equal to some reward minus
some value so it actually it only
some value so it actually it only
matters that you get the magnitude of
matters that you get the magnitude of
the reward right over the course of your
the reward right over the course of your
prediction
that gives you a nice loss function
the value function is
inaccurate yeah it should be
averaged that's fine
value functions early in is noisy early
value functions early in is noisy early
in training anyways
okay well we have the
okay well we have the
idea we have the
idea I have no idea whether this works
idea I have no idea whether this works
or
or
not see it's five I could try to
not see it's five I could try to
implement something on this at least
I could try to implement
something how you going to predict words
something how you going to predict words
into the
into the
future
TD into the I'm just going to change the
TD into the I'm just going to change the
out the size of the value head like the
out the size of the value head like the
value head outputs one variable right
value head outputs one variable right
what if I just make it output 32
what if I just make it output 32
variables I mean if you think about it
variables I mean if you think about it
right a lot of the point of the point of
right a lot of the point of the point of
doing uh generalized Advantage
doing uh generalized Advantage
estimation where did it
go if we look at the paper
go if we look at the paper
here there's kind of some key
here there's kind of some key
Insight so generalized Advantage
Insight so generalized Advantage
estimation is a sort of an interpolation
estimation is a sort of an interpolation
between between these two formulas right
between between these two formulas right
using Lambda to interpolate it's not
using Lambda to interpolate it's not
just alert but it is an interpolation
just alert but it is an interpolation
is um this is the more important of the
is um this is the more important of the
two formulas because Lambda is almost
two formulas because Lambda is almost
always close to one right usually it's
always close to one right usually it's
going to be like 0. n at the
going to be like 0. n at the
lowest so what this is is this is just
lowest so what this is is this is just
training the value function to predict
training the value function to predict
discounted
discounted
returns um but this is because you only
returns um but this is because you only
have a value function for one step right
have a value function for one step right
this is assumed to be a single variable
this is assumed to be a single variable
so if instead if I just predict the
so if instead if I just predict the
value function outed each step into the
value function outed each step into the
future doesn't that just give me more
future doesn't that just give me more
information and I don't need this gamma
information and I don't need this gamma
parameter
anymore I think it should
I mean we can start implementing it a
I mean we can start implementing it a
little bit
right the problem with the TD methods
right the problem with the TD methods
right is having to have the uh the
right is having to have the uh the
tunable parameter same as J same as
tunable parameter same as J same as
everything
else so that means that you can't have
else so that means that you can't have
the same algorithm work on a ton of
the same algorithm work on a ton of
different environments like the same
different environments like the same
hyper parameters
TD error Target it is its own
TD error Target it is its own
bootstrap so it
bootstrap so it
validates how you validating your value
validates how you validating your value
head without supervision you do have
head without supervision you do have
supervision though
supervision though
right so I guess technically they're
right so I guess technically they're
going to be some samples in your batch
going to be some samples in your batch
where you don't have predictions out
where you don't have predictions out
that far
but
but
um you predict 30 step 32 time steps out
um you predict 30 step 32 time steps out
into the future you're going to collect
into the future you're going to collect
a bunch of data right so you're going to
a bunch of data right so you're going to
collect a bunch of rollouts and then
collect a bunch of rollouts and then
you're going to be able to use segments
you're going to be able to use segments
uh and you're going to be able to use
uh and you're going to be able to use
the rewards that you actually got to
the rewards that you actually got to
supervise that value loss which is the
supervise that value loss which is the
exact same thing that you do with
exact same thing that you do with
generalized Advantage estimation
anyways e
okay so you don't have advantages here
right so the first thing
value has to predict
reward uh let me make sure I get the
reward uh let me make sure I get the
indexing correct on this real quick
I think you have to shift it by one
I think you have to shift it by one
don't you
think you have to shift it by
one yeah t next
okay
e e
values T next C rewards minus C
values T next C rewards minus C
values okay so this
values okay so this
is this is just directly the next reward
right so I can leave this in
place advantages plus values
bandages plus
bandages plus
values yeah that ends up being the same
this is just one step prediction so this
this is just one step prediction so this
is going to stop
oh if you're going to collect samples to
oh if you're going to collect samples to
update that's okay you can do regression
update that's okay you can do regression
exactly each environment will have its
exactly each environment will have its
own distribution to
own distribution to
learn what do you mean each environment
learn what do you mean each environment
um well yes you have to yeah you have to
um well yes you have to yeah you have to
run the learning algorithm on each
run the learning algorithm on each
environment that's fine the thing is the
environment that's fine the thing is the
hyperparameter tuning right like right
hyperparameter tuning right like right
now you have to run a 100 experiments
now you have to run a 100 experiments
and the number of experiments you have
and the number of experiments you have
to run depends strongly on the number of
to run depends strongly on the number of
hyper parameters that you have to tune
hyper parameters that you have to tune
and their
and their
sensitivity so if we can remove Lambda
sensitivity so if we can remove Lambda
and Gamma which are two of the most
and Gamma which are two of the most
annoying ones then that's going to put
annoying ones then that's going to put
us in a pretty good spot
us in a pretty good spot
and then we'll basically just have to
and then we'll basically just have to
tune learning rate and uh most of the
tune learning rate and uh most of the
other ones are just scaling forams I
other ones are just scaling forams I
guess there's entropy as well but we'll
guess there's entropy as well but we'll
deal with that one
separately so obviously this isn't going
separately so obviously this isn't going
to work this is just a uh a really bad
to work this is just a uh a really bad
Baseline
Baseline
um with the one separate works
now let's do 32
values
size
32 do that
sensitivity comes from each end having a
sensitivity comes from each end having a
separate distribution
separate distribution
how are you going to adjust your reward
how are you going to adjust your reward
predictor without hyper parameters well
predictor without hyper parameters well
because it's predicting the reward at
because it's predicting the reward at
each step
each step
right it's not predicting returns so at
right it's not predicting returns so at
each step your model is going to predict
each step your model is going to predict
the reward at the next step the reward
the reward at the next step the reward
at the Second Step the third step out to
32 CU right now what you do is you
32 CU right now what you do is you
essentially predict like a
essentially predict like a
soft uh you predict like a soft
soft uh you predict like a soft
exponential decayed sum over those so
exponential decayed sum over those so
instead of predicting an exponentially
instead of predicting an exponentially
decayed sum you're just going to
decayed sum you're just going to
predict each one of them
individually
e e
I think we need to
do e
we don't need teen next at all do we
reward can be very
reward can be very
noisy well it's already hard to predict
noisy well it's already hard to predict
in G
in G
right like how do like these are
right like how do like these are
problems with J as
problems with J as
well be
skewed how it's going to work agent end
skewed how it's going to work agent end
Loop so you I mean this is a learning
Loop so you I mean this is a learning
time thing right so you collect your
time thing right so you collect your
rollouts it's it's on policy okay so you
rollouts it's it's on policy okay so you
collect your rollouts you're going to do
collect your rollouts you're going to do
po as normal but instead of using
po as normal but instead of using
J which is going to compute an
J which is going to compute an
exponentially uh decayed sum and then
exponentially uh decayed sum and then
it's going to compute a loss with that
it's going to compute a loss with that
and the single value prediction right
and the single value prediction right
the value being trained to predict the
the value being trained to predict the
exponentially decaying sum now instead
exponentially decaying sum now instead
you're training a value head that has 32
you're training a value head that has 32
outputs to predict the reward at each of
outputs to predict the reward at each of
the next 32 steps so the algorithm is
the next 32 steps so the algorithm is
almost the same it's just that you're
almost the same it's just that you're
predicting 32 separate rewards instead
predicting 32 separate rewards instead
of predicting an exponentially decayed
of predicting an exponentially decayed
uh exponentially decayed
Su and the cool thing about this as well
Su and the cool thing about this as well
is yes the value the individual values
is yes the value the individual values
are hard to predict but you only have to
are hard to predict but you only have to
get the SU right right you only have to
get the SU right right you only have to
get the sum over the next 32 steps
get the sum over the next 32 steps
approximately right and that'll be
approximately right and that'll be
what's used as the advantage now you
what's used as the advantage now you
know maybe that thing needs to be
know maybe that thing needs to be
decayed as well because your predictions
decayed as well because your predictions
into the future are less accurate or
into the future are less accurate or
something and then we'll have to think
something and then we'll have to think
of something for that but you know
of something for that but you know
initially it seems
reasonable right but all that it does is
reasonable right but all that it does is
gives you like what are those parameters
gives you like what are those parameters
doing right they're giving you an
doing right they're giving you an
effective Horizon to look
at all right like if you have a 0.9
at all right like if you have a 0.9
gamma then your effective Horizon is
gamma then your effective Horizon is
about
about
10 if it's 099 then it's about 100
here let me give you a very very strong
here let me give you a very very strong
motivation for trying to find an
motivation for trying to find an
alternative to uh to
J the proc gen Benchmark was introduced
J the proc gen Benchmark was introduced
uh with the assumption that you're not
uh with the assumption that you're not
supposed to tune the hyper parameters
supposed to tune the hyper parameters
per environment right you just have one
per environment right you just have one
set of hyper parameters and you see what
set of hyper parameters and you see what
generalizes and people came up with all
generalizes and people came up with all
sorts of fancy algorithms and none of
sorts of fancy algorithms and none of
them worked well these guys solved all
them worked well these guys solved all
of proin just by hyperparameter tuning
of proin just by hyperparameter tuning
per environment and it turns out that
per environment and it turns out that
the optimal gamma and Lambda vary like
the optimal gamma and Lambda vary like
dramatically across the different
dramatically across the different
environments so it's probably impossible
environments so it's probably impossible
for any algorithm that relies heavily on
for any algorithm that relies heavily on
that lamb Lambda and Gamma to generalize
that lamb Lambda and Gamma to generalize
across all these ends so as long as you
across all these ends so as long as you
have any method that uses these types of
have any method that uses these types of
parameters you're kind of just
parameters you're kind of just
screwed that's the
issue
e
e e
average reward
papers I mean are there any that are
decent like like reward averaging and
decent like like reward averaging and
any sort of like toast processing on
any sort of like toast processing on
that that doesn't do the same thing
I mean if you have references by all
I mean if you have references by all
means let me
know
e
e e
okay I think this one is good and then
okay I think this one is good and then
you do
you do
this and then you also have to check to
this and then you also have to check to
make
sure for
I think this might do
I think this might do
it so this gives
it so this gives
us continuous setting it's the same
us continuous setting it's the same
thing there's no reason they should be
thing there's no reason they should be
different for discreet or
continuous and the value head is
continuous and the value head is
completely independent of the action
completely independent of the action
head right or can
head right or can
be uh YouTube's not going to let you
be uh YouTube's not going to let you
post a link if you put the uh the paper
post a link if you put the uh the paper
name they'll all grab it
we can just do
we can just do
this perfect
right yeah post post the paper name
discounted is not an optimization
discounted is not an optimization
problem that's
problem that's
funny all
right Sutton
paper what
maximizing average
reward e
all
thank you yeah I'm a little suspicious
thank you yeah I'm a little suspicious
of the framing of this they're
of the framing of this they're
essentially saying that it
essentially saying that it
biases like having this discounted some
biases like having this discounted some
biases your optimization so there's no
biases your optimization so there's no
longer a solution or something but
longer a solution or something but
like you can't say it's
like you can't say it's
incompatible with high-dimensional
incompatible with high-dimensional
continuous control when this is
continuous control when this is
successfully used on high dimensional
successfully used on high dimensional
continuous control all over the
continuous control all over the
place um
so like dis prooof by counter example of
so like dis prooof by counter example of
the thing actually
working but the thing is like do they
working but the thing is like do they
actually have any alternative to this
I mean this is their alternative they
I mean this is their alternative they
just link a bunch of papers from the 9s
just link a bunch of papers from the 9s
with dynamic programming that doesn't do
with dynamic programming that doesn't do
jack for uh for
jack for uh for
this no you're good it's just like this
this no you're good it's just like this
is like you know this is your Sutton
is like you know this is your Sutton
paper where it's like ah yes
paper where it's like ah yes
theoretically the thing that everyone is
theoretically the thing that everyone is
doing is not well grounded okay what's
doing is not well grounded okay what's
your alternative links a bunch of papers
your alternative links a bunch of papers
from the '90s that are clearly not
from the '90s that are clearly not
applicable here
no experiment
paper I mean that's a Schmid hoing right
paper I mean that's a Schmid hoing right
there and it's not even Schmid
there and it's not even Schmid
hoer well yeah Su of course he's got
hoer well yeah Su of course he's got
lots of papers right
suton is like Godfather of RL but um the
suton is like Godfather of RL but um the
problem is this type of a this type of a
problem is this type of a this type of a
thing this is not particularly helpful
thing this is not particularly helpful
right like look literally like look at
right like look literally like look at
what this is right there are several
what this is right there are several
dynamic programming algorithms for
dynamic programming algorithms for
finding op optimal average reward
finding op optimal average reward
policies okay nobody's doing tabular RL
policies okay nobody's doing tabular RL
so that's completely uh that's
so that's completely uh that's
completely
redundant first RL algorithm to maximize
redundant first RL algorithm to maximize
undiscounted average uh reward I don't
undiscounted average uh reward I don't
think any of these
think any of these
Works Q learning this is before deep Q
Works Q learning this is before deep Q
learning this is
tabular as you are not able to find it's
tabular as you are not able to find it's
an active well that means it's an
an active well that means it's an
inactive research area right
inactive research area right
quite the
contrary I think we're just going to do
contrary I think we're just going to do
this this gives us
rewards this gave us
rewards this gave us
two for
I don't know so many papers about RL
I don't know so many papers about RL
without
without
gamas yeah I mean it's a pervasive issue
I think I need to compute a mask here as
I think I need to compute a mask here as
well don't
I
e e
L
Rewards e
okay so there's a little scon function
FS with this a little bit
go ahead and uh drop the paper
name
for
e e
the target for the bias and Advantage
the target for the bias and Advantage
are calculated without discount factors
the average return is subtracted from
the average return is subtracted from
the
reward does that actually work
h
okay this looks like
okay this looks like
something I'm going stick this in the
something I'm going stick this in the
rock
time to gr was dealing with
undergrad how does it get this wrong
undergrad how does it get this wrong
from the link that's incredibly stupid
apparently you can't paste
links I'll drop the PDF the PDF in next
if found it you just have to give it the
if found it you just have to give it the
name of the
paper it has browsing but it doesn't
paper it has browsing but it doesn't
have um link
have um link
following policies are always denoted pi
following policies are always denoted pi
and RL right yes except when they're
not
e
e e
how the heck there's no way that works
right I'd be incredibly surprised if
right I'd be incredibly surprised if
that
works I mean I will play with this I
works I mean I will play with this I
want to finish a little bit on this
want to finish a little bit on this
implementation because I got to go soon
implementation because I got to go soon
but
but
uh I will be very surprised if that one
uh I will be very surprised if that one
works works
well the thing is that like I assume
well the thing is that like I assume
that by default in RL here's the tough
that by default in RL here's the tough
thing I assume that the experiments of
thing I assume that the experiments of
Any Given paper even if it's like a nurs
Any Given paper even if it's like a nurs
oral I assume the experiments are wrong
oral I assume the experiments are wrong
because that's usually the experiments
because that's usually the experiments
are wrong like more often than not I
are wrong like more often than not I
think that more uh the majority of
think that more uh the majority of
papers are just
wrong that's the annoying
thing for
yep well this is the whole idea with
yep well this is the whole idea with
puffer I don't know how much you've seen
puffer I don't know how much you've seen
my other work but like why are why is
my other work but like why are why is
the majority of RL research wrong right
the majority of RL research wrong right
they don't run enough experiments and
they don't run enough experiments and
they don't run full hyper parameter
they don't run full hyper parameter
sweeps on their method and they write
sweeps on their method and they write
terrible code that's like irreproducible
terrible code that's like irreproducible
right so we have very simple and clean
right so we have very simple and clean
code and we have all these environments
code and we have all these environments
that run at a million steps per second
that run at a million steps per second
so now you can run thousands of
so now you can run thousands of
experiments on your one GPU that you can
experiments on your one GPU that you can
afford on your academic
afford on your academic
budget and you can do everything from
budget and you can do everything from
breakout at a million steps per second
breakout at a million steps per second
which is not that hard to make all the
which is not that hard to make all the
way up to neural MMO at a million steps
way up to neural MMO at a million steps
per second which is like one of the most
per second which is like one of the most
complex RL environments out there with
complex RL environments out there with
hundreds of agents and a dynamic economy
hundreds of agents and a dynamic economy
and all sorts of
stuff and this library is growing
stuff and this library is growing
constantly
you Advantage as well from this right
do you need this hang
on you don't need this do
on you don't need this do
you oh you need it for okay so we
you oh you need it for okay so we
do you do need this hang on
started a long time ago thank you very
started a long time ago thank you very
much I mean I just stream all my work so
much I mean I just stream all my work so
I'm kind of always doing this
stuff the thing I'm really excited about
stuff the thing I'm really excited about
I don't know if you've seen the hyper
I don't know if you've seen the hyper
parameter sweep stuff but um yeah I
parameter sweep stuff but um yeah I
think the hyper parameter sweep stuff is
think the hyper parameter sweep stuff is
really
promising we've got like yeah we're like
promising we've got like yeah we're like
automating we're really really
automating we're really really
automating the process
automating the process
of uh of hyperparameter search for
building stuff properly
yeah well it drives me nuts because
yeah well it drives me nuts because
nobody's done it like you look at all
nobody's done it like you look at all
these RL repositories and it's like the
these RL repositories and it's like the
most insane code you've ever seen
I say as I write this God awful
transpose
e e
so
Advantage should still be good this is
Advantage should still be good this is
the
same
e
e
e e
now you can train pong with that just
now you can train pong with that just
fine
let see that's obnoxious
keep getting
keep getting
baited thought I would have to get 20K
baited thought I would have to get 20K
of
of
compute o
well to be fair without puffer lib you
well to be fair without puffer lib you
would be kind of screwed everything else
would be kind of screwed everything else
is about 100 times slower
so we like
so we like
puffer puffer is very
fast
for
e
e
e
e e
oops it's not done it's rewards
empty for
B reward Block B math block
B reward Block B math block
right how'd that happen
then
for e
it's fine actually hold
on so miss says that uh my sithon is
on so miss says that uh my sithon is
slow no big deal I can fix
that e
the
heck well I mean that is uh
heck well I mean that is uh
doing
something
e e
is there uh there should be a group here
is there uh there should be a group here
right
yeah e
it's only slow because of my
it's only slow because of my
um my scon code being unoptimized we can
um my scon code being unoptimized we can
easily optimize
that I mean that's kind of crazy though
that I mean that's kind of crazy though
to just like this is literally the first
to just like this is literally the first
time I'm running it and it does
time I'm running it and it does
something
that's pretty cool well I'm going to let
that's pretty cool well I'm going to let
this run I'm going to go get dinner and
this run I'm going to go get dinner and
then uh I will probably no this is
then uh I will probably no this is
breakout um I will probably be back
breakout um I will probably be back
after dinner cuz this is kind of
after dinner cuz this is kind of
exciting I might just want to keep
exciting I might just want to keep
working on this tonight we'll see we'll
working on this tonight we'll see we'll
see how I feel after dinner but uh yeah
see how I feel after dinner but uh yeah
thanks folks for tuning in for now I'll
thanks folks for tuning in for now I'll
be back with more RL Dev soon and if
be back with more RL Dev soon and if
you're interested in following any of
you're interested in following any of
this work it's all open source just go
this work it's all open source just go
to puffer doai you can check out all the
to puffer doai you can check out all the
code please star it it really helps when
code please star it it really helps when
people star the
people star the
repo uh you can also join the Discord to
repo uh you can also join the Discord to
get involved or follow me on X for more
get involved or follow me on X for more
reinforcement learning content as well
reinforcement learning content as well
as notifications of when I'm live so uh
as notifications of when I'm live so uh
yeah thank you and hopefully we have
yeah thank you and hopefully we have
something here this was kind of just a
something here this was kind of just a
one-off we'll see uh I might keep
one-off we'll see uh I might keep
working on this
