Kind: captions
Language: en
Okay,
Okay,
should be um back live here.
should be um back live here.
Hello.
11:30. A little tidbit late today.
11:30. A little tidbit late today.
A lot of exercise in the morning,
but I've had some good time to think and
but I've had some good time to think and
kept my head very clear. I know exactly
kept my head very clear. I know exactly
what we're going to do today.
what we're going to do today.
Spencer has been working on this HL Gaus
Spencer has been working on this HL Gaus
implementation.
Let's actually see. Whoops. Do it this
Let's actually see. Whoops. Do it this
way
to be sure.
Yeah, this is uh this is better than
Yeah, this is uh this is better than
before.
before.
This is slightly better than before.
This is slightly better than before.
He's got this HL Gaus implementation
He's got this HL Gaus implementation
that's like well, it's hard to really
that's like well, it's hard to really
say with any real significance, but it's
say with any real significance, but it's
at least as good as before. Um,
at least as good as before. Um,
and what we're going to do is because HL
and what we're going to do is because HL
Gaus is also one of the main annoying to
Gaus is also one of the main annoying to
implement components that really matters
implement components that really matters
for off policy,
for off policy,
we're going to see what happens when we
we're going to see what happens when we
uh we use this with off policy method.
Now the thing I don't know about
this implemented.
Okay. So it's the same value function as
Okay. So it's the same value function as
before, right? A singular output.
singular output, right?
How do you handle continuous actions
How do you handle continuous actions
with um
well, do we just do it for con uh for
well, do we just do it for con uh for
discrete first?
discrete first?
I think that's what we do, right?
Honestly, we can kind of just repurpose
Honestly, we can kind of just repurpose
the decoder. No,
and we just train it uh a bit
and we just train it uh a bit
differently. Decoder
differently. Decoder
is going to represent something else.
Okay. So, we have log jets.
Then we do
and we have to sample
and we have to sample
from logit's epsilon gradient now Okay.
Something like this. Right.
This should be your off pulse sample, I
This should be your off pulse sample, I
believe. See if this works.
Enter is not
wants me to do this.
Okay.
Okay.
Well, that seems weird.
Okay, I see some TR in there.
Hello, welcome. Which RL objective are
Hello, welcome. Which RL objective are
you implementing? Uh, I'm trying to make
you implementing? Uh, I'm trying to make
an off policy version of pop for lib.
an off policy version of pop for lib.
So, the basic thing is just getting two
So, the basic thing is just getting two
functions of some type into the trainer.
functions of some type into the trainer.
We have a HL Gaus implementation which
We have a HL Gaus implementation which
from my research it seems like this is
from my research it seems like this is
the most uh crucial trick to have in
the most uh crucial trick to have in
place and we actually already kind of
place and we actually already kind of
have a lot of the other ones that are
have a lot of the other ones that are
required for off policy to work. So
required for off policy to work. So
pretty much the goal is to see if we can
pretty much the goal is to see if we can
get some sort of off policy
get some sort of off policy
objective that matches the performance
objective that matches the performance
of our on policy implementation.
It's probably going to end up looking
It's probably going to end up looking
something like rainbow but with better
something like rainbow but with better
distributional and without a couple of
distributional and without a couple of
the tricks that don't actually seem to
the tricks that don't actually seem to
make a friends.
Oh, so it shouldn't be log prop. It
Oh, so it shouldn't be log prop. It
should be logs.
And I guess it's all the same initial
And I guess it's all the same initial
state. So it's all going to be the same
state. So it's all going to be the same
action. That's fine.
action. That's fine.
Okay. But this gets us our action,
Okay. But this gets us our action,
right?
And that should be all we have to store
And that should be all we have to store
for Yes.
There's HL G in here.
Yeah, we have a jo
value logit. Ah,
value logit. Ah,
do value logits.
do value logits.
Did he actually modify this?
kind of dead
kind of dead
chill.
Okay, so he actually did do this.
Hello, Kyoko.
Hello, Kyoko.
Are you done with imitation learning?
Are you done with imitation learning?
No. Um what this is is I would call this
No. Um what this is is I would call this
somewhat of a continuation. So we're
somewhat of a continuation. So we're
doing off policy work. Now the thing
doing off policy work. Now the thing
that I think is important from the
that I think is important from the
imitation learning right is that there
imitation learning right is that there
is a
is a
uh it is possible to write a algorithm
uh it is possible to write a algorithm
that behaves very qualitatively
that behaves very qualitatively
differently and in places much much
differently and in places much much
better than our current state-of-the-art
better than our current state-of-the-art
on policy implementation.
on policy implementation.
So uh and it does that by reusing the
So uh and it does that by reusing the
highest quality old data. Right?
highest quality old data. Right?
So this is why I'm now playing around
So this is why I'm now playing around
with off policy because given that we
with off policy because given that we
should be able to come up with an off
should be able to come up with an off
policy algorithm that is very close to
policy algorithm that is very close to
our on policy algorithm but lets us
our on policy algorithm but lets us
actually use an experience buffer.
actually use an experience buffer.
That is going to be the objective.
That is going to be the objective.
And the result of this is that the hard
And the result of this is that the hard
exploration tasks like neural MMO 3 um
exploration tasks like neural MMO 3 um
we should see
we should see
we should see progress happen very
we should see progress happen very
quickly there because you when you get a
quickly there because you when you get a
few good samples it will actually keep
few good samples it will actually keep
learning on those.
The cool thing is puffer lilib's kind of
The cool thing is puffer lilib's kind of
already set up such that many we have a
already set up such that many we have a
lot of the uh the tricks that are used
lot of the uh the tricks that are used
in off policy learning already in an on
in off policy learning already in an on
policy setting. So it shouldn't be a
policy setting. So it shouldn't be a
huge modification. Um the main thing
huge modification. Um the main thing
that's confusing is like how we're going
that's confusing is like how we're going
to change up advantage function and such
to change up advantage function and such
with uh discretized
with uh discretized
Q function that has hlg in the mix.
welcome something.
welcome something.
I do think that it should be possible to
I do think that it should be possible to
make something like this work. I don't
make something like this work. I don't
know how close it's going to be to
know how close it's going to be to
existing off policy methods.
existing off policy methods.
Any guideline you're following? Well,
Any guideline you're following? Well,
I've read like I've been reading a ton
I've read like I've been reading a ton
of papers lately and so at this point
of papers lately and so at this point
I'm trusting I'll reference some papers,
I'm trusting I'll reference some papers,
but I'm mostly trusting my own best
but I'm mostly trusting my own best
judgment.
judgment.
Play buffers and puffer. Here's the
Play buffers and puffer. Here's the
thing, Anchel. I implemented them
thing, Anchel. I implemented them
in an early version of 3.0. They didn't
in an early version of 3.0. They didn't
work because of the on polic because
work because of the on polic because
puffer's on policy.
I tried to do exactly this and it didn't
I tried to do exactly this and it didn't
work at all. So now we're going to try
work at all. So now we're going to try
to actually go to off policy and see if
to actually go to off policy and see if
we can get it to work with this
value log value.
Ah okay. So there actually is.
Yeah. Yeah. The value function actually
Yeah. Yeah. The value function actually
works here, doesn't it?
works here, doesn't it?
Logit's value value.
Well, don't go getting excited to try a
Well, don't go getting excited to try a
thing yet when uh it's not I mean,
thing yet when uh it's not I mean,
research is like the most fiddly bits
research is like the most fiddly bits
you can possibly have to deal with,
you can possibly have to deal with,
right?
right?
Actually,
let's let's just make this new
new value log props.
It's an observation help. Yeah, it
It's an observation help. Yeah, it
stores past data. So the idea is you
stores past data. So the idea is you
should be able to reuse past data
should be able to reuse past data
in learning
way to do this.
Guess we just replace the um
I guess right here, right? We have
damn uh function.
squared error, right? On what did they
squared error, right? On what did they
define y i is
Yeah. So it's R plus
Yeah. So it's R plus
Q of next state.
So I should actually be able to just do
So I should actually be able to just do
since I have this over segments, right?
since I have this over segments, right?
And this is done.
The tricky bit and how to reformulate
The tricky bit and how to reformulate
Thanks.
What is the Yi? A y i is
new to RL and tried Tetris with
new to RL and tried Tetris with
gymnasium plus poip for po did you try
gymnasium plus poip for po did you try
our Tetris? We actually have a version
our Tetris? We actually have a version
of Tetris that's very fast.
of Tetris that's very fast.
This is the policy that it just trains
This is the policy that it just trains
out of the box on it.
out of the box on it.
Buffer li for games only. Nope, not at
Buffer li for games only. Nope, not at
all. Here's a driving end.
all. Here's a driving end.
We have a driving in with real world map
We have a driving in with real world map
data. Here we have a drone flying
data. Here we have a drone flying
environment. They're all sorts of
environment. They're all sorts of
synthetic tasks as well, not for games
synthetic tasks as well, not for games
only.
only.
Games are just really, really good for
Games are just really, really good for
research because they're very
research because they're very
interpretable.
So naturally, a lot of research
So naturally, a lot of research
environments that we use for improving
environments that we use for improving
our core algorithms and evaluating
our core algorithms and evaluating
methods are going to be games.
methods are going to be games.
They're frankly much better than control
They're frankly much better than control
tasks or core research.
Gamma
Where can I find code example for Tetris
Where can I find code example for Tetris
M in Ocean? It's in Ocean. Like if you
M in Ocean? It's in Ocean. Like if you
check the docs, right, we have
check the docs, right, we have
walkthroughs on how to build
walkthroughs on how to build
environments and like all of our custom
environments and like all of our custom
M's are just in like puffer lilib ocean
M's are just in like puffer lilib ocean
and there's a folder for each one. The
and there's a folder for each one. The
code is the actual core source is one
code is the actual core source is one
file for each environment. There like
file for each environment. There like
some bindings to Python and stuff, but
some bindings to Python and stuff, but
all the logic is just in theh.
I can promise you that our version of
I can promise you that our version of
the Tetris is going to train at least
the Tetris is going to train at least
100 times faster than whatever you've
100 times faster than whatever you've
been messing around with in gymnasium,
been messing around with in gymnasium,
at least on decent hardware.
Huffer very fast.
Good H.
Okay, that's totally the wrong.
Oh, cuz this ended up having an extra
Oh, cuz this ended up having an extra
dimension somehow.
We just do
Okay. So, this is correct for actions.
Okay. So, this is correct for actions.
[Music]
I don't like that it's all the same.
Um.
Um.
Oh, because the stupid of course.
Oh, because the stupid of course.
Yeah. Hang on.
Now we can't do it twice.
We're going to do this super hacky at
We're going to do this super hacky at
the start, but this is not fast code.
Okay, this is better. 5% random choice.
Okay, this is better. 5% random choice.
That's a long greedy.
Ah.
Wait, hang on. It's Q of actions.
Um,
Um,
we need to view
Is this correct place? What do you mean,
Is this correct place? What do you mean,
Rishi?
If you're trying to post links, YouTube
If you're trying to post links, YouTube
will stop you from doing that.
will stop you from doing that.
It's in pufferlib/ Ocean.
Yeah, that's not right.
No, not at all.
Oh,
the reward.
It's reward plus
It's reward plus
I think that it's the current reward.
I think it's this way.
And this should be QA.
Okay,
Okay,
that is a correct loss. or at least it's
that is a correct loss. or at least it's
a loss that doesn't break.
Interesting.
Well, I mean this is our initial
I don't think it's saving.
I don't think it's saving.
This is our initial implementation.
Oh, Nan's out. I see. Cool.
Oh, Nan's out. I see. Cool.
Well, this is our uh our basic Q
Well, this is our uh our basic Q
function, right?
Do I spend time now
I spend time trying to get this to learn
able to do it on break, right?
able to do it on break, right?
Anything rather.
Anything rather.
I just wanted to get something above the
I just wanted to get something above the
uh the noob reward, which is right
uh the noob reward, which is right
there.
there.
Let me make sure I've done this
Let me make sure I've done this
correctly.
Well, that's probably way too high
Well, that's probably way too high
epsilon graded, isn't it?
H.
Well, the policy full determinizes
Well, the policy full determinizes
there, doesn't it?
Hard to say.
See
Dang. Okay.
Not very familiar with puffer labs and
Not very familiar with puffer labs and
design for typical RL agents have plans
design for typical RL agents have plans
for RL with LLM. No, we do everything
for RL with LLM. No, we do everything
except LLMs.
It's the typical RL agents, but
It's the typical RL agents, but
everything that we have is at least 100
everything that we have is at least 100
times faster,
times faster,
sometimes a thousand times faster. Very
sometimes a thousand times faster. Very
very high performance training of small
very high performance training of small
models.
Where the heck did they put their
Where the heck did they put their
hypers?
I have a table.
Okay. So, it's actually pretty high.
I wouldn't have expected that
using
We're probably still just looking at
We're probably still just looking at
ways. Hey Georgie.
What's the strategy here?
What's the strategy here?
Sace. Now SACE is super freaking slow
Sace. Now SACE is super freaking slow
and I honestly haven't seen good
and I honestly haven't seen good
compare. Like SACE doesn't even compare
compare. Like SACE doesn't even compare
favorably. It's the goal is to get Q
favorably. It's the goal is to get Q
functions of some variety into an
functions of some variety into an
algorithm that is as close as possible
algorithm that is as close as possible
to our current one.
Basically, we want to do our exactly
Basically, we want to do our exactly
what we're doing now, but we want to do
what we're doing now, but we want to do
it off policy so that we can actually
it off policy so that we can actually
have a replay buffer.
have a replay buffer.
You published some paper? Yes. Yes, I
You published some paper? Yes. Yes, I
did. Um,
did. Um,
it will be on archive shortly, but we
it will be on archive shortly, but we
won an award for it.
dang thing.
Why does it not show my freaking posts?
Why does it not show my freaking posts?
What? It literally doesn't show my
What? It literally doesn't show my
posts. You're signed out. Hang on.
posts. You're signed out. Hang on.
Let me go get a separate window on my
Let me go get a separate window on my
personal
Okay, there we go. So,
Okay, there we go. So,
my whole talk is right here. This is the
my whole talk is right here. This is the
talk that I gave on uh it's like a seven
talk that I gave on uh it's like a seven
minute talk at RLC and I put a version
minute talk at RLC and I put a version
of it here on X. It's also on YouTube.
of it here on X. It's also on YouTube.
Oh, is it already live?
Um,
this is RLJ.
this is RLJ.
So, I guess it's on here actually, but I
So, I guess it's on here actually, but I
archived it yesterday
and I don't think it's live on archive
and I don't think it's live on archive
yet.
yet.
So, it's already here, but should be
So, it's already here, but should be
live on archive soon.
AFBRR. We're doing offpaul work or
AFBRR. We're doing offpaul work or
attempting to.
attempting to.
The problem with um offpaul methods is
The problem with um offpaul methods is
like they really don't work until you
like they really don't work until you
add a whole bunch of tricks. So, it's
add a whole bunch of tricks. So, it's
actually hard to know if I've made a
actually hard to know if I've made a
mistake or not until we have a bunch of
mistake or not until we have a bunch of
tricks on it.
Let me take one more look at the uh the
Let me take one more look at the uh the
function, the loss function.
Oh, hang on.
No, because this is done implicitly,
No, because this is done implicitly,
right? The discounting is done
right? The discounting is done
implicitly.
RT. Yeah. Yeah. Yeah. So,
RT. Yeah. Yeah. Yeah. So,
the loss is just they define yi.
the loss is just they define yi.
This is going to be R +
R + gamma max Q.
R + gamma max Q.
Believe I have the correct reward right
Believe I have the correct reward right
because it's observation
because it's observation
and then the reward observation action
and then the reward observation action
then you get the reward. Yeah, I think
then you get the reward. Yeah, I think
that the way I have my buffer set up
that the way I have my buffer set up
this is correct.
drive with a bunch of papers.
Yeah, you can do that.
Yeah, you can do that.
Um,
max.
Yes. You take maximum Q
Yes. You take maximum Q
and you shift over by one
and you shift over by one
and then you take Q of the current
and then you take Q of the current
action.
action.
So unless I've messed up my tensor uh my
So unless I've messed up my tensor uh my
tensor indexing somewhere, this should
tensor indexing somewhere, this should
be correct.
be correct.
Yeah, this should be correct.
Okay. So then why why does this not
Okay. So then why why does this not
work? Um hyperparameters, tensor
work? Um hyperparameters, tensor
indexing problems or base DQN just being
indexing problems or base DQN just being
like horrible horrible bad
like horrible horrible bad
because I think that's all I'm doing,
because I think that's all I'm doing,
right? Is I've just basically hacked DQN
right? Is I've just basically hacked DQN
into this
into this
which obviously is not going to do very
which obviously is not going to do very
much.
Oh, and then wait, do you do
you do take the action that was
you do take the action that was
previously taken? Yes.
previously taken? Yes.
Now, this is fine.
Okay. So, things that I'm going to have
Okay. So, things that I'm going to have
to add to this, right?
to add to this, right?
I'm going to need to have the multi-step
I'm going to need to have the multi-step
bootstrap, obviously.
Then, missing the missing the advantage
Then, missing the missing the advantage
estimation
estimation
is the real big one.
is the real big one.
Obviously the H hl gaus is the other
Obviously the H hl gaus is the other
really big one here.
really big one here.
I should be able to get this to learn
I should be able to get this to learn
something though I think. Let me see if
something though I think. Let me see if
I can play with this a little bit.
I can play with this a little bit.
Um
actually let's go check clean RL's
actually let's go check clean RL's
implementation
and I start with this.
They should have um
They should have um
a super basic
a super basic
Yeah, gotta love Costa.
Yeah, gotta love Costa.
200 line
100 line DQN
exploration fraction ending epsilon.
They actually do kind of eneal it, don't
They actually do kind of eneal it, don't
they?
Yeah, they do kind of eneal it.
these target networks, too.
these target networks, too.
Oh, yeah. What did I miss? I probably
Oh, yeah. What did I miss? I probably
missed something completely there,
missed something completely there,
didn't I?
didn't I?
I haven't done uh
I haven't done uh
I mean, I'll see it in a second. Hang
I mean, I'll see it in a second. Hang
on. Let me at least get epsilon to be
on. Let me at least get epsilon to be
reasonable.
reasonable.
This has to be a yield. So, it would
This has to be a yield. So, it would
probably be something like
two
two
times. No, wait. Not minus 2 times
two.
I think this is right.
Yeah. So after half the steps roughly
loads double Q. You don't need double Q.
loads double Q. You don't need double Q.
It like the initial one doesn't have
It like the initial one doesn't have
double Q and double Q doesn't do
double Q and double Q doesn't do
anything on rainbow and like the more
anything on rainbow and like the more
advanced methods.
advanced methods.
I don't think we need to care too much
I don't think we need to care too much
about that.
Does the original DQN have a separate
Does the original DQN have a separate
target network? I didn't think it did
cuz like the double you don't need
cuz like the double you don't need
double Q. If you look at Rainbow and
double Q. If you look at Rainbow and
like all the different tricks and the
like all the different tricks and the
ablations, like the double Q doesn't
ablations, like the double Q doesn't
actually do anything.
So, that's one that we don't need and
So, that's one that we don't need and
it's expensive as well.
Okay. Now, let's look at clean arrows
Okay. Now, let's look at clean arrows
implementation.
That's any of Yes.
Oh, wait. They do have a separate they
Oh, wait. They do have a separate they
have a separate target already in the
have a separate target already in the
initial uh DQN.
Interesting.
Interesting.
They're fully separate as well.
I forgot I forgot that like the original
I forgot I forgot that like the original
I didn't think there was a separate
I didn't think there was a separate
target network in the original, but I
target network in the original, but I
guess not. I guess there is.
Oh yeah, we're also
then
yeah, I think that we just like go off
yeah, I think that we just like go off
of this and we try to match this as
of this and we try to match this as
close as possible and then we start
close as possible and then we start
adding all of our stuff back in.
kind of modify in place
kind of modify in place
but off policy.
but off policy.
Yeah. Weren't you you were implementing
Yeah. Weren't you you were implementing
some stuff, right?
What did you post?
You you have this nod and puffer lid
You you have this nod and puffer lid
though, right? You have like a
though, right? You have like a
standalone thing,
standalone thing,
don't you?
H guys, help at all. That's what we're
H guys, help at all. That's what we're
going to do right now, Spencer. So, I'm
going to do right now, Spencer. So, I'm
getting like a basic DQN into Puffer
getting like a basic DQN into Puffer
with like reasonable PF at first. And uh
with like reasonable PF at first. And uh
like we're going to need HL Gauss for it
like we're going to need HL Gauss for it
to work at all.
This I open this thing.
Of course, it opens VS Code.
Okay, you did kind of take our buffer
Okay, you did kind of take our buffer
and structure. I see.
Okay, thank you. So, I will put this on
Okay, thank you. So, I will put this on
my other screen then as a additional
my other screen then as a additional
reference for this.
Did you do experiments like to see what
Did you do experiments like to see what
mattered?
is like I was looking through I
is like I was looking through I
basically went and I cross-cheed a bunch
basically went and I cross-cheed a bunch
of off policy papers and it seems like
of off policy papers and it seems like
the most important stuff right is having
the most important stuff right is having
some sort of prioritized replay and then
some sort of prioritized replay and then
having um well that's second actually.
having um well that's second actually.
The most important thing is having some
The most important thing is having some
sort of distributional
sort of distributional
uh some sort of distributional objective
uh some sort of distributional objective
so you're not doing quadratic loss
and then like prioritize replay and a
and then like prioritize replay and a
few other things. Multi-step
few other things. Multi-step
bootstrapping.
I have this now on my other monitor so
I have this now on my other monitor so
we're going to cross reference both. I
we're going to cross reference both. I
got to get the basic thing into a proper
got to get the basic thing into a proper
puffer first.
Oh shoot. Yeah, we can't even do it.
Oh shoot. Yeah, we can't even do it.
Wait, we can't even do it fully
Wait, we can't even do it fully
unshared, right?
Does anybody know if you use a shared
Does anybody know if you use a shared
backbone for Q and target, does it
backbone for Q and target, does it
totally screw everything up or you kind
totally screw everything up or you kind
of Okay.
And I just do self.q
And I just do self.q
target.
Not like well it might be similar.
Not like well it might be similar.
There.
Can I just do this?
Should be a copy.
Yeah, but um
Yeah, but um
A1 um the way that Puffer is set up,
A1 um the way that Puffer is set up,
there's an LSTM in the backbone. So like
you're not going to we'd have to do like
you're not going to we'd have to do like
two separate LSTMs.
I'm going to just share the back. I'm
I'm going to just share the back. I'm
going to share the main net and just
going to share the main net and just
have two separate output heads for
have two separate output heads for
starters. We'll see if that breaks
starters. We'll see if that breaks
Everything.
Get out of here, bot.
Really trying to bot on an RL stream.
All right. So
I think what we do is we just like start
I think what we do is we just like start
cutting crap
you
and let me see how cleaner does
Super. This is like a really good read.
Super. This is like a really good read.
Way easier to follow.
Wait, does this not
Wait, does this not
get the action from
Oh, here you go.
Oh, here you go.
Yeah. So, this is the actions come from
Yeah. So, this is the actions come from
the Q network.
the Q network.
This is just going to be Q.
So, is this not considered the double
So, is this not considered the double
DQN
DQN
or is this
or is this
is this not considered double uh double
is this not considered double uh double
DQN?
DQN?
That's something different. I didn't
That's something different. I didn't
look into the details. It's been like
look into the details. It's been like
several years since I've looked at that.
several years since I've looked at that.
I didn't look into the details because I
I didn't look into the details because I
just saw that um uh in the more recent
just saw that um uh in the more recent
methods having double doesn't actually
methods having double doesn't actually
buy you much.
I kind of just ignored it.
Have Q.
Have Q.
This comes out correctly.
This crap
minimum of values.
So separate from having Q and target,
So separate from having Q and target,
right? This would introduce a third one.
right? This would introduce a third one.
Have you tried Muan for RL training?
Have you tried Muan for RL training?
Yes, we use Muan in all of our
Yes, we use Muan in all of our
baselines. It is much better. This was
baselines. It is much better. This was
one of the main additions to Puffer 3.
Okay, so that's probably the idea is
Okay, so that's probably the idea is
that you take the min in order to
that you take the min in order to
counteract the bias from having a max in
counteract the bias from having a max in
training or something like that. Um,
training or something like that. Um,
yeah, that's a silly hack. It doesn't
yeah, that's a silly hack. It doesn't
actually do much, it looks like. in the
actually do much, it looks like. in the
uh well the better methods the problem
uh well the better methods the problem
with DQN as a starting point is it's
with DQN as a starting point is it's
just such a crappy method that like
just such a crappy method that like
literally anything that you do will
literally anything that you do will
improve over DQN
improve over DQN
that perform better than Adam W in your
that perform better than Adam W in your
test performs better than Adam you don't
test performs better than Adam you don't
use atom W in RL because weight decay is
use atom W in RL because weight decay is
pretty rare you don't really ever use
pretty rare you don't really ever use
weight decay RL
weight decay RL
I mean you're free to try it I have
I mean you're free to try it I have
never gotten anything out of it
Yeah, it's a big difference. I mean, I
Yeah, it's a big difference. I mean, I
would just check out Puffer Lab, right?
would just check out Puffer Lab, right?
Like, we have all of the uh we have all
Like, we have all of the uh we have all
these advancements just there for you by
these advancements just there for you by
default, and there's blog post coverage
default, and there's blog post coverage
of everything that we've done,
of everything that we've done,
and it's going to be much faster than
and it's going to be much faster than
just about every other library.
just about every other library.
Not just about every other library,
Not just about every other library,
faster than every other library really,
better in general.
better in general.
But like from what they should kind of
But like from what they should kind of
be the same thing, right? Like think
be the same thing, right? Like think
about this for two seconds. You train a
about this for two seconds. You train a
policy,
policy,
okay, with a value function or you just
okay, with a value function or you just
train an action condition value
train an action condition value
function. These should be like the same
function. These should be like the same
thing. And it's probably the surrounding
thing. And it's probably the surrounding
details that are making any differences.
You can't just have on policy being
You can't just have on policy being
strictly better because like throwing
strictly better because like throwing
away all of the old data um it means
away all of the old data um it means
like you can't even keep around highv
like you can't even keep around highv
value data. Like let's say you even have
value data. Like let's say you even have
expert data in there, you can't use it.
expert data in there, you can't use it.
It's a big problem.
do this and then where is the
do this and then where is the
rain step
how to train also why would it make a
how to train also why would it make a
difference like how to train Atari games
difference like how to train Atari games
versus whatever results.
It literally shouldn't make a difference
to train a value function.
Much harder to train a value function
how to do a control loop.
how to do a control loop.
But you can train you train an action
But you can train you train an action
condition value function should be the
condition value function should be the
same thing
like training a value function. It's the
like training a value function. It's the
same thing except that the value
same thing except that the value
function has to guess one additional
function has to guess one additional
action from the policy.
action from the policy.
So I'm a beginner in PyTorch. I can
So I'm a beginner in PyTorch. I can
write some custom models where I can
write some custom models where I can
write advance level
write advance level
issue further read uh follow my basic
issue further read uh follow my basic
programming in ML guide on X um it'll
programming in ML guide on X um it'll
direct you to CS231N just do everything
direct you to CS231N just do everything
in CS231N and you'll be good
in CS231N and you'll be good
sampling is different you don't sample
sampling is different you don't sample
entire episode what do you
What do you mean you don't sample an
What do you mean you don't sample an
entire episode? But like you totally
entire episode? But like you totally
can, right?
can, right?
There's no reason you can't.
Like the point that I'm making here and
Like the point that I'm making here and
why I'm doing this, right? This is not
why I'm doing this, right? This is not
like basic like oh implement like simple
like basic like oh implement like simple
like simple like simple offpaul
like simple like simple offpaul
algorithm. The key that I'm trying to
algorithm. The key that I'm trying to
get at here is that these methods really
get at here is that these methods really
aren't that different except that if you
aren't that different except that if you
apply them correctly, uh, having a Q
apply them correctly, uh, having a Q
function should let you reuse old data.
function should let you reuse old data.
So, ignore all the surrounding details
So, ignore all the surrounding details
of how these algorithms are typically
of how these algorithms are typically
used. That should be an advantage,
used. That should be an advantage,
right? So, it's possible that like you
right? So, it's possible that like you
have that and then everything else is a
have that and then everything else is a
mess or like pieces of what is typically
mess or like pieces of what is typically
put around that is a mess. But I want
put around that is a mess. But I want
that core piece of being able to reuse
that core piece of being able to reuse
old data
running some training
for stats. said you
for stats. said you
I'm using 1B just want some training
I'm using 1B just want some training
curves FBRR
curves FBRR
want some training curves on like
want some training curves on like
reasonable problem and then link like an
reasonable problem and then link like an
image or video of policy doing
image or video of policy doing
reasonable stuff
reasonable stuff
I will try to make some time for you
I will try to make some time for you
tomorrow to uh tomorrow or Friday if
tomorrow to uh tomorrow or Friday if
you're around to
you're around to
uh to go through
uh to go through
and take a look at that PR I want to do
and take a look at that PR I want to do
off Paul today because I have it fresh
off Paul today because I have it fresh
in my mind. I've been reading a bunch of
in my mind. I've been reading a bunch of
papers.
So now we do
This comes from the target, right?
Wait, what the heck?
Wait, what the heck?
Target.
Target.
Oh, okay. I see.
Oh, okay. I see.
Yeah. Yeah. Yeah. This is fine.
You do you did actually double check the
You do you did actually double check the
math, right? Like you have the math
math, right? Like you have the math
either from your background or from
either from your background or from
stuff you studied. It's not just from
stuff you studied. It's not just from
Claude, right?
Because that's the key thing.
So this gives us target net.
So this gives us target net.
Now we have to construct wait no this is
Now we have to construct wait no this is
QA
and this gives us the action net right
and this gives us the action net right
and then we have the target net has to
and then we have the target net has to
be
be
the target's going to be different
so we do target
so we do target
max
words
like this. Do
like this. Do
this is your target
times one minus done.
Hey Joseph, if you have time, please
Hey Joseph, if you have time, please
consider making an intro to RL with Py.
So, um, I have literally a full guide
So, um, I have literally a full guide
for you. It's not in video form. uh
for you. It's not in video form. uh
video form alone is a very very bad way
video form alone is a very very bad way
to learn. You cannot learn this stuff
to learn. You cannot learn this stuff
just by watching videos.
just by watching videos.
Um the way the thing that I would
Um the way the thing that I would
suggest and this is a complete reference
suggest and this is a complete reference
guide. If you follow this, you will
guide. If you follow this, you will
understand RL. You'll be able to
understand RL. You'll be able to
contribute to research. You'll be able
contribute to research. You'll be able
to help advance state of the art and
to help advance state of the art and
you'll be able to do a bunch of stuff.
you'll be able to do a bunch of stuff.
Uh it's on X.
Uh it's on X.
It's on my articles tab here.
It's on my articles tab here.
Supposed to be on the website. I think I
Supposed to be on the website. I think I
never finished reformatting everything
never finished reformatting everything
nicely
nicely
on the website. Um, so you'll want to go
on the website. Um, so you'll want to go
here and this is my guide to RL. And if
here and this is my guide to RL. And if
this is too advanced, go back to my
this is too advanced, go back to my
programming and ML guide.
programming and ML guide.
But this will take you through the
But this will take you through the
process of building environments. This
process of building environments. This
will slowly introduce you to background
will slowly introduce you to background
papers. Um, this is how you learn our
papers. Um, this is how you learn our
route.
This is how you do that
This is how you do that
because this is the most common question
because this is the most common question
I get. So, I just I made one, right?
And pretty much the best contributors in
And pretty much the best contributors in
Puffer Lib got good by doing exactly
Puffer Lib got good by doing exactly
that. So,
that. So,
it's a lot of work. Um, it's nowhere
it's a lot of work. Um, it's nowhere
near as much work as I had to put in
near as much work as I had to put in
without having access to that like those
without having access to that like those
resources and puffer lib and everything.
resources and puffer lib and everything.
It's still a fair bit of work. It's a
It's still a fair bit of work. It's a
matter of if go through all of it
matter of if go through all of it
properly.
Pretty much it.
So we have to do
Sims and Rail. for like the last few
Sims and Rail. for like the last few
years. Oh, awesome.
years. Oh, awesome.
The reason I found Puffer
The reason I found Puffer
Rel.
Okay, cool.
Okay, cool.
Ask him.
Ask him.
Okay, good. So, you actually have
Okay, good. So, you actually have
background. Very, very good. I have to
background. Very, very good. I have to
ask, man, because when people submit me
ask, man, because when people submit me
big PRs, occasionally people do just
big PRs, occasionally people do just
like LLM slop a bunch of stuff without
like LLM slop a bunch of stuff without
having, you know, actually put in the
having, you know, actually put in the
effort to understand things and it's a
effort to understand things and it's a
big waste of time. So, as long as like
big waste of time. So, as long as like
this is not that like, yeah, that's
this is not that like, yeah, that's
awesome. Perfect. Then, yeah, if you've
awesome. Perfect. Then, yeah, if you've
written Sims in Ray, then of course it
written Sims in Ray, then of course it
makes sense that you were able to do a
makes sense that you were able to do a
somewhat larger project like this. Very
somewhat larger project like this. Very
nice.
nice.
So, yeah, let me know and I let me know
So, yeah, let me know and I let me know
when that is in. I'm not going to review
when that is in. I'm not going to review
it today because I want to do this
it today because I want to do this
offpaul stuff for a bit, but I will do
offpaul stuff for a bit, but I will do
it this week for you. And uh that's like
it this week for you. And uh that's like
a pretty exciting thing if you can get
a pretty exciting thing if you can get
that to work. Um
that to work. Um
like I uh I sent Finn and Sam who did
like I uh I sent Finn and Sam who did
the drone environment. I sent them a a
the drone environment. I sent them a a
drone to play with. I'd be willing to
drone to play with. I'd be willing to
like if this is good, I'd be willing to
like if this is good, I'd be willing to
send you like a simple robotic arm to
send you like a simple robotic arm to
see if you can get it on the real
see if you can get it on the real
hardware. And yeah, there's a lot of
hardware. And yeah, there's a lot of
stuff you can do from here.
stuff you can do from here.
Depends how far you want to take it,
Depends how far you want to take it,
but it is one of the most obvious
but it is one of the most obvious
applications of puffer lip to industry.
applications of puffer lip to industry.
So there's a lot of opportunity
I Okay.
implement the Oh, I didn't see that last
implement the Oh, I didn't see that last
comment. What do you mean ways to
comment. What do you mean ways to
implement the RL side? Like that is what
implement the RL side? Like that is what
Puffer Lib does, right?
No idea what it was. Now I have a pretty
No idea what it was. Now I have a pretty
good understanding. So the one thing
good understanding. So the one thing
that roboticists do really weird is they
that roboticists do really weird is they
use state-based continuous rewards. So
use state-based continuous rewards. So
they'll do stuff like the closer you get
they'll do stuff like the closer you get
to the target, the reward goes up and
to the target, the reward goes up and
then it's one when you're at the target.
then it's one when you're at the target.
Uh do not do that. It requires a very
Uh do not do that. It requires a very
sketchy RL implementation and will
sketchy RL implementation and will
break. Uh basically it's two errors that
break. Uh basically it's two errors that
they make. They make two errors um and
they make. They make two errors um and
that they cancel each other out. So what
that they cancel each other out. So what
you should do is you do delta to target.
you should do is you do delta to target.
So like let's say that you want to get a
So like let's say that you want to get a
reward for of one for going to the
reward for of one for going to the
target or whatever from infinity or
target or whatever from infinity or
something then you define the reward uh
something then you define the reward uh
as you get reward for getting closer to
as you get reward for getting closer to
the target. You get negative reward for
the target. You get negative reward for
going away and then they add up to one
going away and then they add up to one
right?
right?
you do something like that instead.
Something like change in distance to
Something like change in distance to
target divided by some normalization
target divided by some normalization
factor for instance is a linear one is
factor for instance is a linear one is
very good.
So that's the main thing that you're not
So that's the main thing that you're not
going to be able to look up in a book
going to be able to look up in a book
somewhere.
You can check the uh the drone
You can check the uh the drone
environment for an example if you want
environment for an example if you want
on that cuz I did that there.
Okay, let's see how this is.
This gives us a proper loss.
Same thing as before, but with a proper
Same thing as before, but with a proper
loss function. Let's make this actually
loss function. Let's make this actually
tray and fix anything broken.
Okay.
Okay.
runs.
I have it set. So, gains reward based on
I have it set. So, gains reward based on
percent change.
change.
Uh, make sure that it can't jitter and
Uh, make sure that it can't jitter and
get reward. So, make sure that if it
get reward. So, make sure that if it
moves like farther away and moves closer
moves like farther away and moves closer
or moves like closer and farther away,
or moves like closer and farther away,
make sure it sums to to zero.
Like there's a common hack for instance
Like there's a common hack for instance
where you know if you see uh it just
where you know if you see uh it just
hovering around the goal for instance
hovering around the goal for instance
it's usually that you've messed
it's usually that you've messed
something up with that.
something up with that.
Why does this thing
Why does this thing
you lost?
Okay.
Wait, hang on. Data.
Wait, hang on. Data.
Yeah, this is the target gets next
Yeah, this is the target gets next
observations, right?
observations, right?
And then the Q network gets
And then the Q network gets
observations,
gets rewards,
this is a pretty simple Um
it's a pretty simple simple setup. Okay.
it's a pretty simple simple setup. Okay.
So what do I have something wrong or is
So what do I have something wrong or is
it just like fiddly?
You run Q and target on the mini batch
You run Q and target on the mini batch
of obs, right?
of obs, right?
QA is going to be Q function of the
QA is going to be Q function of the
current action
current action
or rather the Q function of the current
or rather the Q function of the current
state with the selected actions
and then we get target which is going to
and then we get target which is going to
be the maximum
terminals.
terminals.
All right. And then we get an error
All right. And then we get an error
between we take this is going to be you
between we take this is going to be you
overlap them right. So this is the first
overlap them right. So this is the first
one and then it goes forward
QA and target
I was stuck with that earlier because I
I was stuck with that earlier because I
had Hang on.
Oh, they nan out. Lovely.
Oh, they nan out. Lovely.
Uh, hey, Kvert. Stuck with that earlier
Uh, hey, Kvert. Stuck with that earlier
because it hide a reward for moving
because it hide a reward for moving
closer than moving away, so it just
closer than moving away, so it just
jittered. Yeah, exactly.
That's like a very common bug
That's like a very common bug
and it's exactly what I just described
and it's exactly what I just described
to you.
Is the shared target really messing us
Is the shared target really messing us
up this bad or is it something else?
up this bad or is it something else?
Right,
it seems very unstable.
what we're doing makes sense, right?
what we're doing makes sense, right?
Even though we're using like on policy
Even though we're using like on policy
data with off policy learning, but it's
data with off policy learning, but it's
still like a ton of data.
Do I want to do like the same sort of
Do I want to do like the same sort of
advantage estimate? at
I the thing is it's like it's tough to
I the thing is it's like it's tough to
know whether I have something wrong or
know whether I have something wrong or
whether like yeah base DQN's just a
whether like yeah base DQN's just a
sucky algorithm. So, uh obviously
sucky algorithm. So, uh obviously
whatever configuration I have by default
whatever configuration I have by default
on on my much more stable much better
on on my much more stable much better
algorithm is not going to work out of
algorithm is not going to work out of
the box on the much fiddlier like bad
the box on the much fiddlier like bad
worse algorithm.
worse algorithm.
The target network should get the next
The target network should get the next
states not the cur. Doesn't it do this?
states not the cur. Doesn't it do this?
Cuz look, I take right here. This is
Cuz look, I take right here. This is
next date, isn't it?
like I shift it by one.
Like I suppose I can try it without the
Like I suppose I can try it without the
LSTM, but then it's much harder.
Yeah, I shifted logic and targets.
Yeah, I shifted logic and targets.
Exactly. Um, there's still this shared
Exactly. Um, there's still this shared
backbone thing, which is probably iffy,
backbone thing, which is probably iffy,
but I don't know. I really want to
but I don't know. I really want to
remove the LSTM either.
remove the LSTM either.
I just want to see this do something
I just want to see this do something
better than um like six or seven score
better than um like six or seven score
for reference is it's like the trivial
for reference is it's like the trivial
one. Even if I got like 10, I'd be happy
one. Even if I got like 10, I'd be happy
like, oh, okay, this learned something.
But yeah, getting up to like 6 point
But yeah, getting up to like 6 point
something, that's like the sort of the
something, that's like the sort of the
trivial one. I think you can even do
trivial one. I think you can even do
that just by going to one side, like the
that just by going to one side, like the
side that the ball usually falls to or
side that the ball usually falls to or
whatever.
I miss any other like important details?
I miss any other like important details?
I don't think so. It's a very basic
I don't think so. It's a very basic
algorithm, right?
They have this silly copy thing.
It seems like a bad idea to like move on
It seems like a bad idea to like move on
to the optimizations of this if I don't
to the optimizations of this if I don't
have this doing anything. Can I try it
have this doing anything. Can I try it
on like cartpole?
Let me just try it on an easier task
to um
I have some numbers hardcoded.
I have some numbers hardcoded.
Change this
action.
MB OBS.
MB OBS.
What do you mean an MB ops?
Yeah. So, this doesn't definitely
Yeah. So, this doesn't definitely
doesn't work if it doesn't solve cart
doesn't work if it doesn't solve cart
pull.
pull.
We'll use this as our testing task.
Let me make sure I don't have this
Let me make sure I don't have this
backwards.
Yeah, this is eps. F ep's choice, not
Yeah, this is eps. F ep's choice, not
eps.
eps.
I think this is good.
Does carpole even have an LSTM on it?
Oh, it does.
Let's mess with the freaking target
Let's mess with the freaking target
networks.
Odd is not going to do anything but
Odd is not going to do anything but
waste my bloody time.
Okay, I think we can hack this as just
Okay, I think we can hack this as just
like
target is the same thing.
What's this? Um,
this thing here, right?
They like sync it.
Wait. Args.
What is this towel param?
What is this towel param?
What the heck is this? This weird lurp
What the heck is this? This weird lurp
thing.
Target network update rate 1.0. Okay, so
Target network update rate 1.0. Okay, so
it's one anyways.
So you do something like this. Yeah.
And then
Okay, we have this. Now we need the
Okay, we have this. Now we need the
target network update.
Probably like hereish. Yeah.
and then the other thing is there's a
and then the other thing is there's a
stop gr on one of these, isn't there?
There is a stop gr here, right?
Target network.
Wait, do you train both of these? This
Wait, do you train both of these? This
doesn't make sense, right?
doesn't make sense, right?
I'm pretty sure you don't just train
I'm pretty sure you don't just train
both of these.
both of these.
Oh, wait. Is the optimizer Hang on. Hang
Oh, wait. Is the optimizer Hang on. Hang
on. The optimizer only is Yeah, the
on. The optimizer only is Yeah, the
optimizer is only on the Q network.
optimizer is only on the Q network.
So, the target we detach.
Yes.
Still no, huh?
Uh, it's kind of
Uh, it's kind of
being weird now. H
update and slowly update the target
update and slowly update the target
weights.
weights.
So, does it are they not supposed to be
So, does it are they not supposed to be
synchronized
synchronized
as often as possible?
as often as possible?
Target network frequency.
Oh, 500. Holy. Okay.
We update the target more slowly, huh?
Oh,
Oh,
well this is
they should be close. Why wouldn't you
they should be close. Why wouldn't you
just update them all the time then?
just update them all the time then?
They're not going to be close if they're
They're not going to be close if they're
500 updates stale.
Oh, there we go. Okay, look. That
Oh, there we go. Okay, look. That
actually did something.
If we train cartpull for a freaking uh
If we train cartpull for a freaking uh
billion steps, does this do it for us?
billion steps, does this do it for us?
All I need is to see something that's
All I need is to see something that's
actually directionally correct
actually directionally correct
me to start messing with more things.
Of course, now the problem is that um
Of course, now the problem is that um
it's going to take forever on like
it's going to take forever on like
collecting random data.
collecting random data.
Maybe not horrible for cartpull. We'll
Maybe not horrible for cartpull. We'll
see.
Okay, so now it should be pretty much
Okay, so now it should be pretty much
just collecting
just collecting
fresh data.
And now it starts flipping around a
And now it starts flipping around a
bunch.
This didn't work.
This didn't work.
Should need a billion steps for carpull
Should need a billion steps for carpull
anyways, even with like a bad algorithm.
See if this reproduces
Now, see, now it's not there. Same thing
Now, see, now it's not there. Same thing
as before.
Uh,
yeah.
just lucky a few episodes. It doesn't
just lucky a few episodes. It doesn't
seem like it could be. That's like too
seem like it could be. That's like too
much to just be lucky, right?
Look at the details of this.
We have fully unshared nets now, Okay.
and this seems like roughly
and this seems like roughly
this seems like it's the correct
this seems like it's the correct
algorithm to Okay.
The learning rate's freaking massive. We
The learning rate's freaking massive. We
mess with this.
Doesn't help. Do one more order of
Doesn't help. Do one more order of
magnitude just to be sure it's not this.
This is the easiest problem ever to
This is the easiest problem ever to
learn. For reference, you literally can
learn. For reference, you literally can
learn this without reinforcement
learn this without reinforcement
learning. You can literally learn this
learning. You can literally learn this
just by doing imitation learning on like
just by doing imitation learning on like
best experience collected. I did this
best experience collected. I did this
yesterday and it solves instantly.
Oh, wait. Hang on. Hang on. Hang on.
Oh, wait. Hang on. Hang on. Hang on.
Hang on.
Hang on.
Stupid. This has got to be target.
Stupid. This has got to be target.
We're still using the same network,
We're still using the same network,
right?
right?
This is QA
This is QA
the target.
Find the bug and it still doesn't work.
Try this again.
Oh, not any better.
We are we doing this correct?
Euro
and
Wait, what? Where's this come from?
Hang on.
0 1 0 right.
0 1 0 right.
Where does a
Oh, okay. Yeah. Well, this is completely
Oh, okay. Yeah. Well, this is completely
wrong.
wrong.
Cool. It's indexing bug.
Now, this is what I can double check
Now, this is what I can double check
freaking LM for
freaking LM for
cuz this is a basic doc lookup.
Okay, it's a range and then Fine.
Why?
Oh, 68 Okay.
The one one zero
The one one zero
should be this. This this
should be this. This this
we get this.
we get this.
this. Yes. So that's correct now.
Boom. Learns.
Indexing bugs are lovely.
Now, is it super stable? Is it good?
Now, is it super stable? Is it good?
Those are completely different
Those are completely different
questions. But does it learn?
Absolutely.
I just dropped the learning rate a
I just dropped the learning rate a
bunch.
bunch.
Yeah, that actually learns uh funny. It
Yeah, that actually learns uh funny. It
learns faster than our uh fancy
learns faster than our uh fancy
algorithm which is to be fair not
algorithm which is to be fair not
optimized at all for that.
optimized at all for that.
But yeah, there we go.
You go back to breakout. Oh,
still go
five. Uh, I forget. Maybe it does, but
five. Uh, I forget. Maybe it does, but
I thought it took a little longer.
I thought it took a little longer.
Whatever.
This is going to be
Welcome, Spencer. This is our first um
Welcome, Spencer. This is our first um
first off policy thing that we've done
first off policy thing that we've done
in puffer.
Interesting. Doesn't do anything on
Interesting. Doesn't do anything on
breakout like at all.
Um I think we can keep cart pull for
Um I think we can keep cart pull for
now. It's just an easier task
now. It's just an easier task
and like the current implementation just
and like the current implementation just
sucks. So,
sucks. So,
kind of to be expected,
but we can actually use this enough to
but we can actually use this enough to
like add all the extras into this and
like add all the extras into this and
then optimize.
Oh, yeah. You're right. I uh I made it
Oh, yeah. You're right. I uh I made it
way more steps. You're totally right. I
way more steps. You're totally right. I
forgot that I'd made it 200 million
forgot that I'd made it 200 million
steps for freaking cart pole.
But that is our first um first off
But that is our first um first off
policy result in puffer lab. I'm going
policy result in puffer lab. I'm going
to make a
Yeah, I like I 10xed it. You're right. I
Yeah, I like I 10xed it. You're right. I
had forgotten that I 10xed it. I don't
had forgotten that I 10xed it. I don't
It doesn't matter though. Like we're not
It doesn't matter though. Like we're not
expecting this to be good for starters.
expecting this to be good for starters.
We're just expecting it to uh to do
We're just expecting it to uh to do
something, right?
And now what we get to do, uh, we get to
And now what we get to do, uh, we get to
add in HL Gaus as like the first big
add in HL Gaus as like the first big
trick, I think.
trick, I think.
Do you actually know how to do it? It's
Do you actually know how to do it? It's
a little harder cuz I think we have to
a little harder cuz I think we have to
do
Well, I might be able to reuse your same
Well, I might be able to reuse your same
code just slightly differently.
Is this the research? Is this the
Is this the research? Is this the
research easier than Enside? Not
research easier than Enside? Not
exactly.
exactly.
So the difference Spencer and this is
So the difference Spencer and this is
generally true uh the difference is that
generally true uh the difference is that
based on the current capabilities when
based on the current capabilities when
you're building an env you roughly know
you're building an env you roughly know
what the correct end result is right
what the correct end result is right
like if the thing doesn't run fast you
like if the thing doesn't run fast you
know that you just have it slow and you
know that you just have it slow and you
can profile if the thing doesn't learn
can profile if the thing doesn't learn
you know that puffer is good and you
you know that puffer is good and you
should be able to at least train
should be able to at least train
something. The problem with the research
something. The problem with the research
side uh is you literally don't know the
side uh is you literally don't know the
like you don't know what target you're
like you don't know what target you're
trying to even hit, right? And the only
trying to even hit, right? And the only
thing that you can do to build up an a
thing that you can do to build up an a
good estimate of what target you're
good estimate of what target you're
trying to hit is to get really really
trying to hit is to get really really
good at cross referencing tons of papers
good at cross referencing tons of papers
and like fitting a mental model of like
and like fitting a mental model of like
like a basically a probabilistic model
like a basically a probabilistic model
of what is very high likelihood to work
of what is very high likelihood to work
if you do it correctly, what is less
if you do it correctly, what is less
certain and then like an estimate of are
certain and then like an estimate of are
you doing it correct slash uh does it
you doing it correct slash uh does it
just not work etc etc. So the research
just not work etc etc. So the research
the research side is hard due to
the research side is hard due to
uncertainty more than anything. It's
uncertainty more than anything. It's
very very uncertain. Like especially the
very very uncertain. Like especially the
stuff yesterday that I was doing like I
stuff yesterday that I was doing like I
have absolutely no idea what the ceiling
have absolutely no idea what the ceiling
is for that type of work. Like if I just
is for that type of work. Like if I just
did the best possible version of that,
did the best possible version of that,
how good could it do? Nobody in the
how good could it do? Nobody in the
world knows. That's what's hard. Um all
world knows. That's what's hard. Um all
right. I will be right back. I'm use a
right. I will be right back. I'm use a
restroom and then uh we are going to
restroom and then uh we are going to
we're going to add your HL Gaus and it's
we're going to add your HL Gaus and it's
a little different Spencer because we're
a little different Spencer because we're
not adding it on a value function we're
not adding it on a value function we're
adding it on a Q function right so then
adding it on a Q function right so then
this is actually going to go directly
this is actually going to go directly
into the um well this will go directly
into the um well this will go directly
to uh swapping the objective so that the
to uh swapping the objective so that the
main training objective is not quadratic
main training objective is not quadratic
it's not a quadratic loss squared error
it's not a quadratic loss squared error
it is a uh a classification Q function
it is a uh a classification Q function
to bins. Yep, exactly. I don't know if
to bins. Yep, exactly. I don't know if
we need to do the whole Q function
we need to do the whole Q function
actually. If you want to think about
actually. If you want to think about
that for a second, uh while I take a
that for a second, uh while I take a
minute. Um
minute. Um
do I I think I only need to add HL Gaus
do I I think I only need to add HL Gaus
to the action that was actually
to the action that was actually
selected.
selected.
I'm pretty sure. I'm not positive
I'm pretty sure. I'm not positive
though.
though.
What are we researching today? What's
What are we researching today? What's
off policy? Off policy means Q function
off policy? Off policy means Q function
related stuff. I'll be right back,
related stuff. I'll be right back,
Andrew. One sec.
Andrew. One sec.
client stuff. Yeah, do the client stuff
client stuff. Yeah, do the client stuff
first. I will do uh I'll handle research
first. I will do uh I'll handle research
for a little bit.
All
right. So,
right. So,
we have basic Q-learning.
we have basic Q-learning.
Next, HL Gaus.
Next, HL Gaus.
Beware HL Gaus.
You just bin both of these.
Hang on. Let me think.
It's slightly different from normal
It's slightly different from normal
because you have two targets, right?
look up the distributional
look up the distributional
mathy. Um,
let me just I just need to figure out
let me just I just need to figure out
from this how
from this how
how they quantize Guys,
in a discrete distribution
of fixed location
followed by KL minimiz.
followed by KL minimiz.
Oh, so they do this for the
Oh, so they do this for the
So it's not cross entropy, it's KL
So it's not cross entropy, it's KL
right.
Okay. So I see. So uh what we were doing
Okay. So I see. So uh what we were doing
is we were turning it into
is we were turning it into
classification before, right? We were
classification before, right? We were
turning the value function estimation
turning the value function estimation
problem into a classification problem by
problem into a classification problem by
binning the value function uh into a
binning the value function uh into a
number of discrete quanta quantizing the
number of discrete quanta quantizing the
value function the simple way of saying
value function the simple way of saying
that and that lets you do a
that and that lets you do a
classification laws right but because
classification laws right but because
you have two different things to
you have two different things to
discretise here you have the Q function
discretise here you have the Q function
and the target uh you get two different
and the target uh you get two different
distributions and then the natural
distributions and then the natural
optimization metric is a KL distribution
optimization metric is a KL distribution
or as a KL uh divergence term rather
or as a KL uh divergence term rather
which is uh still a much better like
which is uh still a much better like
numerically is still a much better
numerically is still a much better
optimization target than the uh the
optimization target than the uh the
squared error. It's a much more
squared error. It's a much more
expressive loss function.
expressive loss function.
Okay, so
wait, why is this even in the policy?
wait, why is this even in the policy?
So Spencer messed with the policy to
So Spencer messed with the policy to
make this work, right?
Spencer
Spencer took
Spencer took
the normal value function
the normal value function
right here.
HL Gaus
input
input
V min number of bins.
Uh, is this one good?
Uh, is this one good?
Target probs.
Target probs.
I think this is fine, isn't it?
Yeah,
Yeah,
I think we can literally just call this
We can just call this on both of them,
We can just call this on both of them,
can't we?
Like if I just take this?
Like if I just take this?
Okay.
Okay.
Paste this
Paste this
down somewhere.
down somewhere.
Grab this signature.
Grab this signature.
I think you don't need to even make any
I think you don't need to even make any
modification at all. I think you just go
modification at all. I think you just go
up to
up to
you go up to your Q functions.
Anyone can correct me if they have uh
Anyone can correct me if they have uh
knowledge to the contrary,
knowledge to the contrary,
but I'm pretty sure we just go up to our
but I'm pretty sure we just go up to our
Q function.
Q function.
So, here's the action condition one,
So, here's the action condition one,
right?
right?
And here's the target.
Okay.
And then we can do
2 A
2 A
view one
view one
like this
like this
2 A
want
G
target
target
on ized.
Then we do Q loss.
Then we do Q loss.
Uh, is there a KL loss in here? It's
Uh, is there a KL loss in here? It's
really basic, right?
Yeah, it's just this y true time log y
Yeah, it's just this y true time log y
true
loss.
manually did the kale in my version.
manually did the kale in my version.
Yeah.
Yeah.
Uh I don't know if what I just did makes
Uh I don't know if what I just did makes
any sense. We shall see.
any sense. We shall see.
I think it does.
Uh 150
Uh 150
happened.
Size of tensor must match.
Size of tensor must match.
What's the What is this thing supposed
What's the What is this thing supposed
to take?
input
input
B minax.
Is this thing not supposed to be a flat
Is this thing not supposed to be a flat
tensor?
This thing supposed to be something
This thing supposed to be something
else.
see how I can figure out
probably
Oh, is you do not just call this I
Oh, is you do not just call this I
thought you just call this one on the
thought you just call this one on the
other. No.
other. No.
You did it different.
We have this code down here.
This is the same code, isn't it?
Maybe there's not supposed to be an
Maybe there's not supposed to be an
unsqueeze.
Ew.
Heck. Why is this not valid?
Oh, of star N. Yeah, this is wrong.
Okay.
Okay.
Well, this still works, right?
It seems less stable.
Yes. Way less stable. No.
Yes. Way less stable. No.
Way less stable.
Way less stable.
Hm.
Hm.
Oh. Well, because um yeah, the Q
Oh. Well, because um yeah, the Q
function blows up, right?
This one doesn't work though.
Is there something that's done to
Is there something that's done to
normalize this so it doesn't blow up
normalize this so it doesn't blow up
horribly?
Maybe uh let's see if has an
Maybe uh let's see if has an
implementation.
Honestly, the uh the clean RL
Honestly, the uh the clean RL
implementations are so much better than
implementations are so much better than
reading the original mess of papers
reading the original mess of papers
like speedrunning. How can we confuse
like speedrunning. How can we confuse
you with more math than is required?
I guess C C-51 should have um at least
I guess C C-51 should have um at least
some reasonable form of this. How long
some reasonable form of this. How long
is this? Oh, this is not bad.
That's the idea. The idea is that
That's the idea. The idea is that
binning is better than MSA. Yes.
binning is better than MSA. Yes.
And um there's very very extensive
And um there's very very extensive
evidence for this
evidence for this
like probably more than most of the
like probably more than most of the
other things in off policy.
the hell decided to call this Adams.
the hell decided to call this Adams.
Okay, so this is what it is. It's just a
Okay, so this is what it is. It's just a
lin space. They actually do uh negative
lin space. They actually do uh negative
100 to 100, which
100 to 100, which
for this they actually do that.
And then wait, why did they
And then wait, why did they
they actually output
times numbum atoms?
times numbum atoms?
Kind of sketchy, isn't it?
Yes, Spencer. I do notice though that
Yes, Spencer. I do notice though that
they they have the output here. The
they they have the output here. The
output of the Q function is actually
output of the Q function is actually
equal to the number of actions times the
equal to the number of actions times the
number of atoms which is not the way we
number of atoms which is not the way we
have it implemented. Right?
They actually have this big blown up
They actually have this big blown up
final layer. This is the C-51 reference
final layer. This is the C-51 reference
which has
which has
stuff in it.
work
next. Okay, this is Adam thing.
What is the soda
What is the soda
sketch? And
sketch? And
uh we use cosine in um in puffer lib.
uh we use cosine in um in puffer lib.
Most libraries have linear or just don't
Most libraries have linear or just don't
even have it.
From my experiments at least, the choice
From my experiments at least, the choice
of umuler matters a lot less than just
of umuler matters a lot less than just
having it. Like you really need to have
having it. Like you really need to have
auler. The choice of which one you use
auler. The choice of which one you use
matters a lot less than just having one.
And then also being able to tune it
And then also being able to tune it
obviously.
Yeah, that's what we do at the moment.
Yeah, that's what we do at the moment.
You could add additional hypers if you
You could add additional hypers if you
wanted to like mess with final learning
wanted to like mess with final learning
rates and stuff, but we just Yeah, we
rates and stuff, but we just Yeah, we
just decay to zero at the moment.
Doing that does slightly mess with your
Doing that does slightly mess with your
uh your graphs a bit cuz like know it
uh your graphs a bit cuz like know it
makes most things look like they're
makes most things look like they're
leveling out towards the end of training
leveling out towards the end of training
even though it could just be the
even though it could just be the
learning rate, but it's still pretty
learning rate, but it's still pretty
good.
That's the only thing I don't really
That's the only thing I don't really
like about it.
Well, they do have in here negative 100
Well, they do have in here negative 100
to 100
representing the
and I don't see any sort of
and I don't see any sort of
normalization on Yes.
Yeah, this doesn't have enough
Yeah, this doesn't have enough
resolution, right?
like this for now.
Okay. So, the really big thing is the um
Okay. So, the really big thing is the um
the bootstrap, right?
endstep bootstrap.
endstep bootstrap.
I want to grab it from here.
I want to grab it from here.
Right. One
Wait, can we do J with this still? Hang
Wait, can we do J with this still? Hang
on.
We do J.
There's no reason we can't just do J E,
There's no reason we can't just do J E,
right?
They make the targets.
or puffer advantage rather.
Here's
a multi-step learning, right?
Okay, let me see if this makes sense.
What do you need for this? You need
What do you need for this? You need
values.
Yeah, we can totally do this, right?
Yeah, we can totally do this, right?
Let's just move this thing.
Oh, well, we use it for sampling as
Oh, well, we use it for sampling as
well, don't we?
well, don't we?
Uh, which is totally screwed up because
Uh, which is totally screwed up because
Yeah, this thing is totally screwed up.
Because basically we should have
use this for sampling at all. Huh?
use this for sampling at all. Huh?
Can have it be stale.
So super unstable, but there you go.
So super unstable, but there you go.
Does learn.
Does learn.
Go grab all this crap.
Grab all this crap.
And so we do
target
use is the Q value of the target
use is the Q value of the target
function for advantage or do you use the
function for advantage or do you use the
um
um
your own Q value?
your own Q value?
I don't know. Let's try this.
Do
Do
U sub AS.
Eat
ratio. So
this let us do anything
bandages. Just
we can compute advantages.
we can compute advantages.
And do we use these?
I think you do like this, right?
I think you do like this, right?
MB returns. It's going to be advantages
MB returns. It's going to be advantages
plus
QA.
Well, no. You would construct this from
Well, no. You would construct this from
Q target, right?
Q target, right?
Because this is Yeah. Yeah. This is the
Because this is Yeah. Yeah. This is the
target. This is what you're doing
target. This is what you're doing
instead of the target. Okay. So, you do
instead of the target. Okay. So, you do
this is going to be
this is going to be target.max
yada yada.
yada yada.
Don't need to multiply.
Okay. So now you have
Okay. So now you have
advantage computed by target
advantage computed by target
and then
believe it's I believe that it's
believe it's I believe that it's
something like this All right.
H
this.
Okay.
Oh, okay. So, something like this is
Oh, okay. So, something like this is
actually reasonable, I think.
actually reasonable, I think.
Super unstable.
And then
The replay buffer suppose
structure of a replay buffer is
structure of a replay buffer is
trickier.
We could just see if we can get this
We could just see if we can get this
something like this to work first.
We're missing filtering as well.
Yeah, we're missing filtering here,
Yeah, we're missing filtering here,
right?
That's pretty major.
But the idea here is that
so the target is the value function,
so the target is the value function,
right?
The target's the value function of uh
The target's the value function of uh
taking it's the well that's It's an
taking it's the well that's It's an
optimistic one. It's because it's under
optimistic one. It's because it's under
the best action.
Then this should be advantages plus
Then this should be advantages plus
target, right?
target, right?
Advantages plus target
Advantages plus target
will not work.
This is actually not that far off, I'm
This is actually not that far off, I'm
pretty sure. Right.
pretty sure. Right.
So, you need to save
So, you need to save
We need to save the values.
We need to save the values.
Yeah. Yeah. Yeah. Okay. So, I think what
Yeah. Yeah. Yeah. Okay. So, I think what
we the thing that was wrong here.
we the thing that was wrong here.
Okay.
Undo a little bit of this. It's pretty
Undo a little bit of this. It's pretty
close.
Okay. So, this is where we're going to
Okay. So, this is where we're going to
go back to and we're going to store
go back to and we're going to store
the targets
uh where the value function will go.
This is going to be target.
Okay.
And now
And now
this is a slightly off policy
this is a slightly off policy
uh advantage estimate.
Okay. Okay. And we use this
we use this to select data
just like we did before.
And the only difference now,
And the only difference now,
well actually
try something.
try something.
Let's do target equal
Let's do target equal
uh MB.
uh MB.
Try this.
Mess up somehow.
Uh why does the target have this
Uh why does the target have this
dimensionality?
Right. You need the maximum of this.
Right. You need the maximum of this.
Okay.
Okay. So now we have our target is MB
Okay. So now we have our target is MB
advantages. Right
advantages. Right
now we are actually doing general
now we are actually doing general
advantage estimation on this and um
okay this does do some learning.
okay this does do some learning.
Now the only thing that's wrong here
this value can get stale.
What we would like to do is we'd like to
What we would like to do is we'd like to
update
we would like to update the values,
we would like to update the values,
right?
And we do this,
we do this by rerunning.
We have to rerun the target computation,
We have to rerun the target computation,
don't we?
Do you see the problem here? Right. The
Do you see the problem here? Right. The
problem is that this is the original
problem is that this is the original
value that gets stored
value that gets stored
and um this is going to be stale because
and um this is going to be stale because
we're going to have updated the policy
we're going to have updated the policy
and we do not want this to be stale
and we do not want this to be stale
otherwise we lose the uh the generality
otherwise we lose the uh the generality
of policy training. So, I think what we
of policy training. So, I think what we
do for this is we grab this like compute
do for this is we grab this like compute
puff advantage thing from down here
and we're going to do it right.
Okay, we're going to do it right like
Okay, we're going to do it right like
this. We're going to grab this target
this. We're going to grab this target
computation
value.
This is going to be target max.
This is going to be target max.
And so we have this again.
I think this is good.
I think this is good.
Now we take the advantage.
Okay. So, we have this. Where is this
Okay. So, we have this. Where is this
ratio thing?
That's an important sample. Shoot.
That's an important sample. Shoot.
Uh, we have that.
Uh, we have that.
That's going to have to change. We can
That's going to have to change. We can
make this ones for now.
Okay. So,
okay.
And now we just do here
uh this target
Right.
Right.
Think that something like this is
Think that something like this is
correct because now
make sure we get this right. Oh, this is
make sure we get this right. Oh, this is
advantage.
don't know.
Ah, okay. So, target is
mess this up.
MV values
like this
because it's the selected.
Okay. So, evidently I've broken
Okay. So, evidently I've broken
something.
Uh,
I think I've broken something, right?
What have I done wrong here?
What have I done wrong here?
One advantage heroes.
current is going to be
plus this.
Now, this is normally what you would do.
Now, this is normally what you would do.
I think
what I messed up
what I messed up
to your advantage, use this to sample
to your advantage, use this to sample
data.
Then MB values
Then MB values
get your maximum target.
get your maximum target.
This is a solid estimate here.
Do this on the current data.
Uh, and then wait, does this did I
Uh, and then wait, does this did I
forget to shift them over by one?
forget to shift them over by one?
It um
this is like colon
this is like colon
One.
There we are.
Super unstable, but at least we get
Super unstable, but at least we get
something.
something.
Super unstable. Yeah, cuz the target's
Super unstable. Yeah, cuz the target's
blowing up.
blowing up.
But does an algorithm of this form make
But does an algorithm of this form make
sense? Right?
sense? Right?
Is the main thing I want to figure out
Is the main thing I want to figure out
which is
which is
you replace the uh the policy and value
you replace the uh the policy and value
function,
function,
the Q function and the target.
Like to understand why those need to be
Like to understand why those need to be
two separate networks, but sure for now.
This advantage estimate still makes
This advantage estimate still makes
sense.
Well,
it's kind of weird, isn't it?
Like technically, yes, you can write it.
Like technically, yes, you can write it.
I think that there is um
I think that there is um
off policyiness thing again here.
Okay. But like outside of that,
this should be a very close to
this should be a very close to
comparable algorithm, I would think, to
comparable algorithm, I would think, to
um what we had originally, provided I
um what we had originally, provided I
get it correct, right?
get it correct, right?
Because like all I've done is I've
Because like all I've done is I've
replaced the
replaced the
I've replaced the policy and value
I've replaced the policy and value
objective.
Oh, I guess I suppose we also don't have
Oh, I guess I suppose we also don't have
entropy in here, which is a problem,
entropy in here, which is a problem,
right? So, we'll have to fix that.
right? So, we'll have to fix that.
But like this should be a substantially
But like this should be a substantially
similar algorithm to what we had before.
And then the question is just like okay
And then the question is just like okay
why doesn't it work as well
be any number of things.
Well, first of all,
let's try. Let's see if this Well, hang
let's try. Let's see if this Well, hang
on. I think before I do this, I'd like
on. I think before I do this, I'd like
to get this to be stable.
If I just do um the Q target without
If I just do um the Q target without
advantage.
If I just comment this. Does this do it?
Oops.
Uh how'd this happen?
Right. So this has got to be
target equals
this this thing here, right?
this this thing here, right?
Here's previous version.
Uh, this is supposed to be
like stable and good.
Not
Not
we have lots of things to ablate here,
we have lots of things to ablate here,
right?
right?
Got this version.
Okay. Right. This version
Okay. Right. This version
that's stable. So, we have the original
that's stable. So, we have the original
DQN stable at this point. Yeah.
DQN stable at this point. Yeah.
Let's see if we can get the original
Let's see if we can get the original
architecture back in and still have it
architecture back in and still have it
stable.
or if this screws it up somehow, right?
Work basically off of uh off of this.
So, this will be
So, this will be
uncomment these guys.
Comment these guys like this.
L
eight.
This
This
encode observations.
This
and we have to remove this
and we have to remove this
synchronization thingy as well.
Way less stable, right?
I don't like the fact that there's a
I don't like the fact that there's a
freaking separate target network.
Super jank.
And this does something. But
what is the point of this thing again?
This in the original DQN.
Doesn't seem like it
Doesn't seem like it
was introduced later.
was introduced later.
Okay.
Okay.
So, um is there a way to get rid of it?
So, um is there a way to get rid of it?
Because this is kind of uh not a good
Because this is kind of uh not a good
thing to have.
like at all.
like at all.
Freaking double your network size.
Freaking double your network size.
have this super janky synchronization
have this super janky synchronization
problem.
I really like I'm fine if I need a
I really like I'm fine if I need a
separate head on the same network,
separate head on the same network,
but like this is a pretty immediate
but like this is a pretty immediate
Well, hang on.
Well, hang on.
This is a big change. I kind of did this
This is a big change. I kind of did this
stupidly, didn't I?
stupidly, didn't I?
Yeah. Yeah. Yeah. Hang on. I did this I
Yeah. Yeah. Yeah. Hang on. I did this I
did this oblation really stupidly.
Hang on.
I kind of screwed this up massively,
I kind of screwed this up massively,
didn't I?
Not go back that far.
Yes.
do.
Yes.
Okay. So, this is the separate network
Okay. So, this is the separate network
version, right?
Uh, and I think that I don't even have
Uh, and I think that I don't even have
these. Do I have these sync? Yeah. So,
these. Do I have these sync? Yeah. So,
let's not synchronize them. Let's see
let's not synchronize them. Let's see
what happens.
You don't synchronize them.
You don't synchronize them.
Nothing trains.
If I do,
If I do,
if I do like this instead.
Uh, this works perfectly fine.
Uh, this works perfectly fine.
entropy hitting zero sooner, better? No,
entropy hitting zero sooner, better? No,
it is not. You generally don't want your
it is not. You generally don't want your
entropy to collapse.
entropy to collapse.
Okay, so um yeah, this this seems to
Okay, so um yeah, this this seems to
work, guys. Like
work, guys. Like
at least for this simple problem, not
at least for this simple problem, not
having a separate target network is like
having a separate target network is like
totally fine.
totally fine.
Uh, and that's kind of actually
Uh, and that's kind of actually
important because
we should be able to now use uh, LSTM
like this and then self target self.
like this and then self target self.
This
This
we should be able to do this, I think.
Is this what size?
Is this what size?
This
Okay. Well, very slight hit to
Okay. Well, very slight hit to
stability,
stability,
but this is fine, right?
So having shared target a okay
So having shared target a okay
works with LSTM works without LSTM.
Exactly right.
Yes.
Back to Tiny Network.
Introduced by
Is that not the original DQN paper?
Oh, no. That's not the uh the original
Oh, no. That's not the uh the original
DQM paper. What?
Yeah.
This works.
At least now we know it's not like
At least now we know it's not like
everything freaking breaks. At least on
everything freaking breaks. At least on
the simple task, right?
So we have uh we have things that we can
So we have uh we have things that we can
play with now as a result of this. And
play with now as a result of this. And
this target here
this target here
if we do this
super unstable
does learn something but super unstable.
Okay.
Okay.
What is the motivation for adding off
What is the motivation for adding off
policy
policy
should be a guy that
should be a guy that
problems it can given enough data.
problems it can given enough data.
Right? So the idea is that this should
Right? So the idea is that this should
allow you to my goal is to come up with
allow you to my goal is to come up with
an off policy variant here that's
an off policy variant here that's
basically the same exact algorithm as
basically the same exact algorithm as
what I have now except we don't have the
what I have now except we don't have the
off policyiness issue. So if we want to
off policyiness issue. So if we want to
reuse some old data we can do that
reuse some old data we can do that
without crashing the algorithm. That's
without crashing the algorithm. That's
it.
So,
So,
what is wrong with this? Did I construct
what is wrong with this? Did I construct
I construct the target wrong?
Is it just this?
Well, there we Well, little unstable,
Well, there we Well, little unstable,
but um
but um
huh. Very unstable.
Funny. I plugged this into an advantage
Funny. I plugged this into an advantage
estimate, right?
estimate, right?
And we can see that it's like
And we can see that it's like
it's learning.
it's learning.
That's about solves the task right
That's about solves the task right
there. Then it gets unstable
there. Then it gets unstable
potentially.
Okay, let me think about this because
Okay, let me think about this because
maybe I did this weirdly. So the idea
maybe I did this weirdly. So the idea
was to add some sort of advantage
was to add some sort of advantage
estimate in
we use for this
target
network.
I'm resisting the urge to put this into
I'm resisting the urge to put this into
Grock because I think it is just going
Grock because I think it is just going
to give me some
You want to have
the maximum value or the action value.
Try QA.
What if I do like a
You have the same thing of it being like
You have the same thing of it being like
quick but unstable.
I will stick it in the Gro real quick
I will stick it in the Gro real quick
just to sanity check.
just to sanity check.
I don't know why. I know this is a bad
I don't know why. I know this is a bad
idea. I'm going to do it anyway.
think that this is the same thing as
think that this is the same thing as
I've done it.
The only thing I don't know is if you
The only thing I don't know is if you
use the Q function or the target
use the Q function or the target
function.
function.
I should probably just think about it a
I should probably just think about it a
little bit, but I think that there's
little bit, but I think that there's
something else
something else
either way.
either way.
Oh, do I need to look in retrace?
Thought retrace was rather different.
Oh, you know what? You actually can't,
Oh, you know what? You actually can't,
right?
Yeah. You can't do this because it's off
Yeah. You can't do this because it's off
policy, right?
You actually do need to retrace which is
You actually do need to retrace which is
probably going to be the exact same
probably going to be the exact same
thing as I have but with important
thing as I have but with important
sampling right
is retracing here.
Oh,
mirror.
Okay. So, this is what we would do here,
Okay. So, this is what we would do here,
right?
They don't have like a super mathy paper
They don't have like a super mathy paper
without a ton of results.
But if this simplifies to J, then I
But if this simplifies to J, then I
should actually just be able to
should actually just be able to
it's probably going to just be like GA
it's probably going to just be like GA
with important sampling, right?
Yeah. So off policy.
Yeah. So off policy.
Oh, but you also have tree backup.
Oh, but you also have tree backup.
Important
sampling ratio truncated at one.
Now, is this a free trade?
Yeah, I'll read.
In fact, this is going to end up being
In fact, this is going to end up being
almost exactly what I have already
almost exactly what I have already
because I actually already have
because I actually already have
incorporated uh puffer advantage already
incorporated uh puffer advantage already
has important sampling into in it.
has important sampling into in it.
Help.
This is actually quite good because this
This is actually quite good because this
is so this is going to be a bit of a
is so this is going to be a bit of a
pain but this is at least like
pain but this is at least like
reasonably close to what we have right
reasonably close to what we have right
like I already have an important
like I already have an important
sampling based um advantage estimation
sampling based um advantage estimation
kernel.
kernel.
I am going to have to modify it so that
I am going to have to modify it so that
it makes sense.
Okay, before I do this, here's a
Okay, before I do this, here's a
question, right? Should I expect to be
question, right? Should I expect to be
able to have our current J work
with I think I should be able to have
with I think I should be able to have
our current advantage function work
our current advantage function work
because the data is pretty much on
because the data is pretty much on
policy, right? Cuz like I don't actually
policy, right? Cuz like I don't actually
have a buffer sampling yet.
have a buffer sampling yet.
So, let me spend a little bit of time on
So, let me spend a little bit of time on
fiddling with this to see if I can get
fiddling with this to see if I can get
it to be stable. Then, we're going to do
it to be stable. Then, we're going to do
the retrace correction, and we'll see
the retrace correction, and we'll see
how much time I have after that. I'll be
how much time I have after that. I'll be
right back.
go check one other thing
[Music]
There was supposed to be um somebody out
There was supposed to be um somebody out
for the uh the AC here by now. But I
for the uh the AC here by now. But I
haven't heard a Haven't heard a peep.
[Music]
Let's actually let's look at what
Let's actually let's look at what
what does this use? This uses
both the target and
This uses Q target
This uses Q target
evaluated on the action
evaluated on the action
which is shared in my case. So it
which is shared in my case. So it
doesn't matter which one you use but it
doesn't matter which one you use but it
would be QA that you would like the
would be QA that you would like the
action condition one.
action condition one.
And this would be U sub A.
Set the ratio to one, which is fine if
Set the ratio to one, which is fine if
it's on policy.
And this works, but it's unstable,
And this works, but it's unstable,
right?
Oh, wait. This just works.
Oh, wait. This just works.
This QA
This QA
target equals advantage.
target equals advantage.
Huh?
Huh?
Okay. So then this just this works and
Okay. So then this just this works and
is stable.
Yeah. Well, okay. It's It can bounce a
Yeah. Well, okay. It's It can bounce a
little bit, but isn't this supposed to
little bit, but isn't this supposed to
be MB
values plus advantage? Isn't that
values plus advantage? Isn't that
supposed to be this?
I mean, evidently not right.
Let me see what the um how we're
Let me see what the um how we're
supposed to actually construct the loss.
running will be there in about now.
running will be there in about now.
Okay.
Well, so I didn't miss them. They're
Well, so I didn't miss them. They're
just late.
All right. So, back to this. Um,
right. We need to figure out. So, right
right. We need to figure out. So, right
now I have this set to the advantage,
now I have this set to the advantage,
but I think that you're supposed to set
but I think that you're supposed to set
it to the return.
it to the return.
And the return is a different quantity.
And the return is a different quantity.
It's this plus the value.
loss function Q minus Q rat
B wait reward. board.
Okay, so this is recursive formulation
Okay, so this is recursive formulation
and it does have
it has this of the next state, right?
it has this of the next state, right?
So it's plus value.
But what's V target
B target is the maximum.
Is this already taken into account?
Is this already taken into account?
Perhaps
why do you adopt weird terminology?
Do you equate value based policy based
Do you equate value based policy based
off policy
off policy
policy?
Um
Um
I mean off policy is generally just you
I mean off policy is generally just you
have a Q function so that you're that
have a Q function so that you're that
the policy that you are optimizing is
the policy that you are optimizing is
not the policy that's necessarily
not the policy that's necessarily
generating the data. Um and on policy
generating the data. Um and on policy
supposedly it is but not really. Oh look
supposedly it is but not really. Oh look
all the terminology is wrong anyways.
all the terminology is wrong anyways.
Literally all the terminology is wrong
Literally all the terminology is wrong
anyways.
Hey, Kevin.
I'm trying to figure out.
I don't like the fact that this shifted
I don't like the fact that this shifted
one over, right?
Our advantage doesn't our advantage
Our advantage doesn't our advantage
already take this into account?
The advantage is normally nor it's
The advantage is normally nor it's
usually normalized as well, right?
usually normalized as well, right?
Okay. So, let me try something else. We
Okay. So, let me try something else. We
do.
Do I not shift this somewhere?
Okay.
Yeah. So, there is possibly an indexing
Yeah. So, there is possibly an indexing
screw up here.
You do want this to be like
You do want this to be like
MB values plus advantage, right?
But then this doesn't work.
All
right, let's try something something
right, let's try something something
different here. I want to see
where's the advantage normalization.
advantage normalization is like
advantage normalization is like
essential
essential
um for the on policy.
See, where is
See, where is
where did it go
world model for R? Yeah, that's a meme.
world model for R? Yeah, that's a meme.
That's just that's not working.
That's just that's not working.
No way.
Oh, here it is.
So, we do it like this.
So, we do it like this.
What happens if we add just advantage
What happens if we add just advantage
normalization?
Uh, this does solve the task, albeit
Uh, this does solve the task, albeit
quite a bit of instability.
Does doing this make any sense in a
Does doing this make any sense in a
Q-learning context, though?
They're trying to RL on like
They're trying to RL on like
hallucinated Sims.
It like makes even less sense than
It like makes even less sense than
normal modelbased RL does.
At least there you're getting fresh
At least there you're getting fresh
data.
without this.
Without this
now, you still have uh instability,
now, you still have uh instability,
right?
right?
You still have wild instability,
You still have wild instability,
which it gets like towards the end of
which it gets like towards the end of
training pretty good,
training pretty good,
but I think this is kind of okay.
Oh, hang on.
Yeah, this is the advantage function is
Yeah, this is the advantage function is
going to be super messed up because it's
going to be super messed up because it's
actually your off policy immediately
actually your off policy immediately
anyways because of the uh the crazy
anyways because of the uh the crazy
epsilon greediness. All right,
epsilon greediness. All right,
we have this in here as an option at
we have this in here as an option at
least. And we'd like to have this at
least. And we'd like to have this at
some point because it'll make um it'll
some point because it'll make um it'll
make the distributional RL thing
make the distributional RL thing
actually work reasonably.
Can we not try this? Hang on. We can try
Can we not try this? Hang on. We can try
this without advantage estimation, can't
this without advantage estimation, can't
we?
we?
If I just do like targets
If I just do like targets
like this and I just do
like this and I just do
target
target
minus And
it does actually optimize
it does actually optimize
So if you just norm targets,
I'd say it's it's yeah, it's way less
I'd say it's it's yeah, it's way less
stable than not norming the targets.
I have to think whether normalizing
I have to think whether normalizing
targets makes any sense,
targets makes any sense,
but you can technically do it.
but you can technically do it.
All right.
Okay. And then there is now
Okay. And then there is now
do we go figure out retrace? Is that the
do we go figure out retrace? Is that the
main thing that we have to do?
main thing that we have to do?
I mean there kind of seem like there are
I mean there kind of seem like there are
several things.
worlds. Yeah, but the simulated worlds
worlds. Yeah, but the simulated worlds
suck.
I think they're literally just going to
I think they're literally just going to
like exploit errors in the sim
like exploit errors in the sim
basically.
Not like the simulated worlds suck and
Not like the simulated worlds suck and
they're super slow.
They did get it to be coherent enough
They did get it to be coherent enough
that it's not a total fever dream, but
that it's not a total fever dream, but
like it's nothing like an actual fixed
like it's nothing like an actual fixed
sim.
Yeah. Sim real doesn't even remotely
Yeah. Sim real doesn't even remotely
enter into this conversation. Like it's
enter into this conversation. Like it's
a mess way before that.
It's not just a sim to real gap. It's
It's not just a sim to real gap. It's
literally that you are creating a gap.
literally that you are creating a gap.
Like you're optimizing for a gap.
Wait, what? Can you not just
Okay, so
Okay, so
evidently this is actually a thing
evidently this is actually a thing
supposedly.
Let's put this in here.
I don't like how jumpy this thing is,
I don't like how jumpy this thing is,
but
but
it does optimize
somewhat.
somewhat.
Uh, I think we can actually
Uh, I think we can actually
Try the HL G thing with this now, right?
Try the HL G thing with this now, right?
Because this is no longer a mess maybe.
Oh, how are you? Switched Windows. No,
Oh, how are you? Switched Windows. No,
it's just that I had this old uh this
it's just that I had this old uh this
desktop at this place already had
desktop at this place already had
Windows on it. I don't want to go
Windows on it. I don't want to go
through the effort of reinstalling it.
through the effort of reinstalling it.
As you can see, I'm actually I'm working
As you can see, I'm actually I'm working
here sshed into a Linux box. Anyways, so
okay. So this optimizes something.
okay. So this optimizes something.
Not great though.
USL. Yeah, but like I'm on a native
USL. Yeah, but like I'm on a native
Linux environment here. It just sshed
Linux environment here. It just sshed
into a Linux box, so it doesn't make a
into a Linux box, so it doesn't make a
difference.
Yeah. So, this doesn't work at all. Hel
Yeah. So, this doesn't work at all. Hel
Gaus, which is funny. means I'm probably
Gaus, which is funny. means I'm probably
doing it wrong, right?
Like this is still the most stable
Like this is still the most stable
version.
do we try to do retrace now?
do we try to do retrace now?
No, I don't.
No, I don't.
Something is still wrong. My mind I
Something is still wrong. My mind I
think something is still wrong here.
What about this? So we do target.
What I do?
Okay. So here is this version of it.
And like
And like
it doesn't really work if I do this.
Not at all. Right.
Oh, what
are
Yeah, but hang on. Like this is not even
Yeah, but hang on. Like this is not even
supposed to work now, right? Because
Yeah, this is not even supposed to work
Yeah, this is not even supposed to work
because of how off off policy it is,
because of how off off policy it is,
right?
So maybe we should not be worried about
So maybe we should not be worried about
that.
that.
Uh the HL G not working is
Uh the HL G not working is
I think much sketchier in my mind.
I think much sketchier in my mind.
Right.
Right.
Like
Like
say we have this target cuz this works
say we have this target cuz this works
instantly.
instantly.
Maybe that's the thing we should do
Maybe that's the thing we should do
next.
Distributional supposed to work really
Distributional supposed to work really
well and the fact that it doesn't is
well and the fact that it doesn't is
sketchy. This thing works.
Do like this.
just super unstable. Like the queue just
just super unstable. Like the queue just
blows up.
blows up.
Oh, hang on. I see it. I see it. I uh I
Oh, hang on. I see it. I see it. I uh I
see it.
Uh, and I was missing a detach up here
Uh, and I was missing a detach up here
as well, am I not?
as well, am I not?
I'm missing a detach up top. Okay, so
I'm missing a detach up top. Okay, so
there are a lot of things wrong. Hang
there are a lot of things wrong. Hang
on.
on.
View size.
doesn't work at all. Lovely.
doesn't work at all. Lovely.
That's
That's
things bugs.
Okay. So there is
Okay. So there is
there's this working right with the
there's this working right with the
correct detached loss.
correct detached loss.
Okay, next one.
Why does it not?
Not this one.
Well, Q and the targets blow up
Well, Q and the targets blow up
like instantly.
like instantly.
What happens to Q and the targets when I
What happens to Q and the targets when I
don't do they still blow up?
Yeah, they still blow up
Yeah, they still blow up
nowhere near as badly
into 20.
Okay.
This blows up dramatically.
This blows up dramatically.
some reason
this how it's supposed to be
should be um
it's just A quantization of both of
it's just A quantization of both of
these, isn't
Hang on.
Second
Yeah. Put a note on the door.
Okay. So,
did I
did I
did I do this reasonably?
did I do this reasonably?
I think this one is
Now they do have a Q function here.
Target is also a categorical.
How do I apply this?
How do I apply this?
That one sucks.
That one sucks.
Um,
where is this thing?
to represent Q as the expected value
to represent Q as the expected value
categorical distribution.
But what I want to figure out here,
But what I want to figure out here,
right, is I know how to do this when you
right, is I know how to do this when you
have a single value function.
a video showing current task complete
a video showing current task complete
robot arm.
robot arm.
How I did?
How I did?
No. Look, thank you very much. I
No. Look, thank you very much. I
definitely will. Let me I need to finish
definitely will. Let me I need to finish
this off Paul stuff at least like make a
this off Paul stuff at least like make a
decent chunk of progress on this today
decent chunk of progress on this today
but that will be up on the agenda.
but that will be up on the agenda.
Um
I have like basic stuff working
I have like basic stuff working
but like all the additional tricks I
but like all the additional tricks I
need to be very careful to do them
need to be very careful to do them
correctly.
Yes. So you have your Q function
TD cross entropy.
How the hell is this a TD error?
We have the probability
date action data.
It looks like to me
it looks like they've made two different
it looks like they've made two different
distributions, right? Presumably these
distributions, right? Presumably these
are the two different sets of weights.
I'm trying to figure out if I did this
I'm trying to figure out if I did this
correctly because all I did right here
correctly because all I did right here
is I just called it on both of these and
is I just called it on both of these and
I don't know if this is actually how you
I don't know if this is actually how you
construct this because like it's
construct this because like it's
different in the on policy case where
different in the on policy case where
you just have one value function whereas
you just have one value function whereas
here you have the Q function and the
here you have the Q function and the
target function but I kind of just
target function but I kind of just
guessed that you do both of them in KL
guessed that you do both of them in KL
but I don't know if that's All right.
I also don't know how this is supposed
I also don't know how this is supposed
to work
to work
in a Q-learning case when the Q function
in a Q-learning case when the Q function
like tends to blow up a lot more. Well,
like tends to blow up a lot more. Well,
I guess the value function does as well,
I guess the value function does as well,
doesn't it?
But the thing is you predict normalized
But the thing is you predict normalized
advantages
advantages
um in the on policy case
um in the on policy case
and normalized advantages are a lot more
and normalized advantages are a lot more
sensible.
Yeah. And then it just starts
Yeah. And then it just starts
implementing stuff that I didn't ask it
implementing stuff that I didn't ask it
to implement because these models are
to implement because these models are
stupid. Of course. Um
could have written like literally two
could have written like literally two
lines
prediction probs.
fiction logits.
Yeah, these aren't logits though. This
Yeah, these aren't logits though. This
makes no bloody sense, right?
Because the targets now
and then this is just KL, right?
categorical distribution.
Okay, this is gross.
Okay, this is gross.
Super freaking expensive, I guess.
But then you also have the separate
But then you also have the separate
target, right?
Okay. So you only HL Gaus
Okay. So you only HL Gaus
the target network.
It's so stupid that you need this sort
It's so stupid that you need this sort
of a hack. This is why I've always hated
of a hack. This is why I've always hated
off policy learning. They do like a
off policy learning. They do like a
whole bunch of stupid mathematical hacks
whole bunch of stupid mathematical hacks
to make their crappy base algorithm
to make their crappy base algorithm
work.
Like none of the off policy methods get
Like none of the off policy methods get
near the on policy methods until you do
near the on policy methods until you do
all this
That seems screwy to me as well.
Okay. Okay. Well, now I have to test a
Okay. Okay. Well, now I have to test a
whole bunch of things, right?
noxious. Okay.
noxious. Okay.
Uh I guess we just go try to implement
Uh I guess we just go try to implement
it and see what we can do about this. I
it and see what we can do about this. I
mean there are a whole bunch of things I
mean there are a whole bunch of things I
have to do even as prerex though, right?
have to do even as prerex though, right?
So like go back to this one.
Oh, come on. What did I break? This
Oh, come on. What did I break? This
should This one should definitely be
should This one should definitely be
stable, right?
Why is this suddenly not stable? This
Why is this suddenly not stable? This
was the stable one that always worked.
Okay, whatever. There it goes. Um,
hang on
hang on
one sec.
Okay.
HVAC
HVAC
being assessed.
being assessed.
Does not make buzzing noises that drive
Does not make buzzing noises that drive
me insane.
Uh first thing
Uh first thing
target network has to be different right
target network has to be different right
we do encoder we'll go to
we do encoder we'll go to
this
this
target network
target network
try a couple different things
uh Q network first.
So this is this should be the same as
So this is this should be the same as
before,
right?
Act weird.
Act weird.
Uh, is this not the exact same thing as
Uh, is this not the exact same thing as
before?
before?
should be right.
Standard deviation maybe
they do
they do
literally same as before. Is it just
literally same as before. Is it just
different run to run or something
different run to run or something
ridiculous?
Hey, you can get unlucky runs. Cool.
Now we do
Now we do
uh separate target network
still learn
still learn
partially shared
and they're not synchronized.
and they're not synchronized.
In fact, they cannot be synchronized
In fact, they cannot be synchronized
once I make the modifications.
That makes it way worse, right?
Okay. So how is it then that
how is it that supposed to have
how is it that supposed to have
separate
separate
human target nets that you cannot sync
distributional Okay.
Oh, weird.
and this is supposedly
and this is supposedly
one of the main things.
Suppose we can just try it, right?
really man.
Not. We don't need this, do we?
Not. We don't need this, do we?
Hang on. We don't need like this.
Hang on. We don't need like this.
Action actually gets to stay the same.
Action actually gets to stay the same.
Thankfully,
actions do not match action.
and then the Q function.
How do you sample from this?
I do not want
How the hell are these logits? They're
How the hell are these logits? They're
not
not
right.
right.
They're not
obnoxious.
Sample this thing.
Grab random actions for now.
Then
The idea here,
reshape the queue, right?
A
s.
Does this not work?
Oh, okay. It's just the view that's
Oh, okay. It's just the view that's
wrong.
Just we'll hard code everything for now.
Just we'll hard code everything for now.
It's fine.
It's fine.
Okay. So, this is our QA.
Okay. So, this is our QA.
Just ignore
Just ignore
ignore all this stuff for now. Right.
This gives us our target.
And um
right here.
Okay. Okay. So now we have QA
target
and
Okay.
Okay.
Skip this reshape. I
Yep.
Okay.
Okay.
Target
151.
Um,
Um,
I guess it's 51, right?
Okay. And now you get your thing here.
Okay. And now you get your thing here.
And now I think you can just run KL div.
And now I think you can just run KL div.
Yeah,
this is on
this is on
A.
Yeah. And this doesn't learn anything.
Yeah. And this doesn't learn anything.
Lovely.
Nothing at all.
Rock code will check the clean RL code.
Rock code will check the clean RL code.
We'll check everything.
We'll check everything.
Oh, well, of course it doesn't learn
Oh, well, of course it doesn't learn
anything. I forgot the sampling step.
anything. I forgot the sampling step.
Duh.
Duh.
All right, let's see. How do they sample
All right, let's see. How do they sample
in C-51?
They have Yeah, they have the same thing
They have Yeah, they have the same thing
here. Okay.
Get action.
Get action.
What's get action do?
What's get action do?
Probability mass function for action.
Arg max values
gross
self atoms or whatever is
self atoms or whatever is
okay. So you have to do the super gross
okay. So you have to do the super gross
thing to even sample action.
have your atoms.
Then
we have our sample.
Okay. So you do softmax
Okay. So you do softmax
view
151. One,
two.
Two values.
Two.
actions not match action space.
Okay.
do There.
Oh, this should be
There we go.
a little bit.
a little bit.
Not going to exactly call this
Not going to exactly call this
encouraging.
2A gets to be a reasonable range at
2A gets to be a reasonable range at
least.
I think the value is supposed to be
100, right?
Ah, okay.
Ah, okay.
Um,
Um,
it's like
it's like
at least it does something
at least it does something
now. At least it does something.
Probably lots of implementation bugs to
Probably lots of implementation bugs to
fix.
Why is the target so weird? Oh, no. That
Why is the target so weird? Oh, no. That
makes sense.
makes sense.
Okay. So, this is it's something
Okay. So, this is it's something
it's funny that the thing that's worked
it's funny that the thing that's worked
best so far is the original just q val
best so far is the original just q val
formulation
formulation
like q function uh dqn formulation.
This is supposed to be like way better
This is supposed to be like way better
immediately now.
One second.
One second, folks.
Well,
Well,
the uh air is no longer going to be AC
the uh air is no longer going to be AC
is no longer going to be keeping up me
is no longer going to be keeping up me
up at night with horrible rattling.
up at night with horrible rattling.
Nice.
Nice.
We'll buy a couple things for it, but
We'll buy a couple things for it, but
at least for the time being,
at least for the time being,
no more crazy rattling.
What time is it? 4:10.
What time is it? 4:10.
Yeah. Another hour and a halfish.
So, I mean, what's going on with this,
So, I mean, what's going on with this,
right? Like,
is there um
is there um
clean has like something that has
a dagger.
Nothing else with distributional. It's
Nothing else with distributional. It's
just C-51, right?
just C-51, right?
Let's see if I can figure out from here.
Let's see if I can figure out from here.
So you get a Q network. It has madams
So you get a Q network. It has madams
whatever.
whatever.
Wait, actually is there no separate
Wait, actually is there no separate
target?
target?
Wait, what? There's no target network
on
on
Oh, wait. No, it has
network target network with the same
network target network with the same
numb atoms thing.
Do we have an HL G implementation for
Do we have an HL G implementation for
offpaul?
Great. It's a freaking Jax.
and never write any freaking code
where
control tasks as well which is going to
control tasks as well which is going to
be different.
Not like the PyTorch repos are any
Not like the PyTorch repos are any
better half the time.
Has a repo, doesn't it?
Has a repo, doesn't it?
Oh, yeah. This is Lucid Range.
Yeah. But this is not on anything. This
Yeah. But this is not on anything. This
is just uh the loss, right?
is just uh the loss, right?
Yeah, this literally just has the loss.
Okay, so
the C-51 agent is different. Now,
the C-51 agent is different. Now,
how would you apply this?
Don't apply HL goes to keep functions
Don't apply HL goes to keep functions
outputs only to scaler scaler T Target's
C-51.
Oh, it is just uh distributional. Cool.
Oh, it is just uh distributional. Cool.
So, this is the paper, the original
So, this is the paper, the original
distributional paper.
And then this one
And then this one
evidently they do this for
they do this for both the target target
they do this for both the target target
nets.
Then there's the stop progressing paper,
Then there's the stop progressing paper,
right? Is this it?
That's retrace
That's retrace
stop progressing. Yes, it is.
stop progressing. Yes, it is.
So we get classification with a G.
So we get classification with a G.
We get
Okay, so 51
Okay, so 51
get a pretty decent jump with HL Gaus
get a pretty decent jump with HL Gaus
here, right?
here, right?
Actually, MSSE is
Actually, MSSE is
these are actually pretty close and HL
these are actually pretty close and HL
Gaus is the only one that does
Gaus is the only one that does
well. I suppose that directionally here
well. I suppose that directionally here
the MSE gets worse with more updates.
the MSE gets worse with more updates.
G is a lot more stable.
network
by full distributional bellman. Simplify
by full distributional bellman. Simplify
this
apply only to the scalar target not to
apply only to the scalar target not to
the Q network outputs.
the Q network outputs.
Well, how the hell are you getting the
Well, how the hell are you getting the
scalar target then?
target network.
target network.
Oh, I see.
Oh, I see.
So, because here they're doing soft max
So, because here they're doing soft max
times support and then this is going to
times support and then this is going to
give you the mean the mean Q,
give you the mean the mean Q,
right?
right?
So, we can actually do this with a
So, we can actually do this with a
shared network as well.
shared network as well.
Yeah, we can do this with a shared
Yeah, we can do this with a shared
network. I see it.
Do this with a shared net.
Okay. So,
target itself, right?
Okay. So now in order to get
uh the
uh the
two values arg
fine you get your Q values arg max the
fine you get your Q values arg max the
same.
same.
Don't use the target net at all. Right.
Don't use the target net at all. Right.
Yes.
Yes.
Don't use the target net at all here.
Don't use the target net at all here.
Down to here.
We're going to want to take
We're going to want to take
want to take the couple of one, right?
Then we need to do
this.
Okay. And then
learn something.
Makes sense though that we do it
Makes sense though that we do it
this way
this
according to this right it's take the
according to this right it's take the
target network which is batch size
target network which is batch size
num Adams whatever this is the last dim
num Adams whatever this is the last dim
is going to be num Adams
is going to be num Adams
do soft max.
do soft max.
Then you do probs times atoms,
right? Probs times atoms.
Then
Then
3D actions are this
3D actions are this
next key.
This is This is Target.
And you actually take
And you actually take
target probs.
target probs.
Yeah. HL Gauss
on the targets
and the loss is
and the loss is
loss is a KL divergence. Am I stupid?
Y true log one.
Y true log one.
Oh wait, this is different. This is just
Oh wait, this is different. This is just
negative log likelihood, right?
negative log likelihood, right?
Um,
okay.
okay.
This is actually
boss.
Um,
target hunted
probs.
Max
8.
Um,
Um,
something like this.
still way off.
51. It was 51 buckets.
They have this on purple.
They have this on purple.
What they do
negative 100 to 100
negative 100 to 100
uh they do use 101 atoms in right
match exactly
match exactly
10 All right.
Well, at least now we don't have to
Well, at least now we don't have to
worry about parameters, right?
One good thing to do as a sanity test,
One good thing to do as a sanity test,
right?
right?
Let's just do
Let's just do
Q props
off like this, right?
off like this, right?
This can be Q probs.
Okay. And then this one
we can actually do this one here.
we can actually do this one here.
This is Q probs.
Let's see if like this parameterized
Let's see if like this parameterized
version works with an MSE loss.
Uh, wait. What? I do something weird.
there, didn't I?
There's several layers to this freaking
There's several layers to this freaking
thing.
Here's QA
and
Can I not convert this?
I can convert this into
I can convert this into
props, can I?
Oh,
so the value of this is
So you do like this
So you do like this
and you do
like this, right?
like this, right?
Then you do target val.
If you do this
then this is now
target val
is
is
MSC
MSC
QA val and then this is target val like
QA val and then this is target val like
this Okay.
this Okay.
So now this should be
So now this should be
uh discretized. Hang on
target.
Or you need a reap on it.
Yes.
Perfect. So these are whoop target val
one view.
I'm just trying to get something that I
I'm just trying to get something that I
can like work with starting point.
This is
This is
turn type stuff.
Okay. So now we have target val.
Okay. So now we have target val.
have way val.
All right. So now
All right. So now
now we can get target
now we can get target
lab. Come on here
because you don't need this any
need this max.
Okay. So now you have your target
Okay, so you have your values. Yeah, I
Okay, so you have your values. Yeah, I
should be able to define a Q loss,
should be able to define a Q loss,
QA, val, and freaking target.
Comment everything else.
Comment everything else.
And there should have been a long-
And there should have been a long-
winded way of just reconstructing the
winded way of just reconstructing the
original BQN loss
because what I've done here is it's
because what I've done here is it's
discretized
discretized
but then you end up recomputing you end
but then you end up recomputing you end
up uh computing a mean and you just end
up uh computing a mean and you just end
up with original DQN mostly
up with original DQN mostly
slightly different dynamic
and indeed
This is
This is
not super stable, but like
passable.
passable.
It does actually solve the problem at
It does actually solve the problem at
point like passable.
point like passable.
At least at this stage, it's not
At least at this stage, it's not
completely broken, right? It's not like
completely broken, right? It's not like
completely broken.
Now, conceivably we could make some
Now, conceivably we could make some
edits and stabilize this.
edits and stabilize this.
Okay.
Um, so now instead,
how does this make any bloody sense?
how does this make any bloody sense?
Actually
thing is this has got to be Grock lying.
thing is this has got to be Grock lying.
Let me see if I can find it in here. How
Let me see if I can find it in here. How
they do it? Does they have
they do it? Does they have
Okay, this is get action, right?
Listen to Grock.
They actually
They actually
next PMFs.
here. They do.
here. They do.
Oh, wait. Because this is get actions.
Oh, wait. Because this is get actions.
Hang on. Hang on.
Hang on. Hang on.
What do they call get actions?
What do they call get actions?
Get actions.
This returns you
This returns you
um
um
the PMF for this specific action
the PMF for this specific action
and you get it for
and you get it for
target network.
You assumably get it for
You assumably get it for
you get the old PMS
you get the old PMS
from the Q network.
and
and you construct some distributional
and you construct some distributional
loss, right?
So, uh, presumably
So, uh, presumably
presumably I should just be able to use
presumably I should just be able to use
this loss now. No,
which is the soft max
which is the soft max
of the action.
I just have to construct this a little
I just have to construct this a little
bit differently, right?
A
tick off max,
right? because you need to be able to
right? because you need to be able to
take max, right?
take max, right?
So, you take your soft max first
So, you take your soft max first
and then you do
times Adam's sum
times Adam's sum
Okay, so these are your values
Okay, so these are your values
and then you do
and then you do
max idx
uh target
uh target
valid
one.
You do
target.
Oh, wait. Hang on. So, this got to be
Oh, wait. Hang on. So, this got to be
a vowels.
UA
This is just an index select, right?
Oh, yeah. So, this is QA PMFs. Actually,
Oh, yeah. So, this is QA PMFs. Actually,
we're done here. All you need this PMF.
we're done here. All you need this PMF.
You don't need to you don't need it
You don't need to you don't need it
projected onto atoms at all. Okay. But
projected onto atoms at all. Okay. But
for target, you do. And the reason is
for target, you do. And the reason is
that you need to be able to uh select
that you need to be able to uh select
from it. We do target
um emfs
um emfs
soft max
soft max
and then we do
and then we do
they're the probs.
They're not the props are
They're not the props are
val.
val.
Okay. And then we do target val target
Okay. And then we do target val target
index. You take the max
then
target
target
ax.
And we probably
Want this to unbatched
complicated
max.
Okay.
Q A
not
not
let's not view this as a sequence Okay.
Target and QA are good.
Target and QA are good.
Now target has to be
QA PMF
going to be QA
one
this get multip hang on how do you
this get multip hang on how do you
compute
compute
EMF is get action
this already projected.
No, it's literally just the soft max
No, it's literally just the soft max
applied to
applied to
Q values are returned.
This is for sampling. For sampling, you
This is for sampling. For sampling, you
have to do the Q values, right?
Okay, but this is fine. So you get your
Okay, but this is fine. So you get your
target PMF
target PMF
and this is actually target Q,
and this is actually target Q,
right?
Be target Q again. Don't need all of
Be target Q again. Don't need all of
them anymore.
them anymore.
This is target
max of one.
Let's see what this.
Now we have
target
Q
Q
has gotten us the singular
has gotten us the singular
uh values here. But
uh values here. But
index
index
we need target max PMF right
this is 32.
Okay
Okay
target index. Now this gives us a target
target index. Now this gives us a target
max PMF
max PMF
max
max
tape QA PMF tape. Okay, cool. So now we
tape QA PMF tape. Okay, cool. So now we
have probability mass functions for the
have probability mass functions for the
Q function and for the target. We have
Q function and for the target. We have
it for both.
it for both.
So there's a whole bunch of graph down
So there's a whole bunch of graph down
here that we ignore for now.
here that we ignore for now.
And
oh yeah, how the hell do you do this?
oh yeah, how the hell do you do this?
Huh?
weird man.
We got another almost an hour.
Right.
Let me figure this out. Where's the
Okay, so you have right here
Okay, so you have right here
rewards
times Adams. Okay, so you actually
right there's this whole bunch of weird
right there's this whole bunch of weird
projection math
projection math
and this is where uh the HL Gaus
and this is where uh the HL Gaus
comes in instead,
right?
Yes, this is where HL Gaus is supposed
Yes, this is where HL Gaus is supposed
to come in instead.
to come in instead.
This is supposed to be
This is supposed to be
Oh, right. Because you know what it is?
Oh, right. Because you know what it is?
It's the first step of HL Gaus, right?
So, you have
Where is this dude?
Here's your support.
Take your input.
just
it needs to be applied to.
I wish I had like an actually good
I wish I had like an actually good
implementation of this to reference.
implementation of this to reference.
I could try Lucid Drains and like just
I could try Lucid Drains and like just
hope it's good.
I implement literally everything.
Okay. So you can this is what you would
Okay. So you can this is what you would
do, right? So it's transform from logs
do, right? So it's transform from logs
and then it calls transform from probs.
Going to do probs time centers.
That's it.
Forward
transform from logic.
Super confusing the way the stuff is
Super confusing the way the stuff is
written.
I trust Grock to not screw up the
I trust Grock to not screw up the
freaking math if I just want
freaking math if I just want
mathematical description of this thing.
mathematical description of this thing.
Really don't is the problem.
tried to write a thing here, didn't it?
tried to write a thing here, didn't it?
Targets.
Targets.
Scalar TD targets.
Scalar TD targets.
This seems wrong to me that you have
This seems wrong to me that you have
scalar TD targets,
scalar TD targets,
right?
There was another thing in here.
Not this. Where is it?
E51. You apply the full distributional
E51. You apply the full distributional
delman Bellman operator.
you compute the scaler. Okay, so
you compute the scaler. Okay, so
actually this is correct. That's so
actually this is correct. That's so
bizarre though.
Compute the scalar TD targets
Compute the scalar TD targets
and then you transform into the target
and then you transform into the target
distrib using HL Gaus.
That's bizarre, right?
That's bizarre, right?
You compute a full distribution and then
You compute a full distribution and then
you turn it back into a scaler and then
you turn it back into a scaler and then
you repro it into a different target
you repro it into a different target
distribution.
So weird.
Just predict them. Oh, you know why you
Just predict them. Oh, you know why you
don't predict the scaler? It's I know
don't predict the scaler? It's I know
why. It's because uh you can't train
why. It's because uh you can't train
that network, right? So, this makes the
that network, right? So, this makes the
network match the Q network. So, you can
network match the Q network. So, you can
actually train. Okay, so I actually see
actually train. Okay, so I actually see
why why you would have to do it this
why why you would have to do it this
way.
way.
The chat broken? Nobody said anything in
The chat broken? Nobody said anything in
a minute here. I mean, I guess I am kind
a minute here. I mean, I guess I am kind
of just doing crazy algorithm dev at
of just doing crazy algorithm dev at
this point.
this point.
Nope, it's not. There you go. Perfectly
Nope, it's not. There you go. Perfectly
timed A1. It's like one hot encoding of
timed A1. It's like one hot encoding of
a label
a label
into a cross entry.
into a cross entry.
I don't think it is. It's more like
I don't think it is. It's more like
let's say that you had a full
let's say that you had a full
probability distribution as your target.
probability distribution as your target.
Okay. And then you turn it into a one
Okay. And then you turn it into a one
hot and then you turn it back into a
hot and then you turn it back into a
distribution. That's what it's like,
distribution. That's what it's like,
which is crazy.
which is crazy.
But this does make me think that I have
But this does make me think that I have
it done correctly.
I guess the question is like why the
I guess the question is like why the
heck does this do work uh why does this
heck does this do work uh why does this
do better than C-51 that actually keeps
do better than C-51 that actually keeps
the full distribution? I don't know.
Does this even do anything at scale? Or
Does this even do anything at scale? Or
is like or is the entire foundation of
is like or is the entire foundation of
offpaul RL just built on a bunch of dumb
offpaul RL just built on a bunch of dumb
hacks like weren't fully validated? Who
hacks like weren't fully validated? Who
knows?
trying to understand.
trying to understand.
Uh, that makes two of us.
What the heck did they do? They just
What the heck did they do? They just
wrote a ton of math for something that
wrote a ton of math for something that
always ends up being very, very silly.
But let me see. So I have
But let me see. So I have
wait I can actually compute this now
wait I can actually compute this now
though right because
so this is I have my target Q right
so here this is target Q
so here this is target Q
and then this is
and
okay. So if I do this right, I can just
okay. So if I do this right, I can just
do Q
do Q
then this is going to be target
then this is going to be target
and I can just compute this loss and
and I can just compute this loss and
then basically this is
then basically this is
this should give me
this should give me
like the mean squared error loss version
like the mean squared error loss version
of this. Hang on.
So this is not the full power of the
So this is not the full power of the
algorithm at the point at this point,
algorithm at the point at this point,
right? This is supposed to just be
right? This is supposed to just be
project everything and then you kind of
project everything and then you kind of
project it back so it's the same bloody
Okay, this is your queue.
Okay. So now
stable.
stable.
Okay. So this is like reasonably decent
Okay. So this is like reasonably decent
as it flops a bunch.
course and it like flops a bunch at the
course and it like flops a bunch at the
end and totally screws up. But that
end and totally screws up. But that
actually did solve the task. Okay, so
actually did solve the task. Okay, so
what I did here is I did all of the
what I did here is I did all of the
probabilistic I did all the projections.
Well, I did the not the HL gas project.
Well, I did the not the HL gas project.
I did the I computed the PMF and then I
I did the I computed the PMF and then I
projected it back into Q values and then
projected it back into Q values and then
just did MSE.
just did MSE.
Okay.
And now next
what we do is the full HL G
for this.
Take
for this. We go here.
A PMFS.
A M apps
A M apps
2 A U
2 A U
512 64
512 64
and we do
target equals HL G Target
target
one
do hl g on our target
put it back into
shape that we understand
shape that we understand
this.
Okay,
then
then
target.
The Q probs should have just been
soft max
should be something like this.
[Music]
Plug this in real quick.
What is an HL Gaus?
What is an HL Gaus?
It's a lo it's a distributional loss
It's a lo it's a distributional loss
function or it's a dist it's a
function or it's a dist it's a
transformation that's used to create a
transformation that's used to create a
distributional loss function rather
distributional loss function rather
I guess is the simplest way of putting
I guess is the simplest way of putting
it. Um the really simple way of putting
it. Um the really simple way of putting
it is instead of estimating values or Q
it is instead of estimating values or Q
functions like as continuous numbers you
functions like as continuous numbers you
bucket them a bunch and then you write
bucket them a bunch and then you write
loss function over the bucketed values
loss function over the bucketed values
instead.
instead.
And this is supposed to do something,
And this is supposed to do something,
but um
but um
we're not exactly fully seeing it yet.
we're not exactly fully seeing it yet.
Heavy lift G.
Heavy lift G.
Oh, I prefer that.
Oh, I prefer that.
Yeah, the heavy lifting function. Good.
Certainly feels like heavy lifting
Certainly feels like heavy lifting
trying to freaking get it to work.
It's
It's
Oh, I am realizing that I did not shift
Oh, I am realizing that I did not shift
them by one.
This at least
really doesn't work super well, huh?
Uh,
Uh,
yeah, I think that's something
yeah, I think that's something
different.
I think that's something different.
It's uh it's this. The heck does it even
It's uh it's this. The heck does it even
stand for?
stand for?
I don't actually know what it stands
I don't actually know what it stands
for.
It's from this paper though.
are better at class.
Try again with I don't know what mids
Try again with I don't know what mids
is.
I don't know what mids is ora. I don't
I don't know what mids is ora. I don't
know what either of those programs are.
I've tried so many different variations
I've tried so many different variations
on this and it doesn't seem to work
on this and it doesn't seem to work
better. Online mask.
What?
I didn't know about that.
I didn't know about that.
particularly for online programs. I
particularly for online programs. I
don't know about that at all.
It's hard for me to say because like I
It's hard for me to say because like I
mean I I'm not the type of person who
mean I I'm not the type of person who
like will look at credentials and hire
like will look at credentials and hire
anybody based on that because I know
anybody based on that because I know
that that like the credentials never
that that like the credentials never
tell the full story.
What am I doing wrong here?
We go to unshared.
We go to unshared.
Let's go to unshared just to be
Let's go to unshared just to be
absolutely sure,
absolutely sure,
right? Because that technically could be
right? Because that technically could be
a thing.
That would suck if that were the uh the
That would suck if that were the uh the
problem,
problem,
but we will see.
Go to unshared here, right?
There's Yeah, that's something totally
There's Yeah, that's something totally
different.
different.
Yellow is something totally different.
Yellow is something totally different.
Just an activation function. has nothing
Just an activation function. has nothing
to do with what I'm working on. I'm
to do with what I'm working on. I'm
using it, but it has nothing to do with
using it, but it has nothing to do with
what I'm working on.
Okay, there's your target net.
So, here's your target.
So, here's your target.
Got the data copy in.
Got the data copy in.
Now let's do
already have this bit
I'd say coursework really shouldn't be
I'd say coursework really shouldn't be
that hard, but like any of the courses
that hard, but like any of the courses
are basically designed to screw with you
are basically designed to screw with you
rather than to teach you things.
Ah, okay. Hang on.
Well, it's super jumpy, but like
Well, it's super jumpy, but like
we're actually seeing something there
we're actually seeing something there
right
right
now with it unshared.
I suppose the number of updates needs to
I suppose the number of updates needs to
be tuned and such.
There's a huge amount of variability in
Total mini badge.
You know how people can make um like
You know how people can make um like
programs that are just dramatically more
programs that are just dramatically more
complicated than they need to be for no
complicated than they need to be for no
apparent reason? You can do the same
apparent reason? You can do the same
thing with math. It turns out
thing with math. It turns out
the exact same thing with math.
Okay, there's a consistent update
Okay, there's a consistent update
schedule.
Okay, that's way better, right?
It's like I think probably whenever the
It's like I think probably whenever the
target gets hit it like crushes it for a
target gets hit it like crushes it for a
second.
second.
But this is chill.
But this is chill.
That actually made a big difference.
That actually made a big difference.
Okay, so we actually kind of have this
Okay, so we actually kind of have this
working then, right?
working then, right?
That's HL Gausmented.
Um, code's a mess.
Um, code's a mess.
But we do have distributional RL working
plus.
I like C because I like simple things. I
I like C because I like simple things. I
actually I really hate puzzles, you
actually I really hate puzzles, you
know. I hate like clever things. I like
know. I hate like clever things. I like
simple things that work.
simple things that work.
I like it when I can take very hard
I like it when I can take very hard
problems.
problems.
They're like super obnoxious and the
They're like super obnoxious and the
solutions are almost not even worth
solutions are almost not even worth
their weight and like to just cut them
their weight and like to just cut them
all out and do simple things that work.
all out and do simple things that work.
That's what I like.
I'd rather be useful than clever.
Oh, that's the thing that everyone has
Oh, that's the thing that everyone has
to decide for them. Rather feel clever
to decide for them. Rather feel clever
or would you rather be useful?
They're often directly at odds.
Because in fact the least clever
Because in fact the least clever
solution typically among all of them the
solution typically among all of them the
best
best
the easiest to freaking follow and
the easiest to freaking follow and
extend.
I will be very very happy if at the end
I will be very very happy if at the end
of the day I run all the experiments I
of the day I run all the experiments I
test all these methods and I find that
test all these methods and I find that
most of the complicated things are a
most of the complicated things are a
waste of time. I just have this simple
waste of time. I just have this simple
algorithm that like is super fast, it's
algorithm that like is super fast, it's
very easy and just solves all the
very easy and just solves all the
problem. RL becomes a mature field a
problem. RL becomes a mature field a
high performance implementation of that
high performance implementation of that
with extensive validation.
with extensive validation.
Pretty much any problem you can want
Pretty much any problem you can want
it'll solve better than about any of the
it'll solve better than about any of the
other existing algorithms.
other existing algorithms.
No real edge cases works. That is the
No real edge cases works. That is the
goal.
goal.
That is what I would like to see happen.
Well, this gives us um this does give us
Well, this gives us um this does give us
our distributional RL.
We had to do unshared networks to make
We had to do unshared networks to make
it work, but we did do it.
So I guess tomorrow would be um well
So I guess tomorrow would be um well
tomorrow I have other stuff but the next
tomorrow I have other stuff but the next
big chunk on this will be retrace,
right?
And then once we have once we have this
And then once we have once we have this
into retrace,
into retrace,
pretty much this will be at
pretty much this will be at
this will be at the point where it has
this will be at the point where it has
enough of the bells and whistles that
enough of the bells and whistles that
like a clean version of this should
like a clean version of this should
already be enough,
already be enough,
right? like that. I would expect
right? like that. I would expect
um
um
HL Gaus version of this with retrace to
HL Gaus version of this with retrace to
be competitive with our existing method
be competitive with our existing method
if it's any good.
if it's any good.
Not any good,
Not any good,
different story.
We could technically also mess with the
We could technically also mess with the
replay buffer stuff.
We'll get to that later.
different.
different.
Oh jeez.
It's a good start. It's a good starting
It's a good start. It's a good starting
point. Um
point. Um
else I can do
really
really
kind of the
kind of the
main main thing for now is set up.
main main thing for now is set up.
Do I need to do retrace before anything
Do I need to do retrace before anything
else?
Probably do.
Probably do.
I probably do need retrace, right?
I probably do need retrace, right?
Like otherwise you have no advantage
Like otherwise you have no advantage
function basically.
function basically.
You're doing like one step bootstrap.
You're doing like one step bootstrap.
Yeah. Retrace will be next session.
Yeah. Yeah, I mean the the best people
Yeah. Yeah, I mean the the best people
around here are just the people stuck
around here are just the people stuck
around and stuff properly.
around and stuff properly.
I haven't even asked anybody that I've
I haven't even asked anybody that I've
brought on for contracts about their
brought on for contracts about their
credentials. I have people emailing me
credentials. I have people emailing me
resumes all the time and it's like what
resumes all the time and it's like what
am I supposed to do with that?
Yeah.
Yeah.
Okay. So, I got to go get ready for uh
Okay. So, I got to go get ready for uh
for dinner. I'm meeting somebody for
for dinner. I'm meeting somebody for
dinner today.
dinner today.
Tomorrow as well. Tomorrow I have um
Tomorrow as well. Tomorrow I have um
most of the day I have
most of the day I have
uh I've got a meeting to go to and I got
uh I've got a meeting to go to and I got
to set stuff up for that. But
I will be working on either tomorrow or
I will be working on either tomorrow or
Friday most likely. Uh we're going to
Friday most likely. Uh we're going to
figure out retrace. I'm going to do a
figure out retrace. I'm going to do a
little bit of reading offline on retrace
little bit of reading offline on retrace
to see what I can figure out.
to see what I can figure out.
Very fancy method. I bet you that it uh
Very fancy method. I bet you that it uh
it boils down to like not that much
it boils down to like not that much
this paper here
and mod game fun.
and mod game fun.
That's fine. Generally people should at
That's fine. Generally people should at
least at some point go through an intro
least at some point go through an intro
course so you know like what a hashmap
course so you know like what a hashmap
is but um
is but um
also basics of how your computer works,
also basics of how your computer works,
what an operating system does
what an operating system does
at least a little bit but other than
at least a little bit but other than
that yeah anyways
that yeah anyways
uh thank you folks. I will be back
uh thank you folks. I will be back
probably tomorrow. Thank you for tuning
probably tomorrow. Thank you for tuning
in. Uh, it's going to be a few days of
in. Uh, it's going to be a few days of
this I think to like get an off policy
this I think to like get an off policy
thing in. But in my mind, I really want
thing in. But in my mind, I really want
to give off policy a proper try. Like I
to give off policy a proper try. Like I
want to be very very confident if off
want to be very very confident if off
policy stuff doesn't work that it's not
policy stuff doesn't work that it's not
because I've screwed it up and I want a
because I've screwed it up and I want a
clear-cut reason as to why uh it doesn't
clear-cut reason as to why uh it doesn't
work.
work.
But we're going to push the experiments
But we're going to push the experiments
quite far there.
quite far there.
So, thank you folks. Back tomorrow.

Kind: captions
Language: en
Okay,
Okay,
should be um back live here.
should be um back live here.
Hello.
11:30. A little tidbit late today.
11:30. A little tidbit late today.
A lot of exercise in the morning,
but I've had some good time to think and
but I've had some good time to think and
kept my head very clear. I know exactly
kept my head very clear. I know exactly
what we're going to do today.
what we're going to do today.
Spencer has been working on this HL Gaus
Spencer has been working on this HL Gaus
implementation.
Let's actually see. Whoops. Do it this
Let's actually see. Whoops. Do it this
way
to be sure.
Yeah, this is uh this is better than
Yeah, this is uh this is better than
before.
before.
This is slightly better than before.
This is slightly better than before.
He's got this HL Gaus implementation
He's got this HL Gaus implementation
that's like well, it's hard to really
that's like well, it's hard to really
say with any real significance, but it's
say with any real significance, but it's
at least as good as before. Um,
at least as good as before. Um,
and what we're going to do is because HL
and what we're going to do is because HL
Gaus is also one of the main annoying to
Gaus is also one of the main annoying to
implement components that really matters
implement components that really matters
for off policy,
for off policy,
we're going to see what happens when we
we're going to see what happens when we
uh we use this with off policy method.
Now the thing I don't know about
this implemented.
Okay. So it's the same value function as
Okay. So it's the same value function as
before, right? A singular output.
singular output, right?
How do you handle continuous actions
How do you handle continuous actions
with um
well, do we just do it for con uh for
well, do we just do it for con uh for
discrete first?
discrete first?
I think that's what we do, right?
Honestly, we can kind of just repurpose
Honestly, we can kind of just repurpose
the decoder. No,
and we just train it uh a bit
and we just train it uh a bit
differently. Decoder
differently. Decoder
is going to represent something else.
Okay. So, we have log jets.
Then we do
and we have to sample
and we have to sample
from logit's epsilon gradient now Okay.
Something like this. Right.
This should be your off pulse sample, I
This should be your off pulse sample, I
believe. See if this works.
Enter is not
wants me to do this.
Okay.
Okay.
Well, that seems weird.
Okay, I see some TR in there.
Hello, welcome. Which RL objective are
Hello, welcome. Which RL objective are
you implementing? Uh, I'm trying to make
you implementing? Uh, I'm trying to make
an off policy version of pop for lib.
an off policy version of pop for lib.
So, the basic thing is just getting two
So, the basic thing is just getting two
functions of some type into the trainer.
functions of some type into the trainer.
We have a HL Gaus implementation which
We have a HL Gaus implementation which
from my research it seems like this is
from my research it seems like this is
the most uh crucial trick to have in
the most uh crucial trick to have in
place and we actually already kind of
place and we actually already kind of
have a lot of the other ones that are
have a lot of the other ones that are
required for off policy to work. So
required for off policy to work. So
pretty much the goal is to see if we can
pretty much the goal is to see if we can
get some sort of off policy
get some sort of off policy
objective that matches the performance
objective that matches the performance
of our on policy implementation.
It's probably going to end up looking
It's probably going to end up looking
something like rainbow but with better
something like rainbow but with better
distributional and without a couple of
distributional and without a couple of
the tricks that don't actually seem to
the tricks that don't actually seem to
make a friends.
Oh, so it shouldn't be log prop. It
Oh, so it shouldn't be log prop. It
should be logs.
And I guess it's all the same initial
And I guess it's all the same initial
state. So it's all going to be the same
state. So it's all going to be the same
action. That's fine.
action. That's fine.
Okay. But this gets us our action,
Okay. But this gets us our action,
right?
And that should be all we have to store
And that should be all we have to store
for Yes.
There's HL G in here.
Yeah, we have a jo
value logit. Ah,
value logit. Ah,
do value logits.
do value logits.
Did he actually modify this?
kind of dead
kind of dead
chill.
Okay, so he actually did do this.
Hello, Kyoko.
Hello, Kyoko.
Are you done with imitation learning?
Are you done with imitation learning?
No. Um what this is is I would call this
No. Um what this is is I would call this
somewhat of a continuation. So we're
somewhat of a continuation. So we're
doing off policy work. Now the thing
doing off policy work. Now the thing
that I think is important from the
that I think is important from the
imitation learning right is that there
imitation learning right is that there
is a
is a
uh it is possible to write a algorithm
uh it is possible to write a algorithm
that behaves very qualitatively
that behaves very qualitatively
differently and in places much much
differently and in places much much
better than our current state-of-the-art
better than our current state-of-the-art
on policy implementation.
on policy implementation.
So uh and it does that by reusing the
So uh and it does that by reusing the
highest quality old data. Right?
highest quality old data. Right?
So this is why I'm now playing around
So this is why I'm now playing around
with off policy because given that we
with off policy because given that we
should be able to come up with an off
should be able to come up with an off
policy algorithm that is very close to
policy algorithm that is very close to
our on policy algorithm but lets us
our on policy algorithm but lets us
actually use an experience buffer.
actually use an experience buffer.
That is going to be the objective.
That is going to be the objective.
And the result of this is that the hard
And the result of this is that the hard
exploration tasks like neural MMO 3 um
exploration tasks like neural MMO 3 um
we should see
we should see
we should see progress happen very
we should see progress happen very
quickly there because you when you get a
quickly there because you when you get a
few good samples it will actually keep
few good samples it will actually keep
learning on those.
The cool thing is puffer lilib's kind of
The cool thing is puffer lilib's kind of
already set up such that many we have a
already set up such that many we have a
lot of the uh the tricks that are used
lot of the uh the tricks that are used
in off policy learning already in an on
in off policy learning already in an on
policy setting. So it shouldn't be a
policy setting. So it shouldn't be a
huge modification. Um the main thing
huge modification. Um the main thing
that's confusing is like how we're going
that's confusing is like how we're going
to change up advantage function and such
to change up advantage function and such
with uh discretized
with uh discretized
Q function that has hlg in the mix.
welcome something.
welcome something.
I do think that it should be possible to
I do think that it should be possible to
make something like this work. I don't
make something like this work. I don't
know how close it's going to be to
know how close it's going to be to
existing off policy methods.
existing off policy methods.
Any guideline you're following? Well,
Any guideline you're following? Well,
I've read like I've been reading a ton
I've read like I've been reading a ton
of papers lately and so at this point
of papers lately and so at this point
I'm trusting I'll reference some papers,
I'm trusting I'll reference some papers,
but I'm mostly trusting my own best
but I'm mostly trusting my own best
judgment.
judgment.
Play buffers and puffer. Here's the
Play buffers and puffer. Here's the
thing, Anchel. I implemented them
thing, Anchel. I implemented them
in an early version of 3.0. They didn't
in an early version of 3.0. They didn't
work because of the on polic because
work because of the on polic because
puffer's on policy.
I tried to do exactly this and it didn't
I tried to do exactly this and it didn't
work at all. So now we're going to try
work at all. So now we're going to try
to actually go to off policy and see if
to actually go to off policy and see if
we can get it to work with this
value log value.
Ah okay. So there actually is.
Yeah. Yeah. The value function actually
Yeah. Yeah. The value function actually
works here, doesn't it?
works here, doesn't it?
Logit's value value.
Well, don't go getting excited to try a
Well, don't go getting excited to try a
thing yet when uh it's not I mean,
thing yet when uh it's not I mean,
research is like the most fiddly bits
research is like the most fiddly bits
you can possibly have to deal with,
you can possibly have to deal with,
right?
right?
Actually,
let's let's just make this new
new value log props.
It's an observation help. Yeah, it
It's an observation help. Yeah, it
stores past data. So the idea is you
stores past data. So the idea is you
should be able to reuse past data
should be able to reuse past data
in learning
way to do this.
Guess we just replace the um
I guess right here, right? We have
damn uh function.
squared error, right? On what did they
squared error, right? On what did they
define y i is
Yeah. So it's R plus
Yeah. So it's R plus
Q of next state.
So I should actually be able to just do
So I should actually be able to just do
since I have this over segments, right?
since I have this over segments, right?
And this is done.
The tricky bit and how to reformulate
The tricky bit and how to reformulate
Thanks.
What is the Yi? A y i is
new to RL and tried Tetris with
new to RL and tried Tetris with
gymnasium plus poip for po did you try
gymnasium plus poip for po did you try
our Tetris? We actually have a version
our Tetris? We actually have a version
of Tetris that's very fast.
of Tetris that's very fast.
This is the policy that it just trains
This is the policy that it just trains
out of the box on it.
out of the box on it.
Buffer li for games only. Nope, not at
Buffer li for games only. Nope, not at
all. Here's a driving end.
all. Here's a driving end.
We have a driving in with real world map
We have a driving in with real world map
data. Here we have a drone flying
data. Here we have a drone flying
environment. They're all sorts of
environment. They're all sorts of
synthetic tasks as well, not for games
synthetic tasks as well, not for games
only.
only.
Games are just really, really good for
Games are just really, really good for
research because they're very
research because they're very
interpretable.
So naturally, a lot of research
So naturally, a lot of research
environments that we use for improving
environments that we use for improving
our core algorithms and evaluating
our core algorithms and evaluating
methods are going to be games.
methods are going to be games.
They're frankly much better than control
They're frankly much better than control
tasks or core research.
Gamma
Where can I find code example for Tetris
Where can I find code example for Tetris
M in Ocean? It's in Ocean. Like if you
M in Ocean? It's in Ocean. Like if you
check the docs, right, we have
check the docs, right, we have
walkthroughs on how to build
walkthroughs on how to build
environments and like all of our custom
environments and like all of our custom
M's are just in like puffer lilib ocean
M's are just in like puffer lilib ocean
and there's a folder for each one. The
and there's a folder for each one. The
code is the actual core source is one
code is the actual core source is one
file for each environment. There like
file for each environment. There like
some bindings to Python and stuff, but
some bindings to Python and stuff, but
all the logic is just in theh.
I can promise you that our version of
I can promise you that our version of
the Tetris is going to train at least
the Tetris is going to train at least
100 times faster than whatever you've
100 times faster than whatever you've
been messing around with in gymnasium,
been messing around with in gymnasium,
at least on decent hardware.
Huffer very fast.
Good H.
Okay, that's totally the wrong.
Oh, cuz this ended up having an extra
Oh, cuz this ended up having an extra
dimension somehow.
We just do
Okay. So, this is correct for actions.
Okay. So, this is correct for actions.
[Music]
I don't like that it's all the same.
Um.
Um.
Oh, because the stupid of course.
Oh, because the stupid of course.
Yeah. Hang on.
Now we can't do it twice.
We're going to do this super hacky at
We're going to do this super hacky at
the start, but this is not fast code.
Okay, this is better. 5% random choice.
Okay, this is better. 5% random choice.
That's a long greedy.
Ah.
Wait, hang on. It's Q of actions.
Um,
Um,
we need to view
Is this correct place? What do you mean,
Is this correct place? What do you mean,
Rishi?
If you're trying to post links, YouTube
If you're trying to post links, YouTube
will stop you from doing that.
will stop you from doing that.
It's in pufferlib/ Ocean.
Yeah, that's not right.
No, not at all.
Oh,
the reward.
It's reward plus
It's reward plus
I think that it's the current reward.
I think it's this way.
And this should be QA.
Okay,
Okay,
that is a correct loss. or at least it's
that is a correct loss. or at least it's
a loss that doesn't break.
Interesting.
Well, I mean this is our initial
I don't think it's saving.
I don't think it's saving.
This is our initial implementation.
Oh, Nan's out. I see. Cool.
Oh, Nan's out. I see. Cool.
Well, this is our uh our basic Q
Well, this is our uh our basic Q
function, right?
Do I spend time now
I spend time trying to get this to learn
able to do it on break, right?
able to do it on break, right?
Anything rather.
Anything rather.
I just wanted to get something above the
I just wanted to get something above the
uh the noob reward, which is right
uh the noob reward, which is right
there.
there.
Let me make sure I've done this
Let me make sure I've done this
correctly.
Well, that's probably way too high
Well, that's probably way too high
epsilon graded, isn't it?
H.
Well, the policy full determinizes
Well, the policy full determinizes
there, doesn't it?
Hard to say.
See
Dang. Okay.
Not very familiar with puffer labs and
Not very familiar with puffer labs and
design for typical RL agents have plans
design for typical RL agents have plans
for RL with LLM. No, we do everything
for RL with LLM. No, we do everything
except LLMs.
It's the typical RL agents, but
It's the typical RL agents, but
everything that we have is at least 100
everything that we have is at least 100
times faster,
times faster,
sometimes a thousand times faster. Very
sometimes a thousand times faster. Very
very high performance training of small
very high performance training of small
models.
Where the heck did they put their
Where the heck did they put their
hypers?
I have a table.
Okay. So, it's actually pretty high.
I wouldn't have expected that
using
We're probably still just looking at
We're probably still just looking at
ways. Hey Georgie.
What's the strategy here?
What's the strategy here?
Sace. Now SACE is super freaking slow
Sace. Now SACE is super freaking slow
and I honestly haven't seen good
and I honestly haven't seen good
compare. Like SACE doesn't even compare
compare. Like SACE doesn't even compare
favorably. It's the goal is to get Q
favorably. It's the goal is to get Q
functions of some variety into an
functions of some variety into an
algorithm that is as close as possible
algorithm that is as close as possible
to our current one.
Basically, we want to do our exactly
Basically, we want to do our exactly
what we're doing now, but we want to do
what we're doing now, but we want to do
it off policy so that we can actually
it off policy so that we can actually
have a replay buffer.
have a replay buffer.
You published some paper? Yes. Yes, I
You published some paper? Yes. Yes, I
did. Um,
did. Um,
it will be on archive shortly, but we
it will be on archive shortly, but we
won an award for it.
dang thing.
Why does it not show my freaking posts?
Why does it not show my freaking posts?
What? It literally doesn't show my
What? It literally doesn't show my
posts. You're signed out. Hang on.
posts. You're signed out. Hang on.
Let me go get a separate window on my
Let me go get a separate window on my
personal
Okay, there we go. So,
Okay, there we go. So,
my whole talk is right here. This is the
my whole talk is right here. This is the
talk that I gave on uh it's like a seven
talk that I gave on uh it's like a seven
minute talk at RLC and I put a version
minute talk at RLC and I put a version
of it here on X. It's also on YouTube.
of it here on X. It's also on YouTube.
Oh, is it already live?
Um,
this is RLJ.
this is RLJ.
So, I guess it's on here actually, but I
So, I guess it's on here actually, but I
archived it yesterday
and I don't think it's live on archive
and I don't think it's live on archive
yet.
yet.
So, it's already here, but should be
So, it's already here, but should be
live on archive soon.
AFBRR. We're doing offpaul work or
AFBRR. We're doing offpaul work or
attempting to.
attempting to.
The problem with um offpaul methods is
The problem with um offpaul methods is
like they really don't work until you
like they really don't work until you
add a whole bunch of tricks. So, it's
add a whole bunch of tricks. So, it's
actually hard to know if I've made a
actually hard to know if I've made a
mistake or not until we have a bunch of
mistake or not until we have a bunch of
tricks on it.
Let me take one more look at the uh the
Let me take one more look at the uh the
function, the loss function.
Oh, hang on.
No, because this is done implicitly,
No, because this is done implicitly,
right? The discounting is done
right? The discounting is done
implicitly.
RT. Yeah. Yeah. Yeah. So,
RT. Yeah. Yeah. Yeah. So,
the loss is just they define yi.
the loss is just they define yi.
This is going to be R +
R + gamma max Q.
R + gamma max Q.
Believe I have the correct reward right
Believe I have the correct reward right
because it's observation
because it's observation
and then the reward observation action
and then the reward observation action
then you get the reward. Yeah, I think
then you get the reward. Yeah, I think
that the way I have my buffer set up
that the way I have my buffer set up
this is correct.
drive with a bunch of papers.
Yeah, you can do that.
Yeah, you can do that.
Um,
max.
Yes. You take maximum Q
Yes. You take maximum Q
and you shift over by one
and you shift over by one
and then you take Q of the current
and then you take Q of the current
action.
action.
So unless I've messed up my tensor uh my
So unless I've messed up my tensor uh my
tensor indexing somewhere, this should
tensor indexing somewhere, this should
be correct.
be correct.
Yeah, this should be correct.
Okay. So then why why does this not
Okay. So then why why does this not
work? Um hyperparameters, tensor
work? Um hyperparameters, tensor
indexing problems or base DQN just being
indexing problems or base DQN just being
like horrible horrible bad
like horrible horrible bad
because I think that's all I'm doing,
because I think that's all I'm doing,
right? Is I've just basically hacked DQN
right? Is I've just basically hacked DQN
into this
into this
which obviously is not going to do very
which obviously is not going to do very
much.
Oh, and then wait, do you do
you do take the action that was
you do take the action that was
previously taken? Yes.
previously taken? Yes.
Now, this is fine.
Okay. So, things that I'm going to have
Okay. So, things that I'm going to have
to add to this, right?
to add to this, right?
I'm going to need to have the multi-step
I'm going to need to have the multi-step
bootstrap, obviously.
Then, missing the missing the advantage
Then, missing the missing the advantage
estimation
estimation
is the real big one.
is the real big one.
Obviously the H hl gaus is the other
Obviously the H hl gaus is the other
really big one here.
really big one here.
I should be able to get this to learn
I should be able to get this to learn
something though I think. Let me see if
something though I think. Let me see if
I can play with this a little bit.
I can play with this a little bit.
Um
actually let's go check clean RL's
actually let's go check clean RL's
implementation
and I start with this.
They should have um
They should have um
a super basic
a super basic
Yeah, gotta love Costa.
Yeah, gotta love Costa.
200 line
100 line DQN
exploration fraction ending epsilon.
They actually do kind of eneal it, don't
They actually do kind of eneal it, don't
they?
Yeah, they do kind of eneal it.
these target networks, too.
these target networks, too.
Oh, yeah. What did I miss? I probably
Oh, yeah. What did I miss? I probably
missed something completely there,
missed something completely there,
didn't I?
didn't I?
I haven't done uh
I haven't done uh
I mean, I'll see it in a second. Hang
I mean, I'll see it in a second. Hang
on. Let me at least get epsilon to be
on. Let me at least get epsilon to be
reasonable.
reasonable.
This has to be a yield. So, it would
This has to be a yield. So, it would
probably be something like
two
two
times. No, wait. Not minus 2 times
two.
I think this is right.
Yeah. So after half the steps roughly
loads double Q. You don't need double Q.
loads double Q. You don't need double Q.
It like the initial one doesn't have
It like the initial one doesn't have
double Q and double Q doesn't do
double Q and double Q doesn't do
anything on rainbow and like the more
anything on rainbow and like the more
advanced methods.
advanced methods.
I don't think we need to care too much
I don't think we need to care too much
about that.
Does the original DQN have a separate
Does the original DQN have a separate
target network? I didn't think it did
cuz like the double you don't need
cuz like the double you don't need
double Q. If you look at Rainbow and
double Q. If you look at Rainbow and
like all the different tricks and the
like all the different tricks and the
ablations, like the double Q doesn't
ablations, like the double Q doesn't
actually do anything.
So, that's one that we don't need and
So, that's one that we don't need and
it's expensive as well.
Okay. Now, let's look at clean arrows
Okay. Now, let's look at clean arrows
implementation.
That's any of Yes.
Oh, wait. They do have a separate they
Oh, wait. They do have a separate they
have a separate target already in the
have a separate target already in the
initial uh DQN.
Interesting.
Interesting.
They're fully separate as well.
I forgot I forgot that like the original
I forgot I forgot that like the original
I didn't think there was a separate
I didn't think there was a separate
target network in the original, but I
target network in the original, but I
guess not. I guess there is.
Oh yeah, we're also
then
yeah, I think that we just like go off
yeah, I think that we just like go off
of this and we try to match this as
of this and we try to match this as
close as possible and then we start
close as possible and then we start
adding all of our stuff back in.
kind of modify in place
kind of modify in place
but off policy.
but off policy.
Yeah. Weren't you you were implementing
Yeah. Weren't you you were implementing
some stuff, right?
What did you post?
You you have this nod and puffer lid
You you have this nod and puffer lid
though, right? You have like a
though, right? You have like a
standalone thing,
standalone thing,
don't you?
H guys, help at all. That's what we're
H guys, help at all. That's what we're
going to do right now, Spencer. So, I'm
going to do right now, Spencer. So, I'm
getting like a basic DQN into Puffer
getting like a basic DQN into Puffer
with like reasonable PF at first. And uh
with like reasonable PF at first. And uh
like we're going to need HL Gauss for it
like we're going to need HL Gauss for it
to work at all.
This I open this thing.
Of course, it opens VS Code.
Okay, you did kind of take our buffer
Okay, you did kind of take our buffer
and structure. I see.
Okay, thank you. So, I will put this on
Okay, thank you. So, I will put this on
my other screen then as a additional
my other screen then as a additional
reference for this.
Did you do experiments like to see what
Did you do experiments like to see what
mattered?
is like I was looking through I
is like I was looking through I
basically went and I cross-cheed a bunch
basically went and I cross-cheed a bunch
of off policy papers and it seems like
of off policy papers and it seems like
the most important stuff right is having
the most important stuff right is having
some sort of prioritized replay and then
some sort of prioritized replay and then
having um well that's second actually.
having um well that's second actually.
The most important thing is having some
The most important thing is having some
sort of distributional
sort of distributional
uh some sort of distributional objective
uh some sort of distributional objective
so you're not doing quadratic loss
and then like prioritize replay and a
and then like prioritize replay and a
few other things. Multi-step
few other things. Multi-step
bootstrapping.
I have this now on my other monitor so
I have this now on my other monitor so
we're going to cross reference both. I
we're going to cross reference both. I
got to get the basic thing into a proper
got to get the basic thing into a proper
puffer first.
Oh shoot. Yeah, we can't even do it.
Oh shoot. Yeah, we can't even do it.
Wait, we can't even do it fully
Wait, we can't even do it fully
unshared, right?
Does anybody know if you use a shared
Does anybody know if you use a shared
backbone for Q and target, does it
backbone for Q and target, does it
totally screw everything up or you kind
totally screw everything up or you kind
of Okay.
And I just do self.q
And I just do self.q
target.
Not like well it might be similar.
Not like well it might be similar.
There.
Can I just do this?
Should be a copy.
Yeah, but um
Yeah, but um
A1 um the way that Puffer is set up,
A1 um the way that Puffer is set up,
there's an LSTM in the backbone. So like
you're not going to we'd have to do like
you're not going to we'd have to do like
two separate LSTMs.
I'm going to just share the back. I'm
I'm going to just share the back. I'm
going to share the main net and just
going to share the main net and just
have two separate output heads for
have two separate output heads for
starters. We'll see if that breaks
starters. We'll see if that breaks
Everything.
Get out of here, bot.
Really trying to bot on an RL stream.
All right. So
I think what we do is we just like start
I think what we do is we just like start
cutting crap
you
and let me see how cleaner does
Super. This is like a really good read.
Super. This is like a really good read.
Way easier to follow.
Wait, does this not
Wait, does this not
get the action from
Oh, here you go.
Oh, here you go.
Yeah. So, this is the actions come from
Yeah. So, this is the actions come from
the Q network.
the Q network.
This is just going to be Q.
So, is this not considered the double
So, is this not considered the double
DQN
DQN
or is this
or is this
is this not considered double uh double
is this not considered double uh double
DQN?
DQN?
That's something different. I didn't
That's something different. I didn't
look into the details. It's been like
look into the details. It's been like
several years since I've looked at that.
several years since I've looked at that.
I didn't look into the details because I
I didn't look into the details because I
just saw that um uh in the more recent
just saw that um uh in the more recent
methods having double doesn't actually
methods having double doesn't actually
buy you much.
I kind of just ignored it.
Have Q.
Have Q.
This comes out correctly.
This crap
minimum of values.
So separate from having Q and target,
So separate from having Q and target,
right? This would introduce a third one.
right? This would introduce a third one.
Have you tried Muan for RL training?
Have you tried Muan for RL training?
Yes, we use Muan in all of our
Yes, we use Muan in all of our
baselines. It is much better. This was
baselines. It is much better. This was
one of the main additions to Puffer 3.
Okay, so that's probably the idea is
Okay, so that's probably the idea is
that you take the min in order to
that you take the min in order to
counteract the bias from having a max in
counteract the bias from having a max in
training or something like that. Um,
training or something like that. Um,
yeah, that's a silly hack. It doesn't
yeah, that's a silly hack. It doesn't
actually do much, it looks like. in the
actually do much, it looks like. in the
uh well the better methods the problem
uh well the better methods the problem
with DQN as a starting point is it's
with DQN as a starting point is it's
just such a crappy method that like
just such a crappy method that like
literally anything that you do will
literally anything that you do will
improve over DQN
improve over DQN
that perform better than Adam W in your
that perform better than Adam W in your
test performs better than Adam you don't
test performs better than Adam you don't
use atom W in RL because weight decay is
use atom W in RL because weight decay is
pretty rare you don't really ever use
pretty rare you don't really ever use
weight decay RL
weight decay RL
I mean you're free to try it I have
I mean you're free to try it I have
never gotten anything out of it
Yeah, it's a big difference. I mean, I
Yeah, it's a big difference. I mean, I
would just check out Puffer Lab, right?
would just check out Puffer Lab, right?
Like, we have all of the uh we have all
Like, we have all of the uh we have all
these advancements just there for you by
these advancements just there for you by
default, and there's blog post coverage
default, and there's blog post coverage
of everything that we've done,
of everything that we've done,
and it's going to be much faster than
and it's going to be much faster than
just about every other library.
just about every other library.
Not just about every other library,
Not just about every other library,
faster than every other library really,
better in general.
better in general.
But like from what they should kind of
But like from what they should kind of
be the same thing, right? Like think
be the same thing, right? Like think
about this for two seconds. You train a
about this for two seconds. You train a
policy,
policy,
okay, with a value function or you just
okay, with a value function or you just
train an action condition value
train an action condition value
function. These should be like the same
function. These should be like the same
thing. And it's probably the surrounding
thing. And it's probably the surrounding
details that are making any differences.
You can't just have on policy being
You can't just have on policy being
strictly better because like throwing
strictly better because like throwing
away all of the old data um it means
away all of the old data um it means
like you can't even keep around highv
like you can't even keep around highv
value data. Like let's say you even have
value data. Like let's say you even have
expert data in there, you can't use it.
expert data in there, you can't use it.
It's a big problem.
do this and then where is the
do this and then where is the
rain step
how to train also why would it make a
how to train also why would it make a
difference like how to train Atari games
difference like how to train Atari games
versus whatever results.
It literally shouldn't make a difference
to train a value function.
Much harder to train a value function
how to do a control loop.
how to do a control loop.
But you can train you train an action
But you can train you train an action
condition value function should be the
condition value function should be the
same thing
like training a value function. It's the
like training a value function. It's the
same thing except that the value
same thing except that the value
function has to guess one additional
function has to guess one additional
action from the policy.
action from the policy.
So I'm a beginner in PyTorch. I can
So I'm a beginner in PyTorch. I can
write some custom models where I can
write some custom models where I can
write advance level
write advance level
issue further read uh follow my basic
issue further read uh follow my basic
programming in ML guide on X um it'll
programming in ML guide on X um it'll
direct you to CS231N just do everything
direct you to CS231N just do everything
in CS231N and you'll be good
in CS231N and you'll be good
sampling is different you don't sample
sampling is different you don't sample
entire episode what do you
What do you mean you don't sample an
What do you mean you don't sample an
entire episode? But like you totally
entire episode? But like you totally
can, right?
can, right?
There's no reason you can't.
Like the point that I'm making here and
Like the point that I'm making here and
why I'm doing this, right? This is not
why I'm doing this, right? This is not
like basic like oh implement like simple
like basic like oh implement like simple
like simple like simple offpaul
like simple like simple offpaul
algorithm. The key that I'm trying to
algorithm. The key that I'm trying to
get at here is that these methods really
get at here is that these methods really
aren't that different except that if you
aren't that different except that if you
apply them correctly, uh, having a Q
apply them correctly, uh, having a Q
function should let you reuse old data.
function should let you reuse old data.
So, ignore all the surrounding details
So, ignore all the surrounding details
of how these algorithms are typically
of how these algorithms are typically
used. That should be an advantage,
used. That should be an advantage,
right? So, it's possible that like you
right? So, it's possible that like you
have that and then everything else is a
have that and then everything else is a
mess or like pieces of what is typically
mess or like pieces of what is typically
put around that is a mess. But I want
put around that is a mess. But I want
that core piece of being able to reuse
that core piece of being able to reuse
old data
running some training
for stats. said you
for stats. said you
I'm using 1B just want some training
I'm using 1B just want some training
curves FBRR
curves FBRR
want some training curves on like
want some training curves on like
reasonable problem and then link like an
reasonable problem and then link like an
image or video of policy doing
image or video of policy doing
reasonable stuff
reasonable stuff
I will try to make some time for you
I will try to make some time for you
tomorrow to uh tomorrow or Friday if
tomorrow to uh tomorrow or Friday if
you're around to
you're around to
uh to go through
uh to go through
and take a look at that PR I want to do
and take a look at that PR I want to do
off Paul today because I have it fresh
off Paul today because I have it fresh
in my mind. I've been reading a bunch of
in my mind. I've been reading a bunch of
papers.
So now we do
This comes from the target, right?
Wait, what the heck?
Wait, what the heck?
Target.
Target.
Oh, okay. I see.
Oh, okay. I see.
Yeah. Yeah. Yeah. This is fine.
You do you did actually double check the
You do you did actually double check the
math, right? Like you have the math
math, right? Like you have the math
either from your background or from
either from your background or from
stuff you studied. It's not just from
stuff you studied. It's not just from
Claude, right?
Because that's the key thing.
So this gives us target net.
So this gives us target net.
Now we have to construct wait no this is
Now we have to construct wait no this is
QA
and this gives us the action net right
and this gives us the action net right
and then we have the target net has to
and then we have the target net has to
be
be
the target's going to be different
so we do target
so we do target
max
words
like this. Do
like this. Do
this is your target
times one minus done.
Hey Joseph, if you have time, please
Hey Joseph, if you have time, please
consider making an intro to RL with Py.
So, um, I have literally a full guide
So, um, I have literally a full guide
for you. It's not in video form. uh
for you. It's not in video form. uh
video form alone is a very very bad way
video form alone is a very very bad way
to learn. You cannot learn this stuff
to learn. You cannot learn this stuff
just by watching videos.
just by watching videos.
Um the way the thing that I would
Um the way the thing that I would
suggest and this is a complete reference
suggest and this is a complete reference
guide. If you follow this, you will
guide. If you follow this, you will
understand RL. You'll be able to
understand RL. You'll be able to
contribute to research. You'll be able
contribute to research. You'll be able
to help advance state of the art and
to help advance state of the art and
you'll be able to do a bunch of stuff.
you'll be able to do a bunch of stuff.
Uh it's on X.
Uh it's on X.
It's on my articles tab here.
It's on my articles tab here.
Supposed to be on the website. I think I
Supposed to be on the website. I think I
never finished reformatting everything
never finished reformatting everything
nicely
nicely
on the website. Um, so you'll want to go
on the website. Um, so you'll want to go
here and this is my guide to RL. And if
here and this is my guide to RL. And if
this is too advanced, go back to my
this is too advanced, go back to my
programming and ML guide.
programming and ML guide.
But this will take you through the
But this will take you through the
process of building environments. This
process of building environments. This
will slowly introduce you to background
will slowly introduce you to background
papers. Um, this is how you learn our
papers. Um, this is how you learn our
route.
This is how you do that
This is how you do that
because this is the most common question
because this is the most common question
I get. So, I just I made one, right?
And pretty much the best contributors in
And pretty much the best contributors in
Puffer Lib got good by doing exactly
Puffer Lib got good by doing exactly
that. So,
that. So,
it's a lot of work. Um, it's nowhere
it's a lot of work. Um, it's nowhere
near as much work as I had to put in
near as much work as I had to put in
without having access to that like those
without having access to that like those
resources and puffer lib and everything.
resources and puffer lib and everything.
It's still a fair bit of work. It's a
It's still a fair bit of work. It's a
matter of if go through all of it
matter of if go through all of it
properly.
Pretty much it.
So we have to do
Sims and Rail. for like the last few
Sims and Rail. for like the last few
years. Oh, awesome.
years. Oh, awesome.
The reason I found Puffer
The reason I found Puffer
Rel.
Okay, cool.
Okay, cool.
Ask him.
Ask him.
Okay, good. So, you actually have
Okay, good. So, you actually have
background. Very, very good. I have to
background. Very, very good. I have to
ask, man, because when people submit me
ask, man, because when people submit me
big PRs, occasionally people do just
big PRs, occasionally people do just
like LLM slop a bunch of stuff without
like LLM slop a bunch of stuff without
having, you know, actually put in the
having, you know, actually put in the
effort to understand things and it's a
effort to understand things and it's a
big waste of time. So, as long as like
big waste of time. So, as long as like
this is not that like, yeah, that's
this is not that like, yeah, that's
awesome. Perfect. Then, yeah, if you've
awesome. Perfect. Then, yeah, if you've
written Sims in Ray, then of course it
written Sims in Ray, then of course it
makes sense that you were able to do a
makes sense that you were able to do a
somewhat larger project like this. Very
somewhat larger project like this. Very
nice.
nice.
So, yeah, let me know and I let me know
So, yeah, let me know and I let me know
when that is in. I'm not going to review
when that is in. I'm not going to review
it today because I want to do this
it today because I want to do this
offpaul stuff for a bit, but I will do
offpaul stuff for a bit, but I will do
it this week for you. And uh that's like
it this week for you. And uh that's like
a pretty exciting thing if you can get
a pretty exciting thing if you can get
that to work. Um
that to work. Um
like I uh I sent Finn and Sam who did
like I uh I sent Finn and Sam who did
the drone environment. I sent them a a
the drone environment. I sent them a a
drone to play with. I'd be willing to
drone to play with. I'd be willing to
like if this is good, I'd be willing to
like if this is good, I'd be willing to
send you like a simple robotic arm to
send you like a simple robotic arm to
see if you can get it on the real
see if you can get it on the real
hardware. And yeah, there's a lot of
hardware. And yeah, there's a lot of
stuff you can do from here.
stuff you can do from here.
Depends how far you want to take it,
Depends how far you want to take it,
but it is one of the most obvious
but it is one of the most obvious
applications of puffer lip to industry.
applications of puffer lip to industry.
So there's a lot of opportunity
I Okay.
implement the Oh, I didn't see that last
implement the Oh, I didn't see that last
comment. What do you mean ways to
comment. What do you mean ways to
implement the RL side? Like that is what
implement the RL side? Like that is what
Puffer Lib does, right?
No idea what it was. Now I have a pretty
No idea what it was. Now I have a pretty
good understanding. So the one thing
good understanding. So the one thing
that roboticists do really weird is they
that roboticists do really weird is they
use state-based continuous rewards. So
use state-based continuous rewards. So
they'll do stuff like the closer you get
they'll do stuff like the closer you get
to the target, the reward goes up and
to the target, the reward goes up and
then it's one when you're at the target.
then it's one when you're at the target.
Uh do not do that. It requires a very
Uh do not do that. It requires a very
sketchy RL implementation and will
sketchy RL implementation and will
break. Uh basically it's two errors that
break. Uh basically it's two errors that
they make. They make two errors um and
they make. They make two errors um and
that they cancel each other out. So what
that they cancel each other out. So what
you should do is you do delta to target.
you should do is you do delta to target.
So like let's say that you want to get a
So like let's say that you want to get a
reward for of one for going to the
reward for of one for going to the
target or whatever from infinity or
target or whatever from infinity or
something then you define the reward uh
something then you define the reward uh
as you get reward for getting closer to
as you get reward for getting closer to
the target. You get negative reward for
the target. You get negative reward for
going away and then they add up to one
going away and then they add up to one
right?
right?
you do something like that instead.
Something like change in distance to
Something like change in distance to
target divided by some normalization
target divided by some normalization
factor for instance is a linear one is
factor for instance is a linear one is
very good.
So that's the main thing that you're not
So that's the main thing that you're not
going to be able to look up in a book
going to be able to look up in a book
somewhere.
You can check the uh the drone
You can check the uh the drone
environment for an example if you want
environment for an example if you want
on that cuz I did that there.
Okay, let's see how this is.
This gives us a proper loss.
Same thing as before, but with a proper
Same thing as before, but with a proper
loss function. Let's make this actually
loss function. Let's make this actually
tray and fix anything broken.
Okay.
Okay.
runs.
I have it set. So, gains reward based on
I have it set. So, gains reward based on
percent change.
change.
Uh, make sure that it can't jitter and
Uh, make sure that it can't jitter and
get reward. So, make sure that if it
get reward. So, make sure that if it
moves like farther away and moves closer
moves like farther away and moves closer
or moves like closer and farther away,
or moves like closer and farther away,
make sure it sums to to zero.
Like there's a common hack for instance
Like there's a common hack for instance
where you know if you see uh it just
where you know if you see uh it just
hovering around the goal for instance
hovering around the goal for instance
it's usually that you've messed
it's usually that you've messed
something up with that.
something up with that.
Why does this thing
Why does this thing
you lost?
Okay.
Wait, hang on. Data.
Wait, hang on. Data.
Yeah, this is the target gets next
Yeah, this is the target gets next
observations, right?
observations, right?
And then the Q network gets
And then the Q network gets
observations,
gets rewards,
this is a pretty simple Um
it's a pretty simple simple setup. Okay.
it's a pretty simple simple setup. Okay.
So what do I have something wrong or is
So what do I have something wrong or is
it just like fiddly?
You run Q and target on the mini batch
You run Q and target on the mini batch
of obs, right?
of obs, right?
QA is going to be Q function of the
QA is going to be Q function of the
current action
current action
or rather the Q function of the current
or rather the Q function of the current
state with the selected actions
and then we get target which is going to
and then we get target which is going to
be the maximum
terminals.
terminals.
All right. And then we get an error
All right. And then we get an error
between we take this is going to be you
between we take this is going to be you
overlap them right. So this is the first
overlap them right. So this is the first
one and then it goes forward
QA and target
I was stuck with that earlier because I
I was stuck with that earlier because I
had Hang on.
Oh, they nan out. Lovely.
Oh, they nan out. Lovely.
Uh, hey, Kvert. Stuck with that earlier
Uh, hey, Kvert. Stuck with that earlier
because it hide a reward for moving
because it hide a reward for moving
closer than moving away, so it just
closer than moving away, so it just
jittered. Yeah, exactly.
That's like a very common bug
That's like a very common bug
and it's exactly what I just described
and it's exactly what I just described
to you.
Is the shared target really messing us
Is the shared target really messing us
up this bad or is it something else?
up this bad or is it something else?
Right,
it seems very unstable.
what we're doing makes sense, right?
what we're doing makes sense, right?
Even though we're using like on policy
Even though we're using like on policy
data with off policy learning, but it's
data with off policy learning, but it's
still like a ton of data.
Do I want to do like the same sort of
Do I want to do like the same sort of
advantage estimate? at
I the thing is it's like it's tough to
I the thing is it's like it's tough to
know whether I have something wrong or
know whether I have something wrong or
whether like yeah base DQN's just a
whether like yeah base DQN's just a
sucky algorithm. So, uh obviously
sucky algorithm. So, uh obviously
whatever configuration I have by default
whatever configuration I have by default
on on my much more stable much better
on on my much more stable much better
algorithm is not going to work out of
algorithm is not going to work out of
the box on the much fiddlier like bad
the box on the much fiddlier like bad
worse algorithm.
worse algorithm.
The target network should get the next
The target network should get the next
states not the cur. Doesn't it do this?
states not the cur. Doesn't it do this?
Cuz look, I take right here. This is
Cuz look, I take right here. This is
next date, isn't it?
like I shift it by one.
Like I suppose I can try it without the
Like I suppose I can try it without the
LSTM, but then it's much harder.
Yeah, I shifted logic and targets.
Yeah, I shifted logic and targets.
Exactly. Um, there's still this shared
Exactly. Um, there's still this shared
backbone thing, which is probably iffy,
backbone thing, which is probably iffy,
but I don't know. I really want to
but I don't know. I really want to
remove the LSTM either.
remove the LSTM either.
I just want to see this do something
I just want to see this do something
better than um like six or seven score
better than um like six or seven score
for reference is it's like the trivial
for reference is it's like the trivial
one. Even if I got like 10, I'd be happy
one. Even if I got like 10, I'd be happy
like, oh, okay, this learned something.
But yeah, getting up to like 6 point
But yeah, getting up to like 6 point
something, that's like the sort of the
something, that's like the sort of the
trivial one. I think you can even do
trivial one. I think you can even do
that just by going to one side, like the
that just by going to one side, like the
side that the ball usually falls to or
side that the ball usually falls to or
whatever.
I miss any other like important details?
I miss any other like important details?
I don't think so. It's a very basic
I don't think so. It's a very basic
algorithm, right?
They have this silly copy thing.
It seems like a bad idea to like move on
It seems like a bad idea to like move on
to the optimizations of this if I don't
to the optimizations of this if I don't
have this doing anything. Can I try it
have this doing anything. Can I try it
on like cartpole?
Let me just try it on an easier task
to um
I have some numbers hardcoded.
I have some numbers hardcoded.
Change this
action.
MB OBS.
MB OBS.
What do you mean an MB ops?
Yeah. So, this doesn't definitely
Yeah. So, this doesn't definitely
doesn't work if it doesn't solve cart
doesn't work if it doesn't solve cart
pull.
pull.
We'll use this as our testing task.
Let me make sure I don't have this
Let me make sure I don't have this
backwards.
Yeah, this is eps. F ep's choice, not
Yeah, this is eps. F ep's choice, not
eps.
eps.
I think this is good.
Does carpole even have an LSTM on it?
Oh, it does.
Let's mess with the freaking target
Let's mess with the freaking target
networks.
Odd is not going to do anything but
Odd is not going to do anything but
waste my bloody time.
Okay, I think we can hack this as just
Okay, I think we can hack this as just
like
target is the same thing.
What's this? Um,
this thing here, right?
They like sync it.
Wait. Args.
What is this towel param?
What is this towel param?
What the heck is this? This weird lurp
What the heck is this? This weird lurp
thing.
Target network update rate 1.0. Okay, so
Target network update rate 1.0. Okay, so
it's one anyways.
So you do something like this. Yeah.
And then
Okay, we have this. Now we need the
Okay, we have this. Now we need the
target network update.
Probably like hereish. Yeah.
and then the other thing is there's a
and then the other thing is there's a
stop gr on one of these, isn't there?
There is a stop gr here, right?
Target network.
Wait, do you train both of these? This
Wait, do you train both of these? This
doesn't make sense, right?
doesn't make sense, right?
I'm pretty sure you don't just train
I'm pretty sure you don't just train
both of these.
both of these.
Oh, wait. Is the optimizer Hang on. Hang
Oh, wait. Is the optimizer Hang on. Hang
on. The optimizer only is Yeah, the
on. The optimizer only is Yeah, the
optimizer is only on the Q network.
optimizer is only on the Q network.
So, the target we detach.
Yes.
Still no, huh?
Uh, it's kind of
Uh, it's kind of
being weird now. H
update and slowly update the target
update and slowly update the target
weights.
weights.
So, does it are they not supposed to be
So, does it are they not supposed to be
synchronized
synchronized
as often as possible?
as often as possible?
Target network frequency.
Oh, 500. Holy. Okay.
We update the target more slowly, huh?
Oh,
Oh,
well this is
they should be close. Why wouldn't you
they should be close. Why wouldn't you
just update them all the time then?
just update them all the time then?
They're not going to be close if they're
They're not going to be close if they're
500 updates stale.
Oh, there we go. Okay, look. That
Oh, there we go. Okay, look. That
actually did something.
If we train cartpull for a freaking uh
If we train cartpull for a freaking uh
billion steps, does this do it for us?
billion steps, does this do it for us?
All I need is to see something that's
All I need is to see something that's
actually directionally correct
actually directionally correct
me to start messing with more things.
Of course, now the problem is that um
Of course, now the problem is that um
it's going to take forever on like
it's going to take forever on like
collecting random data.
collecting random data.
Maybe not horrible for cartpull. We'll
Maybe not horrible for cartpull. We'll
see.
Okay, so now it should be pretty much
Okay, so now it should be pretty much
just collecting
just collecting
fresh data.
And now it starts flipping around a
And now it starts flipping around a
bunch.
This didn't work.
This didn't work.
Should need a billion steps for carpull
Should need a billion steps for carpull
anyways, even with like a bad algorithm.
See if this reproduces
Now, see, now it's not there. Same thing
Now, see, now it's not there. Same thing
as before.
Uh,
yeah.
just lucky a few episodes. It doesn't
just lucky a few episodes. It doesn't
seem like it could be. That's like too
seem like it could be. That's like too
much to just be lucky, right?
Look at the details of this.
We have fully unshared nets now, Okay.
and this seems like roughly
and this seems like roughly
this seems like it's the correct
this seems like it's the correct
algorithm to Okay.
The learning rate's freaking massive. We
The learning rate's freaking massive. We
mess with this.
Doesn't help. Do one more order of
Doesn't help. Do one more order of
magnitude just to be sure it's not this.
This is the easiest problem ever to
This is the easiest problem ever to
learn. For reference, you literally can
learn. For reference, you literally can
learn this without reinforcement
learn this without reinforcement
learning. You can literally learn this
learning. You can literally learn this
just by doing imitation learning on like
just by doing imitation learning on like
best experience collected. I did this
best experience collected. I did this
yesterday and it solves instantly.
Oh, wait. Hang on. Hang on. Hang on.
Oh, wait. Hang on. Hang on. Hang on.
Hang on.
Hang on.
Stupid. This has got to be target.
Stupid. This has got to be target.
We're still using the same network,
We're still using the same network,
right?
right?
This is QA
This is QA
the target.
Find the bug and it still doesn't work.
Try this again.
Oh, not any better.
We are we doing this correct?
Euro
and
Wait, what? Where's this come from?
Hang on.
0 1 0 right.
0 1 0 right.
Where does a
Oh, okay. Yeah. Well, this is completely
Oh, okay. Yeah. Well, this is completely
wrong.
wrong.
Cool. It's indexing bug.
Now, this is what I can double check
Now, this is what I can double check
freaking LM for
freaking LM for
cuz this is a basic doc lookup.
Okay, it's a range and then Fine.
Why?
Oh, 68 Okay.
The one one zero
The one one zero
should be this. This this
should be this. This this
we get this.
we get this.
this. Yes. So that's correct now.
Boom. Learns.
Indexing bugs are lovely.
Now, is it super stable? Is it good?
Now, is it super stable? Is it good?
Those are completely different
Those are completely different
questions. But does it learn?
Absolutely.
I just dropped the learning rate a
I just dropped the learning rate a
bunch.
bunch.
Yeah, that actually learns uh funny. It
Yeah, that actually learns uh funny. It
learns faster than our uh fancy
learns faster than our uh fancy
algorithm which is to be fair not
algorithm which is to be fair not
optimized at all for that.
optimized at all for that.
But yeah, there we go.
You go back to breakout. Oh,
still go
five. Uh, I forget. Maybe it does, but
five. Uh, I forget. Maybe it does, but
I thought it took a little longer.
I thought it took a little longer.
Whatever.
This is going to be
Welcome, Spencer. This is our first um
Welcome, Spencer. This is our first um
first off policy thing that we've done
first off policy thing that we've done
in puffer.
Interesting. Doesn't do anything on
Interesting. Doesn't do anything on
breakout like at all.
Um I think we can keep cart pull for
Um I think we can keep cart pull for
now. It's just an easier task
now. It's just an easier task
and like the current implementation just
and like the current implementation just
sucks. So,
sucks. So,
kind of to be expected,
but we can actually use this enough to
but we can actually use this enough to
like add all the extras into this and
like add all the extras into this and
then optimize.
Oh, yeah. You're right. I uh I made it
Oh, yeah. You're right. I uh I made it
way more steps. You're totally right. I
way more steps. You're totally right. I
forgot that I'd made it 200 million
forgot that I'd made it 200 million
steps for freaking cart pole.
But that is our first um first off
But that is our first um first off
policy result in puffer lab. I'm going
policy result in puffer lab. I'm going
to make a
Yeah, I like I 10xed it. You're right. I
Yeah, I like I 10xed it. You're right. I
had forgotten that I 10xed it. I don't
had forgotten that I 10xed it. I don't
It doesn't matter though. Like we're not
It doesn't matter though. Like we're not
expecting this to be good for starters.
expecting this to be good for starters.
We're just expecting it to uh to do
We're just expecting it to uh to do
something, right?
And now what we get to do, uh, we get to
And now what we get to do, uh, we get to
add in HL Gaus as like the first big
add in HL Gaus as like the first big
trick, I think.
trick, I think.
Do you actually know how to do it? It's
Do you actually know how to do it? It's
a little harder cuz I think we have to
a little harder cuz I think we have to
do
Well, I might be able to reuse your same
Well, I might be able to reuse your same
code just slightly differently.
Is this the research? Is this the
Is this the research? Is this the
research easier than Enside? Not
research easier than Enside? Not
exactly.
exactly.
So the difference Spencer and this is
So the difference Spencer and this is
generally true uh the difference is that
generally true uh the difference is that
based on the current capabilities when
based on the current capabilities when
you're building an env you roughly know
you're building an env you roughly know
what the correct end result is right
what the correct end result is right
like if the thing doesn't run fast you
like if the thing doesn't run fast you
know that you just have it slow and you
know that you just have it slow and you
can profile if the thing doesn't learn
can profile if the thing doesn't learn
you know that puffer is good and you
you know that puffer is good and you
should be able to at least train
should be able to at least train
something. The problem with the research
something. The problem with the research
side uh is you literally don't know the
side uh is you literally don't know the
like you don't know what target you're
like you don't know what target you're
trying to even hit, right? And the only
trying to even hit, right? And the only
thing that you can do to build up an a
thing that you can do to build up an a
good estimate of what target you're
good estimate of what target you're
trying to hit is to get really really
trying to hit is to get really really
good at cross referencing tons of papers
good at cross referencing tons of papers
and like fitting a mental model of like
and like fitting a mental model of like
like a basically a probabilistic model
like a basically a probabilistic model
of what is very high likelihood to work
of what is very high likelihood to work
if you do it correctly, what is less
if you do it correctly, what is less
certain and then like an estimate of are
certain and then like an estimate of are
you doing it correct slash uh does it
you doing it correct slash uh does it
just not work etc etc. So the research
just not work etc etc. So the research
the research side is hard due to
the research side is hard due to
uncertainty more than anything. It's
uncertainty more than anything. It's
very very uncertain. Like especially the
very very uncertain. Like especially the
stuff yesterday that I was doing like I
stuff yesterday that I was doing like I
have absolutely no idea what the ceiling
have absolutely no idea what the ceiling
is for that type of work. Like if I just
is for that type of work. Like if I just
did the best possible version of that,
did the best possible version of that,
how good could it do? Nobody in the
how good could it do? Nobody in the
world knows. That's what's hard. Um all
world knows. That's what's hard. Um all
right. I will be right back. I'm use a
right. I will be right back. I'm use a
restroom and then uh we are going to
restroom and then uh we are going to
we're going to add your HL Gaus and it's
we're going to add your HL Gaus and it's
a little different Spencer because we're
a little different Spencer because we're
not adding it on a value function we're
not adding it on a value function we're
adding it on a Q function right so then
adding it on a Q function right so then
this is actually going to go directly
this is actually going to go directly
into the um well this will go directly
into the um well this will go directly
to uh swapping the objective so that the
to uh swapping the objective so that the
main training objective is not quadratic
main training objective is not quadratic
it's not a quadratic loss squared error
it's not a quadratic loss squared error
it is a uh a classification Q function
it is a uh a classification Q function
to bins. Yep, exactly. I don't know if
to bins. Yep, exactly. I don't know if
we need to do the whole Q function
we need to do the whole Q function
actually. If you want to think about
actually. If you want to think about
that for a second, uh while I take a
that for a second, uh while I take a
minute. Um
minute. Um
do I I think I only need to add HL Gaus
do I I think I only need to add HL Gaus
to the action that was actually
to the action that was actually
selected.
selected.
I'm pretty sure. I'm not positive
I'm pretty sure. I'm not positive
though.
though.
What are we researching today? What's
What are we researching today? What's
off policy? Off policy means Q function
off policy? Off policy means Q function
related stuff. I'll be right back,
related stuff. I'll be right back,
Andrew. One sec.
Andrew. One sec.
client stuff. Yeah, do the client stuff
client stuff. Yeah, do the client stuff
first. I will do uh I'll handle research
first. I will do uh I'll handle research
for a little bit.
All
right. So,
right. So,
we have basic Q-learning.
we have basic Q-learning.
Next, HL Gaus.
Next, HL Gaus.
Beware HL Gaus.
You just bin both of these.
Hang on. Let me think.
It's slightly different from normal
It's slightly different from normal
because you have two targets, right?
look up the distributional
look up the distributional
mathy. Um,
let me just I just need to figure out
let me just I just need to figure out
from this how
from this how
how they quantize Guys,
in a discrete distribution
of fixed location
followed by KL minimiz.
followed by KL minimiz.
Oh, so they do this for the
Oh, so they do this for the
So it's not cross entropy, it's KL
So it's not cross entropy, it's KL
right.
Okay. So I see. So uh what we were doing
Okay. So I see. So uh what we were doing
is we were turning it into
is we were turning it into
classification before, right? We were
classification before, right? We were
turning the value function estimation
turning the value function estimation
problem into a classification problem by
problem into a classification problem by
binning the value function uh into a
binning the value function uh into a
number of discrete quanta quantizing the
number of discrete quanta quantizing the
value function the simple way of saying
value function the simple way of saying
that and that lets you do a
that and that lets you do a
classification laws right but because
classification laws right but because
you have two different things to
you have two different things to
discretise here you have the Q function
discretise here you have the Q function
and the target uh you get two different
and the target uh you get two different
distributions and then the natural
distributions and then the natural
optimization metric is a KL distribution
optimization metric is a KL distribution
or as a KL uh divergence term rather
or as a KL uh divergence term rather
which is uh still a much better like
which is uh still a much better like
numerically is still a much better
numerically is still a much better
optimization target than the uh the
optimization target than the uh the
squared error. It's a much more
squared error. It's a much more
expressive loss function.
expressive loss function.
Okay, so
wait, why is this even in the policy?
wait, why is this even in the policy?
So Spencer messed with the policy to
So Spencer messed with the policy to
make this work, right?
Spencer
Spencer took
Spencer took
the normal value function
the normal value function
right here.
HL Gaus
input
input
V min number of bins.
Uh, is this one good?
Uh, is this one good?
Target probs.
Target probs.
I think this is fine, isn't it?
Yeah,
Yeah,
I think we can literally just call this
We can just call this on both of them,
We can just call this on both of them,
can't we?
Like if I just take this?
Like if I just take this?
Okay.
Okay.
Paste this
Paste this
down somewhere.
down somewhere.
Grab this signature.
Grab this signature.
I think you don't need to even make any
I think you don't need to even make any
modification at all. I think you just go
modification at all. I think you just go
up to
up to
you go up to your Q functions.
Anyone can correct me if they have uh
Anyone can correct me if they have uh
knowledge to the contrary,
knowledge to the contrary,
but I'm pretty sure we just go up to our
but I'm pretty sure we just go up to our
Q function.
Q function.
So, here's the action condition one,
So, here's the action condition one,
right?
right?
And here's the target.
Okay.
And then we can do
2 A
2 A
view one
view one
like this
like this
2 A
want
G
target
target
on ized.
Then we do Q loss.
Then we do Q loss.
Uh, is there a KL loss in here? It's
Uh, is there a KL loss in here? It's
really basic, right?
Yeah, it's just this y true time log y
Yeah, it's just this y true time log y
true
loss.
manually did the kale in my version.
manually did the kale in my version.
Yeah.
Yeah.
Uh I don't know if what I just did makes
Uh I don't know if what I just did makes
any sense. We shall see.
any sense. We shall see.
I think it does.
Uh 150
Uh 150
happened.
Size of tensor must match.
Size of tensor must match.
What's the What is this thing supposed
What's the What is this thing supposed
to take?
input
input
B minax.
Is this thing not supposed to be a flat
Is this thing not supposed to be a flat
tensor?
This thing supposed to be something
This thing supposed to be something
else.
see how I can figure out
probably
Oh, is you do not just call this I
Oh, is you do not just call this I
thought you just call this one on the
thought you just call this one on the
other. No.
other. No.
You did it different.
We have this code down here.
This is the same code, isn't it?
Maybe there's not supposed to be an
Maybe there's not supposed to be an
unsqueeze.
Ew.
Heck. Why is this not valid?
Oh, of star N. Yeah, this is wrong.
Okay.
Okay.
Well, this still works, right?
It seems less stable.
Yes. Way less stable. No.
Yes. Way less stable. No.
Way less stable.
Way less stable.
Hm.
Hm.
Oh. Well, because um yeah, the Q
Oh. Well, because um yeah, the Q
function blows up, right?
This one doesn't work though.
Is there something that's done to
Is there something that's done to
normalize this so it doesn't blow up
normalize this so it doesn't blow up
horribly?
Maybe uh let's see if has an
Maybe uh let's see if has an
implementation.
Honestly, the uh the clean RL
Honestly, the uh the clean RL
implementations are so much better than
implementations are so much better than
reading the original mess of papers
reading the original mess of papers
like speedrunning. How can we confuse
like speedrunning. How can we confuse
you with more math than is required?
I guess C C-51 should have um at least
I guess C C-51 should have um at least
some reasonable form of this. How long
some reasonable form of this. How long
is this? Oh, this is not bad.
That's the idea. The idea is that
That's the idea. The idea is that
binning is better than MSA. Yes.
binning is better than MSA. Yes.
And um there's very very extensive
And um there's very very extensive
evidence for this
evidence for this
like probably more than most of the
like probably more than most of the
other things in off policy.
the hell decided to call this Adams.
the hell decided to call this Adams.
Okay, so this is what it is. It's just a
Okay, so this is what it is. It's just a
lin space. They actually do uh negative
lin space. They actually do uh negative
100 to 100, which
100 to 100, which
for this they actually do that.
And then wait, why did they
And then wait, why did they
they actually output
times numbum atoms?
times numbum atoms?
Kind of sketchy, isn't it?
Yes, Spencer. I do notice though that
Yes, Spencer. I do notice though that
they they have the output here. The
they they have the output here. The
output of the Q function is actually
output of the Q function is actually
equal to the number of actions times the
equal to the number of actions times the
number of atoms which is not the way we
number of atoms which is not the way we
have it implemented. Right?
They actually have this big blown up
They actually have this big blown up
final layer. This is the C-51 reference
final layer. This is the C-51 reference
which has
which has
stuff in it.
work
next. Okay, this is Adam thing.
What is the soda
What is the soda
sketch? And
sketch? And
uh we use cosine in um in puffer lib.
uh we use cosine in um in puffer lib.
Most libraries have linear or just don't
Most libraries have linear or just don't
even have it.
From my experiments at least, the choice
From my experiments at least, the choice
of umuler matters a lot less than just
of umuler matters a lot less than just
having it. Like you really need to have
having it. Like you really need to have
auler. The choice of which one you use
auler. The choice of which one you use
matters a lot less than just having one.
And then also being able to tune it
And then also being able to tune it
obviously.
Yeah, that's what we do at the moment.
Yeah, that's what we do at the moment.
You could add additional hypers if you
You could add additional hypers if you
wanted to like mess with final learning
wanted to like mess with final learning
rates and stuff, but we just Yeah, we
rates and stuff, but we just Yeah, we
just decay to zero at the moment.
Doing that does slightly mess with your
Doing that does slightly mess with your
uh your graphs a bit cuz like know it
uh your graphs a bit cuz like know it
makes most things look like they're
makes most things look like they're
leveling out towards the end of training
leveling out towards the end of training
even though it could just be the
even though it could just be the
learning rate, but it's still pretty
learning rate, but it's still pretty
good.
That's the only thing I don't really
That's the only thing I don't really
like about it.
Well, they do have in here negative 100
Well, they do have in here negative 100
to 100
representing the
and I don't see any sort of
and I don't see any sort of
normalization on Yes.
Yeah, this doesn't have enough
Yeah, this doesn't have enough
resolution, right?
like this for now.
Okay. So, the really big thing is the um
Okay. So, the really big thing is the um
the bootstrap, right?
endstep bootstrap.
endstep bootstrap.
I want to grab it from here.
I want to grab it from here.
Right. One
Wait, can we do J with this still? Hang
Wait, can we do J with this still? Hang
on.
We do J.
There's no reason we can't just do J E,
There's no reason we can't just do J E,
right?
They make the targets.
or puffer advantage rather.
Here's
a multi-step learning, right?
Okay, let me see if this makes sense.
What do you need for this? You need
What do you need for this? You need
values.
Yeah, we can totally do this, right?
Yeah, we can totally do this, right?
Let's just move this thing.
Oh, well, we use it for sampling as
Oh, well, we use it for sampling as
well, don't we?
well, don't we?
Uh, which is totally screwed up because
Uh, which is totally screwed up because
Yeah, this thing is totally screwed up.
Because basically we should have
use this for sampling at all. Huh?
use this for sampling at all. Huh?
Can have it be stale.
So super unstable, but there you go.
So super unstable, but there you go.
Does learn.
Does learn.
Go grab all this crap.
Grab all this crap.
And so we do
target
use is the Q value of the target
use is the Q value of the target
function for advantage or do you use the
function for advantage or do you use the
um
um
your own Q value?
your own Q value?
I don't know. Let's try this.
Do
Do
U sub AS.
Eat
ratio. So
this let us do anything
bandages. Just
we can compute advantages.
we can compute advantages.
And do we use these?
I think you do like this, right?
I think you do like this, right?
MB returns. It's going to be advantages
MB returns. It's going to be advantages
plus
QA.
Well, no. You would construct this from
Well, no. You would construct this from
Q target, right?
Q target, right?
Because this is Yeah. Yeah. This is the
Because this is Yeah. Yeah. This is the
target. This is what you're doing
target. This is what you're doing
instead of the target. Okay. So, you do
instead of the target. Okay. So, you do
this is going to be
this is going to be target.max
yada yada.
yada yada.
Don't need to multiply.
Okay. So now you have
Okay. So now you have
advantage computed by target
advantage computed by target
and then
believe it's I believe that it's
believe it's I believe that it's
something like this All right.
H
this.
Okay.
Oh, okay. So, something like this is
Oh, okay. So, something like this is
actually reasonable, I think.
actually reasonable, I think.
Super unstable.
And then
The replay buffer suppose
structure of a replay buffer is
structure of a replay buffer is
trickier.
We could just see if we can get this
We could just see if we can get this
something like this to work first.
We're missing filtering as well.
Yeah, we're missing filtering here,
Yeah, we're missing filtering here,
right?
That's pretty major.
But the idea here is that
so the target is the value function,
so the target is the value function,
right?
The target's the value function of uh
The target's the value function of uh
taking it's the well that's It's an
taking it's the well that's It's an
optimistic one. It's because it's under
optimistic one. It's because it's under
the best action.
Then this should be advantages plus
Then this should be advantages plus
target, right?
target, right?
Advantages plus target
Advantages plus target
will not work.
This is actually not that far off, I'm
This is actually not that far off, I'm
pretty sure. Right.
pretty sure. Right.
So, you need to save
So, you need to save
We need to save the values.
We need to save the values.
Yeah. Yeah. Yeah. Okay. So, I think what
Yeah. Yeah. Yeah. Okay. So, I think what
we the thing that was wrong here.
we the thing that was wrong here.
Okay.
Undo a little bit of this. It's pretty
Undo a little bit of this. It's pretty
close.
Okay. So, this is where we're going to
Okay. So, this is where we're going to
go back to and we're going to store
go back to and we're going to store
the targets
uh where the value function will go.
This is going to be target.
Okay.
And now
And now
this is a slightly off policy
this is a slightly off policy
uh advantage estimate.
Okay. Okay. And we use this
we use this to select data
just like we did before.
And the only difference now,
And the only difference now,
well actually
try something.
try something.
Let's do target equal
Let's do target equal
uh MB.
uh MB.
Try this.
Mess up somehow.
Uh why does the target have this
Uh why does the target have this
dimensionality?
Right. You need the maximum of this.
Right. You need the maximum of this.
Okay.
Okay. So now we have our target is MB
Okay. So now we have our target is MB
advantages. Right
advantages. Right
now we are actually doing general
now we are actually doing general
advantage estimation on this and um
okay this does do some learning.
okay this does do some learning.
Now the only thing that's wrong here
this value can get stale.
What we would like to do is we'd like to
What we would like to do is we'd like to
update
we would like to update the values,
we would like to update the values,
right?
And we do this,
we do this by rerunning.
We have to rerun the target computation,
We have to rerun the target computation,
don't we?
Do you see the problem here? Right. The
Do you see the problem here? Right. The
problem is that this is the original
problem is that this is the original
value that gets stored
value that gets stored
and um this is going to be stale because
and um this is going to be stale because
we're going to have updated the policy
we're going to have updated the policy
and we do not want this to be stale
and we do not want this to be stale
otherwise we lose the uh the generality
otherwise we lose the uh the generality
of policy training. So, I think what we
of policy training. So, I think what we
do for this is we grab this like compute
do for this is we grab this like compute
puff advantage thing from down here
and we're going to do it right.
Okay, we're going to do it right like
Okay, we're going to do it right like
this. We're going to grab this target
this. We're going to grab this target
computation
value.
This is going to be target max.
This is going to be target max.
And so we have this again.
I think this is good.
I think this is good.
Now we take the advantage.
Okay. So, we have this. Where is this
Okay. So, we have this. Where is this
ratio thing?
That's an important sample. Shoot.
That's an important sample. Shoot.
Uh, we have that.
Uh, we have that.
That's going to have to change. We can
That's going to have to change. We can
make this ones for now.
Okay. So,
okay.
And now we just do here
uh this target
Right.
Right.
Think that something like this is
Think that something like this is
correct because now
make sure we get this right. Oh, this is
make sure we get this right. Oh, this is
advantage.
don't know.
Ah, okay. So, target is
mess this up.
MV values
like this
because it's the selected.
Okay. So, evidently I've broken
Okay. So, evidently I've broken
something.
Uh,
I think I've broken something, right?
What have I done wrong here?
What have I done wrong here?
One advantage heroes.
current is going to be
plus this.
Now, this is normally what you would do.
Now, this is normally what you would do.
I think
what I messed up
what I messed up
to your advantage, use this to sample
to your advantage, use this to sample
data.
Then MB values
Then MB values
get your maximum target.
get your maximum target.
This is a solid estimate here.
Do this on the current data.
Uh, and then wait, does this did I
Uh, and then wait, does this did I
forget to shift them over by one?
forget to shift them over by one?
It um
this is like colon
this is like colon
One.
There we are.
Super unstable, but at least we get
Super unstable, but at least we get
something.
something.
Super unstable. Yeah, cuz the target's
Super unstable. Yeah, cuz the target's
blowing up.
blowing up.
But does an algorithm of this form make
But does an algorithm of this form make
sense? Right?
sense? Right?
Is the main thing I want to figure out
Is the main thing I want to figure out
which is
which is
you replace the uh the policy and value
you replace the uh the policy and value
function,
function,
the Q function and the target.
Like to understand why those need to be
Like to understand why those need to be
two separate networks, but sure for now.
This advantage estimate still makes
This advantage estimate still makes
sense.
Well,
it's kind of weird, isn't it?
Like technically, yes, you can write it.
Like technically, yes, you can write it.
I think that there is um
I think that there is um
off policyiness thing again here.
Okay. But like outside of that,
this should be a very close to
this should be a very close to
comparable algorithm, I would think, to
comparable algorithm, I would think, to
um what we had originally, provided I
um what we had originally, provided I
get it correct, right?
get it correct, right?
Because like all I've done is I've
Because like all I've done is I've
replaced the
replaced the
I've replaced the policy and value
I've replaced the policy and value
objective.
Oh, I guess I suppose we also don't have
Oh, I guess I suppose we also don't have
entropy in here, which is a problem,
entropy in here, which is a problem,
right? So, we'll have to fix that.
right? So, we'll have to fix that.
But like this should be a substantially
But like this should be a substantially
similar algorithm to what we had before.
And then the question is just like okay
And then the question is just like okay
why doesn't it work as well
be any number of things.
Well, first of all,
let's try. Let's see if this Well, hang
let's try. Let's see if this Well, hang
on. I think before I do this, I'd like
on. I think before I do this, I'd like
to get this to be stable.
If I just do um the Q target without
If I just do um the Q target without
advantage.
If I just comment this. Does this do it?
Oops.
Uh how'd this happen?
Right. So this has got to be
target equals
this this thing here, right?
this this thing here, right?
Here's previous version.
Uh, this is supposed to be
like stable and good.
Not
Not
we have lots of things to ablate here,
we have lots of things to ablate here,
right?
right?
Got this version.
Okay. Right. This version
Okay. Right. This version
that's stable. So, we have the original
that's stable. So, we have the original
DQN stable at this point. Yeah.
DQN stable at this point. Yeah.
Let's see if we can get the original
Let's see if we can get the original
architecture back in and still have it
architecture back in and still have it
stable.
or if this screws it up somehow, right?
Work basically off of uh off of this.
So, this will be
So, this will be
uncomment these guys.
Comment these guys like this.
L
eight.
This
This
encode observations.
This
and we have to remove this
and we have to remove this
synchronization thingy as well.
Way less stable, right?
I don't like the fact that there's a
I don't like the fact that there's a
freaking separate target network.
Super jank.
And this does something. But
what is the point of this thing again?
This in the original DQN.
Doesn't seem like it
Doesn't seem like it
was introduced later.
was introduced later.
Okay.
Okay.
So, um is there a way to get rid of it?
So, um is there a way to get rid of it?
Because this is kind of uh not a good
Because this is kind of uh not a good
thing to have.
like at all.
like at all.
Freaking double your network size.
Freaking double your network size.
have this super janky synchronization
have this super janky synchronization
problem.
I really like I'm fine if I need a
I really like I'm fine if I need a
separate head on the same network,
separate head on the same network,
but like this is a pretty immediate
but like this is a pretty immediate
Well, hang on.
Well, hang on.
This is a big change. I kind of did this
This is a big change. I kind of did this
stupidly, didn't I?
stupidly, didn't I?
Yeah. Yeah. Yeah. Hang on. I did this I
Yeah. Yeah. Yeah. Hang on. I did this I
did this oblation really stupidly.
Hang on.
I kind of screwed this up massively,
I kind of screwed this up massively,
didn't I?
Not go back that far.
Yes.
do.
Yes.
Okay. So, this is the separate network
Okay. So, this is the separate network
version, right?
Uh, and I think that I don't even have
Uh, and I think that I don't even have
these. Do I have these sync? Yeah. So,
these. Do I have these sync? Yeah. So,
let's not synchronize them. Let's see
let's not synchronize them. Let's see
what happens.
You don't synchronize them.
You don't synchronize them.
Nothing trains.
If I do,
If I do,
if I do like this instead.
Uh, this works perfectly fine.
Uh, this works perfectly fine.
entropy hitting zero sooner, better? No,
entropy hitting zero sooner, better? No,
it is not. You generally don't want your
it is not. You generally don't want your
entropy to collapse.
entropy to collapse.
Okay, so um yeah, this this seems to
Okay, so um yeah, this this seems to
work, guys. Like
work, guys. Like
at least for this simple problem, not
at least for this simple problem, not
having a separate target network is like
having a separate target network is like
totally fine.
totally fine.
Uh, and that's kind of actually
Uh, and that's kind of actually
important because
we should be able to now use uh, LSTM
like this and then self target self.
like this and then self target self.
This
This
we should be able to do this, I think.
Is this what size?
Is this what size?
This
Okay. Well, very slight hit to
Okay. Well, very slight hit to
stability,
stability,
but this is fine, right?
So having shared target a okay
So having shared target a okay
works with LSTM works without LSTM.
Exactly right.
Yes.
Back to Tiny Network.
Introduced by
Is that not the original DQN paper?
Oh, no. That's not the uh the original
Oh, no. That's not the uh the original
DQM paper. What?
Yeah.
This works.
At least now we know it's not like
At least now we know it's not like
everything freaking breaks. At least on
everything freaking breaks. At least on
the simple task, right?
So we have uh we have things that we can
So we have uh we have things that we can
play with now as a result of this. And
play with now as a result of this. And
this target here
this target here
if we do this
super unstable
does learn something but super unstable.
Okay.
Okay.
What is the motivation for adding off
What is the motivation for adding off
policy
policy
should be a guy that
should be a guy that
problems it can given enough data.
problems it can given enough data.
Right? So the idea is that this should
Right? So the idea is that this should
allow you to my goal is to come up with
allow you to my goal is to come up with
an off policy variant here that's
an off policy variant here that's
basically the same exact algorithm as
basically the same exact algorithm as
what I have now except we don't have the
what I have now except we don't have the
off policyiness issue. So if we want to
off policyiness issue. So if we want to
reuse some old data we can do that
reuse some old data we can do that
without crashing the algorithm. That's
without crashing the algorithm. That's
it.
So,
So,
what is wrong with this? Did I construct
what is wrong with this? Did I construct
I construct the target wrong?
Is it just this?
Well, there we Well, little unstable,
Well, there we Well, little unstable,
but um
but um
huh. Very unstable.
Funny. I plugged this into an advantage
Funny. I plugged this into an advantage
estimate, right?
estimate, right?
And we can see that it's like
And we can see that it's like
it's learning.
it's learning.
That's about solves the task right
That's about solves the task right
there. Then it gets unstable
there. Then it gets unstable
potentially.
Okay, let me think about this because
Okay, let me think about this because
maybe I did this weirdly. So the idea
maybe I did this weirdly. So the idea
was to add some sort of advantage
was to add some sort of advantage
estimate in
we use for this
target
network.
I'm resisting the urge to put this into
I'm resisting the urge to put this into
Grock because I think it is just going
Grock because I think it is just going
to give me some
You want to have
the maximum value or the action value.
Try QA.
What if I do like a
You have the same thing of it being like
You have the same thing of it being like
quick but unstable.
I will stick it in the Gro real quick
I will stick it in the Gro real quick
just to sanity check.
just to sanity check.
I don't know why. I know this is a bad
I don't know why. I know this is a bad
idea. I'm going to do it anyway.
think that this is the same thing as
think that this is the same thing as
I've done it.
The only thing I don't know is if you
The only thing I don't know is if you
use the Q function or the target
use the Q function or the target
function.
function.
I should probably just think about it a
I should probably just think about it a
little bit, but I think that there's
little bit, but I think that there's
something else
something else
either way.
either way.
Oh, do I need to look in retrace?
Thought retrace was rather different.
Oh, you know what? You actually can't,
Oh, you know what? You actually can't,
right?
Yeah. You can't do this because it's off
Yeah. You can't do this because it's off
policy, right?
You actually do need to retrace which is
You actually do need to retrace which is
probably going to be the exact same
probably going to be the exact same
thing as I have but with important
thing as I have but with important
sampling right
is retracing here.
Oh,
mirror.
Okay. So, this is what we would do here,
Okay. So, this is what we would do here,
right?
They don't have like a super mathy paper
They don't have like a super mathy paper
without a ton of results.
But if this simplifies to J, then I
But if this simplifies to J, then I
should actually just be able to
should actually just be able to
it's probably going to just be like GA
it's probably going to just be like GA
with important sampling, right?
Yeah. So off policy.
Yeah. So off policy.
Oh, but you also have tree backup.
Oh, but you also have tree backup.
Important
sampling ratio truncated at one.
Now, is this a free trade?
Yeah, I'll read.
In fact, this is going to end up being
In fact, this is going to end up being
almost exactly what I have already
almost exactly what I have already
because I actually already have
because I actually already have
incorporated uh puffer advantage already
incorporated uh puffer advantage already
has important sampling into in it.
has important sampling into in it.
Help.
This is actually quite good because this
This is actually quite good because this
is so this is going to be a bit of a
is so this is going to be a bit of a
pain but this is at least like
pain but this is at least like
reasonably close to what we have right
reasonably close to what we have right
like I already have an important
like I already have an important
sampling based um advantage estimation
sampling based um advantage estimation
kernel.
kernel.
I am going to have to modify it so that
I am going to have to modify it so that
it makes sense.
Okay, before I do this, here's a
Okay, before I do this, here's a
question, right? Should I expect to be
question, right? Should I expect to be
able to have our current J work
with I think I should be able to have
with I think I should be able to have
our current advantage function work
our current advantage function work
because the data is pretty much on
because the data is pretty much on
policy, right? Cuz like I don't actually
policy, right? Cuz like I don't actually
have a buffer sampling yet.
have a buffer sampling yet.
So, let me spend a little bit of time on
So, let me spend a little bit of time on
fiddling with this to see if I can get
fiddling with this to see if I can get
it to be stable. Then, we're going to do
it to be stable. Then, we're going to do
the retrace correction, and we'll see
the retrace correction, and we'll see
how much time I have after that. I'll be
how much time I have after that. I'll be
right back.
go check one other thing
[Music]
There was supposed to be um somebody out
There was supposed to be um somebody out
for the uh the AC here by now. But I
for the uh the AC here by now. But I
haven't heard a Haven't heard a peep.
[Music]
Let's actually let's look at what
Let's actually let's look at what
what does this use? This uses
both the target and
This uses Q target
This uses Q target
evaluated on the action
evaluated on the action
which is shared in my case. So it
which is shared in my case. So it
doesn't matter which one you use but it
doesn't matter which one you use but it
would be QA that you would like the
would be QA that you would like the
action condition one.
action condition one.
And this would be U sub A.
Set the ratio to one, which is fine if
Set the ratio to one, which is fine if
it's on policy.
And this works, but it's unstable,
And this works, but it's unstable,
right?
Oh, wait. This just works.
Oh, wait. This just works.
This QA
This QA
target equals advantage.
target equals advantage.
Huh?
Huh?
Okay. So then this just this works and
Okay. So then this just this works and
is stable.
Yeah. Well, okay. It's It can bounce a
Yeah. Well, okay. It's It can bounce a
little bit, but isn't this supposed to
little bit, but isn't this supposed to
be MB
values plus advantage? Isn't that
values plus advantage? Isn't that
supposed to be this?
I mean, evidently not right.
Let me see what the um how we're
Let me see what the um how we're
supposed to actually construct the loss.
running will be there in about now.
running will be there in about now.
Okay.
Well, so I didn't miss them. They're
Well, so I didn't miss them. They're
just late.
All right. So, back to this. Um,
right. We need to figure out. So, right
right. We need to figure out. So, right
now I have this set to the advantage,
now I have this set to the advantage,
but I think that you're supposed to set
but I think that you're supposed to set
it to the return.
it to the return.
And the return is a different quantity.
And the return is a different quantity.
It's this plus the value.
loss function Q minus Q rat
B wait reward. board.
Okay, so this is recursive formulation
Okay, so this is recursive formulation
and it does have
it has this of the next state, right?
it has this of the next state, right?
So it's plus value.
But what's V target
B target is the maximum.
Is this already taken into account?
Is this already taken into account?
Perhaps
why do you adopt weird terminology?
Do you equate value based policy based
Do you equate value based policy based
off policy
off policy
policy?
Um
Um
I mean off policy is generally just you
I mean off policy is generally just you
have a Q function so that you're that
have a Q function so that you're that
the policy that you are optimizing is
the policy that you are optimizing is
not the policy that's necessarily
not the policy that's necessarily
generating the data. Um and on policy
generating the data. Um and on policy
supposedly it is but not really. Oh look
supposedly it is but not really. Oh look
all the terminology is wrong anyways.
all the terminology is wrong anyways.
Literally all the terminology is wrong
Literally all the terminology is wrong
anyways.
Hey, Kevin.
I'm trying to figure out.
I don't like the fact that this shifted
I don't like the fact that this shifted
one over, right?
Our advantage doesn't our advantage
Our advantage doesn't our advantage
already take this into account?
The advantage is normally nor it's
The advantage is normally nor it's
usually normalized as well, right?
usually normalized as well, right?
Okay. So, let me try something else. We
Okay. So, let me try something else. We
do.
Do I not shift this somewhere?
Okay.
Yeah. So, there is possibly an indexing
Yeah. So, there is possibly an indexing
screw up here.
You do want this to be like
You do want this to be like
MB values plus advantage, right?
But then this doesn't work.
All
right, let's try something something
right, let's try something something
different here. I want to see
where's the advantage normalization.
advantage normalization is like
advantage normalization is like
essential
essential
um for the on policy.
See, where is
See, where is
where did it go
world model for R? Yeah, that's a meme.
world model for R? Yeah, that's a meme.
That's just that's not working.
That's just that's not working.
No way.
Oh, here it is.
So, we do it like this.
So, we do it like this.
What happens if we add just advantage
What happens if we add just advantage
normalization?
Uh, this does solve the task, albeit
Uh, this does solve the task, albeit
quite a bit of instability.
Does doing this make any sense in a
Does doing this make any sense in a
Q-learning context, though?
They're trying to RL on like
They're trying to RL on like
hallucinated Sims.
It like makes even less sense than
It like makes even less sense than
normal modelbased RL does.
At least there you're getting fresh
At least there you're getting fresh
data.
without this.
Without this
now, you still have uh instability,
now, you still have uh instability,
right?
right?
You still have wild instability,
You still have wild instability,
which it gets like towards the end of
which it gets like towards the end of
training pretty good,
training pretty good,
but I think this is kind of okay.
Oh, hang on.
Yeah, this is the advantage function is
Yeah, this is the advantage function is
going to be super messed up because it's
going to be super messed up because it's
actually your off policy immediately
actually your off policy immediately
anyways because of the uh the crazy
anyways because of the uh the crazy
epsilon greediness. All right,
epsilon greediness. All right,
we have this in here as an option at
we have this in here as an option at
least. And we'd like to have this at
least. And we'd like to have this at
some point because it'll make um it'll
some point because it'll make um it'll
make the distributional RL thing
make the distributional RL thing
actually work reasonably.
Can we not try this? Hang on. We can try
Can we not try this? Hang on. We can try
this without advantage estimation, can't
this without advantage estimation, can't
we?
we?
If I just do like targets
If I just do like targets
like this and I just do
like this and I just do
target
target
minus And
it does actually optimize
it does actually optimize
So if you just norm targets,
I'd say it's it's yeah, it's way less
I'd say it's it's yeah, it's way less
stable than not norming the targets.
I have to think whether normalizing
I have to think whether normalizing
targets makes any sense,
targets makes any sense,
but you can technically do it.
but you can technically do it.
All right.
Okay. And then there is now
Okay. And then there is now
do we go figure out retrace? Is that the
do we go figure out retrace? Is that the
main thing that we have to do?
main thing that we have to do?
I mean there kind of seem like there are
I mean there kind of seem like there are
several things.
worlds. Yeah, but the simulated worlds
worlds. Yeah, but the simulated worlds
suck.
I think they're literally just going to
I think they're literally just going to
like exploit errors in the sim
like exploit errors in the sim
basically.
Not like the simulated worlds suck and
Not like the simulated worlds suck and
they're super slow.
They did get it to be coherent enough
They did get it to be coherent enough
that it's not a total fever dream, but
that it's not a total fever dream, but
like it's nothing like an actual fixed
like it's nothing like an actual fixed
sim.
Yeah. Sim real doesn't even remotely
Yeah. Sim real doesn't even remotely
enter into this conversation. Like it's
enter into this conversation. Like it's
a mess way before that.
It's not just a sim to real gap. It's
It's not just a sim to real gap. It's
literally that you are creating a gap.
literally that you are creating a gap.
Like you're optimizing for a gap.
Wait, what? Can you not just
Okay, so
Okay, so
evidently this is actually a thing
evidently this is actually a thing
supposedly.
Let's put this in here.
I don't like how jumpy this thing is,
I don't like how jumpy this thing is,
but
but
it does optimize
somewhat.
somewhat.
Uh, I think we can actually
Uh, I think we can actually
Try the HL G thing with this now, right?
Try the HL G thing with this now, right?
Because this is no longer a mess maybe.
Oh, how are you? Switched Windows. No,
Oh, how are you? Switched Windows. No,
it's just that I had this old uh this
it's just that I had this old uh this
desktop at this place already had
desktop at this place already had
Windows on it. I don't want to go
Windows on it. I don't want to go
through the effort of reinstalling it.
through the effort of reinstalling it.
As you can see, I'm actually I'm working
As you can see, I'm actually I'm working
here sshed into a Linux box. Anyways, so
okay. So this optimizes something.
okay. So this optimizes something.
Not great though.
USL. Yeah, but like I'm on a native
USL. Yeah, but like I'm on a native
Linux environment here. It just sshed
Linux environment here. It just sshed
into a Linux box, so it doesn't make a
into a Linux box, so it doesn't make a
difference.
Yeah. So, this doesn't work at all. Hel
Yeah. So, this doesn't work at all. Hel
Gaus, which is funny. means I'm probably
Gaus, which is funny. means I'm probably
doing it wrong, right?
Like this is still the most stable
Like this is still the most stable
version.
do we try to do retrace now?
do we try to do retrace now?
No, I don't.
No, I don't.
Something is still wrong. My mind I
Something is still wrong. My mind I
think something is still wrong here.
What about this? So we do target.
What I do?
Okay. So here is this version of it.
And like
And like
it doesn't really work if I do this.
Not at all. Right.
Oh, what
are
Yeah, but hang on. Like this is not even
Yeah, but hang on. Like this is not even
supposed to work now, right? Because
Yeah, this is not even supposed to work
Yeah, this is not even supposed to work
because of how off off policy it is,
because of how off off policy it is,
right?
So maybe we should not be worried about
So maybe we should not be worried about
that.
that.
Uh the HL G not working is
Uh the HL G not working is
I think much sketchier in my mind.
I think much sketchier in my mind.
Right.
Right.
Like
Like
say we have this target cuz this works
say we have this target cuz this works
instantly.
instantly.
Maybe that's the thing we should do
Maybe that's the thing we should do
next.
Distributional supposed to work really
Distributional supposed to work really
well and the fact that it doesn't is
well and the fact that it doesn't is
sketchy. This thing works.
Do like this.
just super unstable. Like the queue just
just super unstable. Like the queue just
blows up.
blows up.
Oh, hang on. I see it. I see it. I uh I
Oh, hang on. I see it. I see it. I uh I
see it.
Uh, and I was missing a detach up here
Uh, and I was missing a detach up here
as well, am I not?
as well, am I not?
I'm missing a detach up top. Okay, so
I'm missing a detach up top. Okay, so
there are a lot of things wrong. Hang
there are a lot of things wrong. Hang
on.
on.
View size.
doesn't work at all. Lovely.
doesn't work at all. Lovely.
That's
That's
things bugs.
Okay. So there is
Okay. So there is
there's this working right with the
there's this working right with the
correct detached loss.
correct detached loss.
Okay, next one.
Why does it not?
Not this one.
Well, Q and the targets blow up
Well, Q and the targets blow up
like instantly.
like instantly.
What happens to Q and the targets when I
What happens to Q and the targets when I
don't do they still blow up?
Yeah, they still blow up
Yeah, they still blow up
nowhere near as badly
into 20.
Okay.
This blows up dramatically.
This blows up dramatically.
some reason
this how it's supposed to be
should be um
it's just A quantization of both of
it's just A quantization of both of
these, isn't
Hang on.
Second
Yeah. Put a note on the door.
Okay. So,
did I
did I
did I do this reasonably?
did I do this reasonably?
I think this one is
Now they do have a Q function here.
Target is also a categorical.
How do I apply this?
How do I apply this?
That one sucks.
That one sucks.
Um,
where is this thing?
to represent Q as the expected value
to represent Q as the expected value
categorical distribution.
But what I want to figure out here,
But what I want to figure out here,
right, is I know how to do this when you
right, is I know how to do this when you
have a single value function.
a video showing current task complete
a video showing current task complete
robot arm.
robot arm.
How I did?
How I did?
No. Look, thank you very much. I
No. Look, thank you very much. I
definitely will. Let me I need to finish
definitely will. Let me I need to finish
this off Paul stuff at least like make a
this off Paul stuff at least like make a
decent chunk of progress on this today
decent chunk of progress on this today
but that will be up on the agenda.
but that will be up on the agenda.
Um
I have like basic stuff working
I have like basic stuff working
but like all the additional tricks I
but like all the additional tricks I
need to be very careful to do them
need to be very careful to do them
correctly.
Yes. So you have your Q function
TD cross entropy.
How the hell is this a TD error?
We have the probability
date action data.
It looks like to me
it looks like they've made two different
it looks like they've made two different
distributions, right? Presumably these
distributions, right? Presumably these
are the two different sets of weights.
I'm trying to figure out if I did this
I'm trying to figure out if I did this
correctly because all I did right here
correctly because all I did right here
is I just called it on both of these and
is I just called it on both of these and
I don't know if this is actually how you
I don't know if this is actually how you
construct this because like it's
construct this because like it's
different in the on policy case where
different in the on policy case where
you just have one value function whereas
you just have one value function whereas
here you have the Q function and the
here you have the Q function and the
target function but I kind of just
target function but I kind of just
guessed that you do both of them in KL
guessed that you do both of them in KL
but I don't know if that's All right.
I also don't know how this is supposed
I also don't know how this is supposed
to work
to work
in a Q-learning case when the Q function
in a Q-learning case when the Q function
like tends to blow up a lot more. Well,
like tends to blow up a lot more. Well,
I guess the value function does as well,
I guess the value function does as well,
doesn't it?
But the thing is you predict normalized
But the thing is you predict normalized
advantages
advantages
um in the on policy case
um in the on policy case
and normalized advantages are a lot more
and normalized advantages are a lot more
sensible.
Yeah. And then it just starts
Yeah. And then it just starts
implementing stuff that I didn't ask it
implementing stuff that I didn't ask it
to implement because these models are
to implement because these models are
stupid. Of course. Um
could have written like literally two
could have written like literally two
lines
prediction probs.
fiction logits.
Yeah, these aren't logits though. This
Yeah, these aren't logits though. This
makes no bloody sense, right?
Because the targets now
and then this is just KL, right?
categorical distribution.
Okay, this is gross.
Okay, this is gross.
Super freaking expensive, I guess.
But then you also have the separate
But then you also have the separate
target, right?
Okay. So you only HL Gaus
Okay. So you only HL Gaus
the target network.
It's so stupid that you need this sort
It's so stupid that you need this sort
of a hack. This is why I've always hated
of a hack. This is why I've always hated
off policy learning. They do like a
off policy learning. They do like a
whole bunch of stupid mathematical hacks
whole bunch of stupid mathematical hacks
to make their crappy base algorithm
to make their crappy base algorithm
work.
Like none of the off policy methods get
Like none of the off policy methods get
near the on policy methods until you do
near the on policy methods until you do
all this
That seems screwy to me as well.
Okay. Okay. Well, now I have to test a
Okay. Okay. Well, now I have to test a
whole bunch of things, right?
noxious. Okay.
noxious. Okay.
Uh I guess we just go try to implement
Uh I guess we just go try to implement
it and see what we can do about this. I
it and see what we can do about this. I
mean there are a whole bunch of things I
mean there are a whole bunch of things I
have to do even as prerex though, right?
have to do even as prerex though, right?
So like go back to this one.
Oh, come on. What did I break? This
Oh, come on. What did I break? This
should This one should definitely be
should This one should definitely be
stable, right?
Why is this suddenly not stable? This
Why is this suddenly not stable? This
was the stable one that always worked.
Okay, whatever. There it goes. Um,
hang on
hang on
one sec.
Okay.
HVAC
HVAC
being assessed.
being assessed.
Does not make buzzing noises that drive
Does not make buzzing noises that drive
me insane.
Uh first thing
Uh first thing
target network has to be different right
target network has to be different right
we do encoder we'll go to
we do encoder we'll go to
this
this
target network
target network
try a couple different things
uh Q network first.
So this is this should be the same as
So this is this should be the same as
before,
right?
Act weird.
Act weird.
Uh, is this not the exact same thing as
Uh, is this not the exact same thing as
before?
before?
should be right.
Standard deviation maybe
they do
they do
literally same as before. Is it just
literally same as before. Is it just
different run to run or something
different run to run or something
ridiculous?
Hey, you can get unlucky runs. Cool.
Now we do
Now we do
uh separate target network
still learn
still learn
partially shared
and they're not synchronized.
and they're not synchronized.
In fact, they cannot be synchronized
In fact, they cannot be synchronized
once I make the modifications.
That makes it way worse, right?
Okay. So how is it then that
how is it that supposed to have
how is it that supposed to have
separate
separate
human target nets that you cannot sync
distributional Okay.
Oh, weird.
and this is supposedly
and this is supposedly
one of the main things.
Suppose we can just try it, right?
really man.
Not. We don't need this, do we?
Not. We don't need this, do we?
Hang on. We don't need like this.
Hang on. We don't need like this.
Action actually gets to stay the same.
Action actually gets to stay the same.
Thankfully,
actions do not match action.
and then the Q function.
How do you sample from this?
I do not want
How the hell are these logits? They're
How the hell are these logits? They're
not
not
right.
right.
They're not
obnoxious.
Sample this thing.
Grab random actions for now.
Then
The idea here,
reshape the queue, right?
A
s.
Does this not work?
Oh, okay. It's just the view that's
Oh, okay. It's just the view that's
wrong.
Just we'll hard code everything for now.
Just we'll hard code everything for now.
It's fine.
It's fine.
Okay. So, this is our QA.
Okay. So, this is our QA.
Just ignore
Just ignore
ignore all this stuff for now. Right.
This gives us our target.
And um
right here.
Okay. Okay. So now we have QA
target
and
Okay.
Okay.
Skip this reshape. I
Yep.
Okay.
Okay.
Target
151.
Um,
Um,
I guess it's 51, right?
Okay. And now you get your thing here.
Okay. And now you get your thing here.
And now I think you can just run KL div.
And now I think you can just run KL div.
Yeah,
this is on
this is on
A.
Yeah. And this doesn't learn anything.
Yeah. And this doesn't learn anything.
Lovely.
Nothing at all.
Rock code will check the clean RL code.
Rock code will check the clean RL code.
We'll check everything.
We'll check everything.
Oh, well, of course it doesn't learn
Oh, well, of course it doesn't learn
anything. I forgot the sampling step.
anything. I forgot the sampling step.
Duh.
Duh.
All right, let's see. How do they sample
All right, let's see. How do they sample
in C-51?
They have Yeah, they have the same thing
They have Yeah, they have the same thing
here. Okay.
Get action.
Get action.
What's get action do?
What's get action do?
Probability mass function for action.
Arg max values
gross
self atoms or whatever is
self atoms or whatever is
okay. So you have to do the super gross
okay. So you have to do the super gross
thing to even sample action.
have your atoms.
Then
we have our sample.
Okay. So you do softmax
Okay. So you do softmax
view
151. One,
two.
Two values.
Two.
actions not match action space.
Okay.
do There.
Oh, this should be
There we go.
a little bit.
a little bit.
Not going to exactly call this
Not going to exactly call this
encouraging.
2A gets to be a reasonable range at
2A gets to be a reasonable range at
least.
I think the value is supposed to be
100, right?
Ah, okay.
Ah, okay.
Um,
Um,
it's like
it's like
at least it does something
at least it does something
now. At least it does something.
Probably lots of implementation bugs to
Probably lots of implementation bugs to
fix.
Why is the target so weird? Oh, no. That
Why is the target so weird? Oh, no. That
makes sense.
makes sense.
Okay. So, this is it's something
Okay. So, this is it's something
it's funny that the thing that's worked
it's funny that the thing that's worked
best so far is the original just q val
best so far is the original just q val
formulation
formulation
like q function uh dqn formulation.
This is supposed to be like way better
This is supposed to be like way better
immediately now.
One second.
One second, folks.
Well,
Well,
the uh air is no longer going to be AC
the uh air is no longer going to be AC
is no longer going to be keeping up me
is no longer going to be keeping up me
up at night with horrible rattling.
up at night with horrible rattling.
Nice.
Nice.
We'll buy a couple things for it, but
We'll buy a couple things for it, but
at least for the time being,
at least for the time being,
no more crazy rattling.
What time is it? 4:10.
What time is it? 4:10.
Yeah. Another hour and a halfish.
So, I mean, what's going on with this,
So, I mean, what's going on with this,
right? Like,
is there um
is there um
clean has like something that has
a dagger.
Nothing else with distributional. It's
Nothing else with distributional. It's
just C-51, right?
just C-51, right?
Let's see if I can figure out from here.
Let's see if I can figure out from here.
So you get a Q network. It has madams
So you get a Q network. It has madams
whatever.
whatever.
Wait, actually is there no separate
Wait, actually is there no separate
target?
target?
Wait, what? There's no target network
on
on
Oh, wait. No, it has
network target network with the same
network target network with the same
numb atoms thing.
Do we have an HL G implementation for
Do we have an HL G implementation for
offpaul?
Great. It's a freaking Jax.
and never write any freaking code
where
control tasks as well which is going to
control tasks as well which is going to
be different.
Not like the PyTorch repos are any
Not like the PyTorch repos are any
better half the time.
Has a repo, doesn't it?
Has a repo, doesn't it?
Oh, yeah. This is Lucid Range.
Yeah. But this is not on anything. This
Yeah. But this is not on anything. This
is just uh the loss, right?
is just uh the loss, right?
Yeah, this literally just has the loss.
Okay, so
the C-51 agent is different. Now,
the C-51 agent is different. Now,
how would you apply this?
Don't apply HL goes to keep functions
Don't apply HL goes to keep functions
outputs only to scaler scaler T Target's
C-51.
Oh, it is just uh distributional. Cool.
Oh, it is just uh distributional. Cool.
So, this is the paper, the original
So, this is the paper, the original
distributional paper.
And then this one
And then this one
evidently they do this for
they do this for both the target target
they do this for both the target target
nets.
Then there's the stop progressing paper,
Then there's the stop progressing paper,
right? Is this it?
That's retrace
That's retrace
stop progressing. Yes, it is.
stop progressing. Yes, it is.
So we get classification with a G.
So we get classification with a G.
We get
Okay, so 51
Okay, so 51
get a pretty decent jump with HL Gaus
get a pretty decent jump with HL Gaus
here, right?
here, right?
Actually, MSSE is
Actually, MSSE is
these are actually pretty close and HL
these are actually pretty close and HL
Gaus is the only one that does
Gaus is the only one that does
well. I suppose that directionally here
well. I suppose that directionally here
the MSE gets worse with more updates.
the MSE gets worse with more updates.
G is a lot more stable.
network
by full distributional bellman. Simplify
by full distributional bellman. Simplify
this
apply only to the scalar target not to
apply only to the scalar target not to
the Q network outputs.
the Q network outputs.
Well, how the hell are you getting the
Well, how the hell are you getting the
scalar target then?
target network.
target network.
Oh, I see.
Oh, I see.
So, because here they're doing soft max
So, because here they're doing soft max
times support and then this is going to
times support and then this is going to
give you the mean the mean Q,
give you the mean the mean Q,
right?
right?
So, we can actually do this with a
So, we can actually do this with a
shared network as well.
shared network as well.
Yeah, we can do this with a shared
Yeah, we can do this with a shared
network. I see it.
Do this with a shared net.
Okay. So,
target itself, right?
Okay. So now in order to get
uh the
uh the
two values arg
fine you get your Q values arg max the
fine you get your Q values arg max the
same.
same.
Don't use the target net at all. Right.
Don't use the target net at all. Right.
Yes.
Yes.
Don't use the target net at all here.
Don't use the target net at all here.
Down to here.
We're going to want to take
We're going to want to take
want to take the couple of one, right?
Then we need to do
this.
Okay. And then
learn something.
Makes sense though that we do it
Makes sense though that we do it
this way
this
according to this right it's take the
according to this right it's take the
target network which is batch size
target network which is batch size
num Adams whatever this is the last dim
num Adams whatever this is the last dim
is going to be num Adams
is going to be num Adams
do soft max.
do soft max.
Then you do probs times atoms,
right? Probs times atoms.
Then
Then
3D actions are this
3D actions are this
next key.
This is This is Target.
And you actually take
And you actually take
target probs.
target probs.
Yeah. HL Gauss
on the targets
and the loss is
and the loss is
loss is a KL divergence. Am I stupid?
Y true log one.
Y true log one.
Oh wait, this is different. This is just
Oh wait, this is different. This is just
negative log likelihood, right?
negative log likelihood, right?
Um,
okay.
okay.
This is actually
boss.
Um,
target hunted
probs.
Max
8.
Um,
Um,
something like this.
still way off.
51. It was 51 buckets.
They have this on purple.
They have this on purple.
What they do
negative 100 to 100
negative 100 to 100
uh they do use 101 atoms in right
match exactly
match exactly
10 All right.
Well, at least now we don't have to
Well, at least now we don't have to
worry about parameters, right?
One good thing to do as a sanity test,
One good thing to do as a sanity test,
right?
right?
Let's just do
Let's just do
Q props
off like this, right?
off like this, right?
This can be Q probs.
Okay. And then this one
we can actually do this one here.
we can actually do this one here.
This is Q probs.
Let's see if like this parameterized
Let's see if like this parameterized
version works with an MSE loss.
Uh, wait. What? I do something weird.
there, didn't I?
There's several layers to this freaking
There's several layers to this freaking
thing.
Here's QA
and
Can I not convert this?
I can convert this into
I can convert this into
props, can I?
Oh,
so the value of this is
So you do like this
So you do like this
and you do
like this, right?
like this, right?
Then you do target val.
If you do this
then this is now
target val
is
is
MSC
MSC
QA val and then this is target val like
QA val and then this is target val like
this Okay.
this Okay.
So now this should be
So now this should be
uh discretized. Hang on
target.
Or you need a reap on it.
Yes.
Perfect. So these are whoop target val
one view.
I'm just trying to get something that I
I'm just trying to get something that I
can like work with starting point.
This is
This is
turn type stuff.
Okay. So now we have target val.
Okay. So now we have target val.
have way val.
All right. So now
All right. So now
now we can get target
now we can get target
lab. Come on here
because you don't need this any
need this max.
Okay. So now you have your target
Okay, so you have your values. Yeah, I
Okay, so you have your values. Yeah, I
should be able to define a Q loss,
should be able to define a Q loss,
QA, val, and freaking target.
Comment everything else.
Comment everything else.
And there should have been a long-
And there should have been a long-
winded way of just reconstructing the
winded way of just reconstructing the
original BQN loss
because what I've done here is it's
because what I've done here is it's
discretized
discretized
but then you end up recomputing you end
but then you end up recomputing you end
up uh computing a mean and you just end
up uh computing a mean and you just end
up with original DQN mostly
up with original DQN mostly
slightly different dynamic
and indeed
This is
This is
not super stable, but like
passable.
passable.
It does actually solve the problem at
It does actually solve the problem at
point like passable.
point like passable.
At least at this stage, it's not
At least at this stage, it's not
completely broken, right? It's not like
completely broken, right? It's not like
completely broken.
Now, conceivably we could make some
Now, conceivably we could make some
edits and stabilize this.
edits and stabilize this.
Okay.
Um, so now instead,
how does this make any bloody sense?
how does this make any bloody sense?
Actually
thing is this has got to be Grock lying.
thing is this has got to be Grock lying.
Let me see if I can find it in here. How
Let me see if I can find it in here. How
they do it? Does they have
they do it? Does they have
Okay, this is get action, right?
Listen to Grock.
They actually
They actually
next PMFs.
here. They do.
here. They do.
Oh, wait. Because this is get actions.
Oh, wait. Because this is get actions.
Hang on. Hang on.
Hang on. Hang on.
What do they call get actions?
What do they call get actions?
Get actions.
This returns you
This returns you
um
um
the PMF for this specific action
the PMF for this specific action
and you get it for
and you get it for
target network.
You assumably get it for
You assumably get it for
you get the old PMS
you get the old PMS
from the Q network.
and
and you construct some distributional
and you construct some distributional
loss, right?
So, uh, presumably
So, uh, presumably
presumably I should just be able to use
presumably I should just be able to use
this loss now. No,
which is the soft max
which is the soft max
of the action.
I just have to construct this a little
I just have to construct this a little
bit differently, right?
A
tick off max,
right? because you need to be able to
right? because you need to be able to
take max, right?
take max, right?
So, you take your soft max first
So, you take your soft max first
and then you do
times Adam's sum
times Adam's sum
Okay, so these are your values
Okay, so these are your values
and then you do
and then you do
max idx
uh target
uh target
valid
one.
You do
target.
Oh, wait. Hang on. So, this got to be
Oh, wait. Hang on. So, this got to be
a vowels.
UA
This is just an index select, right?
Oh, yeah. So, this is QA PMFs. Actually,
Oh, yeah. So, this is QA PMFs. Actually,
we're done here. All you need this PMF.
we're done here. All you need this PMF.
You don't need to you don't need it
You don't need to you don't need it
projected onto atoms at all. Okay. But
projected onto atoms at all. Okay. But
for target, you do. And the reason is
for target, you do. And the reason is
that you need to be able to uh select
that you need to be able to uh select
from it. We do target
um emfs
um emfs
soft max
soft max
and then we do
and then we do
they're the probs.
They're not the props are
They're not the props are
val.
val.
Okay. And then we do target val target
Okay. And then we do target val target
index. You take the max
then
target
target
ax.
And we probably
Want this to unbatched
complicated
max.
Okay.
Q A
not
not
let's not view this as a sequence Okay.
Target and QA are good.
Target and QA are good.
Now target has to be
QA PMF
going to be QA
one
this get multip hang on how do you
this get multip hang on how do you
compute
compute
EMF is get action
this already projected.
No, it's literally just the soft max
No, it's literally just the soft max
applied to
applied to
Q values are returned.
This is for sampling. For sampling, you
This is for sampling. For sampling, you
have to do the Q values, right?
Okay, but this is fine. So you get your
Okay, but this is fine. So you get your
target PMF
target PMF
and this is actually target Q,
and this is actually target Q,
right?
Be target Q again. Don't need all of
Be target Q again. Don't need all of
them anymore.
them anymore.
This is target
max of one.
Let's see what this.
Now we have
target
Q
Q
has gotten us the singular
has gotten us the singular
uh values here. But
uh values here. But
index
index
we need target max PMF right
this is 32.
Okay
Okay
target index. Now this gives us a target
target index. Now this gives us a target
max PMF
max PMF
max
max
tape QA PMF tape. Okay, cool. So now we
tape QA PMF tape. Okay, cool. So now we
have probability mass functions for the
have probability mass functions for the
Q function and for the target. We have
Q function and for the target. We have
it for both.
it for both.
So there's a whole bunch of graph down
So there's a whole bunch of graph down
here that we ignore for now.
here that we ignore for now.
And
oh yeah, how the hell do you do this?
oh yeah, how the hell do you do this?
Huh?
weird man.
We got another almost an hour.
Right.
Let me figure this out. Where's the
Okay, so you have right here
Okay, so you have right here
rewards
times Adams. Okay, so you actually
right there's this whole bunch of weird
right there's this whole bunch of weird
projection math
projection math
and this is where uh the HL Gaus
and this is where uh the HL Gaus
comes in instead,
right?
Yes, this is where HL Gaus is supposed
Yes, this is where HL Gaus is supposed
to come in instead.
to come in instead.
This is supposed to be
This is supposed to be
Oh, right. Because you know what it is?
Oh, right. Because you know what it is?
It's the first step of HL Gaus, right?
So, you have
Where is this dude?
Here's your support.
Take your input.
just
it needs to be applied to.
I wish I had like an actually good
I wish I had like an actually good
implementation of this to reference.
implementation of this to reference.
I could try Lucid Drains and like just
I could try Lucid Drains and like just
hope it's good.
I implement literally everything.
Okay. So you can this is what you would
Okay. So you can this is what you would
do, right? So it's transform from logs
do, right? So it's transform from logs
and then it calls transform from probs.
Going to do probs time centers.
That's it.
Forward
transform from logic.
Super confusing the way the stuff is
Super confusing the way the stuff is
written.
I trust Grock to not screw up the
I trust Grock to not screw up the
freaking math if I just want
freaking math if I just want
mathematical description of this thing.
mathematical description of this thing.
Really don't is the problem.
tried to write a thing here, didn't it?
tried to write a thing here, didn't it?
Targets.
Targets.
Scalar TD targets.
Scalar TD targets.
This seems wrong to me that you have
This seems wrong to me that you have
scalar TD targets,
scalar TD targets,
right?
There was another thing in here.
Not this. Where is it?
E51. You apply the full distributional
E51. You apply the full distributional
delman Bellman operator.
you compute the scaler. Okay, so
you compute the scaler. Okay, so
actually this is correct. That's so
actually this is correct. That's so
bizarre though.
Compute the scalar TD targets
Compute the scalar TD targets
and then you transform into the target
and then you transform into the target
distrib using HL Gaus.
That's bizarre, right?
That's bizarre, right?
You compute a full distribution and then
You compute a full distribution and then
you turn it back into a scaler and then
you turn it back into a scaler and then
you repro it into a different target
you repro it into a different target
distribution.
So weird.
Just predict them. Oh, you know why you
Just predict them. Oh, you know why you
don't predict the scaler? It's I know
don't predict the scaler? It's I know
why. It's because uh you can't train
why. It's because uh you can't train
that network, right? So, this makes the
that network, right? So, this makes the
network match the Q network. So, you can
network match the Q network. So, you can
actually train. Okay, so I actually see
actually train. Okay, so I actually see
why why you would have to do it this
why why you would have to do it this
way.
way.
The chat broken? Nobody said anything in
The chat broken? Nobody said anything in
a minute here. I mean, I guess I am kind
a minute here. I mean, I guess I am kind
of just doing crazy algorithm dev at
of just doing crazy algorithm dev at
this point.
this point.
Nope, it's not. There you go. Perfectly
Nope, it's not. There you go. Perfectly
timed A1. It's like one hot encoding of
timed A1. It's like one hot encoding of
a label
a label
into a cross entry.
into a cross entry.
I don't think it is. It's more like
I don't think it is. It's more like
let's say that you had a full
let's say that you had a full
probability distribution as your target.
probability distribution as your target.
Okay. And then you turn it into a one
Okay. And then you turn it into a one
hot and then you turn it back into a
hot and then you turn it back into a
distribution. That's what it's like,
distribution. That's what it's like,
which is crazy.
which is crazy.
But this does make me think that I have
But this does make me think that I have
it done correctly.
I guess the question is like why the
I guess the question is like why the
heck does this do work uh why does this
heck does this do work uh why does this
do better than C-51 that actually keeps
do better than C-51 that actually keeps
the full distribution? I don't know.
Does this even do anything at scale? Or
Does this even do anything at scale? Or
is like or is the entire foundation of
is like or is the entire foundation of
offpaul RL just built on a bunch of dumb
offpaul RL just built on a bunch of dumb
hacks like weren't fully validated? Who
hacks like weren't fully validated? Who
knows?
trying to understand.
trying to understand.
Uh, that makes two of us.
What the heck did they do? They just
What the heck did they do? They just
wrote a ton of math for something that
wrote a ton of math for something that
always ends up being very, very silly.
But let me see. So I have
But let me see. So I have
wait I can actually compute this now
wait I can actually compute this now
though right because
so this is I have my target Q right
so here this is target Q
so here this is target Q
and then this is
and
okay. So if I do this right, I can just
okay. So if I do this right, I can just
do Q
do Q
then this is going to be target
then this is going to be target
and I can just compute this loss and
and I can just compute this loss and
then basically this is
then basically this is
this should give me
this should give me
like the mean squared error loss version
like the mean squared error loss version
of this. Hang on.
So this is not the full power of the
So this is not the full power of the
algorithm at the point at this point,
algorithm at the point at this point,
right? This is supposed to just be
right? This is supposed to just be
project everything and then you kind of
project everything and then you kind of
project it back so it's the same bloody
Okay, this is your queue.
Okay. So now
stable.
stable.
Okay. So this is like reasonably decent
Okay. So this is like reasonably decent
as it flops a bunch.
course and it like flops a bunch at the
course and it like flops a bunch at the
end and totally screws up. But that
end and totally screws up. But that
actually did solve the task. Okay, so
actually did solve the task. Okay, so
what I did here is I did all of the
what I did here is I did all of the
probabilistic I did all the projections.
Well, I did the not the HL gas project.
Well, I did the not the HL gas project.
I did the I computed the PMF and then I
I did the I computed the PMF and then I
projected it back into Q values and then
projected it back into Q values and then
just did MSE.
just did MSE.
Okay.
And now next
what we do is the full HL G
for this.
Take
for this. We go here.
A PMFS.
A M apps
A M apps
2 A U
2 A U
512 64
512 64
and we do
target equals HL G Target
target
one
do hl g on our target
put it back into
shape that we understand
shape that we understand
this.
Okay,
then
then
target.
The Q probs should have just been
soft max
should be something like this.
[Music]
Plug this in real quick.
What is an HL Gaus?
What is an HL Gaus?
It's a lo it's a distributional loss
It's a lo it's a distributional loss
function or it's a dist it's a
function or it's a dist it's a
transformation that's used to create a
transformation that's used to create a
distributional loss function rather
distributional loss function rather
I guess is the simplest way of putting
I guess is the simplest way of putting
it. Um the really simple way of putting
it. Um the really simple way of putting
it is instead of estimating values or Q
it is instead of estimating values or Q
functions like as continuous numbers you
functions like as continuous numbers you
bucket them a bunch and then you write
bucket them a bunch and then you write
loss function over the bucketed values
loss function over the bucketed values
instead.
instead.
And this is supposed to do something,
And this is supposed to do something,
but um
but um
we're not exactly fully seeing it yet.
we're not exactly fully seeing it yet.
Heavy lift G.
Heavy lift G.
Oh, I prefer that.
Oh, I prefer that.
Yeah, the heavy lifting function. Good.
Certainly feels like heavy lifting
Certainly feels like heavy lifting
trying to freaking get it to work.
It's
It's
Oh, I am realizing that I did not shift
Oh, I am realizing that I did not shift
them by one.
This at least
really doesn't work super well, huh?
Uh,
Uh,
yeah, I think that's something
yeah, I think that's something
different.
I think that's something different.
It's uh it's this. The heck does it even
It's uh it's this. The heck does it even
stand for?
stand for?
I don't actually know what it stands
I don't actually know what it stands
for.
It's from this paper though.
are better at class.
Try again with I don't know what mids
Try again with I don't know what mids
is.
I don't know what mids is ora. I don't
I don't know what mids is ora. I don't
know what either of those programs are.
I've tried so many different variations
I've tried so many different variations
on this and it doesn't seem to work
on this and it doesn't seem to work
better. Online mask.
What?
I didn't know about that.
I didn't know about that.
particularly for online programs. I
particularly for online programs. I
don't know about that at all.
It's hard for me to say because like I
It's hard for me to say because like I
mean I I'm not the type of person who
mean I I'm not the type of person who
like will look at credentials and hire
like will look at credentials and hire
anybody based on that because I know
anybody based on that because I know
that that like the credentials never
that that like the credentials never
tell the full story.
What am I doing wrong here?
We go to unshared.
We go to unshared.
Let's go to unshared just to be
Let's go to unshared just to be
absolutely sure,
absolutely sure,
right? Because that technically could be
right? Because that technically could be
a thing.
That would suck if that were the uh the
That would suck if that were the uh the
problem,
problem,
but we will see.
Go to unshared here, right?
There's Yeah, that's something totally
There's Yeah, that's something totally
different.
different.
Yellow is something totally different.
Yellow is something totally different.
Just an activation function. has nothing
Just an activation function. has nothing
to do with what I'm working on. I'm
to do with what I'm working on. I'm
using it, but it has nothing to do with
using it, but it has nothing to do with
what I'm working on.
Okay, there's your target net.
So, here's your target.
So, here's your target.
Got the data copy in.
Got the data copy in.
Now let's do
already have this bit
I'd say coursework really shouldn't be
I'd say coursework really shouldn't be
that hard, but like any of the courses
that hard, but like any of the courses
are basically designed to screw with you
are basically designed to screw with you
rather than to teach you things.
Ah, okay. Hang on.
Well, it's super jumpy, but like
Well, it's super jumpy, but like
we're actually seeing something there
we're actually seeing something there
right
right
now with it unshared.
I suppose the number of updates needs to
I suppose the number of updates needs to
be tuned and such.
There's a huge amount of variability in
Total mini badge.
You know how people can make um like
You know how people can make um like
programs that are just dramatically more
programs that are just dramatically more
complicated than they need to be for no
complicated than they need to be for no
apparent reason? You can do the same
apparent reason? You can do the same
thing with math. It turns out
thing with math. It turns out
the exact same thing with math.
Okay, there's a consistent update
Okay, there's a consistent update
schedule.
Okay, that's way better, right?
It's like I think probably whenever the
It's like I think probably whenever the
target gets hit it like crushes it for a
target gets hit it like crushes it for a
second.
second.
But this is chill.
But this is chill.
That actually made a big difference.
That actually made a big difference.
Okay, so we actually kind of have this
Okay, so we actually kind of have this
working then, right?
working then, right?
That's HL Gausmented.
Um, code's a mess.
Um, code's a mess.
But we do have distributional RL working
plus.
I like C because I like simple things. I
I like C because I like simple things. I
actually I really hate puzzles, you
actually I really hate puzzles, you
know. I hate like clever things. I like
know. I hate like clever things. I like
simple things that work.
simple things that work.
I like it when I can take very hard
I like it when I can take very hard
problems.
problems.
They're like super obnoxious and the
They're like super obnoxious and the
solutions are almost not even worth
solutions are almost not even worth
their weight and like to just cut them
their weight and like to just cut them
all out and do simple things that work.
all out and do simple things that work.
That's what I like.
I'd rather be useful than clever.
Oh, that's the thing that everyone has
Oh, that's the thing that everyone has
to decide for them. Rather feel clever
to decide for them. Rather feel clever
or would you rather be useful?
They're often directly at odds.
Because in fact the least clever
Because in fact the least clever
solution typically among all of them the
solution typically among all of them the
best
best
the easiest to freaking follow and
the easiest to freaking follow and
extend.
I will be very very happy if at the end
I will be very very happy if at the end
of the day I run all the experiments I
of the day I run all the experiments I
test all these methods and I find that
test all these methods and I find that
most of the complicated things are a
most of the complicated things are a
waste of time. I just have this simple
waste of time. I just have this simple
algorithm that like is super fast, it's
algorithm that like is super fast, it's
very easy and just solves all the
very easy and just solves all the
problem. RL becomes a mature field a
problem. RL becomes a mature field a
high performance implementation of that
high performance implementation of that
with extensive validation.
with extensive validation.
Pretty much any problem you can want
Pretty much any problem you can want
it'll solve better than about any of the
it'll solve better than about any of the
other existing algorithms.
other existing algorithms.
No real edge cases works. That is the
No real edge cases works. That is the
goal.
goal.
That is what I would like to see happen.
Well, this gives us um this does give us
Well, this gives us um this does give us
our distributional RL.
We had to do unshared networks to make
We had to do unshared networks to make
it work, but we did do it.
So I guess tomorrow would be um well
So I guess tomorrow would be um well
tomorrow I have other stuff but the next
tomorrow I have other stuff but the next
big chunk on this will be retrace,
right?
And then once we have once we have this
And then once we have once we have this
into retrace,
into retrace,
pretty much this will be at
pretty much this will be at
this will be at the point where it has
this will be at the point where it has
enough of the bells and whistles that
enough of the bells and whistles that
like a clean version of this should
like a clean version of this should
already be enough,
already be enough,
right? like that. I would expect
right? like that. I would expect
um
um
HL Gaus version of this with retrace to
HL Gaus version of this with retrace to
be competitive with our existing method
be competitive with our existing method
if it's any good.
if it's any good.
Not any good,
Not any good,
different story.
We could technically also mess with the
We could technically also mess with the
replay buffer stuff.
We'll get to that later.
different.
different.
Oh jeez.
It's a good start. It's a good starting
It's a good start. It's a good starting
point. Um
point. Um
else I can do
really
really
kind of the
kind of the
main main thing for now is set up.
main main thing for now is set up.
Do I need to do retrace before anything
Do I need to do retrace before anything
else?
Probably do.
Probably do.
I probably do need retrace, right?
I probably do need retrace, right?
Like otherwise you have no advantage
Like otherwise you have no advantage
function basically.
function basically.
You're doing like one step bootstrap.
You're doing like one step bootstrap.
Yeah. Retrace will be next session.
Yeah. Yeah, I mean the the best people
Yeah. Yeah, I mean the the best people
around here are just the people stuck
around here are just the people stuck
around and stuff properly.
around and stuff properly.
I haven't even asked anybody that I've
I haven't even asked anybody that I've
brought on for contracts about their
brought on for contracts about their
credentials. I have people emailing me
credentials. I have people emailing me
resumes all the time and it's like what
resumes all the time and it's like what
am I supposed to do with that?
Yeah.
Yeah.
Okay. So, I got to go get ready for uh
Okay. So, I got to go get ready for uh
for dinner. I'm meeting somebody for
for dinner. I'm meeting somebody for
dinner today.
dinner today.
Tomorrow as well. Tomorrow I have um
Tomorrow as well. Tomorrow I have um
most of the day I have
most of the day I have
uh I've got a meeting to go to and I got
uh I've got a meeting to go to and I got
to set stuff up for that. But
I will be working on either tomorrow or
I will be working on either tomorrow or
Friday most likely. Uh we're going to
Friday most likely. Uh we're going to
figure out retrace. I'm going to do a
figure out retrace. I'm going to do a
little bit of reading offline on retrace
little bit of reading offline on retrace
to see what I can figure out.
to see what I can figure out.
Very fancy method. I bet you that it uh
Very fancy method. I bet you that it uh
it boils down to like not that much
it boils down to like not that much
this paper here
and mod game fun.
and mod game fun.
That's fine. Generally people should at
That's fine. Generally people should at
least at some point go through an intro
least at some point go through an intro
course so you know like what a hashmap
course so you know like what a hashmap
is but um
is but um
also basics of how your computer works,
also basics of how your computer works,
what an operating system does
what an operating system does
at least a little bit but other than
at least a little bit but other than
that yeah anyways
that yeah anyways
uh thank you folks. I will be back
uh thank you folks. I will be back
probably tomorrow. Thank you for tuning
probably tomorrow. Thank you for tuning
in. Uh, it's going to be a few days of
in. Uh, it's going to be a few days of
this I think to like get an off policy
this I think to like get an off policy
thing in. But in my mind, I really want
thing in. But in my mind, I really want
to give off policy a proper try. Like I
to give off policy a proper try. Like I
want to be very very confident if off
want to be very very confident if off
policy stuff doesn't work that it's not
policy stuff doesn't work that it's not
because I've screwed it up and I want a
because I've screwed it up and I want a
clear-cut reason as to why uh it doesn't
clear-cut reason as to why uh it doesn't
work.
work.
But we're going to push the experiments
But we're going to push the experiments
quite far there.
quite far there.
So, thank you folks. Back tomorrow.
