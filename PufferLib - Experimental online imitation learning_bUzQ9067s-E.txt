Kind: captions
Language: en
Okay.
Okay.
Should be back live.
Should be back live.
Hi.
I have returned from RLC.
I have returned from RLC.
It's a heck of a week. Busy. Not as easy
It's a heck of a week. Busy. Not as easy
as a conference to work as I thought it
as a conference to work as I thought it
was going to be. Um, but it was good.
was going to be. Um, but it was good.
Uh, particularly with the award.
Uh, particularly with the award.
Huffer won a best paper award
for resourcefulness and reinforcement
for resourcefulness and reinforcement
learning. just posted it five minutes
learning. just posted it five minutes
ago here
and the full talk is now available
and the full talk is now available
on X as well as
on X as well as
I can get rid of the horrible
I can get rid of the horrible
default
default
uh recommendations
uh recommendations
right here.
Watch it on YouTube. You can watch it on
Watch it on YouTube. You can watch it on
X
X
quite nice
quite nice
that.
that.
And um
And um
hopefully with that we should get
hopefully with that we should get
another little spike in growth as people
another little spike in growth as people
come see the library.
come see the library.
Want to keep that trend going. Guar the
Want to keep that trend going. Guar the
puffer. All right. So we have some fun
puffer. All right. So we have some fun
stuff to do today. Um,
stuff to do today. Um,
so I have a lot of research ideas
so I have a lot of research ideas
uh following the conference and we're
uh following the conference and we're
going to just start implementing them.
going to just start implementing them.
Congrats on the award. Did you do
Congrats on the award. Did you do
anything special to your stream post
anything special to your stream post
today to have it send mobile
today to have it send mobile
notifications? No, I did not. Um, I
notifications? No, I did not. Um, I
didn't do anything. There's not even
didn't do anything. There's not even
anything that you can do on that for
anything that you can do on that for
uh for X. Like that's just not a thing.
It's just if you follow then it will
It's just if you follow then it will
sometimes show up either in your feed or
sometimes show up either in your feed or
on the like the right side of your um
on the like the right side of your um
your Twitter account.
Uh I'm going to try
Uh I'm going to try
since I just got back from RLC.
since I just got back from RLC.
I want to try a bunch of kind of crazy
I want to try a bunch of kind of crazy
stuff on the research side.
I'm going to talk a little bit about um
I'm going to talk a little bit about um
where's that upper max one?
Not it.
We're going to try on the remote
We're going to try on the remote
machine.
Uh are my machines down
Uh are my machines down
or am I just on the wrong terminal? Hang
on. Something is off. Let me figure out
on. Something is off. Let me figure out
why um
why um
why this is not working
machines are down on tail scale or not.
Uh, why the hell are all of my machines
Uh, why the hell are all of my machines
down?
Okay, nobody told me about that.
I have to go take a quick call,
I have to go take a quick call,
I think.
I think.
Yeah, I got to go take a quick call. I
Yeah, I got to go take a quick call. I
have no idea why the hell all of my
have no idea why the hell all of my
machines are down,
machines are down,
but I will go do that and then we will
but I will go do that and then we will
be we will be right
lovely.
lovely.
Oh man,
Oh man,
guys that I have there aren't even there
guys that I have there aren't even there
to reboot the machine. So, I'm going to
to reboot the machine. So, I'm going to
have to work on this locally for a bit
have to work on this locally for a bit
and then in a couple hours they should
and then in a couple hours they should
bring them back online.
bring them back online.
Hello, FBRR. I've seen you in the
Hello, FBRR. I've seen you in the
Discord. Thank you very much. Um, and
Discord. Thank you very much. Um, and
hey, James, it was good. Got the award
hey, James, it was good. Got the award
and everything. Um, talk to a bunch of
and everything. Um, talk to a bunch of
people. Have some research to do. We're
people. Have some research to do. We're
going to have to steer a little bit
going to have to steer a little bit
though. I guess I'll just have to
though. I guess I'll just have to
implement on my local.
implement on my local.
Hey, that's toine. How's it going, man?
What's good? Um, buffer is doing well. I
What's good? Um, buffer is doing well. I
have a bunch of research to do. I got to
have a bunch of research to do. I got to
take a look at the Robocode stuff if
take a look at the Robocode stuff if
you've done anything there because if
you've done anything there because if
actually that'd be a really good
actually that'd be a really good
environment to have uh merged in. But
environment to have uh merged in. But
yeah, so
yeah, so
the slightly annoying thing about this
the slightly annoying thing about this
is I literally don't even have my editor
is I literally don't even have my editor
set up on my local
Actually, I do. I have this set up. It's
Actually, I do. I have this set up. It's
just not recognizing my config, I guess.
just not recognizing my config, I guess.
Um,
let me fix this real quick just so I
let me fix this real quick just so I
have something to do for the next couple
have something to do for the next couple
hours. Like, I can actually functionally
hours. Like, I can actually functionally
write some stuff.
I should just be able to grab this from
I should just be able to grab this from
um here and then we should be able to
um here and then we should be able to
get started. Thank you very much, Finn.
get started. Thank you very much, Finn.
I owe you um I owe you some drones, I
I owe you um I owe you some drones, I
think.
think.
Uh, did you send me I'm sorry I've
Uh, did you send me I'm sorry I've
missed a bunch of messages because I've
missed a bunch of messages because I've
just got back from um conference and
just got back from um conference and
crashed for a day or so over the
crashed for a day or so over the
weekend. Did you send me like a new
weekend. Did you send me like a new
ready to order drone thing or not yet?
ready to order drone thing or not yet?
Cuz I'm very happy to just quickly send
Cuz I'm very happy to just quickly send
you a drone if I can get you started on
you a drone if I can get you started on
that.
I didn't realize the polling rate for um
I didn't realize the polling rate for um
telemetrics would be one per second or
telemetrics would be one per second or
whatever. That's insane.
whatever. That's insane.
That's like absolutely insane.
That's like absolutely insane.
So, I got a whole bunch of research to
So, I got a whole bunch of research to
start on, but if you have a parts list
start on, but if you have a parts list
or something that I can just real quick
or something that I can just real quick
order you, I'd be happy to do that.
Oh.
What the heck?
I don't know what the heck happened
I don't know what the heck happened
here.
We're going to give the current setup a
We're going to give the current setup a
bit longer. Maybe we can hack it. Uh, if
bit longer. Maybe we can hack it. Uh, if
you need a drone, I will absolutely just
you need a drone, I will absolutely just
order you one. It's just you're going to
order you one. It's just you're going to
have to find something durable enough
have to find something durable enough
that it's not going to break into a
that it's not going to break into a
million pieces if you crash it
million pieces if you crash it
instantly. But if you find a bigger
instantly. But if you find a bigger
drone that's like several hundred bucks
drone that's like several hundred bucks
or whatever, that's fine. I'll just send
or whatever, that's fine. I'll just send
it to you. I'd very much like to get
it to you. I'd very much like to get
this thing working. Um because pretty
this thing working. Um because pretty
much as just for reference here, right,
much as just for reference here, right,
as soon as you have a cool demo of a
as soon as you have a cool demo of a
drone, uh that's going to let me start
drone, uh that's going to let me start
looking for contracts. And obviously,
looking for contracts. And obviously,
you know, if we get a any sort of
you know, if we get a any sort of
contract with a drone company, I'll be
contract with a drone company, I'll be
able to pull you both in paid on that.
able to pull you both in paid on that.
I'd love to have you guys be able to
I'd love to have you guys be able to
like get some return out of this, work
like get some return out of this, work
on some really cool stuff, right? And
on some really cool stuff, right? And
see it get into into use quickly.
Yeah, somehow this is like the wrong
Yeah, somehow this is like the wrong
noxious, you know. It's probably cuz
noxious, you know. It's probably cuz
it's a bun 220 and it's like super dated
it's a bun 220 and it's like super dated
or whatever.
Even better.
Even better.
Not really at all, right?
Not really at all, right?
Yeah, this dev environment sucks.
Yeah, this dev environment sucks.
I'm trying to think what I can do real
I'm trying to think what I can do real
quick to like fix this or like get
quick to like fix this or like get
something workable. And it's just
something workable. And it's just
this is why I wanted to have my other um
I can kind of just start working on it,
I can kind of just start working on it,
but this is going to be very painful
but this is going to be very painful
with the amount of code that I have to
with the amount of code that I have to
actually edit
to not have like my b like even my super
to not have like my b like even my super
basic dev environment and just be in
basic dev environment and just be in
whatever the hell this this.
There's not an easy way around it,
There's not an easy way around it,
though. I guess I'll just have to deal
though. I guess I'll just have to deal
with it for now, which sucks.
At some point, I'm going to have to get
At some point, I'm going to have to get
like either a new machine or reinstall
like either a new machine or reinstall
this machine or whatever, but we'll
this machine or whatever, but we'll
we'll deal with this for now. Okay, so
we'll deal with this for now. Okay, so
here's the uh the actual plan. Let me
here's the uh the actual plan. Let me
tell you guys a little bit about what I
tell you guys a little bit about what I
want to work on here.
if I just can get myself a couple terms
if I just can get myself a couple terms
open.
open.
Um,
Um,
I got a lot of ideas from RLC about
I got a lot of ideas from RLC about
stuff that is relatively promising.
stuff that is relatively promising.
Uh, some of these are ideas for
Uh, some of these are ideas for
environments. So, I have some cool ideas
environments. So, I have some cool ideas
for a very simple test suite of
for a very simple test suite of
environments is going to let us do a lot
environments is going to let us do a lot
of qualitatively new research. It should
of qualitatively new research. It should
not take me too long. It's just the
not take me too long. It's just the
details are very important. Based on B
details are very important. Based on B
Suite, we'll be an extension to that.
Suite, we'll be an extension to that.
Well, native new environments in Puffer
Well, native new environments in Puffer
of course. Um, but the other thing I
of course. Um, but the other thing I
want to work on is sort of imitation
want to work on is sort of imitation
learning, imitation learning with purely
learning, imitation learning with purely
online data. So, or not purely online
online data. So, or not purely online
data, but no expert uh expert data,
data, but no expert uh expert data,
let's say. So, pretty much you have to
let's say. So, pretty much you have to
collect all your own data, but you don't
collect all your own data, but you don't
do RL at all. You do imitation learning.
do RL at all. You do imitation learning.
And I want to see what that does because
And I want to see what that does because
in my mind at least I'm really not too
in my mind at least I'm really not too
tied to the algorithms uh that we use in
tied to the algorithms uh that we use in
RL at all. Like the important thing
RL at all. Like the important thing
about RL to me is the collection of new
about RL to me is the collection of new
data, the interaction. And if the
data, the interaction. And if the
algorithms change completely and still
algorithms change completely and still
have that property that that works well,
have that property that that works well,
uh that is totally fine by me.
And one of the reasons I want to do this
And one of the reasons I want to do this
is because basically
is because basically
this is the only thing that actually
this is the only thing that actually
seems like it consistently works is
seems like it consistently works is
imitation learning. Um offline RL
imitation learning. Um offline RL
um without like expert data uh off
um without like expert data uh off
policy. I talked to a bunch of people
policy. I talked to a bunch of people
and like it seems like a lot of the even
and like it seems like a lot of the even
the algorithms that are not super fancy
the algorithms that are not super fancy
that have presented as like oh yeah this
that have presented as like oh yeah this
just works they really don't work. Um, a
just works they really don't work. Um, a
lot of the sort of Deep Mind era off off
lot of the sort of Deep Mind era off off
Paul stuff is kind of just like it's not
Paul stuff is kind of just like it's not
like they have a magic thing that's just
like they have a magic thing that's just
way better than anything that we have.
way better than anything that we have.
Essentially, I was kind of under the
Essentially, I was kind of under the
impression that there actually was
impression that there actually was
something to all this sample RL and it
something to all this sample RL and it
seems like at least pure offpaul stuff
seems like at least pure offpaul stuff
doesn't quite work. So, we're going to
doesn't quite work. So, we're going to
try this other new thing.
Okay.
Okay.
And I'm going to do
And I'm going to do
so I get the
just so I get some crazy syntax
just so I get some crazy syntax
highlights.
highlights.
Maximum eye pain. All right. Um, drag
Maximum eye pain. All right. Um, drag
this over. Just need this one file.
this over. Just need this one file.
This can be puffer train puffer
This can be puffer train puffer
park.
park.
Um
2 minutes to train cartpole is
2 minutes to train cartpole is
horrendous.
horrendous.
Why did you switch back to Windows from
Why did you switch back to Windows from
Abuntu? I didn't switch. This is uh this
Abuntu? I didn't switch. This is uh this
is like a random desktop in Palo Alto
is like a random desktop in Palo Alto
that I just happen to have. Yeah, I hate
that I just happen to have. Yeah, I hate
this setup. Believe me, I would like to
this setup. Believe me, I would like to
be on a native Linux machine, but I
be on a native Linux machine, but I
don't want to take the time to go like
don't want to take the time to go like
figure out what data I have on this and
figure out what data I have on this and
reinstall it. I I'm probably just going
reinstall it. I I'm probably just going
to get a new desktop here at some point.
Yeah, believe me, this is awful for
Yeah, believe me, this is awful for
devs. You're way better off on native
devs. You're way better off on native
Linux. All right. Um, I'm trying to
Linux. All right. Um, I'm trying to
think what the best way is to get this
think what the best way is to get this
started.
We need a gold set.
And I just like add this.
And I just like add this.
What if I just add this here?
You only need observation and actions,
You only need observation and actions,
right? We're going to do uh behavioral
right? We're going to do uh behavioral
clothing. I believe that should be all
clothing. I believe that should be all
we need.
we need.
So,
So,
old
old
hobbs.
So now we have gold observations, gold
So now we have gold observations, gold
actions.
I'll leave it as the same size I
I'll leave it as the same size I
suppose.
And
Oh, we need rewards as well, don't we?
Oh, we need rewards as well, don't we?
We need the gold rewards
We need the gold rewards
because otherwise we can't tell when to
because otherwise we can't tell when to
resample, right?
resample, right?
I think we just need
this though. We're not even going to
this though. We're not even going to
need advantage estimate or anything
need advantage estimate or anything
crazy as that is.
Eval will literally be identical.
And then when we get to the start of
And then when we get to the start of
train,
uh this will be like
Oh,
Oh,
I don't like how this didn't uh pull
I don't like how this didn't uh pull
solve.
solve.
We'll do pong instead.
segments horizon. So you thumb this way,
segments horizon. So you thumb this way,
right?
And how do we merge these cleanly?
Way to go with the award. Thank you,
Way to go with the award. Thank you,
Kvert.
Kvert.
I just put the full talk online. So if
I just put the full talk online. So if
anybody wants to watch this talk later,
anybody wants to watch this talk later,
it's on X and it's also on YouTube.
Oh, there you are.
And hopefully you'll forgive the uh
And hopefully you'll forgive the uh
small advertisement.
small advertisement.
We do have a business as well.
announcements incoming project you were
announcements incoming project you were
getting any new there are well there are
getting any new there are well there are
a few other things potentially there are
a few other things potentially there are
a few things in the works uh there's
a few things in the works uh there's
definitely research side stuff to look
definitely research side stuff to look
at so there's this imitation learning
at so there's this imitation learning
thing there new environments to look at
thing there new environments to look at
and there are a few new areas we've been
and there are a few new areas we've been
building stuff out in that aren't yet
building stuff out in that aren't yet
public off topic I want to build a 1v
public off topic I want to build a 1v
one turnbased game four that does
one turnbased game four that does
selfplay target turnbased like to call
selfplay target turnbased like to call
agents turnbas your checkout
agents turnbas your checkout
uh you could modify our connect form to
uh you could modify our connect form to
make it work that way pretty easily.
make it work that way pretty easily.
Yeah. So the one really obnoxious thing
Yeah. So the one really obnoxious thing
with turnbased dems and I and it's just
with turnbased dems and I and it's just
pure infrastructure I haven't found a
pure infrastructure I haven't found a
great way to do this without messing
great way to do this without messing
everything else up uh up yet is you just
everything else up uh up yet is you just
make a multi- aent environment where you
make a multi- aent environment where you
ignore the actions every other time step
ignore the actions every other time step
or whatever for each agent and that
or whatever for each agent and that
slightly messes with advantage
slightly messes with advantage
estimation. It's really not amazing, but
estimation. It's really not amazing, but
um and it's also like a 50x slowdown in
um and it's also like a 50x slowdown in
effective data, but uh that's probably
effective data, but uh that's probably
the best thing to do for now. And at
the best thing to do for now. And at
least for Connect 4, that should be good
least for Connect 4, that should be good
enough. That's what I would just do for
enough. That's what I would just do for
now.
Like selfplay is really obnoxious for
Like selfplay is really obnoxious for
with LSTMs especially to do fully
with LSTMs especially to do fully
correctly in turn-based games
correctly in turn-based games
specifically.
I actually do want to do a bunch of like
I actually do want to do a bunch of like
selfplay RL research soon. Uh I'm going
selfplay RL research soon. Uh I'm going
to I would start with stuff that isn't
to I would start with stuff that isn't
turnbased, right?
turnbased, right?
And then it's just uh we just have to
And then it's just uh we just have to
figure out the little bit of info change
figure out the little bit of info change
for turnbased to make it make more
for turnbased to make it make more
sense.
sense.
So how do I do this here? Um I want to
So how do I do this here? Um I want to
do the port
do the port
both of these
And I do like stacked.
And then
Where's torch argort?
Uh, best
Uh, best
x
All right. So we do like this. Now we
All right. So we do like this. Now we
have the indices.
Then this will be
n equal cells.
Now, how do I index?
Now, how do I index?
What's the best way to do this
What's the best way to do this
operation? I have the indices.
operation? I have the indices.
I want to merge them together.
cuz I can't stack the entire buffer is
cuz I can't stack the entire buffer is
the annoying thing, right?
I guess I could as an initial thing.
I guess I could as an initial thing.
That's so stupid to do it that way
That's so stupid to do it that way
though, isn't it?
What if I just do it this way for now
What if I just do it this way for now
just to see if I can get anything right
just to see if I can get anything right
before I try to make the super optimized
before I try to make the super optimized
implementation?
Yeah. Yeah. Yeah. We'll just do it the
Yeah. Yeah. Yeah. We'll just do it the
stupid way for now, right?
We'll just do all rewards.
Guess I'm refreshed or something. I
Guess I'm refreshed or something. I
usually can't type this fast. That's
usually can't type this fast. That's
kind of funny.
All right. So, now
All right. So, now
we have our rewards tensor. We've got
we have our rewards tensor. We've got
this. Got our best indices,
this. Got our best indices,
observations, actions, old rewards.
Yes. Yeah. Now we have our
Yes. Yeah. Now we have our
our total action set.
I probably want to change a whole bunch
I probably want to change a whole bunch
of this, don't I?
Like a whole bunch of this, right?
cuz we're only training on gold data.
Well,
Well,
It's not entirely clear to me to be
It's not entirely clear to me to be
honest because like
honest because like
it might be that you also want to do
it might be that you also want to do
your RL step. Why don't we just keep
your RL step. Why don't we just keep
both?
both?
We'll keep both.
We'll keep both.
And we'll add this like additional.
We'll add this additional thingy. I'd
We'll add this additional thingy. I'd
say
say
actually we should do this one first,
actually we should do this one first,
right?
right?
Yeah. You know, this is what we should
Yeah. You know, this is what we should
do is actually I see. So, we're going to
do is actually I see. So, we're going to
put this
put this
we're going to put this up here.
we're going to put this up here.
So, this is the original loop, right?
So, this is the original loop, right?
All right. Now, you've done your entire
All right. Now, you've done your entire
loop. Okay. So, now you're going to
loop. Okay. So, now you're going to
resample and fill the gold.
You do not care about advantages.
going to do uniform sampling for now.
and intgments.
and then The shape is going to be
config BPT Horizon. Listen.
All right. So we have right here
All right. So we have right here
we have a random indices for this.
Now we get observations actions don't
Now we get observations actions don't
need anything else at all.
need anything else at all.
The kind of crazy thing about this is
The kind of crazy thing about this is
how simple this is.
how simple this is.
All right now we get observations.
All right now we get observations.
Um,
LSTM state's going to have the exact
LSTM state's going to have the exact
same annoyance as before, but whatever.
same annoyance as before, but whatever.
We just deal with it.
And then you get logits
value
value
that is the policy and you do not need
that is the policy and you do not need
to even sample the logits at all. Right?
Simply take cross entropy. How roughly
Simply take cross entropy. How roughly
does this work? Uh I'm trying to just
does this work? Uh I'm trying to just
set up imitation learning. So the idea
set up imitation learning. So the idea
is that we it's online imitation
is that we it's online imitation
learning. So the idea is that this thing
learning. So the idea is that this thing
is just going to uh gather a bunch of
is just going to uh gather a bunch of
data, sort it so that you have the best
data, sort it so that you have the best
episodes collected or the best segments
episodes collected or the best segments
collected, imitation learn that and then
collected, imitation learn that and then
bootstrap where hopefully you eventually
bootstrap where hopefully you eventually
get uh luckier segments and then you
get uh luckier segments and then you
imitation learn those lucky segments and
imitation learn those lucky segments and
it bootstraps forever without even
it bootstraps forever without even
needing rewards.
needing rewards.
Well, without training on rewards, you
Well, without training on rewards, you
need the rewards, of course. I don't
need the rewards, of course. I don't
know. I had this idea. I have no idea if
know. I had this idea. I have no idea if
it's been done or like if it's been done
it's been done or like if it's been done
competently, which are two very
competently, which are two very
different things. So, I figured I would
different things. So, I figured I would
just try
input target
input
input
budgets
and the target is going to See?
Yeah. Something like this. Yeah.
Yeah. Where'd entropy come from?
We do some something like this. Yeah.
welcome something.
Thank you very much. I just put the talk
Thank you very much. I just put the talk
up on uh on X right here.
up on uh on X right here.
It's also on YouTube.
I'm working on implementing
I'm working on implementing
uh a weird
uh a weird
online imitation learning hybrid as an
online imitation learning hybrid as an
experiment in Puffer Lib right now.
that out. Yep.
It's not the uh recorded talk from RLC.
It's not the uh recorded talk from RLC.
It is a clean recording of the talk from
It is a clean recording of the talk from
uh well of the exact same talk pretty
uh well of the exact same talk pretty
much from home.
much from home.
People like to talk
I recorded it before I left, but then I
I recorded it before I left, but then I
left it um unposted till just now.
Fully recursive learning from its own
Fully recursive learning from its own
learn data pretty much.
Um there's no way that this just worked
Um there's no way that this just worked
first try. So, I'm assuming that this is
first try. So, I'm assuming that this is
not actually running the code that I
not actually running the code that I
think it is.
Exactly.
Exactly.
Pretty good though that I got all the
Pretty good though that I got all the
way down to here before I broke
way down to here before I broke
something.
stack argument tensor.
Yeah, because like the thing is my gripe
Yeah, because like the thing is my gripe
with all the imitation learning work is
with all the imitation learning work is
like oh as an alternative to RL. It's
like oh as an alternative to RL. It's
not an alternative to RL if you rely on
not an alternative to RL if you rely on
expert data. Just not how it works.
expert data. Just not how it works.
Can't even get expert data in a whole
Can't even get expert data in a whole
bunch of domains. But if you can use the
bunch of domains. But if you can use the
same formulation and you can do it with
same formulation and you can do it with
fully online data with of course being
fully online data with of course being
very easy to mix in expert data if you
very easy to mix in expert data if you
have it that is very valid.
I have no idea if it works
I have no idea if it works
cuz uh I mean that's kind of the idea,
cuz uh I mean that's kind of the idea,
right?
It's so funny to me like how I'm writing
It's so funny to me like how I'm writing
this code which would be considered
this code which would be considered
perfectly normal code anywhere other
perfectly normal code anywhere other
than puffer lib and it's like paining me
than puffer lib and it's like paining me
how slow I know this is going to be
how slow I know this is going to be
compared to puffer standards just with
compared to puffer standards just with
like the redundant copies and things.
like the redundant copies and things.
Kind of funny. All
Kind of funny. All
right. was randon.
Okay.
Ah, so this gets
Ah, so this gets
Uh
try this.
Come on. What's wrong with you?
Z Okay.
Yay. There we go.
Yay. There we go.
All right. So, that's your imitation
All right. So, that's your imitation
loss.
Is the build still sad? What build that
Is the build still sad? What build that
guy?
Title of the stream. God damn it. Did I
Title of the stream. God damn it. Did I
forget to
I I totally forgot to do that. All
I I totally forgot to do that. All
right. Here.
right. Here.
Um
Um
online
there.
Uh is this good?
All right. It doesn't uh you can't
All right. It doesn't uh you can't
update X titles, but whatever.
This is why I never switch the title.
This is why I never switch the title.
Everyone always asks me like, "Oh, can
Everyone always asks me like, "Oh, can
you title the stream what it's going to
you title the stream what it's going to
be?" No, cuz I always do this and then I
be?" No, cuz I always do this and then I
forget for the next four days like a
forget for the next four days like a
total goober.
That should be back up.
That should be back up.
Actually, wait. Why the chat hasn't even
Actually, wait. Why the chat hasn't even
been here the whole time? Literally,
been here the whole time? Literally,
nobody bothered to mention to me that I
nobody bothered to mention to me that I
had this thing hidden up here. Really?
had this thing hidden up here. Really?
All right, whatever. We'll be streaming
All right, whatever. We'll be streaming
tomorrow. Should be.
Well, now I have at least I have the
Well, now I have at least I have the
chat back here. We'll put the puffer
chat back here. We'll put the puffer
here. We'll put the chat up here. Um
go here for now.
go here for now.
Actually, can does he fit here? Yeah, he
Actually, can does he fit here? Yeah, he
does.
Buffer. Hey, there. Thought it was by
Buffer. Hey, there. Thought it was by
design today. Nope.
There's the chat.
Yeah, you guys have to tell me when I
Yeah, you guys have to tell me when I
screw random stuff up. All right. I was
screw random stuff up. All right. I was
just thinking about all like, oh, this
just thinking about all like, oh, this
cool research idea and I didn't check
cool research idea and I didn't check
any of this stuff.
This is like uh I had a great lecturer
This is like uh I had a great lecturer
in college who said actually you know if
in college who said actually you know if
I make a mistake I'm like I'm an old guy
I make a mistake I'm like I'm an old guy
I'm supposed to make mistakes or
I'm supposed to make mistakes or
whatever. If you guys don't catch the
whatever. If you guys don't catch the
mistake and just silently nod along it
mistake and just silently nod along it
reflects badly on all of you.
Yeah, that guy.
Okay, so this at least runs.
of course I need a good machine to
of course I need a good machine to
actually run it on.
At least out of the box doesn't do
At least out of the box doesn't do
anything.
Can we figure out why that is?
If you could go back, would you get a
If you could go back, would you get a
PhD again? Ooh.
Trying to think if I could have built
Trying to think if I could have built
this stuff without it.
You know, I honestly think that we're
You know, I honestly think that we're
going to have to wait a couple years to
going to have to wait a couple years to
see the answer to that question with
see the answer to that question with
Puffer Lib. I think we're going to have
Puffer Lib. I think we're going to have
to wait a couple years.
to wait a couple years.
You definitely don't want to ask
You definitely don't want to ask
somebody like right after they uh
somebody like right after they uh
finished their PhD in their first year
finished their PhD in their first year
or two. the longer term thing.
I think the answer is I'd probably be
I think the answer is I'd probably be
happier but accomplish less without it.
happier but accomplish less without it.
So take that for what you will.
Okay, I see. So you're just imitating
Okay, I see. So you're just imitating
garbage here at C.
garbage here at C.
Does it really never get a reward?
Does it really never get a reward?
Asking since you have to apply. Not sure
Asking since you have to apply. Not sure
to focus on top university go to any. Uh
to focus on top university go to any. Uh
I definitely wouldn't
I definitely wouldn't
I definitely wouldn't just like do a PhD
I definitely wouldn't just like do a PhD
for the sake of doing a PhD at a random
for the sake of doing a PhD at a random
university unless you're dead set on
university unless you're dead set on
doing that.
Wouldn't trust you without the PhD
Wouldn't trust you without the PhD
because I only care about credentials.
because I only care about credentials.
That's funny.
There are actually people who think that
There are actually people who think that
way is the thing. Like it actually does
way is the thing. Like it actually does
make a lot of the business side stuff a
make a lot of the business side stuff a
lot easier.
lot easier.
I don't have to waste time establishing
I don't have to waste time establishing
credibility.
This is just not getting copied over or
This is just not getting copied over or
something weird, right?
something is weird if it's just not
something is weird if it's just not
getting any reward ever, right?
Okay.
I suppose it is actually kind of tough
I suppose it is actually kind of tough
uh without the value function
if you don't have a value function of
if you don't have a value function of
some sort, right?
some sort, right?
Like there are environments where you
Like there are environments where you
just get a negative one for dying.
just get a negative one for dying.
So that's going to have you training on
So that's going to have you training on
like random data
actually. Okay. How does cartpole um
actually. Okay. How does cartpole um
work on rewards?
Like this is very slowly going up I
Like this is very slowly going up I
suppose.
Very easy task though.
I was thinking something like breakout
I was thinking something like breakout
would be
would be
way better because that has like a very
way better because that has like a very
consistent chord structure.
We definitely need to get this thing to
We definitely need to get this thing to
train way faster. play with it, right?
Yeah. So, the reward in that buffer goes
Yeah. So, the reward in that buffer goes
up a little bit,
up a little bit,
but this just doesn't work out of the
but this just doesn't work out of the
box at all.
box at all.
Let me think about why why this is be
Let me think about why why this is be
happening here.
We probably should have a way to like
We probably should have a way to like
dump a data set from a good policy as a
dump a data set from a good policy as a
reference, right?
reference, right?
Like if you have good data,
Like if you have good data,
then how well can you learn a policy
then how well can you learn a policy
from that, right?
That's pretty straightforward to come up
That's pretty straightforward to come up
with a way to do that.
You know, I'm actually a little
You know, I'm actually a little
surprised because I would actually
surprised because I would actually
expect this to work a little better out
expect this to work a little better out
of the box. Now, hold on. I want to
of the box. Now, hold on. I want to
think about this a little bit.
is there any way to incorporate a value
is there any way to incorporate a value
function estimate with this?
Like not really, right? Because that's
Like not really, right? Because that's
going to move around. That's the whole
going to move around. That's the whole
on policiness problem.
on policiness problem.
But like how do you judge
ultimately you just judge that a uh a
ultimately you just judge that a uh a
trajectory
trajectory
with the highest reward is better,
with the highest reward is better,
right? That's all you do.
What the heck is wrong with
Wait, why is the imitation loss and
Wait, why is the imitation loss and
entropy identical?
entropy identical?
Oh, hang on. Something's screwy here,
Oh, hang on. Something's screwy here,
right?
How's that make any bloody sense?
Uh, does that make any sense?
doesn't know because it should be
doesn't know because it should be
I guess they could potentially start the
I guess they could potentially start the
same
I'm optimizing entropy. That's a
I'm optimizing entropy. That's a
problem. That's obviously not going to
problem. That's obviously not going to
work.
Oh.
Oh.
Uh, okay.
Wait, entropy definitely should not be
Wait, entropy definitely should not be
the same as this, right? Entropy should
the same as this, right? Entropy should
not take into account the action.
Yeah, see this is logic's time is props.
Yeah, see this is logic's time is props.
There doesn't take into account action.
There doesn't take into account action.
So, how do we get the same number?
So, how do we get the same number?
That's super weird. Now,
like a reasonable thing now.
Well,
Not even optimizing the right thing
Not even optimizing the right thing
remotely.
Well, that'll do it. I wish I had my box
Well, that'll do it. I wish I had my box
back so I could test this faster.
So now they're similar but not
So now they're similar but not
identical.
Go looking for more problems. I didn't
Go looking for more problems. I didn't
realize that was a terrible mistake.
realize that was a terrible mistake.
Um
observation actions.
Okay. Well, that's an immediate
Okay. Well, that's an immediate
improvement, right?
improvement, right?
So obviously we want to run this thing
So obviously we want to run this thing
way faster.
But that actually is at least doing
But that actually is at least doing
something. No.
Yeah. Okay,
Yeah. Okay,
way better.
way better.
Now, as for
Now, as for
how well we can make something like this
how well we can make something like this
do overall, I don't know yet.
do overall, I don't know yet.
Uh, we'd like to see this get above like
Uh, we'd like to see this get above like
the six or seven range, which is kind of
the six or seven range, which is kind of
looking about.
looking about.
If that can do this, then it's actually
If that can do this, then it's actually
potentially feasible.
potentially feasible.
But
yeah, that's actually kind of cool.
yeah, that's actually kind of cool.
So the idea of this thing right is very
So the idea of this thing right is very
very simple.
very simple.
It's kind of draws on some ideas that um
It's kind of draws on some ideas that um
have worked in LLMs as well sort of
have worked in LLMs as well sort of
though I guess be weird to credit it to
though I guess be weird to credit it to
that because it goes way older than that
that because it goes way older than that
but um it's just generating a whole
but um it's just generating a whole
bunch of data and then imitation
bunch of data and then imitation
learning against the best data in there
learning against the best data in there
that you happen to get.
Technically, you could also do like a
Technically, you could also do like a
step of online RL for pretty much free.
step of online RL for pretty much free.
Well, maybe not.
You could definitely mix in online RL
You could definitely mix in online RL
with this if you wanted to, though.
We could try that.
In fact, I might even want to implement
In fact, I might even want to implement
it that way if this if this does like
it that way if this if this does like
anything meaningful.
Like what I basically want to do with
Like what I basically want to do with
this is the Can and we use this to
this is the Can and we use this to
increase sample efficiency
increase sample efficiency
on its own without the RL. I don't know.
on its own without the RL. I don't know.
If it happened to work better without
If it happened to work better without
the RL, that would be insane because
the RL, that would be insane because
that would be like a revolutionary
that would be like a revolutionary
discovery.
discovery.
I highly doubt we get that. I could be
I highly doubt we get that. I could be
wrong, but I highly doubt we get that.
wrong, but I highly doubt we get that.
Time is it 6:19. So, apparently in about
Time is it 6:19. So, apparently in about
less than an hour, I should get my box
less than an hour, I should get my box
back and we can just play around with
back and we can just play around with
some stuff in the meantime.
some stuff in the meantime.
Play around with the hybrid RL thing.
Play around with the hybrid RL thing.
Yeah, there's a lot of stuff we can play
Yeah, there's a lot of stuff we can play
with before we have it fully fully ready
with before we have it fully fully ready
to go. Be right back. Let me use a
to go. Be right back. Let me use a
restroom and grab a drink. Um, this is
restroom and grab a drink. Um, this is
at least a reasonable enough init.
at least a reasonable enough init.
Right back.
Okay. So,
Okay. So,
you know, the more I think about this,
you know, the more I think about this,
the more this makes sense. And actually,
the more this makes sense. And actually,
that's now
that's now
that's better than randomly running
that's better than randomly running
around. Yeah.
around. Yeah.
So, how could this possibly not work,
So, how could this possibly not work,
guys?
Welcome bet.
What if we just do online RL?
We do our exact normal online RL
and we just add this imitation objective
and we just add this imitation objective
over the best data that we've collected.
Completely get around off policy being a
Completely get around off policy being a
thing, right?
Now, I suppose that this result that
Now, I suppose that this result that
we're looking at here is not amazing on
we're looking at here is not amazing on
its own. Right?
its own. Right?
We'll see what score it gets to
at least get like
at least get like
a few points higher. Good.
Uh, this is on CPU bet.
I want to work on something with soft
I want to work on something with soft
rigid physics, something more sim based.
rigid physics, something more sim based.
Any recommendations or ideas?
Any recommendations or ideas?
uh
rigid physics depending on complexity we
rigid physics depending on complexity we
have know our own we just implement our
have know our own we just implement our
own stuff for drones
own stuff for drones
and Sam did that uh that could be done
and Sam did that uh that could be done
and extended to six degree of freedom
and extended to six degree of freedom
arms at like an intermediate skill level
arms at like an intermediate skill level
project intermediate implementation
project intermediate implementation
skill the math is a little tricky
skill the math is a little tricky
soft body I don't know about the
soft body I don't know about the
algorithms You might actually need an
algorithms You might actually need an
engine for that.
engine for that.
I'd go look into some of those other
I'd go look into some of those other
projects though. Like people have
projects though. Like people have
written their own soft body physics
written their own soft body physics
engines
engines
and I'd go look around to see like what
and I'd go look around to see like what
is the minimum complexity thing you need
is the minimum complexity thing you need
in order to make something like that
in order to make something like that
work.
work.
Okay, so this is above random perf here
Okay, so this is above random perf here
for sure.
This is definitely above random perf.
So, let's say that over the next
So, let's say that over the next
hourish, while I'm waiting for my boxes
hourish, while I'm waiting for my boxes
to come back online so I can train at a
to come back online so I can train at a
respectable speed, um,
respectable speed, um,
let's say I get this thing to run with
let's say I get this thing to run with
hybrid online RL imitation learning,
hybrid online RL imitation learning,
right? You do the im you do the RL step
right? You do the im you do the RL step
so that it's on policy roughly and then
so that it's on policy roughly and then
you do as many imitation learning steps
you do as many imitation learning steps
as you want. You have the buffer size
as you want. You have the buffer size
for your goal data be configurable
and then you try to improve the sample
and then you try to improve the sample
efficiency of your online RL using the
efficiency of your online RL using the
imitation learning.
None. All the boxes are screwed or most
None. All the boxes are screwed or most
of the boxes are screwed, but I have
of the boxes are screwed, but I have
somebody that's going to come boot them
somebody that's going to come boot them
in like an hour. Nobody told me that
in like an hour. Nobody told me that
they were screwed or at least nobody
they were screwed or at least nobody
told me loud enough with how fast the
told me loud enough with how fast the
Discord moves while I was at the
Discord moves while I was at the
conference. So, I didn't know to have
conference. So, I didn't know to have
them rebooted.
them rebooted.
They weren't. Looks like they were
They weren't. Looks like they were
going. They went down on August 4th to
going. They went down on August 4th to
me.
me.
at least from um it looks like they've
at least from um it looks like they've
been down for like several days.
Yeah, some of them have autoreboot, some
Yeah, some of them have autoreboot, some
of them don't.
We're going to have to go through and
We're going to have to go through and
reclaim a bunch of boxes cuz I don't
reclaim a bunch of boxes cuz I don't
think everyone's using them.
Yeah. Okay. 8.6. This actually, it's
Yeah. Okay. 8.6. This actually, it's
kind of funny that this just works.
kind of funny that this just works.
Literally, the first thing I implement
Literally, the first thing I implement
works.
It doesn't work great, but it does work
It doesn't work great, but it does work
better than noise.
the monkey ball end if I did rigid body
the monkey ball end if I did rigid body
108 bytes.
108 bytes.
What do you mean it's 108 bytes?
What do you mean it's 108 bytes?
I've done a six degree of freedom arm,
I've done a six degree of freedom arm,
but kinematics go way over my head, so
but kinematics go way over my head, so
it was super slow.
it was super slow.
How did you do a six degree of freedom
How did you do a six degree of freedom
arm without doing kinematics?
arm without doing kinematics?
Math gets a little hairy, you know.
What is online RL getting us that this
What is online RL getting us that this
isn't getting us is my question.
Wait. Like actually.
Well, you do get the value function,
Well, you do get the value function,
right?
Optimal
Optimal
Oh, well, you need to do both uh you
Oh, well, you need to do both uh you
need to do both forward kinematics and
need to do both forward kinematics and
dynamics
dynamics
in order to get like a basic reacher
in order to get like a basic reacher
working.
working.
It's going to be like a fair bit of
It's going to be like a fair bit of
math.
math.
That would actually be a very valuable
That would actually be a very valuable
task to the extent that the first person
task to the extent that the first person
that comes and PRs me like an actually
that comes and PRs me like an actually
good sixth degree of freedom arm at the
good sixth degree of freedom arm at the
level of the drone end. Um, like that's
level of the drone end. Um, like that's
actually probably one of the more
actually probably one of the more
promising ways to get into contracts
promising ways to get into contracts
with us.
Obviously, there's more to it than that.
Obviously, there's more to it than that.
You'd actually have to help us find some
You'd actually have to help us find some
partnerships with robot companies, but
partnerships with robot companies, but
that's probably one of the easier ones
that's probably one of the easier ones
to do.
to do.
Math is a little hairy, though.
Math is a little hairy, though.
Would probably take me like several days
Would probably take me like several days
to figure out.
I'm trying to conceptualize here in my
I'm trying to conceptualize here in my
head.
Let me finish chess. Yeah,
I'm trying to think
the difference between these approaches,
the difference between these approaches,
right?
Yeah, you'll be waiting a while on that
Yeah, you'll be waiting a while on that
convert.
There's an advantage to doing this,
There's an advantage to doing this,
isn't there?
It's slightly obnoxious, I guess, from a
It's slightly obnoxious, I guess, from a
compute perspective because
compute perspective because
what you would like to do, right, is
what you would like to do, right, is
you'd like to add the imitation loss uh
you'd like to add the imitation loss uh
just to the RL loop. But you can't do
just to the RL loop. But you can't do
that because if you do it that way, you
that because if you do it that way, you
make it more off policy, which is bad.
make it more off policy, which is bad.
You want to keep it as on policy as
You want to keep it as on policy as
possible
possible
during the RL loop. And then you want to
during the RL loop. And then you want to
take it off policy with the imitation
take it off policy with the imitation
loop.
loop.
Oh, wait. Doesn't this Wait, does this
Oh, wait. Doesn't this Wait, does this
still screw it up or no?
This probably still screws up the value
This probably still screws up the value
function, doesn't it?
possibly does still screw up the value
possibly does still screw up the value
function. Uh that is a positive result
function. Uh that is a positive result
though from like an early experiment.
There are a lot of different ways we
There are a lot of different ways we
could set this thing up potentially.
I'm trying to RL a chess puzzle.
Uh, it shouldn't because you should be
Uh, it shouldn't because you should be
passing a done signal bet.
passing a done signal bet.
But you can technically, yes, you can
But you can technically, yes, you can
technically mess stuff up depending on
technically mess stuff up depending on
how you it
Yes, you have to be careful.
Well, because it's like
I don't know how you have it be. Is it
I don't know how you have it be. Is it
like ignoring an action? Because it has
like ignoring an action? Because it has
to be able to see. It has to be like
to be able to see. It has to be like
observe the board state, right?
observe the board state, right?
and then it makes its action and then it
and then it makes its action and then it
observes the next state of the board and
observes the next state of the board and
then there has to be a done after that.
then there has to be a done after that.
You basically there's a thing where if
You basically there's a thing where if
you reset it right you can drop either
you reset it right you can drop either
the first observation of the next
the first observation of the next
episode or the last observation of this
episode or the last observation of this
episode. You have to be careful not to
episode. You have to be careful not to
do either of those uh in this case I
do either of those uh in this case I
believe.
Okay. So, this immediately performs
Okay. So, this immediately performs
better. Um, and we can compare this, I
better. Um, and we can compare this, I
believe, to the
believe, to the
we can compare this one to the other
we can compare this one to the other
ones.
ones.
Uh, I definitely need my computer back
Uh, I definitely need my computer back
to be able to run these experiments. So,
to be able to run these experiments. So,
let's just like clean this up and make
let's just like clean this up and make
this reasonable, I think, in the
this reasonable, I think, in the
meantime.
place.
place.
You're going to There's going to have to
You're going to There's going to have to
be one uh extra observation before reset
be one uh extra observation before reset
where the action doesn't do anything
where the action doesn't do anything
basically.
This is stuff you would have figured out
This is stuff you would have figured out
by now if you've been doing your formal
by now if you've been doing your formal
RL
readings in parallel to the
readings in parallel to the
implementation. Yeah.
You did. All right. Do your do your
You did. All right. Do your do your
homework.
Hello Shraan. Welcome.
That's very much how our work. Yes.
That's very much how our work. Yes.
Yes, it does. Bet.
Yes, it does. Bet.
Don't you forget it.
lazy cuz buffer just works. Yeah,
it could potentially be working much
it could potentially be working much
better soon.
Did you get it working, Spencer?
Did you get it working, Spencer?
Hey, look at that. We've got Spencer
Hey, look at that. We've got Spencer
doing real research. Now,
uh, I'm actually have a decent shot at
uh, I'm actually have a decent shot at
solving sample efficiency, or at least
solving sample efficiency, or at least
like majorly improving sample efficiency
like majorly improving sample efficiency
with what I'm doing now.
Do you need um boxes for experiments,
Do you need um boxes for experiments,
Spencer?
Spencer?
Cuz obviously
Cuz obviously
this is going to have to be swept on a
this is going to have to be swept on a
ton of stuff.
Yeah, go for it. I think the tiny ones
Yeah, go for it. I think the tiny ones
are still up. Um,
are still up. Um,
we have a few of the individual boxes
we have a few of the individual boxes
down right now and they should be back
down right now and they should be back
up in less than an hour.
Yeah. So, this is experimental. Uh,
Yeah. So, this is experimental. Uh,
Spencer's doing discrete value
Spencer's doing discrete value
functions. Yeah, Spencer. So, this thing
functions. Yeah, Spencer. So, this thing
I'm working on is pretty cool. Uh, so
I'm working on is pretty cool. Uh, so
what this is going to do is this is
what this is going to do is this is
going to keep the RL objective. HL Gaus
going to keep the RL objective. HL Gaus
and anything that you do it that'll stay
and anything that you do it that'll stay
and it'll stay like our one epoch of RL
and it'll stay like our one epoch of RL
like we normally do. But then it's going
like we normally do. But then it's going
to let you add in uh however much
to let you add in uh however much
imitation learning you want on the best
imitation learning you want on the best
data that you've seen so far. So this
data that you've seen so far. So this
should allow you to keep like uh high
should allow you to keep like uh high
value data around. Yeah, this should
value data around. Yeah, this should
like let you keep high value data around
like let you keep high value data around
and essentially build yourself a data
already applying ideas. Yeah.
already applying ideas. Yeah.
Um, if I get this to work, this would be
Um, if I get this to work, this would be
like puffer four material and this could
like puffer four material and this could
well it would have to work very well.
well it would have to work very well.
But I think algorithmically this is the
But I think algorithmically this is the
type of thing that could definitely
type of thing that could definitely
uh like unlock new applications and be
uh like unlock new applications and be
puffer 4 just straight up.
puffer 4 just straight up.
Like Puffer 4 could literally just end
Like Puffer 4 could literally just end
up being a 500 line change to uh like a
up being a 500 line change to uh like a
400 line change to Puffarel.
I have to do it while they're fresh in
I have to do it while they're fresh in
my head, though, you know. But I sent uh
my head, though, you know. But I sent uh
I did I ran the dish this morning. I did
I did I ran the dish this morning. I did
some weights
some weights
and uh I answered all the emails, all
and uh I answered all the emails, all
the conversations. That's still on
the conversations. That's still on
track, don't worry. Then I'm starting to
track, don't worry. Then I'm starting to
do some of this stuff.
I owe Kathy Woo's group some work. I owe
I owe Kathy Woo's group some work. I owe
clients some work, but have to at least
clients some work, but have to at least
carve out a little bit of time to do
carve out a little bit of time to do
this type of stuff. Yeah.
Are you running stuff on one of them?
Are you running stuff on one of them?
No, not at the moment. And uh I don't
No, not at the moment. And uh I don't
need I don't need either of them for uh
need I don't need either of them for uh
for a bit. So you can just go ahead and
for a bit. So you can just go ahead and
use both.
The only thing that I will need is I'm
The only thing that I will need is I'm
going to need Puffer Max one. um which
going to need Puffer Max one. um which
is like the single 5090 box when it
is like the single 5090 box when it
comes back up because I'm going to set
comes back up because I'm going to set
that up for um imitation learning
that up for um imitation learning
experiments over the next couple Yes.
Uh the point of it bet is to use expert
Uh the point of it bet is to use expert
compute.
best param from
start so that it's quick. Start with
start so that it's quick. Start with
running best breakout params on a few
running best breakout params on a few
others and compare like just get the two
others and compare like just get the two
curves like with HL Gaus and then like
curves like with HL Gaus and then like
with whatever I had in there for the
with whatever I had in there for the
breakout sweeps before on like a few
breakout sweeps before on like a few
other M's like pong, enduro, whatever
other M's like pong, enduro, whatever
else. Um, and then you're going to have
else. Um, and then you're going to have
to rerun an HL G sweep on some of the
to rerun an HL G sweep on some of the
more complex ends. Uh, mazes might be a
more complex ends. Uh, mazes might be a
little tricky because that one's hard to
little tricky because that one's hard to
tune for. If you get better at mazes,
tune for. If you get better at mazes,
that's really good. Obviously, we're
that's really good. Obviously, we're
going to want to do some of the more
going to want to do some of the more
complex ones. We're going to try MOA.
complex ones. We're going to try MOA.
Uh, tuning on neural MMO is hard, but if
Uh, tuning on neural MMO is hard, but if
we can tune and get something on neural
we can tune and get something on neural
MMO, that would be really good.
MMO, that would be really good.
Great job at RLC. Thank you.
Great job at RLC. Thank you.
Conferences are exhausting. They really
Conferences are exhausting. They really
are.
are.
I basically crashed. I got home and I
I basically crashed. I got home and I
was like, "All right, I got 36 hours of
was like, "All right, I got 36 hours of
not doing RL work.
Now I'm back.
Uh, that was kind of the whole
Uh, that was kind of the whole
conference that it was rough.
conference that it was rough.
I'm pretty much good now.
Like a 95%
the game idea
the game idea
mechanics. Yeah, you can do that.
I'm going to work on I'm going to work
I'm going to work on I'm going to work
on like research side stuff for a little
on like research side stuff for a little
bit with this just because I think
bit with this just because I think
there's a good chance that this works.
there's a good chance that this works.
I also have the be sweet thing which is
I also have the be sweet thing which is
I think also a good idea.
There's Jason.
What?
What?
Some facto
that was a
that was a
fly to the previous or what?
Total mini batches.
Total mini batches.
Got total ILIL mini batches.
Then we have IL mini batch size.
kind of it.
and
need two different opt. No, you don't
need two different opt. No, you don't
need two optimizers, right?
Wait, do you need two different
Wait, do you need two different
optimizers if you do two different
optimizers if you do two different
objectives like this?
I guess we'll try it like this first and
I guess we'll try it like this first and
we'll see.
Yeah, we'll just try it like this first
Yeah, we'll just try it like this first
and we'll see, I guess.
Little Need it.
Opera is no
Oh yeah, you can't have these be
Oh yeah, you can't have these be
different, can you? Damn, forgot about
different, can you? Damn, forgot about
that.
that.
Not easily, at least, right?
It kind of sucks. Kind of sucks, doesn't
It kind of sucks. Kind of sucks, doesn't
it?
Good way around this or not really.
really kind of isn't.
You'd have to make average reward work
You'd have to make average reward work
in order for this to make any sense.
decent.
decent.
If this actually runs nepo
I think that this yeah this gives us our
I think that this yeah this gives us our
initial starting point right
idea here have larger batches
do quite a
We do need to go to
selfil section.
Yeah, this is never going to prune
Yeah, this is never going to prune
anything off of this, which is good.
So, we can definitely optimize this, but
So, we can definitely optimize this, but
this is like this should be a decent
this is like this should be a decent
starting point, I would assume.
Things that can go wrong here, right?
You haven't trained the value function.
You haven't trained the value function.
You haven't trained the value function,
You haven't trained the value function,
right?
So I think that what you want to do is
So I think that what you want to do is
you want to compute
you want to compute
you want to compute the value function.
What we would need to do that
new value.
You need the returns. Yeah.
Then to compute the returns,
advantages plus value.
uniformly sampled for now.
Wonder if we want to do the same
Wonder if we want to do the same
sampling thing.
We could pretty easily do that same
We could pretty easily do that same
sampling thing now.
thing like Yes.
Put on the mini battery for now.
We keep around.
Wait, you don't need a clipped value
Wait, you don't need a clipped value
function anymore, do we?
You might need it.
I guess all this stuff I'm thinking
I guess all this stuff I'm thinking
about should be more empirically
about should be more empirically
motivated.
motivated.
Why I need my uh my fast box back so I
Why I need my uh my fast box back so I
can play with this messages. Gotten it
can play with this messages. Gotten it
back.
No messages blocks.
Okay.
Okay.
Well, I can at least put it into place
Well, I can at least put it into place
the code and we can ablade it. That way
the code and we can ablade it. That way
I won't have to implement it after.
Just do.
Yeah, you need like all the freaking
Yeah, you need like all the freaking
things, don't you?
Boing
Boing
ratio advantage.
birds.
If we're able to get this advantage in,
If we're able to get this advantage in,
then we'll be able to prevent the uh the
then we'll be able to prevent the uh the
advantage function from going stale.
advantage function from going stale.
That's the idea. Because if you take the
That's the idea. Because if you take the
um the policy too far off value
um the policy too far off value
and if you take it too far off of the
and if you take it too far off of the
policy,
policy,
it'll collect data correctly,
it'll collect data correctly,
but like the value function will just be
but like the value function will just be
undertrained.
So the hope here is if you keep the
So the hope here is if you keep the
value function fresh,
then you'll still be able to make full
then you'll still be able to make full
use of your on policy data with the
use of your on policy data with the
better policy train via IL.
better policy train via IL.
That's the uh the core idea here.
do this.
Now
Now
I do need to go find this ratio
ratio and self advantage.
This should be
Yeah. So this is your advantage, right?
Then you get MB returns.
You get
Get MB values There.
And where is the
value.
down.
We put here
like this online manipulation learning
like this online manipulation learning
like data aggregation.
like data aggregation.
Uh I looked I was looking around for
Uh I looked I was looking around for
references and I kind of let's see if
references and I kind of let's see if
this is let's see if this matches. Um,
this is let's see if this matches. Um,
so you just you're collecting data
so you just you're collecting data
online, you're filtering it for the best
online, you're filtering it for the best
data and then you do imitation learning
data and then you do imitation learning
on that instead of having any sort of
on that instead of having any sort of
expert data that you're starting off
expert data that you're starting off
with. It's fully bootstrapped
at least that is the goal. Now it's easy
at least that is the goal. Now it's easy
to combine this with online RL
to combine this with online RL
and in that case then what this would
and in that case then what this would
potentially do is uh it would just let
potentially do is uh it would just let
you use less data for the same result.
you use less data for the same result.
Sample efficiency hack.
Sample efficiency hack.
Lots of things we can potentially do
Lots of things we can potentially do
with this.
Is it without reward info? uh the
Is it without reward info? uh the
imitation learning part only uses the
imitation learning part only uses the
reward information in order to decide
reward information in order to decide
what data to keep, right? So you need
what data to keep, right? So you need
something that decides what data you
something that decides what data you
actually want to train on. You're not
actually want to train on. You're not
actually using reward to optimize, but
actually using reward to optimize, but
you're using reward to select your data.
Imitation learning without reward only
Imitation learning without reward only
works if you have an expert data set,
works if you have an expert data set,
which is not what we want to do.
which is not what we want to do.
Filtered behavioral cloning. Yes, except
Filtered behavioral cloning. Yes, except
that like when you talk about these
that like when you talk about these
methods, right, you're almost always
methods, right, you're almost always
talking about using expert data and
talking about using expert data and
we're not,
right? You can filter expert data for
right? You can filter expert data for
the best expert data, but there's no
the best expert data, but there's no
expert at all here.
And that's kind of the cool thing.
Still not offline RL. Offline RL is also
Still not offline RL. Offline RL is also
different and it has its own set of
different and it has its own set of
problems.
problems.
Offline RL is also still typically used
Offline RL is also still typically used
with expert data, right? expert data
with expert data, right? expert data
that includes reward annotations.
that includes reward annotations.
This is not really either of those. I
This is not really either of those. I
don't know if people have done this
don't know if people have done this
before. It seems like something that you
before. It seems like something that you
would probably expect to have been done
would probably expect to have been done
cuz it's pretty simple, but I couldn't
cuz it's pretty simple, but I couldn't
find any references.
Not always.
Not always.
Um I mean if you have examples of online
Um I mean if you have examples of online
offline RL being done with data
offline RL being done with data
collected fully from scratch let me
collected fully from scratch let me
knowly
knowly
random behaviors. That's also not this
random behaviors. That's also not this
though because if you do it with
though because if you do it with
completely random behaviors that will
completely random behaviors that will
only work if your environment is
only work if your environment is
trivial. The thing is that you're
trivial. The thing is that you're
actively collecting new online data.
pretty much any environment for which
pretty much any environment for which
you can train it with purely random data
you can train it with purely random data
is just a trivial problem
as a general rule.
I mean you can construct like degenerate
I mean you can construct like degenerate
counter examples. Um but for the most
counter examples. Um but for the most
part in practice that is true.
Sounds pretty niche. Well, it's niche in
Sounds pretty niche. Well, it's niche in
the sense that I haven't seen research
the sense that I haven't seen research
on it before, but it's not niche in the
on it before, but it's not niche in the
sense that if this works, it will be
sense that if this works, it will be
like it will massively massively improve
like it will massively massively improve
the entire space as a whole immediately.
the entire space as a whole immediately.
Online data is generated by the agent.
Online data is generated by the agent.
Yes, the online data is generated by a
Yes, the online data is generated by a
new tabularasa agent in initialized from
new tabularasa agent in initialized from
scratch that learns over time.
Yeah, Bed is right though. Like we do
Yeah, Bed is right though. Like we do
kind of crazy things around here. And
kind of crazy things around here. And
like we do crazy things in a way that
like we do crazy things in a way that
like even if they've been tried before,
like even if they've been tried before,
uh the way that I'm doing them here is
uh the way that I'm doing them here is
much more likely to work.
All right. Bet.
And so now we have advantages. Yeah.
Well, that's yes, that is obviously
Well, that's yes, that is obviously
true.
Uh,
this work
Okay. So, I think that this is good and
Okay. So, I think that this is good and
we can now try this with um a few
we can now try this with um a few
different combinations of settings
and we can see whether this can be made
and we can see whether this can be made
to work better uh with the IL and the
to work better uh with the IL and the
loop than not. Now, the problem is, of
loop than not. Now, the problem is, of
course, that
course, that
I need my um my box back.
My box back. Yeah.
My box back. Yeah.
And
let me go check if it's back up on tail
let me go check if it's back up on tail
scale.
No, they haven't gotten home to reboot
No, they haven't gotten home to reboot
it yet.
Annoying.
Could go steal box zero for a bit.
Could go steal box zero for a bit.
I might go do that.
Yeah, Spencer.
I'll try seven if I can't get zero
I'll try seven if I can't get zero
working. I use a restroom real quick
working. I use a restroom real quick
though and then uh I mean there's no
though and then uh I mean there's no
point in running a 30 minute experiment.
point in running a 30 minute experiment.
That should take one minute. I'll be
That should take one minute. I'll be
right back.
So, uh, Spencer says he's not using box
So, uh, Spencer says he's not using box
zero for a little bit. Try that.
zero for a little bit. Try that.
Here's what we're going to do. We're
Here's what we're going to do. We're
going to commit this.
That's not bad. 133 lines.
Bye.
much better. Right.
much better. Right.
Get reasonable speeds,
N's losses.
We're going to comment this whole loop
We're going to comment this whole loop
out next for a baseline.
Okay. So, this does worse.
Does worse at the moment.
The normal training perk.
Okay, so we do have stable training on
Okay, so we do have stable training on
the original,
the original,
right?
Funny, it's a lot slower on a 4090 than
Funny, it's a lot slower on a 4090 than
it is on the 5090, but that is stable
it is on the 5090, but that is stable
training,
training,
stable task solve.
90 million steps
90 million steps
pretty much.
pretty much.
That's the baseline to
happens if I just trim the value loss a
happens if I just trim the value loss a
bit. Does this crash it?
Yeah, this nan's out.
Yeah, this nan's out.
Well, it's possible I just have this
Well, it's possible I just have this
thing defined or wrong, All right.
still ns out. Was actually doing pretty
still ns out. Was actually doing pretty
well before it nanned out.
I think what we do with this, right?
No, not that.
This doesn't hand out. It just doesn't
This doesn't hand out. It just doesn't
do as well, right?
At
least this is not completely crashing
least this is not completely crashing
our learning here.
It's worse by a lot. Not completely
It's worse by a lot. Not completely
crashing it though.
The only thing I can think about this is
The only thing I can think about this is
the stale value function, right?
And like somehow
And like somehow
do I just have the value function
do I just have the value function
calculation wrong? Is that all it is?
calculation wrong? Is that all it is?
I can't see what else it could be.
But this seems fine. I guess the other
But this seems fine. I guess the other
thing is it's not clamped,
thing is it's not clamped,
right? The value function is not
right? The value function is not
clamped.
I have to avoid getting too far off the
I have to avoid getting too far off the
freaking policy, isn't it?
Just clamp it. Anyways,
I can technically do it. No.
What if I just do it for the hell of it?
Okay, this is the same clip value loss
Okay, this is the same clip value loss
now from before
man's out
there this other
This is definitely broken, right?
Well, hang on. It's still I was about to
Well, hang on. It's still I was about to
say it's stable, but it's clearly not.
say it's stable, but it's clearly not.
Maybe you need to do plus
train jointly.
hands out at 200. Eh,
hands out at 200. Eh,
now
we not do the entire thing.
and Nan's out at 200 again.
It's awkward.
boards values terminals.
Is there any fundamental reason they
Is there any fundamental reason they
should be unstable? I don't think that
should be unstable? I don't think that
there is.
Right. There's no fundamental thing that
Right. There's no fundamental thing that
uh jumps out at me as to why they should
uh jumps out at me as to why they should
be unstable.
Particularly with a clipped loss like
Particularly with a clipped loss like
this.
Was there any warning or did it just
Was there any warning or did it just
like all of a sudden explode?
Value loss is very low.
Loss is low.
Imitation went to Nan. Then everything
Imitation went to Nan. Then everything
else went to Nanitation.
How did imitation laws go to?
How did imitation laws go to?
They didn't make any sense.
should be like the easiest most stable
should be like the easiest most stable
loss ever. No,
loss ever. No,
the straight up behavioral cloning loss.
It is a cross entropy loss.
So imitation loss goes to Nan
because logic just
policy weights must be screwed up,
policy weights must be screwed up,
right?
Yeah. also nan.
How do we nan out the policy though?
I know it's not fancy fancy cross
I know it's not fancy fancy cross
entropy. It's literally cross entropy
just scaled badly.
just scaled badly.
I don't think it is just scaled badly
I don't think it is just scaled badly
though, right?
though, right?
The value loss being screwy.
still Nan's out
crazy.
Um, it's training like okay, I'd say bet
Um, it's training like okay, I'd say bet
it's not like noticeably
it's not like noticeably
I don't think it's any better than the
I don't think it's any better than the
baseline yet.
This does seem like the type of thing
This does seem like the type of thing
that has to work though.
Yeah, but like the goal is okay, can we
Yeah, but like the goal is okay, can we
solve breakout in fewer steps by
solve breakout in fewer steps by
spending more compute.
Weird to me that we can't
Weird to me that we can't
the value function not want to train.
I'm almost positive that it's the value
I'm almost positive that it's the value
function, right? Like this thing not
function, right? Like this thing not
wanting There.
Yeah.
Yeah.
Do we get in?
Do we get in?
We do this.
Try this only thing.
That's better. Right.
That's no RL.
Yeah, there's no RL here at all.
Yeah, there's no RL here at all.
Betation
puffer.
Okay. I mean, this is like up to 26
Okay. I mean, this is like up to 26
score.
Hey Finn,
welcome.
Very cool. Well, it's not doing super
Very cool. Well, it's not doing super
well yet. It's doing better than random.
well yet. It's doing better than random.
So, the question here, right,
So, the question here, right,
hypothesis one is, can you completely
hypothesis one is, can you completely
replace RL with self imitation? That
replace RL with self imitation? That
seems kind of hard to do without a ton
seems kind of hard to do without a ton
of compute
of compute
um or even if you can do it at all. Uh
um or even if you can do it at all. Uh
option two is like can you improve
option two is like can you improve
sample efficiency?
sample efficiency?
This actually did worse.
should be able to do this and do better.
should be able to do this and do better.
Let's see.
Let's see.
RL for material science. Yeah, we will
RL for material science. Yeah, we will
be doing that as well. Why does that not
be doing that as well. Why does that not
show up in my dashboard? I see it on the
show up in my dashboard? I see it on the
screen.
This doesn't seem to be doing massively
This doesn't seem to be doing massively
better or anything.
Could technically be overfit.
Not doing better though for sure.
Wonder if this does worse.
Oh, this does do work.
Oh, this does do work.
Okay.
Too big.
Well, this will be super sample, right?
Well, this will be super sample, right?
Very fast.
Very fast.
Do you care more about wall clock or
Do you care more about wall clock or
steps? Uh, for your experiments, I would
steps? Uh, for your experiments, I would
say I care more about wall clock.
So, HL G shouldn't make it any slower
So, HL G shouldn't make it any slower
really. So, it should be similar,
really. So, it should be similar,
Spencer.
Spencer.
Like, you should end up with very
Like, you should end up with very
similar results in terms of um wall
similar results in terms of um wall
clock, but like the experiment should
clock, but like the experiment should
take not really any longer to run.
This is interesting. See?
Quite hard to get this thing to
Quite hard to get this thing to
bootstrap.
Yeah. You know, like I would think that
Yeah. You know, like I would think that
doing a longer horizon would help.
Shouldn't be that hard.
Like it is learning something, but it's
Like it is learning something, but it's
not
not
Not a great signal, right?
Learning on segment. Man,
well, wait. Does this this solves cart
well, wait. Does this this solves cart
pole with no RL,
pole with no RL,
right?
That did just solve cart pole, did it
That did just solve cart pole, did it
not?
Believe it did.
Oh, hey. Wait, that almost solves pong
Oh, hey. Wait, that almost solves pong
as well.
as well.
Wait, that's that's almost Pong solved,
Wait, that's that's almost Pong solved,
too.
too.
like 200 bet
like 200 bet
195.
It almost solves pawn.
It almost solves pawn.
Yeah. So, hang on. This there is
Yeah. So, hang on. This there is
something to this. There's something
something to this. There's something
about the structure of the M though.
about the structure of the M though.
Back from the conference, I see any cool
Back from the conference, I see any cool
new inspiration. Yeah. We're currently
new inspiration. Yeah. We're currently
solving RL problems without RL,
solving RL problems without RL,
which is kind of crazy.
Well, no, but it makes sense that this
Well, no, but it makes sense that this
would work.
What about a dense reward like pupper
What about a dense reward like pupper
drone?
The heck happened to drone swap?
trajectories aren't even expert. That's
trajectories aren't even expert. That's
the point.
The full full bootstrap problem.
The full full bootstrap problem.
Uh let me go find another problem that
Uh let me go find another problem that
we can do.
doesn't work on this.
doesn't work on this.
However, you watch what I've missed so
However, you watch what I've missed so
far. I mean, I can give you the TLDDR of
far. I mean, I can give you the TLDDR of
it, right? So, we're doing um
it, right? So, we're doing um
we're saving a buffer of data from the
we're saving a buffer of data from the
agent. The agent's initialized from
agent. The agent's initialized from
scratch. We're saving a big buffer of
scratch. We're saving a big buffer of
data from it. Uh, but we're only saving
data from it. Uh, but we're only saving
the top end trajectory segments like
the top end trajectory segments like
ever from any time that we run the
ever from any time that we run the
agent. And then we're running imitation
agent. And then we're running imitation
learning on those top segments.
Basically, we're seeing if this thing
Basically, we're seeing if this thing
can bootstrap when it from a totally
can bootstrap when it from a totally
random agent. Um,
random agent. Um,
yeah,
worked on. What are some other M's I
worked on. What are some other M's I
would expect like a weak learner to work
would expect like a weak learner to work
on.
I can try like target.
Oh, that doesn't work cuz it's
Oh, that doesn't work cuz it's
multi-discreet, right? I haven't added
multi-discreet, right? I haven't added
multi-isport yet.
Our is probably hard.
Convert single discrete.
Convert single discrete.
Not
Tetris.
Or is going down, not up.
Or is going down, not up.
Racer. We can try that.
Or doesn't get better in Tetris. Odd.
Got like decent gold data.
That doesn't work at all.
I try snake as well.
I try snake as well.
Snake is like a good simple multi task.
Doesn't look right.
Kind of training.
I mean,
I mean,
you can't say that's doing nothing,
you can't say that's doing nothing,
right?
Like actually kind of decent,
especially for like a very first take on
especially for like a very first take on
this.
this.
Four
is not one normalized. So, I actually
is not one normalized. So, I actually
don't know how high it's supposed to go.
don't know how high it's supposed to go.
I think it was like three or four or
I think it was like three or four or
something.
I mean, that does actually do something
I mean, that does actually do something
though.
Tripled score from baseline.
Tripled score from baseline.
Are you sure?
I thought that the baseline was like
I thought that the baseline was like
three or four score. No,
we definitely have signs of life.
Eval will show. Yeah, I'm on a remote
Eval will show. Yeah, I'm on a remote
box. I have to like do something for
box. I have to like do something for
that.
that.
Bye snake.
It's like somehow run out of better
It's like somehow run out of better
data.
That could be an entropy fail though,
That could be an entropy fail though,
right?
right?
Let me try with entropy.
add the uh where's entropy
Is this technique people
Is this technique people
please use for robotics RL?
please use for robotics RL?
No, this isn't like this is new. Um, it
No, this isn't like this is new. Um, it
seems simple enough that somebody
seems simple enough that somebody
probably has done it, but if there is a
probably has done it, but if there is a
reference, I don't know it.
Okay, so snake is not amazing here.
I wonder if it's like stuck on the
I wonder if it's like stuck on the
ability
ability
to have enough reward in one segment or
to have enough reward in one segment or
something.
like the fact that it does cart pull
like the fact that it does cart pull
instantly, right?
instantly, right?
And pretty much pong
And pretty much pong
think is multi multi- aent. Yeah,
think is multi multi- aent. Yeah,
self imitation learn. Is it an actual
self imitation learn. Is it an actual
established thing, Finn, where people
established thing, Finn, where people
like bootstrap imitation learning off of
like bootstrap imitation learning off of
nothing?
Like I've seen people do this for
Like I've seen people do this for
completely different reasons and like
completely different reasons and like
weird context,
weird context,
but I haven't seen this specific thing
but I haven't seen this specific thing
done.
Need to make sure I'm not making that up
Need to make sure I'm not making that up
because there was a paper
Yeah, I know. This is different.
Very different.
Okay. So, this
the fact that it does something like
the fact that it does something like
this, it's like flashes of brilliance,
this, it's like flashes of brilliance,
right?
Oh,
okay. Um,
Hang on, guys. We might have something.
I had a suspicion. I'll explain this in
I had a suspicion. I'll explain this in
a second if I'm right.
Yeah. So, this is what I thought. Um,
Yeah. So, this is what I thought. Um,
so this has the best early curve I've
so this has the best early curve I've
ever seen on Neural MMO 3.
Yeah.
Yeah.
Yeah. Guys, this is the Yeah, this is a
Yeah. Guys, this is the Yeah, this is a
real thing.
No, it's not going to solve it. Let me
No, it's not going to solve it. Let me
explain in a second.
I will explain in a second why this is
I will explain in a second why this is
so important.
Yeah. So, this is pretty much the same
Yeah. So, this is pretty much the same
algorithm as I came up with, I think.
algorithm as I came up with, I think.
Let's see.
No. Well, this portion is the other
No. Well, this portion is the other
thing that I tried pretty much. Um, but
thing that I tried pretty much. Um, but
this is without I'm just doing this
this is without I'm just doing this
portion right now.
There's also prioritize replay as a
There's also prioritize replay as a
potential.
So, this is it is pretty close. I will
So, this is it is pretty close. I will
say it's like it is pretty close.
Who the heck is this paper from?
Who the heck is this paper from?
Some like random paper.
Some like random paper.
Okay.
Okay.
So, here's the thing, guys. So, what's
So, here's the thing, guys. So, what's
hard with neural MMO? Um,
hard with neural MMO? Um,
the reward
the reward
from random actions is incredibly sparse
from random actions is incredibly sparse
at the start. So, like it's very very
at the start. So, like it's very very
difficult to get the reward a few times
difficult to get the reward a few times
at the start. And if you're only getting
at the start. And if you're only getting
a reward like a tiny tiny fraction of
a reward like a tiny tiny fraction of
the time, it's very difficult to latch
the time, it's very difficult to latch
on to that reward and learn.
on to that reward and learn.
But what this does is this is actually
But what this does is this is actually
going to save the trajectory segment
going to save the trajectory segment
that contains the reward and learn on
that contains the reward and learn on
just that. So like let's say you get 10
just that. So like let's say you get 10
million samples and only get like five
million samples and only get like five
good rewards. Well, at least those five
good rewards. Well, at least those five
good rewards are in the batch of s of
good rewards are in the batch of s of
data that you're training on.
the what I don't think that this
the what I don't think that this
algorithm as written is going to like
algorithm as written is going to like
scale to solve the whole task or
scale to solve the whole task or
anything
anything
but like
but like
this is qualitatively very very
this is qualitatively very very
different from just on policy RL
different from just on policy RL
yes bet so it kind of solves
yes bet so it kind of solves
it solves the like the sparse
it solves the like the sparse
exploration in in some sense
This is not world model stuff.
This is not world model stuff.
This is substantially more powerful
This is substantially more powerful
along certain axes,
right? Having a world model does not
right? Having a world model does not
tell you how to get reward. You have to
tell you how to get reward. You have to
roll out like a whole bunch of different
roll out like a whole bunch of different
planning and your planner is going to
planning and your planner is going to
suck at the start. So, that actually
suck at the start. So, that actually
doesn't fix it.
doesn't fix it.
I think it's pretty different from that
I think it's pretty different from that
bet, frankly.
Make sure I'm not trolling here. I'm
Make sure I'm not trolling here. I'm
not. See, this is commented out fully.
Why does this not work on like breakout
Why does this not work on like breakout
or snake or stuff like this?
a little bit more concerned about snake.
a little bit more concerned about snake.
Okay. So breakout has long segments.
I'll be right back. I think about this
So it's not it's not just an entropy
So it's not it's not just an entropy
thing, right?
and set some crazy entropy just to be
and set some crazy entropy just to be
sure.
Oh yeah, we're making sure that we're
Oh yeah, we're making sure that we're
actually training on this, right? Yeah,
actually training on this, right? Yeah,
we've got it.
All right, who has hypothesis?
All right, who has hypothesis?
This is working like ludicrously well on
This is working like ludicrously well on
some environments for some portions and
some environments for some portions and
not so much on others.
You're stuck imitating length 64
You're stuck imitating length 64
segments.
segments.
You don't have like a value function
You don't have like a value function
bootstrap or something like that.
bootstrap or something like that.
Do inverse RL to optimize that for what
Do inverse RL to optimize that for what
would that do for you?
would that do for you?
inver inverse uh inverse RL is to like
inver inverse uh inverse RL is to like
estimate a reward function when you
estimate a reward function when you
don't have the data labeled. We do have
don't have the data labeled. We do have
the data labeled. Why would you do that?
So I mean we can confirm that setting
So I mean we can confirm that setting
the entropy coefficiency for high
the entropy coefficiency for high
destroys the training.
This does confirm though at the very
This does confirm though at the very
least that there are problems where this
least that there are problems where this
idea of keeping around
idea of keeping around
prior good data
prior good data
is very powerful.
solves ponging, solves cart pull. Crazy
solves ponging, solves cart pull. Crazy
good on neural MMO in like seconds.
Actually behaved kind of predictably cuz
Actually behaved kind of predictably cuz
like I was pretty confident it would
like I was pretty confident it would
actually do that for neural MMO.
actually do that for neural MMO.
That's why I ran that experiment.
That's why I ran that experiment.
I didn't just do that randomly. I
I didn't just do that randomly. I
thought, oh, you know, that's an
thought, oh, you know, that's an
environment with super sparse roads, so
environment with super sparse roads, so
it makes sense.
Then why does it just get stuck at a
Then why does it just get stuck at a
point?
Well, Breakout doesn't have the same.
Well, Breakout doesn't have the same.
Okay, there's like some degenerate cases
Okay, there's like some degenerate cases
with Snake, I think.
with Snake, I think.
But with breakout,
But with breakout,
take his multi- agents
one. All right. Well, we'll try
one. All right. Well, we'll try
breakout. Let's see if you can find a
breakout. Let's see if you can find a
reason for breakout. Right. Breakout
reason for breakout. Right. Breakout
here.
here.
It does train.
We only get like 24 score though.
H I'd have to copy it to my local.
H I'd have to copy it to my local.
There's no point. We're very confident
There's no point. We're very confident
in breakout as a task, right?
We've used it extensively.
We've used it extensively.
It's a very small like change.
It's a very small like change.
We haven't changed the end of it all.
Could be. I'd have to copy it over.
I think we have all the information that
I think we have all the information that
we need though. Bet from the design of
we need though. Bet from the design of
the algorithm.
the algorithm.
You collect trajectory segments. You
You collect trajectory segments. You
label them by reward.
That gold reward is telling us the
That gold reward is telling us the
average reward of steps over that
average reward of steps over that
segment. It's still going up the whole
segment. It's still going up the whole
time.
What if breakout also rewards the random
What if breakout also rewards the random
shortest path
filter by all cases the ball starts
filter by all cases the ball starts
initially
initially
straight down reward over trains on that
straight down reward over trains on that
stuff.
Yeah. So, it would technically the best
Yeah. So, it would technically the best
thing for it to do, right,
thing for it to do, right,
uh would be to select the segments where
uh would be to select the segments where
the ball's about to strike the paddle
the ball's about to strike the paddle
and then um the ball strikes the paddle
and then um the ball strikes the paddle
and then it like it hits another brick
and then it like it hits another brick
and bounces it back. It should be able
and bounces it back. It should be able
to fit that in a 64 segment, I would
to fit that in a 64 segment, I would
think.
think.
But you're to you're totally right that
But you're to you're totally right that
it could be something weird about the
it could be something weird about the
trajectory segment bounds. Uh, oddly
trajectory segment bounds. Uh, oddly
though here it this did not do better by
though here it this did not do better by
training on more data potentially
training on more data potentially
overfitting.
Let me think how we do this.
If I do 256, Hey,
We can change BPT horizon, but it's
We can change BPT horizon, but it's
going to change the batch size and other
going to change the batch size and other
things.
It's very hard the way that this is set
It's very hard the way that this is set
up to even include a value function of
up to even include a value function of
any type.
Let's see if we can actually just get it
Let's see if we can actually just get it
to solve the task
to solve the task
um
um
by any sort of
by any sort of
here.
something like it moves corner well but
something like it moves corner well but
tries to stay alive in the uh I actually
tries to stay alive in the uh I actually
know that it cannot be that because that
know that it cannot be that because that
gets you like seven Four.
super long run just to see if it's
super long run just to see if it's
stably increasing.
I mean, so far
I mean, so far
it is like a clean, stable curve.
it is like a clean, stable curve.
I do want to see if it ever gets stuck.
there. Also, we should keep in mind
there. Also, we should keep in mind
breakout is a slow start environment,
breakout is a slow start environment,
right? Where like you have to get to a
right? Where like you have to get to a
certain point before it starts learning
certain point before it starts learning
more quickly. So, this might not be a
more quickly. So, this might not be a
horrible horrible failure, right?
horrible horrible failure, right?
Like if it just hasn't gotten to that
Like if it just hasn't gotten to that
point in the curve early on, it could
point in the curve early on, it could
very well be that once this thing gets
very well be that once this thing gets
to like 50 or whatever score, it just
to like 50 or whatever score, it just
takes off like a rocket.
It does seem to be stuck at
low 30 score.
If we look at the gold reward here,
If we look at the gold reward here,
right,
the gold reward is like not
the gold reward is like not
substantially getting better.
substantially getting better.
It's still going a little bit, but like
It's still going a little bit, but like
this is not good training speed.
No, hang on.
This is also without reuning hypers.
This is also without reuning hypers.
Fair
possible those have shifted. So most of
possible those have shifted. So most of
the hypers that we would even normally
the hypers that we would even normally
tune no longer will exist with this.
We're also we're not getting the same
We're also we're not getting the same
like curve shape. We're not getting like
like curve shape. We're not getting like
the exponential
the exponential
takeoff here, which is
probably the more concerning thing is
probably the more concerning thing is
not seeing the uh the thing like take
not seeing the uh the thing like take
offed
turning.
turning.
No, that should just be like
No, that should just be like
there are problems where at a certain
there are problems where at a certain
point, right, you need to find a little
point, right, you need to find a little
bit of reward and once you do, it's very
bit of reward and once you do, it's very
easy to learn from there. You would
easy to learn from there. You would
think you'd get the same with the
think you'd get the same with the
imitation case, right, where like once
imitation case, right, where like once
you learn to hit the ball enough, you'll
you learn to hit the ball enough, you'll
have rapidly you'll have better games
have rapidly you'll have better games
than others and you should get like a a
than others and you should get like a a
quick signal for how to learn
quick signal for how to learn
Think
how to do this?
Like
if we had expert data, this just works,
if we had expert data, this just works,
right?
We we're going to have to ultimately try
We we're going to have to ultimately try
that I think is going to be the thing
that I think is going to be the thing
for tomorrow because
for tomorrow because
like I've done enough stuff on this to
like I've done enough stuff on this to
see that there is clearly some potential
see that there is clearly some potential
and we're going to need like expert data
and we're going to need like expert data
IL for just for comparison sake like you
IL for just for comparison sake like you
know we train a breakout agent we dump a
know we train a breakout agent we dump a
big data set and we see
big data set and we see
uh how good by comparison we train on
uh how good by comparison we train on
app
like what size data set do we need to
like what size data set do we need to
train a good agent with IIL if we're
train a good agent with IIL if we're
doing that etc etc
will it just go all the way well you got
will it just go all the way well you got
to keep in mind we've been training
to keep in mind we've been training
breakout for nearly a billion steps Now,
oh, this is not great.
oh, this is not great.
Like, it has been stably improving the
Like, it has been stably improving the
whole time, but not anywhere near what
whole time, but not anywhere near what
we would want.
Yeah, it is continuing to improve.
They making the agent that
cycle of marginal.
cycle of marginal.
Um, well, how do we set this up?
Um, well, how do we set this up?
How do we set this up differently?
Like the concept of generating
Like the concept of generating
like the con like imitation learning as
like the con like imitation learning as
a concept works, right? If you have
a concept works, right? If you have
expert data, then you can imitation
expert data, then you can imitation
learn.
learn.
It would seem that you should be able to
It would seem that you should be able to
bootstrap then, right? Like if you just
bootstrap then, right? Like if you just
take your best trials and you pick the
take your best trials and you pick the
lucky ones or whatever. Yeah, we'll do
lucky ones or whatever. Yeah, we'll do
it this way. I mean, I guess technically
it this way. I mean, I guess technically
you can't train on lucky. There are
you can't train on lucky. There are
degeneracies here as well.
Not sure if work. Well, I mean RL works
Not sure if work. Well, I mean RL works
somehow, right?
Yes, bet. That's what we were trying
Yes, bet. That's what we were trying
before. You can do that. There just some
before. You can do that. There just some
issues
actually. How did they do it in here?
actually. How did they do it in here?
Did they update
update their value function.
update their value function.
See, they didn't even update their value
See, they didn't even update their value
function at all here.
function at all here.
Oh, I don't even know if they have a
Oh, I don't even know if they have a
value function. They're just doing like
value function. They're just doing like
reinforce.
reinforce.
No, they do.
Yeah. No, they do.
better at the task.
better at the task.
Um, I don't know if that quite does
Um, I don't know if that quite does
because like
because like
RL is literally learning by trial and
RL is literally learning by trial and
error at the start, right?
error at the start, right?
It's trying to do credit assignment to
It's trying to do credit assignment to
rewards.
Actually, RL is a very hard objective to
Actually, RL is a very hard objective to
learn. Behavioral cloning is a very easy
learn. Behavioral cloning is a very easy
objective to learn.
I mean, we've literally seen it solve.
I mean, we've literally seen it solve.
So, we've seen it solve two easy tasks
So, we've seen it solve two easy tasks
like instantly,
like instantly,
and we've seen it like do better than
and we've seen it like do better than
random on like everything.
random on like everything.
And we've also seen it
we've seen it do
we've seen it do
the first hard exploration bit of a very
the first hard exploration bit of a very
hard task. Way better than our
hard task. Way better than our
state-of-the-art RL.
You know, oddly enough, this would
You know, oddly enough, this would
probably pair really well with search,
probably pair really well with search,
right?
Yeah, this would pair like really well
Yeah, this would pair like really well
with search.
Let's see if we can get the base thing
Let's see if we can get the base thing
to work first.
Yeah, I have this implemented
just scale better.
Well, but Breakout, like the curves for
Well, but Breakout, like the curves for
Breakout, the thing is Breakout and Pong
Breakout, the thing is Breakout and Pong
have very similar curves, but Breakout
have very similar curves, but Breakout
just takes longer.
just takes longer.
Do I have like a breakout curve? I They
Do I have like a breakout curve? I They
look like this. I mean, they look like
look like this. I mean, they look like
the neural MMO curve, honestly. The
the neural MMO curve, honestly. The
breakout curves. It's like Yeah. And
breakout curves. It's like Yeah. And
then it sol like and then it goes up
then it sol like and then it goes up
really fast and solves. Same as pong.
really fast and solves. Same as pong.
Same as a lot of tasks.
Okay, let's try like a slightly dumber
Okay, let's try like a slightly dumber
version of this. E,
version of this. E,
I'll try a slightly dumber version of
I'll try a slightly dumber version of
this.
Oh, okay.
Trying to see if there was like stale
Trying to see if there was like stale
data steness issues.
game plan to weave this into puffer and
game plan to weave this into puffer and
proper move method. The goal is to play
proper move method. The goal is to play
around and see what works, right?
How can I say that this will be the main
How can I say that this will be the main
training method when this is like random
training method when this is like random
new research for an idea I just thought
new research for an idea I just thought
of? Right.
Okay. So, this gets 22.
Okay. So, this gets 22.
If I don't drop the rewards, what do we
If I don't drop the rewards, what do we
get?
get?
0.79 as well.
Okay, so this ended up being it was very
Okay, so this ended up being it was very
slightly better to drop out some indices
like very slightly better.
how jittery.
Like if you made this horizon equal to
Like if you made this horizon equal to
one, you would only ever get data where
one, you would only ever get data where
you have the reward, right?
The fact that you don't have the reward
The fact that you don't have the reward
here is actually kind of bad because you
here is actually kind of bad because you
can't do any credit assignment at all.
You do offline RL instead of behavioral
You do offline RL instead of behavioral
clothing here. Is that the play?
You even set that up. Much harder, isn't
You even set that up. Much harder, isn't
it?
it?
It would make more sense though because
It would make more sense though because
you have the record function, I suppose.
you have the record function, I suppose.
though it makes it harder if you want to
though it makes it harder if you want to
actually add expert data
Why I don't understand why it is the
Why I don't understand why it is the
case that
case that
long and breakout are pretty
long and breakout are pretty
substantially right
substantially right
and breakout are like really
and breakout are like really
substantially
using the same hypers on
I not using the same hypers on them
I'm not using the hypers All
right.
Just a random little Okay.
It doesn't instantly solve our problem
like it does fair bit more work. back.
Okay.
Okay. Next question.
Okay. Next question.
These are separate tunes, right?
These are separate tunes, right?
This
Okay, that is a bit better.
Okay, that is a bit better.
Still not like insta solving our task by
Still not like insta solving our task by
any means.
It still just has this property of being
It still just has this property of being
really slow. Like it's not taking off at
really slow. Like it's not taking off at
all.
all.
get us to 27.
get us to 27.
Let's like
Fact that more updates doesn't do
Fact that more updates doesn't do
anything is also weird to me.
See, like it's not
it's not like
it's not like
it gets like sort of stuck.
it gets like sort of stuck.
And we've seen that if we just run it
And we've seen that if we just run it
for super long, we can get it unstuck.
Slightly worse though.
What else we could do?
You want more research gems like squared
You want more research gems like squared
all for?
Yeah. So, actually that's the other
Yeah. So, actually that's the other
project I have planned. I'm back from
project I have planned. I'm back from
RLC is if you look up B Suite heat mine
RLC is if you look up B Suite heat mine
paper it's like a diagnostic end suite.
paper it's like a diagnostic end suite.
Um
Um
we're going to do a much better version
we're going to do a much better version
of that as a core project. Uh it's not
of that as a core project. Uh it's not
an easy project though, mind you,
an easy project though, mind you,
because like implementing the
because like implementing the
environments is going to be very easy.
environments is going to be very easy.
But um actually coming up with good
But um actually coming up with good
clear-cut environments that are properly
clear-cut environments that are properly
motivated, like a minimal set that
motivated, like a minimal set that
actually tests what you care about is
actually tests what you care about is
hard. very very hard.
Down to help. Sure.
Down to help. Sure.
If you go look at the B Suite paper, I'm
If you go look at the B Suite paper, I'm
warning you that this is like actually
warning you that this is like actually
very hard research. Like this is a task
very hard research. Like this is a task
that I would expect the vast majority of
that I would expect the vast majority of
RL researchers to fail on. Um because if
RL researchers to fail on. Um because if
you look at the B Suite tasks, they're
you look at the B Suite tasks, they're
like kind of motivated, but if you
like kind of motivated, but if you
actually really think about it, like
actually really think about it, like
they actually don't do a good job of of
they actually don't do a good job of of
being like a minimal test suite and they
being like a minimal test suite and they
kind of just throw a bunch of buzzwords
kind of just throw a bunch of buzzwords
out like exploration and generalization
out like exploration and generalization
without really really clearly thinking
without really really clearly thinking
about mechanistically what they're
about mechanistically what they're
testing.
I'd say that this has been pretty good
I'd say that this has been pretty good
progress overall though, right?
progress overall though, right?
We see that the simplest instantiation
We see that the simplest instantiation
of this algorithm
of this algorithm
uh can solve several different
uh can solve several different
environments.
We'll have to think about whether like
We'll have to think about whether like
I mean it would be kind of insane if
I mean it would be kind of insane if
this just replaced all RL, right?
this just replaced all RL, right?
I think that that would be kind of an
I think that that would be kind of an
insane thing to expect.
insane thing to expect.
The question is like does this combine
The question is like does this combine
nicely with RL? Does this benefit from
nicely with RL? Does this benefit from
data scale? Right? There are a lot of
data scale? Right? There are a lot of
things that we'd want to test here.
The fact though that this actually does
The fact though that this actually does
something on neural MMO 3 is kind of
something on neural MMO 3 is kind of
insane.
Like I don't to be clear I don't think
Like I don't to be clear I don't think
that if I just run it for longer on that
that if I just run it for longer on that
it will actually be better. But like the
it will actually be better. But like the
fact that it latches on to something
fact that it latches on to something
that hard that quickly is very
that hard that quickly is very
impressive.
impressive.
Do a quick lunar lander run. We don't
Do a quick lunar lander run. We don't
have lunar lander in here. Did somebody
have lunar lander in here. Did somebody
PR it?
I didn't think it was PR yet.
Is it this invert?
Oh, Kinvert has a bunch of nice little
Oh, Kinvert has a bunch of nice little
PRs.
L.
Yeah, there's no lunar lander, man.
Yeah, there's no lunar lander, man.
Somebody was working on it, I saw, but
Somebody was working on it, I saw, but
it hasn't been PR. I don't know if that
it hasn't been PR. I don't know if that
was you or not.
I'm going to go do some thinking about
I'm going to go do some thinking about
this. I got to go get dinner, folks. Um,
this. I got to go get dinner, folks. Um,
I will be back streaming tomorrow.
But this is pretty cool to get like
But this is pretty cool to get like
initial results day one of this.
initial results day one of this.
I'm going to have to think very
I'm going to have to think very
carefully though about how how I go
carefully though about how how I go
about this and such.
Actually, just commit this before I
Actually, just commit this before I
forget.
my everybody must be dreaming. Uh there
my everybody must be dreaming. Uh there
is the like in like the original lunar
is the like in like the original lunar
lander which is unusably slow. There's
lander which is unusably slow. There's
somebody who is working on Lunar Lander
somebody who is working on Lunar Lander
in the Discord, but I don't I don't
in the Discord, but I don't I don't
think we've gotten a PR from them yet.
think we've gotten a PR from them yet.
So, we kind of can't uh can't do
So, we kind of can't uh can't do
anything with it just yet.
Okay,
Okay,
dropped a quick PR
dropped a quick PR
uh in the
uh in the
dev channel. Let me see. Linky, you
dev channel. Let me see. Linky, you
caught me just in time as I was about to
caught me just in time as I was about to
go.
Is that 800k SPS?
Is that 800k SPS?
Wait, vision test works.
I do need to ask if you'd prefer I
I do need to ask if you'd prefer I
include Rayb's camera as a file or as an
include Rayb's camera as a file or as an
actual project.
actual project.
Do you mean as an actual import?
Do you mean as an actual import?
Uh, you can do either. If you want to
Uh, you can do either. If you want to
just like
just like
if you want to modify our ray download
if you want to modify our ray download
so it includes the camera, that's fine.
so it includes the camera, that's fine.
Either one works.
Either one works.
Wait, 800k SPS? I'm going to have to see
Wait, 800k SPS? I'm going to have to see
this.
this.
If you actually got the thing to render
If you actually got the thing to render
at 800k SPS, that was awesome. Last time
at 800k SPS, that was awesome. Last time
we checked, like it wasn't actually
we checked, like it wasn't actually
computing anything, right?
I will be around tomorrow to look at
I will be around tomorrow to look at
that. You have that to look at.
that. You have that to look at.
If
um
no link key I don't think this is a
no link key I don't think this is a
reasonable thing to include
reasonable thing to include
using rays now what does that mean
using rays now what does that mean
kitting uh I don't think that this is a
kitting uh I don't think that this is a
reasonable thing to include link key
reasonable thing to include link key
Because
Because
if you set it to auto,
if you set it to auto,
like I think it tries to mimic the
like I think it tries to mimic the
number of M's or whatever, you
number of M's or whatever, you
definitely don't want it to be equal to
definitely don't want it to be equal to
just the number of cores regardless of
just the number of cores regardless of
how many M's you have. That will just
how many M's you have. That will just
throw you a hard error if they're not
throw you a hard error if they're not
divisible.
divisible.
Mimicking the camera by doing 64 rays
Mimicking the camera by doing 64 rays
and getting collisions on those.
800k SPS raycast renderer. Huh.
That's kind of funny.
Yeah, that's really funny.
So, I guess for a basic scene, yeah,
So, I guess for a basic scene, yeah,
that works. That's super funny.
that works. That's super funny.
That's actually a good test to have, to
That's actually a good test to have, to
be honest with you.
be honest with you.
I'd be interested to see how it works on
I'd be interested to see how it works on
like my higherend hardware. I'll take a
like my higherend hardware. I'll take a
look through the PRs tomorrow. Is that a
look through the PRs tomorrow. Is that a
satisfactory answer though, Linky? For
satisfactory answer though, Linky? For
now,
now,
I want to head to dinner in a second.
Okay. Well, I'm going to call it for the
Okay. Well, I'm going to call it for the
day, folks. Um, thank you for tuning in.
day, folks. Um, thank you for tuning in.
If you're interested in this work
If you're interested in this work
generally, a few small things in case
generally, a few small things in case
you didn't see it. This
you didn't see it. This
uh talk is on X at J Suarez 5341 same as
uh talk is on X at J Suarez 5341 same as
my Twitch handle everything else. Um my
my Twitch handle everything else. Um my
full talk the uh the paper that won an
full talk the uh the paper that won an
award at RLC only a few minutes like 7
award at RLC only a few minutes like 7
minutes 38 seconds if you want to watch
minutes 38 seconds if you want to watch
that
that
discusses all the stuff in Puffer go
discusses all the stuff in Puffer go
ahead and take that look at that. If
ahead and take that look at that. If
you're interested in my work more
you're interested in my work more
generally or you want to get involved,
generally or you want to get involved,
buffer.ai,
buffer.ai,
star the repo. Really helps me out a
star the repo. Really helps me out a
ton. It's free. Just star it. You can
ton. It's free. Just star it. You can
jump in the Discord if you want to get
jump in the Discord if you want to get
involved with dev. Other than that, on X
involved with dev. Other than that, on X
we have resources. Of course, this is
we have resources. Of course, this is
going to open a popup. But yeah, on X we
going to open a popup. But yeah, on X we
have resources for beginners
have resources for beginners
and just lots of RL material in general.
and just lots of RL material in general.
These guides in particular cover
These guides in particular cover
everything you need to know.
everything you need to know.
So, thank you folks and I will be back
So, thank you folks and I will be back
tomorrow.

Kind: captions
Language: en
Okay.
Okay.
Should be back live.
Should be back live.
Hi.
I have returned from RLC.
I have returned from RLC.
It's a heck of a week. Busy. Not as easy
It's a heck of a week. Busy. Not as easy
as a conference to work as I thought it
as a conference to work as I thought it
was going to be. Um, but it was good.
was going to be. Um, but it was good.
Uh, particularly with the award.
Uh, particularly with the award.
Huffer won a best paper award
for resourcefulness and reinforcement
for resourcefulness and reinforcement
learning. just posted it five minutes
learning. just posted it five minutes
ago here
and the full talk is now available
and the full talk is now available
on X as well as
on X as well as
I can get rid of the horrible
I can get rid of the horrible
default
default
uh recommendations
uh recommendations
right here.
Watch it on YouTube. You can watch it on
Watch it on YouTube. You can watch it on
X
X
quite nice
quite nice
that.
that.
And um
And um
hopefully with that we should get
hopefully with that we should get
another little spike in growth as people
another little spike in growth as people
come see the library.
come see the library.
Want to keep that trend going. Guar the
Want to keep that trend going. Guar the
puffer. All right. So we have some fun
puffer. All right. So we have some fun
stuff to do today. Um,
stuff to do today. Um,
so I have a lot of research ideas
so I have a lot of research ideas
uh following the conference and we're
uh following the conference and we're
going to just start implementing them.
going to just start implementing them.
Congrats on the award. Did you do
Congrats on the award. Did you do
anything special to your stream post
anything special to your stream post
today to have it send mobile
today to have it send mobile
notifications? No, I did not. Um, I
notifications? No, I did not. Um, I
didn't do anything. There's not even
didn't do anything. There's not even
anything that you can do on that for
anything that you can do on that for
uh for X. Like that's just not a thing.
It's just if you follow then it will
It's just if you follow then it will
sometimes show up either in your feed or
sometimes show up either in your feed or
on the like the right side of your um
on the like the right side of your um
your Twitter account.
Uh I'm going to try
Uh I'm going to try
since I just got back from RLC.
since I just got back from RLC.
I want to try a bunch of kind of crazy
I want to try a bunch of kind of crazy
stuff on the research side.
I'm going to talk a little bit about um
I'm going to talk a little bit about um
where's that upper max one?
Not it.
We're going to try on the remote
We're going to try on the remote
machine.
Uh are my machines down
Uh are my machines down
or am I just on the wrong terminal? Hang
on. Something is off. Let me figure out
on. Something is off. Let me figure out
why um
why um
why this is not working
machines are down on tail scale or not.
Uh, why the hell are all of my machines
Uh, why the hell are all of my machines
down?
Okay, nobody told me about that.
I have to go take a quick call,
I have to go take a quick call,
I think.
I think.
Yeah, I got to go take a quick call. I
Yeah, I got to go take a quick call. I
have no idea why the hell all of my
have no idea why the hell all of my
machines are down,
machines are down,
but I will go do that and then we will
but I will go do that and then we will
be we will be right
lovely.
lovely.
Oh man,
Oh man,
guys that I have there aren't even there
guys that I have there aren't even there
to reboot the machine. So, I'm going to
to reboot the machine. So, I'm going to
have to work on this locally for a bit
have to work on this locally for a bit
and then in a couple hours they should
and then in a couple hours they should
bring them back online.
bring them back online.
Hello, FBRR. I've seen you in the
Hello, FBRR. I've seen you in the
Discord. Thank you very much. Um, and
Discord. Thank you very much. Um, and
hey, James, it was good. Got the award
hey, James, it was good. Got the award
and everything. Um, talk to a bunch of
and everything. Um, talk to a bunch of
people. Have some research to do. We're
people. Have some research to do. We're
going to have to steer a little bit
going to have to steer a little bit
though. I guess I'll just have to
though. I guess I'll just have to
implement on my local.
implement on my local.
Hey, that's toine. How's it going, man?
What's good? Um, buffer is doing well. I
What's good? Um, buffer is doing well. I
have a bunch of research to do. I got to
have a bunch of research to do. I got to
take a look at the Robocode stuff if
take a look at the Robocode stuff if
you've done anything there because if
you've done anything there because if
actually that'd be a really good
actually that'd be a really good
environment to have uh merged in. But
environment to have uh merged in. But
yeah, so
yeah, so
the slightly annoying thing about this
the slightly annoying thing about this
is I literally don't even have my editor
is I literally don't even have my editor
set up on my local
Actually, I do. I have this set up. It's
Actually, I do. I have this set up. It's
just not recognizing my config, I guess.
just not recognizing my config, I guess.
Um,
let me fix this real quick just so I
let me fix this real quick just so I
have something to do for the next couple
have something to do for the next couple
hours. Like, I can actually functionally
hours. Like, I can actually functionally
write some stuff.
I should just be able to grab this from
I should just be able to grab this from
um here and then we should be able to
um here and then we should be able to
get started. Thank you very much, Finn.
get started. Thank you very much, Finn.
I owe you um I owe you some drones, I
I owe you um I owe you some drones, I
think.
think.
Uh, did you send me I'm sorry I've
Uh, did you send me I'm sorry I've
missed a bunch of messages because I've
missed a bunch of messages because I've
just got back from um conference and
just got back from um conference and
crashed for a day or so over the
crashed for a day or so over the
weekend. Did you send me like a new
weekend. Did you send me like a new
ready to order drone thing or not yet?
ready to order drone thing or not yet?
Cuz I'm very happy to just quickly send
Cuz I'm very happy to just quickly send
you a drone if I can get you started on
you a drone if I can get you started on
that.
I didn't realize the polling rate for um
I didn't realize the polling rate for um
telemetrics would be one per second or
telemetrics would be one per second or
whatever. That's insane.
whatever. That's insane.
That's like absolutely insane.
That's like absolutely insane.
So, I got a whole bunch of research to
So, I got a whole bunch of research to
start on, but if you have a parts list
start on, but if you have a parts list
or something that I can just real quick
or something that I can just real quick
order you, I'd be happy to do that.
Oh.
What the heck?
I don't know what the heck happened
I don't know what the heck happened
here.
We're going to give the current setup a
We're going to give the current setup a
bit longer. Maybe we can hack it. Uh, if
bit longer. Maybe we can hack it. Uh, if
you need a drone, I will absolutely just
you need a drone, I will absolutely just
order you one. It's just you're going to
order you one. It's just you're going to
have to find something durable enough
have to find something durable enough
that it's not going to break into a
that it's not going to break into a
million pieces if you crash it
million pieces if you crash it
instantly. But if you find a bigger
instantly. But if you find a bigger
drone that's like several hundred bucks
drone that's like several hundred bucks
or whatever, that's fine. I'll just send
or whatever, that's fine. I'll just send
it to you. I'd very much like to get
it to you. I'd very much like to get
this thing working. Um because pretty
this thing working. Um because pretty
much as just for reference here, right,
much as just for reference here, right,
as soon as you have a cool demo of a
as soon as you have a cool demo of a
drone, uh that's going to let me start
drone, uh that's going to let me start
looking for contracts. And obviously,
looking for contracts. And obviously,
you know, if we get a any sort of
you know, if we get a any sort of
contract with a drone company, I'll be
contract with a drone company, I'll be
able to pull you both in paid on that.
able to pull you both in paid on that.
I'd love to have you guys be able to
I'd love to have you guys be able to
like get some return out of this, work
like get some return out of this, work
on some really cool stuff, right? And
on some really cool stuff, right? And
see it get into into use quickly.
Yeah, somehow this is like the wrong
Yeah, somehow this is like the wrong
noxious, you know. It's probably cuz
noxious, you know. It's probably cuz
it's a bun 220 and it's like super dated
it's a bun 220 and it's like super dated
or whatever.
Even better.
Even better.
Not really at all, right?
Not really at all, right?
Yeah, this dev environment sucks.
Yeah, this dev environment sucks.
I'm trying to think what I can do real
I'm trying to think what I can do real
quick to like fix this or like get
quick to like fix this or like get
something workable. And it's just
something workable. And it's just
this is why I wanted to have my other um
I can kind of just start working on it,
I can kind of just start working on it,
but this is going to be very painful
but this is going to be very painful
with the amount of code that I have to
with the amount of code that I have to
actually edit
to not have like my b like even my super
to not have like my b like even my super
basic dev environment and just be in
basic dev environment and just be in
whatever the hell this this.
There's not an easy way around it,
There's not an easy way around it,
though. I guess I'll just have to deal
though. I guess I'll just have to deal
with it for now, which sucks.
At some point, I'm going to have to get
At some point, I'm going to have to get
like either a new machine or reinstall
like either a new machine or reinstall
this machine or whatever, but we'll
this machine or whatever, but we'll
we'll deal with this for now. Okay, so
we'll deal with this for now. Okay, so
here's the uh the actual plan. Let me
here's the uh the actual plan. Let me
tell you guys a little bit about what I
tell you guys a little bit about what I
want to work on here.
if I just can get myself a couple terms
if I just can get myself a couple terms
open.
open.
Um,
Um,
I got a lot of ideas from RLC about
I got a lot of ideas from RLC about
stuff that is relatively promising.
stuff that is relatively promising.
Uh, some of these are ideas for
Uh, some of these are ideas for
environments. So, I have some cool ideas
environments. So, I have some cool ideas
for a very simple test suite of
for a very simple test suite of
environments is going to let us do a lot
environments is going to let us do a lot
of qualitatively new research. It should
of qualitatively new research. It should
not take me too long. It's just the
not take me too long. It's just the
details are very important. Based on B
details are very important. Based on B
Suite, we'll be an extension to that.
Suite, we'll be an extension to that.
Well, native new environments in Puffer
Well, native new environments in Puffer
of course. Um, but the other thing I
of course. Um, but the other thing I
want to work on is sort of imitation
want to work on is sort of imitation
learning, imitation learning with purely
learning, imitation learning with purely
online data. So, or not purely online
online data. So, or not purely online
data, but no expert uh expert data,
data, but no expert uh expert data,
let's say. So, pretty much you have to
let's say. So, pretty much you have to
collect all your own data, but you don't
collect all your own data, but you don't
do RL at all. You do imitation learning.
do RL at all. You do imitation learning.
And I want to see what that does because
And I want to see what that does because
in my mind at least I'm really not too
in my mind at least I'm really not too
tied to the algorithms uh that we use in
tied to the algorithms uh that we use in
RL at all. Like the important thing
RL at all. Like the important thing
about RL to me is the collection of new
about RL to me is the collection of new
data, the interaction. And if the
data, the interaction. And if the
algorithms change completely and still
algorithms change completely and still
have that property that that works well,
have that property that that works well,
uh that is totally fine by me.
And one of the reasons I want to do this
And one of the reasons I want to do this
is because basically
is because basically
this is the only thing that actually
this is the only thing that actually
seems like it consistently works is
seems like it consistently works is
imitation learning. Um offline RL
imitation learning. Um offline RL
um without like expert data uh off
um without like expert data uh off
policy. I talked to a bunch of people
policy. I talked to a bunch of people
and like it seems like a lot of the even
and like it seems like a lot of the even
the algorithms that are not super fancy
the algorithms that are not super fancy
that have presented as like oh yeah this
that have presented as like oh yeah this
just works they really don't work. Um, a
just works they really don't work. Um, a
lot of the sort of Deep Mind era off off
lot of the sort of Deep Mind era off off
Paul stuff is kind of just like it's not
Paul stuff is kind of just like it's not
like they have a magic thing that's just
like they have a magic thing that's just
way better than anything that we have.
way better than anything that we have.
Essentially, I was kind of under the
Essentially, I was kind of under the
impression that there actually was
impression that there actually was
something to all this sample RL and it
something to all this sample RL and it
seems like at least pure offpaul stuff
seems like at least pure offpaul stuff
doesn't quite work. So, we're going to
doesn't quite work. So, we're going to
try this other new thing.
Okay.
Okay.
And I'm going to do
And I'm going to do
so I get the
just so I get some crazy syntax
just so I get some crazy syntax
highlights.
highlights.
Maximum eye pain. All right. Um, drag
Maximum eye pain. All right. Um, drag
this over. Just need this one file.
this over. Just need this one file.
This can be puffer train puffer
This can be puffer train puffer
park.
park.
Um
2 minutes to train cartpole is
2 minutes to train cartpole is
horrendous.
horrendous.
Why did you switch back to Windows from
Why did you switch back to Windows from
Abuntu? I didn't switch. This is uh this
Abuntu? I didn't switch. This is uh this
is like a random desktop in Palo Alto
is like a random desktop in Palo Alto
that I just happen to have. Yeah, I hate
that I just happen to have. Yeah, I hate
this setup. Believe me, I would like to
this setup. Believe me, I would like to
be on a native Linux machine, but I
be on a native Linux machine, but I
don't want to take the time to go like
don't want to take the time to go like
figure out what data I have on this and
figure out what data I have on this and
reinstall it. I I'm probably just going
reinstall it. I I'm probably just going
to get a new desktop here at some point.
Yeah, believe me, this is awful for
Yeah, believe me, this is awful for
devs. You're way better off on native
devs. You're way better off on native
Linux. All right. Um, I'm trying to
Linux. All right. Um, I'm trying to
think what the best way is to get this
think what the best way is to get this
started.
We need a gold set.
And I just like add this.
And I just like add this.
What if I just add this here?
You only need observation and actions,
You only need observation and actions,
right? We're going to do uh behavioral
right? We're going to do uh behavioral
clothing. I believe that should be all
clothing. I believe that should be all
we need.
we need.
So,
So,
old
old
hobbs.
So now we have gold observations, gold
So now we have gold observations, gold
actions.
I'll leave it as the same size I
I'll leave it as the same size I
suppose.
And
Oh, we need rewards as well, don't we?
Oh, we need rewards as well, don't we?
We need the gold rewards
We need the gold rewards
because otherwise we can't tell when to
because otherwise we can't tell when to
resample, right?
resample, right?
I think we just need
this though. We're not even going to
this though. We're not even going to
need advantage estimate or anything
need advantage estimate or anything
crazy as that is.
Eval will literally be identical.
And then when we get to the start of
And then when we get to the start of
train,
uh this will be like
Oh,
Oh,
I don't like how this didn't uh pull
I don't like how this didn't uh pull
solve.
solve.
We'll do pong instead.
segments horizon. So you thumb this way,
segments horizon. So you thumb this way,
right?
And how do we merge these cleanly?
Way to go with the award. Thank you,
Way to go with the award. Thank you,
Kvert.
Kvert.
I just put the full talk online. So if
I just put the full talk online. So if
anybody wants to watch this talk later,
anybody wants to watch this talk later,
it's on X and it's also on YouTube.
Oh, there you are.
And hopefully you'll forgive the uh
And hopefully you'll forgive the uh
small advertisement.
small advertisement.
We do have a business as well.
announcements incoming project you were
announcements incoming project you were
getting any new there are well there are
getting any new there are well there are
a few other things potentially there are
a few other things potentially there are
a few things in the works uh there's
a few things in the works uh there's
definitely research side stuff to look
definitely research side stuff to look
at so there's this imitation learning
at so there's this imitation learning
thing there new environments to look at
thing there new environments to look at
and there are a few new areas we've been
and there are a few new areas we've been
building stuff out in that aren't yet
building stuff out in that aren't yet
public off topic I want to build a 1v
public off topic I want to build a 1v
one turnbased game four that does
one turnbased game four that does
selfplay target turnbased like to call
selfplay target turnbased like to call
agents turnbas your checkout
agents turnbas your checkout
uh you could modify our connect form to
uh you could modify our connect form to
make it work that way pretty easily.
make it work that way pretty easily.
Yeah. So the one really obnoxious thing
Yeah. So the one really obnoxious thing
with turnbased dems and I and it's just
with turnbased dems and I and it's just
pure infrastructure I haven't found a
pure infrastructure I haven't found a
great way to do this without messing
great way to do this without messing
everything else up uh up yet is you just
everything else up uh up yet is you just
make a multi- aent environment where you
make a multi- aent environment where you
ignore the actions every other time step
ignore the actions every other time step
or whatever for each agent and that
or whatever for each agent and that
slightly messes with advantage
slightly messes with advantage
estimation. It's really not amazing, but
estimation. It's really not amazing, but
um and it's also like a 50x slowdown in
um and it's also like a 50x slowdown in
effective data, but uh that's probably
effective data, but uh that's probably
the best thing to do for now. And at
the best thing to do for now. And at
least for Connect 4, that should be good
least for Connect 4, that should be good
enough. That's what I would just do for
enough. That's what I would just do for
now.
Like selfplay is really obnoxious for
Like selfplay is really obnoxious for
with LSTMs especially to do fully
with LSTMs especially to do fully
correctly in turn-based games
correctly in turn-based games
specifically.
I actually do want to do a bunch of like
I actually do want to do a bunch of like
selfplay RL research soon. Uh I'm going
selfplay RL research soon. Uh I'm going
to I would start with stuff that isn't
to I would start with stuff that isn't
turnbased, right?
turnbased, right?
And then it's just uh we just have to
And then it's just uh we just have to
figure out the little bit of info change
figure out the little bit of info change
for turnbased to make it make more
for turnbased to make it make more
sense.
sense.
So how do I do this here? Um I want to
So how do I do this here? Um I want to
do the port
do the port
both of these
And I do like stacked.
And then
Where's torch argort?
Uh, best
Uh, best
x
All right. So we do like this. Now we
All right. So we do like this. Now we
have the indices.
Then this will be
n equal cells.
Now, how do I index?
Now, how do I index?
What's the best way to do this
What's the best way to do this
operation? I have the indices.
operation? I have the indices.
I want to merge them together.
cuz I can't stack the entire buffer is
cuz I can't stack the entire buffer is
the annoying thing, right?
I guess I could as an initial thing.
I guess I could as an initial thing.
That's so stupid to do it that way
That's so stupid to do it that way
though, isn't it?
What if I just do it this way for now
What if I just do it this way for now
just to see if I can get anything right
just to see if I can get anything right
before I try to make the super optimized
before I try to make the super optimized
implementation?
Yeah. Yeah. Yeah. We'll just do it the
Yeah. Yeah. Yeah. We'll just do it the
stupid way for now, right?
We'll just do all rewards.
Guess I'm refreshed or something. I
Guess I'm refreshed or something. I
usually can't type this fast. That's
usually can't type this fast. That's
kind of funny.
All right. So, now
All right. So, now
we have our rewards tensor. We've got
we have our rewards tensor. We've got
this. Got our best indices,
this. Got our best indices,
observations, actions, old rewards.
Yes. Yeah. Now we have our
Yes. Yeah. Now we have our
our total action set.
I probably want to change a whole bunch
I probably want to change a whole bunch
of this, don't I?
Like a whole bunch of this, right?
cuz we're only training on gold data.
Well,
Well,
It's not entirely clear to me to be
It's not entirely clear to me to be
honest because like
honest because like
it might be that you also want to do
it might be that you also want to do
your RL step. Why don't we just keep
your RL step. Why don't we just keep
both?
both?
We'll keep both.
We'll keep both.
And we'll add this like additional.
We'll add this additional thingy. I'd
We'll add this additional thingy. I'd
say
say
actually we should do this one first,
actually we should do this one first,
right?
right?
Yeah. You know, this is what we should
Yeah. You know, this is what we should
do is actually I see. So, we're going to
do is actually I see. So, we're going to
put this
put this
we're going to put this up here.
we're going to put this up here.
So, this is the original loop, right?
So, this is the original loop, right?
All right. Now, you've done your entire
All right. Now, you've done your entire
loop. Okay. So, now you're going to
loop. Okay. So, now you're going to
resample and fill the gold.
You do not care about advantages.
going to do uniform sampling for now.
and intgments.
and then The shape is going to be
config BPT Horizon. Listen.
All right. So we have right here
All right. So we have right here
we have a random indices for this.
Now we get observations actions don't
Now we get observations actions don't
need anything else at all.
need anything else at all.
The kind of crazy thing about this is
The kind of crazy thing about this is
how simple this is.
how simple this is.
All right now we get observations.
All right now we get observations.
Um,
LSTM state's going to have the exact
LSTM state's going to have the exact
same annoyance as before, but whatever.
same annoyance as before, but whatever.
We just deal with it.
And then you get logits
value
value
that is the policy and you do not need
that is the policy and you do not need
to even sample the logits at all. Right?
Simply take cross entropy. How roughly
Simply take cross entropy. How roughly
does this work? Uh I'm trying to just
does this work? Uh I'm trying to just
set up imitation learning. So the idea
set up imitation learning. So the idea
is that we it's online imitation
is that we it's online imitation
learning. So the idea is that this thing
learning. So the idea is that this thing
is just going to uh gather a bunch of
is just going to uh gather a bunch of
data, sort it so that you have the best
data, sort it so that you have the best
episodes collected or the best segments
episodes collected or the best segments
collected, imitation learn that and then
collected, imitation learn that and then
bootstrap where hopefully you eventually
bootstrap where hopefully you eventually
get uh luckier segments and then you
get uh luckier segments and then you
imitation learn those lucky segments and
imitation learn those lucky segments and
it bootstraps forever without even
it bootstraps forever without even
needing rewards.
needing rewards.
Well, without training on rewards, you
Well, without training on rewards, you
need the rewards, of course. I don't
need the rewards, of course. I don't
know. I had this idea. I have no idea if
know. I had this idea. I have no idea if
it's been done or like if it's been done
it's been done or like if it's been done
competently, which are two very
competently, which are two very
different things. So, I figured I would
different things. So, I figured I would
just try
input target
input
input
budgets
and the target is going to See?
Yeah. Something like this. Yeah.
Yeah. Where'd entropy come from?
We do some something like this. Yeah.
welcome something.
Thank you very much. I just put the talk
Thank you very much. I just put the talk
up on uh on X right here.
up on uh on X right here.
It's also on YouTube.
I'm working on implementing
I'm working on implementing
uh a weird
uh a weird
online imitation learning hybrid as an
online imitation learning hybrid as an
experiment in Puffer Lib right now.
that out. Yep.
It's not the uh recorded talk from RLC.
It's not the uh recorded talk from RLC.
It is a clean recording of the talk from
It is a clean recording of the talk from
uh well of the exact same talk pretty
uh well of the exact same talk pretty
much from home.
much from home.
People like to talk
I recorded it before I left, but then I
I recorded it before I left, but then I
left it um unposted till just now.
Fully recursive learning from its own
Fully recursive learning from its own
learn data pretty much.
Um there's no way that this just worked
Um there's no way that this just worked
first try. So, I'm assuming that this is
first try. So, I'm assuming that this is
not actually running the code that I
not actually running the code that I
think it is.
Exactly.
Exactly.
Pretty good though that I got all the
Pretty good though that I got all the
way down to here before I broke
way down to here before I broke
something.
stack argument tensor.
Yeah, because like the thing is my gripe
Yeah, because like the thing is my gripe
with all the imitation learning work is
with all the imitation learning work is
like oh as an alternative to RL. It's
like oh as an alternative to RL. It's
not an alternative to RL if you rely on
not an alternative to RL if you rely on
expert data. Just not how it works.
expert data. Just not how it works.
Can't even get expert data in a whole
Can't even get expert data in a whole
bunch of domains. But if you can use the
bunch of domains. But if you can use the
same formulation and you can do it with
same formulation and you can do it with
fully online data with of course being
fully online data with of course being
very easy to mix in expert data if you
very easy to mix in expert data if you
have it that is very valid.
I have no idea if it works
I have no idea if it works
cuz uh I mean that's kind of the idea,
cuz uh I mean that's kind of the idea,
right?
It's so funny to me like how I'm writing
It's so funny to me like how I'm writing
this code which would be considered
this code which would be considered
perfectly normal code anywhere other
perfectly normal code anywhere other
than puffer lib and it's like paining me
than puffer lib and it's like paining me
how slow I know this is going to be
how slow I know this is going to be
compared to puffer standards just with
compared to puffer standards just with
like the redundant copies and things.
like the redundant copies and things.
Kind of funny. All
Kind of funny. All
right. was randon.
Okay.
Ah, so this gets
Ah, so this gets
Uh
try this.
Come on. What's wrong with you?
Z Okay.
Yay. There we go.
Yay. There we go.
All right. So, that's your imitation
All right. So, that's your imitation
loss.
Is the build still sad? What build that
Is the build still sad? What build that
guy?
Title of the stream. God damn it. Did I
Title of the stream. God damn it. Did I
forget to
I I totally forgot to do that. All
I I totally forgot to do that. All
right. Here.
right. Here.
Um
Um
online
there.
Uh is this good?
All right. It doesn't uh you can't
All right. It doesn't uh you can't
update X titles, but whatever.
This is why I never switch the title.
This is why I never switch the title.
Everyone always asks me like, "Oh, can
Everyone always asks me like, "Oh, can
you title the stream what it's going to
you title the stream what it's going to
be?" No, cuz I always do this and then I
be?" No, cuz I always do this and then I
forget for the next four days like a
forget for the next four days like a
total goober.
That should be back up.
That should be back up.
Actually, wait. Why the chat hasn't even
Actually, wait. Why the chat hasn't even
been here the whole time? Literally,
been here the whole time? Literally,
nobody bothered to mention to me that I
nobody bothered to mention to me that I
had this thing hidden up here. Really?
had this thing hidden up here. Really?
All right, whatever. We'll be streaming
All right, whatever. We'll be streaming
tomorrow. Should be.
Well, now I have at least I have the
Well, now I have at least I have the
chat back here. We'll put the puffer
chat back here. We'll put the puffer
here. We'll put the chat up here. Um
go here for now.
go here for now.
Actually, can does he fit here? Yeah, he
Actually, can does he fit here? Yeah, he
does.
Buffer. Hey, there. Thought it was by
Buffer. Hey, there. Thought it was by
design today. Nope.
There's the chat.
Yeah, you guys have to tell me when I
Yeah, you guys have to tell me when I
screw random stuff up. All right. I was
screw random stuff up. All right. I was
just thinking about all like, oh, this
just thinking about all like, oh, this
cool research idea and I didn't check
cool research idea and I didn't check
any of this stuff.
This is like uh I had a great lecturer
This is like uh I had a great lecturer
in college who said actually you know if
in college who said actually you know if
I make a mistake I'm like I'm an old guy
I make a mistake I'm like I'm an old guy
I'm supposed to make mistakes or
I'm supposed to make mistakes or
whatever. If you guys don't catch the
whatever. If you guys don't catch the
mistake and just silently nod along it
mistake and just silently nod along it
reflects badly on all of you.
Yeah, that guy.
Okay, so this at least runs.
of course I need a good machine to
of course I need a good machine to
actually run it on.
At least out of the box doesn't do
At least out of the box doesn't do
anything.
Can we figure out why that is?
If you could go back, would you get a
If you could go back, would you get a
PhD again? Ooh.
Trying to think if I could have built
Trying to think if I could have built
this stuff without it.
You know, I honestly think that we're
You know, I honestly think that we're
going to have to wait a couple years to
going to have to wait a couple years to
see the answer to that question with
see the answer to that question with
Puffer Lib. I think we're going to have
Puffer Lib. I think we're going to have
to wait a couple years.
to wait a couple years.
You definitely don't want to ask
You definitely don't want to ask
somebody like right after they uh
somebody like right after they uh
finished their PhD in their first year
finished their PhD in their first year
or two. the longer term thing.
I think the answer is I'd probably be
I think the answer is I'd probably be
happier but accomplish less without it.
happier but accomplish less without it.
So take that for what you will.
Okay, I see. So you're just imitating
Okay, I see. So you're just imitating
garbage here at C.
garbage here at C.
Does it really never get a reward?
Does it really never get a reward?
Asking since you have to apply. Not sure
Asking since you have to apply. Not sure
to focus on top university go to any. Uh
to focus on top university go to any. Uh
I definitely wouldn't
I definitely wouldn't
I definitely wouldn't just like do a PhD
I definitely wouldn't just like do a PhD
for the sake of doing a PhD at a random
for the sake of doing a PhD at a random
university unless you're dead set on
university unless you're dead set on
doing that.
Wouldn't trust you without the PhD
Wouldn't trust you without the PhD
because I only care about credentials.
because I only care about credentials.
That's funny.
There are actually people who think that
There are actually people who think that
way is the thing. Like it actually does
way is the thing. Like it actually does
make a lot of the business side stuff a
make a lot of the business side stuff a
lot easier.
lot easier.
I don't have to waste time establishing
I don't have to waste time establishing
credibility.
This is just not getting copied over or
This is just not getting copied over or
something weird, right?
something is weird if it's just not
something is weird if it's just not
getting any reward ever, right?
Okay.
I suppose it is actually kind of tough
I suppose it is actually kind of tough
uh without the value function
if you don't have a value function of
if you don't have a value function of
some sort, right?
some sort, right?
Like there are environments where you
Like there are environments where you
just get a negative one for dying.
just get a negative one for dying.
So that's going to have you training on
So that's going to have you training on
like random data
actually. Okay. How does cartpole um
actually. Okay. How does cartpole um
work on rewards?
Like this is very slowly going up I
Like this is very slowly going up I
suppose.
Very easy task though.
I was thinking something like breakout
I was thinking something like breakout
would be
would be
way better because that has like a very
way better because that has like a very
consistent chord structure.
We definitely need to get this thing to
We definitely need to get this thing to
train way faster. play with it, right?
Yeah. So, the reward in that buffer goes
Yeah. So, the reward in that buffer goes
up a little bit,
up a little bit,
but this just doesn't work out of the
but this just doesn't work out of the
box at all.
box at all.
Let me think about why why this is be
Let me think about why why this is be
happening here.
We probably should have a way to like
We probably should have a way to like
dump a data set from a good policy as a
dump a data set from a good policy as a
reference, right?
reference, right?
Like if you have good data,
Like if you have good data,
then how well can you learn a policy
then how well can you learn a policy
from that, right?
That's pretty straightforward to come up
That's pretty straightforward to come up
with a way to do that.
You know, I'm actually a little
You know, I'm actually a little
surprised because I would actually
surprised because I would actually
expect this to work a little better out
expect this to work a little better out
of the box. Now, hold on. I want to
of the box. Now, hold on. I want to
think about this a little bit.
is there any way to incorporate a value
is there any way to incorporate a value
function estimate with this?
Like not really, right? Because that's
Like not really, right? Because that's
going to move around. That's the whole
going to move around. That's the whole
on policiness problem.
on policiness problem.
But like how do you judge
ultimately you just judge that a uh a
ultimately you just judge that a uh a
trajectory
trajectory
with the highest reward is better,
with the highest reward is better,
right? That's all you do.
What the heck is wrong with
Wait, why is the imitation loss and
Wait, why is the imitation loss and
entropy identical?
entropy identical?
Oh, hang on. Something's screwy here,
Oh, hang on. Something's screwy here,
right?
How's that make any bloody sense?
Uh, does that make any sense?
doesn't know because it should be
doesn't know because it should be
I guess they could potentially start the
I guess they could potentially start the
same
I'm optimizing entropy. That's a
I'm optimizing entropy. That's a
problem. That's obviously not going to
problem. That's obviously not going to
work.
Oh.
Oh.
Uh, okay.
Wait, entropy definitely should not be
Wait, entropy definitely should not be
the same as this, right? Entropy should
the same as this, right? Entropy should
not take into account the action.
Yeah, see this is logic's time is props.
Yeah, see this is logic's time is props.
There doesn't take into account action.
There doesn't take into account action.
So, how do we get the same number?
So, how do we get the same number?
That's super weird. Now,
like a reasonable thing now.
Well,
Not even optimizing the right thing
Not even optimizing the right thing
remotely.
Well, that'll do it. I wish I had my box
Well, that'll do it. I wish I had my box
back so I could test this faster.
So now they're similar but not
So now they're similar but not
identical.
Go looking for more problems. I didn't
Go looking for more problems. I didn't
realize that was a terrible mistake.
realize that was a terrible mistake.
Um
observation actions.
Okay. Well, that's an immediate
Okay. Well, that's an immediate
improvement, right?
improvement, right?
So obviously we want to run this thing
So obviously we want to run this thing
way faster.
But that actually is at least doing
But that actually is at least doing
something. No.
Yeah. Okay,
Yeah. Okay,
way better.
way better.
Now, as for
Now, as for
how well we can make something like this
how well we can make something like this
do overall, I don't know yet.
do overall, I don't know yet.
Uh, we'd like to see this get above like
Uh, we'd like to see this get above like
the six or seven range, which is kind of
the six or seven range, which is kind of
looking about.
looking about.
If that can do this, then it's actually
If that can do this, then it's actually
potentially feasible.
potentially feasible.
But
yeah, that's actually kind of cool.
yeah, that's actually kind of cool.
So the idea of this thing right is very
So the idea of this thing right is very
very simple.
very simple.
It's kind of draws on some ideas that um
It's kind of draws on some ideas that um
have worked in LLMs as well sort of
have worked in LLMs as well sort of
though I guess be weird to credit it to
though I guess be weird to credit it to
that because it goes way older than that
that because it goes way older than that
but um it's just generating a whole
but um it's just generating a whole
bunch of data and then imitation
bunch of data and then imitation
learning against the best data in there
learning against the best data in there
that you happen to get.
Technically, you could also do like a
Technically, you could also do like a
step of online RL for pretty much free.
step of online RL for pretty much free.
Well, maybe not.
You could definitely mix in online RL
You could definitely mix in online RL
with this if you wanted to, though.
We could try that.
In fact, I might even want to implement
In fact, I might even want to implement
it that way if this if this does like
it that way if this if this does like
anything meaningful.
Like what I basically want to do with
Like what I basically want to do with
this is the Can and we use this to
this is the Can and we use this to
increase sample efficiency
increase sample efficiency
on its own without the RL. I don't know.
on its own without the RL. I don't know.
If it happened to work better without
If it happened to work better without
the RL, that would be insane because
the RL, that would be insane because
that would be like a revolutionary
that would be like a revolutionary
discovery.
discovery.
I highly doubt we get that. I could be
I highly doubt we get that. I could be
wrong, but I highly doubt we get that.
wrong, but I highly doubt we get that.
Time is it 6:19. So, apparently in about
Time is it 6:19. So, apparently in about
less than an hour, I should get my box
less than an hour, I should get my box
back and we can just play around with
back and we can just play around with
some stuff in the meantime.
some stuff in the meantime.
Play around with the hybrid RL thing.
Play around with the hybrid RL thing.
Yeah, there's a lot of stuff we can play
Yeah, there's a lot of stuff we can play
with before we have it fully fully ready
with before we have it fully fully ready
to go. Be right back. Let me use a
to go. Be right back. Let me use a
restroom and grab a drink. Um, this is
restroom and grab a drink. Um, this is
at least a reasonable enough init.
at least a reasonable enough init.
Right back.
Okay. So,
Okay. So,
you know, the more I think about this,
you know, the more I think about this,
the more this makes sense. And actually,
the more this makes sense. And actually,
that's now
that's now
that's better than randomly running
that's better than randomly running
around. Yeah.
around. Yeah.
So, how could this possibly not work,
So, how could this possibly not work,
guys?
Welcome bet.
What if we just do online RL?
We do our exact normal online RL
and we just add this imitation objective
and we just add this imitation objective
over the best data that we've collected.
Completely get around off policy being a
Completely get around off policy being a
thing, right?
Now, I suppose that this result that
Now, I suppose that this result that
we're looking at here is not amazing on
we're looking at here is not amazing on
its own. Right?
its own. Right?
We'll see what score it gets to
at least get like
at least get like
a few points higher. Good.
Uh, this is on CPU bet.
I want to work on something with soft
I want to work on something with soft
rigid physics, something more sim based.
rigid physics, something more sim based.
Any recommendations or ideas?
Any recommendations or ideas?
uh
rigid physics depending on complexity we
rigid physics depending on complexity we
have know our own we just implement our
have know our own we just implement our
own stuff for drones
own stuff for drones
and Sam did that uh that could be done
and Sam did that uh that could be done
and extended to six degree of freedom
and extended to six degree of freedom
arms at like an intermediate skill level
arms at like an intermediate skill level
project intermediate implementation
project intermediate implementation
skill the math is a little tricky
skill the math is a little tricky
soft body I don't know about the
soft body I don't know about the
algorithms You might actually need an
algorithms You might actually need an
engine for that.
engine for that.
I'd go look into some of those other
I'd go look into some of those other
projects though. Like people have
projects though. Like people have
written their own soft body physics
written their own soft body physics
engines
engines
and I'd go look around to see like what
and I'd go look around to see like what
is the minimum complexity thing you need
is the minimum complexity thing you need
in order to make something like that
in order to make something like that
work.
work.
Okay, so this is above random perf here
Okay, so this is above random perf here
for sure.
This is definitely above random perf.
So, let's say that over the next
So, let's say that over the next
hourish, while I'm waiting for my boxes
hourish, while I'm waiting for my boxes
to come back online so I can train at a
to come back online so I can train at a
respectable speed, um,
respectable speed, um,
let's say I get this thing to run with
let's say I get this thing to run with
hybrid online RL imitation learning,
hybrid online RL imitation learning,
right? You do the im you do the RL step
right? You do the im you do the RL step
so that it's on policy roughly and then
so that it's on policy roughly and then
you do as many imitation learning steps
you do as many imitation learning steps
as you want. You have the buffer size
as you want. You have the buffer size
for your goal data be configurable
and then you try to improve the sample
and then you try to improve the sample
efficiency of your online RL using the
efficiency of your online RL using the
imitation learning.
None. All the boxes are screwed or most
None. All the boxes are screwed or most
of the boxes are screwed, but I have
of the boxes are screwed, but I have
somebody that's going to come boot them
somebody that's going to come boot them
in like an hour. Nobody told me that
in like an hour. Nobody told me that
they were screwed or at least nobody
they were screwed or at least nobody
told me loud enough with how fast the
told me loud enough with how fast the
Discord moves while I was at the
Discord moves while I was at the
conference. So, I didn't know to have
conference. So, I didn't know to have
them rebooted.
them rebooted.
They weren't. Looks like they were
They weren't. Looks like they were
going. They went down on August 4th to
going. They went down on August 4th to
me.
me.
at least from um it looks like they've
at least from um it looks like they've
been down for like several days.
Yeah, some of them have autoreboot, some
Yeah, some of them have autoreboot, some
of them don't.
We're going to have to go through and
We're going to have to go through and
reclaim a bunch of boxes cuz I don't
reclaim a bunch of boxes cuz I don't
think everyone's using them.
Yeah. Okay. 8.6. This actually, it's
Yeah. Okay. 8.6. This actually, it's
kind of funny that this just works.
kind of funny that this just works.
Literally, the first thing I implement
Literally, the first thing I implement
works.
It doesn't work great, but it does work
It doesn't work great, but it does work
better than noise.
the monkey ball end if I did rigid body
the monkey ball end if I did rigid body
108 bytes.
108 bytes.
What do you mean it's 108 bytes?
What do you mean it's 108 bytes?
I've done a six degree of freedom arm,
I've done a six degree of freedom arm,
but kinematics go way over my head, so
but kinematics go way over my head, so
it was super slow.
it was super slow.
How did you do a six degree of freedom
How did you do a six degree of freedom
arm without doing kinematics?
arm without doing kinematics?
Math gets a little hairy, you know.
What is online RL getting us that this
What is online RL getting us that this
isn't getting us is my question.
Wait. Like actually.
Well, you do get the value function,
Well, you do get the value function,
right?
Optimal
Optimal
Oh, well, you need to do both uh you
Oh, well, you need to do both uh you
need to do both forward kinematics and
need to do both forward kinematics and
dynamics
dynamics
in order to get like a basic reacher
in order to get like a basic reacher
working.
working.
It's going to be like a fair bit of
It's going to be like a fair bit of
math.
math.
That would actually be a very valuable
That would actually be a very valuable
task to the extent that the first person
task to the extent that the first person
that comes and PRs me like an actually
that comes and PRs me like an actually
good sixth degree of freedom arm at the
good sixth degree of freedom arm at the
level of the drone end. Um, like that's
level of the drone end. Um, like that's
actually probably one of the more
actually probably one of the more
promising ways to get into contracts
promising ways to get into contracts
with us.
Obviously, there's more to it than that.
Obviously, there's more to it than that.
You'd actually have to help us find some
You'd actually have to help us find some
partnerships with robot companies, but
partnerships with robot companies, but
that's probably one of the easier ones
that's probably one of the easier ones
to do.
to do.
Math is a little hairy, though.
Math is a little hairy, though.
Would probably take me like several days
Would probably take me like several days
to figure out.
I'm trying to conceptualize here in my
I'm trying to conceptualize here in my
head.
Let me finish chess. Yeah,
I'm trying to think
the difference between these approaches,
the difference between these approaches,
right?
Yeah, you'll be waiting a while on that
Yeah, you'll be waiting a while on that
convert.
There's an advantage to doing this,
There's an advantage to doing this,
isn't there?
It's slightly obnoxious, I guess, from a
It's slightly obnoxious, I guess, from a
compute perspective because
compute perspective because
what you would like to do, right, is
what you would like to do, right, is
you'd like to add the imitation loss uh
you'd like to add the imitation loss uh
just to the RL loop. But you can't do
just to the RL loop. But you can't do
that because if you do it that way, you
that because if you do it that way, you
make it more off policy, which is bad.
make it more off policy, which is bad.
You want to keep it as on policy as
You want to keep it as on policy as
possible
possible
during the RL loop. And then you want to
during the RL loop. And then you want to
take it off policy with the imitation
take it off policy with the imitation
loop.
loop.
Oh, wait. Doesn't this Wait, does this
Oh, wait. Doesn't this Wait, does this
still screw it up or no?
This probably still screws up the value
This probably still screws up the value
function, doesn't it?
possibly does still screw up the value
possibly does still screw up the value
function. Uh that is a positive result
function. Uh that is a positive result
though from like an early experiment.
There are a lot of different ways we
There are a lot of different ways we
could set this thing up potentially.
I'm trying to RL a chess puzzle.
Uh, it shouldn't because you should be
Uh, it shouldn't because you should be
passing a done signal bet.
passing a done signal bet.
But you can technically, yes, you can
But you can technically, yes, you can
technically mess stuff up depending on
technically mess stuff up depending on
how you it
Yes, you have to be careful.
Well, because it's like
I don't know how you have it be. Is it
I don't know how you have it be. Is it
like ignoring an action? Because it has
like ignoring an action? Because it has
to be able to see. It has to be like
to be able to see. It has to be like
observe the board state, right?
observe the board state, right?
and then it makes its action and then it
and then it makes its action and then it
observes the next state of the board and
observes the next state of the board and
then there has to be a done after that.
then there has to be a done after that.
You basically there's a thing where if
You basically there's a thing where if
you reset it right you can drop either
you reset it right you can drop either
the first observation of the next
the first observation of the next
episode or the last observation of this
episode or the last observation of this
episode. You have to be careful not to
episode. You have to be careful not to
do either of those uh in this case I
do either of those uh in this case I
believe.
Okay. So, this immediately performs
Okay. So, this immediately performs
better. Um, and we can compare this, I
better. Um, and we can compare this, I
believe, to the
believe, to the
we can compare this one to the other
we can compare this one to the other
ones.
ones.
Uh, I definitely need my computer back
Uh, I definitely need my computer back
to be able to run these experiments. So,
to be able to run these experiments. So,
let's just like clean this up and make
let's just like clean this up and make
this reasonable, I think, in the
this reasonable, I think, in the
meantime.
place.
place.
You're going to There's going to have to
You're going to There's going to have to
be one uh extra observation before reset
be one uh extra observation before reset
where the action doesn't do anything
where the action doesn't do anything
basically.
This is stuff you would have figured out
This is stuff you would have figured out
by now if you've been doing your formal
by now if you've been doing your formal
RL
readings in parallel to the
readings in parallel to the
implementation. Yeah.
You did. All right. Do your do your
You did. All right. Do your do your
homework.
Hello Shraan. Welcome.
That's very much how our work. Yes.
That's very much how our work. Yes.
Yes, it does. Bet.
Yes, it does. Bet.
Don't you forget it.
lazy cuz buffer just works. Yeah,
it could potentially be working much
it could potentially be working much
better soon.
Did you get it working, Spencer?
Did you get it working, Spencer?
Hey, look at that. We've got Spencer
Hey, look at that. We've got Spencer
doing real research. Now,
uh, I'm actually have a decent shot at
uh, I'm actually have a decent shot at
solving sample efficiency, or at least
solving sample efficiency, or at least
like majorly improving sample efficiency
like majorly improving sample efficiency
with what I'm doing now.
Do you need um boxes for experiments,
Do you need um boxes for experiments,
Spencer?
Spencer?
Cuz obviously
Cuz obviously
this is going to have to be swept on a
this is going to have to be swept on a
ton of stuff.
Yeah, go for it. I think the tiny ones
Yeah, go for it. I think the tiny ones
are still up. Um,
are still up. Um,
we have a few of the individual boxes
we have a few of the individual boxes
down right now and they should be back
down right now and they should be back
up in less than an hour.
Yeah. So, this is experimental. Uh,
Yeah. So, this is experimental. Uh,
Spencer's doing discrete value
Spencer's doing discrete value
functions. Yeah, Spencer. So, this thing
functions. Yeah, Spencer. So, this thing
I'm working on is pretty cool. Uh, so
I'm working on is pretty cool. Uh, so
what this is going to do is this is
what this is going to do is this is
going to keep the RL objective. HL Gaus
going to keep the RL objective. HL Gaus
and anything that you do it that'll stay
and anything that you do it that'll stay
and it'll stay like our one epoch of RL
and it'll stay like our one epoch of RL
like we normally do. But then it's going
like we normally do. But then it's going
to let you add in uh however much
to let you add in uh however much
imitation learning you want on the best
imitation learning you want on the best
data that you've seen so far. So this
data that you've seen so far. So this
should allow you to keep like uh high
should allow you to keep like uh high
value data around. Yeah, this should
value data around. Yeah, this should
like let you keep high value data around
like let you keep high value data around
and essentially build yourself a data
already applying ideas. Yeah.
already applying ideas. Yeah.
Um, if I get this to work, this would be
Um, if I get this to work, this would be
like puffer four material and this could
like puffer four material and this could
well it would have to work very well.
well it would have to work very well.
But I think algorithmically this is the
But I think algorithmically this is the
type of thing that could definitely
type of thing that could definitely
uh like unlock new applications and be
uh like unlock new applications and be
puffer 4 just straight up.
puffer 4 just straight up.
Like Puffer 4 could literally just end
Like Puffer 4 could literally just end
up being a 500 line change to uh like a
up being a 500 line change to uh like a
400 line change to Puffarel.
I have to do it while they're fresh in
I have to do it while they're fresh in
my head, though, you know. But I sent uh
my head, though, you know. But I sent uh
I did I ran the dish this morning. I did
I did I ran the dish this morning. I did
some weights
some weights
and uh I answered all the emails, all
and uh I answered all the emails, all
the conversations. That's still on
the conversations. That's still on
track, don't worry. Then I'm starting to
track, don't worry. Then I'm starting to
do some of this stuff.
I owe Kathy Woo's group some work. I owe
I owe Kathy Woo's group some work. I owe
clients some work, but have to at least
clients some work, but have to at least
carve out a little bit of time to do
carve out a little bit of time to do
this type of stuff. Yeah.
Are you running stuff on one of them?
Are you running stuff on one of them?
No, not at the moment. And uh I don't
No, not at the moment. And uh I don't
need I don't need either of them for uh
need I don't need either of them for uh
for a bit. So you can just go ahead and
for a bit. So you can just go ahead and
use both.
The only thing that I will need is I'm
The only thing that I will need is I'm
going to need Puffer Max one. um which
going to need Puffer Max one. um which
is like the single 5090 box when it
is like the single 5090 box when it
comes back up because I'm going to set
comes back up because I'm going to set
that up for um imitation learning
that up for um imitation learning
experiments over the next couple Yes.
Uh the point of it bet is to use expert
Uh the point of it bet is to use expert
compute.
best param from
start so that it's quick. Start with
start so that it's quick. Start with
running best breakout params on a few
running best breakout params on a few
others and compare like just get the two
others and compare like just get the two
curves like with HL Gaus and then like
curves like with HL Gaus and then like
with whatever I had in there for the
with whatever I had in there for the
breakout sweeps before on like a few
breakout sweeps before on like a few
other M's like pong, enduro, whatever
other M's like pong, enduro, whatever
else. Um, and then you're going to have
else. Um, and then you're going to have
to rerun an HL G sweep on some of the
to rerun an HL G sweep on some of the
more complex ends. Uh, mazes might be a
more complex ends. Uh, mazes might be a
little tricky because that one's hard to
little tricky because that one's hard to
tune for. If you get better at mazes,
tune for. If you get better at mazes,
that's really good. Obviously, we're
that's really good. Obviously, we're
going to want to do some of the more
going to want to do some of the more
complex ones. We're going to try MOA.
complex ones. We're going to try MOA.
Uh, tuning on neural MMO is hard, but if
Uh, tuning on neural MMO is hard, but if
we can tune and get something on neural
we can tune and get something on neural
MMO, that would be really good.
MMO, that would be really good.
Great job at RLC. Thank you.
Great job at RLC. Thank you.
Conferences are exhausting. They really
Conferences are exhausting. They really
are.
are.
I basically crashed. I got home and I
I basically crashed. I got home and I
was like, "All right, I got 36 hours of
was like, "All right, I got 36 hours of
not doing RL work.
Now I'm back.
Uh, that was kind of the whole
Uh, that was kind of the whole
conference that it was rough.
conference that it was rough.
I'm pretty much good now.
Like a 95%
the game idea
the game idea
mechanics. Yeah, you can do that.
I'm going to work on I'm going to work
I'm going to work on I'm going to work
on like research side stuff for a little
on like research side stuff for a little
bit with this just because I think
bit with this just because I think
there's a good chance that this works.
there's a good chance that this works.
I also have the be sweet thing which is
I also have the be sweet thing which is
I think also a good idea.
There's Jason.
What?
What?
Some facto
that was a
that was a
fly to the previous or what?
Total mini batches.
Total mini batches.
Got total ILIL mini batches.
Then we have IL mini batch size.
kind of it.
and
need two different opt. No, you don't
need two different opt. No, you don't
need two optimizers, right?
Wait, do you need two different
Wait, do you need two different
optimizers if you do two different
optimizers if you do two different
objectives like this?
I guess we'll try it like this first and
I guess we'll try it like this first and
we'll see.
Yeah, we'll just try it like this first
Yeah, we'll just try it like this first
and we'll see, I guess.
Little Need it.
Opera is no
Oh yeah, you can't have these be
Oh yeah, you can't have these be
different, can you? Damn, forgot about
different, can you? Damn, forgot about
that.
that.
Not easily, at least, right?
It kind of sucks. Kind of sucks, doesn't
It kind of sucks. Kind of sucks, doesn't
it?
Good way around this or not really.
really kind of isn't.
You'd have to make average reward work
You'd have to make average reward work
in order for this to make any sense.
decent.
decent.
If this actually runs nepo
I think that this yeah this gives us our
I think that this yeah this gives us our
initial starting point right
idea here have larger batches
do quite a
We do need to go to
selfil section.
Yeah, this is never going to prune
Yeah, this is never going to prune
anything off of this, which is good.
So, we can definitely optimize this, but
So, we can definitely optimize this, but
this is like this should be a decent
this is like this should be a decent
starting point, I would assume.
Things that can go wrong here, right?
You haven't trained the value function.
You haven't trained the value function.
You haven't trained the value function,
You haven't trained the value function,
right?
So I think that what you want to do is
So I think that what you want to do is
you want to compute
you want to compute
you want to compute the value function.
What we would need to do that
new value.
You need the returns. Yeah.
Then to compute the returns,
advantages plus value.
uniformly sampled for now.
Wonder if we want to do the same
Wonder if we want to do the same
sampling thing.
We could pretty easily do that same
We could pretty easily do that same
sampling thing now.
thing like Yes.
Put on the mini battery for now.
We keep around.
Wait, you don't need a clipped value
Wait, you don't need a clipped value
function anymore, do we?
You might need it.
I guess all this stuff I'm thinking
I guess all this stuff I'm thinking
about should be more empirically
about should be more empirically
motivated.
motivated.
Why I need my uh my fast box back so I
Why I need my uh my fast box back so I
can play with this messages. Gotten it
can play with this messages. Gotten it
back.
No messages blocks.
Okay.
Okay.
Well, I can at least put it into place
Well, I can at least put it into place
the code and we can ablade it. That way
the code and we can ablade it. That way
I won't have to implement it after.
Just do.
Yeah, you need like all the freaking
Yeah, you need like all the freaking
things, don't you?
Boing
Boing
ratio advantage.
birds.
If we're able to get this advantage in,
If we're able to get this advantage in,
then we'll be able to prevent the uh the
then we'll be able to prevent the uh the
advantage function from going stale.
advantage function from going stale.
That's the idea. Because if you take the
That's the idea. Because if you take the
um the policy too far off value
um the policy too far off value
and if you take it too far off of the
and if you take it too far off of the
policy,
policy,
it'll collect data correctly,
it'll collect data correctly,
but like the value function will just be
but like the value function will just be
undertrained.
So the hope here is if you keep the
So the hope here is if you keep the
value function fresh,
then you'll still be able to make full
then you'll still be able to make full
use of your on policy data with the
use of your on policy data with the
better policy train via IL.
better policy train via IL.
That's the uh the core idea here.
do this.
Now
Now
I do need to go find this ratio
ratio and self advantage.
This should be
Yeah. So this is your advantage, right?
Then you get MB returns.
You get
Get MB values There.
And where is the
value.
down.
We put here
like this online manipulation learning
like this online manipulation learning
like data aggregation.
like data aggregation.
Uh I looked I was looking around for
Uh I looked I was looking around for
references and I kind of let's see if
references and I kind of let's see if
this is let's see if this matches. Um,
this is let's see if this matches. Um,
so you just you're collecting data
so you just you're collecting data
online, you're filtering it for the best
online, you're filtering it for the best
data and then you do imitation learning
data and then you do imitation learning
on that instead of having any sort of
on that instead of having any sort of
expert data that you're starting off
expert data that you're starting off
with. It's fully bootstrapped
at least that is the goal. Now it's easy
at least that is the goal. Now it's easy
to combine this with online RL
to combine this with online RL
and in that case then what this would
and in that case then what this would
potentially do is uh it would just let
potentially do is uh it would just let
you use less data for the same result.
you use less data for the same result.
Sample efficiency hack.
Sample efficiency hack.
Lots of things we can potentially do
Lots of things we can potentially do
with this.
Is it without reward info? uh the
Is it without reward info? uh the
imitation learning part only uses the
imitation learning part only uses the
reward information in order to decide
reward information in order to decide
what data to keep, right? So you need
what data to keep, right? So you need
something that decides what data you
something that decides what data you
actually want to train on. You're not
actually want to train on. You're not
actually using reward to optimize, but
actually using reward to optimize, but
you're using reward to select your data.
Imitation learning without reward only
Imitation learning without reward only
works if you have an expert data set,
works if you have an expert data set,
which is not what we want to do.
which is not what we want to do.
Filtered behavioral cloning. Yes, except
Filtered behavioral cloning. Yes, except
that like when you talk about these
that like when you talk about these
methods, right, you're almost always
methods, right, you're almost always
talking about using expert data and
talking about using expert data and
we're not,
right? You can filter expert data for
right? You can filter expert data for
the best expert data, but there's no
the best expert data, but there's no
expert at all here.
And that's kind of the cool thing.
Still not offline RL. Offline RL is also
Still not offline RL. Offline RL is also
different and it has its own set of
different and it has its own set of
problems.
problems.
Offline RL is also still typically used
Offline RL is also still typically used
with expert data, right? expert data
with expert data, right? expert data
that includes reward annotations.
that includes reward annotations.
This is not really either of those. I
This is not really either of those. I
don't know if people have done this
don't know if people have done this
before. It seems like something that you
before. It seems like something that you
would probably expect to have been done
would probably expect to have been done
cuz it's pretty simple, but I couldn't
cuz it's pretty simple, but I couldn't
find any references.
Not always.
Not always.
Um I mean if you have examples of online
Um I mean if you have examples of online
offline RL being done with data
offline RL being done with data
collected fully from scratch let me
collected fully from scratch let me
knowly
knowly
random behaviors. That's also not this
random behaviors. That's also not this
though because if you do it with
though because if you do it with
completely random behaviors that will
completely random behaviors that will
only work if your environment is
only work if your environment is
trivial. The thing is that you're
trivial. The thing is that you're
actively collecting new online data.
pretty much any environment for which
pretty much any environment for which
you can train it with purely random data
you can train it with purely random data
is just a trivial problem
as a general rule.
I mean you can construct like degenerate
I mean you can construct like degenerate
counter examples. Um but for the most
counter examples. Um but for the most
part in practice that is true.
Sounds pretty niche. Well, it's niche in
Sounds pretty niche. Well, it's niche in
the sense that I haven't seen research
the sense that I haven't seen research
on it before, but it's not niche in the
on it before, but it's not niche in the
sense that if this works, it will be
sense that if this works, it will be
like it will massively massively improve
like it will massively massively improve
the entire space as a whole immediately.
the entire space as a whole immediately.
Online data is generated by the agent.
Online data is generated by the agent.
Yes, the online data is generated by a
Yes, the online data is generated by a
new tabularasa agent in initialized from
new tabularasa agent in initialized from
scratch that learns over time.
Yeah, Bed is right though. Like we do
Yeah, Bed is right though. Like we do
kind of crazy things around here. And
kind of crazy things around here. And
like we do crazy things in a way that
like we do crazy things in a way that
like even if they've been tried before,
like even if they've been tried before,
uh the way that I'm doing them here is
uh the way that I'm doing them here is
much more likely to work.
All right. Bet.
And so now we have advantages. Yeah.
Well, that's yes, that is obviously
Well, that's yes, that is obviously
true.
Uh,
this work
Okay. So, I think that this is good and
Okay. So, I think that this is good and
we can now try this with um a few
we can now try this with um a few
different combinations of settings
and we can see whether this can be made
and we can see whether this can be made
to work better uh with the IL and the
to work better uh with the IL and the
loop than not. Now, the problem is, of
loop than not. Now, the problem is, of
course, that
course, that
I need my um my box back.
My box back. Yeah.
My box back. Yeah.
And
let me go check if it's back up on tail
let me go check if it's back up on tail
scale.
No, they haven't gotten home to reboot
No, they haven't gotten home to reboot
it yet.
Annoying.
Could go steal box zero for a bit.
Could go steal box zero for a bit.
I might go do that.
Yeah, Spencer.
I'll try seven if I can't get zero
I'll try seven if I can't get zero
working. I use a restroom real quick
working. I use a restroom real quick
though and then uh I mean there's no
though and then uh I mean there's no
point in running a 30 minute experiment.
point in running a 30 minute experiment.
That should take one minute. I'll be
That should take one minute. I'll be
right back.
So, uh, Spencer says he's not using box
So, uh, Spencer says he's not using box
zero for a little bit. Try that.
zero for a little bit. Try that.
Here's what we're going to do. We're
Here's what we're going to do. We're
going to commit this.
That's not bad. 133 lines.
Bye.
much better. Right.
much better. Right.
Get reasonable speeds,
N's losses.
We're going to comment this whole loop
We're going to comment this whole loop
out next for a baseline.
Okay. So, this does worse.
Does worse at the moment.
The normal training perk.
Okay, so we do have stable training on
Okay, so we do have stable training on
the original,
the original,
right?
Funny, it's a lot slower on a 4090 than
Funny, it's a lot slower on a 4090 than
it is on the 5090, but that is stable
it is on the 5090, but that is stable
training,
training,
stable task solve.
90 million steps
90 million steps
pretty much.
pretty much.
That's the baseline to
happens if I just trim the value loss a
happens if I just trim the value loss a
bit. Does this crash it?
Yeah, this nan's out.
Yeah, this nan's out.
Well, it's possible I just have this
Well, it's possible I just have this
thing defined or wrong, All right.
still ns out. Was actually doing pretty
still ns out. Was actually doing pretty
well before it nanned out.
I think what we do with this, right?
No, not that.
This doesn't hand out. It just doesn't
This doesn't hand out. It just doesn't
do as well, right?
At
least this is not completely crashing
least this is not completely crashing
our learning here.
It's worse by a lot. Not completely
It's worse by a lot. Not completely
crashing it though.
The only thing I can think about this is
The only thing I can think about this is
the stale value function, right?
And like somehow
And like somehow
do I just have the value function
do I just have the value function
calculation wrong? Is that all it is?
calculation wrong? Is that all it is?
I can't see what else it could be.
But this seems fine. I guess the other
But this seems fine. I guess the other
thing is it's not clamped,
thing is it's not clamped,
right? The value function is not
right? The value function is not
clamped.
I have to avoid getting too far off the
I have to avoid getting too far off the
freaking policy, isn't it?
Just clamp it. Anyways,
I can technically do it. No.
What if I just do it for the hell of it?
Okay, this is the same clip value loss
Okay, this is the same clip value loss
now from before
man's out
there this other
This is definitely broken, right?
Well, hang on. It's still I was about to
Well, hang on. It's still I was about to
say it's stable, but it's clearly not.
say it's stable, but it's clearly not.
Maybe you need to do plus
train jointly.
hands out at 200. Eh,
hands out at 200. Eh,
now
we not do the entire thing.
and Nan's out at 200 again.
It's awkward.
boards values terminals.
Is there any fundamental reason they
Is there any fundamental reason they
should be unstable? I don't think that
should be unstable? I don't think that
there is.
Right. There's no fundamental thing that
Right. There's no fundamental thing that
uh jumps out at me as to why they should
uh jumps out at me as to why they should
be unstable.
Particularly with a clipped loss like
Particularly with a clipped loss like
this.
Was there any warning or did it just
Was there any warning or did it just
like all of a sudden explode?
Value loss is very low.
Loss is low.
Imitation went to Nan. Then everything
Imitation went to Nan. Then everything
else went to Nanitation.
How did imitation laws go to?
How did imitation laws go to?
They didn't make any sense.
should be like the easiest most stable
should be like the easiest most stable
loss ever. No,
loss ever. No,
the straight up behavioral cloning loss.
It is a cross entropy loss.
So imitation loss goes to Nan
because logic just
policy weights must be screwed up,
policy weights must be screwed up,
right?
Yeah. also nan.
How do we nan out the policy though?
I know it's not fancy fancy cross
I know it's not fancy fancy cross
entropy. It's literally cross entropy
just scaled badly.
just scaled badly.
I don't think it is just scaled badly
I don't think it is just scaled badly
though, right?
though, right?
The value loss being screwy.
still Nan's out
crazy.
Um, it's training like okay, I'd say bet
Um, it's training like okay, I'd say bet
it's not like noticeably
it's not like noticeably
I don't think it's any better than the
I don't think it's any better than the
baseline yet.
This does seem like the type of thing
This does seem like the type of thing
that has to work though.
Yeah, but like the goal is okay, can we
Yeah, but like the goal is okay, can we
solve breakout in fewer steps by
solve breakout in fewer steps by
spending more compute.
Weird to me that we can't
Weird to me that we can't
the value function not want to train.
I'm almost positive that it's the value
I'm almost positive that it's the value
function, right? Like this thing not
function, right? Like this thing not
wanting There.
Yeah.
Yeah.
Do we get in?
Do we get in?
We do this.
Try this only thing.
That's better. Right.
That's no RL.
Yeah, there's no RL here at all.
Yeah, there's no RL here at all.
Betation
puffer.
Okay. I mean, this is like up to 26
Okay. I mean, this is like up to 26
score.
Hey Finn,
welcome.
Very cool. Well, it's not doing super
Very cool. Well, it's not doing super
well yet. It's doing better than random.
well yet. It's doing better than random.
So, the question here, right,
So, the question here, right,
hypothesis one is, can you completely
hypothesis one is, can you completely
replace RL with self imitation? That
replace RL with self imitation? That
seems kind of hard to do without a ton
seems kind of hard to do without a ton
of compute
of compute
um or even if you can do it at all. Uh
um or even if you can do it at all. Uh
option two is like can you improve
option two is like can you improve
sample efficiency?
sample efficiency?
This actually did worse.
should be able to do this and do better.
should be able to do this and do better.
Let's see.
Let's see.
RL for material science. Yeah, we will
RL for material science. Yeah, we will
be doing that as well. Why does that not
be doing that as well. Why does that not
show up in my dashboard? I see it on the
show up in my dashboard? I see it on the
screen.
This doesn't seem to be doing massively
This doesn't seem to be doing massively
better or anything.
Could technically be overfit.
Not doing better though for sure.
Wonder if this does worse.
Oh, this does do work.
Oh, this does do work.
Okay.
Too big.
Well, this will be super sample, right?
Well, this will be super sample, right?
Very fast.
Very fast.
Do you care more about wall clock or
Do you care more about wall clock or
steps? Uh, for your experiments, I would
steps? Uh, for your experiments, I would
say I care more about wall clock.
So, HL G shouldn't make it any slower
So, HL G shouldn't make it any slower
really. So, it should be similar,
really. So, it should be similar,
Spencer.
Spencer.
Like, you should end up with very
Like, you should end up with very
similar results in terms of um wall
similar results in terms of um wall
clock, but like the experiment should
clock, but like the experiment should
take not really any longer to run.
This is interesting. See?
Quite hard to get this thing to
Quite hard to get this thing to
bootstrap.
Yeah. You know, like I would think that
Yeah. You know, like I would think that
doing a longer horizon would help.
Shouldn't be that hard.
Like it is learning something, but it's
Like it is learning something, but it's
not
not
Not a great signal, right?
Learning on segment. Man,
well, wait. Does this this solves cart
well, wait. Does this this solves cart
pole with no RL,
pole with no RL,
right?
That did just solve cart pole, did it
That did just solve cart pole, did it
not?
Believe it did.
Oh, hey. Wait, that almost solves pong
Oh, hey. Wait, that almost solves pong
as well.
as well.
Wait, that's that's almost Pong solved,
Wait, that's that's almost Pong solved,
too.
too.
like 200 bet
like 200 bet
195.
It almost solves pawn.
It almost solves pawn.
Yeah. So, hang on. This there is
Yeah. So, hang on. This there is
something to this. There's something
something to this. There's something
about the structure of the M though.
about the structure of the M though.
Back from the conference, I see any cool
Back from the conference, I see any cool
new inspiration. Yeah. We're currently
new inspiration. Yeah. We're currently
solving RL problems without RL,
solving RL problems without RL,
which is kind of crazy.
Well, no, but it makes sense that this
Well, no, but it makes sense that this
would work.
What about a dense reward like pupper
What about a dense reward like pupper
drone?
The heck happened to drone swap?
trajectories aren't even expert. That's
trajectories aren't even expert. That's
the point.
The full full bootstrap problem.
The full full bootstrap problem.
Uh let me go find another problem that
Uh let me go find another problem that
we can do.
doesn't work on this.
doesn't work on this.
However, you watch what I've missed so
However, you watch what I've missed so
far. I mean, I can give you the TLDDR of
far. I mean, I can give you the TLDDR of
it, right? So, we're doing um
it, right? So, we're doing um
we're saving a buffer of data from the
we're saving a buffer of data from the
agent. The agent's initialized from
agent. The agent's initialized from
scratch. We're saving a big buffer of
scratch. We're saving a big buffer of
data from it. Uh, but we're only saving
data from it. Uh, but we're only saving
the top end trajectory segments like
the top end trajectory segments like
ever from any time that we run the
ever from any time that we run the
agent. And then we're running imitation
agent. And then we're running imitation
learning on those top segments.
Basically, we're seeing if this thing
Basically, we're seeing if this thing
can bootstrap when it from a totally
can bootstrap when it from a totally
random agent. Um,
random agent. Um,
yeah,
worked on. What are some other M's I
worked on. What are some other M's I
would expect like a weak learner to work
would expect like a weak learner to work
on.
I can try like target.
Oh, that doesn't work cuz it's
Oh, that doesn't work cuz it's
multi-discreet, right? I haven't added
multi-discreet, right? I haven't added
multi-isport yet.
Our is probably hard.
Convert single discrete.
Convert single discrete.
Not
Tetris.
Or is going down, not up.
Or is going down, not up.
Racer. We can try that.
Or doesn't get better in Tetris. Odd.
Got like decent gold data.
That doesn't work at all.
I try snake as well.
I try snake as well.
Snake is like a good simple multi task.
Doesn't look right.
Kind of training.
I mean,
I mean,
you can't say that's doing nothing,
you can't say that's doing nothing,
right?
Like actually kind of decent,
especially for like a very first take on
especially for like a very first take on
this.
this.
Four
is not one normalized. So, I actually
is not one normalized. So, I actually
don't know how high it's supposed to go.
don't know how high it's supposed to go.
I think it was like three or four or
I think it was like three or four or
something.
I mean, that does actually do something
I mean, that does actually do something
though.
Tripled score from baseline.
Tripled score from baseline.
Are you sure?
I thought that the baseline was like
I thought that the baseline was like
three or four score. No,
we definitely have signs of life.
Eval will show. Yeah, I'm on a remote
Eval will show. Yeah, I'm on a remote
box. I have to like do something for
box. I have to like do something for
that.
that.
Bye snake.
It's like somehow run out of better
It's like somehow run out of better
data.
That could be an entropy fail though,
That could be an entropy fail though,
right?
right?
Let me try with entropy.
add the uh where's entropy
Is this technique people
Is this technique people
please use for robotics RL?
please use for robotics RL?
No, this isn't like this is new. Um, it
No, this isn't like this is new. Um, it
seems simple enough that somebody
seems simple enough that somebody
probably has done it, but if there is a
probably has done it, but if there is a
reference, I don't know it.
Okay, so snake is not amazing here.
I wonder if it's like stuck on the
I wonder if it's like stuck on the
ability
ability
to have enough reward in one segment or
to have enough reward in one segment or
something.
like the fact that it does cart pull
like the fact that it does cart pull
instantly, right?
instantly, right?
And pretty much pong
And pretty much pong
think is multi multi- aent. Yeah,
think is multi multi- aent. Yeah,
self imitation learn. Is it an actual
self imitation learn. Is it an actual
established thing, Finn, where people
established thing, Finn, where people
like bootstrap imitation learning off of
like bootstrap imitation learning off of
nothing?
Like I've seen people do this for
Like I've seen people do this for
completely different reasons and like
completely different reasons and like
weird context,
weird context,
but I haven't seen this specific thing
but I haven't seen this specific thing
done.
Need to make sure I'm not making that up
Need to make sure I'm not making that up
because there was a paper
Yeah, I know. This is different.
Very different.
Okay. So, this
the fact that it does something like
the fact that it does something like
this, it's like flashes of brilliance,
this, it's like flashes of brilliance,
right?
Oh,
okay. Um,
Hang on, guys. We might have something.
I had a suspicion. I'll explain this in
I had a suspicion. I'll explain this in
a second if I'm right.
Yeah. So, this is what I thought. Um,
Yeah. So, this is what I thought. Um,
so this has the best early curve I've
so this has the best early curve I've
ever seen on Neural MMO 3.
Yeah.
Yeah.
Yeah. Guys, this is the Yeah, this is a
Yeah. Guys, this is the Yeah, this is a
real thing.
No, it's not going to solve it. Let me
No, it's not going to solve it. Let me
explain in a second.
I will explain in a second why this is
I will explain in a second why this is
so important.
Yeah. So, this is pretty much the same
Yeah. So, this is pretty much the same
algorithm as I came up with, I think.
algorithm as I came up with, I think.
Let's see.
No. Well, this portion is the other
No. Well, this portion is the other
thing that I tried pretty much. Um, but
thing that I tried pretty much. Um, but
this is without I'm just doing this
this is without I'm just doing this
portion right now.
There's also prioritize replay as a
There's also prioritize replay as a
potential.
So, this is it is pretty close. I will
So, this is it is pretty close. I will
say it's like it is pretty close.
Who the heck is this paper from?
Who the heck is this paper from?
Some like random paper.
Some like random paper.
Okay.
Okay.
So, here's the thing, guys. So, what's
So, here's the thing, guys. So, what's
hard with neural MMO? Um,
hard with neural MMO? Um,
the reward
the reward
from random actions is incredibly sparse
from random actions is incredibly sparse
at the start. So, like it's very very
at the start. So, like it's very very
difficult to get the reward a few times
difficult to get the reward a few times
at the start. And if you're only getting
at the start. And if you're only getting
a reward like a tiny tiny fraction of
a reward like a tiny tiny fraction of
the time, it's very difficult to latch
the time, it's very difficult to latch
on to that reward and learn.
on to that reward and learn.
But what this does is this is actually
But what this does is this is actually
going to save the trajectory segment
going to save the trajectory segment
that contains the reward and learn on
that contains the reward and learn on
just that. So like let's say you get 10
just that. So like let's say you get 10
million samples and only get like five
million samples and only get like five
good rewards. Well, at least those five
good rewards. Well, at least those five
good rewards are in the batch of s of
good rewards are in the batch of s of
data that you're training on.
the what I don't think that this
the what I don't think that this
algorithm as written is going to like
algorithm as written is going to like
scale to solve the whole task or
scale to solve the whole task or
anything
anything
but like
but like
this is qualitatively very very
this is qualitatively very very
different from just on policy RL
different from just on policy RL
yes bet so it kind of solves
yes bet so it kind of solves
it solves the like the sparse
it solves the like the sparse
exploration in in some sense
This is not world model stuff.
This is not world model stuff.
This is substantially more powerful
This is substantially more powerful
along certain axes,
right? Having a world model does not
right? Having a world model does not
tell you how to get reward. You have to
tell you how to get reward. You have to
roll out like a whole bunch of different
roll out like a whole bunch of different
planning and your planner is going to
planning and your planner is going to
suck at the start. So, that actually
suck at the start. So, that actually
doesn't fix it.
doesn't fix it.
I think it's pretty different from that
I think it's pretty different from that
bet, frankly.
Make sure I'm not trolling here. I'm
Make sure I'm not trolling here. I'm
not. See, this is commented out fully.
Why does this not work on like breakout
Why does this not work on like breakout
or snake or stuff like this?
a little bit more concerned about snake.
a little bit more concerned about snake.
Okay. So breakout has long segments.
I'll be right back. I think about this
So it's not it's not just an entropy
So it's not it's not just an entropy
thing, right?
and set some crazy entropy just to be
and set some crazy entropy just to be
sure.
Oh yeah, we're making sure that we're
Oh yeah, we're making sure that we're
actually training on this, right? Yeah,
actually training on this, right? Yeah,
we've got it.
All right, who has hypothesis?
All right, who has hypothesis?
This is working like ludicrously well on
This is working like ludicrously well on
some environments for some portions and
some environments for some portions and
not so much on others.
You're stuck imitating length 64
You're stuck imitating length 64
segments.
segments.
You don't have like a value function
You don't have like a value function
bootstrap or something like that.
bootstrap or something like that.
Do inverse RL to optimize that for what
Do inverse RL to optimize that for what
would that do for you?
would that do for you?
inver inverse uh inverse RL is to like
inver inverse uh inverse RL is to like
estimate a reward function when you
estimate a reward function when you
don't have the data labeled. We do have
don't have the data labeled. We do have
the data labeled. Why would you do that?
So I mean we can confirm that setting
So I mean we can confirm that setting
the entropy coefficiency for high
the entropy coefficiency for high
destroys the training.
This does confirm though at the very
This does confirm though at the very
least that there are problems where this
least that there are problems where this
idea of keeping around
idea of keeping around
prior good data
prior good data
is very powerful.
solves ponging, solves cart pull. Crazy
solves ponging, solves cart pull. Crazy
good on neural MMO in like seconds.
Actually behaved kind of predictably cuz
Actually behaved kind of predictably cuz
like I was pretty confident it would
like I was pretty confident it would
actually do that for neural MMO.
actually do that for neural MMO.
That's why I ran that experiment.
That's why I ran that experiment.
I didn't just do that randomly. I
I didn't just do that randomly. I
thought, oh, you know, that's an
thought, oh, you know, that's an
environment with super sparse roads, so
environment with super sparse roads, so
it makes sense.
Then why does it just get stuck at a
Then why does it just get stuck at a
point?
Well, Breakout doesn't have the same.
Well, Breakout doesn't have the same.
Okay, there's like some degenerate cases
Okay, there's like some degenerate cases
with Snake, I think.
with Snake, I think.
But with breakout,
But with breakout,
take his multi- agents
one. All right. Well, we'll try
one. All right. Well, we'll try
breakout. Let's see if you can find a
breakout. Let's see if you can find a
reason for breakout. Right. Breakout
reason for breakout. Right. Breakout
here.
here.
It does train.
We only get like 24 score though.
H I'd have to copy it to my local.
H I'd have to copy it to my local.
There's no point. We're very confident
There's no point. We're very confident
in breakout as a task, right?
We've used it extensively.
We've used it extensively.
It's a very small like change.
It's a very small like change.
We haven't changed the end of it all.
Could be. I'd have to copy it over.
I think we have all the information that
I think we have all the information that
we need though. Bet from the design of
we need though. Bet from the design of
the algorithm.
the algorithm.
You collect trajectory segments. You
You collect trajectory segments. You
label them by reward.
That gold reward is telling us the
That gold reward is telling us the
average reward of steps over that
average reward of steps over that
segment. It's still going up the whole
segment. It's still going up the whole
time.
What if breakout also rewards the random
What if breakout also rewards the random
shortest path
filter by all cases the ball starts
filter by all cases the ball starts
initially
initially
straight down reward over trains on that
straight down reward over trains on that
stuff.
Yeah. So, it would technically the best
Yeah. So, it would technically the best
thing for it to do, right,
thing for it to do, right,
uh would be to select the segments where
uh would be to select the segments where
the ball's about to strike the paddle
the ball's about to strike the paddle
and then um the ball strikes the paddle
and then um the ball strikes the paddle
and then it like it hits another brick
and then it like it hits another brick
and bounces it back. It should be able
and bounces it back. It should be able
to fit that in a 64 segment, I would
to fit that in a 64 segment, I would
think.
think.
But you're to you're totally right that
But you're to you're totally right that
it could be something weird about the
it could be something weird about the
trajectory segment bounds. Uh, oddly
trajectory segment bounds. Uh, oddly
though here it this did not do better by
though here it this did not do better by
training on more data potentially
training on more data potentially
overfitting.
Let me think how we do this.
If I do 256, Hey,
We can change BPT horizon, but it's
We can change BPT horizon, but it's
going to change the batch size and other
going to change the batch size and other
things.
It's very hard the way that this is set
It's very hard the way that this is set
up to even include a value function of
up to even include a value function of
any type.
Let's see if we can actually just get it
Let's see if we can actually just get it
to solve the task
to solve the task
um
um
by any sort of
by any sort of
here.
something like it moves corner well but
something like it moves corner well but
tries to stay alive in the uh I actually
tries to stay alive in the uh I actually
know that it cannot be that because that
know that it cannot be that because that
gets you like seven Four.
super long run just to see if it's
super long run just to see if it's
stably increasing.
I mean, so far
I mean, so far
it is like a clean, stable curve.
it is like a clean, stable curve.
I do want to see if it ever gets stuck.
there. Also, we should keep in mind
there. Also, we should keep in mind
breakout is a slow start environment,
breakout is a slow start environment,
right? Where like you have to get to a
right? Where like you have to get to a
certain point before it starts learning
certain point before it starts learning
more quickly. So, this might not be a
more quickly. So, this might not be a
horrible horrible failure, right?
horrible horrible failure, right?
Like if it just hasn't gotten to that
Like if it just hasn't gotten to that
point in the curve early on, it could
point in the curve early on, it could
very well be that once this thing gets
very well be that once this thing gets
to like 50 or whatever score, it just
to like 50 or whatever score, it just
takes off like a rocket.
It does seem to be stuck at
low 30 score.
If we look at the gold reward here,
If we look at the gold reward here,
right,
the gold reward is like not
the gold reward is like not
substantially getting better.
substantially getting better.
It's still going a little bit, but like
It's still going a little bit, but like
this is not good training speed.
No, hang on.
This is also without reuning hypers.
This is also without reuning hypers.
Fair
possible those have shifted. So most of
possible those have shifted. So most of
the hypers that we would even normally
the hypers that we would even normally
tune no longer will exist with this.
We're also we're not getting the same
We're also we're not getting the same
like curve shape. We're not getting like
like curve shape. We're not getting like
the exponential
the exponential
takeoff here, which is
probably the more concerning thing is
probably the more concerning thing is
not seeing the uh the thing like take
not seeing the uh the thing like take
offed
turning.
turning.
No, that should just be like
No, that should just be like
there are problems where at a certain
there are problems where at a certain
point, right, you need to find a little
point, right, you need to find a little
bit of reward and once you do, it's very
bit of reward and once you do, it's very
easy to learn from there. You would
easy to learn from there. You would
think you'd get the same with the
think you'd get the same with the
imitation case, right, where like once
imitation case, right, where like once
you learn to hit the ball enough, you'll
you learn to hit the ball enough, you'll
have rapidly you'll have better games
have rapidly you'll have better games
than others and you should get like a a
than others and you should get like a a
quick signal for how to learn
quick signal for how to learn
Think
how to do this?
Like
if we had expert data, this just works,
if we had expert data, this just works,
right?
We we're going to have to ultimately try
We we're going to have to ultimately try
that I think is going to be the thing
that I think is going to be the thing
for tomorrow because
for tomorrow because
like I've done enough stuff on this to
like I've done enough stuff on this to
see that there is clearly some potential
see that there is clearly some potential
and we're going to need like expert data
and we're going to need like expert data
IL for just for comparison sake like you
IL for just for comparison sake like you
know we train a breakout agent we dump a
know we train a breakout agent we dump a
big data set and we see
big data set and we see
uh how good by comparison we train on
uh how good by comparison we train on
app
like what size data set do we need to
like what size data set do we need to
train a good agent with IIL if we're
train a good agent with IIL if we're
doing that etc etc
will it just go all the way well you got
will it just go all the way well you got
to keep in mind we've been training
to keep in mind we've been training
breakout for nearly a billion steps Now,
oh, this is not great.
oh, this is not great.
Like, it has been stably improving the
Like, it has been stably improving the
whole time, but not anywhere near what
whole time, but not anywhere near what
we would want.
Yeah, it is continuing to improve.
They making the agent that
cycle of marginal.
cycle of marginal.
Um, well, how do we set this up?
Um, well, how do we set this up?
How do we set this up differently?
Like the concept of generating
Like the concept of generating
like the con like imitation learning as
like the con like imitation learning as
a concept works, right? If you have
a concept works, right? If you have
expert data, then you can imitation
expert data, then you can imitation
learn.
learn.
It would seem that you should be able to
It would seem that you should be able to
bootstrap then, right? Like if you just
bootstrap then, right? Like if you just
take your best trials and you pick the
take your best trials and you pick the
lucky ones or whatever. Yeah, we'll do
lucky ones or whatever. Yeah, we'll do
it this way. I mean, I guess technically
it this way. I mean, I guess technically
you can't train on lucky. There are
you can't train on lucky. There are
degeneracies here as well.
Not sure if work. Well, I mean RL works
Not sure if work. Well, I mean RL works
somehow, right?
Yes, bet. That's what we were trying
Yes, bet. That's what we were trying
before. You can do that. There just some
before. You can do that. There just some
issues
actually. How did they do it in here?
actually. How did they do it in here?
Did they update
update their value function.
update their value function.
See, they didn't even update their value
See, they didn't even update their value
function at all here.
function at all here.
Oh, I don't even know if they have a
Oh, I don't even know if they have a
value function. They're just doing like
value function. They're just doing like
reinforce.
reinforce.
No, they do.
Yeah. No, they do.
better at the task.
better at the task.
Um, I don't know if that quite does
Um, I don't know if that quite does
because like
because like
RL is literally learning by trial and
RL is literally learning by trial and
error at the start, right?
error at the start, right?
It's trying to do credit assignment to
It's trying to do credit assignment to
rewards.
Actually, RL is a very hard objective to
Actually, RL is a very hard objective to
learn. Behavioral cloning is a very easy
learn. Behavioral cloning is a very easy
objective to learn.
I mean, we've literally seen it solve.
I mean, we've literally seen it solve.
So, we've seen it solve two easy tasks
So, we've seen it solve two easy tasks
like instantly,
like instantly,
and we've seen it like do better than
and we've seen it like do better than
random on like everything.
random on like everything.
And we've also seen it
we've seen it do
we've seen it do
the first hard exploration bit of a very
the first hard exploration bit of a very
hard task. Way better than our
hard task. Way better than our
state-of-the-art RL.
You know, oddly enough, this would
You know, oddly enough, this would
probably pair really well with search,
probably pair really well with search,
right?
Yeah, this would pair like really well
Yeah, this would pair like really well
with search.
Let's see if we can get the base thing
Let's see if we can get the base thing
to work first.
Yeah, I have this implemented
just scale better.
Well, but Breakout, like the curves for
Well, but Breakout, like the curves for
Breakout, the thing is Breakout and Pong
Breakout, the thing is Breakout and Pong
have very similar curves, but Breakout
have very similar curves, but Breakout
just takes longer.
just takes longer.
Do I have like a breakout curve? I They
Do I have like a breakout curve? I They
look like this. I mean, they look like
look like this. I mean, they look like
the neural MMO curve, honestly. The
the neural MMO curve, honestly. The
breakout curves. It's like Yeah. And
breakout curves. It's like Yeah. And
then it sol like and then it goes up
then it sol like and then it goes up
really fast and solves. Same as pong.
really fast and solves. Same as pong.
Same as a lot of tasks.
Okay, let's try like a slightly dumber
Okay, let's try like a slightly dumber
version of this. E,
version of this. E,
I'll try a slightly dumber version of
I'll try a slightly dumber version of
this.
Oh, okay.
Trying to see if there was like stale
Trying to see if there was like stale
data steness issues.
game plan to weave this into puffer and
game plan to weave this into puffer and
proper move method. The goal is to play
proper move method. The goal is to play
around and see what works, right?
How can I say that this will be the main
How can I say that this will be the main
training method when this is like random
training method when this is like random
new research for an idea I just thought
new research for an idea I just thought
of? Right.
Okay. So, this gets 22.
Okay. So, this gets 22.
If I don't drop the rewards, what do we
If I don't drop the rewards, what do we
get?
get?
0.79 as well.
Okay, so this ended up being it was very
Okay, so this ended up being it was very
slightly better to drop out some indices
like very slightly better.
how jittery.
Like if you made this horizon equal to
Like if you made this horizon equal to
one, you would only ever get data where
one, you would only ever get data where
you have the reward, right?
The fact that you don't have the reward
The fact that you don't have the reward
here is actually kind of bad because you
here is actually kind of bad because you
can't do any credit assignment at all.
You do offline RL instead of behavioral
You do offline RL instead of behavioral
clothing here. Is that the play?
You even set that up. Much harder, isn't
You even set that up. Much harder, isn't
it?
it?
It would make more sense though because
It would make more sense though because
you have the record function, I suppose.
you have the record function, I suppose.
though it makes it harder if you want to
though it makes it harder if you want to
actually add expert data
Why I don't understand why it is the
Why I don't understand why it is the
case that
case that
long and breakout are pretty
long and breakout are pretty
substantially right
substantially right
and breakout are like really
and breakout are like really
substantially
using the same hypers on
I not using the same hypers on them
I'm not using the hypers All
right.
Just a random little Okay.
It doesn't instantly solve our problem
like it does fair bit more work. back.
Okay.
Okay. Next question.
Okay. Next question.
These are separate tunes, right?
These are separate tunes, right?
This
Okay, that is a bit better.
Okay, that is a bit better.
Still not like insta solving our task by
Still not like insta solving our task by
any means.
It still just has this property of being
It still just has this property of being
really slow. Like it's not taking off at
really slow. Like it's not taking off at
all.
all.
get us to 27.
get us to 27.
Let's like
Fact that more updates doesn't do
Fact that more updates doesn't do
anything is also weird to me.
See, like it's not
it's not like
it's not like
it gets like sort of stuck.
it gets like sort of stuck.
And we've seen that if we just run it
And we've seen that if we just run it
for super long, we can get it unstuck.
Slightly worse though.
What else we could do?
You want more research gems like squared
You want more research gems like squared
all for?
Yeah. So, actually that's the other
Yeah. So, actually that's the other
project I have planned. I'm back from
project I have planned. I'm back from
RLC is if you look up B Suite heat mine
RLC is if you look up B Suite heat mine
paper it's like a diagnostic end suite.
paper it's like a diagnostic end suite.
Um
Um
we're going to do a much better version
we're going to do a much better version
of that as a core project. Uh it's not
of that as a core project. Uh it's not
an easy project though, mind you,
an easy project though, mind you,
because like implementing the
because like implementing the
environments is going to be very easy.
environments is going to be very easy.
But um actually coming up with good
But um actually coming up with good
clear-cut environments that are properly
clear-cut environments that are properly
motivated, like a minimal set that
motivated, like a minimal set that
actually tests what you care about is
actually tests what you care about is
hard. very very hard.
Down to help. Sure.
Down to help. Sure.
If you go look at the B Suite paper, I'm
If you go look at the B Suite paper, I'm
warning you that this is like actually
warning you that this is like actually
very hard research. Like this is a task
very hard research. Like this is a task
that I would expect the vast majority of
that I would expect the vast majority of
RL researchers to fail on. Um because if
RL researchers to fail on. Um because if
you look at the B Suite tasks, they're
you look at the B Suite tasks, they're
like kind of motivated, but if you
like kind of motivated, but if you
actually really think about it, like
actually really think about it, like
they actually don't do a good job of of
they actually don't do a good job of of
being like a minimal test suite and they
being like a minimal test suite and they
kind of just throw a bunch of buzzwords
kind of just throw a bunch of buzzwords
out like exploration and generalization
out like exploration and generalization
without really really clearly thinking
without really really clearly thinking
about mechanistically what they're
about mechanistically what they're
testing.
I'd say that this has been pretty good
I'd say that this has been pretty good
progress overall though, right?
progress overall though, right?
We see that the simplest instantiation
We see that the simplest instantiation
of this algorithm
of this algorithm
uh can solve several different
uh can solve several different
environments.
We'll have to think about whether like
We'll have to think about whether like
I mean it would be kind of insane if
I mean it would be kind of insane if
this just replaced all RL, right?
this just replaced all RL, right?
I think that that would be kind of an
I think that that would be kind of an
insane thing to expect.
insane thing to expect.
The question is like does this combine
The question is like does this combine
nicely with RL? Does this benefit from
nicely with RL? Does this benefit from
data scale? Right? There are a lot of
data scale? Right? There are a lot of
things that we'd want to test here.
The fact though that this actually does
The fact though that this actually does
something on neural MMO 3 is kind of
something on neural MMO 3 is kind of
insane.
Like I don't to be clear I don't think
Like I don't to be clear I don't think
that if I just run it for longer on that
that if I just run it for longer on that
it will actually be better. But like the
it will actually be better. But like the
fact that it latches on to something
fact that it latches on to something
that hard that quickly is very
that hard that quickly is very
impressive.
impressive.
Do a quick lunar lander run. We don't
Do a quick lunar lander run. We don't
have lunar lander in here. Did somebody
have lunar lander in here. Did somebody
PR it?
I didn't think it was PR yet.
Is it this invert?
Oh, Kinvert has a bunch of nice little
Oh, Kinvert has a bunch of nice little
PRs.
L.
Yeah, there's no lunar lander, man.
Yeah, there's no lunar lander, man.
Somebody was working on it, I saw, but
Somebody was working on it, I saw, but
it hasn't been PR. I don't know if that
it hasn't been PR. I don't know if that
was you or not.
I'm going to go do some thinking about
I'm going to go do some thinking about
this. I got to go get dinner, folks. Um,
this. I got to go get dinner, folks. Um,
I will be back streaming tomorrow.
But this is pretty cool to get like
But this is pretty cool to get like
initial results day one of this.
initial results day one of this.
I'm going to have to think very
I'm going to have to think very
carefully though about how how I go
carefully though about how how I go
about this and such.
Actually, just commit this before I
Actually, just commit this before I
forget.
my everybody must be dreaming. Uh there
my everybody must be dreaming. Uh there
is the like in like the original lunar
is the like in like the original lunar
lander which is unusably slow. There's
lander which is unusably slow. There's
somebody who is working on Lunar Lander
somebody who is working on Lunar Lander
in the Discord, but I don't I don't
in the Discord, but I don't I don't
think we've gotten a PR from them yet.
think we've gotten a PR from them yet.
So, we kind of can't uh can't do
So, we kind of can't uh can't do
anything with it just yet.
Okay,
Okay,
dropped a quick PR
dropped a quick PR
uh in the
uh in the
dev channel. Let me see. Linky, you
dev channel. Let me see. Linky, you
caught me just in time as I was about to
caught me just in time as I was about to
go.
Is that 800k SPS?
Is that 800k SPS?
Wait, vision test works.
I do need to ask if you'd prefer I
I do need to ask if you'd prefer I
include Rayb's camera as a file or as an
include Rayb's camera as a file or as an
actual project.
actual project.
Do you mean as an actual import?
Do you mean as an actual import?
Uh, you can do either. If you want to
Uh, you can do either. If you want to
just like
just like
if you want to modify our ray download
if you want to modify our ray download
so it includes the camera, that's fine.
so it includes the camera, that's fine.
Either one works.
Either one works.
Wait, 800k SPS? I'm going to have to see
Wait, 800k SPS? I'm going to have to see
this.
this.
If you actually got the thing to render
If you actually got the thing to render
at 800k SPS, that was awesome. Last time
at 800k SPS, that was awesome. Last time
we checked, like it wasn't actually
we checked, like it wasn't actually
computing anything, right?
I will be around tomorrow to look at
I will be around tomorrow to look at
that. You have that to look at.
that. You have that to look at.
If
um
no link key I don't think this is a
no link key I don't think this is a
reasonable thing to include
reasonable thing to include
using rays now what does that mean
using rays now what does that mean
kitting uh I don't think that this is a
kitting uh I don't think that this is a
reasonable thing to include link key
reasonable thing to include link key
Because
Because
if you set it to auto,
if you set it to auto,
like I think it tries to mimic the
like I think it tries to mimic the
number of M's or whatever, you
number of M's or whatever, you
definitely don't want it to be equal to
definitely don't want it to be equal to
just the number of cores regardless of
just the number of cores regardless of
how many M's you have. That will just
how many M's you have. That will just
throw you a hard error if they're not
throw you a hard error if they're not
divisible.
divisible.
Mimicking the camera by doing 64 rays
Mimicking the camera by doing 64 rays
and getting collisions on those.
800k SPS raycast renderer. Huh.
That's kind of funny.
Yeah, that's really funny.
So, I guess for a basic scene, yeah,
So, I guess for a basic scene, yeah,
that works. That's super funny.
that works. That's super funny.
That's actually a good test to have, to
That's actually a good test to have, to
be honest with you.
be honest with you.
I'd be interested to see how it works on
I'd be interested to see how it works on
like my higherend hardware. I'll take a
like my higherend hardware. I'll take a
look through the PRs tomorrow. Is that a
look through the PRs tomorrow. Is that a
satisfactory answer though, Linky? For
satisfactory answer though, Linky? For
now,
now,
I want to head to dinner in a second.
Okay. Well, I'm going to call it for the
Okay. Well, I'm going to call it for the
day, folks. Um, thank you for tuning in.
day, folks. Um, thank you for tuning in.
If you're interested in this work
If you're interested in this work
generally, a few small things in case
generally, a few small things in case
you didn't see it. This
you didn't see it. This
uh talk is on X at J Suarez 5341 same as
uh talk is on X at J Suarez 5341 same as
my Twitch handle everything else. Um my
my Twitch handle everything else. Um my
full talk the uh the paper that won an
full talk the uh the paper that won an
award at RLC only a few minutes like 7
award at RLC only a few minutes like 7
minutes 38 seconds if you want to watch
minutes 38 seconds if you want to watch
that
that
discusses all the stuff in Puffer go
discusses all the stuff in Puffer go
ahead and take that look at that. If
ahead and take that look at that. If
you're interested in my work more
you're interested in my work more
generally or you want to get involved,
generally or you want to get involved,
buffer.ai,
buffer.ai,
star the repo. Really helps me out a
star the repo. Really helps me out a
ton. It's free. Just star it. You can
ton. It's free. Just star it. You can
jump in the Discord if you want to get
jump in the Discord if you want to get
involved with dev. Other than that, on X
involved with dev. Other than that, on X
we have resources. Of course, this is
we have resources. Of course, this is
going to open a popup. But yeah, on X we
going to open a popup. But yeah, on X we
have resources for beginners
have resources for beginners
and just lots of RL material in general.
and just lots of RL material in general.
These guides in particular cover
These guides in particular cover
everything you need to know.
everything you need to know.
So, thank you folks and I will be back
So, thank you folks and I will be back
tomorrow.
