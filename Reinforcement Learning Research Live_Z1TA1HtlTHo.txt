Kind: captions
Language: en
Okay,
Okay,
be live here.
There we go.
There we go.
Hi.
A few things to get done today.
Mostly it's the uh meta env and robotics
Mostly it's the uh meta env and robotics
work are the two main things.
Trying to think what we do with this.
Trying to think what we do with this.
Let me let me pull up some results and
Let me let me pull up some results and
sort of just show you what I'm uh what
sort of just show you what I'm uh what
I'm looking at here.
I'm looking at here.
Okay, so we ran this overnight.
Uh this cube sweep doesn't find anything
Uh this cube sweep doesn't find anything
good at all. So, we'll come back to
good at all. So, we'll come back to
this.
That's inconclusive.
That's inconclusive.
But this one's a lot weirder.
You see
You see
this gets up to 3.5ish.
200 experiments worth of data.
But then the thing that's weird about
But then the thing that's weird about
this is that we did these 200
this is that we did these 200
experiments.
experiments.
We did quite a bit better.
Interesting.
Yeah, I ran the five one for longer and
Yeah, I ran the five one for longer and
it does actually reproduce,
it does actually reproduce,
but then it oscillates between four and
but then it oscillates between four and
five unstably.
Maybe we need to just reproduce this
Maybe we need to just reproduce this
policy and then look at it, you
Damn,
kind of tired.
Got should have gotten enough sleep, but
Got should have gotten enough sleep, but
I've been studying quantum mechanics and
I've been studying quantum mechanics and
uh like just this mind-boggling topic.
uh like just this mind-boggling topic.
Um, even the basic stuff is very
Um, even the basic stuff is very
mindboggling. It's just very
mindboggling. It's just very
unintuitive. So, I think I fell asleep
unintuitive. So, I think I fell asleep
thinking about wave functions.
Was it wave functions? No, I fell
Was it wave functions? No, I fell
asleep. It was even weirder.
asleep. It was even weirder.
I um I fell asleep thinking about the
I um I fell asleep thinking about the
nature of fields in general and whether
nature of fields in general and whether
fields can be described by particles.
fields can be described by particles.
um
um
whether it's a model or whether it's an
whether it's a model or whether it's an
actual phenomena, I don't know.
I'm having a good time kind of on the
I'm having a good time kind of on the
side here. Um,
I'm having a good time on this side here
I'm having a good time on this side here
just kind of like going through
just kind of like going through
different areas hard science like
different areas hard science like
chemistry, physics, trying to get like a
chemistry, physics, trying to get like a
low-level understanding of different
low-level understanding of different
areas of science.
areas of science.
Mainly I'm looking to like
Mainly I'm looking to like
figure out enough of uh the fundamentals
figure out enough of uh the fundamentals
here to figure out like at what level of
here to figure out like at what level of
simulation what levels of simulation are
simulation what levels of simulation are
feasible.
So yeah, we need all these hypers
and this should actually give us a
and this should actually give us a
pretty cool to look at policy within a a
pretty cool to look at policy within a a
few minutes of training it.
And I will want to what I'll do is I'll
And I will want to what I'll do is I'll
take these hypers and then I'll go
take these hypers and then I'll go
compare these to the neural MMO ones.
compare these to the neural MMO ones.
Um, basically I just want to figure out
Um, basically I just want to figure out
if these look sane to me
and then we'll also look at the
and then we'll also look at the
behaviors that are learned.
Like this gamma is super weird.
Like this gamma is super weird.
The fact that like you have a gamma
The fact that like you have a gamma
that's that ludicrously high
like that doesn't sit right with me.
like that doesn't sit right with me.
Like lambda is reasonable but gamma is
Like lambda is reasonable but gamma is
way higher than I would expect for this
way higher than I would expect for this
problem.
It's actually somewhat disturbing that
It's actually somewhat disturbing that
despite all the progress on 3 0 and like
despite all the progress on 3 0 and like
despite all the progress that we've made
despite all the progress that we've made
with making reinforcement learning
with making reinforcement learning
pretty sane and reproducible
pretty sane and reproducible
um we know what the hyperparameters do
um we know what the hyperparameters do
and the experimental data does not fit
and the experimental data does not fit
the expectations of literally anyone in
the expectations of literally anyone in
the field.
the field.
That is somewhat concerning.
And actually, this is like something
And actually, this is like something
that's probably pretty difficult for new
that's probably pretty difficult for new
people in RL is like, you know, they'll
people in RL is like, you know, they'll
ask me tons of questions and most of the
ask me tons of questions and most of the
time I can answer stuff, but then the
time I can answer stuff, but then the
thing is like you don't even know what
thing is like you don't even know what
questions have answers to them
questions have answers to them
basically. Um, coming into reinforcement
basically. Um, coming into reinforcement
learning, like there's just a lot of
learning, like there's just a lot of
stuff where it's like, yeah, we have
stuff where it's like, yeah, we have
some idea. we have predictions of how uh
some idea. we have predictions of how uh
this should work but in practice it
this should work but in practice it
doesn't work that way at all and nobody
doesn't work that way at all and nobody
knows why and there just tons and tons
knows why and there just tons and tons
and tons of things like that in RL
and tons of things like that in RL
and you hit it very quickly as well you
and you hit it very quickly as well you
know like
know like
if you're studying physics or you're
if you're studying physics or you're
studying chemistry it sometimes it can
studying chemistry it sometimes it can
take you a while to like come up with
take you a while to like come up with
questions um that like nobody knows the
questions um that like nobody knows the
answer to or like is not related to some
answer to or like is not related to some
other field of study. In reinforcement
other field of study. In reinforcement
learning, you can very very quickly run
learning, you can very very quickly run
into questions where it's like, yeah,
into questions where it's like, yeah,
nobody knows
and just nobody knows at all. Good luck.
Like, well, that's silly. Surely we
Like, well, that's silly. Surely we
should just be able to answer that. And
should just be able to answer that. And
then you go spend two months running
then you go spend two months running
experiments and you're like, okay, yeah,
experiments and you're like, okay, yeah,
nobody knows
is ridiculous. But that is the sad state
is ridiculous. But that is the sad state
of the field.
Of course, we've made some major
Of course, we've made some major
improvements to that.
Now we're at like all right, we know it
Now we're at like all right, we know it
works. We don't know exactly precisely
works. We don't know exactly precisely
why or how, but at least it does work.
why or how, but at least it does work.
Whereas before it was like sometimes it
Whereas before it was like sometimes it
works maybe.
No cuda jeep. Lovely.
Some reason cuda likes to detach from
Some reason cuda likes to detach from
Docker.
All right. So, that's like a three
All right. So, that's like a three
minute experiment tops.
We'll see if this repros as it should.
But if this does repro, then we actually
But if this does repro, then we actually
should be able to look at the policy,
should be able to look at the policy,
which I don't think I did before.
which I don't think I did before.
I don't know why I didn't bother doing
I don't know why I didn't bother doing
that, but that's like the super obvious
that, but that's like the super obvious
thing, right?
thing, right?
Look at the policy.
What the heck?
Huh?
Something's off here completely.
This does not work period like at all.
This does not work period like at all.
So,
So,
first thing to check is that I messed up
first thing to check is that I messed up
a hyper maybe.
Very likely I mess up a hyper.
Very likely I mess up a hyper.
I really should not be copying these
I really should not be copying these
things manually.
things manually.
Kind of bad practice and I should
Kind of bad practice and I should
probably just have a way to do it
probably just have a way to do it
automatically.
automatically.
This is 108 mil.
Got Veum M. Oh, you know what it is?
Got Veum M. Oh, you know what it is?
It's just the reward coefficients.
It's just the reward coefficients.
Now, that's easy. I can fix that.
Now, that's easy. I can fix that.
I'd forgotten that I'd swept these as
I'd forgotten that I'd swept these as
well. So, you actually have um
well. So, you actually have um
This is one of the things that we do
This is one of the things that we do
that uh almost nobody does. And it's
that uh almost nobody does. And it's
like this is just straight incompetence
like this is just straight incompetence
that nobody does it.
that nobody does it.
At least in
At least in
that's not being entirely fair, I guess,
that's not being entirely fair, I guess,
because the thing is they don't have our
because the thing is they don't have our
tools. So, it's actually quite difficult
tools. So, it's actually quite difficult
to do without our tools. But it's like
to do without our tools. But it's like
it's one of those things where there's
it's one of those things where there's
really no excuse for not doing it
really no excuse for not doing it
because it's your stuff just won't work.
because it's your stuff just won't work.
Like it's like ah it's hard so we won't
Like it's like ah it's hard so we won't
do it. But like you just if you don't
do it. But like you just if you don't
sweep reward coefficients and complex
sweep reward coefficients and complex
Ms. Your stuff just won't work.
Let's see if this does better. Welcome
Let's see if this does better. Welcome
YouTube folks.
dead
plan for today is going to be meta
plan for today is going to be meta
policies. Uh we figure out we're going
policies. Uh we figure out we're going
to pull these up once I actually get
to pull these up once I actually get
this to repro. We will see what that
this to repro. We will see what that
looks like. We'll see whether we think
looks like. We'll see whether we think
it can learn much better or whether this
it can learn much better or whether this
copy of the end is just solved.
copy of the end is just solved.
We'll send that over to those guys. They
We'll send that over to those guys. They
have some other stuff for me to do as
have some other stuff for me to do as
well, but at least we're going to get
well, but at least we're going to get
this to them today.
this to them today.
And I do want to understand why our
And I do want to understand why our
stuff just does not work on these new
stuff just does not work on these new
robotics ends.
robotics ends.
Could be the policy, could be the setup,
Could be the policy, could be the setup,
the end finding, could be a number of
the end finding, could be a number of
things, but we're going to try to figure
things, but we're going to try to figure
that out today.
Welcome boxing.
Welcome boxing.
Finally have something that somehow
Finally have something that somehow
trains.
trains.
Does it solve like the comments problem?
like you actually have something that
like you actually have something that
solves the commons problem.
I'd actually be very interested if that
I'd actually be very interested if that
worked
cuz like humans usually fail that On.
metric is percentage of agents that
metric is percentage of agents that
manage to make it out alive at the end.
manage to make it out alive at the end.
Get 40% surviving agents.
Have you actually watched what the
Have you actually watched what the
policy does?
Definitely some improvements. Cool.
Definitely some improvements. Cool.
Oh, we're at
This is a pretty different learning
This is a pretty different learning
curve than I was getting before. Now,
why is this lower?
why is this lower?
Did I miss something again or is it
Did I miss something again or is it
lucky?
You stopped rushing the food and somehow
You stopped rushing the food and somehow
wait before clicking.
wait before clicking.
Also use the new puffers stars.
That is called pandering and it does
That is called pandering and it does
work.
It just it amuses me to no end. We have
It just it amuses me to no end. We have
the ends. They're just all the puffers.
So funny.
So funny.
Yeah. So, this only gets like three.
Can we not just like
Just compare
that. These are always matching the
that. These are always matching the
green one.
These all match.
Unless I miss something else.
You're the one who asked me for the
You're the one who asked me for the
stars.
stars.
Yes, I like I said, that does work. It
Yes, I like I said, that does work. It
does amuse me very much.
Better logs is what? Yeah, it's always,
Better logs is what? Yeah, it's always,
man, it's always it's always a data
man, it's always it's always a data
problem of some type. Almost always. And
problem of some type. Almost always. And
you just you need good logs. And good
you just you need good logs. And good
logs is not the same as log everything,
logs is not the same as log everything,
right? Like this project I'm working on
right? Like this project I'm working on
here, they've got hundreds of logs and
here, they've got hundreds of logs and
it's just it's too much. You really you
it's just it's too much. You really you
just need to be tracking like key
just need to be tracking like key
metrics that actually matter and you
metrics that actually matter and you
just need to think about them a little
just need to think about them a little
bit.
If you do that, you'll pretty much
If you do that, you'll pretty much
always be in a good spot.
build to run 257 experiments.
Not there.
Not there.
TPU has been
TPU has been
Yeah,
Yeah,
TPUs are pretty useful, it turns out.
Yeah, that's just straight up not going
Yeah, that's just straight up not going
to be it, man. If you find yourself
to be it, man. If you find yourself
deving a new environment and then trying
deving a new environment and then trying
to do new algorithm research at the same
to do new algorithm research at the same
time because it doesn't work, you're
time because it doesn't work, you're
almost always wrong.
almost always wrong.
Almost always wrong.
Almost always wrong.
In fact, there is actually um it was
In fact, there is actually um it was
Wooden added uh a research M
Wooden added uh a research M
specifically for testing DM.
specifically for testing DM.
You want to mess with that.
It's like a very simple environment
It's like a very simple environment
where we should be able to test with and
where we should be able to test with and
without and like actually get some
without and like actually get some
results.
I think it's convert circle,
I think it's convert circle,
isn't it? The grid. No, grid is the maze
isn't it? The grid. No, grid is the maze
env. It's similar. Well, it's similar to
env. It's similar. Well, it's similar to
convert. So, basically, I got him to do
I got him to make like this N except the
I got him to make like this N except the
uh the targets are in uh a circle. So,
uh the targets are in uh a circle. So,
it's like the idea is that
it's like the idea is that
where did he put it somewhere?
Maybe he put it somewhere.
Oh.
Not telling me I'm forgetting meeting
to read an article. Fine.
there's any images in uh
Oh, yeah. So, here,
Oh, yeah. So, here,
for some reason, this is not expanding
for some reason, this is not expanding
correctly, but you can kind of see the
correctly, but you can kind of see the
stars are in a circle. So, the goal is
stars are in a circle. So, the goal is
going to be we spawn all the puffers at
going to be we spawn all the puffers at
the center and they're going to have to
the center and they're going to have to
go to a star
go to a star
and we'll basically just see if they
and we'll basically just see if they
always go to the same one or not.
Spencer's article.
Yeah, that's a good image.
Good article other than the sounds.
Oh, okay. Yeah.
Oh, okay. Yeah.
Uh, I don't know what was happening
Uh, I don't know what was happening
before, but uh, yeah, guys, I think we
before, but uh, yeah, guys, I think we
got it to work. Yeah. Okay.
I love it when that happens.
trying to remember what the hell all the
trying to remember what the hell all the
commands were that I had in order to uh
commands were that I had in order to uh
start all the servers and [ __ ]
start all the servers and [ __ ]
things.
things.
Uh
probably Yes.
Okay.
Okay.
We have to do eval with this. And then
We have to do eval with this. And then
we have to turn on the
send them this replay as soon as we make
send them this replay as soon as we make
sure that it's any Good.
I got to love how the play button
I got to love how the play button
doesn't work, but the scroll button
doesn't work, but the scroll button
does.
This guy's got six batteries
This guy's got six batteries
and two ore. So, he's going to make two
and two ore. So, he's going to make two
hearts out of this, right?
The replayer is kind of glitchy, but
so much header. Just the ray viewer,
isn't Oh, isn't it? Yeah, that's an old
isn't Oh, isn't it? Yeah, that's an old
message. Cool.
YouTube chat doesn't seem to be working
YouTube chat doesn't seem to be working
really.
Uh, that would be bizarre.
Oh,
Oh,
hi Brad.
hi Brad.
Um, yeah, Sakana's cool. I don't know
Um, yeah, Sakana's cool. I don't know
why my reream chat is not uh
why my reream chat is not uh
broadcasting that. That is weird. Hang
broadcasting that. That is weird. Hang
on. Did I pop out the wrong chat or
on. Did I pop out the wrong chat or
something?
No, I've had this exact same chat. Oh,
No, I've had this exact same chat. Oh,
hi Brad.
hi Brad.
Um, yeah.
Um, yeah.
I don't know why my chat is not.
I don't know why my chat is not.
Yeah, just Oh, no. Now it shows up.
Yeah, just Oh, no. Now it shows up.
wrong chat or something. Does it show up
wrong chat or something. Does it show up
on the screen? It doesn't. I've had this
on the screen? It doesn't. I've had this
exact same chat. Wait, it does now.
I didn't even do anything and it shows
I didn't even do anything and it shows
up on both windows.
up on both windows.
Can one of you write something? See if
Can one of you write something? See if
it works for non-adins.
it works for non-adins.
Oh, that's probably horrendous with the
Oh, that's probably horrendous with the
double audio.
double audio.
My bad.
have done and it doesn't. Oh, there it
have done and it doesn't. Oh, there it
goes. Yeah, now I see it. I don't know
goes. Yeah, now I see it. I don't know
what that was, man. That's just reream
what that was, man. That's just reream
or YouTube being weird. All right.
or YouTube being weird. All right.
Hopefully double audio bug is fixed. I
Hopefully double audio bug is fixed. I
was wondering why there was nobody in
was wondering why there was nobody in
the chat.
the chat.
Hey, An's
Hey, An's
still here. Hello.
Yeah. So, Sakana's cool. Sakana's David
Yeah. So, Sakana's cool. Sakana's David
Ha's thing. I have a colleague that does
Ha's thing. I have a colleague that does
well that did some work with them. Uh,
well that did some work with them. Uh,
I've interacted with David Ha a few
I've interacted with David Ha a few
times. I saw him at Nurups last year as
times. I saw him at Nurups last year as
well. Uh, they do cool stuff. David Ha's
well. Uh, they do cool stuff. David Ha's
always kind of been like one of the cool
always kind of been like one of the cool
like has crazy out there ideas guys.
like has crazy out there ideas guys.
He's actually I think among the like
He's actually I think among the like
crazy ideas people. He's been one of the
crazy ideas people. He's been one of the
ones who's actually like gotten them to
ones who's actually like gotten them to
work a bit more consistently.
work a bit more consistently.
So,
So,
and it seems like a lot of their stuff
and it seems like a lot of their stuff
they're doing at the moment is also just
they're doing at the moment is also just
like they're one just trying to do crazy
like they're one just trying to do crazy
out there research that isn't the norm,
out there research that isn't the norm,
which is awesome. And then B is just
which is awesome. And then B is just
like, you know, they're also applying
like, you know, they're also applying
doesn't have to be super super
doesn't have to be super super
state-of-the-art, but pretty good uh
state-of-the-art, but pretty good uh
modern AI stuff just to all of the crap
modern AI stuff just to all of the crap
in Japanese government. So, they're
in Japanese government. So, they're
trying to just help streamline stuff.
trying to just help streamline stuff.
It's like a national lab,
It's like a national lab,
which is cool.
All right. So, this is going to run for
All right. So, this is going to run for
quite a while here.
quite a while here.
Let me just send these meta guys this
Let me just send these meta guys this
replay.
added 2048 with a few changes. Could you
added 2048 with a few changes. Could you
do another review? Yep. Give me one
do another review? Yep. Give me one
minute and I will get right on that.
actually gotten it to solve 2048.
actually gotten it to solve 2048.
I didn't manage out of the box with any
I didn't manage out of the box with any
other RL repo.
We do try to make good stuff here, man.
We do try to make good stuff here, man.
I think there's a lot of improvements to
I think there's a lot of improvements to
be made, but pretty exciting.
be made, but pretty exciting.
Yeah, that's cool. Let me see if we can
Yeah, that's cool. Let me see if we can
even do better. Let's we'll take a quick
even do better. Let's we'll take a quick
look. Let me use a restroom real quick.
look. Let me use a restroom real quick.
Grab another cup of coffee so I'm fresh
Grab another cup of coffee so I'm fresh
and then we'll uh we'll get on that. I
and then we'll uh we'll get on that. I
just figured out the meta stuff. I think
just figured out the meta stuff. I think
that's pretty solid. I to give you an
that's pretty solid. I to give you an
idea, they've had like they have a big
idea, they've had like they have a big
team working on this thing and uh we
team working on this thing and uh we
just set out with like a model we
just set out with like a model we
trained in 10 minutes. So, heck yeah. Be
trained in 10 minutes. So, heck yeah. Be
right back, Anick. And then you'll get
right back, Anick. And then you'll get
your review.
Yes. And uh about the morning deadlift.
Okay, let's get to uh
Okay, let's get to uh
2048.
Oh, and let me reply to Kyle real quick.
Oh, and let me reply to Kyle real quick.
do that.
What I found weird is that with RNN
What I found weird is that with RNN
enabled, it worked much better. Even
enabled, it worked much better. Even
though the end is marovian,
though the end is marovian,
maybe the normal policy has too few
maybe the normal policy has too few
parameters.
parameters.
Uh I it it adds some parameters and it
Uh I it it adds some parameters and it
adds like a reasonable layer structure.
adds like a reasonable layer structure.
We just throw it on by default because
We just throw it on by default because
like our RNN implementation is fast. So
like our RNN implementation is fast. So
like we don't really care. we just use
like we don't really care. we just use
it by default. Plus, it can let the
it by default. Plus, it can let the
agent like it gives the agent working
agent like it gives the agent working
memory in case it wants to use it for
memory in case it wants to use it for
anything.
Like most people, the reason that most
Like most people, the reason that most
people don't throw LSTMs on everything
people don't throw LSTMs on everything
is because their implementations are
is because their implementations are
bad. That's it. That's the only reason.
bad. That's it. That's the only reason.
Like if you actually look at the flops,
Like if you actually look at the flops,
it's like a perfectly reasonable thing
it's like a perfectly reasonable thing
to just chuck in as a layer. LSTMs are
to just chuck in as a layer. LSTMs are
incredibly fast. Like you can train a
incredibly fast. Like you can train a
1024 dim LSTM at a million steps per
1024 dim LSTM at a million steps per
second. Like they're really really fast.
second. Like they're really really fast.
But the thing is in most implementations
But the thing is in most implementations
they're very very slow. And it's purely
they're very very slow. And it's purely
because people suck at coding. That's
because people suck at coding. That's
it. There's no fundamental problem with
it. There's no fundamental problem with
them.
Okay. Where's 2048 here?
Okay. Where's 2048 here?
Added 2048 ocean ends.
All changes should be good now.
Got some hypers here.
Is this meant to be two?
Two billion.
Two billion.
Two billion.
Huh. That's a surprisingly large number
Huh. That's a surprisingly large number
of steps to have to train 2048.
of steps to have to train 2048.
Did you get these from a hyper pram
Did you get these from a hyper pram
sweep? Like a full sweep?
sweep? Like a full sweep?
I'd be surprised if it takes two billion
I'd be surprised if it takes two billion
uh steps to solve 2048.
values for any are just my working
values for any are just my working
stuff.
stuff.
Not sure if these are the newest prams.
Not sure if these are the newest prams.
Gotcha.
copied them from another end. Yeah. So,
copied them from another end. Yeah. So,
you don't want to do that. You want to
you don't want to do that. You want to
run a sweep. I'll help you though. I'll
run a sweep. I'll help you though. I'll
help you. I'll help you do that.
help you. I'll help you do that.
Do you have a GPU to run stuff on or no?
Do you have a GPU to run stuff on or no?
Because if not, I can just set something
Because if not, I can just set something
up for you.
I'm supposed to drink the mug left like
I'm supposed to drink the mug left like
this left-handed the MIT mug.
Everyone always comments on the dumb
Everyone always comments on the dumb
mugs.
My thing is when you're a researcher you
My thing is when you're a researcher you
get a lot of mugs. Like all the
get a lot of mugs. Like all the
conferences give mugs out, universities
conferences give mugs out, universities
give mugs out. Like all they like they
give mugs out. Like all they like they
always just give mugs. They know that
always just give mugs. They know that
all you do is drink coffee,
all you do is drink coffee,
right? Code
[Music]
B 2048
work. So, I run it over the weekend.
I can help you set something up
I can help you set something up
initially.
initially.
We do give uh repeat contributors
We do give uh repeat contributors
hardware access.
hardware access.
I need to buy some more desktops to be
I need to buy some more desktops to be
fair, but you have some capacity.
Is this thing superhuman on Tetris? I
Is this thing superhuman on Tetris? I
think it's a little better than I am,
think it's a little better than I am,
but I don't think it's super human. I
but I don't think it's super human. I
mean, we like to be fair, we have not
mean, we like to be fair, we have not
tried we haven't really tried very hard
tried we haven't really tried very hard
on any of the individual arcade ms. Like
on any of the individual arcade ms. Like
you could run it for longer and it would
you could run it for longer and it would
keep getting better most likely, you
keep getting better most likely, you
know, or you could train a bigger policy
know, or you could train a bigger policy
and it would keep getting better.
and it would keep getting better.
It's pretty cool to watch though, right?
It's pretty cool to watch though, right?
It's like a pretty mesmerizing demo to
It's like a pretty mesmerizing demo to
watch.
watch.
I really like this environment. It came
I really like this environment. It came
out very, very well,
out very, very well,
right?
right?
This is just fun to watch.
It's definitely a heck of a lot faster
It's definitely a heck of a lot faster
than uh most humans are. I think maybe
than uh most humans are. I think maybe
like the top top players are a bit
like the top top players are a bit
faster than this,
faster than this,
but like it's pretty cool.
Green screen save a background to
Green screen save a background to
website. I mean you can technically do
website. I mean you can technically do
that like
that like
this. The website itself is open source
this. The website itself is open source
and this is literally just um embedded
and this is literally just um embedded
web assembly.
So it's like actually very easy to load
So it's like actually very easy to load
this stuff if you just go there's a a
this stuff if you just go there's a a
repo that's instead of puffer lib it's
repo that's instead of puffer lib it's
puffer.ai as a repo and it just hosts
puffer.ai as a repo and it just hosts
everything.
everything.
So you're free to embed it as long as
So you're free to embed it as long as
you uh you know credit the original like
you uh you know credit the original like
you know from puffer liib
you know from puffer liib
or from puffer.ai.
I believe that was Adrian's uh one of
I believe that was Adrian's uh one of
Adrian's ends as well. Adrien should be
Adrian's ends as well. Adrien should be
credited somewhere there.
credited somewhere there.
Yeah. Bye, Adrian.
This is theh.
Let me make sure there aren't any errors
Let me make sure there aren't any errors
in here.
in here.
Here's some bite and magic three. The
Here's some bite and magic three. The
current AI is trash. Yeah. Yeah. So, the
current AI is trash. Yeah. Yeah. So, the
difficult part of training it on um like
difficult part of training it on um like
real games is not even that the problems
real games is not even that the problems
are hard, it's that the games are slow.
are hard, it's that the games are slow.
So, like we can pretty much do it on any
So, like we can pretty much do it on any
individual game we want. It just gets
individual game we want. It just gets
expensive.
Theoretically, my lab also has
Theoretically, my lab also has
resources, a few A10s. That's painful. I
resources, a few A10s. That's painful. I
mean, we have Can you see this wall of
mean, we have Can you see this wall of
servers behind me? We technically we've
servers behind me? We technically we've
got 20 4090s that we can use for uh
got 20 4090s that we can use for uh
trading stuff.
This is one of the nice things that
This is one of the nice things that
we're finally in a pretty good spot
we're finally in a pretty good spot
with.
with.
See is a little buggy. Not too concerned
See is a little buggy. Not too concerned
about that. I'm just concerned about
about that. I'm just concerned about
making sure there's no major errors in
making sure there's no major errors in
here.
here.
H unsigned char int float
H unsigned char int float
r
Huh?
4090 better than an A10? Yes, hugely
for reinforcement learning. If you're
for reinforcement learning. If you're
not using a Bflat 16, uh a 4090 is
not using a Bflat 16, uh a 4090 is
basically the same speed as an 800.
basically the same speed as an 800.
which is crazy because the 800 is more
which is crazy because the 800 is more
than 10 times the price,
than 10 times the price,
but it's true. This is the arbitrage,
but it's true. This is the arbitrage,
right? This is why I don't rent our
right? This is why I don't rent our
hardware. It's because we have to rent
hardware. It's because we have to rent
10 times more expensive chips to match
10 times more expensive chips to match
this
reward engineering and say 100 million
reward engineering and say 100 million
steps.
Uh sometimes you can get something
Uh sometimes you can get something
decent in like a 100 million frames or
decent in like a 100 million frames or
whatever. Uh there's like a lot of work
whatever. Uh there's like a lot of work
on I mean people do stuff like that on
on I mean people do stuff like that on
relatively slower environments. The
relatively slower environments. The
thing that's just tricky two weeks is um
thing that's just tricky two weeks is um
you still have to do like the
you still have to do like the
development work, right? Where like
development work, right? Where like
ideally to get the like the fastest
ideally to get the like the fastest
quickest training result, you want to
quickest training result, you want to
run a hyper pram sweep and then that's
run a hyper pram sweep and then that's
like like many times more expensive than
like like many times more expensive than
an actual single training run.
an actual single training run.
And then also there's the thing where
And then also there's the thing where
usually you get CPUbound, not GPU bound.
usually you get CPUbound, not GPU bound.
like even though you have your nice
like even though you have your nice
fancy GPU, you're actually spending like
fancy GPU, you're actually spending like
99% time on the CPU. So it's like it's
99% time on the CPU. So it's like it's
not that there are any fundamental
not that there are any fundamental
barriers, right? We are solving harder
barriers, right? We are solving harder
problems in many cases uh than real
problems in many cases uh than real
games. It's just that the games are
games. It's just that the games are
slow, but you can think of it as like a
slow, but you can think of it as like a
cost and an infrastructure premium more
cost and an infrastructure premium more
than anything.
than anything.
Only because you don't need the memory.
Only because you don't need the memory.
Yep. Don't need the memory.
Yep. Don't need the memory.
We literally keep our entire data back
We literally keep our entire data back
on the GPU and we're still not even
on the GPU and we're still not even
close to running out of memory.
close to running out of memory.
Even with the bigger models,
transformers are just like obnoxiously
transformers are just like obnoxiously
memory inefficient. Why everyone needs
memory inefficient. Why everyone needs
that much memory. Hyper we forgot. Yeah,
that much memory. Hyper we forgot. Yeah,
you can technically run default hypers,
you can technically run default hypers,
but the thing is like it's not great.
but the thing is like it's not great.
Like we're getting better at it. We're
Like we're getting better at it. We're
like now more of our hypers work like at
like now more of our hypers work like at
least kind of okay out of the box on
least kind of okay out of the box on
most problems. Um it's not ideal though,
most problems. Um it's not ideal though,
right? Which is why we do all this
right? Which is why we do all this
research on really fast environments,
research on really fast environments,
right? Like the more we can advance
right? Like the more we can advance
research while going really fast, right?
research while going really fast, right?
The fewer experiments and fiddly bits
The fewer experiments and fiddly bits
have to be involved when we actually
have to be involved when we actually
want to take it and throw it on any
want to take it and throw it on any
fixed problem where stuff is slower.
fixed problem where stuff is slower.
which is why all this research is really
which is why all this research is really
useful. And uh yeah, you want to
useful. And uh yeah, you want to
contribute to it, we're right here,
Yanick, do you zero out the observations
Yanick, do you zero out the observations
anywhere?
Wait, one hot and coated.
Wait, one hot and coated.
This is in China that can send you.
This is in China that can send you.
Yeah, that is a thing.
You probably burn your building down
You probably burn your building down
though,
though,
for all I know.
Okay, so here you are.
Okay, so here you are.
So this is not a one hot encoding.
But Yan, this is not a one hot encoding,
But Yan, this is not a one hot encoding,
right?
This is you're setting the observation
This is you're setting the observation
at a position.
Okay. But then let's see. Do you have a
Okay. But then let's see. Do you have a
custom policy?
custom policy?
You should if you're going to do it this
You should if you're going to do it this
way.
Not log two. Yeah. Okay.
Not log two. Yeah. Okay.
Oh, it's now log. Okay.
Yes. mem copy would be faster in many
Yes. mem copy would be faster in many
cases. Um, I'm I'm trying to make sure
cases. Um, I'm I'm trying to make sure
he has the ends right.
There's a few things I'm worried about
There's a few things I'm worried about
here.
Okay. So, you do have a custom No, you
Okay. So, you do have a custom No, you
don't have a Wait,
don't have a Wait,
that's just binding. Okay, you do not
that's just binding. Okay, you do not
have a custom policy.
Once upon a time I was a system engineer
Once upon a time I was a system engineer
for Microsoft Xbox 360. Systems people
for Microsoft Xbox 360. Systems people
will do very very well in uh
will do very very well in uh
reinforcement learning the way we do
reinforcement learning the way we do
stuff in pop for lip. So I definitely
stuff in pop for lip. So I definitely
encourage you to come try some stuff out
encourage you to come try some stuff out
with us here. Like we basically just
with us here. Like we basically just
write really really basic C in a way
write really really basic C in a way
that makes stuff good for uh
that makes stuff good for uh
reinforcement learning like everything's
reinforcement learning like everything's
written into shared memory like no
written into shared memory like no
dynamic allocations and stuff like that.
Let me think about the policy for you.
We probably need to make you a con
We probably need to make you a con
architecture.
Maybe not.
There's no vibe coding in Puffer.
Well, you definitely need a um a data
Well, you definitely need a um a data
norm
norm
at the very least.
at the very least.
What's the biggest number in
What's the biggest number in
observation?
Wait, required flattened is
this is not updated.
16 tiles.
Tape is 4x4
11 for 2048.
Okay.
Trying to think if it makes sense to one
Trying to think if it makes sense to one
hot or what.
2048 solidly.
2048 solidly.
Let me merge this and let me I'm going
Let me merge this and let me I'm going
to merge this and I'm going to start
to merge this and I'm going to start
doing some work on this live with you.
doing some work on this live with you.
All right.
Thinking how we want to do this.
Oh yeah. So this is what I saw before
Oh yeah. So this is what I saw before
where it like didn't
where it like didn't
very odd
learning rate of healing.
Let it chill.
Let it chill.
One hot game state sounds right. Kind
One hot game state sounds right. Kind
of. It's a little weird though cuz like
of. It's a little weird though cuz like
you kind of have to like relearn you
you kind of have to like relearn you
have to sort of like relearn
have to sort of like relearn
uh behaviors for every separate number.
uh behaviors for every separate number.
It's kind of like halfway between a one
It's kind of like halfway between a one
hot and not
hot and not
right.
Well, you don't want to do transformer.
Well, you don't want to do transformer.
They're just slow.
They're just slow.
They're kind of just slow.
They're kind of just slow.
I mean, I at least know that I want to
I mean, I at least know that I want to
normalize your observations, right?
Is there a reason that it's prefixed
Is there a reason that it's prefixed
with a G
MLP?
MLP?
It's not deep enough to need residuals
It's not deep enough to need residuals
like our nets and our RL networks are
like our nets and our RL networks are
usually very shallow.
I think your classes are not allowed to
I think your classes are not allowed to
be called 20 like the
be called 20 like the
that get
that get
exponentially harder.
exponentially harder.
I don't know if it gets exponentially
I don't know if it gets exponentially
harder. It gets exponentially longer,
harder. It gets exponentially longer,
right?
right?
Can't start the end of name with a
Can't start the end of name with a
number.
Could do puff 2048. I don't know.
Do I have to move this?
Okay. So, something's great here.
Upper puff. Yeah, that'd be weird.
We want it to be like puffer_2048,
We want it to be like puffer_2048,
however that has to happen. I think
however that has to happen. I think
there is a name mapping thing that we
there is a name mapping thing that we
can do.
So, yeah. What's up with this C file? Is
So, yeah. What's up with this C file? Is
this supposed to compile?
All right. See if you can get me a patch
All right. See if you can get me a patch
for that while I attempt to RL this.
I think you said you're at work, so no
I think you said you're at work, so no
big deal. Do it later.
over a weekend is
Something's weird with this thing,
Something's weird with this thing,
right?
right?
Buffer eval. Okay, I can do that.
Buffer eval. Okay, I can do that.
Wait, we have return and we have score.
Neither of these are good.
Interesting.
Recommend Neptune over W to be. Um,
Recommend Neptune over W to be. Um,
so at the moment, yes, Neptune is better
so at the moment, yes, Neptune is better
than Wbe. However, I've been told that
than Wbe. However, I've been told that
they're attempting to kamicazi their
they're attempting to kamicazi their
product later this year.
product later this year.
Um, I don't know what the hell they're
Um, I don't know what the hell they're
doing, but for now it's There.
Oh, wait. Now is score stable once
Oh, wait. Now is score stable once
again.
Yeah, it looks like it is
This is cheeky.
This is cheeky.
Eight minute experiment for two billion
Eight minute experiment for two billion
steps.
steps.
So ran into sometimes ran into nanss
So ran into sometimes ran into nanss
when training. I was stress probably and
when training. I was stress probably and
this can be a number of things. Nanss
this can be a number of things. Nanss
are numerical issues mostly.
are numerical issues mostly.
Do you know what score is solved?
Do you know what score is solved?
What problem you're trying to solve?
What problem you're trying to solve?
This is 2048.
So Yanick, we usually PF. We do Perf is
So Yanick, we usually PF. We do Perf is
like a zero to one normalized score.
like a zero to one normalized score.
You know what solved?
You know what solved?
Oh, 2048 is the best score.
Oh, 2048 is the best score.
Wait, should the score be 2048
to solve it?
Is that how it works?
Is that how it works?
Greater than or equal to 2048. Okay.
Greater than or equal to 2048. Okay.
So, yeah, it looks like you just have
So, yeah, it looks like you just have
some unstable
some unstable
learning dynamics.
Take a look.
So Perf in your case would just be score
So Perf in your case would just be score
divided by 2048 because one is solved.
divided by 2048 because one is solved.
Can you clip activations or loss? Is the
Can you clip activations or loss? Is the
reward normalized?
reward normalized?
Uh we already have several different
Uh we already have several different
forms of clipping in place. Yes. And the
forms of clipping in place. Yes. And the
reward is normalized, but his
reward is normalized, but his
observations are not yet normalized,
observations are not yet normalized,
which is probably an issue.
Okay. That's immediately better.
That's with the defaults.
I'm on the side on Discord here. I'm
I'm on the side on Discord here. I'm
driving Spencer crazy with uh
driving Spencer crazy with uh
edits on his article.
edits on his article.
Observations are in charge. Whoops.
Observations are in charge. Whoops.
Thank you.
Let's uh Yep. Wondering what I screwed
Let's uh Yep. Wondering what I screwed
up there.
All right, buckle up. We're going to
All right, buckle up. We're going to
show you how to write a custom policy.
Good.
Copy this thing. put this here G48
uh we'll start with
uh we'll start with
this this
encoder is going to be
encoder is going to be
11 times this
11 times this
we'll start with just like This
we'll start with just like This
This is a fairly vanilla network here.
Why do you prefer yellow?
Why do you prefer yellow?
Uh mild empirical benefit.
Uh mild empirical benefit.
I I don't have huge I don't have
I I don't have huge I don't have
tremendous confidence in it, but in like
tremendous confidence in it, but in like
I did some experiment and it seemed to
I did some experiment and it seemed to
be a little bit better a while ago.
be a little bit better a while ago.
It's been fine.
All right, everybody go uh repost. I'm
All right, everybody go uh repost. I'm
gonna put this in the chat.
gonna put this in the chat.
Go help out Spencer if you're on X cuz
Go help out Spencer if you're on X cuz
he just posted the uh Terraform article.
he just posted the uh Terraform article.
I worked on this with uh this end with
I worked on this with uh this end with
him for a bit.
It was re before. Yes.
Is there curriculum learning in Pupper?
Is there curriculum learning in Pupper?
Uh yes, technically we do have some we
Uh yes, technically we do have some we
have some research we've done uh Aaron
have some research we've done uh Aaron
specifically and we have a version. It's
specifically and we have a version. It's
not like very well integrated but it is
not like very well integrated but it is
there and it's quite simple.
there and it's quite simple.
And then we also have several
And then we also have several
environments where we've like manually
environments where we've like manually
done some form specific to to that one
done some form specific to to that one
problem.
Generally we have a fair few curriculum
Generally we have a fair few curriculum
learning people around here though.
Myself, Ryan, and Aaron have all done uh
Myself, Ryan, and Aaron have all done uh
curriculum learning research at some
curriculum learning research at some
point.
the optimal hypers drift over time.
the optimal hypers drift over time.
Uh
Uh
you can't re sweep every hundred billion
you can't re sweep every hundred billion
like that with how expensive stuff is.
like that with how expensive stuff is.
Uh there are definitely some components
Uh there are definitely some components
that need to be tweaked though. Let's
that need to be tweaked though. Let's
say so Finn in particular gamma I think
say so Finn in particular gamma I think
that the biggest culprit is um gamma and
that the biggest culprit is um gamma and
lambda mostly gamma really not just
lambda mostly gamma really not just
lambda
lambda
cuz gamma here hang on let me finish
cuz gamma here hang on let me finish
this
Nope.
Good articles.
Good articles.
So yeah, Finn. Um I
The thing is that gamma tells you this
The thing is that gamma tells you this
is how much you care about a dollar
is how much you care about a dollar
today versus a dollar tomorrow. And like
today versus a dollar tomorrow. And like
credit assignment doesn't work on a
credit assignment doesn't work on a
fixed temporal hierarchy like that. And
fixed temporal hierarchy like that. And
gamma tends to be like the really weird
gamma tends to be like the really weird
and sensitive and janky hyperparameter
and sensitive and janky hyperparameter
that tends to vary a lot and sometimes
that tends to vary a lot and sometimes
needs to be annealed in other things.
needs to be annealed in other things.
Uh, so yes, like that specific PR. But
Uh, so yes, like that specific PR. But
the thing is like that's kind of a
the thing is like that's kind of a
band-aid patch is just like what if you
band-aid patch is just like what if you
reweep it because you have to like do so
reweep it because you have to like do so
many more experiments. Ideally, we
many more experiments. Ideally, we
should just come up with something that
should just come up with something that
doesn't have that problem at an
doesn't have that problem at an
algorithmic level.
That's hard. Like that's hard
That's hard. Like that's hard
algorithmic work. I tried uh the one
algorithmic work. I tried uh the one
thing that I tried didn't work super
thing that I tried didn't work super
well. This was my work on P3O. I don't
well. This was my work on P3O. I don't
think you guys were watching the stream
think you guys were watching the stream
then, but I spent a couple weeks on like
then, but I spent a couple weeks on like
some pretty hardcore algorithm dev
some pretty hardcore algorithm dev
around that. And basically like there's
around that. And basically like there's
definitely stuff we can do, but it's
definitely stuff we can do, but it's
going to it's going to require like at
going to it's going to require like at
least my full attention and probably
least my full attention and probably
help from a couple other people for like
help from a couple other people for like
at least several weeks to even have a
at least several weeks to even have a
good chance of figuring that type of
good chance of figuring that type of
stuff out.
So tricky.
I was going to do
All right, my bad, Yanick. I keep
All right, my bad, Yanick. I keep
getting uh I keep getting distracted
getting uh I keep getting distracted
here.
here.
I know. I've been trying to get this
I know. I've been trying to get this
thing for you. My brain is also all
thing for you. My brain is also all
super fuzzy. Like always happens like in
super fuzzy. Like always happens like in
between chunks of work. Takes me a while
between chunks of work. Takes me a while
to like get back to 100% after doing a
to like get back to 100% after doing a
whole bunch of work on like release for
whole bunch of work on like release for
several months. Let me get rid of this
several months. Let me get rid of this
bot. Get out of here, bot.
bot. Get out of here, bot.
We trained bot. We train bots. You do
We trained bot. We train bots. You do
not come here. We train you
to observe.
to observe.
Probably something like this.
There we go.
Okay, so this is a custom policy that
Okay, so this is a custom policy that
literally just does the exact same thing
literally just does the exact same thing
uh but with normalized observations. I
uh but with normalized observations. I
don't have any great confidence that
don't have any great confidence that
this will do anything, but uh it's at
this will do anything, but uh it's at
least going to be less prone to
least going to be less prone to
horrendous failure that makes no sense.
horrendous failure that makes no sense.
And it is actually a little tiny bit
And it is actually a little tiny bit
better uh early on already. Normalize
better uh early on already. Normalize
your observations, folks.
your observations, folks.
between roughly negative one to one. It
between roughly negative one to one. It
doesn't have to be precise, but don't
doesn't have to be precise, but don't
have values that are like five.
have values that are like five.
We have 10 people on YouTube for once.
We have 10 people on YouTube for once.
Hi, it's all open source research.
Hi, it's all open source research.
Come contribute to this and uh get
Come contribute to this and uh get
involved with pushing the cutting edge
involved with pushing the cutting edge
of reinforcement learning right now at
of reinforcement learning right now at
puffer.ai. Start the repo to help me out
puffer.ai. Start the repo to help me out
for free as well. Rest of it's all on
for free as well. Rest of it's all on
Discord.
Discord.
Back to work here.
Back to work here.
Okay. So, marginal benefit from uh this.
Okay. So, marginal benefit from uh this.
Now, let me show you how to one hot a
Now, let me show you how to one hot a
policy.
policy.
Now, it's all positive. It was always
Now, it's all positive. It was always
all positive. You had a char. You had an
all positive. You had a char. You had an
unsigned char.
unsigned char.
I just divided by 11.
Okay. Let me be clear. It doesn't have
Okay. Let me be clear. It doesn't have
to be mean zero, standard deviation one
to be mean zero, standard deviation one
or anything like that. Okay. You don't
or anything like that. Okay. You don't
have to have it be between positive and
have to have it be between positive and
negative. That's not a thing. You just
negative. That's not a thing. You just
need to have it be roughly on the scale
need to have it be roughly on the scale
of -1 to one. Does that make sense?
of -1 to one. Does that make sense?
That doesn't mean that it has to
That doesn't mean that it has to
begative -1 to one. It means that it
begative -1 to one. It means that it
could be it could be -1 to 0 or it could
could be it could be -1 to 0 or it could
be 0 to one, right? You want roughly
be 0 to one, right? You want roughly
that scale. It can't be like 0 to
that scale. It can't be like 0 to
0.00001.
0.00001.
That's too small.
That's too small.
It can't be negative a million to a
It can't be negative a million to a
million. That's too big, right? You want
million. That's too big, right? You want
it roughly on the order of negative one
it roughly on the order of negative one
to one.
Let me show you how to one hot a policy
Let me show you how to one hot a policy
because this is actually going to be
because this is actually going to be
useful for people. So the idea here uh
useful for people. So the idea here uh
is that these are not exactly continuous
is that these are not exactly continuous
values, right? You've encoded like one
values, right? You've encoded like one
is the tile that like is one, two is the
is the tile that like is one, two is the
tile that corresponds to two, three is
tile that corresponds to two, three is
the tile that corresponds to four,
the tile that corresponds to four,
right? And these are kind of discreet in
right? And these are kind of discreet in
a way because like you have to combine
a way because like you have to combine
tiles that are of the same value and the
tiles that are of the same value and the
policy can't see that because you've
policy can't see that because you've
just given it a bunch of floatingoint
just given it a bunch of floatingoint
numbers. So we're going to see and I
numbers. So we're going to see and I
don't know if this environment
don't know if this environment
specifically it's going to make a big
specifically it's going to make a big
difference or not. But we're going to
difference or not. But we're going to
see what happens if instead we tell it
see what happens if instead we tell it
that hey the tile that is one is a
that hey the tile that is one is a
different object than the tile that is a
different object than the tile that is a
two or the tile is a four. And the way
two or the tile is a four. And the way
that we do that is through one hot
that we do that is through one hot
encoding. And what one hot encoding
encoding. And what one hot encoding
does, let me draw this up.
Let me do this properly because this is
Let me do this properly because this is
an important topic for people.
Okay. So,
there's your board.
like
like
like one
goes to
this is one for instance. And this one
this is one for instance. And this one
is like
is like
three,
five, right?
But one hot coating here.
All
right.
So this is an important topic.
So this is an important topic.
One hot encoding
One hot encoding
allows your agent to distinguish between
allows your agent to distinguish between
different values as though they are
different values as though they are
discrete objects and they are not
discrete objects and they are not
continuous values on a number line. So
continuous values on a number line. So
for 2048 here, it allows it to say that
for 2048 here, it allows it to say that
this tile one is not any more similar to
this tile one is not any more similar to
the tile four than the tile four is to
the tile four than the tile four is to
16, which is a reasonable thing to say
16, which is a reasonable thing to say
because you can only combine two tiles
because you can only combine two tiles
that are one, two tiles that are four,
that are one, two tiles that are four,
or two tiles that are 16. You cannot
or two tiles that are 16. You cannot
combine a one with a four any better
combine a one with a four any better
than you can combine a one with a 16. So
than you can combine a one with a 16. So
the way that you do this with a one hot
the way that you do this with a one hot
encoding, let's say that these are
encoding, let's say that these are
originally mapped to their log values
originally mapped to their log values
plus one. So 1 3 and five. What you do
plus one. So 1 3 and five. What you do
is you make an array of zeros of at
is you make an array of zeros of at
least 5 + 1 is six elements each and
least 5 + 1 is six elements each and
then you set that index to one for each
then you set that index to one for each
of these elements and then the rest of
of these elements and then the rest of
them remain zero. So one goes to 0 1 0 0
them remain zero. So one goes to 0 1 0 0
0 three goes to in the third index 0
0 three goes to in the third index 0
index one so on and so forth and then
index one so on and so forth and then
you can flatten that and there's your
you can flatten that and there's your
one hot observation and now your agent
one hot observation and now your agent
can actually see the game the way that
can actually see the game the way that
the game is.
There you go.
I should clip that and put it on
I should clip that and put it on
YouTube.
YouTube.
I've been experimenting with that.
So the way that we do this in practice,
So the way that we do this in practice,
right, there are 11 different values.
right, there are 11 different values.
What did we say?
long and float. Hang on, let me make
long and float. Hang on, let me make
sure.
Okay. I don't think that there are 11
Okay. I don't think that there are 11
things. Are there? Is the maximum value
things. Are there? Is the maximum value
11 or is the maximum value 10?
Yeah, the maximum value is 11. That
Yeah, the maximum value is 11. That
means that they're actually 12 elements.
That's that CUDA error is very common by
That's that CUDA error is very common by
the way. like that will you'll always
the way. like that will you'll always
get hit by that if you mess up a one hot
love your content I'm doing research on
love your content I'm doing research on
lambda policy iteration
lambda policy iteration
randomization the goal is to make a
randomization the goal is to make a
hybrid of two policies
hybrid of two policies
a policy iteration I'm not actually
a policy iteration I'm not actually
familiar with that hybrid of two
familiar with that hybrid of two
policies
huh yeah I'd need a link or something to
huh yeah I'd need a link or something to
see what that is but yeah thanks for
see what that is but yeah thanks for
stopping by. We do all sorts of RL
stopping by. We do all sorts of RL
around here.
around here.
Uh
Uh
oh. Okay. So, this was doing way better
oh. Okay. So, this was doing way better
and then somehow there's
and then somehow there's
there's some instability associated with
there's some instability associated with
this thing.
Let's see if it's consistent.
value loss is blowing up.
Yeah, I could make it. I doubt that's
Yeah, I could make it. I doubt that's
what's happening to be fair.
what's happening to be fair.
You'd get a CUDA assert
You'd get a CUDA assert
source of paper of mine here. It's
source of paper of mine here. It's
related with RF. Yeah. Uh, if YouTube
related with RF. Yeah. Uh, if YouTube
doesn't let you post the link, stick it
doesn't let you post the link, stick it
in Discord. I don't have any settings
in Discord. I don't have any settings
on, but sometimes YouTube's obnoxious
on, but sometimes YouTube's obnoxious
about that.
about that.
Discord.gg/puffer
Discord.gg/puffer
if you're not already there.
if you're not already there.
If you insist on not doing one hot, I am
If you insist on not doing one hot, I am
doing one hot.
doing one hot.
Two weeks. I literally just added that
Two weeks. I literally just added that
right here. See?
We're just trying to figure out what's
We're just trying to figure out what's
up with this unstable
up with this unstable
stable mess.
I missed that. Yeah. So, we normalized
I missed that. Yeah. So, we normalized
first from 0 to one. Doesn't matter.
first from 0 to one. Doesn't matter.
Negative 1 versus negative one to versus
Negative 1 versus negative one to versus
uh 0 to one doesn't matter. Um,
we have one hot and we actually, you see
we have one hot and we actually, you see
that like before this thing crashes, we
that like before this thing crashes, we
actually were getting a better result
actually were getting a better result
here like a much better result.
here like a much better result.
So why is this thing not stable?
Usually a data bug.
Usually a data bug.
Usually our stuff is pretty stable.
Usually our stuff is pretty stable.
learning rate. We use that learning rate
learning rate. We use that learning rate
with everything though. Like you're
with everything though. Like you're
using now our default settings that
using now our default settings that
should work for this and pretty much
should work for this and pretty much
everything,
especially for a network of this size.
Let me think about what uh what other
Let me think about what uh what other
weird things could be happening here.
We have not looked at your reward
We have not looked at your reward
structure at all.
structure at all.
So that shouldn't really nan stuff. I
So that shouldn't really nan stuff. I
guess it technically can.
Okay, so we have
Okay, so we have
your actions unsigned car chart
your actions unsigned car chart
observation.
observation.
Make sure that this is done correctly.
You went to eight. Good.
You went to eight. Good.
So this is specified reasonably
passing it to a view of 4x4 and torch
passing it to a view of 4x4 and torch
but continuous vector and see
but continuous vector and see
that shouldn't cause an issue because it
that shouldn't cause an issue because it
should just be laid out contiguous in
should just be laid out contiguous in
memory.
Is your output for actions across all
Is your output for actions across all
possible actions?
It is across all possible actions
It is across all possible actions
and uh the actions uh is very small for
and uh the actions uh is very small for
2048. It's only there are that four
2048. It's only there are that four
possible actions.
possible actions.
Wait up.
Wait up.
Wait, wait, wait. Did I find something?
Wait, wait, wait. Did I find something?
Move.
Okay. No, you do add plus one. to take
Okay. No, you do add plus one. to take
game actions of zero plus one.
game actions of zero plus one.
You don't need to mask actions. That's
You don't need to mask actions. That's
not a thing.
Hang on. So, you just have right here.
Hang on. So, you just have right here.
It just gets Does it get negative.1 for
It just gets Does it get negative.1 for
losing and that's it? That doesn't make
losing and that's it? That doesn't make
sense. There's got to be more rewards
sense. There's got to be more rewards
than that, right? Okay. No, there's
enums are proper. What do you mean?
enums are proper. What do you mean?
They don't need to be made an enum. A
They don't need to be made an enum. A
pound define like this is totally fine.
Oh, do you mean make sure that they're
Oh, do you mean make sure that they're
ordered correctly?
As long as he's checking them by name
As long as he's checking them by name
here instead of by by value, that should
here instead of by by value, that should
be fine. Cuz it doesn't matter what
be fine. Cuz it doesn't matter what
order you list the actions in for the
order you list the actions in for the
agent.
But it does get a reward here, right? It
But it does get a reward here, right? It
gets a reward for merging stuff.
Row of I
Row of I
/ 11F
Okay, I see.
You probably do need that um
that reward.
that reward.
I'm going to try it without this for a
I'm going to try it without this for a
second though.
second though.
Wait, let me open this in the other tab.
This way.
If it can't do an action,
If it can't do an action,
don't you lose the game?
You can't do anything.
You can't do anything.
Is there no lose condition? There should
Is there no lose condition? There should
be a lose condition, right?
Small negative reward should be fine.
No, sometimes you can move left but up.
It should reset when it can't move in
It should reset when it can't move in
any direction. Right? That's the lose
any direction. Right? That's the lose
condition, isn't it?
Yeah.
Yeah.
Small negative reward for invalid is
Small negative reward for invalid is
fine.
Oh.
You said you got it to solve the game
You said you got it to solve the game
though with and with the thing that you
though with and with the thing that you
committed and it definitely wasn't
committed and it definitely wasn't
solving the game, right?
I definitely got it to just do way
I definitely got it to just do way
better for a bit. But the fact that this
better for a bit. But the fact that this
is so unstable makes me think that there
is so unstable makes me think that there
is something wrong with the game data.
is something wrong with the game data.
Like an environment like this should not
Like an environment like this should not
be so unstable.
Like I would suspect some sort of bug
lot of runs.
lot of runs.
Can you play the game yourself and
Can you play the game yourself and
verify no bugs? No, I can't because it
verify no bugs? No, I can't because it
doesn't have the uh the playable version
doesn't have the uh the playable version
of it working at the moment. So, I think
of it working at the moment. So, I think
that's probably
that's probably
I think I'm going to wait for you to add
I think I'm going to wait for you to add
some changes to this. Um, if you can PR
some changes to this. Um, if you can PR
the C file, that would be nice. I think
the C file, that would be nice. I think
that there's likely some sort of data
that there's likely some sort of data
corruption, though.
I don't vibe code anything two weeks.
I don't vibe code anything two weeks.
Maybe more than in your average M.
Maybe more than in your average M.
That really shouldn't matter.
For something like with a discrete
For something like with a discrete
action space, it really shouldn't blow
action space, it really shouldn't blow
up like Yes.
Me try something real quick. I'm going
Me try something real quick. I'm going
to try one more thing for you here. Then
to try one more thing for you here. Then
I got to do some robotic stuff, I
I got to do some robotic stuff, I
believe.
I just want to see if this works.
Be a fair bit slower probably.
What's the compile error?
What's the compile error?
Uh, some missing packages. Hang on.
Uh, some missing packages. Hang on.
You know, this is actually this is like
You know, this is actually this is like
a slow crash, not a fast crash. This
a slow crash, not a fast crash. This
could just be hypers.
Yeah, this could just be hypers.
Yeah, this could just be hypers.
I'm not confident it's a data bug.
Oh, this was also wrong. End of action.
Where's the minus one
fine in
Is this like a weird double include? But
I don't know why your memes in yourh
I don't know why your memes in yourh
doesn't work.
Oh, there should be a minus one.
Okay.
Okay.
But then there's something wrong with
But then there's something wrong with
these includes. Yeah.
Double included.
I yeah I don't something is screw with
I yeah I don't something is screw with
like the includes in here. I don't know
like the includes in here. I don't know
what.
what.
Uh
I guess this just
I guess something else includes string.h
I guess something else includes string.h
page for you.
Yes.
Where is this supposed to be from?
This function doesn't exist.
And it's not in here. So, I think
And it's not in here. So, I think
there's some stuff missing, man.
there's some stuff missing, man.
And you shouldn't even need that
And you shouldn't even need that
anyways. Like, it should be
You should just do that with Rayb.
I think you can get rid of this
I think you can get rid of this
entirely.
All right, there you go.
Yeah, it looks like it works.
Okay, there's 256. Cool. So, I mean, the
Okay, there's 256. Cool. So, I mean, the
game seems to work the way I would
game seems to work the way I would
expect it to, right?
It will learn to spam keys.
Yeah, it it's a little tricky with this
Yeah, it it's a little tricky with this
because the the signal is pretty delayed
because the the signal is pretty delayed
if you rewarded every merge, right?
if you rewarded every merge, right?
which is why I was trying to not do
which is why I was trying to not do
that.
I was basically just telling it to not
I was basically just telling it to not
lose. Our RL should be good enough that
lose. Our RL should be good enough that
it should be able to learn from just
it should be able to learn from just
like don't lose.
Like just don't like there's no don't
Like just don't like there's no don't
lose and not win in this game, right?
lose and not win in this game, right?
Like if you don't lose for long enough,
Like if you don't lose for long enough,
you win because you have to merge stuff
you win because you have to merge stuff
to get it out of the way.
to get it out of the way.
So,
So,
I think that that's probably going to be
I think that that's probably going to be
like just not even shaping the reward at
like just not even shaping the reward at
all will probably be better for this
all will probably be better for this
specific environment.
How many moves does it take to merge?
How many moves does it take to merge?
You get like um
You get like um
you get like a few additional units
you get like a few additional units
every single step. So, it's probably
every single step. So, it's probably
it's probably like somewhere between 512
it's probably like somewhere between 512
and 1024ish or whatever.
I would imagine.
Wait,
Wait,
if you can't make invalid moves, right,
if you can't make invalid moves, right,
in this like it just doesn't let you.
in this like it just doesn't let you.
Like,
Like,
is that how the original works?
you get a random two or four.
you get a random two or four.
You could just make it do that then if
You could just make it do that then if
uh instead of having to do invalid
uh instead of having to do invalid
moves, it just gives you a thing. So
moves, it just gives you a thing. So
random two or four would be between 512
random two or four would be between 512
and uh 1024 steps, I believe, to solve
and uh 1024 steps, I believe, to solve
the game.
the game.
That's like a useful thing to know.
That's like a useful thing to know.
It's probably like 384 or something on
It's probably like 384 or something on
average.
average.
Right.
Okay, that's enough of that. Cool. So,
let me commit you this policy.
I think once we have this thing working,
I think once we have this thing working,
it'll be a very cool thing to watch.
Like it'll be pretty similar to Tetris.
Nothing happens. Okay. So that's then
Nothing happens. Okay. So that's then
you've done it correctly.
Okay. So, there you go. I will leave it
Okay. So, there you go. I will leave it
up to you to like get something a bit
up to you to like get something a bit
more stable. Um, if you have a sweep set
more stable. Um, if you have a sweep set
up and you want me to just like run
up and you want me to just like run
something a little longer, you let me
something a little longer, you let me
know and I can make that happen for you.
know and I can make that happen for you.
Thank you for the M. Let's get it uh
Thank you for the M. Let's get it uh
finished and I think this will look
finished and I think this will look
pretty cool.
pretty cool.
Okay, next thing. Let me make sure I
Okay, next thing. Let me make sure I
have not missed anything.
go-to benchmark for RL.
go-to benchmark for RL.
Surprisingly hard to solve. I think
Surprisingly hard to solve. I think
you'll be surprised. I think once you
you'll be surprised. I think once you
actually have this thing set up
actually have this thing set up
correctly, you'll solve it very very
correctly, you'll solve it very very
quickly. Just purely based on like the
quickly. Just purely based on like the
action space and the episode length and
action space and the episode length and
the verifiability. Like I think he'll
the verifiability. Like I think he'll
solve it quite quickly.
All right, this has gotten a little bit
All right, this has gotten a little bit
more attention. 12K views on this. And
more attention. 12K views on this. And
then
then
Spencer is hopefully getting some
Spencer is hopefully getting some
follows. Go follow Spencer, guys. He's
follows. Go follow Spencer, guys. He's
doing real cool work.
Okay, so next we're going to do some
Okay, so next we're going to do some
robotics.
robotics.
Um,
Um,
that might seem like a hard pivot, but
that might seem like a hard pivot, but
basically all of these ends look the
basically all of these ends look the
same. uh in from the perspective of
same. uh in from the perspective of
puffer liib like a robotics M doesn't
puffer liib like a robotics M doesn't
look all that different from 2048
look all that different from 2048
like kind of all the problems just look
like kind of all the problems just look
the same.
Ah and perfect. So this thing this
Ah and perfect. So this thing this
experiment that I was running this is
experiment that I was running this is
what I was waiting for. So there's this
what I was waiting for. So there's this
weird thing that happens here probably
weird thing that happens here probably
because of the learning rate but it is
because of the learning rate but it is
doing uh better. So hopefully this
doing uh better. So hopefully this
continues to learn stuff. We'll see.
continues to learn stuff. We'll see.
Yeah, my guy does robotics now. Uh I
Yeah, my guy does robotics now. Uh I
just I told Stone I would integrate some
just I told Stone I would integrate some
robotic stuff for him cuz he develops
robotic stuff for him cuz he develops
Manny skill. Let me show off Manny skill
Manny skill. Let me show off Manny skill
so folks can see what this is.
so folks can see what this is.
So this is one of like the better
So this is one of like the better
robotics uh sim libraries out there. It
robotics uh sim libraries out there. It
doesn't have a giant mess of horrible
doesn't have a giant mess of horrible
code as best as I can tell.
And let's see. Get started.
And let's see. Get started.
You do need to have better docks.
Okay. So, here are docs. You can see all
Okay. So, here are docs. You can see all
these different tasks. They have state
these different tasks. They have state
based tasks. They have RGB like rendered
based tasks. They have RGB like rendered
tasks. Got all sorts of stuff. And uh we
tasks. Got all sorts of stuff. And uh we
did get it running with Puffer in not
did get it running with Puffer in not
very long at all,
very long at all,
but uh it doesn't train very well. So I
but uh it doesn't train very well. So I
got to figure out why that is. I'm going
got to figure out why that is. I'm going
to try their policy. I'm going to try
to try their policy. I'm going to try
some other stuff. And yeah, we're going
some other stuff. And yeah, we're going
to see if we can figure out what is
to see if we can figure out what is
going on with uh this.
So, given that I ran a sweep and it
So, given that I ran a sweep and it
didn't do anything,
didn't do anything,
I think we're going to start by sticking
I think we're going to start by sticking
a little bit more close to their script
a little bit more close to their script
to start with
to start with
and uh we'll go from there.
So, we'll take PTO fast.
Put this
Put this
however high I can get away with without
however high I can get away with without
covering too much.
So, this is our binding for Manny.
I don't know. So many people have asked
I don't know. So many people have asked
me to do robotics, and I kind of just
me to do robotics, and I kind of just
haven't until now. I guess it was mostly
haven't until now. I guess it was mostly
I didn't want to deal with Isaac Jim and
I didn't want to deal with Isaac Jim and
Isaac Sim because we had to do Isaac Jim
Isaac Sim because we had to do Isaac Jim
for one contract and it just was not
for one contract and it just was not
fun.
fun.
Like it frankly it was way more work
Like it frankly it was way more work
than it was worth dealing with that
than it was worth dealing with that
horrendous code base.
Stone writes uh Stone writes pretty nice
Stone writes uh Stone writes pretty nice
code though. Of course we all owe to
code though. Of course we all owe to
Costa for clean RL.
Yeah, Isaac Jim is pretty rough.
Yeah, Isaac Jim is pretty rough.
And then the thing is all the people
And then the thing is all the people
using Isaac Jimmer are also like their
using Isaac Jimmer are also like their
code also sucks. So it's just not a fun
code also sucks. So it's just not a fun
time. Like there were literally 10,000
time. Like there were literally 10,000
lines of code when there should have
lines of code when there should have
been maybe a thousand.
Where's the end of initialization?
Yeah, Isaac's underdev.
at all. Manny looks nice.
I would have to talk to more robotics
I would have to talk to more robotics
people to see like what they think of
people to see like what they think of
Manny,
but it looks nice from an outside
but it looks nice from an outside
perspective here.
perspective here.
Of
Of
course, everything in robotics needs to
course, everything in robotics needs to
just be 10 times faster,
just be 10 times faster,
but uh yeah, I'm not getting into
but uh yeah, I'm not getting into
hardcore sim until uh you have something
hardcore sim until uh you have something
major for that.
Reconfiguration frequency.
What is this reconfiguration frequency?
This seems weird to me. Like there's
This seems weird to me. Like there's
some parameter.
None.
None.
Okay. So, it's an optional thing. So,
Okay. So, it's an optional thing. So,
that's not it.
that's not it.
You have gym.make here.
They flatten some spaces
and then their M's are
args num ms
args num ms
ignore terminations equals not args.p
ignore terminations equals not args.p
Partial reset.
So this should be false because we want
So this should be false because we want
to do partial resets, right?
to do partial resets, right?
Partial reset is true. Nor terms is
Partial reset is true. Nor terms is
false.
What we have
What we have
and do they have auto reset?
and do they have auto reset?
We see auto reset. I I forgot.
It does not look like they do auto
It does not look like they do auto
resets,
resets,
but that's fine.
Okay. So then they have this policy.
Let's try their policy.
We'll just graft this onto our default.
We'll just graft this onto our default.
So, we have a default policy that can
So, we have a default policy that can
handle
handle
discrete spaces, continuous spaces,
discrete spaces, continuous spaces,
whatever. And it's really easy to use
whatever. And it's really easy to use
with an LSTM.
with an LSTM.
Uh so what we're going to do here
Uh so what we're going to do here
we're just going to make their policy
we're just going to make their policy
on our architecture.
See how we do this?
This is the actor and then we have the
This is the actor and then we have the
critic.
We'll just have encoder do nothing, I
We'll just have encoder do nothing, I
guess.
Yeah. So now we have our decoder mean
value will be similar.
Okay,
Okay,
so this is a little bit janky here.
so this is a little bit janky here.
So, uh bear with me,
So, uh bear with me,
but we will get this to something that
but we will get this to something that
is relatively clean and uh hopefully
is relatively clean and uh hopefully
useful quite quickly.
Yes.
Yes.
decode action.
Let me answer this DM.
Okay. So, let me make sure this is the
Okay. So, let me make sure this is the
same.
same.
They have the critic layer knitted.
I believe I messed up one of these. Hang
I believe I messed up one of these. Hang
on. You have to be very careful even
on. You have to be very careful even
with stuff like init scales.
with stuff like init scales.
decoder
decoder
standard deviation 01 and value get
standard deviation 01 and value get
standard deviation of one the last
standard deviation of one the last
layer. So I could have messed that up.
layer. So I could have messed that up.
Yep.
All right. So let's see if this runs.
My attribute is
some sort of Matt mall
some sort of Matt mall
issue.
96 by 42.
This is
okay. So this fails and
okay. So this fails and
decoder mean
Weird
hidden shape
decoder. me in
decoder. me in
eight in feature. That's wrong
eight in feature. That's wrong
the N obs.
And then this is going to be um
And then this is going to be um
I don't know how many this is.
I don't know how many this is.
Not huge.
Thought
I had fixed that.
Why is this still eight?
What's What's wrong with this thing?
Oh, observation space. Silly.
All right.
Clearly, I need more coffee.
Clearly, I need more coffee.
Get this training first.
Ideally on to Neptune even.
Well, this looks fine.
The plan is to uh match their
The plan is to uh match their
architecture and roughly match their
architecture and roughly match their
hypers. See if that magically fixes it.
hypers. See if that magically fixes it.
And then if not, we assume it's a data
And then if not, we assume it's a data
issue. We figure out what's different
issue. We figure out what's different
with our config.
the issue here.
Bandage size.
Bandage size.
That looks like a batching issue.
Uh, why would that be a batching issue?
That's weird.
That's weird.
They do a bad reshape.
Wait.
Wait.
Oh, I just messed up.
Yeah, this is one.
All right. Amateur hour here.
Abracadabra.
Abracadabra.
did some work on asteroids and now the
did some work on asteroids and now the
agent learns. Take a look at the PR.
agent learns. Take a look at the PR.
Yep.
Yep.
One second here. I want to uh at least
One second here. I want to uh at least
get something running on this and then I
get something running on this and then I
will look at that.
Let me put this Neptune
Let me put this Neptune
magnet
magnet
attached a video of the agent playing.
attached a video of the agent playing.
Let's see.
General
Where'd you put it? Oh, in the PR. The
folks do star the puffer.
folks do star the puffer.
Little tiny uptick here. See this little
Little tiny uptick here. See this little
tiny uptick in stars? This represents a
tiny uptick in stars? This represents a
generational leap in uh reinforcement
generational leap in uh reinforcement
learning capabilities from 2.0 to 3.0.
learning capabilities from 2.0 to 3.0.
So, this should be like 10x
So, this should be like 10x
based on the uh the quality of the RL in
based on the uh the quality of the RL in
there.
Asteroids.
Oh, that dodges.
Kind of dodges.
Kind of dodges.
That's you playing. Okay.
That's you playing. Okay.
I was gonna say that agent needs to be a
I was gonna say that agent needs to be a
little smarter, but yeah. No, it's just
little smarter, but yeah. No, it's just
the game is hard.
Wait, this thing gets merked instantly,
Wait, this thing gets merked instantly,
doesn't it?
Uh, kind of plays.
Uh, kind of plays.
All right, let's see what you've got.
All right, let's see what you've got.
So,
well, you have some crazy parameters in
well, you have some crazy parameters in
here. Explain.
here. Explain.
Are you doing four agents or like four
Are you doing four agents or like four
environments?
environments?
What's this mini batch? You shouldn't be
What's this mini batch? You shouldn't be
able to train anything on this
local testing.
local testing.
Do you have the hypers for uh training?
Do you have the hypers for uh training?
Like do you have like hypers that give
Like do you have like hypers that give
you a stable curve
eight? Okay.
eight? Okay.
Still 20 million steps
your code
ob shape and I I'd already looked
ob shape and I I'd already looked
through the code of this so I assume
through the code of this so I assume
that you uh you sorted the obs like I
that you uh you sorted the obs like I
asked or did you not do that yet?
recorded obs good added velocities
recorded obs good added velocities
and those are all normalized right
and those are all normalized right
positions
positions
like it's all normalized yeah
Good.
Okay. So, I had previously reviewed
Okay. So, I had previously reviewed
this. So, I think the most useful thing
this. So, I think the most useful thing
for me to do now to actually play with
for me to do now to actually play with
it,
it,
see if I can get you uh anything out of
see if I can get you uh anything out of
that.
that.
Pull this
I mean, we're getting all these cool
I mean, we're getting all these cool
game PRs now, which is cool.
game PRs now, which is cool.
But you'll be able to solve asteroids
But you'll be able to solve asteroids
within a few minutes once you have it
within a few minutes once you have it
correct. Guaranteed.
Okay, let me try one more time.
Okay, let me try one more time.
See what the uh
See what the uh
I have to like actually do something
I have to like actually do something
that's like not trivial, but like
game's freaking hard.
game's freaking hard.
Tank controls are always difficult.
I don't know why it's like it shouldn't
I don't know why it's like it shouldn't
be that hard.
be that hard.
I don't know. Probably just takes like a
I don't know. Probably just takes like a
fair bit of practice.
Oh yeah, cuz there's no back button is
Oh yeah, cuz there's no back button is
the thing. You're not allowed to back
the thing. You're not allowed to back
up. You have to like tank control
up. You have to like tank control
everything.
Okay.
Yeah.
Env is kind of Hello.
uh vecum m
that case then you do like this.
Okay, it's
Okay, it's
Still a little slow, but it's all right.
did not get penalized for dying.
Hey, uh where's the uh the death penalty
Hey, uh where's the uh the death penalty
here?
Okay. So, where's it die? So, I can add
Okay. So, where's it die? So, I can add
a penalty there.
four or
the score is not correct.
Is there a max score in this?
A max score
A max score
type.
What's your SPS training?
Probably because my GPU is faster than
Probably because my GPU is faster than
yours.
yours.
I can crank more data than you.
Weird.
4090 is a bit faster than an 800.
Up to 2x, I think.
Try this.
Takes too long to train. So, we're going
Takes too long to train. So, we're going
to do something slightly shorter.
Max ticks 60
Max ticks 60
seconds.
seconds.
So it's 3600 ticks as max
fine.
H.
H.
There you go.
There you go.
Unless I forgot to reset this.
It's going to be pretty funny if I just
It's going to be pretty funny if I just
did that and now it works.
There were two errors. Your logging was
There were two errors. Your logging was
wrong. You weren't logging what you
wrong. You weren't logging what you
thought you were. And uh there was not a
thought you were. And uh there was not a
death penalty.
Hello, Bet. We're getting asteroids
Hello, Bet. We're getting asteroids
added today
added today
and doing robotics and other things.
And Beta is currently suffering because
And Beta is currently suffering because
I gave him a project that was too hard.
I gave him a project that was too hard.
Once again,
What's this thing doing?
What's this thing doing?
Oh, that's so cheeky.
Oh, that's so cheeky.
It's literally doing I The score is
It's literally doing I The score is
correct. This thing is doing way better
correct. This thing is doing way better
than I have just by doing this. You see
than I have just by doing this. You see
it?
What the heck?
[Laughter]
That's crazy.
Yeah, it's just decided it doesn't feel
Yeah, it's just decided it doesn't feel
like flying.
like flying.
It totally can.
Okay.
We're going to do a
We're going to do a
just real quick here. Uh where is it?
just real quick here. Uh where is it?
And I'm going to give you this so that
And I'm going to give you this so that
you can run a proper sweep. I'll give
you can run a proper sweep. I'll give
you the honors of doing that.
you the honors of doing that.
But uh I am going to get
few small
few small
color options here.
Huge congratulations on the release,
Huge congratulations on the release,
man. Really excited to play around with
man. Really excited to play around with
the library. Thank you very much. Yeah,
the library. Thank you very much. Yeah,
it was a huge huge amount of work. Um,
it was a huge huge amount of work. Um,
but it should be pretty good, pretty
but it should be pretty good, pretty
stable. And if it's not, come yell at me
stable. And if it's not, come yell at me
on here cuz I'm pretty much just going
on here cuz I'm pretty much just going
to be streaming for quite a bit.
to be streaming for quite a bit.
Yeah, it only knows One Direction
Yeah, it only knows One Direction
Splinter, but the thing is it's still
Splinter, but the thing is it's still
way better at the game than I am.
way better at the game than I am.
Literally pressing It's holding down
Literally pressing It's holding down
space and pressing one button, and it's
space and pressing one button, and it's
better at the game than I am.
better at the game than I am.
This is literally the first run that we
This is literally the first run that we
did. It like it'll get better from
did. It like it'll get better from
There.
Uh, I want to use different colors.
Like, it's going to be slightly
Like, it's going to be slightly
I know what it I know how to replace. I
I know what it I know how to replace. I
know how to find and replace in Vim. I'm
know how to find and replace in Vim. I'm
choosing not to.
I'll chill
cuz I want to basically figure out like
cuz I want to basically figure out like
what like what color I want to replace
what like what color I want to replace
with what, right?
And like if I want to do different
And like if I want to do different
pieces,
pieces,
do puff white.
Uh don't split on 80 like this
Uh don't split on 80 like this
neurotically. If you want to split for
neurotically. If you want to split for
80, you can split it like
80, you can split it like
here or whatever. There's no reason to
here or whatever. There's no reason to
like split like right at 80 like and
like split like right at 80 like and
leave like five characters on next line
leave like five characters on next line
of white. I just copy it over from go.
of white. I just copy it over from go.
It's
It's
fine format. Pang format is not a
fine format. Pang format is not a
requirement for puffer.
I I don't recall ever telling anyone to
I I don't recall ever telling anyone to
use clank format.
comes with Clang, LSP, and Neo Bam.
comes with Clang, LSP, and Neo Bam.
Well, I don't have an LSP,
Well, I don't have an LSP,
so I guess I wouldn't know.
Oh, yeah. You can't see these at all.
Oh, yeah. You can't see these at all.
these little particles.
I'll fix it. Don't worry.
Is this the draw particles? Is this the
Is this the draw particles? Is this the
uh the bullets or what?
Yeah, but which one is it? Wooden. Is it
Yeah, but which one is it? Wooden. Is it
this one?
Here we go.
Here we go.
That looks pretty good.
I like that.
Okay. Wouldn't that's up to you to uh to
Okay. Wouldn't that's up to you to uh to
sweep and get a good policy. Now,
sweep and get a good policy. Now,
once you get a good policy that like
once you get a good policy that like
actually looks interesting and hopefully
actually looks interesting and hopefully
doesn't just go in a straight line, we
doesn't just go in a straight line, we
can put it on the website. It'll be
can put it on the website. It'll be
official.
official.
Should be a lot easier. Now it like just
Should be a lot easier. Now it like just
trains trains super fast. 100 to 200
trains trains super fast. 100 to 200
million step sweep and you should be
million step sweep and you should be
able to be uh pretty well set up there.
That's a little better.
I'm happy with um
uh 13K on an article that technical was
uh 13K on an article that technical was
like at 2K before and I was like come on
like at 2K before and I was like come on
you got to be kidding
Okay,
Okay,
I will be uh I'm going to use the
I will be uh I'm going to use the
restroom. I'll be right back and then we
restroom. I'll be right back and then we
will figure out how to get Manny skill
will figure out how to get Manny skill
uh how to get the robot to pick up a
uh how to get the robot to pick up a
cube or whatever.
cube or whatever.
And hopefully that doesn't take too
And hopefully that doesn't take too
long. We shall see. I'll be right back.
All
right.
right.
So,
interestingly, we got roughly the same
interestingly, we got roughly the same
result out of this.
turn goes up to like 20
turn goes up to like 20
success once kind of does a thing and
success once kind of does a thing and
pitters out.
pitters out.
So, let's actually go look at our sweep
So, let's actually go look at our sweep
results. Didn't really get much of a
results. Didn't really get much of a
full sweep, but
full sweep, but
Oh,
I got to export this policy for these
I got to export this policy for these
guys real quick.
Yeah, because this is way better.
cuz I got soda for them
cuz I got soda for them
by another big margin.
I also want to look at this policy. It's
I also want to look at this policy. It's
probably pretty cool looking. Whatever
probably pretty cool looking. Whatever
this soda thing
so that'll eval. We'll jump us a replay.
a minute to generate us a replay
and we'll see what it looks like.
Why is this not dumping replays?
Oh, dumb.
I forgot to actually uncomment that.
I forgot to actually uncomment that.
All
right, give this a minute.
We will look at what this is.
Okay.
All right, we're going to actually see
All right, we're going to actually see
what this thing is doing. They sent me a
what this thing is doing. They sent me a
message that like um apparently the
message that like um apparently the
replay is it works but not quite as well
replay is it works but not quite as well
as it should. I don't know. We'll check.
viewer.
Click cable icon.
Okay.
Total reward.
Cool.
So, there's their soda policy
So, there's their soda policy
and then the
All right, they ought to be pretty happy
All right, they ought to be pretty happy
with that. That's a soda policy.
And we've uh confirmed that it actually
And we've uh confirmed that it actually
does what we think.
All right, let's go back to robotics.
I've done enough stuff on this for the
I've done enough stuff on this for the
uh the time being. Give them a little
uh the time being. Give them a little
bit of time to look at the policies and
bit of time to look at the policies and
tell me what they think.
tell me what they think.
So, we did have some sweeps on robotics.
Yeah. So, of all these they we don't
Yeah. So, of all these they we don't
really ever get above like 25 return for
really ever get above like 25 return for
some reason.
And this one here
And this one here
is pretty similar looking.
Why don't we go look at their uh their
Why don't we go look at their uh their
baseline and figure out what the heck is
baseline and figure out what the heck is
going on?
Like they have their own PO and we have
Like they have their own PO and we have
we should look at it.
we should look at it.
The thing is we we don't use PO so it's
The thing is we we don't use PO so it's
not going to be
not going to be
it's not going to be like one to one
it's not going to be like one to one
comparable
comparable
and the key annoying thing is that like
and the key annoying thing is that like
our hyper prams are going to be
our hyper prams are going to be
different
different
like our defaults are optimized for
like our defaults are optimized for
large batch.
Uh so technically our algorithm is a
Uh so technically our algorithm is a
variant of PO but we've made enough
variant of PO but we've made enough
changes to it that it's it's a
changes to it that it's it's a
meaningfully different algorithm at this
meaningfully different algorithm at this
point. It's just puffer trainer.
Yeah. But like the thing is
Yeah. But like the thing is
it depends if you want to categorize
it depends if you want to categorize
stuff by the way they behave or by a
stuff by the way they behave or by a
bunch of math that doesn't mean
bunch of math that doesn't mean
anything. If you want to write down the
anything. If you want to write down the
math, they look pretty similar. But if
math, they look pretty similar. But if
you want to actually look at like how
you want to actually look at like how
this behaves versus the original, it's
this behaves versus the original, it's
completely different. So,
Push cube v1 can take less than a minute
Push cube v1 can take less than a minute
to train on the GPU. Let's try push cube
to train on the GPU. Let's try push cube
v1.
Okay.
Okay.
Um, this is still not really training.
Um, this is still not really training.
Yeah.
And this should be a very very easy
And this should be a very very easy
task.
Yeah,
a very very easy task.
See if reward is stable at least.
Okay, so this sort of just works. I
Okay, so this sort of just works. I
guess
guess
it takes it a little longer, but it just
it takes it a little longer, but it just
works.
Do we tune hypers for this then since
Do we tune hypers for this then since
it's stable and then attempt to apply
it's stable and then attempt to apply
those to the other problem?
That would make sense to me, right?
What architecture do we want to use
What architecture do we want to use
though?
I kind of want to compare this to our
I kind of want to compare this to our
other architecture.
Okay, so 96% out of the box,
4 minutes, but again, we're fully
4 minutes, but again, we're fully
unbound here.
unbound here.
And what we're going to do is we are
And what we're going to do is we are
going to do
going to do
an RNM
We're going to compare this to our
We're going to compare this to our uh
We're going to compare this to our uh
original defaults.
I don't know why this is taking so long
I don't know why this is taking so long
to eval as well. I guess it's just the
to eval as well. I guess it's just the
end of being slow.
Uh yeah, because it's going to do like
Uh yeah, because it's going to do like
two minutes worth of epochs.
two minutes worth of epochs.
I think I have it set to like 32
I think I have it set to like 32
32 iterations by default, which is
32 iterations by default, which is
generally pretty fast, but I guess for
generally pretty fast, but I guess for
this end of it's not.
That's no big deal though.
That's no big deal though.
Try again with our other architecture
just to see how it uh how it compares.
And the goal of this is to tell us if
And the goal of this is to tell us if
there's something magical to these tages
there's something magical to these tages
that everybody in robotics seems to use
that everybody in robotics seems to use
for some reason.
That's pretty much it.
Cool.
Cool.
New Brock model.
Compare curves.
That's amusing.
That's amusing.
So, it looks like our smaller default
So, it looks like our smaller default
policy is better than their um larger
policy is better than their um larger
than their larger tuned policy
than their larger tuned policy
just out of the box.
that this is actually the result. And
that this is actually the result. And
I'll send this to Stone before we uh
I'll send this to Stone before we uh
start a sweep.
if it's stable.
Uhoh.
Weird. The reward keeps going up though,
Weird. The reward keeps going up though,
even though the success is down.
Maybe our RL is too good. I don't know
Maybe our RL is too good. I don't know
if the success is not actually correctly
if the success is not actually correctly
correlated with the reward.
What uh Stone has to say about this
What uh Stone has to say about this
new chess and SPS in first run 40k that
new chess and SPS in first run 40k that
against like some
against like some
meaningful opponent or something bet
are the open spiel is the open spiel
are the open spiel is the open spiel
just that incompetent of a object,
right? But is it running against like a
right? But is it running against like a
like is it running against an AI
like is it running against an AI
opponent, right? That it's like doing
opponent, right? That it's like doing
some deep search.
some deep search.
Oh, this is your 100% rewrite. I see.
Oh, this is your 100% rewrite. I see.
Versus random.
Versus random.
Well, you ought to be able to fix that.
Well, you ought to be able to fix that.
The chest should run billions, Okay.
stats appeared. Episode length is too
stats appeared. Episode length is too
high for chess.
high for chess.
Just taking all invalid moves. Yes,
Just taking all invalid moves. Yes,
that's expected. So, you're going to
that's expected. So, you're going to
have to do some re like some actual
have to do some re like some actual
research on that type of a thing that to
research on that type of a thing that to
figure out how to fix that.
streaming right now. Rock plays Pokemon.
streaming right now. Rock plays Pokemon.
That's fun.
just trying for save a video of this.
I'll get
just dang.
finally got an intern for myself.
finally got an intern for myself.
Congratulations
Okay, has linked to me the reward
Okay, has linked to me the reward
function.
They encourage the robot gripper to go
They encourage the robot gripper to go
behind the cube.
behind the cube.
Move the cube to the goal region. Ensure
Move the cube to the goal region. Ensure
the cube isn't being lifted up.
What's the stream about today? Currently
What's the stream about today? Currently
doing robotic stuff.
doing robotic stuff.
We're trying to get Puffer to solve
We're trying to get Puffer to solve
robotics.
code handle early resets properly.
Terminated.
Ted.
Yes, it works.
Yeah. So, this is weird. It's not
Yeah. So, this is weird. It's not
actually solving it.
Max reward of one.
Am I using the right checkpoint?
Try regenerating.
Damn you wrong.
Bizarro man
going Come on.
That's getting zero observations.
Weird.
Ah, I see it.
77k.
End of eval time is zero. Oh, so it's
End of eval time is zero. Oh, so it's
probably the action space being huge
probably the action space being huge
bat. Yeah, we're going to have to figure
bat. Yeah, we're going to have to figure
something out about that. I have no idea
something out about that. I have no idea
what, but we're going to have to figure
what, but we're going to have to figure
something out.
We have to do mana skill from source.
for Manny. I left soda their stuff
for Manny. I left soda their stuff
first.
to run this for longer. Eh,
this gives you a new metric that wasn't
this gives you a new metric that wasn't
here before.
That's once
and
If it's more stable at least.
Apply to total agents
Apply to total agents
across all workers need be less than or
across all workers need be less than or
equal to segments
equal to segments
batch size.
batch size.
So Spencer, the way it works, you have a
So Spencer, the way it works, you have a
number of agents
number of agents
and then there's a number of steps
and then there's a number of steps
you're collecting effectively per each
you're collecting effectively per each
agent
agent
and then that has to be at least the
and then that has to be at least the
batch size, right? Like that has to be
batch size, right? Like that has to be
the batch size.
the batch size.
It's like it's just two numbers or like
It's like it's just two numbers or like
one number is the product of two number
one number is the product of two number
two other numbers. So if you set three
two other numbers. So if you set three
numbers that don't make any sense, then
numbers that don't make any sense, then
it doesn't make any sense.
it doesn't make any sense.
Welcome BDF.
Welcome BDF.
We're doing some robotics. Robotics
We're doing some robotics. Robotics
doesn't make any sense. And I think it's
doesn't make any sense. And I think it's
because people have screw reward
because people have screw reward
functions.
functions.
We will see
because according to this
because according to this
in terms of reward, we're shredding. But
in terms of reward, we're shredding. But
then actually what happens is success
then actually what happens is success
can go down even though reward goes up.
Uh, I think it's that they have too much
Uh, I think it's that they have too much
reward shaping in this instance That
130k using a Mac CPU. Um,
130k using a Mac CPU. Um,
I think that's roughly right. Let's put
I think that's roughly right. Let's put
it this way. It's about 20 times faster
it this way. It's about 20 times faster
than you would get using a high-end GPU
than you would get using a high-end GPU
on anything except Puffer Lib.
I run this for 20 mil. Damn it.
uh let me see can we control worker
uh let me see can we control worker
let's say I want 20 worker training
let's say I want 20 worker training
parallel
total CPU so that means that you're
total CPU so that means that you're
bottlenecked by the training op. So, no,
bottlenecked by the training op. So, no,
you can't you can't do um
you can't you can't do um
at least I don't think that you can do
at least I don't think that you can do
CPU parallel training. You can do GPU
CPU parallel training. You can do GPU
parallel training. You can technically
parallel training. You can technically
if you want you could try to like make
if you want you could try to like make
our distributed data parallel work
our distributed data parallel work
multicore. That's kind of a weird thing
multicore. That's kind of a weird thing
to do. Um it's basically you're just
to do. Um it's basically you're just
getting bound by the training operation
getting bound by the training operation
which is just kind of expected because
which is just kind of expected because
you're training on CPU.
you're training on CPU.
Did you fix 2048?
Did you fix 2048?
Um, sort of. We fixed a couple of things
Um, sort of. We fixed a couple of things
and then sent it back. It'll return to
and then sent it back. It'll return to
sender for edits.
sender for edits.
We did fix asteroids. We fixed meta
We did fix asteroids. We fixed meta
today as well. Meta now works perfectly.
today as well. Meta now works perfectly.
And uh, we're working on robotic stuff.
CPU is not going to be maxed out unless
CPU is not going to be maxed out unless
we're running. Yeah. So, it's
we're running. Yeah. So, it's
essentially your CPU. You're looking at
essentially your CPU. You're looking at
the you're looking at your dashboard or
the you're looking at your dashboard or
you're looking at like top or something.
you're looking at like top or something.
But the thing is CPUs get used mostly
But the thing is CPUs get used mostly
for the environment. And I don't know
for the environment. And I don't know
what Torch does for training. it
what Torch does for training. it
probably doesn't do like multi-core CPU
probably doesn't do like multi-core CPU
proper training correctly. Um,
proper training correctly. Um,
but you shouldn't ever be bottlenecked
but you shouldn't ever be bottlenecked
by the CPU when you're running fast ends
by the CPU when you're running fast ends
if you're running GPU training, if that
if you're running GPU training, if that
makes sense. You shouldn't have to scale
makes sense. You shouldn't have to scale
up like you shouldn't be trying to max
up like you shouldn't be trying to max
your CPU because you should get bound by
your CPU because you should get bound by
GPU normally.
Okay, this is 97%.
I have
I have
256 gigs of V RAM and VRAM. Yeah, that
256 gigs of V RAM and VRAM. Yeah, that
just doesn't matter in RL is the thing.
just doesn't matter in RL is the thing.
You need just pure compute. Like
You need just pure compute. Like
it's not a language model where you're
it's not a language model where you're
like RAM bound. You're always going to
like RAM bound. You're always going to
be you're always like you're always
be you're always like you're always
computebound, not memory bound.
try some of the other environments like
try some of the other environments like
you're currently with 130K you're
you're currently with 130K you're
literally running RL 20 times faster
literally running RL 20 times faster
than uh those M's run on GPU and other
than uh those M's run on GPU and other
libraries but like it's you're running
libraries but like it's you're running
like I don't know 5% of the speed of a
like I don't know 5% of the speed of a
GPU because your CPU is 5% of the speed
GPU because your CPU is 5% of the speed
of a GPU
Oh, I guess it could be a dense reward
Oh, I guess it could be a dense reward
thing technically.
Maybe not. Maybe this is an oversight.
and check their implementation.
Thank you.
Next value.
final values. Yes.
Next values is what is
Do I need to register an end on properly
Do I need to register an end on properly
before training for a custom thing? So,
before training for a custom thing? So,
for a custom thing, follow the uh the
for a custom thing, follow the uh the
tutorial. Like,
tutorial. Like,
if you want to use our like just our out
if you want to use our like just our out
of the box trainer, then yeah, the M's
of the box trainer, then yeah, the M's
have to be in there. If you want to like
have to be in there. If you want to like
hack it up and use like the trainer as
hack it up and use like the trainer as
an API thing, then no, it's not like a
an API thing, then no, it's not like a
gym.register, though. It's just like the
gym.register, though. It's just like the
ocean environments just inside of
ocean environments just inside of
environment. Just add one line.
That's in the docs, right?
That's in the docs, right?
I don't think I forgot that.
I don't think I forgot that.
See if I forgot that.
Did I say add a line? Yeah, add a line
Did I say add a line? Yeah, add a line
to ocean environment.py.
Just at the bottom of there.
Yeah. Switch farming reward.
We add
final values.
because that's an annoying thing to fix.
because that's an annoying thing to fix.
That's like quite difficult to fix
That's like quite difficult to fix
actually.
Not easy at all.
I mean to think about at least it's
I mean to think about at least it's
probably a twoline change but it doesn't
probably a twoline change but it doesn't
make it
How can I train more time instead of
How can I train more time instead of
default
default
train until it reaches a certain min
train until it reaches a certain min
reward
reward
for each that that as you've stated it
for each that that as you've stated it
doesn't make sense. You can increase the
doesn't make sense. You can increase the
number of training steps though. You can
number of training steps though. You can
essentially just increase the number of
essentially just increase the number of
training steps in any of the configs
training steps in any of the configs
or from command
Is that a reasonable thing to Okay.
I got to look at clean's original.
Drives me nuts. This is always like such
Drives me nuts. This is always like such
an awkward
an awkward
uh implementation.
uh implementation.
Everybody does it wrong.
should be built 10.
Weird
docs does not have how to set the number
docs does not have how to set the number
of steps for training using args. d-help
of steps for training using args. d-help
will give you all of the valid
will give you all of the valid
arguments. And you'll see that there's a
arguments. And you'll see that there's a
total time steps argument.
See
if what I did makes any sense.
I think it does, right?
We'll see if it breaks other ends.
Yeah, I think that this works for this,
Yeah, I think that this works for this,
but it doesn't make sense for like
but it doesn't make sense for like
literally anything That's
Oh, do we still break?
Yeah, it still breaks. Never mind.
Uh that's for if you don't have enough
Uh that's for if you don't have enough
GPU memory for observations, which in
GPU memory for observations, which in
practice is never the case.
If I just do n rewards equals reward
not breaking that will set.
Wait, what? Star and
Wait, what? Star and
I think that should set the first
I think that should set the first
element of the array
element of the array
to the reward. I think
to the reward. I think
for a single agent env. I think that's
for a single agent env. I think that's
correct.
correct.
It's so much easier to just do end
It's so much easier to just do end
rewards of zero equals reward though. I
rewards of zero equals reward though. I
don't know why you would do that. No,
don't know why you would do that. No,
it's not rewriting the whole pointer.
That's something else. That would be end
That's something else. That would be end
rewards equals reward
rewards equals reward
without the star. Great. That would
without the star. Great. That would
write the pointer.
write the pointer.
Don't want to do
and I let me be I'll be right back and
and I let me be I'll be right back and
I'm going to have to think about what to
I'm going to have to think about what to
do about this because they're like
do about this because they're like
they're fundamentally treating the
they're fundamentally treating the
reward signal and the terminal
reward signal and the terminal
conditions as something that it isn't.
conditions as something that it isn't.
So, as expected, robotics very cursed.
So, as expected, robotics very cursed.
I'll be right back.
I've misclicked on mute. Okay, so
I've misclicked on mute. Okay, so
I think I figured it out. And uh it's
I think I figured it out. And uh it's
literally that all the roboticists are
literally that all the roboticists are
just doing their rewards wrong.
just doing their rewards wrong.
I'm pretty sure that's what it is, which
I'm pretty sure that's what it is, which
is kind of funny, but I wouldn't be
is kind of funny, but I wouldn't be
surprised. And like the thing is they're
surprised. And like the thing is they're
not aware of it because they also like
not aware of it because they also like
they also have their bootstrapping
they also have their bootstrapping
function wrong and the errors cancel
function wrong and the errors cancel
out. But it makes it so that you
out. But it makes it so that you
literally can't use their
literally can't use their
implementations on anything else or
implementations on anything else or
other implementations on their problems
other implementations on their problems
which would give you the effect of
which would give you the effect of
making the problems look very hard when
making the problems look very hard when
they're not.
So let's figure that out.
the reporters that he sent me.
asks.
So this is 209.
Just real quick, put this here. This
Just real quick, put this here. This
here. We'll swap these.
dude. This is negative 10h.
Okay. So now all the rewards are
Okay. So now all the rewards are
negative.
turn.
turn.
Yeah, this goes
So now if this doesn't work then I have
So now if this doesn't work then I have
to think way harder about the math but I
to think way harder about the math but I
think that this should work.
Um BDF env
that's for ocean m specifically and it
that's for ocean m specifically and it
sets the number of copies that are run
sets the number of copies that are run
on each core.
on each core.
So when we do like vecum ms 8 and mnum m
So when we do like vecum ms 8 and mnum m
8,192
8,192
total ms on eight different cores
But we'll see if this still crashes.
But we'll see if this still crashes.
I'm thinking about this and I can't I
I'm thinking about this and I can't I
can't see how this could
can't see how this could
I guess technically some of like the
I guess technically some of like the
reward normalization stuff could mess it
reward normalization stuff could mess it
up still.
up still.
I don't think so though.
This looks good though.
That's ridiculous.
I don't know what to do about this, man.
I don't know what to do about this, man.
It's literally all the rewards are
It's literally all the rewards are
backwards.
backwards.
Like the rewards are all backwards from
Like the rewards are all backwards from
outside of robotics.
That's kind of a big one. A
Try the other task.
Well, that's a crazy thing to discover.
Also,
hey Spencer, how's it
emailing more RL bros. Cool. Yeah,
emailing more RL bros. Cool. Yeah,
Spencer, I just figured out that all of
Spencer, I just figured out that all of
robotics is backwards. That's cool.
You know,
pretty much
it's not one over everything. You just
it's not one over everything. You just
have to delete. They do one minus
have to delete. They do one minus
everything. I guess it's not backwards.
everything. I guess it's not backwards.
It's offset by a coefficient that
It's offset by a coefficient that
totally screws you.
You literally just need to subtract one
You literally just need to subtract one
from all their reward components.
This thing is somehow
I think I have to mess with this one a
I think I have to mess with this one a
little more.
little more.
But why though?
But why though?
Because if it's So these are dense
Because if it's So these are dense
rewards. So basically the way they set
rewards. So basically the way they set
up their M there's a goal state and they
up their M there's a goal state and they
have it so that you get a positive goal
have it so that you get a positive goal
a positive reward as you get closer to
a positive reward as you get closer to
the goal.
the goal.
So as a result of that
So as a result of that
uh
uh
as a result of that you can farm reward
as a result of that you can farm reward
by getting close to the goal but never
by getting close to the goal but never
reaching it.
Okay. So, it's still doing the same
Okay. So, it's still doing the same
thing cuz apparently I missed a
thing cuz apparently I missed a
component.
Try this again.
Yeah, that's not how it works. That
return should stay negative here.
return should stay negative here.
I will be surprised if um
if this stays negative and doesn't
if this stays negative and doesn't
solve.
solve.
So before I missed a component so it
So before I missed a component so it
could still farm.
Interesting.
That has zero success so far.
There it goes.
Should
I manually be calling reset on terminal
like this is done? We do not auto reset
like this is done? We do not auto reset
for you. You are respons the environment
for you. You are respons the environment
is responsible for handling its own
is responsible for handling its own
resets.
This is true in all ends.
At least here it's actually the reward
At least here it's actually the reward
is well aligned now, right?
Return gets better, success goes up.
Return gets better, success goes up.
That was not the case before.
rap look like.
Oh no, it's still learning.
Oh no, it's still learning.
Yeah, it's totally still learning.
We're fine.
episode return is still improving.
Okay, so this bothers me that this is
Okay, so this bothers me that this is
not solving. It's training,
not solving. It's training,
but it's not solving.
but it's not solving.
We'll have to look at the policy for
We'll have to look at the policy for
this and see what it's doing.
Okay, so that's very close.
It's still farming it.
Yeah, but it's not.
Yeah, but it's not.
Why is this thing not registering is
Why is this thing not registering is
correct?
I thought I'd fix that.
Uh, that's not how it works.
Uh, that's not how it works.
That works a bit better.
Okay.
static reward
place.
got to be the advantage normalization.
what we can do about that.
Hey.
Yeah. Okay. I will not use
Yeah. Okay. I will not use
the sink.
Okay, let me think about this.
Damn it.
Damn it.
This would work without reward without
This would work without reward without
advantage normalization I believe.
Let me think what we can do.
First reward.
Yeah, but the thing is you're not ever
Yeah, but the thing is you're not ever
going to solve it randomly.
going to solve it randomly.
Okay.
I should just make them a better reward
I should just make them a better reward
function.
What I should
Hey, major
You know what I can do? I can do it as a
You know what I can do? I can do it as a
delta.
I can do this entire thing as a delta.
So, what I'm doing here to be clear,
So, what I'm doing here to be clear,
they have this dense reward signal
they have this dense reward signal
defined, but it has this annoying
defined, but it has this annoying
farming property where like get some
farming property where like get some
amount of reward near the goal. You just
amount of reward near the goal. You just
want to hang out around the goal. What
want to hang out around the goal. What
I've done is I've reformulated the
I've done is I've reformulated the
reward to be in terms of a delta from
reward to be in terms of a delta from
the previous. Um, and since it's dense,
the previous. Um, and since it's dense,
this essentially just rewards you for
this essentially just rewards you for
getting closer to the goal state, but
getting closer to the goal state, but
not actually for hanging out there. You
not actually for hanging out there. You
only get rewarded for your actions. You
only get rewarded for your actions. You
don't get rewarded for just chilling in
don't get rewarded for just chilling in
any given state.
any given state.
And we'll see if this makes a
And we'll see if this makes a
difference.
I have a question for you. Can math
I have a question for you. Can math
problems be solved with reinforcement
problems be solved with reinforcement
learning? For example, there's a problem
learning? For example, there's a problem
that starts with an equation solve step
that starts with an equation solve step
by step. Each step includes information
by step. Each step includes information
from the previous one. If this satisfies
from the previous one. If this satisfies
the MVP, then something like an RL
the MVP, then something like an RL
language model that can solve equations
language model that can solve equations
by selecting. So, the problem with RL
by selecting. So, the problem with RL
language models in general is you kind
language models in general is you kind
of just have to light all your money on
of just have to light all your money on
fire. um you kind of have to be like
fire. um you kind of have to be like
Google or Deep Mind to do anything at
Google or Deep Mind to do anything at
reasonable scale.
reasonable scale.
If I were trying to do some RL math
If I were trying to do some RL math
thing, I would look to something like um
thing, I would look to something like um
I would make actions like informal logic
I would make actions like informal logic
and probably have some sort of fast
and probably have some sort of fast
verifier
verifier
if I wanted to get it to like uh you
if I wanted to get it to like uh you
know write proofs or something.
know write proofs or something.
But then that would be tricky because
But then that would be tricky because
you'd basically have to tokenize you
you'd basically have to tokenize you
tokenize math in a much smaller more
tokenize math in a much smaller more
constrained way that doesn't correspond
constrained way that doesn't correspond
to an actual language basically.
Also, I think it would be difficult to
Also, I think it would be difficult to
have multiple solutions
have multiple solutions
give reward for all of them being
give reward for all of them being
correct. No, because you would have a
correct. No, because you would have a
verifier. You need some sort of like
verifier. You need some sort of like
formal proof verifier or something.
These aren't these aren't private
These aren't these aren't private
general RL discussion.
Can we solve this?
This could be hypers now to be fair.
This could be hypers now to be fair.
This could just be hypers.
What is our reward?
Uh, kind of going up
Uh, kind of going up
to say.
Oh, the reward's just way too low at
Oh, the reward's just way too low at
this point. Hang on.
board is like way too low.
This is 100x larger scale. They should
This is 100x larger scale. They should
get to like fun.
not stable.
Word's probably a bit too big here.
Oh yeah, this is too big.
This
This
How do you know if it's actually better
How do you know if it's actually better
when you change the reward function? I'm
when you change the reward function? I'm
looking at the success rate.
looking at the success rate.
The success rate is independent of the
The success rate is independent of the
reward. That's a very important concept.
When you're tuning a RL environment, you
When you're tuning a RL environment, you
always want to have a metric that is
always want to have a metric that is
separate from the scale of the reward
separate from the scale of the reward
that tells you how well you're doing.
that tells you how well you're doing.
So, if it's a game, you look at the
So, if it's a game, you look at the
actual score of the game, which is
actual score of the game, which is
separate from, say, the amount of reward
separate from, say, the amount of reward
you're getting per point that you
just being a little fiddly.
Are these things offset?
Are these things offset?
But I did have them offset.
They're not offset.
Oh, no. They would get offset.
Oh, no. They would get offset.
Yeah, they would get offset.
Not super stable.
Well, it'll be interesting regardless to
Well, it'll be interesting regardless to
see what this agent's actually doing.
It's got to just be idling around the uh
It's got to just be idling around the uh
the solution still somehow,
the solution still somehow,
which how would that even happen?
which how would that even happen?
Wait, how's this how's this reward thing
Wait, how's this how's this reward thing
work?
This is a 0ero to one reward.
Yeah, this thing is now at zero reward.
Nonzero success
thing.
Would like to see what this is.
Sure.
I'm sir.
This just looks like it failing for some
This just looks like it failing for some
reason.
Why would it do that?
Silly robot.
Why the heck does it do that?
Do we try this on um
Do we try this on um
simpler ends or what?
or just like pitters out.
beard.
Not terrible. Huh?
Like reasonable rewards to me.
So weird.
fact that it evens out to like zero
fact that it evens out to like zero
reward as well.
the idx and option. Oh,
the idx and option. Oh,
I see
I see
happening.
I think I can probably
I think I can probably
probably just hack it a little bit.
So I think what was happening is every
So I think what was happening is every
time any end was getting reset it was
time any end was getting reset it was
just resetting all of the uh the states.
Okay,
that's doing something.
Here's a restroom real quick while this
Here's a restroom real quick while this
runs.
We'll see how this goes.
Okay. Well, we're getting um
Okay. Well, we're getting um
values
values
returns jumping all over the place.
We have here
Let me clear
Oh.
Huh.
GG.
GG.
GG. Robotics.
I think we just win robotics now.
Send stone a quick message.
Hey, Spencer.
Hey, Spencer.
Actually, I can share this publicly.
Actually, I can share this publicly.
There's nothing private here.
Yeah, that's GG to robotics though.
Yeah, that's GG to robotics though.
At least for uh the application of
At least for uh the application of
having puffer lib work on these.
This is for reference. This is like
This is for reference. This is like
garbage hyperparameters for this task.
garbage hyperparameters for this task.
Like complete random model. like this is
Like complete random model. like this is
the least optimized possible thing and
the least optimized possible thing and
it works perfectly now that I have fixed
it works perfectly now that I have fixed
the rewards. So, it's literally just
the rewards. So, it's literally just
that uh robotics is trolling with
that uh robotics is trolling with
rewards.
Let me get this thing implemented with
Let me get this thing implemented with
stone fencer and then yeah, we'll be
stone fencer and then yeah, we'll be
able to do it. I got to run it on some
able to do it. I got to run it on some
harder problems, but yeah,
harder problems, but yeah,
it'll be good.
it'll be good.
I think our trainer is just going to be
I think our trainer is just going to be
like the thing is anybody who touches RL
like the thing is anybody who touches RL
like our trainer is just better. So,
like our trainer is just better. So,
and this is kind of why I still want to
and this is kind of why I still want to
do core algorithm work, right? Because
do core algorithm work, right? Because
like it makes stuff like this happen.
like it makes stuff like this happen.
Like no amount of fiddling would do this
Like no amount of fiddling would do this
on on Puffer Lib 2, but now it just
on on Puffer Lib 2, but now it just
works.
works.
This took me literally a day, not even.
And the only reason it didn't work out
And the only reason it didn't work out
of the box is because the rewards were
of the box is because the rewards were
defined incorrectly.
Like robotics apparently has a weird way
Like robotics apparently has a weird way
of defining rewards. It's just not how
of defining rewards. It's just not how
rewards are supposed to work.
No.
That's a GG to robotics.
How many samples does it take them to do
How many samples does it take them to do
this?
Hey Tim,
Hey Tim,
I want to see like how many how many
I want to see like how many how many
more samples it takes us to do it than
more samples it takes us to do it than
them because like
them because like
we're not optimized at all. So if we're
we're not optimized at all. So if we're
in within a factor of like two or three
in within a factor of like two or three
that would be ridiculous. I don't know
that would be ridiculous. I don't know
if we are. I think it's probably like
to be fair, we are in a factor though
to be fair, we are in a factor though
with time like time is about the same.
with time like time is about the same.
So that's not bad.
Yeah, we we can shred this can shred
Yeah, we we can shred this can shred
this super easily.
What we should do is we should run a
What we should do is we should run a
little sweep on this.
I should run like a little sweep on this
I should run like a little sweep on this
task specifically.
be good.
be good.
But the thing is I do have some more
But the thing is I do have some more
time before we have to do this.
I'm going to wait for a stone to uh to
I'm going to wait for a stone to uh to
tell me how I should implement this.
But yeah, pretty darn good robotics
But yeah, pretty darn good robotics
project progress.
All right. What are the other things we
All right. What are the other things we
have to get done around here? Let me
have to get done around here? Let me
think. So, I know I want to start on the
think. So, I know I want to start on the
drone stuff or the uh the tactics the
drone stuff or the uh the tactics the
tactical drone stuff um for most of
tactical drone stuff um for most of
tomorrow.
tomorrow.
So yeah, for folks uh interested, the
So yeah, for folks uh interested, the
end we're going to be messing with
end we're going to be messing with
tomorrow.
I'll show the quick version of it. It's
I'll show the quick version of it. It's
not going to look anywhere near as cool
not going to look anywhere near as cool
as it'll look once you have trained
as it'll look once you have trained
agents on it. You can see like all these
agents on it. You can see like all these
different ships that are kind of
different ships that are kind of
attempting to fly out. They're cars as
attempting to fly out. They're cars as
well. Uh these are random policies, so
well. Uh these are random policies, so
they literally don't know how to fly.
they literally don't know how to fly.
But imagine like a full scale battle
But imagine like a full scale battle
with a thousand ships, a whole bunch of
with a thousand ships, a whole bunch of
different armies. Yeah, we're going to
different armies. Yeah, we're going to
do that as an RLN and actually make this
do that as an RLN and actually make this
thing work tomorrow. It kind of works at
thing work tomorrow. It kind of works at
the moment, but it's not great.
All right. Um,
think what to do.
I kind of just solved their thing super
I kind of just solved their thing super
fast.
And this is one of their easier ends.
Still though, for it to work out of the
Still though, for it to work out of the
box kind of ridiculous. I wonder what
box kind of ridiculous. I wonder what
happens if I just crank update epochs.
happens if I just crank update epochs.
Does that do anything?
We all know what's going to happen here,
We all know what's going to happen here,
right? It gets the puffer treatment and
right? It gets the puffer treatment and
then like as soon as we have the
then like as soon as we have the
speedrun target, it just drops like a
speedrun target, it just drops like a
[ __ ]
if this is data limited or not.
The thing is like robotics tasks from a
The thing is like robotics tasks from a
learning perspective are like really
learning perspective are like really
easy if you think about it. Like you can
easy if you think about it. Like you can
kind of basically flail around and get
kind of basically flail around and get
all the data that you need. That's not
all the data that you need. That's not
true in the rest of RL. Like when people
true in the rest of RL. Like when people
talk about like these really hard
talk about like these really hard
robotics tasks, they're really kind of
robotics tasks, they're really kind of
just data limited. They should not be
just data limited. They should not be
hard tasks.
Okay, so they actually do it pretty
Okay, so they actually do it pretty
quick here. 72 seconds.
We have a little ways to go to beat
We have a little ways to go to beat
that.
It's not going to survive a sweep,
It's not going to survive a sweep,
though. There's no way.
Interesting. It doesn't actually seem
Interesting. It doesn't actually seem
like the uh the additional data helps
like the uh the additional data helps
very much.
You're getting maybe very slightly
You're getting maybe very slightly
faster takeoff.
faster takeoff.
Very slightly.
Okay. Their gamma's kind of insane
Okay. Their gamma's kind of insane
though, right?
We get rid of this
We get rid of this
9. Play with this a little bit, you
9. Play with this a little bit, you
know.
Yeah. So, 3 minutes 40 seconds. We're
Yeah. So, 3 minutes 40 seconds. We're
off by a factor of three, I believe,
off by a factor of three, I believe,
from their policy.
from their policy.
Get them uh by a factor of three.
try making policy bigger next
The return is very low.
The return is very low.
Gets to be reasonable.
kind of need to just figure out how to
kind of need to just figure out how to
implement the other thing because this
implement the other thing because this
negative shouldn't happen. This is just
negative shouldn't happen. This is just
from bad resets and that can mess with
from bad resets and that can mess with
learning.
Maybe we'll do that next.
need the infos though.
need the infos though.
I don't really know how to do this
I don't really know how to do this
the way that their code is.
I think we kind of have to go from here
I think we kind of have to go from here
to uh
to uh
wherever they call this from, which is
wherever they call this from, which is
probably in the basin.
This is worse. Okay.
Try
That
Okay. So you compute your delta reward
Okay. So you compute your delta reward
There.
Let's
see what their
see what their
their ve rapper does.
Fanny skill vector rapper gymnasium.
Okay. So it calls self.reset
which then calls en reset
with options.
with options.
Okay. Okay. And then this gets passed.
The options is Nvid.
The options is Nvid.
So they directly will call reset
So they directly will call reset
which returns observations.
Weird that there's a action in the
Weird that there's a action in the
reward function.
Oh, it's not used.
on this.
Yeah, it's a little tricky.
Yeah, it's a little tricky.
I think what we're going to have to do,
I think what we're going to have to do,
we have to recmp compute all of them
we have to recmp compute all of them
now.
This works.
Now we need to set last reward in the
Now we need to set last reward in the
inette.
be cool.
Presume there's a self num ms
but this is this castle.
Oh, we also need to set the reward type,
Oh, we also need to set the reward type,
don't we?
Okay.
Ah,
Oh, there is a self device.
Try this.
Well, the return looks a lot more
Well, the return looks a lot more
reasonable now.
See?
Oh, yeah. That's way better.
Think negative is very weird.
on reset.
Oh yeah, we look at that. That's a big
Oh yeah, we look at that. That's a big
difference. Way more consistent return.
difference. Way more consistent return.
What was happening before, right, is as
What was happening before, right, is as
soon as you reset the environment,
soon as you reset the environment,
you get the full penalty
you get the full penalty
equal pretty much to your original uh
equal pretty much to your original uh
you get the full penalty of like going
you get the full penalty of like going
from the goal to the start, which is
from the goal to the start, which is
pretty much your whole reward. So, it
pretty much your whole reward. So, it
was screwing up our curves.
was screwing up our curves.
Now, as to whether or not this will do
Now, as to whether or not this will do
anything for training or make it worse,
anything for training or make it worse,
who knows? because reinforcement
who knows? because reinforcement
learning. But at least we have something
learning. But at least we have something
way more reasonable
and we should have a return that should
and we should have a return that should
be perfect perfectly correlated with
be perfect perfectly correlated with
success now.
Yeah. So minor tiny little pickup here
Yeah. So minor tiny little pickup here
but overall perfect.
We'll just do a couple quick little
We'll just do a couple quick little
things
just to see if we are like missing any
just to see if we are like missing any
major stuff and then we'll see if we can
major stuff and then we'll see if we can
get sweeps to just cover it.
Delta report based on replied
Yeah, this thing doesn't seem to be
Yeah, this thing doesn't seem to be
doing very much. Eh,
doing very much. Eh,
returns super low.
I don't know what I expected to be fair.
I don't know what I expected to be fair.
That was like their set of params, but
That was like their set of params, but
those don't really make much sense.
Let's go back to 32
Let's go back to 32
horizon and let's just go to 16.
Can't we just move? Hang on.
Can't we just move? Hang on.
I just realized something.
I can just do this.
I literally just do this and it works
I literally just do this and it works
for all ends.
This is doing
discernable difference.
Do I believe
Do I believe
that uh 60 degree of freedom or whatever
that uh 60 degree of freedom or whatever
robot
robot
can only run 40k SPS?
can only run 40k SPS?
This thing's got to be able to run like
This thing's got to be able to run like
a millie, right?
a millie, right?
I don't know about humanoids, but I
I don't know about humanoids, but I
think this thing should be able to run
think this thing should be able to run
way faster than this.
way faster than this.
Way faster,
I would hope.
I would hope.
try the bigger batch after this because
try the bigger batch after this because
who knows maybe counterintuitively it's
who knows maybe counterintuitively it's
better
actually that's dependent on the M's
actually that's dependent on the M's
isn't it never mind
Which M's do you think would be the most
Which M's do you think would be the most
aggressive
aggressive
experiments for making gamma less
experiments for making gamma less
cooked?
Honestly,
Honestly,
we'd have to come up with a synthetic
we'd have to come up with a synthetic
one.
one.
We'd probably what we would do is
we'd like make a periodic function I
we'd like make a periodic function I
think.
think.
So imagine that the reward is like the
So imagine that the reward is like the
sum of a bunch of waves, right? Like you
sum of a bunch of waves, right? Like you
have something where you get a reward
have something where you get a reward
every two like every time step. Then
every two like every time step. Then
there's something that's like you can
there's something that's like you can
get a reward every 10 time steps if you
get a reward every 10 time steps if you
do the right thing 100 time steps
do the right thing 100 time steps
thousand like something like that.
thousand like something like that.
And then the idea is that probably gamma
And then the idea is that probably gamma
prevents you doing multiscale learning
prevents you doing multiscale learning
as well as you'd want. It kind of
as well as you'd want. It kind of
depends though.
The other thing you could do would just
The other thing you could do would just
literally be to be to like make an N
literally be to be to like make an N
that has a configurable reward density,
that has a configurable reward density,
right? And then like show that like,
right? And then like show that like,
hey, you can change the reward density
hey, you can change the reward density
and then gamma has to change with it.
and then gamma has to change with it.
And then you could try to patch the
And then you could try to patch the
algorithm so that it doesn't need that.
It's tough. Very tough.
Good.
There's no way. I just happen to guess
There's no way. I just happen to guess
the optimal,
the optimal,
right?
right?
Ah, there's got to be way more wiggle
Ah, there's got to be way more wiggle
room in here.
Interesting.
Expecting this to be
Expecting this to be
in some way.
Kind of a latching problem, right?
before Horizon seems better though
about it.
I don't see how this gamma makes any
I don't see how this gamma makes any
sense. Do you?
sense. Do you?
40step problem.
40step problem.
I guess it's because the rewards are
I guess it's because the rewards are
like so
like so
the rewards are so dense.
the rewards are so dense.
Like every single step is basically
Like every single step is basically
local optimal, right?
Oh, actually with the delta you don't
Oh, actually with the delta you don't
even have to do credit assignment,
even have to do credit assignment,
right?
Hang on. If you don't even have to do
Hang on. If you don't even have to do
credit assignment, then like
credit assignment, then like
Huh?
Huh?
Wait, isn't this totally trivial?
reduces to almost a bandit problem with
reduces to almost a bandit problem with
a dense reward.
Yeah,
let me go back to
Go back to this.
Just want to see something.
Just want to see something.
I doubt that this will match my
I doubt that this will match my
intuition, but like this is basically a
intuition, but like this is basically a
bandit problem.
bandit problem.
Do you see what I mean?
Do you see what I mean?
Like if you have a perfectly dense
Like if you have a perfectly dense
reward that's not un like that's
reward that's not un like that's
perfectly informative, then it's
perfectly informative, then it's
literally a bandit problem. It should be
literally a bandit problem. It should be
trivial.
Somehow isn't.
Okay.
You can go the other way.
If it's the opposite of my intuition,
If it's the opposite of my intuition,
why not?
And after this, we're just going to go
And after this, we're just going to go
set up a sweep.
Oh, I guess I should You know what?
Oh, I guess I should You know what?
Instead of trying this, cuz this will
Instead of trying this, cuz this will
get covered by the sweep automatically.
Try this instead.
Try this instead.
Make sure it's not policy bound.
Make sure it's not policy bound.
Shouldn't be, but never know.
lower so far.
I liken this to Pong.
Not even right.
It probably just needs a sweep. Like
It probably just needs a sweep. Like
this is such an incredibly easy task
this is such an incredibly easy task
when you think about it
when you think about it
with this reward signal.
still lower.
At the very least, you don't just
At the very least, you don't just
immediately win by increasing the policy
immediately win by increasing the policy
size without changing anything else.
Yeah, you can see this is like so much
Yeah, you can see this is like so much
slower.
slower.
Well,
Well,
we'll let the empirical thing
we'll let the empirical thing
uh we'll let the empirical thing just do
uh we'll let the empirical thing just do
whatever it's going to.
Okay,
Okay,
that will run
that will run
uh and by tomorrow we will have ideas.
What is this?
Okay. Well,
how are you all doing on YouTube?
how are you all doing on YouTube?
You kind of just solved everything I
You kind of just solved everything I
wanted to do today. I thought that was
wanted to do today. I thought that was
going to take like multiple days.
going to take like multiple days.
Obviously, we still have sweeps,
Obviously, we still have sweeps,
sweeps and stuff to figure out, but
sweeps and stuff to figure out, but
we're in a pretty good spot with uh the
we're in a pretty good spot with uh the
robotics integrations at the moment.
robotics integrations at the moment.
Anyone have any questions? Any pressing
Anyone have any questions? Any pressing
concerns?
Use star of the puffer
Use star of the puffer
helps me out a lot. Free
is nice.
I'm pretty sure. Yeah, this is so
I'm pretty sure. Yeah, this is so
tomorrow I guess the plan will be that
tomorrow I guess the plan will be that
we check in on our robotics. I'll
we check in on our robotics. I'll
probably check on this later tonight to
probably check on this later tonight to
make sure, you know, we're actually
make sure, you know, we're actually
getting something out of the sweep.
getting something out of the sweep.
And uh I'll chat with Stone for a little
And uh I'll chat with Stone for a little
bit on how we do
how we integrate all this
how we integrate all this
and you know, we'll start running more
and you know, we'll start running more
of the uh more of the tasks in here.
of the uh more of the tasks in here.
But yeah, other than that, uh we will
But yeah, other than that, uh we will
work on the battle, like the large scale
work on the battle, like the large scale
battle ends. And I guess we'll see how
battle ends. And I guess we'll see how
crazy I go on that.
crazy I go on that.
I think probably I'm just going to
I think probably I'm just going to
downscale it to like a hundred something
downscale it to like a hundred something
agents, two teams to start with, just
agents, two teams to start with, just
because it'll make it quick to iterate
because it'll make it quick to iterate
on and uh yeah, we'll go from there
on and uh yeah, we'll go from there
after that.
after that.
That will be the plan.
That will be the plan.
But there's nothing else from any of you
But there's nothing else from any of you
then. Now, I think I will uh I'll call
then. Now, I think I will uh I'll call
it early. I got a couple emails to
it early. I got a couple emails to
respond to, a couple documents to read,
respond to, a couple documents to read,
and then yeah, after that,
and then yeah, after that,
uh I'm going to get some dinner, get
uh I'm going to get some dinner, get
some rest, and I'll be back bright and
some rest, and I'll be back bright and
early in the morning. So, thanks for
early in the morning. So, thanks for
tuning in, folks. See you tomorrow.

Kind: captions
Language: en
Okay,
Okay,
be live here.
There we go.
There we go.
Hi.
A few things to get done today.
Mostly it's the uh meta env and robotics
Mostly it's the uh meta env and robotics
work are the two main things.
Trying to think what we do with this.
Trying to think what we do with this.
Let me let me pull up some results and
Let me let me pull up some results and
sort of just show you what I'm uh what
sort of just show you what I'm uh what
I'm looking at here.
I'm looking at here.
Okay, so we ran this overnight.
Uh this cube sweep doesn't find anything
Uh this cube sweep doesn't find anything
good at all. So, we'll come back to
good at all. So, we'll come back to
this.
That's inconclusive.
That's inconclusive.
But this one's a lot weirder.
You see
You see
this gets up to 3.5ish.
200 experiments worth of data.
But then the thing that's weird about
But then the thing that's weird about
this is that we did these 200
this is that we did these 200
experiments.
experiments.
We did quite a bit better.
Interesting.
Yeah, I ran the five one for longer and
Yeah, I ran the five one for longer and
it does actually reproduce,
it does actually reproduce,
but then it oscillates between four and
but then it oscillates between four and
five unstably.
Maybe we need to just reproduce this
Maybe we need to just reproduce this
policy and then look at it, you
Damn,
kind of tired.
Got should have gotten enough sleep, but
Got should have gotten enough sleep, but
I've been studying quantum mechanics and
I've been studying quantum mechanics and
uh like just this mind-boggling topic.
uh like just this mind-boggling topic.
Um, even the basic stuff is very
Um, even the basic stuff is very
mindboggling. It's just very
mindboggling. It's just very
unintuitive. So, I think I fell asleep
unintuitive. So, I think I fell asleep
thinking about wave functions.
Was it wave functions? No, I fell
Was it wave functions? No, I fell
asleep. It was even weirder.
asleep. It was even weirder.
I um I fell asleep thinking about the
I um I fell asleep thinking about the
nature of fields in general and whether
nature of fields in general and whether
fields can be described by particles.
fields can be described by particles.
um
um
whether it's a model or whether it's an
whether it's a model or whether it's an
actual phenomena, I don't know.
I'm having a good time kind of on the
I'm having a good time kind of on the
side here. Um,
I'm having a good time on this side here
I'm having a good time on this side here
just kind of like going through
just kind of like going through
different areas hard science like
different areas hard science like
chemistry, physics, trying to get like a
chemistry, physics, trying to get like a
low-level understanding of different
low-level understanding of different
areas of science.
areas of science.
Mainly I'm looking to like
Mainly I'm looking to like
figure out enough of uh the fundamentals
figure out enough of uh the fundamentals
here to figure out like at what level of
here to figure out like at what level of
simulation what levels of simulation are
simulation what levels of simulation are
feasible.
So yeah, we need all these hypers
and this should actually give us a
and this should actually give us a
pretty cool to look at policy within a a
pretty cool to look at policy within a a
few minutes of training it.
And I will want to what I'll do is I'll
And I will want to what I'll do is I'll
take these hypers and then I'll go
take these hypers and then I'll go
compare these to the neural MMO ones.
compare these to the neural MMO ones.
Um, basically I just want to figure out
Um, basically I just want to figure out
if these look sane to me
and then we'll also look at the
and then we'll also look at the
behaviors that are learned.
Like this gamma is super weird.
Like this gamma is super weird.
The fact that like you have a gamma
The fact that like you have a gamma
that's that ludicrously high
like that doesn't sit right with me.
like that doesn't sit right with me.
Like lambda is reasonable but gamma is
Like lambda is reasonable but gamma is
way higher than I would expect for this
way higher than I would expect for this
problem.
It's actually somewhat disturbing that
It's actually somewhat disturbing that
despite all the progress on 3 0 and like
despite all the progress on 3 0 and like
despite all the progress that we've made
despite all the progress that we've made
with making reinforcement learning
with making reinforcement learning
pretty sane and reproducible
pretty sane and reproducible
um we know what the hyperparameters do
um we know what the hyperparameters do
and the experimental data does not fit
and the experimental data does not fit
the expectations of literally anyone in
the expectations of literally anyone in
the field.
the field.
That is somewhat concerning.
And actually, this is like something
And actually, this is like something
that's probably pretty difficult for new
that's probably pretty difficult for new
people in RL is like, you know, they'll
people in RL is like, you know, they'll
ask me tons of questions and most of the
ask me tons of questions and most of the
time I can answer stuff, but then the
time I can answer stuff, but then the
thing is like you don't even know what
thing is like you don't even know what
questions have answers to them
questions have answers to them
basically. Um, coming into reinforcement
basically. Um, coming into reinforcement
learning, like there's just a lot of
learning, like there's just a lot of
stuff where it's like, yeah, we have
stuff where it's like, yeah, we have
some idea. we have predictions of how uh
some idea. we have predictions of how uh
this should work but in practice it
this should work but in practice it
doesn't work that way at all and nobody
doesn't work that way at all and nobody
knows why and there just tons and tons
knows why and there just tons and tons
and tons of things like that in RL
and tons of things like that in RL
and you hit it very quickly as well you
and you hit it very quickly as well you
know like
know like
if you're studying physics or you're
if you're studying physics or you're
studying chemistry it sometimes it can
studying chemistry it sometimes it can
take you a while to like come up with
take you a while to like come up with
questions um that like nobody knows the
questions um that like nobody knows the
answer to or like is not related to some
answer to or like is not related to some
other field of study. In reinforcement
other field of study. In reinforcement
learning, you can very very quickly run
learning, you can very very quickly run
into questions where it's like, yeah,
into questions where it's like, yeah,
nobody knows
and just nobody knows at all. Good luck.
Like, well, that's silly. Surely we
Like, well, that's silly. Surely we
should just be able to answer that. And
should just be able to answer that. And
then you go spend two months running
then you go spend two months running
experiments and you're like, okay, yeah,
experiments and you're like, okay, yeah,
nobody knows
is ridiculous. But that is the sad state
is ridiculous. But that is the sad state
of the field.
Of course, we've made some major
Of course, we've made some major
improvements to that.
Now we're at like all right, we know it
Now we're at like all right, we know it
works. We don't know exactly precisely
works. We don't know exactly precisely
why or how, but at least it does work.
why or how, but at least it does work.
Whereas before it was like sometimes it
Whereas before it was like sometimes it
works maybe.
No cuda jeep. Lovely.
Some reason cuda likes to detach from
Some reason cuda likes to detach from
Docker.
All right. So, that's like a three
All right. So, that's like a three
minute experiment tops.
We'll see if this repros as it should.
But if this does repro, then we actually
But if this does repro, then we actually
should be able to look at the policy,
should be able to look at the policy,
which I don't think I did before.
which I don't think I did before.
I don't know why I didn't bother doing
I don't know why I didn't bother doing
that, but that's like the super obvious
that, but that's like the super obvious
thing, right?
thing, right?
Look at the policy.
What the heck?
Huh?
Something's off here completely.
This does not work period like at all.
This does not work period like at all.
So,
So,
first thing to check is that I messed up
first thing to check is that I messed up
a hyper maybe.
Very likely I mess up a hyper.
Very likely I mess up a hyper.
I really should not be copying these
I really should not be copying these
things manually.
things manually.
Kind of bad practice and I should
Kind of bad practice and I should
probably just have a way to do it
probably just have a way to do it
automatically.
automatically.
This is 108 mil.
Got Veum M. Oh, you know what it is?
Got Veum M. Oh, you know what it is?
It's just the reward coefficients.
It's just the reward coefficients.
Now, that's easy. I can fix that.
Now, that's easy. I can fix that.
I'd forgotten that I'd swept these as
I'd forgotten that I'd swept these as
well. So, you actually have um
well. So, you actually have um
This is one of the things that we do
This is one of the things that we do
that uh almost nobody does. And it's
that uh almost nobody does. And it's
like this is just straight incompetence
like this is just straight incompetence
that nobody does it.
that nobody does it.
At least in
At least in
that's not being entirely fair, I guess,
that's not being entirely fair, I guess,
because the thing is they don't have our
because the thing is they don't have our
tools. So, it's actually quite difficult
tools. So, it's actually quite difficult
to do without our tools. But it's like
to do without our tools. But it's like
it's one of those things where there's
it's one of those things where there's
really no excuse for not doing it
really no excuse for not doing it
because it's your stuff just won't work.
because it's your stuff just won't work.
Like it's like ah it's hard so we won't
Like it's like ah it's hard so we won't
do it. But like you just if you don't
do it. But like you just if you don't
sweep reward coefficients and complex
sweep reward coefficients and complex
Ms. Your stuff just won't work.
Let's see if this does better. Welcome
Let's see if this does better. Welcome
YouTube folks.
dead
plan for today is going to be meta
plan for today is going to be meta
policies. Uh we figure out we're going
policies. Uh we figure out we're going
to pull these up once I actually get
to pull these up once I actually get
this to repro. We will see what that
this to repro. We will see what that
looks like. We'll see whether we think
looks like. We'll see whether we think
it can learn much better or whether this
it can learn much better or whether this
copy of the end is just solved.
copy of the end is just solved.
We'll send that over to those guys. They
We'll send that over to those guys. They
have some other stuff for me to do as
have some other stuff for me to do as
well, but at least we're going to get
well, but at least we're going to get
this to them today.
this to them today.
And I do want to understand why our
And I do want to understand why our
stuff just does not work on these new
stuff just does not work on these new
robotics ends.
robotics ends.
Could be the policy, could be the setup,
Could be the policy, could be the setup,
the end finding, could be a number of
the end finding, could be a number of
things, but we're going to try to figure
things, but we're going to try to figure
that out today.
Welcome boxing.
Welcome boxing.
Finally have something that somehow
Finally have something that somehow
trains.
trains.
Does it solve like the comments problem?
like you actually have something that
like you actually have something that
solves the commons problem.
I'd actually be very interested if that
I'd actually be very interested if that
worked
cuz like humans usually fail that On.
metric is percentage of agents that
metric is percentage of agents that
manage to make it out alive at the end.
manage to make it out alive at the end.
Get 40% surviving agents.
Have you actually watched what the
Have you actually watched what the
policy does?
Definitely some improvements. Cool.
Definitely some improvements. Cool.
Oh, we're at
This is a pretty different learning
This is a pretty different learning
curve than I was getting before. Now,
why is this lower?
why is this lower?
Did I miss something again or is it
Did I miss something again or is it
lucky?
You stopped rushing the food and somehow
You stopped rushing the food and somehow
wait before clicking.
wait before clicking.
Also use the new puffers stars.
That is called pandering and it does
That is called pandering and it does
work.
It just it amuses me to no end. We have
It just it amuses me to no end. We have
the ends. They're just all the puffers.
So funny.
So funny.
Yeah. So, this only gets like three.
Can we not just like
Just compare
that. These are always matching the
that. These are always matching the
green one.
These all match.
Unless I miss something else.
You're the one who asked me for the
You're the one who asked me for the
stars.
stars.
Yes, I like I said, that does work. It
Yes, I like I said, that does work. It
does amuse me very much.
Better logs is what? Yeah, it's always,
Better logs is what? Yeah, it's always,
man, it's always it's always a data
man, it's always it's always a data
problem of some type. Almost always. And
problem of some type. Almost always. And
you just you need good logs. And good
you just you need good logs. And good
logs is not the same as log everything,
logs is not the same as log everything,
right? Like this project I'm working on
right? Like this project I'm working on
here, they've got hundreds of logs and
here, they've got hundreds of logs and
it's just it's too much. You really you
it's just it's too much. You really you
just need to be tracking like key
just need to be tracking like key
metrics that actually matter and you
metrics that actually matter and you
just need to think about them a little
just need to think about them a little
bit.
If you do that, you'll pretty much
If you do that, you'll pretty much
always be in a good spot.
build to run 257 experiments.
Not there.
Not there.
TPU has been
TPU has been
Yeah,
Yeah,
TPUs are pretty useful, it turns out.
Yeah, that's just straight up not going
Yeah, that's just straight up not going
to be it, man. If you find yourself
to be it, man. If you find yourself
deving a new environment and then trying
deving a new environment and then trying
to do new algorithm research at the same
to do new algorithm research at the same
time because it doesn't work, you're
time because it doesn't work, you're
almost always wrong.
almost always wrong.
Almost always wrong.
Almost always wrong.
In fact, there is actually um it was
In fact, there is actually um it was
Wooden added uh a research M
Wooden added uh a research M
specifically for testing DM.
specifically for testing DM.
You want to mess with that.
It's like a very simple environment
It's like a very simple environment
where we should be able to test with and
where we should be able to test with and
without and like actually get some
without and like actually get some
results.
I think it's convert circle,
I think it's convert circle,
isn't it? The grid. No, grid is the maze
isn't it? The grid. No, grid is the maze
env. It's similar. Well, it's similar to
env. It's similar. Well, it's similar to
convert. So, basically, I got him to do
I got him to make like this N except the
I got him to make like this N except the
uh the targets are in uh a circle. So,
uh the targets are in uh a circle. So,
it's like the idea is that
it's like the idea is that
where did he put it somewhere?
Maybe he put it somewhere.
Oh.
Not telling me I'm forgetting meeting
to read an article. Fine.
there's any images in uh
Oh, yeah. So, here,
Oh, yeah. So, here,
for some reason, this is not expanding
for some reason, this is not expanding
correctly, but you can kind of see the
correctly, but you can kind of see the
stars are in a circle. So, the goal is
stars are in a circle. So, the goal is
going to be we spawn all the puffers at
going to be we spawn all the puffers at
the center and they're going to have to
the center and they're going to have to
go to a star
go to a star
and we'll basically just see if they
and we'll basically just see if they
always go to the same one or not.
Spencer's article.
Yeah, that's a good image.
Good article other than the sounds.
Oh, okay. Yeah.
Oh, okay. Yeah.
Uh, I don't know what was happening
Uh, I don't know what was happening
before, but uh, yeah, guys, I think we
before, but uh, yeah, guys, I think we
got it to work. Yeah. Okay.
I love it when that happens.
trying to remember what the hell all the
trying to remember what the hell all the
commands were that I had in order to uh
commands were that I had in order to uh
start all the servers and [ __ ]
start all the servers and [ __ ]
things.
things.
Uh
probably Yes.
Okay.
Okay.
We have to do eval with this. And then
We have to do eval with this. And then
we have to turn on the
send them this replay as soon as we make
send them this replay as soon as we make
sure that it's any Good.
I got to love how the play button
I got to love how the play button
doesn't work, but the scroll button
doesn't work, but the scroll button
does.
This guy's got six batteries
This guy's got six batteries
and two ore. So, he's going to make two
and two ore. So, he's going to make two
hearts out of this, right?
The replayer is kind of glitchy, but
so much header. Just the ray viewer,
isn't Oh, isn't it? Yeah, that's an old
isn't Oh, isn't it? Yeah, that's an old
message. Cool.
YouTube chat doesn't seem to be working
YouTube chat doesn't seem to be working
really.
Uh, that would be bizarre.
Oh,
Oh,
hi Brad.
hi Brad.
Um, yeah, Sakana's cool. I don't know
Um, yeah, Sakana's cool. I don't know
why my reream chat is not uh
why my reream chat is not uh
broadcasting that. That is weird. Hang
broadcasting that. That is weird. Hang
on. Did I pop out the wrong chat or
on. Did I pop out the wrong chat or
something?
No, I've had this exact same chat. Oh,
No, I've had this exact same chat. Oh,
hi Brad.
hi Brad.
Um, yeah.
Um, yeah.
I don't know why my chat is not.
I don't know why my chat is not.
Yeah, just Oh, no. Now it shows up.
Yeah, just Oh, no. Now it shows up.
wrong chat or something. Does it show up
wrong chat or something. Does it show up
on the screen? It doesn't. I've had this
on the screen? It doesn't. I've had this
exact same chat. Wait, it does now.
I didn't even do anything and it shows
I didn't even do anything and it shows
up on both windows.
up on both windows.
Can one of you write something? See if
Can one of you write something? See if
it works for non-adins.
it works for non-adins.
Oh, that's probably horrendous with the
Oh, that's probably horrendous with the
double audio.
double audio.
My bad.
have done and it doesn't. Oh, there it
have done and it doesn't. Oh, there it
goes. Yeah, now I see it. I don't know
goes. Yeah, now I see it. I don't know
what that was, man. That's just reream
what that was, man. That's just reream
or YouTube being weird. All right.
or YouTube being weird. All right.
Hopefully double audio bug is fixed. I
Hopefully double audio bug is fixed. I
was wondering why there was nobody in
was wondering why there was nobody in
the chat.
the chat.
Hey, An's
Hey, An's
still here. Hello.
Yeah. So, Sakana's cool. Sakana's David
Yeah. So, Sakana's cool. Sakana's David
Ha's thing. I have a colleague that does
Ha's thing. I have a colleague that does
well that did some work with them. Uh,
well that did some work with them. Uh,
I've interacted with David Ha a few
I've interacted with David Ha a few
times. I saw him at Nurups last year as
times. I saw him at Nurups last year as
well. Uh, they do cool stuff. David Ha's
well. Uh, they do cool stuff. David Ha's
always kind of been like one of the cool
always kind of been like one of the cool
like has crazy out there ideas guys.
like has crazy out there ideas guys.
He's actually I think among the like
He's actually I think among the like
crazy ideas people. He's been one of the
crazy ideas people. He's been one of the
ones who's actually like gotten them to
ones who's actually like gotten them to
work a bit more consistently.
work a bit more consistently.
So,
So,
and it seems like a lot of their stuff
and it seems like a lot of their stuff
they're doing at the moment is also just
they're doing at the moment is also just
like they're one just trying to do crazy
like they're one just trying to do crazy
out there research that isn't the norm,
out there research that isn't the norm,
which is awesome. And then B is just
which is awesome. And then B is just
like, you know, they're also applying
like, you know, they're also applying
doesn't have to be super super
doesn't have to be super super
state-of-the-art, but pretty good uh
state-of-the-art, but pretty good uh
modern AI stuff just to all of the crap
modern AI stuff just to all of the crap
in Japanese government. So, they're
in Japanese government. So, they're
trying to just help streamline stuff.
trying to just help streamline stuff.
It's like a national lab,
It's like a national lab,
which is cool.
All right. So, this is going to run for
All right. So, this is going to run for
quite a while here.
quite a while here.
Let me just send these meta guys this
Let me just send these meta guys this
replay.
added 2048 with a few changes. Could you
added 2048 with a few changes. Could you
do another review? Yep. Give me one
do another review? Yep. Give me one
minute and I will get right on that.
actually gotten it to solve 2048.
actually gotten it to solve 2048.
I didn't manage out of the box with any
I didn't manage out of the box with any
other RL repo.
We do try to make good stuff here, man.
We do try to make good stuff here, man.
I think there's a lot of improvements to
I think there's a lot of improvements to
be made, but pretty exciting.
be made, but pretty exciting.
Yeah, that's cool. Let me see if we can
Yeah, that's cool. Let me see if we can
even do better. Let's we'll take a quick
even do better. Let's we'll take a quick
look. Let me use a restroom real quick.
look. Let me use a restroom real quick.
Grab another cup of coffee so I'm fresh
Grab another cup of coffee so I'm fresh
and then we'll uh we'll get on that. I
and then we'll uh we'll get on that. I
just figured out the meta stuff. I think
just figured out the meta stuff. I think
that's pretty solid. I to give you an
that's pretty solid. I to give you an
idea, they've had like they have a big
idea, they've had like they have a big
team working on this thing and uh we
team working on this thing and uh we
just set out with like a model we
just set out with like a model we
trained in 10 minutes. So, heck yeah. Be
trained in 10 minutes. So, heck yeah. Be
right back, Anick. And then you'll get
right back, Anick. And then you'll get
your review.
Yes. And uh about the morning deadlift.
Okay, let's get to uh
Okay, let's get to uh
2048.
Oh, and let me reply to Kyle real quick.
Oh, and let me reply to Kyle real quick.
do that.
What I found weird is that with RNN
What I found weird is that with RNN
enabled, it worked much better. Even
enabled, it worked much better. Even
though the end is marovian,
though the end is marovian,
maybe the normal policy has too few
maybe the normal policy has too few
parameters.
parameters.
Uh I it it adds some parameters and it
Uh I it it adds some parameters and it
adds like a reasonable layer structure.
adds like a reasonable layer structure.
We just throw it on by default because
We just throw it on by default because
like our RNN implementation is fast. So
like our RNN implementation is fast. So
like we don't really care. we just use
like we don't really care. we just use
it by default. Plus, it can let the
it by default. Plus, it can let the
agent like it gives the agent working
agent like it gives the agent working
memory in case it wants to use it for
memory in case it wants to use it for
anything.
Like most people, the reason that most
Like most people, the reason that most
people don't throw LSTMs on everything
people don't throw LSTMs on everything
is because their implementations are
is because their implementations are
bad. That's it. That's the only reason.
bad. That's it. That's the only reason.
Like if you actually look at the flops,
Like if you actually look at the flops,
it's like a perfectly reasonable thing
it's like a perfectly reasonable thing
to just chuck in as a layer. LSTMs are
to just chuck in as a layer. LSTMs are
incredibly fast. Like you can train a
incredibly fast. Like you can train a
1024 dim LSTM at a million steps per
1024 dim LSTM at a million steps per
second. Like they're really really fast.
second. Like they're really really fast.
But the thing is in most implementations
But the thing is in most implementations
they're very very slow. And it's purely
they're very very slow. And it's purely
because people suck at coding. That's
because people suck at coding. That's
it. There's no fundamental problem with
it. There's no fundamental problem with
them.
Okay. Where's 2048 here?
Okay. Where's 2048 here?
Added 2048 ocean ends.
All changes should be good now.
Got some hypers here.
Is this meant to be two?
Two billion.
Two billion.
Two billion.
Huh. That's a surprisingly large number
Huh. That's a surprisingly large number
of steps to have to train 2048.
of steps to have to train 2048.
Did you get these from a hyper pram
Did you get these from a hyper pram
sweep? Like a full sweep?
sweep? Like a full sweep?
I'd be surprised if it takes two billion
I'd be surprised if it takes two billion
uh steps to solve 2048.
values for any are just my working
values for any are just my working
stuff.
stuff.
Not sure if these are the newest prams.
Not sure if these are the newest prams.
Gotcha.
copied them from another end. Yeah. So,
copied them from another end. Yeah. So,
you don't want to do that. You want to
you don't want to do that. You want to
run a sweep. I'll help you though. I'll
run a sweep. I'll help you though. I'll
help you. I'll help you do that.
help you. I'll help you do that.
Do you have a GPU to run stuff on or no?
Do you have a GPU to run stuff on or no?
Because if not, I can just set something
Because if not, I can just set something
up for you.
I'm supposed to drink the mug left like
I'm supposed to drink the mug left like
this left-handed the MIT mug.
Everyone always comments on the dumb
Everyone always comments on the dumb
mugs.
My thing is when you're a researcher you
My thing is when you're a researcher you
get a lot of mugs. Like all the
get a lot of mugs. Like all the
conferences give mugs out, universities
conferences give mugs out, universities
give mugs out. Like all they like they
give mugs out. Like all they like they
always just give mugs. They know that
always just give mugs. They know that
all you do is drink coffee,
all you do is drink coffee,
right? Code
[Music]
B 2048
work. So, I run it over the weekend.
I can help you set something up
I can help you set something up
initially.
initially.
We do give uh repeat contributors
We do give uh repeat contributors
hardware access.
hardware access.
I need to buy some more desktops to be
I need to buy some more desktops to be
fair, but you have some capacity.
Is this thing superhuman on Tetris? I
Is this thing superhuman on Tetris? I
think it's a little better than I am,
think it's a little better than I am,
but I don't think it's super human. I
but I don't think it's super human. I
mean, we like to be fair, we have not
mean, we like to be fair, we have not
tried we haven't really tried very hard
tried we haven't really tried very hard
on any of the individual arcade ms. Like
on any of the individual arcade ms. Like
you could run it for longer and it would
you could run it for longer and it would
keep getting better most likely, you
keep getting better most likely, you
know, or you could train a bigger policy
know, or you could train a bigger policy
and it would keep getting better.
and it would keep getting better.
It's pretty cool to watch though, right?
It's pretty cool to watch though, right?
It's like a pretty mesmerizing demo to
It's like a pretty mesmerizing demo to
watch.
watch.
I really like this environment. It came
I really like this environment. It came
out very, very well,
out very, very well,
right?
right?
This is just fun to watch.
It's definitely a heck of a lot faster
It's definitely a heck of a lot faster
than uh most humans are. I think maybe
than uh most humans are. I think maybe
like the top top players are a bit
like the top top players are a bit
faster than this,
faster than this,
but like it's pretty cool.
Green screen save a background to
Green screen save a background to
website. I mean you can technically do
website. I mean you can technically do
that like
that like
this. The website itself is open source
this. The website itself is open source
and this is literally just um embedded
and this is literally just um embedded
web assembly.
So it's like actually very easy to load
So it's like actually very easy to load
this stuff if you just go there's a a
this stuff if you just go there's a a
repo that's instead of puffer lib it's
repo that's instead of puffer lib it's
puffer.ai as a repo and it just hosts
puffer.ai as a repo and it just hosts
everything.
everything.
So you're free to embed it as long as
So you're free to embed it as long as
you uh you know credit the original like
you uh you know credit the original like
you know from puffer liib
you know from puffer liib
or from puffer.ai.
I believe that was Adrian's uh one of
I believe that was Adrian's uh one of
Adrian's ends as well. Adrien should be
Adrian's ends as well. Adrien should be
credited somewhere there.
credited somewhere there.
Yeah. Bye, Adrian.
This is theh.
Let me make sure there aren't any errors
Let me make sure there aren't any errors
in here.
in here.
Here's some bite and magic three. The
Here's some bite and magic three. The
current AI is trash. Yeah. Yeah. So, the
current AI is trash. Yeah. Yeah. So, the
difficult part of training it on um like
difficult part of training it on um like
real games is not even that the problems
real games is not even that the problems
are hard, it's that the games are slow.
are hard, it's that the games are slow.
So, like we can pretty much do it on any
So, like we can pretty much do it on any
individual game we want. It just gets
individual game we want. It just gets
expensive.
Theoretically, my lab also has
Theoretically, my lab also has
resources, a few A10s. That's painful. I
resources, a few A10s. That's painful. I
mean, we have Can you see this wall of
mean, we have Can you see this wall of
servers behind me? We technically we've
servers behind me? We technically we've
got 20 4090s that we can use for uh
got 20 4090s that we can use for uh
trading stuff.
This is one of the nice things that
This is one of the nice things that
we're finally in a pretty good spot
we're finally in a pretty good spot
with.
with.
See is a little buggy. Not too concerned
See is a little buggy. Not too concerned
about that. I'm just concerned about
about that. I'm just concerned about
making sure there's no major errors in
making sure there's no major errors in
here.
here.
H unsigned char int float
H unsigned char int float
r
Huh?
4090 better than an A10? Yes, hugely
for reinforcement learning. If you're
for reinforcement learning. If you're
not using a Bflat 16, uh a 4090 is
not using a Bflat 16, uh a 4090 is
basically the same speed as an 800.
basically the same speed as an 800.
which is crazy because the 800 is more
which is crazy because the 800 is more
than 10 times the price,
than 10 times the price,
but it's true. This is the arbitrage,
but it's true. This is the arbitrage,
right? This is why I don't rent our
right? This is why I don't rent our
hardware. It's because we have to rent
hardware. It's because we have to rent
10 times more expensive chips to match
10 times more expensive chips to match
this
reward engineering and say 100 million
reward engineering and say 100 million
steps.
Uh sometimes you can get something
Uh sometimes you can get something
decent in like a 100 million frames or
decent in like a 100 million frames or
whatever. Uh there's like a lot of work
whatever. Uh there's like a lot of work
on I mean people do stuff like that on
on I mean people do stuff like that on
relatively slower environments. The
relatively slower environments. The
thing that's just tricky two weeks is um
thing that's just tricky two weeks is um
you still have to do like the
you still have to do like the
development work, right? Where like
development work, right? Where like
ideally to get the like the fastest
ideally to get the like the fastest
quickest training result, you want to
quickest training result, you want to
run a hyper pram sweep and then that's
run a hyper pram sweep and then that's
like like many times more expensive than
like like many times more expensive than
an actual single training run.
an actual single training run.
And then also there's the thing where
And then also there's the thing where
usually you get CPUbound, not GPU bound.
usually you get CPUbound, not GPU bound.
like even though you have your nice
like even though you have your nice
fancy GPU, you're actually spending like
fancy GPU, you're actually spending like
99% time on the CPU. So it's like it's
99% time on the CPU. So it's like it's
not that there are any fundamental
not that there are any fundamental
barriers, right? We are solving harder
barriers, right? We are solving harder
problems in many cases uh than real
problems in many cases uh than real
games. It's just that the games are
games. It's just that the games are
slow, but you can think of it as like a
slow, but you can think of it as like a
cost and an infrastructure premium more
cost and an infrastructure premium more
than anything.
than anything.
Only because you don't need the memory.
Only because you don't need the memory.
Yep. Don't need the memory.
Yep. Don't need the memory.
We literally keep our entire data back
We literally keep our entire data back
on the GPU and we're still not even
on the GPU and we're still not even
close to running out of memory.
close to running out of memory.
Even with the bigger models,
transformers are just like obnoxiously
transformers are just like obnoxiously
memory inefficient. Why everyone needs
memory inefficient. Why everyone needs
that much memory. Hyper we forgot. Yeah,
that much memory. Hyper we forgot. Yeah,
you can technically run default hypers,
you can technically run default hypers,
but the thing is like it's not great.
but the thing is like it's not great.
Like we're getting better at it. We're
Like we're getting better at it. We're
like now more of our hypers work like at
like now more of our hypers work like at
least kind of okay out of the box on
least kind of okay out of the box on
most problems. Um it's not ideal though,
most problems. Um it's not ideal though,
right? Which is why we do all this
right? Which is why we do all this
research on really fast environments,
research on really fast environments,
right? Like the more we can advance
right? Like the more we can advance
research while going really fast, right?
research while going really fast, right?
The fewer experiments and fiddly bits
The fewer experiments and fiddly bits
have to be involved when we actually
have to be involved when we actually
want to take it and throw it on any
want to take it and throw it on any
fixed problem where stuff is slower.
fixed problem where stuff is slower.
which is why all this research is really
which is why all this research is really
useful. And uh yeah, you want to
useful. And uh yeah, you want to
contribute to it, we're right here,
Yanick, do you zero out the observations
Yanick, do you zero out the observations
anywhere?
Wait, one hot and coated.
Wait, one hot and coated.
This is in China that can send you.
This is in China that can send you.
Yeah, that is a thing.
You probably burn your building down
You probably burn your building down
though,
though,
for all I know.
Okay, so here you are.
Okay, so here you are.
So this is not a one hot encoding.
But Yan, this is not a one hot encoding,
But Yan, this is not a one hot encoding,
right?
This is you're setting the observation
This is you're setting the observation
at a position.
Okay. But then let's see. Do you have a
Okay. But then let's see. Do you have a
custom policy?
custom policy?
You should if you're going to do it this
You should if you're going to do it this
way.
Not log two. Yeah. Okay.
Not log two. Yeah. Okay.
Oh, it's now log. Okay.
Yes. mem copy would be faster in many
Yes. mem copy would be faster in many
cases. Um, I'm I'm trying to make sure
cases. Um, I'm I'm trying to make sure
he has the ends right.
There's a few things I'm worried about
There's a few things I'm worried about
here.
Okay. So, you do have a custom No, you
Okay. So, you do have a custom No, you
don't have a Wait,
don't have a Wait,
that's just binding. Okay, you do not
that's just binding. Okay, you do not
have a custom policy.
Once upon a time I was a system engineer
Once upon a time I was a system engineer
for Microsoft Xbox 360. Systems people
for Microsoft Xbox 360. Systems people
will do very very well in uh
will do very very well in uh
reinforcement learning the way we do
reinforcement learning the way we do
stuff in pop for lip. So I definitely
stuff in pop for lip. So I definitely
encourage you to come try some stuff out
encourage you to come try some stuff out
with us here. Like we basically just
with us here. Like we basically just
write really really basic C in a way
write really really basic C in a way
that makes stuff good for uh
that makes stuff good for uh
reinforcement learning like everything's
reinforcement learning like everything's
written into shared memory like no
written into shared memory like no
dynamic allocations and stuff like that.
Let me think about the policy for you.
We probably need to make you a con
We probably need to make you a con
architecture.
Maybe not.
There's no vibe coding in Puffer.
Well, you definitely need a um a data
Well, you definitely need a um a data
norm
norm
at the very least.
at the very least.
What's the biggest number in
What's the biggest number in
observation?
Wait, required flattened is
this is not updated.
16 tiles.
Tape is 4x4
11 for 2048.
Okay.
Trying to think if it makes sense to one
Trying to think if it makes sense to one
hot or what.
2048 solidly.
2048 solidly.
Let me merge this and let me I'm going
Let me merge this and let me I'm going
to merge this and I'm going to start
to merge this and I'm going to start
doing some work on this live with you.
doing some work on this live with you.
All right.
Thinking how we want to do this.
Oh yeah. So this is what I saw before
Oh yeah. So this is what I saw before
where it like didn't
where it like didn't
very odd
learning rate of healing.
Let it chill.
Let it chill.
One hot game state sounds right. Kind
One hot game state sounds right. Kind
of. It's a little weird though cuz like
of. It's a little weird though cuz like
you kind of have to like relearn you
you kind of have to like relearn you
have to sort of like relearn
have to sort of like relearn
uh behaviors for every separate number.
uh behaviors for every separate number.
It's kind of like halfway between a one
It's kind of like halfway between a one
hot and not
hot and not
right.
Well, you don't want to do transformer.
Well, you don't want to do transformer.
They're just slow.
They're just slow.
They're kind of just slow.
They're kind of just slow.
I mean, I at least know that I want to
I mean, I at least know that I want to
normalize your observations, right?
Is there a reason that it's prefixed
Is there a reason that it's prefixed
with a G
MLP?
MLP?
It's not deep enough to need residuals
It's not deep enough to need residuals
like our nets and our RL networks are
like our nets and our RL networks are
usually very shallow.
I think your classes are not allowed to
I think your classes are not allowed to
be called 20 like the
be called 20 like the
that get
that get
exponentially harder.
exponentially harder.
I don't know if it gets exponentially
I don't know if it gets exponentially
harder. It gets exponentially longer,
harder. It gets exponentially longer,
right?
right?
Can't start the end of name with a
Can't start the end of name with a
number.
Could do puff 2048. I don't know.
Do I have to move this?
Okay. So, something's great here.
Upper puff. Yeah, that'd be weird.
We want it to be like puffer_2048,
We want it to be like puffer_2048,
however that has to happen. I think
however that has to happen. I think
there is a name mapping thing that we
there is a name mapping thing that we
can do.
So, yeah. What's up with this C file? Is
So, yeah. What's up with this C file? Is
this supposed to compile?
All right. See if you can get me a patch
All right. See if you can get me a patch
for that while I attempt to RL this.
I think you said you're at work, so no
I think you said you're at work, so no
big deal. Do it later.
over a weekend is
Something's weird with this thing,
Something's weird with this thing,
right?
right?
Buffer eval. Okay, I can do that.
Buffer eval. Okay, I can do that.
Wait, we have return and we have score.
Neither of these are good.
Interesting.
Recommend Neptune over W to be. Um,
Recommend Neptune over W to be. Um,
so at the moment, yes, Neptune is better
so at the moment, yes, Neptune is better
than Wbe. However, I've been told that
than Wbe. However, I've been told that
they're attempting to kamicazi their
they're attempting to kamicazi their
product later this year.
product later this year.
Um, I don't know what the hell they're
Um, I don't know what the hell they're
doing, but for now it's There.
Oh, wait. Now is score stable once
Oh, wait. Now is score stable once
again.
Yeah, it looks like it is
This is cheeky.
This is cheeky.
Eight minute experiment for two billion
Eight minute experiment for two billion
steps.
steps.
So ran into sometimes ran into nanss
So ran into sometimes ran into nanss
when training. I was stress probably and
when training. I was stress probably and
this can be a number of things. Nanss
this can be a number of things. Nanss
are numerical issues mostly.
are numerical issues mostly.
Do you know what score is solved?
Do you know what score is solved?
What problem you're trying to solve?
What problem you're trying to solve?
This is 2048.
So Yanick, we usually PF. We do Perf is
So Yanick, we usually PF. We do Perf is
like a zero to one normalized score.
like a zero to one normalized score.
You know what solved?
You know what solved?
Oh, 2048 is the best score.
Oh, 2048 is the best score.
Wait, should the score be 2048
to solve it?
Is that how it works?
Is that how it works?
Greater than or equal to 2048. Okay.
Greater than or equal to 2048. Okay.
So, yeah, it looks like you just have
So, yeah, it looks like you just have
some unstable
some unstable
learning dynamics.
Take a look.
So Perf in your case would just be score
So Perf in your case would just be score
divided by 2048 because one is solved.
divided by 2048 because one is solved.
Can you clip activations or loss? Is the
Can you clip activations or loss? Is the
reward normalized?
reward normalized?
Uh we already have several different
Uh we already have several different
forms of clipping in place. Yes. And the
forms of clipping in place. Yes. And the
reward is normalized, but his
reward is normalized, but his
observations are not yet normalized,
observations are not yet normalized,
which is probably an issue.
Okay. That's immediately better.
That's with the defaults.
I'm on the side on Discord here. I'm
I'm on the side on Discord here. I'm
driving Spencer crazy with uh
driving Spencer crazy with uh
edits on his article.
edits on his article.
Observations are in charge. Whoops.
Observations are in charge. Whoops.
Thank you.
Let's uh Yep. Wondering what I screwed
Let's uh Yep. Wondering what I screwed
up there.
All right, buckle up. We're going to
All right, buckle up. We're going to
show you how to write a custom policy.
Good.
Copy this thing. put this here G48
uh we'll start with
uh we'll start with
this this
encoder is going to be
encoder is going to be
11 times this
11 times this
we'll start with just like This
we'll start with just like This
This is a fairly vanilla network here.
Why do you prefer yellow?
Why do you prefer yellow?
Uh mild empirical benefit.
Uh mild empirical benefit.
I I don't have huge I don't have
I I don't have huge I don't have
tremendous confidence in it, but in like
tremendous confidence in it, but in like
I did some experiment and it seemed to
I did some experiment and it seemed to
be a little bit better a while ago.
be a little bit better a while ago.
It's been fine.
All right, everybody go uh repost. I'm
All right, everybody go uh repost. I'm
gonna put this in the chat.
gonna put this in the chat.
Go help out Spencer if you're on X cuz
Go help out Spencer if you're on X cuz
he just posted the uh Terraform article.
he just posted the uh Terraform article.
I worked on this with uh this end with
I worked on this with uh this end with
him for a bit.
It was re before. Yes.
Is there curriculum learning in Pupper?
Is there curriculum learning in Pupper?
Uh yes, technically we do have some we
Uh yes, technically we do have some we
have some research we've done uh Aaron
have some research we've done uh Aaron
specifically and we have a version. It's
specifically and we have a version. It's
not like very well integrated but it is
not like very well integrated but it is
there and it's quite simple.
there and it's quite simple.
And then we also have several
And then we also have several
environments where we've like manually
environments where we've like manually
done some form specific to to that one
done some form specific to to that one
problem.
Generally we have a fair few curriculum
Generally we have a fair few curriculum
learning people around here though.
Myself, Ryan, and Aaron have all done uh
Myself, Ryan, and Aaron have all done uh
curriculum learning research at some
curriculum learning research at some
point.
the optimal hypers drift over time.
the optimal hypers drift over time.
Uh
Uh
you can't re sweep every hundred billion
you can't re sweep every hundred billion
like that with how expensive stuff is.
like that with how expensive stuff is.
Uh there are definitely some components
Uh there are definitely some components
that need to be tweaked though. Let's
that need to be tweaked though. Let's
say so Finn in particular gamma I think
say so Finn in particular gamma I think
that the biggest culprit is um gamma and
that the biggest culprit is um gamma and
lambda mostly gamma really not just
lambda mostly gamma really not just
lambda
lambda
cuz gamma here hang on let me finish
cuz gamma here hang on let me finish
this
Nope.
Good articles.
Good articles.
So yeah, Finn. Um I
The thing is that gamma tells you this
The thing is that gamma tells you this
is how much you care about a dollar
is how much you care about a dollar
today versus a dollar tomorrow. And like
today versus a dollar tomorrow. And like
credit assignment doesn't work on a
credit assignment doesn't work on a
fixed temporal hierarchy like that. And
fixed temporal hierarchy like that. And
gamma tends to be like the really weird
gamma tends to be like the really weird
and sensitive and janky hyperparameter
and sensitive and janky hyperparameter
that tends to vary a lot and sometimes
that tends to vary a lot and sometimes
needs to be annealed in other things.
needs to be annealed in other things.
Uh, so yes, like that specific PR. But
Uh, so yes, like that specific PR. But
the thing is like that's kind of a
the thing is like that's kind of a
band-aid patch is just like what if you
band-aid patch is just like what if you
reweep it because you have to like do so
reweep it because you have to like do so
many more experiments. Ideally, we
many more experiments. Ideally, we
should just come up with something that
should just come up with something that
doesn't have that problem at an
doesn't have that problem at an
algorithmic level.
That's hard. Like that's hard
That's hard. Like that's hard
algorithmic work. I tried uh the one
algorithmic work. I tried uh the one
thing that I tried didn't work super
thing that I tried didn't work super
well. This was my work on P3O. I don't
well. This was my work on P3O. I don't
think you guys were watching the stream
think you guys were watching the stream
then, but I spent a couple weeks on like
then, but I spent a couple weeks on like
some pretty hardcore algorithm dev
some pretty hardcore algorithm dev
around that. And basically like there's
around that. And basically like there's
definitely stuff we can do, but it's
definitely stuff we can do, but it's
going to it's going to require like at
going to it's going to require like at
least my full attention and probably
least my full attention and probably
help from a couple other people for like
help from a couple other people for like
at least several weeks to even have a
at least several weeks to even have a
good chance of figuring that type of
good chance of figuring that type of
stuff out.
So tricky.
I was going to do
All right, my bad, Yanick. I keep
All right, my bad, Yanick. I keep
getting uh I keep getting distracted
getting uh I keep getting distracted
here.
here.
I know. I've been trying to get this
I know. I've been trying to get this
thing for you. My brain is also all
thing for you. My brain is also all
super fuzzy. Like always happens like in
super fuzzy. Like always happens like in
between chunks of work. Takes me a while
between chunks of work. Takes me a while
to like get back to 100% after doing a
to like get back to 100% after doing a
whole bunch of work on like release for
whole bunch of work on like release for
several months. Let me get rid of this
several months. Let me get rid of this
bot. Get out of here, bot.
bot. Get out of here, bot.
We trained bot. We train bots. You do
We trained bot. We train bots. You do
not come here. We train you
to observe.
to observe.
Probably something like this.
There we go.
Okay, so this is a custom policy that
Okay, so this is a custom policy that
literally just does the exact same thing
literally just does the exact same thing
uh but with normalized observations. I
uh but with normalized observations. I
don't have any great confidence that
don't have any great confidence that
this will do anything, but uh it's at
this will do anything, but uh it's at
least going to be less prone to
least going to be less prone to
horrendous failure that makes no sense.
horrendous failure that makes no sense.
And it is actually a little tiny bit
And it is actually a little tiny bit
better uh early on already. Normalize
better uh early on already. Normalize
your observations, folks.
your observations, folks.
between roughly negative one to one. It
between roughly negative one to one. It
doesn't have to be precise, but don't
doesn't have to be precise, but don't
have values that are like five.
have values that are like five.
We have 10 people on YouTube for once.
We have 10 people on YouTube for once.
Hi, it's all open source research.
Hi, it's all open source research.
Come contribute to this and uh get
Come contribute to this and uh get
involved with pushing the cutting edge
involved with pushing the cutting edge
of reinforcement learning right now at
of reinforcement learning right now at
puffer.ai. Start the repo to help me out
puffer.ai. Start the repo to help me out
for free as well. Rest of it's all on
for free as well. Rest of it's all on
Discord.
Discord.
Back to work here.
Back to work here.
Okay. So, marginal benefit from uh this.
Okay. So, marginal benefit from uh this.
Now, let me show you how to one hot a
Now, let me show you how to one hot a
policy.
policy.
Now, it's all positive. It was always
Now, it's all positive. It was always
all positive. You had a char. You had an
all positive. You had a char. You had an
unsigned char.
unsigned char.
I just divided by 11.
Okay. Let me be clear. It doesn't have
Okay. Let me be clear. It doesn't have
to be mean zero, standard deviation one
to be mean zero, standard deviation one
or anything like that. Okay. You don't
or anything like that. Okay. You don't
have to have it be between positive and
have to have it be between positive and
negative. That's not a thing. You just
negative. That's not a thing. You just
need to have it be roughly on the scale
need to have it be roughly on the scale
of -1 to one. Does that make sense?
of -1 to one. Does that make sense?
That doesn't mean that it has to
That doesn't mean that it has to
begative -1 to one. It means that it
begative -1 to one. It means that it
could be it could be -1 to 0 or it could
could be it could be -1 to 0 or it could
be 0 to one, right? You want roughly
be 0 to one, right? You want roughly
that scale. It can't be like 0 to
that scale. It can't be like 0 to
0.00001.
0.00001.
That's too small.
That's too small.
It can't be negative a million to a
It can't be negative a million to a
million. That's too big, right? You want
million. That's too big, right? You want
it roughly on the order of negative one
it roughly on the order of negative one
to one.
Let me show you how to one hot a policy
Let me show you how to one hot a policy
because this is actually going to be
because this is actually going to be
useful for people. So the idea here uh
useful for people. So the idea here uh
is that these are not exactly continuous
is that these are not exactly continuous
values, right? You've encoded like one
values, right? You've encoded like one
is the tile that like is one, two is the
is the tile that like is one, two is the
tile that corresponds to two, three is
tile that corresponds to two, three is
the tile that corresponds to four,
the tile that corresponds to four,
right? And these are kind of discreet in
right? And these are kind of discreet in
a way because like you have to combine
a way because like you have to combine
tiles that are of the same value and the
tiles that are of the same value and the
policy can't see that because you've
policy can't see that because you've
just given it a bunch of floatingoint
just given it a bunch of floatingoint
numbers. So we're going to see and I
numbers. So we're going to see and I
don't know if this environment
don't know if this environment
specifically it's going to make a big
specifically it's going to make a big
difference or not. But we're going to
difference or not. But we're going to
see what happens if instead we tell it
see what happens if instead we tell it
that hey the tile that is one is a
that hey the tile that is one is a
different object than the tile that is a
different object than the tile that is a
two or the tile is a four. And the way
two or the tile is a four. And the way
that we do that is through one hot
that we do that is through one hot
encoding. And what one hot encoding
encoding. And what one hot encoding
does, let me draw this up.
Let me do this properly because this is
Let me do this properly because this is
an important topic for people.
Okay. So,
there's your board.
like
like
like one
goes to
this is one for instance. And this one
this is one for instance. And this one
is like
is like
three,
five, right?
But one hot coating here.
All
right.
So this is an important topic.
So this is an important topic.
One hot encoding
One hot encoding
allows your agent to distinguish between
allows your agent to distinguish between
different values as though they are
different values as though they are
discrete objects and they are not
discrete objects and they are not
continuous values on a number line. So
continuous values on a number line. So
for 2048 here, it allows it to say that
for 2048 here, it allows it to say that
this tile one is not any more similar to
this tile one is not any more similar to
the tile four than the tile four is to
the tile four than the tile four is to
16, which is a reasonable thing to say
16, which is a reasonable thing to say
because you can only combine two tiles
because you can only combine two tiles
that are one, two tiles that are four,
that are one, two tiles that are four,
or two tiles that are 16. You cannot
or two tiles that are 16. You cannot
combine a one with a four any better
combine a one with a four any better
than you can combine a one with a 16. So
than you can combine a one with a 16. So
the way that you do this with a one hot
the way that you do this with a one hot
encoding, let's say that these are
encoding, let's say that these are
originally mapped to their log values
originally mapped to their log values
plus one. So 1 3 and five. What you do
plus one. So 1 3 and five. What you do
is you make an array of zeros of at
is you make an array of zeros of at
least 5 + 1 is six elements each and
least 5 + 1 is six elements each and
then you set that index to one for each
then you set that index to one for each
of these elements and then the rest of
of these elements and then the rest of
them remain zero. So one goes to 0 1 0 0
them remain zero. So one goes to 0 1 0 0
0 three goes to in the third index 0
0 three goes to in the third index 0
index one so on and so forth and then
index one so on and so forth and then
you can flatten that and there's your
you can flatten that and there's your
one hot observation and now your agent
one hot observation and now your agent
can actually see the game the way that
can actually see the game the way that
the game is.
There you go.
I should clip that and put it on
I should clip that and put it on
YouTube.
YouTube.
I've been experimenting with that.
So the way that we do this in practice,
So the way that we do this in practice,
right, there are 11 different values.
right, there are 11 different values.
What did we say?
long and float. Hang on, let me make
long and float. Hang on, let me make
sure.
Okay. I don't think that there are 11
Okay. I don't think that there are 11
things. Are there? Is the maximum value
things. Are there? Is the maximum value
11 or is the maximum value 10?
Yeah, the maximum value is 11. That
Yeah, the maximum value is 11. That
means that they're actually 12 elements.
That's that CUDA error is very common by
That's that CUDA error is very common by
the way. like that will you'll always
the way. like that will you'll always
get hit by that if you mess up a one hot
love your content I'm doing research on
love your content I'm doing research on
lambda policy iteration
lambda policy iteration
randomization the goal is to make a
randomization the goal is to make a
hybrid of two policies
hybrid of two policies
a policy iteration I'm not actually
a policy iteration I'm not actually
familiar with that hybrid of two
familiar with that hybrid of two
policies
huh yeah I'd need a link or something to
huh yeah I'd need a link or something to
see what that is but yeah thanks for
see what that is but yeah thanks for
stopping by. We do all sorts of RL
stopping by. We do all sorts of RL
around here.
around here.
Uh
Uh
oh. Okay. So, this was doing way better
oh. Okay. So, this was doing way better
and then somehow there's
and then somehow there's
there's some instability associated with
there's some instability associated with
this thing.
Let's see if it's consistent.
value loss is blowing up.
Yeah, I could make it. I doubt that's
Yeah, I could make it. I doubt that's
what's happening to be fair.
what's happening to be fair.
You'd get a CUDA assert
You'd get a CUDA assert
source of paper of mine here. It's
source of paper of mine here. It's
related with RF. Yeah. Uh, if YouTube
related with RF. Yeah. Uh, if YouTube
doesn't let you post the link, stick it
doesn't let you post the link, stick it
in Discord. I don't have any settings
in Discord. I don't have any settings
on, but sometimes YouTube's obnoxious
on, but sometimes YouTube's obnoxious
about that.
about that.
Discord.gg/puffer
Discord.gg/puffer
if you're not already there.
if you're not already there.
If you insist on not doing one hot, I am
If you insist on not doing one hot, I am
doing one hot.
doing one hot.
Two weeks. I literally just added that
Two weeks. I literally just added that
right here. See?
We're just trying to figure out what's
We're just trying to figure out what's
up with this unstable
up with this unstable
stable mess.
I missed that. Yeah. So, we normalized
I missed that. Yeah. So, we normalized
first from 0 to one. Doesn't matter.
first from 0 to one. Doesn't matter.
Negative 1 versus negative one to versus
Negative 1 versus negative one to versus
uh 0 to one doesn't matter. Um,
we have one hot and we actually, you see
we have one hot and we actually, you see
that like before this thing crashes, we
that like before this thing crashes, we
actually were getting a better result
actually were getting a better result
here like a much better result.
here like a much better result.
So why is this thing not stable?
Usually a data bug.
Usually a data bug.
Usually our stuff is pretty stable.
Usually our stuff is pretty stable.
learning rate. We use that learning rate
learning rate. We use that learning rate
with everything though. Like you're
with everything though. Like you're
using now our default settings that
using now our default settings that
should work for this and pretty much
should work for this and pretty much
everything,
especially for a network of this size.
Let me think about what uh what other
Let me think about what uh what other
weird things could be happening here.
We have not looked at your reward
We have not looked at your reward
structure at all.
structure at all.
So that shouldn't really nan stuff. I
So that shouldn't really nan stuff. I
guess it technically can.
Okay, so we have
Okay, so we have
your actions unsigned car chart
your actions unsigned car chart
observation.
observation.
Make sure that this is done correctly.
You went to eight. Good.
You went to eight. Good.
So this is specified reasonably
passing it to a view of 4x4 and torch
passing it to a view of 4x4 and torch
but continuous vector and see
but continuous vector and see
that shouldn't cause an issue because it
that shouldn't cause an issue because it
should just be laid out contiguous in
should just be laid out contiguous in
memory.
Is your output for actions across all
Is your output for actions across all
possible actions?
It is across all possible actions
It is across all possible actions
and uh the actions uh is very small for
and uh the actions uh is very small for
2048. It's only there are that four
2048. It's only there are that four
possible actions.
possible actions.
Wait up.
Wait up.
Wait, wait, wait. Did I find something?
Wait, wait, wait. Did I find something?
Move.
Okay. No, you do add plus one. to take
Okay. No, you do add plus one. to take
game actions of zero plus one.
game actions of zero plus one.
You don't need to mask actions. That's
You don't need to mask actions. That's
not a thing.
Hang on. So, you just have right here.
Hang on. So, you just have right here.
It just gets Does it get negative.1 for
It just gets Does it get negative.1 for
losing and that's it? That doesn't make
losing and that's it? That doesn't make
sense. There's got to be more rewards
sense. There's got to be more rewards
than that, right? Okay. No, there's
enums are proper. What do you mean?
enums are proper. What do you mean?
They don't need to be made an enum. A
They don't need to be made an enum. A
pound define like this is totally fine.
Oh, do you mean make sure that they're
Oh, do you mean make sure that they're
ordered correctly?
As long as he's checking them by name
As long as he's checking them by name
here instead of by by value, that should
here instead of by by value, that should
be fine. Cuz it doesn't matter what
be fine. Cuz it doesn't matter what
order you list the actions in for the
order you list the actions in for the
agent.
But it does get a reward here, right? It
But it does get a reward here, right? It
gets a reward for merging stuff.
Row of I
Row of I
/ 11F
Okay, I see.
You probably do need that um
that reward.
that reward.
I'm going to try it without this for a
I'm going to try it without this for a
second though.
second though.
Wait, let me open this in the other tab.
This way.
If it can't do an action,
If it can't do an action,
don't you lose the game?
You can't do anything.
You can't do anything.
Is there no lose condition? There should
Is there no lose condition? There should
be a lose condition, right?
Small negative reward should be fine.
No, sometimes you can move left but up.
It should reset when it can't move in
It should reset when it can't move in
any direction. Right? That's the lose
any direction. Right? That's the lose
condition, isn't it?
Yeah.
Yeah.
Small negative reward for invalid is
Small negative reward for invalid is
fine.
Oh.
You said you got it to solve the game
You said you got it to solve the game
though with and with the thing that you
though with and with the thing that you
committed and it definitely wasn't
committed and it definitely wasn't
solving the game, right?
I definitely got it to just do way
I definitely got it to just do way
better for a bit. But the fact that this
better for a bit. But the fact that this
is so unstable makes me think that there
is so unstable makes me think that there
is something wrong with the game data.
is something wrong with the game data.
Like an environment like this should not
Like an environment like this should not
be so unstable.
Like I would suspect some sort of bug
lot of runs.
lot of runs.
Can you play the game yourself and
Can you play the game yourself and
verify no bugs? No, I can't because it
verify no bugs? No, I can't because it
doesn't have the uh the playable version
doesn't have the uh the playable version
of it working at the moment. So, I think
of it working at the moment. So, I think
that's probably
that's probably
I think I'm going to wait for you to add
I think I'm going to wait for you to add
some changes to this. Um, if you can PR
some changes to this. Um, if you can PR
the C file, that would be nice. I think
the C file, that would be nice. I think
that there's likely some sort of data
that there's likely some sort of data
corruption, though.
I don't vibe code anything two weeks.
I don't vibe code anything two weeks.
Maybe more than in your average M.
Maybe more than in your average M.
That really shouldn't matter.
For something like with a discrete
For something like with a discrete
action space, it really shouldn't blow
action space, it really shouldn't blow
up like Yes.
Me try something real quick. I'm going
Me try something real quick. I'm going
to try one more thing for you here. Then
to try one more thing for you here. Then
I got to do some robotic stuff, I
I got to do some robotic stuff, I
believe.
I just want to see if this works.
Be a fair bit slower probably.
What's the compile error?
What's the compile error?
Uh, some missing packages. Hang on.
Uh, some missing packages. Hang on.
You know, this is actually this is like
You know, this is actually this is like
a slow crash, not a fast crash. This
a slow crash, not a fast crash. This
could just be hypers.
Yeah, this could just be hypers.
Yeah, this could just be hypers.
I'm not confident it's a data bug.
Oh, this was also wrong. End of action.
Where's the minus one
fine in
Is this like a weird double include? But
I don't know why your memes in yourh
I don't know why your memes in yourh
doesn't work.
Oh, there should be a minus one.
Okay.
Okay.
But then there's something wrong with
But then there's something wrong with
these includes. Yeah.
Double included.
I yeah I don't something is screw with
I yeah I don't something is screw with
like the includes in here. I don't know
like the includes in here. I don't know
what.
what.
Uh
I guess this just
I guess something else includes string.h
I guess something else includes string.h
page for you.
Yes.
Where is this supposed to be from?
This function doesn't exist.
And it's not in here. So, I think
And it's not in here. So, I think
there's some stuff missing, man.
there's some stuff missing, man.
And you shouldn't even need that
And you shouldn't even need that
anyways. Like, it should be
You should just do that with Rayb.
I think you can get rid of this
I think you can get rid of this
entirely.
All right, there you go.
Yeah, it looks like it works.
Okay, there's 256. Cool. So, I mean, the
Okay, there's 256. Cool. So, I mean, the
game seems to work the way I would
game seems to work the way I would
expect it to, right?
It will learn to spam keys.
Yeah, it it's a little tricky with this
Yeah, it it's a little tricky with this
because the the signal is pretty delayed
because the the signal is pretty delayed
if you rewarded every merge, right?
if you rewarded every merge, right?
which is why I was trying to not do
which is why I was trying to not do
that.
I was basically just telling it to not
I was basically just telling it to not
lose. Our RL should be good enough that
lose. Our RL should be good enough that
it should be able to learn from just
it should be able to learn from just
like don't lose.
Like just don't like there's no don't
Like just don't like there's no don't
lose and not win in this game, right?
lose and not win in this game, right?
Like if you don't lose for long enough,
Like if you don't lose for long enough,
you win because you have to merge stuff
you win because you have to merge stuff
to get it out of the way.
to get it out of the way.
So,
So,
I think that that's probably going to be
I think that that's probably going to be
like just not even shaping the reward at
like just not even shaping the reward at
all will probably be better for this
all will probably be better for this
specific environment.
How many moves does it take to merge?
How many moves does it take to merge?
You get like um
You get like um
you get like a few additional units
you get like a few additional units
every single step. So, it's probably
every single step. So, it's probably
it's probably like somewhere between 512
it's probably like somewhere between 512
and 1024ish or whatever.
I would imagine.
Wait,
Wait,
if you can't make invalid moves, right,
if you can't make invalid moves, right,
in this like it just doesn't let you.
in this like it just doesn't let you.
Like,
Like,
is that how the original works?
you get a random two or four.
you get a random two or four.
You could just make it do that then if
You could just make it do that then if
uh instead of having to do invalid
uh instead of having to do invalid
moves, it just gives you a thing. So
moves, it just gives you a thing. So
random two or four would be between 512
random two or four would be between 512
and uh 1024 steps, I believe, to solve
and uh 1024 steps, I believe, to solve
the game.
the game.
That's like a useful thing to know.
That's like a useful thing to know.
It's probably like 384 or something on
It's probably like 384 or something on
average.
average.
Right.
Okay, that's enough of that. Cool. So,
let me commit you this policy.
I think once we have this thing working,
I think once we have this thing working,
it'll be a very cool thing to watch.
Like it'll be pretty similar to Tetris.
Nothing happens. Okay. So that's then
Nothing happens. Okay. So that's then
you've done it correctly.
Okay. So, there you go. I will leave it
Okay. So, there you go. I will leave it
up to you to like get something a bit
up to you to like get something a bit
more stable. Um, if you have a sweep set
more stable. Um, if you have a sweep set
up and you want me to just like run
up and you want me to just like run
something a little longer, you let me
something a little longer, you let me
know and I can make that happen for you.
know and I can make that happen for you.
Thank you for the M. Let's get it uh
Thank you for the M. Let's get it uh
finished and I think this will look
finished and I think this will look
pretty cool.
pretty cool.
Okay, next thing. Let me make sure I
Okay, next thing. Let me make sure I
have not missed anything.
go-to benchmark for RL.
go-to benchmark for RL.
Surprisingly hard to solve. I think
Surprisingly hard to solve. I think
you'll be surprised. I think once you
you'll be surprised. I think once you
actually have this thing set up
actually have this thing set up
correctly, you'll solve it very very
correctly, you'll solve it very very
quickly. Just purely based on like the
quickly. Just purely based on like the
action space and the episode length and
action space and the episode length and
the verifiability. Like I think he'll
the verifiability. Like I think he'll
solve it quite quickly.
All right, this has gotten a little bit
All right, this has gotten a little bit
more attention. 12K views on this. And
more attention. 12K views on this. And
then
then
Spencer is hopefully getting some
Spencer is hopefully getting some
follows. Go follow Spencer, guys. He's
follows. Go follow Spencer, guys. He's
doing real cool work.
Okay, so next we're going to do some
Okay, so next we're going to do some
robotics.
robotics.
Um,
Um,
that might seem like a hard pivot, but
that might seem like a hard pivot, but
basically all of these ends look the
basically all of these ends look the
same. uh in from the perspective of
same. uh in from the perspective of
puffer liib like a robotics M doesn't
puffer liib like a robotics M doesn't
look all that different from 2048
look all that different from 2048
like kind of all the problems just look
like kind of all the problems just look
the same.
Ah and perfect. So this thing this
Ah and perfect. So this thing this
experiment that I was running this is
experiment that I was running this is
what I was waiting for. So there's this
what I was waiting for. So there's this
weird thing that happens here probably
weird thing that happens here probably
because of the learning rate but it is
because of the learning rate but it is
doing uh better. So hopefully this
doing uh better. So hopefully this
continues to learn stuff. We'll see.
continues to learn stuff. We'll see.
Yeah, my guy does robotics now. Uh I
Yeah, my guy does robotics now. Uh I
just I told Stone I would integrate some
just I told Stone I would integrate some
robotic stuff for him cuz he develops
robotic stuff for him cuz he develops
Manny skill. Let me show off Manny skill
Manny skill. Let me show off Manny skill
so folks can see what this is.
so folks can see what this is.
So this is one of like the better
So this is one of like the better
robotics uh sim libraries out there. It
robotics uh sim libraries out there. It
doesn't have a giant mess of horrible
doesn't have a giant mess of horrible
code as best as I can tell.
And let's see. Get started.
And let's see. Get started.
You do need to have better docks.
Okay. So, here are docs. You can see all
Okay. So, here are docs. You can see all
these different tasks. They have state
these different tasks. They have state
based tasks. They have RGB like rendered
based tasks. They have RGB like rendered
tasks. Got all sorts of stuff. And uh we
tasks. Got all sorts of stuff. And uh we
did get it running with Puffer in not
did get it running with Puffer in not
very long at all,
very long at all,
but uh it doesn't train very well. So I
but uh it doesn't train very well. So I
got to figure out why that is. I'm going
got to figure out why that is. I'm going
to try their policy. I'm going to try
to try their policy. I'm going to try
some other stuff. And yeah, we're going
some other stuff. And yeah, we're going
to see if we can figure out what is
to see if we can figure out what is
going on with uh this.
So, given that I ran a sweep and it
So, given that I ran a sweep and it
didn't do anything,
didn't do anything,
I think we're going to start by sticking
I think we're going to start by sticking
a little bit more close to their script
a little bit more close to their script
to start with
to start with
and uh we'll go from there.
So, we'll take PTO fast.
Put this
Put this
however high I can get away with without
however high I can get away with without
covering too much.
So, this is our binding for Manny.
I don't know. So many people have asked
I don't know. So many people have asked
me to do robotics, and I kind of just
me to do robotics, and I kind of just
haven't until now. I guess it was mostly
haven't until now. I guess it was mostly
I didn't want to deal with Isaac Jim and
I didn't want to deal with Isaac Jim and
Isaac Sim because we had to do Isaac Jim
Isaac Sim because we had to do Isaac Jim
for one contract and it just was not
for one contract and it just was not
fun.
fun.
Like it frankly it was way more work
Like it frankly it was way more work
than it was worth dealing with that
than it was worth dealing with that
horrendous code base.
Stone writes uh Stone writes pretty nice
Stone writes uh Stone writes pretty nice
code though. Of course we all owe to
code though. Of course we all owe to
Costa for clean RL.
Yeah, Isaac Jim is pretty rough.
Yeah, Isaac Jim is pretty rough.
And then the thing is all the people
And then the thing is all the people
using Isaac Jimmer are also like their
using Isaac Jimmer are also like their
code also sucks. So it's just not a fun
code also sucks. So it's just not a fun
time. Like there were literally 10,000
time. Like there were literally 10,000
lines of code when there should have
lines of code when there should have
been maybe a thousand.
Where's the end of initialization?
Yeah, Isaac's underdev.
at all. Manny looks nice.
I would have to talk to more robotics
I would have to talk to more robotics
people to see like what they think of
people to see like what they think of
Manny,
but it looks nice from an outside
but it looks nice from an outside
perspective here.
perspective here.
Of
Of
course, everything in robotics needs to
course, everything in robotics needs to
just be 10 times faster,
just be 10 times faster,
but uh yeah, I'm not getting into
but uh yeah, I'm not getting into
hardcore sim until uh you have something
hardcore sim until uh you have something
major for that.
Reconfiguration frequency.
What is this reconfiguration frequency?
This seems weird to me. Like there's
This seems weird to me. Like there's
some parameter.
None.
None.
Okay. So, it's an optional thing. So,
Okay. So, it's an optional thing. So,
that's not it.
that's not it.
You have gym.make here.
They flatten some spaces
and then their M's are
args num ms
args num ms
ignore terminations equals not args.p
ignore terminations equals not args.p
Partial reset.
So this should be false because we want
So this should be false because we want
to do partial resets, right?
to do partial resets, right?
Partial reset is true. Nor terms is
Partial reset is true. Nor terms is
false.
What we have
What we have
and do they have auto reset?
and do they have auto reset?
We see auto reset. I I forgot.
It does not look like they do auto
It does not look like they do auto
resets,
resets,
but that's fine.
Okay. So then they have this policy.
Let's try their policy.
We'll just graft this onto our default.
We'll just graft this onto our default.
So, we have a default policy that can
So, we have a default policy that can
handle
handle
discrete spaces, continuous spaces,
discrete spaces, continuous spaces,
whatever. And it's really easy to use
whatever. And it's really easy to use
with an LSTM.
with an LSTM.
Uh so what we're going to do here
Uh so what we're going to do here
we're just going to make their policy
we're just going to make their policy
on our architecture.
See how we do this?
This is the actor and then we have the
This is the actor and then we have the
critic.
We'll just have encoder do nothing, I
We'll just have encoder do nothing, I
guess.
Yeah. So now we have our decoder mean
value will be similar.
Okay,
Okay,
so this is a little bit janky here.
so this is a little bit janky here.
So, uh bear with me,
So, uh bear with me,
but we will get this to something that
but we will get this to something that
is relatively clean and uh hopefully
is relatively clean and uh hopefully
useful quite quickly.
Yes.
Yes.
decode action.
Let me answer this DM.
Okay. So, let me make sure this is the
Okay. So, let me make sure this is the
same.
same.
They have the critic layer knitted.
I believe I messed up one of these. Hang
I believe I messed up one of these. Hang
on. You have to be very careful even
on. You have to be very careful even
with stuff like init scales.
with stuff like init scales.
decoder
decoder
standard deviation 01 and value get
standard deviation 01 and value get
standard deviation of one the last
standard deviation of one the last
layer. So I could have messed that up.
layer. So I could have messed that up.
Yep.
All right. So let's see if this runs.
My attribute is
some sort of Matt mall
some sort of Matt mall
issue.
96 by 42.
This is
okay. So this fails and
okay. So this fails and
decoder mean
Weird
hidden shape
decoder. me in
decoder. me in
eight in feature. That's wrong
eight in feature. That's wrong
the N obs.
And then this is going to be um
And then this is going to be um
I don't know how many this is.
I don't know how many this is.
Not huge.
Thought
I had fixed that.
Why is this still eight?
What's What's wrong with this thing?
Oh, observation space. Silly.
All right.
Clearly, I need more coffee.
Clearly, I need more coffee.
Get this training first.
Ideally on to Neptune even.
Well, this looks fine.
The plan is to uh match their
The plan is to uh match their
architecture and roughly match their
architecture and roughly match their
hypers. See if that magically fixes it.
hypers. See if that magically fixes it.
And then if not, we assume it's a data
And then if not, we assume it's a data
issue. We figure out what's different
issue. We figure out what's different
with our config.
the issue here.
Bandage size.
Bandage size.
That looks like a batching issue.
Uh, why would that be a batching issue?
That's weird.
That's weird.
They do a bad reshape.
Wait.
Wait.
Oh, I just messed up.
Yeah, this is one.
All right. Amateur hour here.
Abracadabra.
Abracadabra.
did some work on asteroids and now the
did some work on asteroids and now the
agent learns. Take a look at the PR.
agent learns. Take a look at the PR.
Yep.
Yep.
One second here. I want to uh at least
One second here. I want to uh at least
get something running on this and then I
get something running on this and then I
will look at that.
Let me put this Neptune
Let me put this Neptune
magnet
magnet
attached a video of the agent playing.
attached a video of the agent playing.
Let's see.
General
Where'd you put it? Oh, in the PR. The
folks do star the puffer.
folks do star the puffer.
Little tiny uptick here. See this little
Little tiny uptick here. See this little
tiny uptick in stars? This represents a
tiny uptick in stars? This represents a
generational leap in uh reinforcement
generational leap in uh reinforcement
learning capabilities from 2.0 to 3.0.
learning capabilities from 2.0 to 3.0.
So, this should be like 10x
So, this should be like 10x
based on the uh the quality of the RL in
based on the uh the quality of the RL in
there.
Asteroids.
Oh, that dodges.
Kind of dodges.
Kind of dodges.
That's you playing. Okay.
That's you playing. Okay.
I was gonna say that agent needs to be a
I was gonna say that agent needs to be a
little smarter, but yeah. No, it's just
little smarter, but yeah. No, it's just
the game is hard.
Wait, this thing gets merked instantly,
Wait, this thing gets merked instantly,
doesn't it?
Uh, kind of plays.
Uh, kind of plays.
All right, let's see what you've got.
All right, let's see what you've got.
So,
well, you have some crazy parameters in
well, you have some crazy parameters in
here. Explain.
here. Explain.
Are you doing four agents or like four
Are you doing four agents or like four
environments?
environments?
What's this mini batch? You shouldn't be
What's this mini batch? You shouldn't be
able to train anything on this
local testing.
local testing.
Do you have the hypers for uh training?
Do you have the hypers for uh training?
Like do you have like hypers that give
Like do you have like hypers that give
you a stable curve
eight? Okay.
eight? Okay.
Still 20 million steps
your code
ob shape and I I'd already looked
ob shape and I I'd already looked
through the code of this so I assume
through the code of this so I assume
that you uh you sorted the obs like I
that you uh you sorted the obs like I
asked or did you not do that yet?
recorded obs good added velocities
recorded obs good added velocities
and those are all normalized right
and those are all normalized right
positions
positions
like it's all normalized yeah
Good.
Okay. So, I had previously reviewed
Okay. So, I had previously reviewed
this. So, I think the most useful thing
this. So, I think the most useful thing
for me to do now to actually play with
for me to do now to actually play with
it,
it,
see if I can get you uh anything out of
see if I can get you uh anything out of
that.
that.
Pull this
I mean, we're getting all these cool
I mean, we're getting all these cool
game PRs now, which is cool.
game PRs now, which is cool.
But you'll be able to solve asteroids
But you'll be able to solve asteroids
within a few minutes once you have it
within a few minutes once you have it
correct. Guaranteed.
Okay, let me try one more time.
Okay, let me try one more time.
See what the uh
See what the uh
I have to like actually do something
I have to like actually do something
that's like not trivial, but like
game's freaking hard.
game's freaking hard.
Tank controls are always difficult.
I don't know why it's like it shouldn't
I don't know why it's like it shouldn't
be that hard.
be that hard.
I don't know. Probably just takes like a
I don't know. Probably just takes like a
fair bit of practice.
Oh yeah, cuz there's no back button is
Oh yeah, cuz there's no back button is
the thing. You're not allowed to back
the thing. You're not allowed to back
up. You have to like tank control
up. You have to like tank control
everything.
Okay.
Yeah.
Env is kind of Hello.
uh vecum m
that case then you do like this.
Okay, it's
Okay, it's
Still a little slow, but it's all right.
did not get penalized for dying.
Hey, uh where's the uh the death penalty
Hey, uh where's the uh the death penalty
here?
Okay. So, where's it die? So, I can add
Okay. So, where's it die? So, I can add
a penalty there.
four or
the score is not correct.
Is there a max score in this?
A max score
A max score
type.
What's your SPS training?
Probably because my GPU is faster than
Probably because my GPU is faster than
yours.
yours.
I can crank more data than you.
Weird.
4090 is a bit faster than an 800.
Up to 2x, I think.
Try this.
Takes too long to train. So, we're going
Takes too long to train. So, we're going
to do something slightly shorter.
Max ticks 60
Max ticks 60
seconds.
seconds.
So it's 3600 ticks as max
fine.
H.
H.
There you go.
There you go.
Unless I forgot to reset this.
It's going to be pretty funny if I just
It's going to be pretty funny if I just
did that and now it works.
There were two errors. Your logging was
There were two errors. Your logging was
wrong. You weren't logging what you
wrong. You weren't logging what you
thought you were. And uh there was not a
thought you were. And uh there was not a
death penalty.
Hello, Bet. We're getting asteroids
Hello, Bet. We're getting asteroids
added today
added today
and doing robotics and other things.
And Beta is currently suffering because
And Beta is currently suffering because
I gave him a project that was too hard.
I gave him a project that was too hard.
Once again,
What's this thing doing?
What's this thing doing?
Oh, that's so cheeky.
Oh, that's so cheeky.
It's literally doing I The score is
It's literally doing I The score is
correct. This thing is doing way better
correct. This thing is doing way better
than I have just by doing this. You see
than I have just by doing this. You see
it?
What the heck?
[Laughter]
That's crazy.
Yeah, it's just decided it doesn't feel
Yeah, it's just decided it doesn't feel
like flying.
like flying.
It totally can.
Okay.
We're going to do a
We're going to do a
just real quick here. Uh where is it?
just real quick here. Uh where is it?
And I'm going to give you this so that
And I'm going to give you this so that
you can run a proper sweep. I'll give
you can run a proper sweep. I'll give
you the honors of doing that.
you the honors of doing that.
But uh I am going to get
few small
few small
color options here.
Huge congratulations on the release,
Huge congratulations on the release,
man. Really excited to play around with
man. Really excited to play around with
the library. Thank you very much. Yeah,
the library. Thank you very much. Yeah,
it was a huge huge amount of work. Um,
it was a huge huge amount of work. Um,
but it should be pretty good, pretty
but it should be pretty good, pretty
stable. And if it's not, come yell at me
stable. And if it's not, come yell at me
on here cuz I'm pretty much just going
on here cuz I'm pretty much just going
to be streaming for quite a bit.
to be streaming for quite a bit.
Yeah, it only knows One Direction
Yeah, it only knows One Direction
Splinter, but the thing is it's still
Splinter, but the thing is it's still
way better at the game than I am.
way better at the game than I am.
Literally pressing It's holding down
Literally pressing It's holding down
space and pressing one button, and it's
space and pressing one button, and it's
better at the game than I am.
better at the game than I am.
This is literally the first run that we
This is literally the first run that we
did. It like it'll get better from
did. It like it'll get better from
There.
Uh, I want to use different colors.
Like, it's going to be slightly
Like, it's going to be slightly
I know what it I know how to replace. I
I know what it I know how to replace. I
know how to find and replace in Vim. I'm
know how to find and replace in Vim. I'm
choosing not to.
I'll chill
cuz I want to basically figure out like
cuz I want to basically figure out like
what like what color I want to replace
what like what color I want to replace
with what, right?
And like if I want to do different
And like if I want to do different
pieces,
pieces,
do puff white.
Uh don't split on 80 like this
Uh don't split on 80 like this
neurotically. If you want to split for
neurotically. If you want to split for
80, you can split it like
80, you can split it like
here or whatever. There's no reason to
here or whatever. There's no reason to
like split like right at 80 like and
like split like right at 80 like and
leave like five characters on next line
leave like five characters on next line
of white. I just copy it over from go.
of white. I just copy it over from go.
It's
It's
fine format. Pang format is not a
fine format. Pang format is not a
requirement for puffer.
I I don't recall ever telling anyone to
I I don't recall ever telling anyone to
use clank format.
comes with Clang, LSP, and Neo Bam.
comes with Clang, LSP, and Neo Bam.
Well, I don't have an LSP,
Well, I don't have an LSP,
so I guess I wouldn't know.
Oh, yeah. You can't see these at all.
Oh, yeah. You can't see these at all.
these little particles.
I'll fix it. Don't worry.
Is this the draw particles? Is this the
Is this the draw particles? Is this the
uh the bullets or what?
Yeah, but which one is it? Wooden. Is it
Yeah, but which one is it? Wooden. Is it
this one?
Here we go.
Here we go.
That looks pretty good.
I like that.
Okay. Wouldn't that's up to you to uh to
Okay. Wouldn't that's up to you to uh to
sweep and get a good policy. Now,
sweep and get a good policy. Now,
once you get a good policy that like
once you get a good policy that like
actually looks interesting and hopefully
actually looks interesting and hopefully
doesn't just go in a straight line, we
doesn't just go in a straight line, we
can put it on the website. It'll be
can put it on the website. It'll be
official.
official.
Should be a lot easier. Now it like just
Should be a lot easier. Now it like just
trains trains super fast. 100 to 200
trains trains super fast. 100 to 200
million step sweep and you should be
million step sweep and you should be
able to be uh pretty well set up there.
That's a little better.
I'm happy with um
uh 13K on an article that technical was
uh 13K on an article that technical was
like at 2K before and I was like come on
like at 2K before and I was like come on
you got to be kidding
Okay,
Okay,
I will be uh I'm going to use the
I will be uh I'm going to use the
restroom. I'll be right back and then we
restroom. I'll be right back and then we
will figure out how to get Manny skill
will figure out how to get Manny skill
uh how to get the robot to pick up a
uh how to get the robot to pick up a
cube or whatever.
cube or whatever.
And hopefully that doesn't take too
And hopefully that doesn't take too
long. We shall see. I'll be right back.
All
right.
right.
So,
interestingly, we got roughly the same
interestingly, we got roughly the same
result out of this.
turn goes up to like 20
turn goes up to like 20
success once kind of does a thing and
success once kind of does a thing and
pitters out.
pitters out.
So, let's actually go look at our sweep
So, let's actually go look at our sweep
results. Didn't really get much of a
results. Didn't really get much of a
full sweep, but
full sweep, but
Oh,
I got to export this policy for these
I got to export this policy for these
guys real quick.
Yeah, because this is way better.
cuz I got soda for them
cuz I got soda for them
by another big margin.
I also want to look at this policy. It's
I also want to look at this policy. It's
probably pretty cool looking. Whatever
probably pretty cool looking. Whatever
this soda thing
so that'll eval. We'll jump us a replay.
a minute to generate us a replay
and we'll see what it looks like.
Why is this not dumping replays?
Oh, dumb.
I forgot to actually uncomment that.
I forgot to actually uncomment that.
All
right, give this a minute.
We will look at what this is.
Okay.
All right, we're going to actually see
All right, we're going to actually see
what this thing is doing. They sent me a
what this thing is doing. They sent me a
message that like um apparently the
message that like um apparently the
replay is it works but not quite as well
replay is it works but not quite as well
as it should. I don't know. We'll check.
viewer.
Click cable icon.
Okay.
Total reward.
Cool.
So, there's their soda policy
So, there's their soda policy
and then the
All right, they ought to be pretty happy
All right, they ought to be pretty happy
with that. That's a soda policy.
And we've uh confirmed that it actually
And we've uh confirmed that it actually
does what we think.
All right, let's go back to robotics.
I've done enough stuff on this for the
I've done enough stuff on this for the
uh the time being. Give them a little
uh the time being. Give them a little
bit of time to look at the policies and
bit of time to look at the policies and
tell me what they think.
tell me what they think.
So, we did have some sweeps on robotics.
Yeah. So, of all these they we don't
Yeah. So, of all these they we don't
really ever get above like 25 return for
really ever get above like 25 return for
some reason.
And this one here
And this one here
is pretty similar looking.
Why don't we go look at their uh their
Why don't we go look at their uh their
baseline and figure out what the heck is
baseline and figure out what the heck is
going on?
Like they have their own PO and we have
Like they have their own PO and we have
we should look at it.
we should look at it.
The thing is we we don't use PO so it's
The thing is we we don't use PO so it's
not going to be
not going to be
it's not going to be like one to one
it's not going to be like one to one
comparable
comparable
and the key annoying thing is that like
and the key annoying thing is that like
our hyper prams are going to be
our hyper prams are going to be
different
different
like our defaults are optimized for
like our defaults are optimized for
large batch.
Uh so technically our algorithm is a
Uh so technically our algorithm is a
variant of PO but we've made enough
variant of PO but we've made enough
changes to it that it's it's a
changes to it that it's it's a
meaningfully different algorithm at this
meaningfully different algorithm at this
point. It's just puffer trainer.
Yeah. But like the thing is
Yeah. But like the thing is
it depends if you want to categorize
it depends if you want to categorize
stuff by the way they behave or by a
stuff by the way they behave or by a
bunch of math that doesn't mean
bunch of math that doesn't mean
anything. If you want to write down the
anything. If you want to write down the
math, they look pretty similar. But if
math, they look pretty similar. But if
you want to actually look at like how
you want to actually look at like how
this behaves versus the original, it's
this behaves versus the original, it's
completely different. So,
Push cube v1 can take less than a minute
Push cube v1 can take less than a minute
to train on the GPU. Let's try push cube
to train on the GPU. Let's try push cube
v1.
Okay.
Okay.
Um, this is still not really training.
Um, this is still not really training.
Yeah.
And this should be a very very easy
And this should be a very very easy
task.
Yeah,
a very very easy task.
See if reward is stable at least.
Okay, so this sort of just works. I
Okay, so this sort of just works. I
guess
guess
it takes it a little longer, but it just
it takes it a little longer, but it just
works.
Do we tune hypers for this then since
Do we tune hypers for this then since
it's stable and then attempt to apply
it's stable and then attempt to apply
those to the other problem?
That would make sense to me, right?
What architecture do we want to use
What architecture do we want to use
though?
I kind of want to compare this to our
I kind of want to compare this to our
other architecture.
Okay, so 96% out of the box,
4 minutes, but again, we're fully
4 minutes, but again, we're fully
unbound here.
unbound here.
And what we're going to do is we are
And what we're going to do is we are
going to do
going to do
an RNM
We're going to compare this to our
We're going to compare this to our uh
We're going to compare this to our uh
original defaults.
I don't know why this is taking so long
I don't know why this is taking so long
to eval as well. I guess it's just the
to eval as well. I guess it's just the
end of being slow.
Uh yeah, because it's going to do like
Uh yeah, because it's going to do like
two minutes worth of epochs.
two minutes worth of epochs.
I think I have it set to like 32
I think I have it set to like 32
32 iterations by default, which is
32 iterations by default, which is
generally pretty fast, but I guess for
generally pretty fast, but I guess for
this end of it's not.
That's no big deal though.
That's no big deal though.
Try again with our other architecture
just to see how it uh how it compares.
And the goal of this is to tell us if
And the goal of this is to tell us if
there's something magical to these tages
there's something magical to these tages
that everybody in robotics seems to use
that everybody in robotics seems to use
for some reason.
That's pretty much it.
Cool.
Cool.
New Brock model.
Compare curves.
That's amusing.
That's amusing.
So, it looks like our smaller default
So, it looks like our smaller default
policy is better than their um larger
policy is better than their um larger
than their larger tuned policy
than their larger tuned policy
just out of the box.
that this is actually the result. And
that this is actually the result. And
I'll send this to Stone before we uh
I'll send this to Stone before we uh
start a sweep.
if it's stable.
Uhoh.
Weird. The reward keeps going up though,
Weird. The reward keeps going up though,
even though the success is down.
Maybe our RL is too good. I don't know
Maybe our RL is too good. I don't know
if the success is not actually correctly
if the success is not actually correctly
correlated with the reward.
What uh Stone has to say about this
What uh Stone has to say about this
new chess and SPS in first run 40k that
new chess and SPS in first run 40k that
against like some
against like some
meaningful opponent or something bet
are the open spiel is the open spiel
are the open spiel is the open spiel
just that incompetent of a object,
right? But is it running against like a
right? But is it running against like a
like is it running against an AI
like is it running against an AI
opponent, right? That it's like doing
opponent, right? That it's like doing
some deep search.
some deep search.
Oh, this is your 100% rewrite. I see.
Oh, this is your 100% rewrite. I see.
Versus random.
Versus random.
Well, you ought to be able to fix that.
Well, you ought to be able to fix that.
The chest should run billions, Okay.
stats appeared. Episode length is too
stats appeared. Episode length is too
high for chess.
high for chess.
Just taking all invalid moves. Yes,
Just taking all invalid moves. Yes,
that's expected. So, you're going to
that's expected. So, you're going to
have to do some re like some actual
have to do some re like some actual
research on that type of a thing that to
research on that type of a thing that to
figure out how to fix that.
streaming right now. Rock plays Pokemon.
streaming right now. Rock plays Pokemon.
That's fun.
just trying for save a video of this.
I'll get
just dang.
finally got an intern for myself.
finally got an intern for myself.
Congratulations
Okay, has linked to me the reward
Okay, has linked to me the reward
function.
They encourage the robot gripper to go
They encourage the robot gripper to go
behind the cube.
behind the cube.
Move the cube to the goal region. Ensure
Move the cube to the goal region. Ensure
the cube isn't being lifted up.
What's the stream about today? Currently
What's the stream about today? Currently
doing robotic stuff.
doing robotic stuff.
We're trying to get Puffer to solve
We're trying to get Puffer to solve
robotics.
code handle early resets properly.
Terminated.
Ted.
Yes, it works.
Yeah. So, this is weird. It's not
Yeah. So, this is weird. It's not
actually solving it.
Max reward of one.
Am I using the right checkpoint?
Try regenerating.
Damn you wrong.
Bizarro man
going Come on.
That's getting zero observations.
Weird.
Ah, I see it.
77k.
End of eval time is zero. Oh, so it's
End of eval time is zero. Oh, so it's
probably the action space being huge
probably the action space being huge
bat. Yeah, we're going to have to figure
bat. Yeah, we're going to have to figure
something out about that. I have no idea
something out about that. I have no idea
what, but we're going to have to figure
what, but we're going to have to figure
something out.
We have to do mana skill from source.
for Manny. I left soda their stuff
for Manny. I left soda their stuff
first.
to run this for longer. Eh,
this gives you a new metric that wasn't
this gives you a new metric that wasn't
here before.
That's once
and
If it's more stable at least.
Apply to total agents
Apply to total agents
across all workers need be less than or
across all workers need be less than or
equal to segments
equal to segments
batch size.
batch size.
So Spencer, the way it works, you have a
So Spencer, the way it works, you have a
number of agents
number of agents
and then there's a number of steps
and then there's a number of steps
you're collecting effectively per each
you're collecting effectively per each
agent
agent
and then that has to be at least the
and then that has to be at least the
batch size, right? Like that has to be
batch size, right? Like that has to be
the batch size.
the batch size.
It's like it's just two numbers or like
It's like it's just two numbers or like
one number is the product of two number
one number is the product of two number
two other numbers. So if you set three
two other numbers. So if you set three
numbers that don't make any sense, then
numbers that don't make any sense, then
it doesn't make any sense.
it doesn't make any sense.
Welcome BDF.
Welcome BDF.
We're doing some robotics. Robotics
We're doing some robotics. Robotics
doesn't make any sense. And I think it's
doesn't make any sense. And I think it's
because people have screw reward
because people have screw reward
functions.
functions.
We will see
because according to this
because according to this
in terms of reward, we're shredding. But
in terms of reward, we're shredding. But
then actually what happens is success
then actually what happens is success
can go down even though reward goes up.
Uh, I think it's that they have too much
Uh, I think it's that they have too much
reward shaping in this instance That
130k using a Mac CPU. Um,
130k using a Mac CPU. Um,
I think that's roughly right. Let's put
I think that's roughly right. Let's put
it this way. It's about 20 times faster
it this way. It's about 20 times faster
than you would get using a high-end GPU
than you would get using a high-end GPU
on anything except Puffer Lib.
I run this for 20 mil. Damn it.
uh let me see can we control worker
uh let me see can we control worker
let's say I want 20 worker training
let's say I want 20 worker training
parallel
total CPU so that means that you're
total CPU so that means that you're
bottlenecked by the training op. So, no,
bottlenecked by the training op. So, no,
you can't you can't do um
you can't you can't do um
at least I don't think that you can do
at least I don't think that you can do
CPU parallel training. You can do GPU
CPU parallel training. You can do GPU
parallel training. You can technically
parallel training. You can technically
if you want you could try to like make
if you want you could try to like make
our distributed data parallel work
our distributed data parallel work
multicore. That's kind of a weird thing
multicore. That's kind of a weird thing
to do. Um it's basically you're just
to do. Um it's basically you're just
getting bound by the training operation
getting bound by the training operation
which is just kind of expected because
which is just kind of expected because
you're training on CPU.
you're training on CPU.
Did you fix 2048?
Did you fix 2048?
Um, sort of. We fixed a couple of things
Um, sort of. We fixed a couple of things
and then sent it back. It'll return to
and then sent it back. It'll return to
sender for edits.
sender for edits.
We did fix asteroids. We fixed meta
We did fix asteroids. We fixed meta
today as well. Meta now works perfectly.
today as well. Meta now works perfectly.
And uh, we're working on robotic stuff.
CPU is not going to be maxed out unless
CPU is not going to be maxed out unless
we're running. Yeah. So, it's
we're running. Yeah. So, it's
essentially your CPU. You're looking at
essentially your CPU. You're looking at
the you're looking at your dashboard or
the you're looking at your dashboard or
you're looking at like top or something.
you're looking at like top or something.
But the thing is CPUs get used mostly
But the thing is CPUs get used mostly
for the environment. And I don't know
for the environment. And I don't know
what Torch does for training. it
what Torch does for training. it
probably doesn't do like multi-core CPU
probably doesn't do like multi-core CPU
proper training correctly. Um,
proper training correctly. Um,
but you shouldn't ever be bottlenecked
but you shouldn't ever be bottlenecked
by the CPU when you're running fast ends
by the CPU when you're running fast ends
if you're running GPU training, if that
if you're running GPU training, if that
makes sense. You shouldn't have to scale
makes sense. You shouldn't have to scale
up like you shouldn't be trying to max
up like you shouldn't be trying to max
your CPU because you should get bound by
your CPU because you should get bound by
GPU normally.
Okay, this is 97%.
I have
I have
256 gigs of V RAM and VRAM. Yeah, that
256 gigs of V RAM and VRAM. Yeah, that
just doesn't matter in RL is the thing.
just doesn't matter in RL is the thing.
You need just pure compute. Like
You need just pure compute. Like
it's not a language model where you're
it's not a language model where you're
like RAM bound. You're always going to
like RAM bound. You're always going to
be you're always like you're always
be you're always like you're always
computebound, not memory bound.
try some of the other environments like
try some of the other environments like
you're currently with 130K you're
you're currently with 130K you're
literally running RL 20 times faster
literally running RL 20 times faster
than uh those M's run on GPU and other
than uh those M's run on GPU and other
libraries but like it's you're running
libraries but like it's you're running
like I don't know 5% of the speed of a
like I don't know 5% of the speed of a
GPU because your CPU is 5% of the speed
GPU because your CPU is 5% of the speed
of a GPU
Oh, I guess it could be a dense reward
Oh, I guess it could be a dense reward
thing technically.
Maybe not. Maybe this is an oversight.
and check their implementation.
Thank you.
Next value.
final values. Yes.
Next values is what is
Do I need to register an end on properly
Do I need to register an end on properly
before training for a custom thing? So,
before training for a custom thing? So,
for a custom thing, follow the uh the
for a custom thing, follow the uh the
tutorial. Like,
tutorial. Like,
if you want to use our like just our out
if you want to use our like just our out
of the box trainer, then yeah, the M's
of the box trainer, then yeah, the M's
have to be in there. If you want to like
have to be in there. If you want to like
hack it up and use like the trainer as
hack it up and use like the trainer as
an API thing, then no, it's not like a
an API thing, then no, it's not like a
gym.register, though. It's just like the
gym.register, though. It's just like the
ocean environments just inside of
ocean environments just inside of
environment. Just add one line.
That's in the docs, right?
That's in the docs, right?
I don't think I forgot that.
I don't think I forgot that.
See if I forgot that.
Did I say add a line? Yeah, add a line
Did I say add a line? Yeah, add a line
to ocean environment.py.
Just at the bottom of there.
Yeah. Switch farming reward.
We add
final values.
because that's an annoying thing to fix.
because that's an annoying thing to fix.
That's like quite difficult to fix
That's like quite difficult to fix
actually.
Not easy at all.
I mean to think about at least it's
I mean to think about at least it's
probably a twoline change but it doesn't
probably a twoline change but it doesn't
make it
How can I train more time instead of
How can I train more time instead of
default
default
train until it reaches a certain min
train until it reaches a certain min
reward
reward
for each that that as you've stated it
for each that that as you've stated it
doesn't make sense. You can increase the
doesn't make sense. You can increase the
number of training steps though. You can
number of training steps though. You can
essentially just increase the number of
essentially just increase the number of
training steps in any of the configs
training steps in any of the configs
or from command
Is that a reasonable thing to Okay.
I got to look at clean's original.
Drives me nuts. This is always like such
Drives me nuts. This is always like such
an awkward
an awkward
uh implementation.
uh implementation.
Everybody does it wrong.
should be built 10.
Weird
docs does not have how to set the number
docs does not have how to set the number
of steps for training using args. d-help
of steps for training using args. d-help
will give you all of the valid
will give you all of the valid
arguments. And you'll see that there's a
arguments. And you'll see that there's a
total time steps argument.
See
if what I did makes any sense.
I think it does, right?
We'll see if it breaks other ends.
Yeah, I think that this works for this,
Yeah, I think that this works for this,
but it doesn't make sense for like
but it doesn't make sense for like
literally anything That's
Oh, do we still break?
Yeah, it still breaks. Never mind.
Uh that's for if you don't have enough
Uh that's for if you don't have enough
GPU memory for observations, which in
GPU memory for observations, which in
practice is never the case.
If I just do n rewards equals reward
not breaking that will set.
Wait, what? Star and
Wait, what? Star and
I think that should set the first
I think that should set the first
element of the array
element of the array
to the reward. I think
to the reward. I think
for a single agent env. I think that's
for a single agent env. I think that's
correct.
correct.
It's so much easier to just do end
It's so much easier to just do end
rewards of zero equals reward though. I
rewards of zero equals reward though. I
don't know why you would do that. No,
don't know why you would do that. No,
it's not rewriting the whole pointer.
That's something else. That would be end
That's something else. That would be end
rewards equals reward
rewards equals reward
without the star. Great. That would
without the star. Great. That would
write the pointer.
write the pointer.
Don't want to do
and I let me be I'll be right back and
and I let me be I'll be right back and
I'm going to have to think about what to
I'm going to have to think about what to
do about this because they're like
do about this because they're like
they're fundamentally treating the
they're fundamentally treating the
reward signal and the terminal
reward signal and the terminal
conditions as something that it isn't.
conditions as something that it isn't.
So, as expected, robotics very cursed.
So, as expected, robotics very cursed.
I'll be right back.
I've misclicked on mute. Okay, so
I've misclicked on mute. Okay, so
I think I figured it out. And uh it's
I think I figured it out. And uh it's
literally that all the roboticists are
literally that all the roboticists are
just doing their rewards wrong.
just doing their rewards wrong.
I'm pretty sure that's what it is, which
I'm pretty sure that's what it is, which
is kind of funny, but I wouldn't be
is kind of funny, but I wouldn't be
surprised. And like the thing is they're
surprised. And like the thing is they're
not aware of it because they also like
not aware of it because they also like
they also have their bootstrapping
they also have their bootstrapping
function wrong and the errors cancel
function wrong and the errors cancel
out. But it makes it so that you
out. But it makes it so that you
literally can't use their
literally can't use their
implementations on anything else or
implementations on anything else or
other implementations on their problems
other implementations on their problems
which would give you the effect of
which would give you the effect of
making the problems look very hard when
making the problems look very hard when
they're not.
So let's figure that out.
the reporters that he sent me.
asks.
So this is 209.
Just real quick, put this here. This
Just real quick, put this here. This
here. We'll swap these.
dude. This is negative 10h.
Okay. So now all the rewards are
Okay. So now all the rewards are
negative.
turn.
turn.
Yeah, this goes
So now if this doesn't work then I have
So now if this doesn't work then I have
to think way harder about the math but I
to think way harder about the math but I
think that this should work.
Um BDF env
that's for ocean m specifically and it
that's for ocean m specifically and it
sets the number of copies that are run
sets the number of copies that are run
on each core.
on each core.
So when we do like vecum ms 8 and mnum m
So when we do like vecum ms 8 and mnum m
8,192
8,192
total ms on eight different cores
But we'll see if this still crashes.
But we'll see if this still crashes.
I'm thinking about this and I can't I
I'm thinking about this and I can't I
can't see how this could
can't see how this could
I guess technically some of like the
I guess technically some of like the
reward normalization stuff could mess it
reward normalization stuff could mess it
up still.
up still.
I don't think so though.
This looks good though.
That's ridiculous.
I don't know what to do about this, man.
I don't know what to do about this, man.
It's literally all the rewards are
It's literally all the rewards are
backwards.
backwards.
Like the rewards are all backwards from
Like the rewards are all backwards from
outside of robotics.
That's kind of a big one. A
Try the other task.
Well, that's a crazy thing to discover.
Also,
hey Spencer, how's it
emailing more RL bros. Cool. Yeah,
emailing more RL bros. Cool. Yeah,
Spencer, I just figured out that all of
Spencer, I just figured out that all of
robotics is backwards. That's cool.
You know,
pretty much
it's not one over everything. You just
it's not one over everything. You just
have to delete. They do one minus
have to delete. They do one minus
everything. I guess it's not backwards.
everything. I guess it's not backwards.
It's offset by a coefficient that
It's offset by a coefficient that
totally screws you.
You literally just need to subtract one
You literally just need to subtract one
from all their reward components.
This thing is somehow
I think I have to mess with this one a
I think I have to mess with this one a
little more.
little more.
But why though?
But why though?
Because if it's So these are dense
Because if it's So these are dense
rewards. So basically the way they set
rewards. So basically the way they set
up their M there's a goal state and they
up their M there's a goal state and they
have it so that you get a positive goal
have it so that you get a positive goal
a positive reward as you get closer to
a positive reward as you get closer to
the goal.
the goal.
So as a result of that
So as a result of that
uh
uh
as a result of that you can farm reward
as a result of that you can farm reward
by getting close to the goal but never
by getting close to the goal but never
reaching it.
Okay. So, it's still doing the same
Okay. So, it's still doing the same
thing cuz apparently I missed a
thing cuz apparently I missed a
component.
Try this again.
Yeah, that's not how it works. That
return should stay negative here.
return should stay negative here.
I will be surprised if um
if this stays negative and doesn't
if this stays negative and doesn't
solve.
solve.
So before I missed a component so it
So before I missed a component so it
could still farm.
Interesting.
That has zero success so far.
There it goes.
Should
I manually be calling reset on terminal
like this is done? We do not auto reset
like this is done? We do not auto reset
for you. You are respons the environment
for you. You are respons the environment
is responsible for handling its own
is responsible for handling its own
resets.
This is true in all ends.
At least here it's actually the reward
At least here it's actually the reward
is well aligned now, right?
Return gets better, success goes up.
Return gets better, success goes up.
That was not the case before.
rap look like.
Oh no, it's still learning.
Oh no, it's still learning.
Yeah, it's totally still learning.
We're fine.
episode return is still improving.
Okay, so this bothers me that this is
Okay, so this bothers me that this is
not solving. It's training,
not solving. It's training,
but it's not solving.
but it's not solving.
We'll have to look at the policy for
We'll have to look at the policy for
this and see what it's doing.
Okay, so that's very close.
It's still farming it.
Yeah, but it's not.
Yeah, but it's not.
Why is this thing not registering is
Why is this thing not registering is
correct?
I thought I'd fix that.
Uh, that's not how it works.
Uh, that's not how it works.
That works a bit better.
Okay.
static reward
place.
got to be the advantage normalization.
what we can do about that.
Hey.
Yeah. Okay. I will not use
Yeah. Okay. I will not use
the sink.
Okay, let me think about this.
Damn it.
Damn it.
This would work without reward without
This would work without reward without
advantage normalization I believe.
Let me think what we can do.
First reward.
Yeah, but the thing is you're not ever
Yeah, but the thing is you're not ever
going to solve it randomly.
going to solve it randomly.
Okay.
I should just make them a better reward
I should just make them a better reward
function.
What I should
Hey, major
You know what I can do? I can do it as a
You know what I can do? I can do it as a
delta.
I can do this entire thing as a delta.
So, what I'm doing here to be clear,
So, what I'm doing here to be clear,
they have this dense reward signal
they have this dense reward signal
defined, but it has this annoying
defined, but it has this annoying
farming property where like get some
farming property where like get some
amount of reward near the goal. You just
amount of reward near the goal. You just
want to hang out around the goal. What
want to hang out around the goal. What
I've done is I've reformulated the
I've done is I've reformulated the
reward to be in terms of a delta from
reward to be in terms of a delta from
the previous. Um, and since it's dense,
the previous. Um, and since it's dense,
this essentially just rewards you for
this essentially just rewards you for
getting closer to the goal state, but
getting closer to the goal state, but
not actually for hanging out there. You
not actually for hanging out there. You
only get rewarded for your actions. You
only get rewarded for your actions. You
don't get rewarded for just chilling in
don't get rewarded for just chilling in
any given state.
any given state.
And we'll see if this makes a
And we'll see if this makes a
difference.
I have a question for you. Can math
I have a question for you. Can math
problems be solved with reinforcement
problems be solved with reinforcement
learning? For example, there's a problem
learning? For example, there's a problem
that starts with an equation solve step
that starts with an equation solve step
by step. Each step includes information
by step. Each step includes information
from the previous one. If this satisfies
from the previous one. If this satisfies
the MVP, then something like an RL
the MVP, then something like an RL
language model that can solve equations
language model that can solve equations
by selecting. So, the problem with RL
by selecting. So, the problem with RL
language models in general is you kind
language models in general is you kind
of just have to light all your money on
of just have to light all your money on
fire. um you kind of have to be like
fire. um you kind of have to be like
Google or Deep Mind to do anything at
Google or Deep Mind to do anything at
reasonable scale.
reasonable scale.
If I were trying to do some RL math
If I were trying to do some RL math
thing, I would look to something like um
thing, I would look to something like um
I would make actions like informal logic
I would make actions like informal logic
and probably have some sort of fast
and probably have some sort of fast
verifier
verifier
if I wanted to get it to like uh you
if I wanted to get it to like uh you
know write proofs or something.
know write proofs or something.
But then that would be tricky because
But then that would be tricky because
you'd basically have to tokenize you
you'd basically have to tokenize you
tokenize math in a much smaller more
tokenize math in a much smaller more
constrained way that doesn't correspond
constrained way that doesn't correspond
to an actual language basically.
Also, I think it would be difficult to
Also, I think it would be difficult to
have multiple solutions
have multiple solutions
give reward for all of them being
give reward for all of them being
correct. No, because you would have a
correct. No, because you would have a
verifier. You need some sort of like
verifier. You need some sort of like
formal proof verifier or something.
These aren't these aren't private
These aren't these aren't private
general RL discussion.
Can we solve this?
This could be hypers now to be fair.
This could be hypers now to be fair.
This could just be hypers.
What is our reward?
Uh, kind of going up
Uh, kind of going up
to say.
Oh, the reward's just way too low at
Oh, the reward's just way too low at
this point. Hang on.
board is like way too low.
This is 100x larger scale. They should
This is 100x larger scale. They should
get to like fun.
not stable.
Word's probably a bit too big here.
Oh yeah, this is too big.
This
This
How do you know if it's actually better
How do you know if it's actually better
when you change the reward function? I'm
when you change the reward function? I'm
looking at the success rate.
looking at the success rate.
The success rate is independent of the
The success rate is independent of the
reward. That's a very important concept.
When you're tuning a RL environment, you
When you're tuning a RL environment, you
always want to have a metric that is
always want to have a metric that is
separate from the scale of the reward
separate from the scale of the reward
that tells you how well you're doing.
that tells you how well you're doing.
So, if it's a game, you look at the
So, if it's a game, you look at the
actual score of the game, which is
actual score of the game, which is
separate from, say, the amount of reward
separate from, say, the amount of reward
you're getting per point that you
just being a little fiddly.
Are these things offset?
Are these things offset?
But I did have them offset.
They're not offset.
Oh, no. They would get offset.
Oh, no. They would get offset.
Yeah, they would get offset.
Not super stable.
Well, it'll be interesting regardless to
Well, it'll be interesting regardless to
see what this agent's actually doing.
It's got to just be idling around the uh
It's got to just be idling around the uh
the solution still somehow,
the solution still somehow,
which how would that even happen?
which how would that even happen?
Wait, how's this how's this reward thing
Wait, how's this how's this reward thing
work?
This is a 0ero to one reward.
Yeah, this thing is now at zero reward.
Nonzero success
thing.
Would like to see what this is.
Sure.
I'm sir.
This just looks like it failing for some
This just looks like it failing for some
reason.
Why would it do that?
Silly robot.
Why the heck does it do that?
Do we try this on um
Do we try this on um
simpler ends or what?
or just like pitters out.
beard.
Not terrible. Huh?
Like reasonable rewards to me.
So weird.
fact that it evens out to like zero
fact that it evens out to like zero
reward as well.
the idx and option. Oh,
the idx and option. Oh,
I see
I see
happening.
I think I can probably
I think I can probably
probably just hack it a little bit.
So I think what was happening is every
So I think what was happening is every
time any end was getting reset it was
time any end was getting reset it was
just resetting all of the uh the states.
Okay,
that's doing something.
Here's a restroom real quick while this
Here's a restroom real quick while this
runs.
We'll see how this goes.
Okay. Well, we're getting um
Okay. Well, we're getting um
values
values
returns jumping all over the place.
We have here
Let me clear
Oh.
Huh.
GG.
GG.
GG. Robotics.
I think we just win robotics now.
Send stone a quick message.
Hey, Spencer.
Hey, Spencer.
Actually, I can share this publicly.
Actually, I can share this publicly.
There's nothing private here.
Yeah, that's GG to robotics though.
Yeah, that's GG to robotics though.
At least for uh the application of
At least for uh the application of
having puffer lib work on these.
This is for reference. This is like
This is for reference. This is like
garbage hyperparameters for this task.
garbage hyperparameters for this task.
Like complete random model. like this is
Like complete random model. like this is
the least optimized possible thing and
the least optimized possible thing and
it works perfectly now that I have fixed
it works perfectly now that I have fixed
the rewards. So, it's literally just
the rewards. So, it's literally just
that uh robotics is trolling with
that uh robotics is trolling with
rewards.
Let me get this thing implemented with
Let me get this thing implemented with
stone fencer and then yeah, we'll be
stone fencer and then yeah, we'll be
able to do it. I got to run it on some
able to do it. I got to run it on some
harder problems, but yeah,
harder problems, but yeah,
it'll be good.
it'll be good.
I think our trainer is just going to be
I think our trainer is just going to be
like the thing is anybody who touches RL
like the thing is anybody who touches RL
like our trainer is just better. So,
like our trainer is just better. So,
and this is kind of why I still want to
and this is kind of why I still want to
do core algorithm work, right? Because
do core algorithm work, right? Because
like it makes stuff like this happen.
like it makes stuff like this happen.
Like no amount of fiddling would do this
Like no amount of fiddling would do this
on on Puffer Lib 2, but now it just
on on Puffer Lib 2, but now it just
works.
works.
This took me literally a day, not even.
And the only reason it didn't work out
And the only reason it didn't work out
of the box is because the rewards were
of the box is because the rewards were
defined incorrectly.
Like robotics apparently has a weird way
Like robotics apparently has a weird way
of defining rewards. It's just not how
of defining rewards. It's just not how
rewards are supposed to work.
No.
That's a GG to robotics.
How many samples does it take them to do
How many samples does it take them to do
this?
Hey Tim,
Hey Tim,
I want to see like how many how many
I want to see like how many how many
more samples it takes us to do it than
more samples it takes us to do it than
them because like
them because like
we're not optimized at all. So if we're
we're not optimized at all. So if we're
in within a factor of like two or three
in within a factor of like two or three
that would be ridiculous. I don't know
that would be ridiculous. I don't know
if we are. I think it's probably like
to be fair, we are in a factor though
to be fair, we are in a factor though
with time like time is about the same.
with time like time is about the same.
So that's not bad.
Yeah, we we can shred this can shred
Yeah, we we can shred this can shred
this super easily.
What we should do is we should run a
What we should do is we should run a
little sweep on this.
I should run like a little sweep on this
I should run like a little sweep on this
task specifically.
be good.
be good.
But the thing is I do have some more
But the thing is I do have some more
time before we have to do this.
I'm going to wait for a stone to uh to
I'm going to wait for a stone to uh to
tell me how I should implement this.
But yeah, pretty darn good robotics
But yeah, pretty darn good robotics
project progress.
All right. What are the other things we
All right. What are the other things we
have to get done around here? Let me
have to get done around here? Let me
think. So, I know I want to start on the
think. So, I know I want to start on the
drone stuff or the uh the tactics the
drone stuff or the uh the tactics the
tactical drone stuff um for most of
tactical drone stuff um for most of
tomorrow.
tomorrow.
So yeah, for folks uh interested, the
So yeah, for folks uh interested, the
end we're going to be messing with
end we're going to be messing with
tomorrow.
I'll show the quick version of it. It's
I'll show the quick version of it. It's
not going to look anywhere near as cool
not going to look anywhere near as cool
as it'll look once you have trained
as it'll look once you have trained
agents on it. You can see like all these
agents on it. You can see like all these
different ships that are kind of
different ships that are kind of
attempting to fly out. They're cars as
attempting to fly out. They're cars as
well. Uh these are random policies, so
well. Uh these are random policies, so
they literally don't know how to fly.
they literally don't know how to fly.
But imagine like a full scale battle
But imagine like a full scale battle
with a thousand ships, a whole bunch of
with a thousand ships, a whole bunch of
different armies. Yeah, we're going to
different armies. Yeah, we're going to
do that as an RLN and actually make this
do that as an RLN and actually make this
thing work tomorrow. It kind of works at
thing work tomorrow. It kind of works at
the moment, but it's not great.
All right. Um,
think what to do.
I kind of just solved their thing super
I kind of just solved their thing super
fast.
And this is one of their easier ends.
Still though, for it to work out of the
Still though, for it to work out of the
box kind of ridiculous. I wonder what
box kind of ridiculous. I wonder what
happens if I just crank update epochs.
happens if I just crank update epochs.
Does that do anything?
We all know what's going to happen here,
We all know what's going to happen here,
right? It gets the puffer treatment and
right? It gets the puffer treatment and
then like as soon as we have the
then like as soon as we have the
speedrun target, it just drops like a
speedrun target, it just drops like a
[ __ ]
if this is data limited or not.
The thing is like robotics tasks from a
The thing is like robotics tasks from a
learning perspective are like really
learning perspective are like really
easy if you think about it. Like you can
easy if you think about it. Like you can
kind of basically flail around and get
kind of basically flail around and get
all the data that you need. That's not
all the data that you need. That's not
true in the rest of RL. Like when people
true in the rest of RL. Like when people
talk about like these really hard
talk about like these really hard
robotics tasks, they're really kind of
robotics tasks, they're really kind of
just data limited. They should not be
just data limited. They should not be
hard tasks.
Okay, so they actually do it pretty
Okay, so they actually do it pretty
quick here. 72 seconds.
We have a little ways to go to beat
We have a little ways to go to beat
that.
It's not going to survive a sweep,
It's not going to survive a sweep,
though. There's no way.
Interesting. It doesn't actually seem
Interesting. It doesn't actually seem
like the uh the additional data helps
like the uh the additional data helps
very much.
You're getting maybe very slightly
You're getting maybe very slightly
faster takeoff.
faster takeoff.
Very slightly.
Okay. Their gamma's kind of insane
Okay. Their gamma's kind of insane
though, right?
We get rid of this
We get rid of this
9. Play with this a little bit, you
9. Play with this a little bit, you
know.
Yeah. So, 3 minutes 40 seconds. We're
Yeah. So, 3 minutes 40 seconds. We're
off by a factor of three, I believe,
off by a factor of three, I believe,
from their policy.
from their policy.
Get them uh by a factor of three.
try making policy bigger next
The return is very low.
The return is very low.
Gets to be reasonable.
kind of need to just figure out how to
kind of need to just figure out how to
implement the other thing because this
implement the other thing because this
negative shouldn't happen. This is just
negative shouldn't happen. This is just
from bad resets and that can mess with
from bad resets and that can mess with
learning.
Maybe we'll do that next.
need the infos though.
need the infos though.
I don't really know how to do this
I don't really know how to do this
the way that their code is.
I think we kind of have to go from here
I think we kind of have to go from here
to uh
to uh
wherever they call this from, which is
wherever they call this from, which is
probably in the basin.
This is worse. Okay.
Try
That
Okay. So you compute your delta reward
Okay. So you compute your delta reward
There.
Let's
see what their
see what their
their ve rapper does.
Fanny skill vector rapper gymnasium.
Okay. So it calls self.reset
which then calls en reset
with options.
with options.
Okay. Okay. And then this gets passed.
The options is Nvid.
The options is Nvid.
So they directly will call reset
So they directly will call reset
which returns observations.
Weird that there's a action in the
Weird that there's a action in the
reward function.
Oh, it's not used.
on this.
Yeah, it's a little tricky.
Yeah, it's a little tricky.
I think what we're going to have to do,
I think what we're going to have to do,
we have to recmp compute all of them
we have to recmp compute all of them
now.
This works.
Now we need to set last reward in the
Now we need to set last reward in the
inette.
be cool.
Presume there's a self num ms
but this is this castle.
Oh, we also need to set the reward type,
Oh, we also need to set the reward type,
don't we?
Okay.
Ah,
Oh, there is a self device.
Try this.
Well, the return looks a lot more
Well, the return looks a lot more
reasonable now.
See?
Oh, yeah. That's way better.
Think negative is very weird.
on reset.
Oh yeah, we look at that. That's a big
Oh yeah, we look at that. That's a big
difference. Way more consistent return.
difference. Way more consistent return.
What was happening before, right, is as
What was happening before, right, is as
soon as you reset the environment,
soon as you reset the environment,
you get the full penalty
you get the full penalty
equal pretty much to your original uh
equal pretty much to your original uh
you get the full penalty of like going
you get the full penalty of like going
from the goal to the start, which is
from the goal to the start, which is
pretty much your whole reward. So, it
pretty much your whole reward. So, it
was screwing up our curves.
was screwing up our curves.
Now, as to whether or not this will do
Now, as to whether or not this will do
anything for training or make it worse,
anything for training or make it worse,
who knows? because reinforcement
who knows? because reinforcement
learning. But at least we have something
learning. But at least we have something
way more reasonable
and we should have a return that should
and we should have a return that should
be perfect perfectly correlated with
be perfect perfectly correlated with
success now.
Yeah. So minor tiny little pickup here
Yeah. So minor tiny little pickup here
but overall perfect.
We'll just do a couple quick little
We'll just do a couple quick little
things
just to see if we are like missing any
just to see if we are like missing any
major stuff and then we'll see if we can
major stuff and then we'll see if we can
get sweeps to just cover it.
Delta report based on replied
Yeah, this thing doesn't seem to be
Yeah, this thing doesn't seem to be
doing very much. Eh,
doing very much. Eh,
returns super low.
I don't know what I expected to be fair.
I don't know what I expected to be fair.
That was like their set of params, but
That was like their set of params, but
those don't really make much sense.
Let's go back to 32
Let's go back to 32
horizon and let's just go to 16.
Can't we just move? Hang on.
Can't we just move? Hang on.
I just realized something.
I can just do this.
I literally just do this and it works
I literally just do this and it works
for all ends.
This is doing
discernable difference.
Do I believe
Do I believe
that uh 60 degree of freedom or whatever
that uh 60 degree of freedom or whatever
robot
robot
can only run 40k SPS?
can only run 40k SPS?
This thing's got to be able to run like
This thing's got to be able to run like
a millie, right?
a millie, right?
I don't know about humanoids, but I
I don't know about humanoids, but I
think this thing should be able to run
think this thing should be able to run
way faster than this.
way faster than this.
Way faster,
I would hope.
I would hope.
try the bigger batch after this because
try the bigger batch after this because
who knows maybe counterintuitively it's
who knows maybe counterintuitively it's
better
actually that's dependent on the M's
actually that's dependent on the M's
isn't it never mind
Which M's do you think would be the most
Which M's do you think would be the most
aggressive
aggressive
experiments for making gamma less
experiments for making gamma less
cooked?
Honestly,
Honestly,
we'd have to come up with a synthetic
we'd have to come up with a synthetic
one.
one.
We'd probably what we would do is
we'd like make a periodic function I
we'd like make a periodic function I
think.
think.
So imagine that the reward is like the
So imagine that the reward is like the
sum of a bunch of waves, right? Like you
sum of a bunch of waves, right? Like you
have something where you get a reward
have something where you get a reward
every two like every time step. Then
every two like every time step. Then
there's something that's like you can
there's something that's like you can
get a reward every 10 time steps if you
get a reward every 10 time steps if you
do the right thing 100 time steps
do the right thing 100 time steps
thousand like something like that.
thousand like something like that.
And then the idea is that probably gamma
And then the idea is that probably gamma
prevents you doing multiscale learning
prevents you doing multiscale learning
as well as you'd want. It kind of
as well as you'd want. It kind of
depends though.
The other thing you could do would just
The other thing you could do would just
literally be to be to like make an N
literally be to be to like make an N
that has a configurable reward density,
that has a configurable reward density,
right? And then like show that like,
right? And then like show that like,
hey, you can change the reward density
hey, you can change the reward density
and then gamma has to change with it.
and then gamma has to change with it.
And then you could try to patch the
And then you could try to patch the
algorithm so that it doesn't need that.
It's tough. Very tough.
Good.
There's no way. I just happen to guess
There's no way. I just happen to guess
the optimal,
the optimal,
right?
right?
Ah, there's got to be way more wiggle
Ah, there's got to be way more wiggle
room in here.
Interesting.
Expecting this to be
Expecting this to be
in some way.
Kind of a latching problem, right?
before Horizon seems better though
about it.
I don't see how this gamma makes any
I don't see how this gamma makes any
sense. Do you?
sense. Do you?
40step problem.
40step problem.
I guess it's because the rewards are
I guess it's because the rewards are
like so
like so
the rewards are so dense.
the rewards are so dense.
Like every single step is basically
Like every single step is basically
local optimal, right?
Oh, actually with the delta you don't
Oh, actually with the delta you don't
even have to do credit assignment,
even have to do credit assignment,
right?
Hang on. If you don't even have to do
Hang on. If you don't even have to do
credit assignment, then like
credit assignment, then like
Huh?
Huh?
Wait, isn't this totally trivial?
reduces to almost a bandit problem with
reduces to almost a bandit problem with
a dense reward.
Yeah,
let me go back to
Go back to this.
Just want to see something.
Just want to see something.
I doubt that this will match my
I doubt that this will match my
intuition, but like this is basically a
intuition, but like this is basically a
bandit problem.
bandit problem.
Do you see what I mean?
Do you see what I mean?
Like if you have a perfectly dense
Like if you have a perfectly dense
reward that's not un like that's
reward that's not un like that's
perfectly informative, then it's
perfectly informative, then it's
literally a bandit problem. It should be
literally a bandit problem. It should be
trivial.
Somehow isn't.
Okay.
You can go the other way.
If it's the opposite of my intuition,
If it's the opposite of my intuition,
why not?
And after this, we're just going to go
And after this, we're just going to go
set up a sweep.
Oh, I guess I should You know what?
Oh, I guess I should You know what?
Instead of trying this, cuz this will
Instead of trying this, cuz this will
get covered by the sweep automatically.
Try this instead.
Try this instead.
Make sure it's not policy bound.
Make sure it's not policy bound.
Shouldn't be, but never know.
lower so far.
I liken this to Pong.
Not even right.
It probably just needs a sweep. Like
It probably just needs a sweep. Like
this is such an incredibly easy task
this is such an incredibly easy task
when you think about it
when you think about it
with this reward signal.
still lower.
At the very least, you don't just
At the very least, you don't just
immediately win by increasing the policy
immediately win by increasing the policy
size without changing anything else.
Yeah, you can see this is like so much
Yeah, you can see this is like so much
slower.
slower.
Well,
Well,
we'll let the empirical thing
we'll let the empirical thing
uh we'll let the empirical thing just do
uh we'll let the empirical thing just do
whatever it's going to.
Okay,
Okay,
that will run
that will run
uh and by tomorrow we will have ideas.
What is this?
Okay. Well,
how are you all doing on YouTube?
how are you all doing on YouTube?
You kind of just solved everything I
You kind of just solved everything I
wanted to do today. I thought that was
wanted to do today. I thought that was
going to take like multiple days.
going to take like multiple days.
Obviously, we still have sweeps,
Obviously, we still have sweeps,
sweeps and stuff to figure out, but
sweeps and stuff to figure out, but
we're in a pretty good spot with uh the
we're in a pretty good spot with uh the
robotics integrations at the moment.
robotics integrations at the moment.
Anyone have any questions? Any pressing
Anyone have any questions? Any pressing
concerns?
Use star of the puffer
Use star of the puffer
helps me out a lot. Free
is nice.
I'm pretty sure. Yeah, this is so
I'm pretty sure. Yeah, this is so
tomorrow I guess the plan will be that
tomorrow I guess the plan will be that
we check in on our robotics. I'll
we check in on our robotics. I'll
probably check on this later tonight to
probably check on this later tonight to
make sure, you know, we're actually
make sure, you know, we're actually
getting something out of the sweep.
getting something out of the sweep.
And uh I'll chat with Stone for a little
And uh I'll chat with Stone for a little
bit on how we do
how we integrate all this
how we integrate all this
and you know, we'll start running more
and you know, we'll start running more
of the uh more of the tasks in here.
of the uh more of the tasks in here.
But yeah, other than that, uh we will
But yeah, other than that, uh we will
work on the battle, like the large scale
work on the battle, like the large scale
battle ends. And I guess we'll see how
battle ends. And I guess we'll see how
crazy I go on that.
crazy I go on that.
I think probably I'm just going to
I think probably I'm just going to
downscale it to like a hundred something
downscale it to like a hundred something
agents, two teams to start with, just
agents, two teams to start with, just
because it'll make it quick to iterate
because it'll make it quick to iterate
on and uh yeah, we'll go from there
on and uh yeah, we'll go from there
after that.
after that.
That will be the plan.
That will be the plan.
But there's nothing else from any of you
But there's nothing else from any of you
then. Now, I think I will uh I'll call
then. Now, I think I will uh I'll call
it early. I got a couple emails to
it early. I got a couple emails to
respond to, a couple documents to read,
respond to, a couple documents to read,
and then yeah, after that,
and then yeah, after that,
uh I'm going to get some dinner, get
uh I'm going to get some dinner, get
some rest, and I'll be back bright and
some rest, and I'll be back bright and
early in the morning. So, thanks for
early in the morning. So, thanks for
tuning in, folks. See you tomorrow.
