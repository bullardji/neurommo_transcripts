Kind: captions
Language: en
should be
live Let's
live Let's
see we got some stuff to do
today take a quick double check this is
today take a quick double check this is
just yeah like group chats uh let's take
just yeah like group chats uh let's take
a quick look
a quick look
through see what's new here and then
through see what's new here and then
we'll get started on some Dev
holy okay
so we got Xander
M I think I'll do this
review hi how's it going
welcome just catching up real quick on
welcome just catching up real quick on
what I've missed since yesterday here
what I've missed since yesterday here
and we'll see what I'm going to do at
and we'll see what I'm going to do at
the
the
moment
uh okay betc got Blaster
here this is just set up
some silly formatting
errors some support
errors some support
okay I I will get this working very
okay I I will get this working very
easily
okay this is just them trying to figure
okay this is just them trying to figure
out how to do
out how to do
sweeps uh new video from BET on
Blaster it's funny that it like goes up
Blaster it's funny that it like goes up
to like Ram stuff
I think I just saw a
bug wait four
bug wait four
five did you see that oh maybe that one
five did you see that oh maybe that one
just has more Health that one look any
just has more Health that one look any
different yeah that one does look
different yeah that one does look
different okay so maybe that one just
different okay so maybe that one just
has more health
cool I think we're set here uh I look at
cool I think we're set here uh I look at
Sanders and at the
Sanders and at the
moment and we'll see how that goes
okay so we have this open
okay so we have this open
cool and where
is yeah it's this one
what's the latest
here two days
here two days
ago wait did he push this
oh yeah no it would be two days ago
oh yeah no it would be two days ago
right yeah okay this is
right yeah okay this is
good so this is going to 2.0 ddev that
good so this is going to 2.0 ddev that
is
is
fine one change
requested it's
requested it's
fine and let's take a look and see how
fine and let's take a look and see how
this
works that's a good thing to do with the
works that's a good thing to do with the
morning
morning
it's
let's see what Xander
added um
s
fault
well that shouldn't even be
possible we literally have
possible we literally have
tests we have this sanitizer it
tests we have this sanitizer it
shouldn't even be able to seg fault
um
okay no big deal
well now it doesn't
EGA here's the
end we definitely need to make like some
end we definitely need to make like some
sort of little grid
sort of little grid
engine at some
point I'm fine having people do their
point I'm fine having people do their
own like dedicated things for now
though these appear to be
random so if I run
random so if I run
this oh you know what the seg fault
this oh you know what the seg fault
might just be on my end I have some
might just be on my end I have some
weird display things
weird display things
um that's fine this seems like it
works cool so let's take a look um first
works cool so let's take a look um first
of
of
all these are I believe these are just
all these are I believe these are just
the normal rib dependency seg fold I
the normal rib dependency seg fold I
mean M leaks these are fine so good job
mean M leaks these are fine so good job
there what are you doing I'm currently
there what are you doing I'm currently
uh reviewing and integrating a new
uh reviewing and integrating a new
environment in two lines
environment in two lines
I'm integrating a new reinforcement
I'm integrating a new reinforcement
learning environment made by one of our
learning environment made by one of our
contributors at the
moment let's see if we can watch
moment let's see if we can watch
it it's like this collection like trash
it it's like this collection like trash
like pickup and deposit
environment new gy m not quite gy
environment new gy m not quite gy
compatible with gym yes but these are in
compatible with gym yes but these are in
puffer API so these will run about a
puffer API so these will run about a
thousand times faster than uh anything
thousand times faster than uh anything
that you see over there if you haven't
that you see over there if you haven't
seen our demos you can play all of them
seen our demos you can play all of them
right
right
here here's a MOA and we have all these
here here's a MOA and we have all these
different environments they're all Ultra
different environments they're all Ultra
Ultra
Ultra
fast
fast
snake lots of fun stuff
let me add
him yeah it's this guy's end xanders
oh he
oh he
actually wait use free trained okay this
actually wait use free trained okay this
is set to false so he has the model but
is set to false so he has the model but
it's not trained and integrated it
seems right so can I just
seems right so can I just
do first of all
okay hupper was it trash pick
up there you
go okay so we have some speed issues
go okay so we have some speed issues
here we'll see
why basically new puffer M should never
why basically new puffer M should never
ever have 20% time spent on the
ever have 20% time spent on the
environment like this so this should be
environment like this so this should be
like 800 like 750 800 steps per second
like 800 like 750 800 steps per second
train and with optimization we should be
train and with optimization we should be
able to get this to a
million CU know an easy way to
million CU know an easy way to
implement from
rayb so standard seems Independent
rayb so standard seems Independent
Learning an easy way to match IDs to
Learning an easy way to match IDs to
policies
uh what is
uh what is
it
really oh that's you oh cool um yeah so
really oh that's you oh cool um yeah so
we we're planning that stuff for January
we we're planning that stuff for January
um you're welcome to help actually if
um you're welcome to help actually if
you'd like uh we have it basically we
you'd like uh we have it basically we
had something that we implemented that's
had something that we implemented that's
just not being used and hasn't been
just not being used and hasn't been
updated at the moment because like it
updated at the moment because like it
was an layer and we weren't really using
was an layer and we weren't really using
it for much but like when you're
it for much but like when you're
training at 500,000 to a million steps
training at 500,000 to a million steps
per second a lot of these problems that
per second a lot of these problems that
seem hard are just suddenly very very
seem hard are just suddenly very very
easy uh now if you have like a research
easy uh now if you have like a research
reason for fundamentally wanting to do
reason for fundamentally wanting to do
separate policies and stuff that's
separate policies and stuff that's
totally understandable and uh there are
totally understandable and uh there are
ways to hack it in the current
ways to hack it in the current
implementation and then there are you
implementation and then there are you
know things we can do to support things
know things we can do to support things
uh better that and that will be coming
uh better that and that will be coming
in
in
January you know like one thing you can
January you know like one thing you can
do is just re shape the batch in the
do is just re shape the batch in the
forward pass right and then send it
forward pass right and then send it
through different sub networks that's a
through different sub networks that's a
really easy way of doing
really easy way of doing
stuff definitely take a look at our
stuff definitely take a look at our
environment code though
environment code though
like things can be made much much
like things can be made much much
simpler oh I also just shipped yesterday
simpler oh I also just shipped yesterday
a much easier way if you're trying to
a much easier way if you're trying to
Port like a python environment to puffer
Port like a python environment to puffer
uh I added a much easier way to do that
uh I added a much easier way to do that
and still get a lot of our speed UPS not
and still get a lot of our speed UPS not
as much as in see but still get a lot of
as much as in see but still get a lot of
the speed UPS
you can look at Ocean slpy squared now
you can look at Ocean slpy squared now
for the python version of our squared
for the python version of our squared
test
environment need to get into this not
environment need to get into this not
computer scientists just applying RL for
computer scientists just applying RL for
Logistics oh if you're applying RL to
Logistics oh if you're applying RL to
Logistics then I can almost guarantee
Logistics then I can almost guarantee
you that whatever problem is not going
you that whatever problem is not going
to need like fancy separate policy stuff
to need like fancy separate policy stuff
um if you're in Academia you know we're
um if you're in Academia you know we're
very happy to support stuff for free all
very happy to support stuff for free all
puffers free and open source uh if you
puffers free and open source uh if you
have a if you're at a company that is
have a if you're at a company that is
trying to do this stuff and having
trying to do this stuff and having
trouble I would also point you to uh we
trouble I would also point you to uh we
do have service packages on our
do have service packages on our
website but all our stuff's also free
website but all our stuff's also free
and open source so you're also just more
and open source so you're also just more
than welcome to hang around and ask
than welcome to hang around and ask
questions and use stuff as it
questions and use stuff as it
is that basically just gets you custom
is that basically just gets you custom
features if you need them for your
features if you need them for your
specific
specific
application eyes on your problem
Etc is this
working I can't tell if this is training
working I can't tell if this is training
finishing PhD a okay then free
support we're very happy to support
support we're very happy to support
um
um
Academia I just finished my PhD up
Academia I just finished my PhD up
myself uh last spring
yeah okay so Xander I don't think that
yeah okay so Xander I don't think that
this thing is quite working the way you
this thing is quite working the way you
would expect it to be working let's
would expect it to be working let's
figure out why that
figure out why that
is
is
ctde is common approach multi-agent I
ctde is common approach multi-agent I
would say uh it is the common approach
would say uh it is the common approach
but it's overbuilt you really don't need
but it's overbuilt you really don't need
it in a lot of cases my specialty in my
it in a lot of cases my specialty in my
PhD was
PhD was
multi-agent and generally IPO is just
multi-agent and generally IPO is just
tremendously effective when applied
tremendously effective when applied
quickly so if you have a slow Sim then
quickly so if you have a slow Sim then
maybe you look into fancy algorithms but
maybe you look into fancy algorithms but
if you like if you're doing stuff in
if you like if you're doing stuff in
logistics for which you can build a fast
logistics for which you can build a fast
simulator it's going to be tremendously
simulator it's going to be tremendously
more effective to just apply the simple
more effective to just apply the simple
method really really
method really really
fast um for contact my
fast um for contact my
PhD I built what is probably the most
PhD I built what is probably the most
complex multi-agent set of simulators
complex multi-agent set of simulators
out there like this is the newest
out there like this is the newest
version of it this this is a full MMO
version of it this this is a full MMO
with like an inbuilt economy system
with like an inbuilt economy system
leveling items equipment like thousand
leveling items equipment like thousand
different agents in the same environment
different agents in the same environment
uh they run for like hours and hours
uh they run for like hours and hours
we've trained policies for like 2,000
we've trained policies for like 2,000
years worth of simulations in this thing
years worth of simulations in this thing
so you know I can tell
so you know I can tell
you a thing or two about multi-agent
learning and I am very much aware that
learning and I am very much aware that
CT uh ctde is very common it was even
CT uh ctde is very common it was even
common back when I started this stuff in
common back when I started this stuff in
like
like
2018 but uh a lot of it can be replaced
2018 but uh a lot of it can be replaced
just by making thing go
fast and it's a lot simpler to do that
fast and it's a lot simpler to do that
as well
the other thing is
the other thing is
ctde doesn't even imply separate
ctde doesn't even imply separate
policies um you can still have the same
policies um you can still have the same
policy per agent and then just share the
policy per agent and then just share the
value function that's a little bit
value function that's a little bit
easier I would suggest looking to that
easier I would suggest looking to that
if
if
anything maybe something to consider and
anything maybe something to consider and
it's also really for team based stuff uh
it's also really for team based stuff uh
where you train where you train that as
where you train where you train that as
well it's not really for adverse areial
well it's not really for adverse areial
or like free-for-all settings
all right let's see what Xander
all right let's see what Xander
did four agents 1024
environments okay so he did this
correctly batch mini batch
correctly batch mini batch
oo is this mini batch
oo is this mini batch
good I think this is actually good right
I forget what the max is before GPU
I forget what the max is before GPU
starts to
degrade I think I tested this okay so it
degrade I think I tested this okay so it
is 32k so he's fine
report interval is too big or too
small so that doesn't make it any faster
small so that doesn't make it any faster
it does make the metrix slow to report
532 okay I don't know what's up with
532 okay I don't know what's up with
this report
this report
interval I guess it's cuz all the MS are
interval I guess it's cuz all the MS are
synced up how old are you search
27 newly minted MIT PhD finished last
27 newly minted MIT PhD finished last
spring now working full-time on making
spring now working full-time on making
reinforcement learning actually work
properly and building a small company
properly and building a small company
around
around
it all three an open source code
though let's see what Xander did here
though let's see what Xander did here
something
screwy he messed with this parameter
screwy he messed with this parameter
which I'd advise not doing but he didn't
which I'd advise not doing but he didn't
mess with it very
mess with it very
much he was running sweeps so presumably
much he was running sweeps so presumably
it's not that
wait is this bat size
sketchy 4096 no I think that's still
sketchy 4096 no I think that's still
that's 16 or
32 uh
32 uh
130 wait oops
130 wait oops
one 2 over 32 no not over that what is
one 2 over 32 no not over that what is
it
over uh 4096
over uh 4096
yeah 32 I'm right I'm fine uh that
yeah 32 I'm right I'm fine uh that
should be fine for this
environment okay I'm a Lambda look
reasonable is the curve going up at
all let's pull up
logs welcome YouTube
logs welcome YouTube
folks we're currently integrating uh
folks we're currently integrating uh
Xander's new environment
Xander's new environment
I do full code reviews and I help with
I do full code reviews and I help with
Integrations for all all contributed
Integrations for all all contributed
environments so if you're looking to get
environments so if you're looking to get
into RL it's a really great way to get
into RL it's a really great way to get
started we generally have people like
started we generally have people like
start off by building the new end it's a
start off by building the new end it's a
really good way to get familiar with
really good way to get familiar with
puffer and all the stuff that we work on
puffer and all the stuff that we work on
and then I slowly onboard people onto
and then I slowly onboard people onto
the science side uh using their
the science side uh using their
experience with building at least one
experience with building at least one
environment
environment
themselves let's see if we can f figure
themselves let's see if we can f figure
out what's going on here
out what's going on here
so need a few more data
points come
on SO loss is
on SO loss is
here this is fine this is
here this is fine this is
fine entropy is kind of
fine entropy is kind of
low value is good value is very good
low value is good value is very good
suspiciously good
even so nothing sketchy
there this just looks random with
there this just looks random with
variant to
me did Xander hold on I thought that he
me did Xander hold on I thought that he
committed some
committed some
stuff I thought he said that he had this
stuff I thought he said that he had this
working
it's working I
it's working I
think yeah he has it
here local
2D pushed everything
up try to find tomorrow h
all
settings this one
doesn't do you have a only used with
doesn't do you have a only used with
local 2D crop but I don't see a bull
local 2D crop but I don't see a bull
here
I don't even see the uh the option in
here 2D crop
space
um I don't see anything egregious in
um I don't see anything egregious in
here so
here so
far what policy is he
far what policy is he
using custom policy
okay so you don't need to do
okay so you don't need to do
this because your model is literally the
this because your model is literally the
default here so this is just going to
default here so this is just going to
[Music]
[Music]
be uh whatever it is
recurrent that's just redundant that
recurrent that's just redundant that
won't fix
won't fix
anything and then what you do here you
anything and then what you do here you
have a
have a
cons a lot of
channels uh I don't see anything
channels uh I don't see anything
terrible here
either oh is it
learning wait what was the x- axis in
learning wait what was the x- axis in
this thing
maybe it is learning
yeah that's like
weird yeah that's
weird yeah that's
bizarre
bizarre
um cuz this is a very simple
um cuz this is a very simple
environment probably there's some stuff
wrong for
okay so it is
okay so it is
learning so I suspect then there's just
learning so I suspect then there's just
something crazy going on with uh with
something crazy going on with uh with
his policy
lines very simple end logic it looks
lines very simple end logic it looks
like oh no cuz he's called okay he made
like oh no cuz he's called okay he made
like sub function for St that's
like sub function for St that's
fine
so total
so total
trash not collected
there's no reward
there's no reward
here where do he put the where did he
here where do he put the where did he
put stuff message is too long we'll Post
put stuff message is too long we'll Post
in Discord
in Discord
yeah I'm pretty responsive
there I mean this is mainly the point of
there I mean this is mainly the point of
streaming right is that I can like
streaming right is that I can like
engage with people that are looking to
engage with people that are looking to
use stuff in puffer have RL
use stuff in puffer have RL
problems um merge contributor code I
problems um merge contributor code I
also do the dev like my own personal Dev
also do the dev like my own personal Dev
live just because why not
but you know people do watch it we've
but you know people do watch it we've
got four on YouTube One on Twitch at the
moment the peak has been
moment the peak has been
20 take a while to get back to that
okay so here's this compute observation
okay so here's this compute observation
function this is what I wanted to see so
function this is what I wanted to see so
num cell types empty trash being the
num cell types empty trash being the
agent right
iterate oversight
iterate oversight
range check if cell is within bounds
one hot in
code
code
um wait OBS of OBS index
Plus+ I don't think this a one hot
encoding I think he just has the OBS
wrong cuz this is like you're setting
wrong cuz this is like you're setting
the current index to one or to zero but
the current index to one or to zero but
you're
you're
not okay I think the encoding is scy
not okay I think the encoding is scy
here
see what you did here come
on
485 so what is
485 so what is
um agent Site range times
um agent Site range times
2 agents okay so this is a crop with
2 agents okay so this is a crop with
four channels so that actually is the
four channels so that actually is the
correct one hot encoding
correct one hot encoding
Dimension what is the agent Site
Dimension what is the agent Site
range is it five by
range is it five by
default yeah so self observations 0 do
shape
um well wait a second that's
um well wait a second that's
40 self. num OBS oh plus one
okay so this
okay so this
is
is
zero that looks weird to me
right hold on
oh I found your thing I think my use
oh I found your thing I think my use
case CD of makes sense because why
case CD of makes sense because why
should I train different policies for
should I train different policies for
the same agv
the same agv
type so this is not what ctde means
type so this is not what ctde means
though you can train the same policy
though you can train the same policy
without doing
ctde I this is what ippo is which is
ctde I this is what ippo is which is
just standard po you share the weights
just standard po you share the weights
for the same policy and you apply them
for the same policy and you apply them
independently to each agent in the scene
independently to each agent in the scene
ctde sometimes has separate policies
ctde sometimes has separate policies
sometimes has the same one the defining
sometimes has the same one the defining
feature of ctde is typically that you're
feature of ctde is typically that you're
sharing at least the value function uh
sharing at least the value function uh
during the course of training which can
during the course of training which can
be useful for Cooperative tasks but
be useful for Cooperative tasks but
we've had plenty of success without
we've had plenty of success without
bothering doing
bothering doing
that Independent Learning if I had yeah
that Independent Learning if I had yeah
so I think that you
so I think that you
have I think that you've got your
have I think that you've got your
definitions backwards
definitions backwards
um and
um and
actually the like assigning policies to
actually the like assigning policies to
agent it's doing the opposite of what
agent it's doing the opposite of what
you want in R alib right now and the
you want in R alib right now and the
thing that puffer is doing if you have
thing that puffer is doing if you have
the same uh agv type is the thing that
the same uh agv type is the thing that
you want and is the simpler thing and is
you want and is the simpler thing and is
the faster thing so I think that you
the faster thing so I think that you
just win here
Logistics is a great area is a great
Logistics is a great area is a great
area to apply RL though
let me see how he structures
OBS Marcel yeah so what we do first of
OBS Marcel yeah so what we do first of
all I PPO poo is the same thing people
all I PPO poo is the same thing people
just are making meaningless distinctions
just are making meaningless distinctions
the simplest most common way to apply Po
the simplest most common way to apply Po
in a multi-agent setting is to just
in a multi-agent setting is to just
treat each agent independently like it's
treat each agent independently like it's
from a different environment but using
from a different environment but using
the same policy so the same policy is
the same policy so the same policy is
used independently to compute the
used independently to compute the
actions for each agent this is the same
actions for each agent this is the same
thing as just contena all the agents
thing as just contena all the agents
into the batch Dimension um this is
into the batch Dimension um this is
tremendously effective in a wide variety
tremendously effective in a wide variety
of cases now technically for cooperative
of cases now technically for cooperative
problems right what you can do is
problems right what you can do is
centralize the value function so you can
centralize the value function so you can
take all of the outputs from the
take all of the outputs from the
different agents and use them in a
different agents and use them in a
single- shared value function but there
single- shared value function but there
are some subtleties that you have to
are some subtleties that you have to
take into account when doing that to get
take into account when doing that to get
it correct which at least as far I as I
it correct which at least as far I as I
have seen are generally not worth it um
have seen are generally not worth it um
and I think that the vast vast majority
and I think that the vast vast majority
of problems you will just solve
of problems you will just solve
immediately uh with our basic po given
immediately uh with our basic po given
how fast it is
how fast it is
if you train 100 agents agent one
if you train 100 agents agent one
learned something from agent two it's
learned something from agent two it's
the same network there is one policy
the same network there is one policy
it's independently controlling each
it's independently controlling each
agent right so you have one neural
agent right so you have one neural
network that neural network computes the
network that neural network computes the
actions for agent one computes the
actions for agent one computes the
actions for agent two computes actions
actions for agent two computes actions
for agent 3 independently so this can be
for agent 3 independently so this can be
deployed you know in plenty of real
deployed you know in plenty of real
world settings so pretty much you know
world settings so pretty much you know
this is the standard way you do stuff in
this is the standard way you do stuff in
every real world setting really you're
every real world setting really you're
training one policy you're using all the
training one policy you're using all the
data right to train that one policy so
data right to train that one policy so
it's very
it's very
effective
effective
um yeah you're not training 100 separate
um yeah you're not training 100 separate
policies this is that's not ctde though
policies this is that's not ctde though
ctde is ctde is something
different ctte
different ctte
so
here okay so it depends heavily on your
here okay so it depends heavily on your
algorithm but for like PO for for
algorithm but for like PO for for
instance right the main thing that
instance right the main thing that
you're training for uh reward
you're training for uh reward
stabilization is this value function so
stabilization is this value function so
you have like your policy and then you
you have like your policy and then you
get like
action ATN not
action ATN not
ant and then you get
ant and then you get
reward reward right uh not reward hold
reward reward right uh not reward hold
on Val
on Val
value okay and then every single agent
value okay and then every single agent
is going to produce these values which
is going to produce these values which
is just like a b Dimension okay so you
is just like a b Dimension okay so you
just take this you get your agent data
just take this you get your agent data
right across all your
right across all your
agents right and then what you can do is
agents right and then what you can do is
you can either get a whole bunch of
you can either get a whole bunch of
value
value
functions a whole bunch of separate
functions a whole bunch of separate
values this is if you do it
values this is if you do it
independently but what ctde does is it
independently but what ctde does is it
takes like agent One agent two agent
takes like agent One agent two agent
three agent four and they each are going
three agent four and they each are going
to produce their own actions
but then they all produce one
but then they all produce one
value that's
value that's
ctde uh and the reason this is ctde is
ctde uh and the reason this is ctde is
because you only need the value function
because you only need the value function
at train time so you're training them
at train time so you're training them
like together in aggregate on this one
like together in aggregate on this one
value
value
function but then when you deploy the
function but then when you deploy the
policy you don't care about the value
policy you don't care about the value
function so that they still operate
function so that they still operate
independently
and this is like kind of a useful idea
and this is like kind of a useful idea
and it's been useful in a number of
and it's been useful in a number of
cases but like vast majority of
cases but like vast majority of
multi-agent problems you don't even need
multi-agent problems you don't even need
this if you just get a fast Sim and a
this if you just get a fast Sim and a
good implementation of Po you'll solve
good implementation of Po you'll solve
it the simpler way and save yourself a
it the simpler way and save yourself a
lot of
lot of
headaches you train centralized one
headaches you train centralized one
policy and
decentralized if that's how people are
decentralized if that's how people are
referring to it then they're mucking
referring to it then they're mucking
with the definitions
it makes far more sense to look at the
it makes far more sense to look at the
the value function specifically because
the value function specifically because
like it really doesn't matter if you
like it really doesn't matter if you
train one policies or n policies like
train one policies or n policies like
that has no implications on anything um
that has no implications on anything um
the value function
the value function
specifically is interesting because it's
specifically is interesting because it's
you only need it during training time
you only need it during training time
which is why can centralize the training
which is why can centralize the training
of the value
of the value
function and decentralize the
function and decentralize the
execution you're not train you're not
execution you're not train you're not
centralized training one policy though
centralized training one policy though
that's that's a different thing that's
that's that's a different thing that's
if you were to have one policy with like
if you were to have one policy with like
let's say five agents on your team it
let's say five agents on your team it
has five different action heads and it's
has five different action heads and it's
jointly Computing actions that's not
jointly Computing actions that's not
what is happening here it is
what is happening here it is
independently Computing actions for each
agent people people muck with the
agent people people muck with the
definitions uh especially in multi-agent
definitions uh especially in multi-agent
like if that's how they have it defined
like if that's how they have it defined
that's how they have it defined but
that's how they have it defined but
that's not a useful
that's not a useful
definition because it's like it's
definition because it's like it's
focused on it's focused on something
focused on it's focused on something
that's completely irrelevant which is
that's completely irrelevant which is
whether like whether you're training one
whether like whether you're training one
set of weights or multiple set of
set of weights or multiple set of
Weights is completely
Weights is completely
irrelevant um the fashion of whether you
irrelevant um the fashion of whether you
are jointly Computing actions and
are jointly Computing actions and
jointly Computing values or
jointly Computing values or
independently Computing actions jointly
independently Computing actions jointly
Computing values that is the thing that
matters is that the new multi-agent RL
book centralized
training yeah but you're Miss you're
training yeah but you're Miss you're
missing the key of the the value
missing the key of the the value
function being shared like it's not
function being shared like it's not
centralized training because you're
centralized training because you're
training one policy like you're always
training one policy like you're always
doing that anyways because you're
doing that anyways because you're
sharing data across parallel copies of
sharing data across parallel copies of
the environment right like you have
the environment right like you have
different instances with different like
different instances with different like
agents anyways that's just a batch
Dimension the relevance right is that if
Dimension the relevance right is that if
you jointly compute actions then you
you jointly compute actions then you
cannot deploy the policy in dependently
cannot deploy the policy in dependently
you like each agent cannot just take its
you like each agent cannot just take its
own data source you need all the data
own data source you need all the data
for all the agents to deploy it but if
for all the agents to deploy it but if
you only centralize the value function
you only centralize the value function
then you can train centralized and then
then you can train centralized and then
deploy it decentralized right that's the
deploy it decentralized right that's the
key
distinction like I said people might
distinction like I said people might
have mucked with the definition what is
have mucked with the definition what is
this is this the new moral book
oh yeah this was the new I saw this and
oh yeah this was the new I saw this and
didn't like
it
right no no no it's not independent
right no no no it's not independent
policies I me and the thing is like this
policies I me and the thing is like this
is like completely I mean this is like
is like completely I mean this is like
so so out of touch if that's what they
so so out of touch if that's what they
do because nobody does that
yeah so I saw this thing and I saw the
yeah so I saw this thing and I saw the
table of contents uh this is insane like
table of contents uh this is insane like
this is 10 times more math than is ever
this is 10 times more math than is ever
relevant and like a tenth of the
relevant and like a tenth of the
engineering and like implementation
engineering and like implementation
detail that actually matters so
detail that actually matters so
yeah I uh I don't necessarily recommend
yeah I uh I don't necessarily recommend
oh look at this they actually have right
oh look at this they actually have right
here
here
well this is actually very funny because
well this is actually very funny because
these are very trivial environments all
these are very trivial environments all
of these but
328 where is
it they literally
have see this rware environment we have
have see this rware environment we have
a port of this in C that's like shorter
a port of this in C that's like shorter
than the original or whatever you can
than the original or whatever you can
play it online look here this is trained
play it online look here this is trained
with this is trained with puffer lib
with this is trained with puffer lib
with our default
with our default
policy with nothing fancy
policy with nothing fancy
whatsoever this runs like a thousand
whatsoever this runs like a thousand
times faster than the original as
well look how nicely they get out of the
well look how nicely they get out of the
way for each other and like go find new
way for each other and like go find new
uh new boxes this has trained in like a
uh new boxes this has trained in like a
few minutes on one GPU very simple
few minutes on one GPU very simple
policy very simple po
I didn't like the book because first of
I didn't like the book because first of
all it's this is like these are not
all it's this is like these are not
complex environments at all so these are
complex environments at all so these are
like trivial trivial tasks um they did a
like trivial trivial tasks um they did a
ton of math on trivial tasks which is
ton of math on trivial tasks which is
completely irrelevant like you can solve
completely irrelevant like you can solve
all of these trivially with like the
all of these trivially with like the
simplest methods you can think of and
simplest methods you can think of and
they completely omit any of the actually
they completely omit any of the actually
more complex T tasks can I place
more complex T tasks can I place
obstacles there and the Agents can
obstacles there and the Agents can
Collide uh the agents can Collide I
Collide uh the agents can Collide I
don't know if this one has obstacles I
don't know if this one has obstacles I
Les but like generally these things are
Les but like generally these things are
very trivial to
very trivial to
implement like rware is not one of our
implement like rware is not one of our
more complex environments at all like
more complex environments at all like
rware is um okay to be fair the rware
rware is um okay to be fair the rware
original environment has this really
original environment has this really
obnoxious like redundant graph algorithm
obnoxious like redundant graph algorithm
in it for no reason uh if you cut that
in it for no reason uh if you cut that
out then it is a very very simple
out then it is a very very simple
environment and you can Implement
environment and you can Implement
anything like this you want uh for your
anything like this you want uh for your
specific use case in like a
specific use case in like a
day and have it be a thousand times
day and have it be a thousand times
faster than uh than anything that's
faster than uh than anything that's
being mentioned here like I know all of
being mentioned here like I know all of
these or almost all these environments
these or almost all these environments
hanab is cool overcooked has literally
hanab is cool overcooked has literally
the worst code base I've ever seen uh
the worst code base I've ever seen uh
this thing is about probably a 100 times
this thing is about probably a 100 times
100 thousand times slower than it should
100 thousand times slower than it should
be smack is cool but unfortunately
be smack is cool but unfortunately
really really slow because it's stuck to
really really slow because it's stuck to
the Blizzard engine engine uh it's
the Blizzard engine engine uh it's
insulting to call MP a complex
insulting to call MP a complex
environment that's hilarious and then
environment that's hilarious and then
yeah this is fine but I wouldn't call it
yeah this is fine but I wouldn't call it
complex open Spiel is um board games so
complex open Spiel is um board games so
not necessarily I guess kind of
not necessarily I guess kind of
multi-agent and then petting zoo yeah
multi-agent and then petting zoo yeah
they have some
they have some
um yeah I don't know it's not like this
um yeah I don't know it's not like this
is bad content it's just like out of
is bad content it's just like out of
touch for how this work is actually done
touch for how this work is actually done
now I can change grid and add walls I
now I can change grid and add walls I
don't know if we have walls but like
don't know if we have walls but like
adding walls would be like a probably 30
adding walls would be like a probably 30
line code change
line code change
uh what you're looking for is kind of
uh what you're looking for is kind of
like a grid based engine we've had a
like a grid based engine we've had a
couple people mess with that I tried to
couple people mess with that I tried to
mess with that a little bit you can
mess with that a little bit you can
definitely change the grid easily that's
definitely change the grid easily that's
just a custom map that's trivial uh I
just a custom map that's trivial uh I
can ask Spencer about walls that
can ask Spencer about walls that
shouldn't be
hard I mean here I'll show you like for
hard I mean here I'll show you like for
any of these environments
any of these environments
right here let me show you cuz I think
right here let me show you cuz I think
you've been misled a little bit uh
you've been misled a little bit uh
you've been I think you've been a little
you've been I think you've been a little
bit misled
here I'm not mad at you I'm mad at like
here I'm not mad at you I'm mad at like
people over complicating stuff for
people over complicating stuff for
newcomers to like I don't know why
newcomers to like I don't know why
people over complicate this so much I
people over complicate this so much I
need to know C okay but have you
need to know C okay but have you
actually read any of our C it's like the
actually read any of our C it's like the
easiest C you've ever
seen
um okay okay so this the reason this is
um okay okay so this the reason this is
800 lines is entirely because uh Spencer
800 lines is entirely because uh Spencer
also qued this completely redundant
also qued this completely redundant
graph algorithm uh from the original
graph algorithm uh from the original
rware environment you can literally just
rware environment you can literally just
ignore that like this whole like process
ignore that like this whole like process
cycle movement and stuff is like
cycle movement and stuff is like
completely redundant
completely redundant
so yeah the code looks like this it's
so yeah the code looks like this it's
literally like ifs it's just
literally like ifs it's just
conditionals and
Loops it's literally just conditionals
Loops it's literally just conditionals
and loops
and loops
so like it's not any harder to write
so like it's not any harder to write
this than it is to write python I wrote
this than it is to write python I wrote
python for 10 years within 3 weeks I was
python for 10 years within 3 weeks I was
just as comfortable writing this now I
just as comfortable writing this now I
think this is probably
easier I am not a systems
easier I am not a systems
Guru yeah so this whole thing this whole
Guru yeah so this whole thing this whole
block of code is irrelevant all the
block of code is irrelevant all the
cycle stuff is irrelevant in fact this
cycle stuff is irrelevant in fact this
trash pickup end is
trash pickup end is
probably simpler to look cuz this say
probably simpler to look cuz this say
600 lines and doesn't have all the fancy
600 lines and doesn't have all the fancy
stuff so this is login code and then the
stuff so this is login code and then the
actual code for the environment we've
actual code for the environment we've
got compute
got compute
observations we've got Place random
observations we've got Place random
entities we've got move
agent this is also more complicated than
agent this is also more complicated than
it should be to be fair let me see if I
it should be to be fair let me see if I
find you like one of my one of the mems
find you like one of my one of the mems
that I wrote cuz like we also we have
that I wrote cuz like we also we have
people from like a variety of different
people from like a variety of different
like engineering skill levels on these
like engineering skill levels on these
things
things
um let me think probably my snake
um let me think probably my snake
environment would be a nice a nice one
environment would be a nice a nice one
for you so this thing is only 350 lines
for you so this thing is only 350 lines
for multi-agent snake um including this
for multi-agent snake um including this
logging there
logging there
initialization there's your observation
initialization there's your observation
function right there here's like adding
function right there here's like adding
and deleting snakes very nice little
and deleting snakes very nice little
Snippets of
Snippets of
code there's spawning food there's the
code there's spawning food there's the
environment
environment
reset this this is the step function
reset this this is the step function
it's like one screen of code little more
it's like one screen of code little more
than one screen of code and that's it
than one screen of code and that's it
rest of it's
rest of it's
rendering not so bad
rendering not so bad
right I had this in Python before and
right I had this in Python before and
it's like about the same length as it
it's like about the same length as it
would be in
Python we've got sample environments and
Python we've got sample environments and
tons of people on the Discord to help as
tons of people on the Discord to help as
well
I've also been uh converting major RL
I've also been uh converting major RL
Labs uh to this as well um we've got a
Labs uh to this as well um we've got a
collab with
collab with
NYU uh we've got one with
NYU uh we've got one with
MIT and uh one with Oxford as
well on this stuff so I'm not just
well on this stuff so I'm not just
making this up like this is I promise
making this up like this is I promise
you this is very very much simpler and
you this is very very much simpler and
easier than the other stuff out
there like something like this
there like something like this
environment here you can train this at a
environment here you can train this at a
million steps per second and just
million steps per second and just
automatically Solve
IT dictionary with agent IDs nope do not
IT dictionary with agent IDs nope do not
do that if you do that you are screwed
do that if you do that you are screwed
because python dictionaries are slow but
because python dictionaries are slow but
we have an easier way for you look here
we have an easier way for you look here
I can even show you you can even start
I can even show you you can even start
prototyping in Python without going to
prototyping in Python without going to
see
see
and it'll still be faster than uh than
and it'll still be faster than uh than
RL to start
RL to start
with so if I just look up squared Pi squ
with so if I just look up squared Pi squ
I added this yesterday this is a pure
I added this yesterday this is a pure
python version of a very simple test
python version of a very simple test
environment right
environment right
so uper Pi
squ so here this is a very simple little
squ so here this is a very simple little
environment that I wrote
environment that I wrote
this is just a random policy playing it
this is just a random policy playing it
this whole thing is 100 lines of code
this whole thing is 100 lines of code
not even in Python it is way faster than
not even in Python it is way faster than
the uh the rlb API reason being it is a
the uh the rlb API reason being it is a
vector
vector
implementation so this buffer here is
implementation so this buffer here is
going to actually be uh is going to give
going to actually be uh is going to give
you a nice slice of shared memory and
you a nice slice of shared memory and
then when you write stuff into
then when you write stuff into
observations you're going to be writing
observations you're going to be writing
directly into shared memory I get a m
directly into shared memory I get a m
ion steps per train a million steps per
ion steps per train a million steps per
second training even in pure python of
second training even in pure python of
course that's only for very simple M as
course that's only for very simple M as
the M gets more complicated that's going
the M gets more complicated that's going
to dip and that's when you think about
to dip and that's when you think about
going to C but you can prototype this in
going to C but you can prototype this in
Python work exactly as you'd like to
Python work exactly as you'd like to
that'll probably be a very nice way for
that'll probably be a very nice way for
you to um to try out puffer as well and
you to um to try out puffer as well and
I can promise you it'll be faster than
I can promise you it'll be faster than
uh than RL Li because that library is
uh than RL Li because that library is
hideously
slow you'll probably end up solving your
slow you'll probably end up solving your
python uh your problem in pure python
python uh your problem in pure python
before you even have to touch C and then
before you even have to touch C and then
you can go to C if you want it to be
you can go to C if you want it to be
really
fast uh well step just takes oh yeah I
fast uh well step just takes oh yeah I
didn't answer your original question so
didn't answer your original question so
uh it just takes a an array of actions
uh it just takes a an array of actions
so it's a vector API so if you've got 10
so it's a vector API so if you've got 10
agents then you just give it you know an
agents then you just give it you know an
array with 10 numbers which are the
array with 10 numbers which are the
actions we also handle like multi-
actions we also handle like multi-
discreete spaces if you need those um um
discreete spaces if you need those um um
but yeah it's just a vector
API if you have to like dynamically add
API if you have to like dynamically add
and remove agents that's a little bit
and remove agents that's a little bit
more annoying but we do support that
more annoying but we do support that
there's like an extra mask field that
there's like an extra mask field that
you can update to like mask out dead
agents so there you
go I hope this helps I I'm saying this
go I hope this helps I I'm saying this
as somebody who wasted like I literally
as somebody who wasted like I literally
wasted a thousand hours of my time
wasted a thousand hours of my time
during my PhD trying to make R work in
during my PhD trying to make R work in
like old versions of neural MMO is a
like old versions of neural MMO is a
terrible Library I've tried many many
terrible Library I've tried many many
times to help them with it um you know
times to help them with it um you know
the devs they really were trying with it
the devs they really were trying with it
it's just like the way that they built
it's just like the way that they built
that library is not the way that things
that library is not the way that things
work in
work in
RL
RL
yeah I think it'll it'll take a lot less
yeah I think it'll it'll take a lot less
time as well than you might think to do
time as well than you might think to do
this type of stuff because like if
this type of stuff because like if
you're used to rlb they have a giant
you're used to rlb they have a giant
blob of code where nothing makes any
blob of code where nothing makes any
sense right and you have to really like
sense right and you have to really like
look at the Docks and then fiddle with
look at the Docks and then fiddle with
stuff and whatever we have a tiny tiny
stuff and whatever we have a tiny tiny
amount of Code by comparison and it's
amount of Code by comparison and it's
faster it is simpler and it works on a
faster it is simpler and it works on a
huge range of problems we have all sorts
huge range of problems we have all sorts
of high performance environments built
of high performance environments built
in this is what I'm doing fulltime right
in this is what I'm doing fulltime right
I'm making RL not suck to work in
I got to get back to this trash pickup M
I got to get back to this trash pickup M
because I want to make sure I get uh
because I want to make sure I get uh
some progress on this let me see what
some progress on this let me see what
the heck is going on with
this and don't let the C scare you it's
this and don't let the C scare you it's
not
scary like if you took one systems
scary like if you took one systems
course is n underr you're
good heck we have brand new programmers
good heck we have brand new programmers
writing this stuff as well not this
writing this stuff as well not this
particular M but
others why is
it what's this extra OB
observation so the first scaler is this
observation so the first scaler is this
carrying status I
carrying status I
guess okay
one wall
on one is trash
apparently I
apparently I
is I'm screwy
here so what did he do out of bound
here so what did he do out of bound
cells or negative one
okay I wouldn't suggest that but fine
is this really how it is like this line
is this really how it is like this line
of trash here I doubt that
right actually hold on EMP no empty is
something's not right here something's
something's not right here something's
definitely not right
definitely not right
here with the observations
so wait four you Loop over
so wait four you Loop over
agents you Loop over sight
range and
then I don't understand how this
works OBS of of ob's
index hold on you know
index hold on you know
what if this is
what if this is
wrong then I would think
gotcha
gotcha
Xander the Ops are
wrong
gotcha so uh I see exactly what he did
gotcha so uh I see exactly what he did
so was Zander did here is he made the
so was Zander did here is he made the
observations for one hot so like he
observations for one hot so like he
added four channels uh but then in the
added four channels uh but then in the
code he's not actually filling it in by
code he's not actually filling it in by
channels so he's filling in like the
channels so he's filling in like the
first quarter of the data and then the
first quarter of the data and then the
rest of it's
rest of it's
empty ti
empty ti
ti um so that's why it doesn't
work I guess I
work I guess I
will should I just fix it for him or
will should I just fix it for him or
should I message him let me think I got
should I message him let me think I got
got half an
hour I kind of sometimes you know after
hour I kind of sometimes you know after
I've done a few PRS I kind of like to
I've done a few PRS I kind of like to
just fix people
just fix people
stuff shows I'm not
stuff shows I'm not
[Music]
[Music]
bullshitting uh let me see if I can just
bullshitting uh let me see if I can just
piix it real quick for
him every once in a
while uh let me see
while uh let me see
so this is not a Time 4
buddy uh Time
for num cell
types compute
observations oh wait is
he did he wait no actually did he do it
he did he wait no actually did he do it
right
right
here hold
here hold
on
what then why
is this laid out correctly in
is this laid out correctly in
memory I can see the observations are
memory I can see the observations are
wrong here
maybe only for one
maybe only for one
agent what did he
do I did I missed this Loop when I was
do I did I missed this Loop when I was
looking at
it get out of here bot
um s types
oh hold on so if I do
oh hold on so if I do
zero
zero
2 three
what if it's
this that's not it
either okay so if it's
either okay so if it's
this mostly
empty
rash I might have just been being stupid
rash I might have just been being stupid
hold
hold
on B is not oh no there is a
on B is not oh no there is a
Ben agents
by 11
by4 I thought I'd I thought I saw it and
by4 I thought I'd I thought I saw it and
it looked pretty obvious but I didn't
it looked pretty obvious but I didn't
see that there was a
see that there was a
weird there was a weird Loop in a way I
weird there was a weird Loop in a way I
wasn't expecting
let me try in the in the start of the
let me try in the in the start of the
network
[Music]
then see what the heck is wrong with
then see what the heck is wrong with
this thing
put it
here oh I think I see it
is this
is this
right yeah so this is an 11 by 11 that's
right yeah so this is an 11 by 11 that's
supposed to be one hunted right you can
supposed to be one hunted right you can
see this data is completely screwy and
see this data is completely screwy and
the reason is uh
the reason is uh
permute he messed this up so it's like
permute he messed this up so it's like
this
this
um per mute
empty let's try
this I think I forgot to remove the
this I think I forgot to remove the
break
break
point maybe not
[Music]
h h
no what you mean no
tnm local
tnm local
crop uhhuh
five at Stride three three at Stride one
five at Stride three three at Stride one
is
good I actually see he was trying to
good I actually see he was trying to
print observations here I don't think
print observations here I don't think
I'm wrong though
like something's very screw with
like something's very screw with
this
meow combine features
I guess that just means more stuff is
broken let this run for a bit while I
broken let this run for a bit while I
figure out
so that was definitely an issue like the
so that was definitely an issue like the
observations were definitely
observations were definitely
screwy so yeah that would completely
screwy so yeah that would completely
make sense then like why it would take
make sense then like why it would take
forever to learn
forever to learn
anything
uh so we setting rewards and DS
uh so we setting rewards and DS
correctly
correctly
here uh
add
reward well that doesn't make any
reward well that doesn't make any
sense
for for
crash trash not
collected something with the recepts I
collected something with the recepts I
wonder
what does he do when you resarch the
M NS the grid
carrying does he know the entities
is okay yeah he overwrites them
is okay yeah he overwrites them
so that should be
fine what's up with these rewards being
fine what's up with these rewards being
so screwy
though for
it doesn't make any sense for it to be
it doesn't make any sense for it to be
so
variable across uh so many environments
see if it's
see if it's
this buter no this is
correct so I can't fault him for most of
correct so I can't fault him for most of
this well something screwy
here has he played it
there's no human play mode here
oh no he does have
this left
shift oh he doesn't have a um a no op is
shift oh he doesn't have a um a no op is
wi so you have to
like I think you have to like hold the
like I think you have to like hold the
puffer to
it okay so I'm the red
puffer what you can push the trash
bin hold on I didn't realiz you can push
bin hold on I didn't realiz you can push
the trash
bin it's
amusing total episode reward is Tiny
amusing total episode reward is Tiny
though I don't know what that
is that doesn't seem to be
is that doesn't seem to be
working for
okay so it gets 0 five everyone deposits
okay so it gets 0 five everyone deposits
right
that seems
that seems
better let's see if I can finish the
better let's see if I can finish the
environment and let's see if it resets
environment and let's see if it resets
cleanly cuz that's the other common
cleanly cuz that's the other common
another common RL bug is like you play
another common RL bug is like you play
your environment but you don't actually
your environment but you don't actually
check to make sure it resets
cleanly it's funny how you can actually
cleanly it's funny how you can actually
grief by pushing it into a
grief by pushing it into a
corner welcome YouTube
corner welcome YouTube
folks currently debugging this new
folks currently debugging this new
contributor RL
contributor RL
environment it's kind of nice um just
environment it's kind of nice um just
not
not
working ideally
yet does it not
yet does it not
reset hang
reset hang
on do it only reset at uh
on do it only reset at uh
steps I would break
it yeah I know they're holding it but it
it yeah I know they're holding it but it
says craft collected 20 out of
says craft collected 20 out of
20 okay so it does
reset oh
reset oh
well hold
on that's
on that's
bizarre
right so it does solve
it but it's just weirdly slow
yeah so they literally all have to uh
yeah so they literally all have to uh
they all have to drop the trash
in I bet you that's
in I bet you that's
the the
issue e
what did he have like 1.5k steps or
what did he have like 1.5k steps or
something
um the hell why don't I have did they
um the hell why don't I have did they
change their UI
H this is
gross what the
where's the freaking thing to change the
where's the freaking thing to change the
axes
now what is wrong with these
people x axis
this doesn't do anything now
okay so 500
okay so 500
steps so he had like 1.5k
before so I think I may this thing 3x
before so I think I may this thing 3x
faster Maybe
yeah more because this is 2,000 steps
yeah more because this is 2,000 steps
okay so I had it um now it's at like 500
okay so I had it um now it's at like 500
for
solve let's see what this does
I'm still not happy with the 500
though this is 400 million steps which
though this is 400 million steps which
is way too much for this environment it
is way too much for this environment it
shouldn't take this long to get off the
shouldn't take this long to get off the
ground though maybe they have bad whoops
ground though maybe they have bad whoops
camera
camera
fro
fro
um hey welcome YouTube folks
um hey welcome YouTube folks
first stream over 10 viewers in a
first stream over 10 viewers in a
while
um so this was just like observations
um so this was just like observations
were uh incorrectly
were uh incorrectly
formatted this is a bad transpose on the
formatted this is a bad transpose on the
observations um causing this to not work
before here you can see this one going
before here you can see this one going
up fast
up fast
now let's see how long this takes now
now let's see how long this takes now
this one's not necessarily expected to
this one's not necessarily expected to
get to 20 because it doesn't have enough
get to 20 because it doesn't have enough
time to solve the whole task uh let me
time to solve the whole task uh let me
explain my reasoning here so if you give
explain my reasoning here so if you give
it 500 steps right uh it's going to get
it 500 steps right uh it's going to get
most of the trash early on and then
most of the trash early on and then
there's going to be like some puffers
there's going to be like some puffers
holding some trash or whatever and if
holding some trash or whatever and if
the environment doesn't reset everyone
the environment doesn't reset everyone
else is just there twiddling their
else is just there twiddling their
thumbs uh waiting for new opportunities
thumbs uh waiting for new opportunities
to get reward so this is just like a
to get reward so this is just like a
really obnoxious learning
really obnoxious learning
Dynamic um in general
Dynamic um in general
and the way to solve this immediately is
and the way to solve this immediately is
to just like add a trash back randomly
to just like add a trash back randomly
every time one is deposited that will
every time one is deposited that will
make it you know way
make it you know way
quicker so I will let Xandra know this
quicker so I will let Xandra know this
let me see how well this does though
first oh it actually does solve it haha
first oh it actually does solve it haha
in 100
in 100
steps let's watch that policy
actually
actually
1,400 let's wait till we get a new
checkpoint 19.7
checkpoint 19.7
[Music]
and we'll watch this policy back and
and we'll watch this policy back and
then that'll be uh the morning stream
then that'll be uh the morning stream
for us with this environment
for us with this environment
fixed good feedback for Xander I'll push
fixed good feedback for Xander I'll push
these changes you'll all be free to try
these changes you'll all be free to try
it as
well okay 16004
pick up mode
pick up mode
eval um
Native 6 C
that's a little
fast I'll just uh for
fast I'll just uh for
now I'll just add a little
delay for
we wait for this to finish and then I'm
we wait for this to finish and then I'm
going to send him a nice gift
it's a nice policy
hello
hello
nerd how's it
going nerd Ru man
going nerd Ru man
[Music]
solve 200 M
steps compute OB
steps compute OB
bations uh
what mute no was it
what mute no was it
your see had uh observations
awesome good to see us dropping by
stream for
push this for Xander
that's like the really satisfying policy
that's like the really satisfying policy
to watch for whatever reason as
well what just
well what just
happened oh they're
happened oh they're
stuck that's so funny okay so there are
stuck that's so funny okay so there are
some degenerate cases where they get
some degenerate cases where they get
stuck that's so funny
that's reinforcement learning for
you it's really cool to see stuff like
you it's really cool to see stuff like
this
this
work it's like really basic and yeah you
work it's like really basic and yeah you
could totally just write a scripted
could totally just write a scripted
method to do this but still cool
[Music]
[Music]
last thing I'm going to do
background if steps were to complete
background if steps were to complete
were part of the score do you think it
were part of the score do you think it
would develop more complex coordinates
would develop more complex coordinates
so there are less collisions uh it
so there are less collisions uh it
implicitly is there is a penalty for
implicitly is there is a penalty for
taking longer implicit in the RL
taking longer implicit in the RL
algorithm hey Tim yeah this is a
algorithm hey Tim yeah this is a
contributor and had a couple bugs just
contributor and had a couple bugs just
fixed
fixed
them uh hold on trash this will be puff
them uh hold on trash this will be puff
red
C this is fine this is be puff
red
carrying uh
carrying uh
red is this everything
aren't there
lines
yeah pick up
we' give it the
uh puffer
uh puffer
canonicalization uh the lines are not
canonicalization uh the lines are not
good here hold on the lines don't look
good here hold on the lines don't look
very
very
good so let's do this we'll leave his
good so let's do this we'll leave his
light
light
gray and then
huh RL based chess engine
huh RL based chess engine
probably we have good vectorization if
probably we have good vectorization if
nothing
else I got to come up with another color
else I got to come up with another color
for this this is
weird background
weird background
624 so let's do
there we
go code immediately oh I just messed
go code immediately oh I just messed
with
that I think we'll do the other way
that I think we'll do the other way
around as well
this one will be puff
this one will be puff
cyan and this one will be
cyan and this one will be
whoops
red there we go
and then the text has to be PFF
white I don't like the uh
there we
go I don't like that it's like
uh let's
do
do
15 is this like a tint is there a way to
15 is this like a tint is there a way to
just do this as a
just do this as a
tint no there's not
tint no there's not
I think what we can do
is
here there we
here there we
go now it's apply to
tent cool
nice
okay um I
okay um I
am I'm going to go to the gym and get
am I'm going to go to the gym and get
some brunch I will be back streaming
some brunch I will be back streaming
probably for a bit uh later in the
probably for a bit uh later in the
afternoon probably around 34 3:30 or 4:
afternoon probably around 34 3:30 or 4:
uh for new folks
uh for new folks
here this is a project to make
here this is a project to make
reinforcement learning fast and sane and
reinforcement learning fast and sane and
pretty accessible as well it's all free
pretty accessible as well it's all free
and open source you can find more at
and open source you can find more at
puffer doai please start the repository
puffer doai please start the repository
here it helps us out a tremendous
here it helps us out a tremendous
amount uh we also have a discort 867
amount uh we also have a discort 867
members at the moment you can get fast
members at the moment you can get fast
help here if you're interested in doing
help here if you're interested in doing
some RL yourself or using these
some RL yourself or using these
environments or adding some uh
environments or adding some uh
contributing environments is also the
contributing environments is also the
best way to get involved with puffer and
best way to get involved with puffer and
onboard yourself onto this space if
onboard yourself onto this space if
you're new I also post all about RL
you're new I also post all about RL
right here you can get notifications
right here you can get notifications
when I'm live here too cuz I stream on X
when I'm live here too cuz I stream on X
twitch and YouTube
twitch and YouTube
simultaneously see it's all RL content
simultaneously see it's all RL content
not a bunch of
BS and other than that I stream most of
BS and other than that I stream most of
the development and we've got big plans
the development and we've got big plans
for 2025 with this project so thanks for

Kind: captions
Language: en
should be
live Let's
live Let's
see we got some stuff to do
today take a quick double check this is
today take a quick double check this is
just yeah like group chats uh let's take
just yeah like group chats uh let's take
a quick look
a quick look
through see what's new here and then
through see what's new here and then
we'll get started on some Dev
holy okay
so we got Xander
M I think I'll do this
review hi how's it going
welcome just catching up real quick on
welcome just catching up real quick on
what I've missed since yesterday here
what I've missed since yesterday here
and we'll see what I'm going to do at
and we'll see what I'm going to do at
the
the
moment
uh okay betc got Blaster
here this is just set up
some silly formatting
errors some support
errors some support
okay I I will get this working very
okay I I will get this working very
easily
okay this is just them trying to figure
okay this is just them trying to figure
out how to do
out how to do
sweeps uh new video from BET on
Blaster it's funny that it like goes up
Blaster it's funny that it like goes up
to like Ram stuff
I think I just saw a
bug wait four
bug wait four
five did you see that oh maybe that one
five did you see that oh maybe that one
just has more Health that one look any
just has more Health that one look any
different yeah that one does look
different yeah that one does look
different okay so maybe that one just
different okay so maybe that one just
has more health
cool I think we're set here uh I look at
cool I think we're set here uh I look at
Sanders and at the
Sanders and at the
moment and we'll see how that goes
okay so we have this open
okay so we have this open
cool and where
is yeah it's this one
what's the latest
here two days
here two days
ago wait did he push this
oh yeah no it would be two days ago
oh yeah no it would be two days ago
right yeah okay this is
right yeah okay this is
good so this is going to 2.0 ddev that
good so this is going to 2.0 ddev that
is
is
fine one change
requested it's
requested it's
fine and let's take a look and see how
fine and let's take a look and see how
this
works that's a good thing to do with the
works that's a good thing to do with the
morning
morning
it's
let's see what Xander
added um
s
fault
well that shouldn't even be
possible we literally have
possible we literally have
tests we have this sanitizer it
tests we have this sanitizer it
shouldn't even be able to seg fault
um
okay no big deal
well now it doesn't
EGA here's the
end we definitely need to make like some
end we definitely need to make like some
sort of little grid
sort of little grid
engine at some
point I'm fine having people do their
point I'm fine having people do their
own like dedicated things for now
though these appear to be
random so if I run
random so if I run
this oh you know what the seg fault
this oh you know what the seg fault
might just be on my end I have some
might just be on my end I have some
weird display things
weird display things
um that's fine this seems like it
works cool so let's take a look um first
works cool so let's take a look um first
of
of
all these are I believe these are just
all these are I believe these are just
the normal rib dependency seg fold I
the normal rib dependency seg fold I
mean M leaks these are fine so good job
mean M leaks these are fine so good job
there what are you doing I'm currently
there what are you doing I'm currently
uh reviewing and integrating a new
uh reviewing and integrating a new
environment in two lines
environment in two lines
I'm integrating a new reinforcement
I'm integrating a new reinforcement
learning environment made by one of our
learning environment made by one of our
contributors at the
moment let's see if we can watch
moment let's see if we can watch
it it's like this collection like trash
it it's like this collection like trash
like pickup and deposit
environment new gy m not quite gy
environment new gy m not quite gy
compatible with gym yes but these are in
compatible with gym yes but these are in
puffer API so these will run about a
puffer API so these will run about a
thousand times faster than uh anything
thousand times faster than uh anything
that you see over there if you haven't
that you see over there if you haven't
seen our demos you can play all of them
seen our demos you can play all of them
right
right
here here's a MOA and we have all these
here here's a MOA and we have all these
different environments they're all Ultra
different environments they're all Ultra
Ultra
Ultra
fast
fast
snake lots of fun stuff
let me add
him yeah it's this guy's end xanders
oh he
oh he
actually wait use free trained okay this
actually wait use free trained okay this
is set to false so he has the model but
is set to false so he has the model but
it's not trained and integrated it
seems right so can I just
seems right so can I just
do first of all
okay hupper was it trash pick
up there you
go okay so we have some speed issues
go okay so we have some speed issues
here we'll see
why basically new puffer M should never
why basically new puffer M should never
ever have 20% time spent on the
ever have 20% time spent on the
environment like this so this should be
environment like this so this should be
like 800 like 750 800 steps per second
like 800 like 750 800 steps per second
train and with optimization we should be
train and with optimization we should be
able to get this to a
million CU know an easy way to
million CU know an easy way to
implement from
rayb so standard seems Independent
rayb so standard seems Independent
Learning an easy way to match IDs to
Learning an easy way to match IDs to
policies
uh what is
uh what is
it
really oh that's you oh cool um yeah so
really oh that's you oh cool um yeah so
we we're planning that stuff for January
we we're planning that stuff for January
um you're welcome to help actually if
um you're welcome to help actually if
you'd like uh we have it basically we
you'd like uh we have it basically we
had something that we implemented that's
had something that we implemented that's
just not being used and hasn't been
just not being used and hasn't been
updated at the moment because like it
updated at the moment because like it
was an layer and we weren't really using
was an layer and we weren't really using
it for much but like when you're
it for much but like when you're
training at 500,000 to a million steps
training at 500,000 to a million steps
per second a lot of these problems that
per second a lot of these problems that
seem hard are just suddenly very very
seem hard are just suddenly very very
easy uh now if you have like a research
easy uh now if you have like a research
reason for fundamentally wanting to do
reason for fundamentally wanting to do
separate policies and stuff that's
separate policies and stuff that's
totally understandable and uh there are
totally understandable and uh there are
ways to hack it in the current
ways to hack it in the current
implementation and then there are you
implementation and then there are you
know things we can do to support things
know things we can do to support things
uh better that and that will be coming
uh better that and that will be coming
in
in
January you know like one thing you can
January you know like one thing you can
do is just re shape the batch in the
do is just re shape the batch in the
forward pass right and then send it
forward pass right and then send it
through different sub networks that's a
through different sub networks that's a
really easy way of doing
really easy way of doing
stuff definitely take a look at our
stuff definitely take a look at our
environment code though
environment code though
like things can be made much much
like things can be made much much
simpler oh I also just shipped yesterday
simpler oh I also just shipped yesterday
a much easier way if you're trying to
a much easier way if you're trying to
Port like a python environment to puffer
Port like a python environment to puffer
uh I added a much easier way to do that
uh I added a much easier way to do that
and still get a lot of our speed UPS not
and still get a lot of our speed UPS not
as much as in see but still get a lot of
as much as in see but still get a lot of
the speed UPS
you can look at Ocean slpy squared now
you can look at Ocean slpy squared now
for the python version of our squared
for the python version of our squared
test
environment need to get into this not
environment need to get into this not
computer scientists just applying RL for
computer scientists just applying RL for
Logistics oh if you're applying RL to
Logistics oh if you're applying RL to
Logistics then I can almost guarantee
Logistics then I can almost guarantee
you that whatever problem is not going
you that whatever problem is not going
to need like fancy separate policy stuff
to need like fancy separate policy stuff
um if you're in Academia you know we're
um if you're in Academia you know we're
very happy to support stuff for free all
very happy to support stuff for free all
puffers free and open source uh if you
puffers free and open source uh if you
have a if you're at a company that is
have a if you're at a company that is
trying to do this stuff and having
trying to do this stuff and having
trouble I would also point you to uh we
trouble I would also point you to uh we
do have service packages on our
do have service packages on our
website but all our stuff's also free
website but all our stuff's also free
and open source so you're also just more
and open source so you're also just more
than welcome to hang around and ask
than welcome to hang around and ask
questions and use stuff as it
questions and use stuff as it
is that basically just gets you custom
is that basically just gets you custom
features if you need them for your
features if you need them for your
specific
specific
application eyes on your problem
Etc is this
working I can't tell if this is training
working I can't tell if this is training
finishing PhD a okay then free
support we're very happy to support
support we're very happy to support
um
um
Academia I just finished my PhD up
Academia I just finished my PhD up
myself uh last spring
yeah okay so Xander I don't think that
yeah okay so Xander I don't think that
this thing is quite working the way you
this thing is quite working the way you
would expect it to be working let's
would expect it to be working let's
figure out why that
figure out why that
is
is
ctde is common approach multi-agent I
ctde is common approach multi-agent I
would say uh it is the common approach
would say uh it is the common approach
but it's overbuilt you really don't need
but it's overbuilt you really don't need
it in a lot of cases my specialty in my
it in a lot of cases my specialty in my
PhD was
PhD was
multi-agent and generally IPO is just
multi-agent and generally IPO is just
tremendously effective when applied
tremendously effective when applied
quickly so if you have a slow Sim then
quickly so if you have a slow Sim then
maybe you look into fancy algorithms but
maybe you look into fancy algorithms but
if you like if you're doing stuff in
if you like if you're doing stuff in
logistics for which you can build a fast
logistics for which you can build a fast
simulator it's going to be tremendously
simulator it's going to be tremendously
more effective to just apply the simple
more effective to just apply the simple
method really really
method really really
fast um for contact my
fast um for contact my
PhD I built what is probably the most
PhD I built what is probably the most
complex multi-agent set of simulators
complex multi-agent set of simulators
out there like this is the newest
out there like this is the newest
version of it this this is a full MMO
version of it this this is a full MMO
with like an inbuilt economy system
with like an inbuilt economy system
leveling items equipment like thousand
leveling items equipment like thousand
different agents in the same environment
different agents in the same environment
uh they run for like hours and hours
uh they run for like hours and hours
we've trained policies for like 2,000
we've trained policies for like 2,000
years worth of simulations in this thing
years worth of simulations in this thing
so you know I can tell
so you know I can tell
you a thing or two about multi-agent
learning and I am very much aware that
learning and I am very much aware that
CT uh ctde is very common it was even
CT uh ctde is very common it was even
common back when I started this stuff in
common back when I started this stuff in
like
like
2018 but uh a lot of it can be replaced
2018 but uh a lot of it can be replaced
just by making thing go
fast and it's a lot simpler to do that
fast and it's a lot simpler to do that
as well
the other thing is
the other thing is
ctde doesn't even imply separate
ctde doesn't even imply separate
policies um you can still have the same
policies um you can still have the same
policy per agent and then just share the
policy per agent and then just share the
value function that's a little bit
value function that's a little bit
easier I would suggest looking to that
easier I would suggest looking to that
if
if
anything maybe something to consider and
anything maybe something to consider and
it's also really for team based stuff uh
it's also really for team based stuff uh
where you train where you train that as
where you train where you train that as
well it's not really for adverse areial
well it's not really for adverse areial
or like free-for-all settings
all right let's see what Xander
all right let's see what Xander
did four agents 1024
environments okay so he did this
correctly batch mini batch
correctly batch mini batch
oo is this mini batch
oo is this mini batch
good I think this is actually good right
I forget what the max is before GPU
I forget what the max is before GPU
starts to
degrade I think I tested this okay so it
degrade I think I tested this okay so it
is 32k so he's fine
report interval is too big or too
small so that doesn't make it any faster
small so that doesn't make it any faster
it does make the metrix slow to report
532 okay I don't know what's up with
532 okay I don't know what's up with
this report
this report
interval I guess it's cuz all the MS are
interval I guess it's cuz all the MS are
synced up how old are you search
27 newly minted MIT PhD finished last
27 newly minted MIT PhD finished last
spring now working full-time on making
spring now working full-time on making
reinforcement learning actually work
properly and building a small company
properly and building a small company
around
around
it all three an open source code
though let's see what Xander did here
though let's see what Xander did here
something
screwy he messed with this parameter
screwy he messed with this parameter
which I'd advise not doing but he didn't
which I'd advise not doing but he didn't
mess with it very
mess with it very
much he was running sweeps so presumably
much he was running sweeps so presumably
it's not that
wait is this bat size
sketchy 4096 no I think that's still
sketchy 4096 no I think that's still
that's 16 or
32 uh
32 uh
130 wait oops
130 wait oops
one 2 over 32 no not over that what is
one 2 over 32 no not over that what is
it
over uh 4096
over uh 4096
yeah 32 I'm right I'm fine uh that
yeah 32 I'm right I'm fine uh that
should be fine for this
environment okay I'm a Lambda look
reasonable is the curve going up at
all let's pull up
logs welcome YouTube
logs welcome YouTube
folks we're currently integrating uh
folks we're currently integrating uh
Xander's new environment
Xander's new environment
I do full code reviews and I help with
I do full code reviews and I help with
Integrations for all all contributed
Integrations for all all contributed
environments so if you're looking to get
environments so if you're looking to get
into RL it's a really great way to get
into RL it's a really great way to get
started we generally have people like
started we generally have people like
start off by building the new end it's a
start off by building the new end it's a
really good way to get familiar with
really good way to get familiar with
puffer and all the stuff that we work on
puffer and all the stuff that we work on
and then I slowly onboard people onto
and then I slowly onboard people onto
the science side uh using their
the science side uh using their
experience with building at least one
experience with building at least one
environment
environment
themselves let's see if we can f figure
themselves let's see if we can f figure
out what's going on here
out what's going on here
so need a few more data
points come
on SO loss is
on SO loss is
here this is fine this is
here this is fine this is
fine entropy is kind of
fine entropy is kind of
low value is good value is very good
low value is good value is very good
suspiciously good
even so nothing sketchy
there this just looks random with
there this just looks random with
variant to
me did Xander hold on I thought that he
me did Xander hold on I thought that he
committed some
committed some
stuff I thought he said that he had this
stuff I thought he said that he had this
working
it's working I
it's working I
think yeah he has it
here local
2D pushed everything
up try to find tomorrow h
all
settings this one
doesn't do you have a only used with
doesn't do you have a only used with
local 2D crop but I don't see a bull
local 2D crop but I don't see a bull
here
I don't even see the uh the option in
here 2D crop
space
um I don't see anything egregious in
um I don't see anything egregious in
here so
here so
far what policy is he
far what policy is he
using custom policy
okay so you don't need to do
okay so you don't need to do
this because your model is literally the
this because your model is literally the
default here so this is just going to
default here so this is just going to
[Music]
[Music]
be uh whatever it is
recurrent that's just redundant that
recurrent that's just redundant that
won't fix
won't fix
anything and then what you do here you
anything and then what you do here you
have a
have a
cons a lot of
channels uh I don't see anything
channels uh I don't see anything
terrible here
either oh is it
learning wait what was the x- axis in
learning wait what was the x- axis in
this thing
maybe it is learning
yeah that's like
weird yeah that's
weird yeah that's
bizarre
bizarre
um cuz this is a very simple
um cuz this is a very simple
environment probably there's some stuff
wrong for
okay so it is
okay so it is
learning so I suspect then there's just
learning so I suspect then there's just
something crazy going on with uh with
something crazy going on with uh with
his policy
lines very simple end logic it looks
lines very simple end logic it looks
like oh no cuz he's called okay he made
like oh no cuz he's called okay he made
like sub function for St that's
like sub function for St that's
fine
so total
so total
trash not collected
there's no reward
there's no reward
here where do he put the where did he
here where do he put the where did he
put stuff message is too long we'll Post
put stuff message is too long we'll Post
in Discord
in Discord
yeah I'm pretty responsive
there I mean this is mainly the point of
there I mean this is mainly the point of
streaming right is that I can like
streaming right is that I can like
engage with people that are looking to
engage with people that are looking to
use stuff in puffer have RL
use stuff in puffer have RL
problems um merge contributor code I
problems um merge contributor code I
also do the dev like my own personal Dev
also do the dev like my own personal Dev
live just because why not
but you know people do watch it we've
but you know people do watch it we've
got four on YouTube One on Twitch at the
moment the peak has been
moment the peak has been
20 take a while to get back to that
okay so here's this compute observation
okay so here's this compute observation
function this is what I wanted to see so
function this is what I wanted to see so
num cell types empty trash being the
num cell types empty trash being the
agent right
iterate oversight
iterate oversight
range check if cell is within bounds
one hot in
code
code
um wait OBS of OBS index
Plus+ I don't think this a one hot
encoding I think he just has the OBS
wrong cuz this is like you're setting
wrong cuz this is like you're setting
the current index to one or to zero but
the current index to one or to zero but
you're
you're
not okay I think the encoding is scy
not okay I think the encoding is scy
here
see what you did here come
on
485 so what is
485 so what is
um agent Site range times
um agent Site range times
2 agents okay so this is a crop with
2 agents okay so this is a crop with
four channels so that actually is the
four channels so that actually is the
correct one hot encoding
correct one hot encoding
Dimension what is the agent Site
Dimension what is the agent Site
range is it five by
range is it five by
default yeah so self observations 0 do
shape
um well wait a second that's
um well wait a second that's
40 self. num OBS oh plus one
okay so this
okay so this
is
is
zero that looks weird to me
right hold on
oh I found your thing I think my use
oh I found your thing I think my use
case CD of makes sense because why
case CD of makes sense because why
should I train different policies for
should I train different policies for
the same agv
the same agv
type so this is not what ctde means
type so this is not what ctde means
though you can train the same policy
though you can train the same policy
without doing
ctde I this is what ippo is which is
ctde I this is what ippo is which is
just standard po you share the weights
just standard po you share the weights
for the same policy and you apply them
for the same policy and you apply them
independently to each agent in the scene
independently to each agent in the scene
ctde sometimes has separate policies
ctde sometimes has separate policies
sometimes has the same one the defining
sometimes has the same one the defining
feature of ctde is typically that you're
feature of ctde is typically that you're
sharing at least the value function uh
sharing at least the value function uh
during the course of training which can
during the course of training which can
be useful for Cooperative tasks but
be useful for Cooperative tasks but
we've had plenty of success without
we've had plenty of success without
bothering doing
bothering doing
that Independent Learning if I had yeah
that Independent Learning if I had yeah
so I think that you
so I think that you
have I think that you've got your
have I think that you've got your
definitions backwards
definitions backwards
um and
um and
actually the like assigning policies to
actually the like assigning policies to
agent it's doing the opposite of what
agent it's doing the opposite of what
you want in R alib right now and the
you want in R alib right now and the
thing that puffer is doing if you have
thing that puffer is doing if you have
the same uh agv type is the thing that
the same uh agv type is the thing that
you want and is the simpler thing and is
you want and is the simpler thing and is
the faster thing so I think that you
the faster thing so I think that you
just win here
Logistics is a great area is a great
Logistics is a great area is a great
area to apply RL though
let me see how he structures
OBS Marcel yeah so what we do first of
OBS Marcel yeah so what we do first of
all I PPO poo is the same thing people
all I PPO poo is the same thing people
just are making meaningless distinctions
just are making meaningless distinctions
the simplest most common way to apply Po
the simplest most common way to apply Po
in a multi-agent setting is to just
in a multi-agent setting is to just
treat each agent independently like it's
treat each agent independently like it's
from a different environment but using
from a different environment but using
the same policy so the same policy is
the same policy so the same policy is
used independently to compute the
used independently to compute the
actions for each agent this is the same
actions for each agent this is the same
thing as just contena all the agents
thing as just contena all the agents
into the batch Dimension um this is
into the batch Dimension um this is
tremendously effective in a wide variety
tremendously effective in a wide variety
of cases now technically for cooperative
of cases now technically for cooperative
problems right what you can do is
problems right what you can do is
centralize the value function so you can
centralize the value function so you can
take all of the outputs from the
take all of the outputs from the
different agents and use them in a
different agents and use them in a
single- shared value function but there
single- shared value function but there
are some subtleties that you have to
are some subtleties that you have to
take into account when doing that to get
take into account when doing that to get
it correct which at least as far I as I
it correct which at least as far I as I
have seen are generally not worth it um
have seen are generally not worth it um
and I think that the vast vast majority
and I think that the vast vast majority
of problems you will just solve
of problems you will just solve
immediately uh with our basic po given
immediately uh with our basic po given
how fast it is
how fast it is
if you train 100 agents agent one
if you train 100 agents agent one
learned something from agent two it's
learned something from agent two it's
the same network there is one policy
the same network there is one policy
it's independently controlling each
it's independently controlling each
agent right so you have one neural
agent right so you have one neural
network that neural network computes the
network that neural network computes the
actions for agent one computes the
actions for agent one computes the
actions for agent two computes actions
actions for agent two computes actions
for agent 3 independently so this can be
for agent 3 independently so this can be
deployed you know in plenty of real
deployed you know in plenty of real
world settings so pretty much you know
world settings so pretty much you know
this is the standard way you do stuff in
this is the standard way you do stuff in
every real world setting really you're
every real world setting really you're
training one policy you're using all the
training one policy you're using all the
data right to train that one policy so
data right to train that one policy so
it's very
it's very
effective
effective
um yeah you're not training 100 separate
um yeah you're not training 100 separate
policies this is that's not ctde though
policies this is that's not ctde though
ctde is ctde is something
different ctte
different ctte
so
here okay so it depends heavily on your
here okay so it depends heavily on your
algorithm but for like PO for for
algorithm but for like PO for for
instance right the main thing that
instance right the main thing that
you're training for uh reward
you're training for uh reward
stabilization is this value function so
stabilization is this value function so
you have like your policy and then you
you have like your policy and then you
get like
action ATN not
action ATN not
ant and then you get
ant and then you get
reward reward right uh not reward hold
reward reward right uh not reward hold
on Val
on Val
value okay and then every single agent
value okay and then every single agent
is going to produce these values which
is going to produce these values which
is just like a b Dimension okay so you
is just like a b Dimension okay so you
just take this you get your agent data
just take this you get your agent data
right across all your
right across all your
agents right and then what you can do is
agents right and then what you can do is
you can either get a whole bunch of
you can either get a whole bunch of
value
value
functions a whole bunch of separate
functions a whole bunch of separate
values this is if you do it
values this is if you do it
independently but what ctde does is it
independently but what ctde does is it
takes like agent One agent two agent
takes like agent One agent two agent
three agent four and they each are going
three agent four and they each are going
to produce their own actions
but then they all produce one
but then they all produce one
value that's
value that's
ctde uh and the reason this is ctde is
ctde uh and the reason this is ctde is
because you only need the value function
because you only need the value function
at train time so you're training them
at train time so you're training them
like together in aggregate on this one
like together in aggregate on this one
value
value
function but then when you deploy the
function but then when you deploy the
policy you don't care about the value
policy you don't care about the value
function so that they still operate
function so that they still operate
independently
and this is like kind of a useful idea
and this is like kind of a useful idea
and it's been useful in a number of
and it's been useful in a number of
cases but like vast majority of
cases but like vast majority of
multi-agent problems you don't even need
multi-agent problems you don't even need
this if you just get a fast Sim and a
this if you just get a fast Sim and a
good implementation of Po you'll solve
good implementation of Po you'll solve
it the simpler way and save yourself a
it the simpler way and save yourself a
lot of
lot of
headaches you train centralized one
headaches you train centralized one
policy and
decentralized if that's how people are
decentralized if that's how people are
referring to it then they're mucking
referring to it then they're mucking
with the definitions
it makes far more sense to look at the
it makes far more sense to look at the
the value function specifically because
the value function specifically because
like it really doesn't matter if you
like it really doesn't matter if you
train one policies or n policies like
train one policies or n policies like
that has no implications on anything um
that has no implications on anything um
the value function
the value function
specifically is interesting because it's
specifically is interesting because it's
you only need it during training time
you only need it during training time
which is why can centralize the training
which is why can centralize the training
of the value
of the value
function and decentralize the
function and decentralize the
execution you're not train you're not
execution you're not train you're not
centralized training one policy though
centralized training one policy though
that's that's a different thing that's
that's that's a different thing that's
if you were to have one policy with like
if you were to have one policy with like
let's say five agents on your team it
let's say five agents on your team it
has five different action heads and it's
has five different action heads and it's
jointly Computing actions that's not
jointly Computing actions that's not
what is happening here it is
what is happening here it is
independently Computing actions for each
agent people people muck with the
agent people people muck with the
definitions uh especially in multi-agent
definitions uh especially in multi-agent
like if that's how they have it defined
like if that's how they have it defined
that's how they have it defined but
that's how they have it defined but
that's not a useful
that's not a useful
definition because it's like it's
definition because it's like it's
focused on it's focused on something
focused on it's focused on something
that's completely irrelevant which is
that's completely irrelevant which is
whether like whether you're training one
whether like whether you're training one
set of weights or multiple set of
set of weights or multiple set of
Weights is completely
Weights is completely
irrelevant um the fashion of whether you
irrelevant um the fashion of whether you
are jointly Computing actions and
are jointly Computing actions and
jointly Computing values or
jointly Computing values or
independently Computing actions jointly
independently Computing actions jointly
Computing values that is the thing that
matters is that the new multi-agent RL
book centralized
training yeah but you're Miss you're
training yeah but you're Miss you're
missing the key of the the value
missing the key of the the value
function being shared like it's not
function being shared like it's not
centralized training because you're
centralized training because you're
training one policy like you're always
training one policy like you're always
doing that anyways because you're
doing that anyways because you're
sharing data across parallel copies of
sharing data across parallel copies of
the environment right like you have
the environment right like you have
different instances with different like
different instances with different like
agents anyways that's just a batch
Dimension the relevance right is that if
Dimension the relevance right is that if
you jointly compute actions then you
you jointly compute actions then you
cannot deploy the policy in dependently
cannot deploy the policy in dependently
you like each agent cannot just take its
you like each agent cannot just take its
own data source you need all the data
own data source you need all the data
for all the agents to deploy it but if
for all the agents to deploy it but if
you only centralize the value function
you only centralize the value function
then you can train centralized and then
then you can train centralized and then
deploy it decentralized right that's the
deploy it decentralized right that's the
key
distinction like I said people might
distinction like I said people might
have mucked with the definition what is
have mucked with the definition what is
this is this the new moral book
oh yeah this was the new I saw this and
oh yeah this was the new I saw this and
didn't like
it
right no no no it's not independent
right no no no it's not independent
policies I me and the thing is like this
policies I me and the thing is like this
is like completely I mean this is like
is like completely I mean this is like
so so out of touch if that's what they
so so out of touch if that's what they
do because nobody does that
yeah so I saw this thing and I saw the
yeah so I saw this thing and I saw the
table of contents uh this is insane like
table of contents uh this is insane like
this is 10 times more math than is ever
this is 10 times more math than is ever
relevant and like a tenth of the
relevant and like a tenth of the
engineering and like implementation
engineering and like implementation
detail that actually matters so
detail that actually matters so
yeah I uh I don't necessarily recommend
yeah I uh I don't necessarily recommend
oh look at this they actually have right
oh look at this they actually have right
here
here
well this is actually very funny because
well this is actually very funny because
these are very trivial environments all
these are very trivial environments all
of these but
328 where is
it they literally
have see this rware environment we have
have see this rware environment we have
a port of this in C that's like shorter
a port of this in C that's like shorter
than the original or whatever you can
than the original or whatever you can
play it online look here this is trained
play it online look here this is trained
with this is trained with puffer lib
with this is trained with puffer lib
with our default
with our default
policy with nothing fancy
policy with nothing fancy
whatsoever this runs like a thousand
whatsoever this runs like a thousand
times faster than the original as
well look how nicely they get out of the
well look how nicely they get out of the
way for each other and like go find new
way for each other and like go find new
uh new boxes this has trained in like a
uh new boxes this has trained in like a
few minutes on one GPU very simple
few minutes on one GPU very simple
policy very simple po
I didn't like the book because first of
I didn't like the book because first of
all it's this is like these are not
all it's this is like these are not
complex environments at all so these are
complex environments at all so these are
like trivial trivial tasks um they did a
like trivial trivial tasks um they did a
ton of math on trivial tasks which is
ton of math on trivial tasks which is
completely irrelevant like you can solve
completely irrelevant like you can solve
all of these trivially with like the
all of these trivially with like the
simplest methods you can think of and
simplest methods you can think of and
they completely omit any of the actually
they completely omit any of the actually
more complex T tasks can I place
more complex T tasks can I place
obstacles there and the Agents can
obstacles there and the Agents can
Collide uh the agents can Collide I
Collide uh the agents can Collide I
don't know if this one has obstacles I
don't know if this one has obstacles I
Les but like generally these things are
Les but like generally these things are
very trivial to
very trivial to
implement like rware is not one of our
implement like rware is not one of our
more complex environments at all like
more complex environments at all like
rware is um okay to be fair the rware
rware is um okay to be fair the rware
original environment has this really
original environment has this really
obnoxious like redundant graph algorithm
obnoxious like redundant graph algorithm
in it for no reason uh if you cut that
in it for no reason uh if you cut that
out then it is a very very simple
out then it is a very very simple
environment and you can Implement
environment and you can Implement
anything like this you want uh for your
anything like this you want uh for your
specific use case in like a
specific use case in like a
day and have it be a thousand times
day and have it be a thousand times
faster than uh than anything that's
faster than uh than anything that's
being mentioned here like I know all of
being mentioned here like I know all of
these or almost all these environments
these or almost all these environments
hanab is cool overcooked has literally
hanab is cool overcooked has literally
the worst code base I've ever seen uh
the worst code base I've ever seen uh
this thing is about probably a 100 times
this thing is about probably a 100 times
100 thousand times slower than it should
100 thousand times slower than it should
be smack is cool but unfortunately
be smack is cool but unfortunately
really really slow because it's stuck to
really really slow because it's stuck to
the Blizzard engine engine uh it's
the Blizzard engine engine uh it's
insulting to call MP a complex
insulting to call MP a complex
environment that's hilarious and then
environment that's hilarious and then
yeah this is fine but I wouldn't call it
yeah this is fine but I wouldn't call it
complex open Spiel is um board games so
complex open Spiel is um board games so
not necessarily I guess kind of
not necessarily I guess kind of
multi-agent and then petting zoo yeah
multi-agent and then petting zoo yeah
they have some
they have some
um yeah I don't know it's not like this
um yeah I don't know it's not like this
is bad content it's just like out of
is bad content it's just like out of
touch for how this work is actually done
touch for how this work is actually done
now I can change grid and add walls I
now I can change grid and add walls I
don't know if we have walls but like
don't know if we have walls but like
adding walls would be like a probably 30
adding walls would be like a probably 30
line code change
line code change
uh what you're looking for is kind of
uh what you're looking for is kind of
like a grid based engine we've had a
like a grid based engine we've had a
couple people mess with that I tried to
couple people mess with that I tried to
mess with that a little bit you can
mess with that a little bit you can
definitely change the grid easily that's
definitely change the grid easily that's
just a custom map that's trivial uh I
just a custom map that's trivial uh I
can ask Spencer about walls that
can ask Spencer about walls that
shouldn't be
hard I mean here I'll show you like for
hard I mean here I'll show you like for
any of these environments
any of these environments
right here let me show you cuz I think
right here let me show you cuz I think
you've been misled a little bit uh
you've been misled a little bit uh
you've been I think you've been a little
you've been I think you've been a little
bit misled
here I'm not mad at you I'm mad at like
here I'm not mad at you I'm mad at like
people over complicating stuff for
people over complicating stuff for
newcomers to like I don't know why
newcomers to like I don't know why
people over complicate this so much I
people over complicate this so much I
need to know C okay but have you
need to know C okay but have you
actually read any of our C it's like the
actually read any of our C it's like the
easiest C you've ever
seen
um okay okay so this the reason this is
um okay okay so this the reason this is
800 lines is entirely because uh Spencer
800 lines is entirely because uh Spencer
also qued this completely redundant
also qued this completely redundant
graph algorithm uh from the original
graph algorithm uh from the original
rware environment you can literally just
rware environment you can literally just
ignore that like this whole like process
ignore that like this whole like process
cycle movement and stuff is like
cycle movement and stuff is like
completely redundant
completely redundant
so yeah the code looks like this it's
so yeah the code looks like this it's
literally like ifs it's just
literally like ifs it's just
conditionals and
Loops it's literally just conditionals
Loops it's literally just conditionals
and loops
and loops
so like it's not any harder to write
so like it's not any harder to write
this than it is to write python I wrote
this than it is to write python I wrote
python for 10 years within 3 weeks I was
python for 10 years within 3 weeks I was
just as comfortable writing this now I
just as comfortable writing this now I
think this is probably
easier I am not a systems
easier I am not a systems
Guru yeah so this whole thing this whole
Guru yeah so this whole thing this whole
block of code is irrelevant all the
block of code is irrelevant all the
cycle stuff is irrelevant in fact this
cycle stuff is irrelevant in fact this
trash pickup end is
trash pickup end is
probably simpler to look cuz this say
probably simpler to look cuz this say
600 lines and doesn't have all the fancy
600 lines and doesn't have all the fancy
stuff so this is login code and then the
stuff so this is login code and then the
actual code for the environment we've
actual code for the environment we've
got compute
got compute
observations we've got Place random
observations we've got Place random
entities we've got move
agent this is also more complicated than
agent this is also more complicated than
it should be to be fair let me see if I
it should be to be fair let me see if I
find you like one of my one of the mems
find you like one of my one of the mems
that I wrote cuz like we also we have
that I wrote cuz like we also we have
people from like a variety of different
people from like a variety of different
like engineering skill levels on these
like engineering skill levels on these
things
things
um let me think probably my snake
um let me think probably my snake
environment would be a nice a nice one
environment would be a nice a nice one
for you so this thing is only 350 lines
for you so this thing is only 350 lines
for multi-agent snake um including this
for multi-agent snake um including this
logging there
logging there
initialization there's your observation
initialization there's your observation
function right there here's like adding
function right there here's like adding
and deleting snakes very nice little
and deleting snakes very nice little
Snippets of
Snippets of
code there's spawning food there's the
code there's spawning food there's the
environment
environment
reset this this is the step function
reset this this is the step function
it's like one screen of code little more
it's like one screen of code little more
than one screen of code and that's it
than one screen of code and that's it
rest of it's
rest of it's
rendering not so bad
rendering not so bad
right I had this in Python before and
right I had this in Python before and
it's like about the same length as it
it's like about the same length as it
would be in
Python we've got sample environments and
Python we've got sample environments and
tons of people on the Discord to help as
tons of people on the Discord to help as
well
I've also been uh converting major RL
I've also been uh converting major RL
Labs uh to this as well um we've got a
Labs uh to this as well um we've got a
collab with
collab with
NYU uh we've got one with
NYU uh we've got one with
MIT and uh one with Oxford as
well on this stuff so I'm not just
well on this stuff so I'm not just
making this up like this is I promise
making this up like this is I promise
you this is very very much simpler and
you this is very very much simpler and
easier than the other stuff out
there like something like this
there like something like this
environment here you can train this at a
environment here you can train this at a
million steps per second and just
million steps per second and just
automatically Solve
IT dictionary with agent IDs nope do not
IT dictionary with agent IDs nope do not
do that if you do that you are screwed
do that if you do that you are screwed
because python dictionaries are slow but
because python dictionaries are slow but
we have an easier way for you look here
we have an easier way for you look here
I can even show you you can even start
I can even show you you can even start
prototyping in Python without going to
prototyping in Python without going to
see
see
and it'll still be faster than uh than
and it'll still be faster than uh than
RL to start
RL to start
with so if I just look up squared Pi squ
with so if I just look up squared Pi squ
I added this yesterday this is a pure
I added this yesterday this is a pure
python version of a very simple test
python version of a very simple test
environment right
environment right
so uper Pi
squ so here this is a very simple little
squ so here this is a very simple little
environment that I wrote
environment that I wrote
this is just a random policy playing it
this is just a random policy playing it
this whole thing is 100 lines of code
this whole thing is 100 lines of code
not even in Python it is way faster than
not even in Python it is way faster than
the uh the rlb API reason being it is a
the uh the rlb API reason being it is a
vector
vector
implementation so this buffer here is
implementation so this buffer here is
going to actually be uh is going to give
going to actually be uh is going to give
you a nice slice of shared memory and
you a nice slice of shared memory and
then when you write stuff into
then when you write stuff into
observations you're going to be writing
observations you're going to be writing
directly into shared memory I get a m
directly into shared memory I get a m
ion steps per train a million steps per
ion steps per train a million steps per
second training even in pure python of
second training even in pure python of
course that's only for very simple M as
course that's only for very simple M as
the M gets more complicated that's going
the M gets more complicated that's going
to dip and that's when you think about
to dip and that's when you think about
going to C but you can prototype this in
going to C but you can prototype this in
Python work exactly as you'd like to
Python work exactly as you'd like to
that'll probably be a very nice way for
that'll probably be a very nice way for
you to um to try out puffer as well and
you to um to try out puffer as well and
I can promise you it'll be faster than
I can promise you it'll be faster than
uh than RL Li because that library is
uh than RL Li because that library is
hideously
slow you'll probably end up solving your
slow you'll probably end up solving your
python uh your problem in pure python
python uh your problem in pure python
before you even have to touch C and then
before you even have to touch C and then
you can go to C if you want it to be
you can go to C if you want it to be
really
fast uh well step just takes oh yeah I
fast uh well step just takes oh yeah I
didn't answer your original question so
didn't answer your original question so
uh it just takes a an array of actions
uh it just takes a an array of actions
so it's a vector API so if you've got 10
so it's a vector API so if you've got 10
agents then you just give it you know an
agents then you just give it you know an
array with 10 numbers which are the
array with 10 numbers which are the
actions we also handle like multi-
actions we also handle like multi-
discreete spaces if you need those um um
discreete spaces if you need those um um
but yeah it's just a vector
API if you have to like dynamically add
API if you have to like dynamically add
and remove agents that's a little bit
and remove agents that's a little bit
more annoying but we do support that
more annoying but we do support that
there's like an extra mask field that
there's like an extra mask field that
you can update to like mask out dead
agents so there you
go I hope this helps I I'm saying this
go I hope this helps I I'm saying this
as somebody who wasted like I literally
as somebody who wasted like I literally
wasted a thousand hours of my time
wasted a thousand hours of my time
during my PhD trying to make R work in
during my PhD trying to make R work in
like old versions of neural MMO is a
like old versions of neural MMO is a
terrible Library I've tried many many
terrible Library I've tried many many
times to help them with it um you know
times to help them with it um you know
the devs they really were trying with it
the devs they really were trying with it
it's just like the way that they built
it's just like the way that they built
that library is not the way that things
that library is not the way that things
work in
work in
RL
RL
yeah I think it'll it'll take a lot less
yeah I think it'll it'll take a lot less
time as well than you might think to do
time as well than you might think to do
this type of stuff because like if
this type of stuff because like if
you're used to rlb they have a giant
you're used to rlb they have a giant
blob of code where nothing makes any
blob of code where nothing makes any
sense right and you have to really like
sense right and you have to really like
look at the Docks and then fiddle with
look at the Docks and then fiddle with
stuff and whatever we have a tiny tiny
stuff and whatever we have a tiny tiny
amount of Code by comparison and it's
amount of Code by comparison and it's
faster it is simpler and it works on a
faster it is simpler and it works on a
huge range of problems we have all sorts
huge range of problems we have all sorts
of high performance environments built
of high performance environments built
in this is what I'm doing fulltime right
in this is what I'm doing fulltime right
I'm making RL not suck to work in
I got to get back to this trash pickup M
I got to get back to this trash pickup M
because I want to make sure I get uh
because I want to make sure I get uh
some progress on this let me see what
some progress on this let me see what
the heck is going on with
this and don't let the C scare you it's
this and don't let the C scare you it's
not
scary like if you took one systems
scary like if you took one systems
course is n underr you're
good heck we have brand new programmers
good heck we have brand new programmers
writing this stuff as well not this
writing this stuff as well not this
particular M but
others why is
it what's this extra OB
observation so the first scaler is this
observation so the first scaler is this
carrying status I
carrying status I
guess okay
one wall
on one is trash
apparently I
apparently I
is I'm screwy
here so what did he do out of bound
here so what did he do out of bound
cells or negative one
okay I wouldn't suggest that but fine
is this really how it is like this line
is this really how it is like this line
of trash here I doubt that
right actually hold on EMP no empty is
something's not right here something's
something's not right here something's
definitely not right
definitely not right
here with the observations
so wait four you Loop over
so wait four you Loop over
agents you Loop over sight
range and
then I don't understand how this
works OBS of of ob's
index hold on you know
index hold on you know
what if this is
what if this is
wrong then I would think
gotcha
gotcha
Xander the Ops are
wrong
gotcha so uh I see exactly what he did
gotcha so uh I see exactly what he did
so was Zander did here is he made the
so was Zander did here is he made the
observations for one hot so like he
observations for one hot so like he
added four channels uh but then in the
added four channels uh but then in the
code he's not actually filling it in by
code he's not actually filling it in by
channels so he's filling in like the
channels so he's filling in like the
first quarter of the data and then the
first quarter of the data and then the
rest of it's
rest of it's
empty ti
empty ti
ti um so that's why it doesn't
work I guess I
work I guess I
will should I just fix it for him or
will should I just fix it for him or
should I message him let me think I got
should I message him let me think I got
got half an
hour I kind of sometimes you know after
hour I kind of sometimes you know after
I've done a few PRS I kind of like to
I've done a few PRS I kind of like to
just fix people
just fix people
stuff shows I'm not
stuff shows I'm not
[Music]
[Music]
bullshitting uh let me see if I can just
bullshitting uh let me see if I can just
piix it real quick for
him every once in a
while uh let me see
while uh let me see
so this is not a Time 4
buddy uh Time
for num cell
types compute
observations oh wait is
he did he wait no actually did he do it
he did he wait no actually did he do it
right
right
here hold
here hold
on
what then why
is this laid out correctly in
is this laid out correctly in
memory I can see the observations are
memory I can see the observations are
wrong here
maybe only for one
maybe only for one
agent what did he
do I did I missed this Loop when I was
do I did I missed this Loop when I was
looking at
it get out of here bot
um s types
oh hold on so if I do
oh hold on so if I do
zero
zero
2 three
what if it's
this that's not it
either okay so if it's
either okay so if it's
this mostly
empty
rash I might have just been being stupid
rash I might have just been being stupid
hold
hold
on B is not oh no there is a
on B is not oh no there is a
Ben agents
by 11
by4 I thought I'd I thought I saw it and
by4 I thought I'd I thought I saw it and
it looked pretty obvious but I didn't
it looked pretty obvious but I didn't
see that there was a
see that there was a
weird there was a weird Loop in a way I
weird there was a weird Loop in a way I
wasn't expecting
let me try in the in the start of the
let me try in the in the start of the
network
[Music]
then see what the heck is wrong with
then see what the heck is wrong with
this thing
put it
here oh I think I see it
is this
is this
right yeah so this is an 11 by 11 that's
right yeah so this is an 11 by 11 that's
supposed to be one hunted right you can
supposed to be one hunted right you can
see this data is completely screwy and
see this data is completely screwy and
the reason is uh
the reason is uh
permute he messed this up so it's like
permute he messed this up so it's like
this
this
um per mute
empty let's try
this I think I forgot to remove the
this I think I forgot to remove the
break
break
point maybe not
[Music]
h h
no what you mean no
tnm local
tnm local
crop uhhuh
five at Stride three three at Stride one
five at Stride three three at Stride one
is
good I actually see he was trying to
good I actually see he was trying to
print observations here I don't think
print observations here I don't think
I'm wrong though
like something's very screw with
like something's very screw with
this
meow combine features
I guess that just means more stuff is
broken let this run for a bit while I
broken let this run for a bit while I
figure out
so that was definitely an issue like the
so that was definitely an issue like the
observations were definitely
observations were definitely
screwy so yeah that would completely
screwy so yeah that would completely
make sense then like why it would take
make sense then like why it would take
forever to learn
forever to learn
anything
uh so we setting rewards and DS
uh so we setting rewards and DS
correctly
correctly
here uh
add
reward well that doesn't make any
reward well that doesn't make any
sense
for for
crash trash not
collected something with the recepts I
collected something with the recepts I
wonder
what does he do when you resarch the
M NS the grid
carrying does he know the entities
is okay yeah he overwrites them
is okay yeah he overwrites them
so that should be
fine what's up with these rewards being
fine what's up with these rewards being
so screwy
though for
it doesn't make any sense for it to be
it doesn't make any sense for it to be
so
variable across uh so many environments
see if it's
see if it's
this buter no this is
correct so I can't fault him for most of
correct so I can't fault him for most of
this well something screwy
here has he played it
there's no human play mode here
oh no he does have
this left
shift oh he doesn't have a um a no op is
shift oh he doesn't have a um a no op is
wi so you have to
like I think you have to like hold the
like I think you have to like hold the
puffer to
it okay so I'm the red
puffer what you can push the trash
bin hold on I didn't realiz you can push
bin hold on I didn't realiz you can push
the trash
bin it's
amusing total episode reward is Tiny
amusing total episode reward is Tiny
though I don't know what that
is that doesn't seem to be
is that doesn't seem to be
working for
okay so it gets 0 five everyone deposits
okay so it gets 0 five everyone deposits
right
that seems
that seems
better let's see if I can finish the
better let's see if I can finish the
environment and let's see if it resets
environment and let's see if it resets
cleanly cuz that's the other common
cleanly cuz that's the other common
another common RL bug is like you play
another common RL bug is like you play
your environment but you don't actually
your environment but you don't actually
check to make sure it resets
cleanly it's funny how you can actually
cleanly it's funny how you can actually
grief by pushing it into a
grief by pushing it into a
corner welcome YouTube
corner welcome YouTube
folks currently debugging this new
folks currently debugging this new
contributor RL
contributor RL
environment it's kind of nice um just
environment it's kind of nice um just
not
not
working ideally
yet does it not
yet does it not
reset hang
reset hang
on do it only reset at uh
on do it only reset at uh
steps I would break
it yeah I know they're holding it but it
it yeah I know they're holding it but it
says craft collected 20 out of
says craft collected 20 out of
20 okay so it does
reset oh
reset oh
well hold
on that's
on that's
bizarre
right so it does solve
it but it's just weirdly slow
yeah so they literally all have to uh
yeah so they literally all have to uh
they all have to drop the trash
in I bet you that's
in I bet you that's
the the
issue e
what did he have like 1.5k steps or
what did he have like 1.5k steps or
something
um the hell why don't I have did they
um the hell why don't I have did they
change their UI
H this is
gross what the
where's the freaking thing to change the
where's the freaking thing to change the
axes
now what is wrong with these
people x axis
this doesn't do anything now
okay so 500
okay so 500
steps so he had like 1.5k
before so I think I may this thing 3x
before so I think I may this thing 3x
faster Maybe
yeah more because this is 2,000 steps
yeah more because this is 2,000 steps
okay so I had it um now it's at like 500
okay so I had it um now it's at like 500
for
solve let's see what this does
I'm still not happy with the 500
though this is 400 million steps which
though this is 400 million steps which
is way too much for this environment it
is way too much for this environment it
shouldn't take this long to get off the
shouldn't take this long to get off the
ground though maybe they have bad whoops
ground though maybe they have bad whoops
camera
camera
fro
fro
um hey welcome YouTube folks
um hey welcome YouTube folks
first stream over 10 viewers in a
first stream over 10 viewers in a
while
um so this was just like observations
um so this was just like observations
were uh incorrectly
were uh incorrectly
formatted this is a bad transpose on the
formatted this is a bad transpose on the
observations um causing this to not work
before here you can see this one going
before here you can see this one going
up fast
up fast
now let's see how long this takes now
now let's see how long this takes now
this one's not necessarily expected to
this one's not necessarily expected to
get to 20 because it doesn't have enough
get to 20 because it doesn't have enough
time to solve the whole task uh let me
time to solve the whole task uh let me
explain my reasoning here so if you give
explain my reasoning here so if you give
it 500 steps right uh it's going to get
it 500 steps right uh it's going to get
most of the trash early on and then
most of the trash early on and then
there's going to be like some puffers
there's going to be like some puffers
holding some trash or whatever and if
holding some trash or whatever and if
the environment doesn't reset everyone
the environment doesn't reset everyone
else is just there twiddling their
else is just there twiddling their
thumbs uh waiting for new opportunities
thumbs uh waiting for new opportunities
to get reward so this is just like a
to get reward so this is just like a
really obnoxious learning
really obnoxious learning
Dynamic um in general
Dynamic um in general
and the way to solve this immediately is
and the way to solve this immediately is
to just like add a trash back randomly
to just like add a trash back randomly
every time one is deposited that will
every time one is deposited that will
make it you know way
make it you know way
quicker so I will let Xandra know this
quicker so I will let Xandra know this
let me see how well this does though
first oh it actually does solve it haha
first oh it actually does solve it haha
in 100
in 100
steps let's watch that policy
actually
actually
1,400 let's wait till we get a new
checkpoint 19.7
checkpoint 19.7
[Music]
and we'll watch this policy back and
and we'll watch this policy back and
then that'll be uh the morning stream
then that'll be uh the morning stream
for us with this environment
for us with this environment
fixed good feedback for Xander I'll push
fixed good feedback for Xander I'll push
these changes you'll all be free to try
these changes you'll all be free to try
it as
well okay 16004
pick up mode
pick up mode
eval um
Native 6 C
that's a little
fast I'll just uh for
fast I'll just uh for
now I'll just add a little
delay for
we wait for this to finish and then I'm
we wait for this to finish and then I'm
going to send him a nice gift
it's a nice policy
hello
hello
nerd how's it
going nerd Ru man
going nerd Ru man
[Music]
solve 200 M
steps compute OB
steps compute OB
bations uh
what mute no was it
what mute no was it
your see had uh observations
awesome good to see us dropping by
stream for
push this for Xander
that's like the really satisfying policy
that's like the really satisfying policy
to watch for whatever reason as
well what just
well what just
happened oh they're
happened oh they're
stuck that's so funny okay so there are
stuck that's so funny okay so there are
some degenerate cases where they get
some degenerate cases where they get
stuck that's so funny
that's reinforcement learning for
you it's really cool to see stuff like
you it's really cool to see stuff like
this
this
work it's like really basic and yeah you
work it's like really basic and yeah you
could totally just write a scripted
could totally just write a scripted
method to do this but still cool
[Music]
[Music]
last thing I'm going to do
background if steps were to complete
background if steps were to complete
were part of the score do you think it
were part of the score do you think it
would develop more complex coordinates
would develop more complex coordinates
so there are less collisions uh it
so there are less collisions uh it
implicitly is there is a penalty for
implicitly is there is a penalty for
taking longer implicit in the RL
taking longer implicit in the RL
algorithm hey Tim yeah this is a
algorithm hey Tim yeah this is a
contributor and had a couple bugs just
contributor and had a couple bugs just
fixed
fixed
them uh hold on trash this will be puff
them uh hold on trash this will be puff
red
C this is fine this is be puff
red
carrying uh
carrying uh
red is this everything
aren't there
lines
yeah pick up
we' give it the
uh puffer
uh puffer
canonicalization uh the lines are not
canonicalization uh the lines are not
good here hold on the lines don't look
good here hold on the lines don't look
very
very
good so let's do this we'll leave his
good so let's do this we'll leave his
light
light
gray and then
huh RL based chess engine
huh RL based chess engine
probably we have good vectorization if
probably we have good vectorization if
nothing
else I got to come up with another color
else I got to come up with another color
for this this is
weird background
weird background
624 so let's do
there we
go code immediately oh I just messed
go code immediately oh I just messed
with
that I think we'll do the other way
that I think we'll do the other way
around as well
this one will be puff
this one will be puff
cyan and this one will be
cyan and this one will be
whoops
red there we go
and then the text has to be PFF
white I don't like the uh
there we
go I don't like that it's like
uh let's
do
do
15 is this like a tint is there a way to
15 is this like a tint is there a way to
just do this as a
just do this as a
tint no there's not
tint no there's not
I think what we can do
is
here there we
here there we
go now it's apply to
tent cool
nice
okay um I
okay um I
am I'm going to go to the gym and get
am I'm going to go to the gym and get
some brunch I will be back streaming
some brunch I will be back streaming
probably for a bit uh later in the
probably for a bit uh later in the
afternoon probably around 34 3:30 or 4:
afternoon probably around 34 3:30 or 4:
uh for new folks
uh for new folks
here this is a project to make
here this is a project to make
reinforcement learning fast and sane and
reinforcement learning fast and sane and
pretty accessible as well it's all free
pretty accessible as well it's all free
and open source you can find more at
and open source you can find more at
puffer doai please start the repository
puffer doai please start the repository
here it helps us out a tremendous
here it helps us out a tremendous
amount uh we also have a discort 867
amount uh we also have a discort 867
members at the moment you can get fast
members at the moment you can get fast
help here if you're interested in doing
help here if you're interested in doing
some RL yourself or using these
some RL yourself or using these
environments or adding some uh
environments or adding some uh
contributing environments is also the
contributing environments is also the
best way to get involved with puffer and
best way to get involved with puffer and
onboard yourself onto this space if
onboard yourself onto this space if
you're new I also post all about RL
you're new I also post all about RL
right here you can get notifications
right here you can get notifications
when I'm live here too cuz I stream on X
when I'm live here too cuz I stream on X
twitch and YouTube
twitch and YouTube
simultaneously see it's all RL content
simultaneously see it's all RL content
not a bunch of
BS and other than that I stream most of
BS and other than that I stream most of
the development and we've got big plans
the development and we've got big plans
for 2025 with this project so thanks for
