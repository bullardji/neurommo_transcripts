Kind: captions
Language: en
we are back
live stupid
timeline let's look at uh carbs
so yeah right the the thing that we were
so yeah right the the thing that we were
doing last time we got this data right
doing last time we got this data right
so what we're going to do
is test
is test
GP okay
data hold
on success observation
these need to get mapped to score and
cost oh
cost oh
wait yeah they are
wait yeah they are
there
okay
um
um
output real number input
output real number input
okay so we just do
cost.
tensor
tensor
from
p e c or
score and then we
do uh
OBS
OBS
stack real
number or in data
okay now we have this
okay now we have this
here
and let's go look at a Galan process
DP
regression iro
look at all this
junk heck is this Jitter
GP so this is inputs which
GP so this is inputs which
is
OBS outputs
score kernel
kernel get
model a turn
okay so we want this
[Music]
input
dim 4ms length scale
it's one
okay um
H wait nope
success don't think we need
this for
I'm num
fine ah
okay so now we have our
model now how do we run it on the
data get
kernel surrogate to Target
fit
suggestions get
suggestions get
model oh is it just a Ford pass yeah it
model oh is it just a Ford pass yeah it
is
what in the hell
where the hell do they call the
model get parito surette
I think you just call the damn
thing H yeah you do and this is going to
thing H yeah you do and this is going to
be values and variances I
be values and variances I
believe GP regression let's
check forward
returns
returns
invariance I assume that's mean
invariance for
okay
so so this fits
right look at this tiny errors
time
time
steps and what are the uh the parameters
here for
so it's learn to completely ignore that
so it's learn to completely ignore that
variable right
yep doesn't
yep doesn't
care this one it probably
cares that's three oh nope one
cares that's three oh nope one
more uh I'm a little concerned I don't
more uh I'm a little concerned I don't
know which one's the learning rate
[Music]
excuse
excuse
me what the hell
oh I see yep that's fine it's two rows I
oh I see yep that's fine it's two rows I
didn't
realize for
the
heck for
really
what the heck was I doing before
little no there's no coin and there will
little no there's no coin and there will
be no coins
the heck if am I doing here thanks love
the heck if am I doing here thanks love
your yeah no problem man I didn't mean
your yeah no problem man I didn't mean
to be rude it's just I get
to be rude it's just I get
like I get crazy crypto folks in here
like I get crazy crypto folks in here
all day asking me because some [ __ ]
all day asking me because some [ __ ]
stole my brand and launched up some like
stole my brand and launched up some like
rug pull coin it's silly I'm currently
rug pull coin it's silly I'm currently
working on uh hyper pram sweep
working on uh hyper pram sweep
stuff trying to get a better hyper pram
stuff trying to get a better hyper pram
sweep algorithm but uh the base code I'm
sweep algorithm but uh the base code I'm
working off of is really not very good
working off of is really not very good
and there's some weird stuff going on so
and there's some weird stuff going on so
I'm currently trying to figure out if
I'm currently trying to figure out if
their Gan process is just messed up or
their Gan process is just messed up or
uh if there's something fundamentally
uh if there's something fundamentally
wrong with what they're
doing clone
this doesn't make sense to me how this
is I was just like
this now the error is
this now the error is
there is that not actually
there is that not actually
copying you know yeah it's going to be
copying you know yeah it's going to be
really cool when it's done but uh I mean
really cool when it's done but uh I mean
the repo is nearly 2,000 lines of
the repo is nearly 2,000 lines of
nonsense should be like 500 at most and
nonsense should be like 500 at most and
uh I I can't tell if they have errors in
uh I I can't tell if they have errors in
there or if there's something weird but
there or if there's something weird but
basically the U it at the moment what's
basically the U it at the moment what's
happening is they've got all these
happening is they've got all these
gaussian processes and they're way
gaussian processes and they're way
underperforming what they should be so
underperforming what they should be so
like anything that you build on top of
like anything that you build on top of
them doesn't work because the core
them doesn't work because the core
predictions coming out of them are not
predictions coming out of them are not
working well enough they're
working well enough they're
directionally correct but they're just
directionally correct but they're just
very
poor this is
Bizarro am I doing something stupid wait
clone diff
ops.
ops.
clone clone do what I think it does
if I set this in
place what in the
heck for
it like
cashing that's freaking so dumb it is
cashing that's freaking so dumb it is
cashing
it is that a torched thing what in the
heck you change it in place and then
heck you change it in place and then
freaking cash it
freaking cash it
holy
holy
hell that's like so
stupid okay so all of these have a quite
stupid okay so all of these have a quite
a bit of error on
a bit of error on
them the two big ones I imagine are
them the two big ones I imagine are
probably the two parameters we care
about I mean isn't it literally like
they're doing linear aggression with SGD
they're doing linear aggression with SGD
that's so stupid um
what's the freaking
what's the freaking
formula it's like pseudo inverse or
formula it's like pseudo inverse or
whatever
so we should just be able to
so we should just be able to
[Music]
do OBS times weights
the
the
hell why are these people over
hell why are these people over
complicating the hell out of it it's
complicating the hell out of it it's
super
easy there it is
does torch have
PMS
yep
for e
something is screwy
something is screwy
here something is very screwy here
okay
AR a
sweep for
now I would assume these are maintained
now I would assume these are maintained
in the same order
okay so
okay so
this this makes sense this is total time
this this makes sense this is total time
time steps
right these other ones don't make sense
right these other ones don't make sense
at all
yeah so it's not just the gtin process
yeah so it's not just the gtin process
it looks like something's wrong with the
it looks like something's wrong with the
data because a linear
fit linear fit should not produce this
fit linear fit should not produce this
and I think that this corresponds to the
and I think that this corresponds to the
errors as well
errors as well
right highest error
right highest error
highest oh it
highest oh it
doesn't yes it does CU this is zero
doesn't yes it does CU this is zero
one low low higher higher low
one low low higher higher low
low yeah okay
okay so this seems to
okay so this seems to
correspond so it looks to me like the
correspond so it looks to me like the
Gan process and the linear like function
Gan process and the linear like function
are learning but the data seems wrong so
are learning but the data seems wrong so
why is the data wrong
suggestion in Pam
suggestion in Pam
suggest
output to param space
observation in
basic
B for
wonder if this returns him in the wrong
order I still don't think that would
order I still don't think that would
do would this do that
goinging to think how I can check on
goinging to think how I can check on
this
wait
huh what the hell is
this this just my clipping
but I doesn't it doesn't get
applied yeah this doesn't do
applied yeah this doesn't do
anything I think this was just
anything I think this was just
checks basic from
Pam real number space
Pam real number space
by
name you think copyright concerns will
name you think copyright concerns will
be an
be an
issue so making a big Kur fuffle about
issue so making a big Kur fuffle about
copyright for uh AI training is probably
copyright for uh AI training is probably
the single stupidest thing you could
the single stupidest thing you could
possibly do
possibly do
um that is like a great way to
um that is like a great way to
completely [ __ ] all of your AI stuff
completely [ __ ] all of your AI stuff
instantly um that should just be thrown
instantly um that should just be thrown
out should just be thrown out
out should just be thrown out
completely you think they care about
completely you think they care about
copyright in China they don't give a
copyright in China they don't give a
[ __ ] you're just putting yourself at a
[ __ ] you're just putting yourself at a
massive massive disadvantage
I mean Japan has just straight up said
I mean Japan has just straight up said
like haha copyright doesn't matter for
like haha copyright doesn't matter for
llm training um that is the correct
llm training um that is the correct
thing to do
mhm
I mean just think about it right like
I mean just think about it right like
this is one of the contributing factors
this is one of the contributing factors
for sure I mean definitely not the only
for sure I mean definitely not the only
one but it's a contributing factor to
one but it's a contributing factor to
why China is able to come out of left
why China is able to come out of left
field and just drop a model like that we
field and just drop a model like that we
have stupid laws like oh yeah we're
have stupid laws like oh yeah we're
going to shoe open AI for like using New
going to shoe open AI for like using New
York Times stuff and you got a license
York Times stuff and you got a license
this data and license that data like
this data and license that data like
piss off you're literally doing nothing
piss off you're literally doing nothing
but harming uh harming progress in
AI Europe is as far as Tech goes Europe
AI Europe is as far as Tech goes Europe
is a
is a
Backwater nothing is coming out of
Backwater nothing is coming out of
Europe anytime soon until they fix their
Europe anytime soon until they fix their
uh their broken legal
system mistol is pretty cool I have no
system mistol is pretty cool I have no
idea how they do anything with the laws
idea how they do anything with the laws
over
there they're probably just scraping
there they're probably just scraping
stuff and hoping they don't get caught
frankly okay so this is in the same
frankly okay so this is in the same
order right holders have
yeah but they're just going to get
yeah but they're just going to get
steamrolled by economic interests
I mean I sympathize with the artists in
I mean I sympathize with the artists in
the sense that we have we got some
the sense that we have we got some
really stupid Tech Bros that seem to go
really stupid Tech Bros that seem to go
out of their way to piss off artists and
out of their way to piss off artists and
go haha look we don't need artists now
go haha look we don't need artists now
we can just generate stuff you're
we can just generate stuff you're
useless haha it's like no you're dumb
useless haha it's like no you're dumb
Tech Bros right um but on the other hand
Tech Bros right um but on the other hand
it's like no you also can't hold all of
it's like no you also can't hold all of
us Tech progress hotl hostage because
us Tech progress hotl hostage because
mopy rights Mist trolls
mopy rights Mist trolls
over they're still releasing model
over they're still releasing model
didn't they just release a model
they'll probably get acquired or
they'll probably get acquired or
something or they'll decide to make all
something or they'll decide to make all
the models for Europe and actually abide
the models for Europe and actually abide
by all the crazy laws there I don't know
by all the crazy laws there I don't know
they'll do something
okay so we do have
this that's so
weird basic from Pam
it like decoding
wrong man
ooh that looks bad
they don't care that
much yeah I mean they're
much yeah I mean they're
just it just being ridiculous
oh wa I'm stupid so this
oh wa I'm stupid so this
is them
okay
research yeah I saw there was some
research yeah I saw there was some
stupid Bill they're just going to get
stupid Bill they're just going to get
smacked for that that's not
happening they also there was another
happening they also there was another
bill was like 20 years in prison for
bill was like 20 years in prison for
downloading deep seek weights or
downloading deep seek weights or
something they're just going to like
something they're just going to like
this [ __ ] doesn't
pass like they've got too many tech
pass like they've got too many tech
people involved now
people involved now
uh in government for [ __ ] like that to
uh in government for [ __ ] like that to
pass you would hope right like Elon or
pass you would hope right like Elon or
somebody is going to look at that and go
somebody is going to look at that and go
what what didot wrote this stop
what what didot wrote this stop
it
right okay so this is not the problem
right okay so this is not the problem
the encode to the decode works just fine
the encode to the decode works just fine
it's very
weird what China Hawk comes up
weird what China Hawk comes up
with I there're senators in their 80s
with I there're senators in their 80s
that have no idea what they're
doing
e e
one is the one that we're running right
one is the one that we're running right
now
linear total time
linear total time
steps learning
steps learning
rate just these two right
so
why um but open source open source here
why um but open source open source here
is good
putting the us behind China is just
putting the us behind China is just
incredibly
incredibly
stupid all that bill does is put the us
stupid all that bill does is put the us
behind
China well the uh deep seek one I don't
China well the uh deep seek one I don't
know about the collaboration
one
e e
the third highest
the third highest
weight is
weight is
there how does it get all these other
there how does it get all these other
spous parameters though what the
heck open source one is it the same I
heck open source one is it the same I
saw like a onepage
saw like a onepage
bill AI for Tesla
no cuz he's he has all of xai now
right I would like to know why this is
right I would like to know why this is
happening
is it just getting spous correlation
is it just getting spous correlation
because it doesn't have enough
because it doesn't have enough
unique sample points or
something guess that would make
something guess that would make
sense but why are none of the weights
sense but why are none of the weights
near
near
zero 0.14 I guess and
.04 which of these
.04 which of these
are wait wait and
are wait wait and
num time steps batch size mini
batches that's crazy for the learning
batches that's crazy for the learning
rate
rate
entropy
Lambda yeah that doesn't make any damn
sense
for e
I guess I have to get other data don't
I can I just
like you know what I know what I can
like you know what I know what I can
do I know what I can
do 15
not this
one this should be 50
so I'm going to see what happens if I
so I'm going to see what happens if I
get random samples if I fit
get random samples if I fit
those and the model is still bad then it
those and the model is still bad then it
is a a bias data problem from the online
is a a bias data problem from the online
sampling procedure
wait a second what the heck this
wait a second what the heck this
shouldn't take this
shouldn't take this
long he doing something stupid
long he doing something stupid
here what is it fitting a model for
what the hell is it
doing
for e
a hello
do I not pass this
what's wrong with
this
uh
for e
kind of
ridiculous Oops I Did did I do torch
ridiculous Oops I Did did I do torch
save I did that's okay I can I can
save I did that's okay I can I can
modify this
how's it still come up with this mess
like what
is this a good
fit I mean it's probably a good fit as
fit I mean it's probably a good fit as
well
it's
wild wa the errors are way lower than
wild wa the errors are way lower than
before for some reason update EPO I
before for some reason update EPO I
guess that gets partially
guess that gets partially
sampled but everything else the errors
sampled but everything else the errors
are way lower than
are way lower than
before so there probably is a sampling
before so there probably is a sampling
bias screw
up
yeah well but the scores are lower as
well yeah you see
why is it that
bad is my intuition just totally shot
bad is my intuition just totally shot
now these days like this
now these days like this
is a regression
is a regression
problem with one target
problem with one target
variable two relevant input variables
variable two relevant input variables
and some distractor
variables
e
e
e e
no Diet Coke for me thank you
okay since this isn't working we're
okay since this isn't working we're
going to try one last
going to try one last
thing on
this for
I
oh top
oh top
pouch that's funny as hell
we have just decided to optimize against
we have just decided to optimize against
zero
okay
yeah that doesn't make any sense at
all that is a
all that is a
fit you get the same parameters
consistently oh no you get totally
consistently oh no you get totally
different
stuff shouldn't be able to
stuff shouldn't be able to
overfit it's a freaking line
h
I can't figure this out I can't figure
I can't figure this out I can't figure
out why this model why you can't fit a
out why this model why you can't fit a
model to this
model to this
data um
is it the we is it like data transform
shenanigans
e
e
e
e
e e
there
scores cost hyers
we will figure this
out scores
got
list cool that's uh normalization
required and we're just going to
do
OBS okay very nice
and
still 1.0 1.0
what the
hell how the hell's that what
on
G okay
how is this
still not
clicking it's random
data e
pushes everything down that's
weird I don't understand how it is that
weird I don't understand how it is that
I'm failing to fit a
line with three different methods
like
shouldn't take that many
shouldn't take that many
points
right
e
for
e
e e
not liking
this I'm actually actually trying to
this I'm actually actually trying to
think what the hell is wrong here and I
think what the hell is wrong here and I
can't
like okay you know
what [ __ ] Test
time super [ __ ] Test time for
okay so that's what I
okay so that's what I
thought that's what's supposed to happen
is you generate
data I guess I have 50 points
only it'll still get it with 50 points
only it'll still get it with 50 points
heck it should get it with 10
points doesn't actually get it with 10
points doesn't actually get it with 10
points interestingly enough 30
okay hold on that's
okay hold on that's
interesting it gets it with
50 wait does it 0 one two three
50 wait does it 0 one two three
yeah two
yeah two
three oh it starts to get I see
yeah that's 10x
yeah that's 10x
okay
okay
so and by 50 points it's got it
tell me that
why does it know this
why does it know this
one learning
rate I guess this is not a linear
rate I guess this is not a linear
function the learning rate
function the learning rate
isn't uh the cost
isn't uh the cost
is but way it's not negative
is but way it's not negative
right shouldn't be
negative oh no it didn't get it at
all that's so
weird I did 5,000 points as well didn't
weird I did 5,000 points as well didn't
I
oh no I apparently didn't
before what did I
do
do
5,000 and and
5,000 and and
data work and then I did it
again well no because we have we still
again well no because we have we still
have extraneous
have extraneous
variables so just came up with a
variables so just came up with a
different bad
different bad
solution maybe it's figured out some of
solution maybe it's figured out some of
these though I mean this looks
these though I mean this looks
better let's see which ones it hasn't
better let's see which ones it hasn't
figured out GAE Lambda I guess yes one
figured out GAE Lambda I guess yes one
two I don't know why that one would be
two I don't know why that one would be
hard gamma
entropy learning rate shouldn't be
negative
e e
isn't this still a bad
isn't this still a bad
model hang
model hang
on that still a bad model isn't it
wait
swear for
oh my God you idiot
how do you [ __ ] that
how do you [ __ ] that
up there you
up there you
go way
closer did it just
closer did it just
fail oh no it just didn't optimize all
fail oh no it just didn't optimize all
the way it looks
the way it looks
like 04 kind of funny it CS
like 04 kind of funny it CS
that uh
there you
there you
go that's what we are looking for
yep you did the same thing here I don't
yep you did the same thing here I don't
know how I [ __ ] that up
okay
and M's total times this what the hell
and M's total times this what the hell
one two three four
one two three four
okay so we get learning right
here there are a couple spous
ones this with freaking 5,000 points
though love this project great to see it
though love this project great to see it
come together thank you I'm currently
come together thank you I'm currently
fighting weird weird things very weird
fighting weird weird things very weird
things
hyper pram optimization is going to work
hyper pram optimization is going to work
honestly the only problem I'm having
honestly the only problem I'm having
right now is I'm trying to figure out
right now is I'm trying to figure out
what the heck is wrong with um they're
what the heck is wrong with um they're
just like these tiny little models that
just like these tiny little models that
need to fit hyper prams um that just
need to fit hyper prams um that just
need to like predict from a small data
need to like predict from a small data
set of experiments what are what hyper
set of experiments what are what hyper
prams are going to do and uh for some
prams are going to do and uh for some
reason it's it's being weird
this like substantially different from
this like substantially different from
run to
run no wait
what yeah okay so this is
nuts oh no this is the same
so for whatever reason though it's not
so for whatever reason though it's not
picking up total time steps as a
variable this should be very weird
variable this should be very weird
right what one did I
right what one did I
have test hold on maybe I did the wrong
have test hold on maybe I did the wrong
data it's
data it's
possible if so I think I'm going to take
possible if so I think I'm going to take
a break tomorrow because I should not be
making these
mistakes yeah see this is just me being
mistakes yeah see this is just me being
like that's such a stupid mistake to get
like that's such a stupid mistake to get
hung up on Jesus
yep and immediately there we go so we
yep and immediately there we go so we
have it's a little weird that we get
have it's a little weird that we get
like these spur coefficient still on
here it's a little weird
okay yeah it's literally it's just
okay yeah it's literally it's just
um
Optimizer cool
so now you only get 50
points and this is
fit this spous one is very weird
oh I guess it biases off with
this can I disable the bias
it's really weird how they
it's really weird how they
still do
this what's the plan I'm currently
this what's the plan I'm currently
attempting to improve uh this hyper pram
attempting to improve uh this hyper pram
sweep algorithm but I'm having a bit of
sweep algorithm but I'm having a bit of
trouble what essentially set all this
trouble what essentially set all this
mess off uh that I'm dealing with at the
mess off uh that I'm dealing with at the
moment is the gaussian processes that
moment is the gaussian processes that
are trained online to estimate the score
are trained online to estimate the score
of Any Given like query set of hyper
of Any Given like query set of hyper
parameters these are very important they
parameters these are very important they
were very inaccurate and uh basically
were very inaccurate and uh basically
anything you do on top of them is going
anything you do on top of them is going
to be very limited by the accuracy of
to be very limited by the accuracy of
that little model and it should be able
that little model and it should be able
to be pretty good so I was trying to
to be pretty good so I was trying to
figure out what was wrong
98 as low as it goes
98 as low as it goes
huh well at least now we get the strong
huh well at least now we get the strong
cost effect in there and uh we get Z
cost effect in there and uh we get Z
wait 1 two 3
wait 1 two 3
four yeah we get the the learning rate
four yeah we get the the learning rate
param in here and then there's some
param in here and then there's some
spous stuff over
spous stuff over
here uh the spous ones
here uh the spous ones
are for some reason gamma and
are for some reason gamma and
Lambda you know what I bet that is I bet
Lambda you know what I bet that is I bet
they just offset each other because
they just offset each other because
they're pretty close to
they're pretty close to
constant I bet that's probably all it is
constant I bet that's probably all it is
what if I just add a little weight Decay
what if I just add a little weight Decay
now is this actually will this fix it
[Music]
now that's just crushing the model
now that's just crushing the model
though it is actually fixing the
though it is actually fixing the
uh this is the highest you get away
with better than
with better than
nothing okay so
can we at least recover this from
can we at least recover this from
the the gaussian process let's
the the gaussian process let's
see what's our gaussian
see what's our gaussian
process going to uh going to give
us I'll tell you what let's do
linear
error six
error six
then let's do this
now come on
okay so I mean if this is
okay so I mean if this is
comparable it fits the train set way
comparable it fits the train set way
better and then
this
this
parameter these two are the important
parameter these two are the important
ones that we care about and then these
ones that we care about and then these
are the spous
are the spous
ones which again I think probably
ones which again I think probably
because they're so close to
constants that's probably what's messing
constants that's probably what's messing
it
up that would make sense
which is why the data transforms are
important what's a quick transform to
important what's a quick transform to
unit normal
subtract
subtract
oh duh
oh duh
uh okay I'm definitely getting
tired subtract the mean and divide by
tired subtract the mean and divide by
the standard
deviation okay so the the gaussian
deviation okay so the the gaussian
process error went way
process error went way
down and the spous correlations went
down and the spous correlations went
down as
down as
well uh the this did not go
well uh the this did not go
down let me see
linear error went up oh yeah because
linear error went up oh yeah because
it's transformed into normal space
it's transformed into normal space
obviously so it's no longer a linear fit
obviously so it's no longer a linear fit
so that's why I couldn't fit a normal uh
so that's why I couldn't fit a normal uh
a linear to the data before obviously
a linear to the data before obviously
because it was transformed into uh into
because it was transformed into uh into
that space But the gine is perfectly
that space But the gine is perfectly
fine and this actually fixes it cool um
fine and this actually fixes it cool um
very nice so
cool I now
cool I now
understand
understand
roughly how to make these things
work this is with 50
points it's a little disappointing that
points it's a little disappointing that
there's still so much
weight on this
so mhm
that worse yeah
interestingly still
worse yeah they're right this is the
worse yeah they're right this is the
best kernel
h
it's weird that there is so much
it's weird that there is so much
substantial error
how much does more data
help man help the bed
still have a lot of
Arrow
H their parameter
here way
worse I'm just playing around to see if
worse I'm just playing around to see if
I happen on anything
I think that's worse for 500 points
I think that's worse for 500 points
right so two and point2
[Music]
there are some tuning things there is
there are some tuning things there is
some tuning that can be done
there still a lot of error
well hang on it's a lot of error if you
well hang on it's a lot of error if you
delete a parameter from the
delete a parameter from the
set that's not a good
set that's not a good
measure it fits perfectly
I'm trying to think if I want
I'm trying to think if I want
to just start rewriting the uh the code
to just start rewriting the uh the code
base because the original is just such a
base because the original is just such a
mess it's like impossible to figure
mess it's like impossible to figure
anything out like
anything out like
this I think I could pretty easily do it
this I think I could pretty easily do it
in a small fraction of the original
in a small fraction of the original
length probably make it a lot easier to
length probably make it a lot easier to
reason
reason
about but there are a lot of little
about but there are a lot of little
details to be careful with
I am really sick of dealing with this
I am really sick of dealing with this
code
though let me
think
yeah okay here's what I'm going to do
yeah okay here's what I'm going to do
for folks on YouTube Here's the uh the
for folks on YouTube Here's the uh the
current
current
situation I've got what I think should
situation I've got what I think should
be some very major improvements to this
be some very major improvements to this
algorithm the original doesn't make
algorithm the original doesn't make
mathematical sense in a ton of different
mathematical sense in a ton of different
ways they got very good results
ways they got very good results
regardless but there were definitely
regardless but there were definitely
places for improvement I made some
places for improvement I made some
modifications and got some initial
modifications and got some initial
results including on a bunch of real RL
results including on a bunch of real RL
environments that are very promising
environments that are very promising
but there's still a lot more that can be
but there's still a lot more that can be
done and there's still some edge cases
done and there's still some edge cases
this morning I got very compelling
this morning I got very compelling
evidence of these edge cases and of
evidence of these edge cases and of
places where uh my new and improved
places where uh my new and improved
algorithm still does not work I did this
algorithm still does not work I did this
by running it on synthetic
by running it on synthetic
benchmarks where the outcome is known
benchmarks where the outcome is known
and the task should be easy and uh any
and the task should be easy and uh any
San algorithm should be able to do it
San algorithm should be able to do it
doesn't mean that most of them do but
doesn't mean that most of them do but
you know they should be able to solve
you know they should be able to solve
it um
it um
then I started trying to modify the
then I started trying to modify the
original code base to include my changes
original code base to include my changes
which was very easy but I ran into some
which was very easy but I ran into some
difficulties because the code is just
difficulties because the code is just
really noisy um you're kind of rely on
really noisy um you're kind of rely on
having a good estimate of the uh the
having a good estimate of the uh the
gaussian process it has to be a good
gaussian process it has to be a good
estimate for you to really get
estimate for you to really get
anything and that wasn't really
anything and that wasn't really
happening um
now on one hand it's the it is the case
now on one hand it's the it is the case
that I you kind of have the same
that I you kind of have the same
gaussian process before and after so I
gaussian process before and after so I
could
could
probably continue editing on it a
bit and maybe I should I don't
know it's like I really want to just
know it's like I really want to just
start rewriting it but it still might be
start rewriting it but it still might be
a little bit
early maybe we run a few more tests
first knowing that this is an issue so
first knowing that this is an issue so
even this simple test should not yield
even this simple test should not yield
perfect performance
perfect performance
right let me maybe do that first and
right let me maybe do that first and
then we'll try rewriting
let's you know what we can do that will
let's you know what we can do that will
make it a little easier let's seat it
make it a little easier let's seat it
with
with
like let's seat it with 50 random
samples that should make it a little
samples that should make it a little
easier for the
easier for the
uh the algorithm and they should run
uh the algorithm and they should run
instantly
synthetic
linear so we'll basically we'll cheat
linear so we'll basically we'll cheat
some random samples in and we'll assume
some random samples in and we'll assume
that we'll be able to get the random
that we'll be able to get the random
samples F uh reduced uh requirement
samples F uh reduced uh requirement
reduced just by improving the GP I think
reduced just by improving the GP I think
that's
reasonable data path is going to be
so 100 runs which is 50 carb samples
yeah so here is
here's our current
function this is the full
version of our function
oh also I've had this run going in the
oh also I've had this run going in the
background the whole time so you can
background the whole time so you can
actually see even the current
version where we've gotten uh our
results looks like still around 91% is
results looks like still around 91% is
the best
the best
91 on this
task oh yeah this will be a very nice
task oh yeah this will be a very nice
algorithm if we can improve over this
algorithm if we can improve over this
even more
even more
holy will be great and I think that this
holy will be great and I think that this
is one of the most important things you
is one of the most important things you
can do in RL right now I really
can do in RL right now I really
do having consistent hyper pram sweeps
do having consistent hyper pram sweeps
that you can trust very
that you can trust very
important this was an earlier
important this was an earlier
test the shape of this is decent but uh
test the shape of this is decent but uh
it doesn't it needs to go explore
it doesn't it needs to go explore
further out in cost quicker than
this so we're basically what we're doing
this so we're basically what we're doing
right now is just because I know that
right now is just because I know that
the sh gin processes are a little wonky
the sh gin processes are a little wonky
we're just spotting it a little bit of
we're just spotting it a little bit of
training data in this region right here
training data in this region right here
for the most part you know it might
for the most part you know it might
occasionally sample a way out there
occasionally sample a way out there
point but we're spotting it a little bit
point but we're spotting it a little bit
of training data and hopefully we can
of training data and hopefully we can
get something out of
get something out of
that this one seems to be very
conservative and we will load this up in
conservative and we will load this up in
a
a
second once it finishes
second once it finishes
running it's pretty quick amusingly I
running it's pretty quick amusingly I
sped their algorithm up like a by factor
sped their algorithm up like a by factor
of five by taking it off of the GPU
of five by taking it off of the GPU
because they were training a tiny little
because they were training a tiny little
model on a
model on a
GPU for
okay so here is currently what we've got
okay so here is currently what we've got
uh like I said the blue points down here
uh like I said the blue points down here
we spotted at some data these are
we spotted at some data these are
randomly sampled including this one and
randomly sampled including this one and
even this random one out here so we
even this random one out here so we
don't blame these points at
don't blame these points at
all the only thing that we're concerned
all the only thing that we're concerned
about here is the fact that this takes
about here is the fact that this takes
so long to fit this line This Is 50
so long to fit this line This Is 50
points and we should be fitting way
points and we should be fitting way
farther out there so let's look at what
farther out there so let's look at what
we have
here the first
thing that we are going to look at
is is commenting the cost in this new
version so this no longer penalizes you
version so this no longer penalizes you
for running long
experiments and then if this does not do
experiments and then if this does not do
it for us then we will uh we'll rethink
it for us then we will uh we'll rethink
a
a
bit because right now we have
bit because right now we have
gpyx which is the model's prediction of
gpyx which is the model's prediction of
our
our
performance we subtract y nearest which
performance we subtract y nearest which
is the uh the nearest Paro point that is
is the uh the nearest Paro point that is
faster a faster experiment than what we
faster a faster experiment than what we
currently
have so you're taking the difference in
have so you're taking the difference in
performance between your the neck like
performance between your the neck like
the next fastest run that you have and
the next fastest run that you have and
this sample and then you divide by the
this sample and then you divide by the
cost of the sample
you would think that this would
you would think that this would
actually oh you know
actually oh you know
what when you divide by the cost like
what when you divide by the cost like
that I think you have to already have
that I think you have to already have
dealt with uh you have to have already
dealt with uh you have to have already
normalized the target estimates right
normalized the target estimates right
yeah yeah yeah okay so there was I was
yeah yeah yeah okay so there was I was
going to I got sidetracked with a lot of
going to I got sidetracked with a lot of
stuff but I was going to do some
stuff but I was going to do some
normalization on the top term um because
normalization on the top term um because
right now it's just whatever score comes
right now it's just whatever score comes
out of the environment
out of the environment
so yeah there's not much incentive based
so yeah there's not much incentive based
on the slope of this thing right this is
on the slope of this thing right this is
just like a slope of one so if you if
just like a slope of one so if you if
you double the cost you double the
you double the cost you double the
performance but you get charged twice as
performance but you get charged twice as
much so there's really not much
much so there's really not much
incentive to
incentive to
explore
um cuz let me see well no because you
um cuz let me see well no because you
still are incentivized to fill out the
still are incentivized to fill out the
Pito front I
Pito front I
think you should still be incentivized
think you should still be incentivized
to fill out the Pito front it doesn't
to fill out the Pito front it doesn't
look like that's what's happened
though here we're going to do we're
though here we're going to do we're
going to take this one we're going to
going to take this one we're going to
put it
put it
here so it's a little bit less
here so it's a little bit less
conservative and it makes a few more
conservative and it makes a few more
errors we're not too concerned with
errors we're not too concerned with
these but really
barely so why is
that this should
that this should
be this should be pushing you towards
be this should be pushing you towards
higher cost like mad
Target
estimate hey
welcome they should be pushing you to
welcome they should be pushing you to
higher cost like really
higher cost like really
fast
fast
gpyx minus y
nearest well we don't need the REO right
nearest well we don't need the REO right
here
you don't need the reu right here I
you don't need the reu right here I
don't know
don't know
why that would
why that would
matter we don't need the re
there it'll be interesting if it does
there it'll be interesting if it does
matter
though cuz that will also tell us
though cuz that will also tell us
something
let's play around with
this is it also it is possible
this is it also it is possible
technically that I've messed something
technically that I've messed something
up in this code if anybody sees
up in this code if anybody sees
anything um so this is our estimate this
anything um so this is our estimate this
is our Paro optimal cost or the cost of
is our Paro optimal cost or the cost of
our poo optimal
our poo optimal
points wait
what no GPC X yes okay this is the cost
what no GPC X yes okay this is the cost
estimate this is the Paro cost
anything that is more
expensive than our current estimated
expensive than our current estimated
point is not going to get sampled we're
point is not going to get sampled we're
going to take a
going to take a
in we find the index of this plus
difference then we get the Paro
index and we use that as y
nearest okay this R does not seem to
nearest okay this R does not seem to
matter which is what we would expect
I can just tell looking at these numbers
I can just tell looking at these numbers
that this isn't going to
help here is the simplest possible term
instantly instantly jumps to the highend
instantly instantly jumps to the highend
cost so it knows
look at
that okay so when I remove this Baseline
that okay so when I remove this Baseline
turn it instantly jumps up to the top so
turn it instantly jumps up to the top so
I think that there's something might be
I think that there's something might be
wrong in this Cal
wrong in this Cal
because I it shouldn't it should not
because I it shouldn't it should not
make a difference like that
okay so here are your parito
costs and here's your G
PCX
PCX
GPC
X okay
cost
cost
difference this is
difference this is
like 7 which is correct and
like 7 which is correct and
then
then
S 4 so these look good
so then let me see if for 1.7 it's
so then let me see if for 1.7 it's
probably going to
be wait
be wait
zero
seven
closest
Six well that's fine because these are
Six well that's fine because these are
equivalent
and then what was
and then what was
it 25 for this High
one yeah it's going to be all the way at
one yeah it's going to be all the way at
the
the
end the 08 is zero
that's a little bit of a
that's a little bit of a
quirk but 08 is zero yeah
hey these Pito indices look fine to
me so
right well du
process
me see
this so parito
this so parito
costs
gpyx GP no GP
CX
1.7 goes to here which is
1.49 how's this happen oh wait no it's
1.49 how's this happen oh wait no it's
the other
the other
one yeah 1.49 okay
one yeah 1.49 okay
and this is
10.8 uh
10.8 uh
10.18 which yeah probably did
this
okay
okay
so
GPC and this only predict 7 okay so this
GPC and this only predict 7 okay so this
estimate is just
wrong it has no capability at
wrong it has no capability at
all uh to predict score it
seems or maybe not to predict
cost this difference is right this
cost this difference is right this
formula looks good
I guess this is the issue when you have
I guess this is the issue when you have
two estimates it's directionally
two estimates it's directionally
correct but then you you take two of
correct but then you you take two of
them you put them together as a
ratio it
ratio it
underestimates very severely
Target
estimate if you take gpyx the thing is
the cost estimate ends up being
the cost estimate ends up being
right or more
right so if you divide by
screwed
screwed
man I mean this is tough I have the
man I mean this is tough I have the
right formula here but it's
like we're just getting screwed by by
like we're just getting screwed by by
the uh the inaccuracy of the gaussian
process which must be why our previous
process which must be why our previous
formula works so well because it just
formula works so well because it just
biases you towards absolute performance
biases you towards absolute performance
and to kind of to screw everything else
how do you solve
that you got wine here
nearest it's very weird
those things aren't bothering
us I mean okay let's say that I use my
us I mean okay let's say that I use my
full formula
when referencing the M's and
puffer and assum
puffer and assum
code um not really there are a couple
code um not really there are a couple
contributor M that were before people
contributor M that were before people
were uh before like I had the
were uh before like I had the
opportunity to do a bunch of code
opportunity to do a bunch of code
reviews um so if you start seeing like
reviews um so if you start seeing like
double or triple pointers or Shenanigans
double or triple pointers or Shenanigans
or stuff that looks weird and redundant
or stuff that looks weird and redundant
it prob probably
it prob probably
is the M's that I have personally
is the M's that I have personally
written at this point are MOA Nur Mo
written at this point are MOA Nur Mo
snake and then the new grid that's in
ablations some of Spencer's recent stuff
ablations some of Spencer's recent stuff
is pretty darn good Captain's physics M
is pretty darn good Captain's physics M
is so far pretty good know his TCG stuff
is so far pretty good know his TCG stuff
has been mostly uh pretty good as well
has been mostly uh pretty good as well
his uh additions to
his uh additions to
that I think that the current code in
that I think that the current code in
there just TCG and and um robocode I
there just TCG and and um robocode I
don't think we've merged them yet so I
don't think we've merged them yet so I
think both of those are my prototypes as
well anything that's currently on my
well anything that's currently on my
screen here is not well written code
hang on have I been stupid
no I think
actually no I've been good this is this
actually no I've been good this is this
is smart this is the way to go e
there's the expected Improvement
formula do I need
that e
so
so
weird m
this is towards highest higher
this is towards highest higher
variance don't want to bias towards
variance don't want to bias towards
higher variance
it's annoying because the only reason
it's annoying because the only reason
that it's staying in that zone is the
that it's staying in that zone is the
model sucks
right maybe it is legitimately time to
right maybe it is legitimately time to
rewrite a big chunk of this code because
like I'm looking at this this formula
like I'm looking at this this formula
looks good to
looks good to
me this looks like this should
me this looks like this should
work e
I mean it's
all you want is for this to be a little
all you want is for this to be a little
sparer
right
okay for
there's no oh there is a
great burrito scores is
correct I could do quantal
correct I could do quantal
or my percentile transform I could do
or my percentile transform I could do
that
now e
instar
instar
nope I do the first one
so what you do is you go and you you go
so what you do is you go and you you go
into a bar with a bunch of programmers
into a bar with a bunch of programmers
and you ask that question and then they
and you ask that question and then they
all shoot each other
oops [ __ ] I closed the wrong window I've
oops [ __ ] I closed the wrong window I've
had that all day well I think that was
had that all day well I think that was
mostly converged anyways but
mostly converged anyways but
[ __ ] uh
so this is what we
have really not that much to be gained
we took cost out and it didn't even
we took cost out and it didn't even
help that's the weird
thing so
thing so
weird why near
don't think I've tried this
yet I want to see if it's really just
yet I want to see if it's really just
the Baseline that's somehow screwing it
the Baseline that's somehow screwing it
up I don't think I tried this
no use the damn
Arrow don't do start thing du use the
Arrow don't do start thing du use the
arrow it exists it exists for that
arrow it exists it exists for that
reason
it's three characters all split up
it's three characters all split up
versus two characters
grouped in the same place you'd expect a
grouped in the same place you'd expect a
DOT to be so yes use the arrow
you can see that the cost predictions
you can see that the cost predictions
are actually like mostly kind of
reasonable and the score predictions
reasonable and the score predictions
just
suck nope no AI editors for me thank you
suck nope no AI editors for me thank you
I use super Maven for mostly single line
I use super Maven for mostly single line
complete that is what I want
okay so
okay so
interestingly just this four
term is also
term is also
enough to screw this up
it's also enough to screw this
it's also enough to screw this
up though I think that that one is more
acceptable let's add um let me add
acceptable let's add um let me add
something
so
for
e
e
e e
nice screw up GPT
oh I do pay for super
Maven I it's stupid not to pay for uh
Maven I it's stupid not to pay for uh
tools that improve your productivity you
tools that improve your productivity you
can afford
them
e e
I do this
instead average what the
instead average what the
hell why all these functions
suck
yeah
yeah
um I honestly prefer Python's white
um I honestly prefer Python's white
space and uh lack of curly braces
space and uh lack of curly braces
I honestly
do
e e
okay
good
e e
interesting e
maybe rank transform is all you
need e
I wonder what happens just with this
oh well that's going to be too much
[ __ ] I mean if you divide the entire
[ __ ] I mean if you divide the entire
thing
okay I don't think this is going to do
okay I don't think this is going to do
anything on its
own what the hell break point did I for
own what the hell break point did I for
get this
get this
one go away
I don't think the rank transform on both
I don't think the rank transform on both
of these makes any sense anymore
of these makes any sense anymore
actually for
h
we just mess everything up with that
don't know how that
don't know how that
happened I'm getting pretty tired here
happened I'm getting pretty tired here
though going to have to break for dinner
though going to have to break for dinner
in a minute
all right I got some stuff that I got to
all right I got some stuff that I got to
think about with
this I'm pretty tired for the time
this I'm pretty tired for the time
being I'm uh I'm going to go get some
being I'm uh I'm going to go get some
food do some
food do some
stuff and we will see from
stuff and we will see from
there for folks tuning
there for folks tuning
in it's uh you know some days you get
in it's uh you know some days you get
like ridiculous progress all at once
like ridiculous progress all at once
some days things go a little slower
I think there is something here it's
I think there is something here it's
just going to take a while to get it
just going to take a while to get it
right and we're definitely going to have
right and we're definitely going to have
to rewrite a lot of that code um but if
to rewrite a lot of that code um but if
you're interested in the work and
you're interested in the work and
following all this stuff it's all free
following all this stuff it's all free
it's all open source the only thing we
it's all open source the only thing we
sell are service packages for companies
sell are service packages for companies
trying to do alll a little bit better um
trying to do alll a little bit better um
you can check out the code right here
you can check out the code right here
star the repo really helps us out if you
star the repo really helps us out if you
star the repo it's all on puffer doai
star the repo it's all on puffer doai
and if you're interested in getting
and if you're interested in getting
involved in contributing using the
involved in contributing using the
library and whatnot go in the Discord
library and whatnot go in the Discord
and if you're looking for Cutting Edge
and if you're looking for Cutting Edge
RL RL tutorials anything in between got
RL RL tutorials anything in between got
a Blog right here all the latest stuff
a Blog right here all the latest stuff
and uh if you want even more RL content
and uh if you want even more RL content
you can follow my
you can follow my
ex there's some articles here you can't
ex there's some articles here you can't
find anywhere else as well as a lot of
find anywhere else as well as a lot of
RL posts pretty much all
RL posts pretty much all
RL so thank you and uh I don't know
RL so thank you and uh I don't know
might be back later tonight might

Kind: captions
Language: en
we are back
live stupid
timeline let's look at uh carbs
so yeah right the the thing that we were
so yeah right the the thing that we were
doing last time we got this data right
doing last time we got this data right
so what we're going to do
is test
is test
GP okay
data hold
on success observation
these need to get mapped to score and
cost oh
cost oh
wait yeah they are
wait yeah they are
there
okay
um
um
output real number input
output real number input
okay so we just do
cost.
tensor
tensor
from
p e c or
score and then we
do uh
OBS
OBS
stack real
number or in data
okay now we have this
okay now we have this
here
and let's go look at a Galan process
DP
regression iro
look at all this
junk heck is this Jitter
GP so this is inputs which
GP so this is inputs which
is
OBS outputs
score kernel
kernel get
model a turn
okay so we want this
[Music]
input
dim 4ms length scale
it's one
okay um
H wait nope
success don't think we need
this for
I'm num
fine ah
okay so now we have our
model now how do we run it on the
data get
kernel surrogate to Target
fit
suggestions get
suggestions get
model oh is it just a Ford pass yeah it
model oh is it just a Ford pass yeah it
is
what in the hell
where the hell do they call the
model get parito surette
I think you just call the damn
thing H yeah you do and this is going to
thing H yeah you do and this is going to
be values and variances I
be values and variances I
believe GP regression let's
check forward
returns
returns
invariance I assume that's mean
invariance for
okay
so so this fits
right look at this tiny errors
time
time
steps and what are the uh the parameters
here for
so it's learn to completely ignore that
so it's learn to completely ignore that
variable right
yep doesn't
yep doesn't
care this one it probably
cares that's three oh nope one
cares that's three oh nope one
more uh I'm a little concerned I don't
more uh I'm a little concerned I don't
know which one's the learning rate
[Music]
excuse
excuse
me what the hell
oh I see yep that's fine it's two rows I
oh I see yep that's fine it's two rows I
didn't
realize for
the
heck for
really
what the heck was I doing before
little no there's no coin and there will
little no there's no coin and there will
be no coins
the heck if am I doing here thanks love
the heck if am I doing here thanks love
your yeah no problem man I didn't mean
your yeah no problem man I didn't mean
to be rude it's just I get
to be rude it's just I get
like I get crazy crypto folks in here
like I get crazy crypto folks in here
all day asking me because some [ __ ]
all day asking me because some [ __ ]
stole my brand and launched up some like
stole my brand and launched up some like
rug pull coin it's silly I'm currently
rug pull coin it's silly I'm currently
working on uh hyper pram sweep
working on uh hyper pram sweep
stuff trying to get a better hyper pram
stuff trying to get a better hyper pram
sweep algorithm but uh the base code I'm
sweep algorithm but uh the base code I'm
working off of is really not very good
working off of is really not very good
and there's some weird stuff going on so
and there's some weird stuff going on so
I'm currently trying to figure out if
I'm currently trying to figure out if
their Gan process is just messed up or
their Gan process is just messed up or
uh if there's something fundamentally
uh if there's something fundamentally
wrong with what they're
doing clone
this doesn't make sense to me how this
is I was just like
this now the error is
this now the error is
there is that not actually
there is that not actually
copying you know yeah it's going to be
copying you know yeah it's going to be
really cool when it's done but uh I mean
really cool when it's done but uh I mean
the repo is nearly 2,000 lines of
the repo is nearly 2,000 lines of
nonsense should be like 500 at most and
nonsense should be like 500 at most and
uh I I can't tell if they have errors in
uh I I can't tell if they have errors in
there or if there's something weird but
there or if there's something weird but
basically the U it at the moment what's
basically the U it at the moment what's
happening is they've got all these
happening is they've got all these
gaussian processes and they're way
gaussian processes and they're way
underperforming what they should be so
underperforming what they should be so
like anything that you build on top of
like anything that you build on top of
them doesn't work because the core
them doesn't work because the core
predictions coming out of them are not
predictions coming out of them are not
working well enough they're
working well enough they're
directionally correct but they're just
directionally correct but they're just
very
poor this is
Bizarro am I doing something stupid wait
clone diff
ops.
ops.
clone clone do what I think it does
if I set this in
place what in the
heck for
it like
cashing that's freaking so dumb it is
cashing that's freaking so dumb it is
cashing
it is that a torched thing what in the
heck you change it in place and then
heck you change it in place and then
freaking cash it
freaking cash it
holy
holy
hell that's like so
stupid okay so all of these have a quite
stupid okay so all of these have a quite
a bit of error on
a bit of error on
them the two big ones I imagine are
them the two big ones I imagine are
probably the two parameters we care
about I mean isn't it literally like
they're doing linear aggression with SGD
they're doing linear aggression with SGD
that's so stupid um
what's the freaking
what's the freaking
formula it's like pseudo inverse or
formula it's like pseudo inverse or
whatever
so we should just be able to
so we should just be able to
[Music]
do OBS times weights
the
the
hell why are these people over
hell why are these people over
complicating the hell out of it it's
complicating the hell out of it it's
super
easy there it is
does torch have
PMS
yep
for e
something is screwy
something is screwy
here something is very screwy here
okay
AR a
sweep for
now I would assume these are maintained
now I would assume these are maintained
in the same order
okay so
okay so
this this makes sense this is total time
this this makes sense this is total time
time steps
right these other ones don't make sense
right these other ones don't make sense
at all
yeah so it's not just the gtin process
yeah so it's not just the gtin process
it looks like something's wrong with the
it looks like something's wrong with the
data because a linear
fit linear fit should not produce this
fit linear fit should not produce this
and I think that this corresponds to the
and I think that this corresponds to the
errors as well
errors as well
right highest error
right highest error
highest oh it
highest oh it
doesn't yes it does CU this is zero
doesn't yes it does CU this is zero
one low low higher higher low
one low low higher higher low
low yeah okay
okay so this seems to
okay so this seems to
correspond so it looks to me like the
correspond so it looks to me like the
Gan process and the linear like function
Gan process and the linear like function
are learning but the data seems wrong so
are learning but the data seems wrong so
why is the data wrong
suggestion in Pam
suggestion in Pam
suggest
output to param space
observation in
basic
B for
wonder if this returns him in the wrong
order I still don't think that would
order I still don't think that would
do would this do that
goinging to think how I can check on
goinging to think how I can check on
this
wait
huh what the hell is
this this just my clipping
but I doesn't it doesn't get
applied yeah this doesn't do
applied yeah this doesn't do
anything I think this was just
anything I think this was just
checks basic from
Pam real number space
Pam real number space
by
name you think copyright concerns will
name you think copyright concerns will
be an
be an
issue so making a big Kur fuffle about
issue so making a big Kur fuffle about
copyright for uh AI training is probably
copyright for uh AI training is probably
the single stupidest thing you could
the single stupidest thing you could
possibly do
possibly do
um that is like a great way to
um that is like a great way to
completely [ __ ] all of your AI stuff
completely [ __ ] all of your AI stuff
instantly um that should just be thrown
instantly um that should just be thrown
out should just be thrown out
out should just be thrown out
completely you think they care about
completely you think they care about
copyright in China they don't give a
copyright in China they don't give a
[ __ ] you're just putting yourself at a
[ __ ] you're just putting yourself at a
massive massive disadvantage
I mean Japan has just straight up said
I mean Japan has just straight up said
like haha copyright doesn't matter for
like haha copyright doesn't matter for
llm training um that is the correct
llm training um that is the correct
thing to do
mhm
I mean just think about it right like
I mean just think about it right like
this is one of the contributing factors
this is one of the contributing factors
for sure I mean definitely not the only
for sure I mean definitely not the only
one but it's a contributing factor to
one but it's a contributing factor to
why China is able to come out of left
why China is able to come out of left
field and just drop a model like that we
field and just drop a model like that we
have stupid laws like oh yeah we're
have stupid laws like oh yeah we're
going to shoe open AI for like using New
going to shoe open AI for like using New
York Times stuff and you got a license
York Times stuff and you got a license
this data and license that data like
this data and license that data like
piss off you're literally doing nothing
piss off you're literally doing nothing
but harming uh harming progress in
AI Europe is as far as Tech goes Europe
AI Europe is as far as Tech goes Europe
is a
is a
Backwater nothing is coming out of
Backwater nothing is coming out of
Europe anytime soon until they fix their
Europe anytime soon until they fix their
uh their broken legal
system mistol is pretty cool I have no
system mistol is pretty cool I have no
idea how they do anything with the laws
idea how they do anything with the laws
over
there they're probably just scraping
there they're probably just scraping
stuff and hoping they don't get caught
frankly okay so this is in the same
frankly okay so this is in the same
order right holders have
yeah but they're just going to get
yeah but they're just going to get
steamrolled by economic interests
I mean I sympathize with the artists in
I mean I sympathize with the artists in
the sense that we have we got some
the sense that we have we got some
really stupid Tech Bros that seem to go
really stupid Tech Bros that seem to go
out of their way to piss off artists and
out of their way to piss off artists and
go haha look we don't need artists now
go haha look we don't need artists now
we can just generate stuff you're
we can just generate stuff you're
useless haha it's like no you're dumb
useless haha it's like no you're dumb
Tech Bros right um but on the other hand
Tech Bros right um but on the other hand
it's like no you also can't hold all of
it's like no you also can't hold all of
us Tech progress hotl hostage because
us Tech progress hotl hostage because
mopy rights Mist trolls
mopy rights Mist trolls
over they're still releasing model
over they're still releasing model
didn't they just release a model
they'll probably get acquired or
they'll probably get acquired or
something or they'll decide to make all
something or they'll decide to make all
the models for Europe and actually abide
the models for Europe and actually abide
by all the crazy laws there I don't know
by all the crazy laws there I don't know
they'll do something
okay so we do have
this that's so
weird basic from Pam
it like decoding
wrong man
ooh that looks bad
they don't care that
much yeah I mean they're
much yeah I mean they're
just it just being ridiculous
oh wa I'm stupid so this
oh wa I'm stupid so this
is them
okay
research yeah I saw there was some
research yeah I saw there was some
stupid Bill they're just going to get
stupid Bill they're just going to get
smacked for that that's not
happening they also there was another
happening they also there was another
bill was like 20 years in prison for
bill was like 20 years in prison for
downloading deep seek weights or
downloading deep seek weights or
something they're just going to like
something they're just going to like
this [ __ ] doesn't
pass like they've got too many tech
pass like they've got too many tech
people involved now
people involved now
uh in government for [ __ ] like that to
uh in government for [ __ ] like that to
pass you would hope right like Elon or
pass you would hope right like Elon or
somebody is going to look at that and go
somebody is going to look at that and go
what what didot wrote this stop
what what didot wrote this stop
it
right okay so this is not the problem
right okay so this is not the problem
the encode to the decode works just fine
the encode to the decode works just fine
it's very
weird what China Hawk comes up
weird what China Hawk comes up
with I there're senators in their 80s
with I there're senators in their 80s
that have no idea what they're
doing
e e
one is the one that we're running right
one is the one that we're running right
now
linear total time
linear total time
steps learning
steps learning
rate just these two right
so
why um but open source open source here
why um but open source open source here
is good
putting the us behind China is just
putting the us behind China is just
incredibly
incredibly
stupid all that bill does is put the us
stupid all that bill does is put the us
behind
China well the uh deep seek one I don't
China well the uh deep seek one I don't
know about the collaboration
one
e e
the third highest
the third highest
weight is
weight is
there how does it get all these other
there how does it get all these other
spous parameters though what the
heck open source one is it the same I
heck open source one is it the same I
saw like a onepage
saw like a onepage
bill AI for Tesla
no cuz he's he has all of xai now
right I would like to know why this is
right I would like to know why this is
happening
is it just getting spous correlation
is it just getting spous correlation
because it doesn't have enough
because it doesn't have enough
unique sample points or
something guess that would make
something guess that would make
sense but why are none of the weights
sense but why are none of the weights
near
near
zero 0.14 I guess and
.04 which of these
.04 which of these
are wait wait and
are wait wait and
num time steps batch size mini
batches that's crazy for the learning
batches that's crazy for the learning
rate
rate
entropy
Lambda yeah that doesn't make any damn
sense
for e
I guess I have to get other data don't
I can I just
like you know what I know what I can
like you know what I know what I can
do I know what I can
do 15
not this
one this should be 50
so I'm going to see what happens if I
so I'm going to see what happens if I
get random samples if I fit
get random samples if I fit
those and the model is still bad then it
those and the model is still bad then it
is a a bias data problem from the online
is a a bias data problem from the online
sampling procedure
wait a second what the heck this
wait a second what the heck this
shouldn't take this
shouldn't take this
long he doing something stupid
long he doing something stupid
here what is it fitting a model for
what the hell is it
doing
for e
a hello
do I not pass this
what's wrong with
this
uh
for e
kind of
ridiculous Oops I Did did I do torch
ridiculous Oops I Did did I do torch
save I did that's okay I can I can
save I did that's okay I can I can
modify this
how's it still come up with this mess
like what
is this a good
fit I mean it's probably a good fit as
fit I mean it's probably a good fit as
well
it's
wild wa the errors are way lower than
wild wa the errors are way lower than
before for some reason update EPO I
before for some reason update EPO I
guess that gets partially
guess that gets partially
sampled but everything else the errors
sampled but everything else the errors
are way lower than
are way lower than
before so there probably is a sampling
before so there probably is a sampling
bias screw
up
yeah well but the scores are lower as
well yeah you see
why is it that
bad is my intuition just totally shot
bad is my intuition just totally shot
now these days like this
now these days like this
is a regression
is a regression
problem with one target
problem with one target
variable two relevant input variables
variable two relevant input variables
and some distractor
variables
e
e
e e
no Diet Coke for me thank you
okay since this isn't working we're
okay since this isn't working we're
going to try one last
going to try one last
thing on
this for
I
oh top
oh top
pouch that's funny as hell
we have just decided to optimize against
we have just decided to optimize against
zero
okay
yeah that doesn't make any sense at
all that is a
all that is a
fit you get the same parameters
consistently oh no you get totally
consistently oh no you get totally
different
stuff shouldn't be able to
stuff shouldn't be able to
overfit it's a freaking line
h
I can't figure this out I can't figure
I can't figure this out I can't figure
out why this model why you can't fit a
out why this model why you can't fit a
model to this
model to this
data um
is it the we is it like data transform
shenanigans
e
e
e
e
e e
there
scores cost hyers
we will figure this
out scores
got
list cool that's uh normalization
required and we're just going to
do
OBS okay very nice
and
still 1.0 1.0
what the
hell how the hell's that what
on
G okay
how is this
still not
clicking it's random
data e
pushes everything down that's
weird I don't understand how it is that
weird I don't understand how it is that
I'm failing to fit a
line with three different methods
like
shouldn't take that many
shouldn't take that many
points
right
e
for
e
e e
not liking
this I'm actually actually trying to
this I'm actually actually trying to
think what the hell is wrong here and I
think what the hell is wrong here and I
can't
like okay you know
what [ __ ] Test
time super [ __ ] Test time for
okay so that's what I
okay so that's what I
thought that's what's supposed to happen
is you generate
data I guess I have 50 points
only it'll still get it with 50 points
only it'll still get it with 50 points
heck it should get it with 10
points doesn't actually get it with 10
points doesn't actually get it with 10
points interestingly enough 30
okay hold on that's
okay hold on that's
interesting it gets it with
50 wait does it 0 one two three
50 wait does it 0 one two three
yeah two
yeah two
three oh it starts to get I see
yeah that's 10x
yeah that's 10x
okay
okay
so and by 50 points it's got it
tell me that
why does it know this
why does it know this
one learning
rate I guess this is not a linear
rate I guess this is not a linear
function the learning rate
function the learning rate
isn't uh the cost
isn't uh the cost
is but way it's not negative
is but way it's not negative
right shouldn't be
negative oh no it didn't get it at
all that's so
weird I did 5,000 points as well didn't
weird I did 5,000 points as well didn't
I
oh no I apparently didn't
before what did I
do
do
5,000 and and
5,000 and and
data work and then I did it
again well no because we have we still
again well no because we have we still
have extraneous
have extraneous
variables so just came up with a
variables so just came up with a
different bad
different bad
solution maybe it's figured out some of
solution maybe it's figured out some of
these though I mean this looks
these though I mean this looks
better let's see which ones it hasn't
better let's see which ones it hasn't
figured out GAE Lambda I guess yes one
figured out GAE Lambda I guess yes one
two I don't know why that one would be
two I don't know why that one would be
hard gamma
entropy learning rate shouldn't be
negative
e e
isn't this still a bad
isn't this still a bad
model hang
model hang
on that still a bad model isn't it
wait
swear for
oh my God you idiot
how do you [ __ ] that
how do you [ __ ] that
up there you
up there you
go way
closer did it just
closer did it just
fail oh no it just didn't optimize all
fail oh no it just didn't optimize all
the way it looks
the way it looks
like 04 kind of funny it CS
like 04 kind of funny it CS
that uh
there you
there you
go that's what we are looking for
yep you did the same thing here I don't
yep you did the same thing here I don't
know how I [ __ ] that up
okay
and M's total times this what the hell
and M's total times this what the hell
one two three four
one two three four
okay so we get learning right
here there are a couple spous
ones this with freaking 5,000 points
though love this project great to see it
though love this project great to see it
come together thank you I'm currently
come together thank you I'm currently
fighting weird weird things very weird
fighting weird weird things very weird
things
hyper pram optimization is going to work
hyper pram optimization is going to work
honestly the only problem I'm having
honestly the only problem I'm having
right now is I'm trying to figure out
right now is I'm trying to figure out
what the heck is wrong with um they're
what the heck is wrong with um they're
just like these tiny little models that
just like these tiny little models that
need to fit hyper prams um that just
need to fit hyper prams um that just
need to like predict from a small data
need to like predict from a small data
set of experiments what are what hyper
set of experiments what are what hyper
prams are going to do and uh for some
prams are going to do and uh for some
reason it's it's being weird
this like substantially different from
this like substantially different from
run to
run no wait
what yeah okay so this is
nuts oh no this is the same
so for whatever reason though it's not
so for whatever reason though it's not
picking up total time steps as a
variable this should be very weird
variable this should be very weird
right what one did I
right what one did I
have test hold on maybe I did the wrong
have test hold on maybe I did the wrong
data it's
data it's
possible if so I think I'm going to take
possible if so I think I'm going to take
a break tomorrow because I should not be
making these
mistakes yeah see this is just me being
mistakes yeah see this is just me being
like that's such a stupid mistake to get
like that's such a stupid mistake to get
hung up on Jesus
yep and immediately there we go so we
yep and immediately there we go so we
have it's a little weird that we get
have it's a little weird that we get
like these spur coefficient still on
here it's a little weird
okay yeah it's literally it's just
okay yeah it's literally it's just
um
Optimizer cool
so now you only get 50
points and this is
fit this spous one is very weird
oh I guess it biases off with
this can I disable the bias
it's really weird how they
it's really weird how they
still do
this what's the plan I'm currently
this what's the plan I'm currently
attempting to improve uh this hyper pram
attempting to improve uh this hyper pram
sweep algorithm but I'm having a bit of
sweep algorithm but I'm having a bit of
trouble what essentially set all this
trouble what essentially set all this
mess off uh that I'm dealing with at the
mess off uh that I'm dealing with at the
moment is the gaussian processes that
moment is the gaussian processes that
are trained online to estimate the score
are trained online to estimate the score
of Any Given like query set of hyper
of Any Given like query set of hyper
parameters these are very important they
parameters these are very important they
were very inaccurate and uh basically
were very inaccurate and uh basically
anything you do on top of them is going
anything you do on top of them is going
to be very limited by the accuracy of
to be very limited by the accuracy of
that little model and it should be able
that little model and it should be able
to be pretty good so I was trying to
to be pretty good so I was trying to
figure out what was wrong
98 as low as it goes
98 as low as it goes
huh well at least now we get the strong
huh well at least now we get the strong
cost effect in there and uh we get Z
cost effect in there and uh we get Z
wait 1 two 3
wait 1 two 3
four yeah we get the the learning rate
four yeah we get the the learning rate
param in here and then there's some
param in here and then there's some
spous stuff over
spous stuff over
here uh the spous ones
here uh the spous ones
are for some reason gamma and
are for some reason gamma and
Lambda you know what I bet that is I bet
Lambda you know what I bet that is I bet
they just offset each other because
they just offset each other because
they're pretty close to
they're pretty close to
constant I bet that's probably all it is
constant I bet that's probably all it is
what if I just add a little weight Decay
what if I just add a little weight Decay
now is this actually will this fix it
[Music]
now that's just crushing the model
now that's just crushing the model
though it is actually fixing the
though it is actually fixing the
uh this is the highest you get away
with better than
with better than
nothing okay so
can we at least recover this from
can we at least recover this from
the the gaussian process let's
the the gaussian process let's
see what's our gaussian
see what's our gaussian
process going to uh going to give
us I'll tell you what let's do
linear
error six
error six
then let's do this
now come on
okay so I mean if this is
okay so I mean if this is
comparable it fits the train set way
comparable it fits the train set way
better and then
this
this
parameter these two are the important
parameter these two are the important
ones that we care about and then these
ones that we care about and then these
are the spous
are the spous
ones which again I think probably
ones which again I think probably
because they're so close to
constants that's probably what's messing
constants that's probably what's messing
it
up that would make sense
which is why the data transforms are
important what's a quick transform to
important what's a quick transform to
unit normal
subtract
subtract
oh duh
oh duh
uh okay I'm definitely getting
tired subtract the mean and divide by
tired subtract the mean and divide by
the standard
deviation okay so the the gaussian
deviation okay so the the gaussian
process error went way
process error went way
down and the spous correlations went
down and the spous correlations went
down as
down as
well uh the this did not go
well uh the this did not go
down let me see
linear error went up oh yeah because
linear error went up oh yeah because
it's transformed into normal space
it's transformed into normal space
obviously so it's no longer a linear fit
obviously so it's no longer a linear fit
so that's why I couldn't fit a normal uh
so that's why I couldn't fit a normal uh
a linear to the data before obviously
a linear to the data before obviously
because it was transformed into uh into
because it was transformed into uh into
that space But the gine is perfectly
that space But the gine is perfectly
fine and this actually fixes it cool um
fine and this actually fixes it cool um
very nice so
cool I now
cool I now
understand
understand
roughly how to make these things
work this is with 50
points it's a little disappointing that
points it's a little disappointing that
there's still so much
weight on this
so mhm
that worse yeah
interestingly still
worse yeah they're right this is the
worse yeah they're right this is the
best kernel
h
it's weird that there is so much
it's weird that there is so much
substantial error
how much does more data
help man help the bed
still have a lot of
Arrow
H their parameter
here way
worse I'm just playing around to see if
worse I'm just playing around to see if
I happen on anything
I think that's worse for 500 points
I think that's worse for 500 points
right so two and point2
[Music]
there are some tuning things there is
there are some tuning things there is
some tuning that can be done
there still a lot of error
well hang on it's a lot of error if you
well hang on it's a lot of error if you
delete a parameter from the
delete a parameter from the
set that's not a good
set that's not a good
measure it fits perfectly
I'm trying to think if I want
I'm trying to think if I want
to just start rewriting the uh the code
to just start rewriting the uh the code
base because the original is just such a
base because the original is just such a
mess it's like impossible to figure
mess it's like impossible to figure
anything out like
anything out like
this I think I could pretty easily do it
this I think I could pretty easily do it
in a small fraction of the original
in a small fraction of the original
length probably make it a lot easier to
length probably make it a lot easier to
reason
reason
about but there are a lot of little
about but there are a lot of little
details to be careful with
I am really sick of dealing with this
I am really sick of dealing with this
code
though let me
think
yeah okay here's what I'm going to do
yeah okay here's what I'm going to do
for folks on YouTube Here's the uh the
for folks on YouTube Here's the uh the
current
current
situation I've got what I think should
situation I've got what I think should
be some very major improvements to this
be some very major improvements to this
algorithm the original doesn't make
algorithm the original doesn't make
mathematical sense in a ton of different
mathematical sense in a ton of different
ways they got very good results
ways they got very good results
regardless but there were definitely
regardless but there were definitely
places for improvement I made some
places for improvement I made some
modifications and got some initial
modifications and got some initial
results including on a bunch of real RL
results including on a bunch of real RL
environments that are very promising
environments that are very promising
but there's still a lot more that can be
but there's still a lot more that can be
done and there's still some edge cases
done and there's still some edge cases
this morning I got very compelling
this morning I got very compelling
evidence of these edge cases and of
evidence of these edge cases and of
places where uh my new and improved
places where uh my new and improved
algorithm still does not work I did this
algorithm still does not work I did this
by running it on synthetic
by running it on synthetic
benchmarks where the outcome is known
benchmarks where the outcome is known
and the task should be easy and uh any
and the task should be easy and uh any
San algorithm should be able to do it
San algorithm should be able to do it
doesn't mean that most of them do but
doesn't mean that most of them do but
you know they should be able to solve
you know they should be able to solve
it um
it um
then I started trying to modify the
then I started trying to modify the
original code base to include my changes
original code base to include my changes
which was very easy but I ran into some
which was very easy but I ran into some
difficulties because the code is just
difficulties because the code is just
really noisy um you're kind of rely on
really noisy um you're kind of rely on
having a good estimate of the uh the
having a good estimate of the uh the
gaussian process it has to be a good
gaussian process it has to be a good
estimate for you to really get
estimate for you to really get
anything and that wasn't really
anything and that wasn't really
happening um
now on one hand it's the it is the case
now on one hand it's the it is the case
that I you kind of have the same
that I you kind of have the same
gaussian process before and after so I
gaussian process before and after so I
could
could
probably continue editing on it a
bit and maybe I should I don't
know it's like I really want to just
know it's like I really want to just
start rewriting it but it still might be
start rewriting it but it still might be
a little bit
early maybe we run a few more tests
first knowing that this is an issue so
first knowing that this is an issue so
even this simple test should not yield
even this simple test should not yield
perfect performance
perfect performance
right let me maybe do that first and
right let me maybe do that first and
then we'll try rewriting
let's you know what we can do that will
let's you know what we can do that will
make it a little easier let's seat it
make it a little easier let's seat it
with
with
like let's seat it with 50 random
samples that should make it a little
samples that should make it a little
easier for the
easier for the
uh the algorithm and they should run
uh the algorithm and they should run
instantly
synthetic
linear so we'll basically we'll cheat
linear so we'll basically we'll cheat
some random samples in and we'll assume
some random samples in and we'll assume
that we'll be able to get the random
that we'll be able to get the random
samples F uh reduced uh requirement
samples F uh reduced uh requirement
reduced just by improving the GP I think
reduced just by improving the GP I think
that's
reasonable data path is going to be
so 100 runs which is 50 carb samples
yeah so here is
here's our current
function this is the full
version of our function
oh also I've had this run going in the
oh also I've had this run going in the
background the whole time so you can
background the whole time so you can
actually see even the current
version where we've gotten uh our
results looks like still around 91% is
results looks like still around 91% is
the best
the best
91 on this
task oh yeah this will be a very nice
task oh yeah this will be a very nice
algorithm if we can improve over this
algorithm if we can improve over this
even more
even more
holy will be great and I think that this
holy will be great and I think that this
is one of the most important things you
is one of the most important things you
can do in RL right now I really
can do in RL right now I really
do having consistent hyper pram sweeps
do having consistent hyper pram sweeps
that you can trust very
that you can trust very
important this was an earlier
important this was an earlier
test the shape of this is decent but uh
test the shape of this is decent but uh
it doesn't it needs to go explore
it doesn't it needs to go explore
further out in cost quicker than
this so we're basically what we're doing
this so we're basically what we're doing
right now is just because I know that
right now is just because I know that
the sh gin processes are a little wonky
the sh gin processes are a little wonky
we're just spotting it a little bit of
we're just spotting it a little bit of
training data in this region right here
training data in this region right here
for the most part you know it might
for the most part you know it might
occasionally sample a way out there
occasionally sample a way out there
point but we're spotting it a little bit
point but we're spotting it a little bit
of training data and hopefully we can
of training data and hopefully we can
get something out of
get something out of
that this one seems to be very
conservative and we will load this up in
conservative and we will load this up in
a
a
second once it finishes
second once it finishes
running it's pretty quick amusingly I
running it's pretty quick amusingly I
sped their algorithm up like a by factor
sped their algorithm up like a by factor
of five by taking it off of the GPU
of five by taking it off of the GPU
because they were training a tiny little
because they were training a tiny little
model on a
model on a
GPU for
okay so here is currently what we've got
okay so here is currently what we've got
uh like I said the blue points down here
uh like I said the blue points down here
we spotted at some data these are
we spotted at some data these are
randomly sampled including this one and
randomly sampled including this one and
even this random one out here so we
even this random one out here so we
don't blame these points at
don't blame these points at
all the only thing that we're concerned
all the only thing that we're concerned
about here is the fact that this takes
about here is the fact that this takes
so long to fit this line This Is 50
so long to fit this line This Is 50
points and we should be fitting way
points and we should be fitting way
farther out there so let's look at what
farther out there so let's look at what
we have
here the first
thing that we are going to look at
is is commenting the cost in this new
version so this no longer penalizes you
version so this no longer penalizes you
for running long
experiments and then if this does not do
experiments and then if this does not do
it for us then we will uh we'll rethink
it for us then we will uh we'll rethink
a
a
bit because right now we have
bit because right now we have
gpyx which is the model's prediction of
gpyx which is the model's prediction of
our
our
performance we subtract y nearest which
performance we subtract y nearest which
is the uh the nearest Paro point that is
is the uh the nearest Paro point that is
faster a faster experiment than what we
faster a faster experiment than what we
currently
have so you're taking the difference in
have so you're taking the difference in
performance between your the neck like
performance between your the neck like
the next fastest run that you have and
the next fastest run that you have and
this sample and then you divide by the
this sample and then you divide by the
cost of the sample
you would think that this would
you would think that this would
actually oh you know
actually oh you know
what when you divide by the cost like
what when you divide by the cost like
that I think you have to already have
that I think you have to already have
dealt with uh you have to have already
dealt with uh you have to have already
normalized the target estimates right
normalized the target estimates right
yeah yeah yeah okay so there was I was
yeah yeah yeah okay so there was I was
going to I got sidetracked with a lot of
going to I got sidetracked with a lot of
stuff but I was going to do some
stuff but I was going to do some
normalization on the top term um because
normalization on the top term um because
right now it's just whatever score comes
right now it's just whatever score comes
out of the environment
out of the environment
so yeah there's not much incentive based
so yeah there's not much incentive based
on the slope of this thing right this is
on the slope of this thing right this is
just like a slope of one so if you if
just like a slope of one so if you if
you double the cost you double the
you double the cost you double the
performance but you get charged twice as
performance but you get charged twice as
much so there's really not much
much so there's really not much
incentive to
incentive to
explore
um cuz let me see well no because you
um cuz let me see well no because you
still are incentivized to fill out the
still are incentivized to fill out the
Pito front I
Pito front I
think you should still be incentivized
think you should still be incentivized
to fill out the Pito front it doesn't
to fill out the Pito front it doesn't
look like that's what's happened
though here we're going to do we're
though here we're going to do we're
going to take this one we're going to
going to take this one we're going to
put it
put it
here so it's a little bit less
here so it's a little bit less
conservative and it makes a few more
conservative and it makes a few more
errors we're not too concerned with
errors we're not too concerned with
these but really
barely so why is
that this should
that this should
be this should be pushing you towards
be this should be pushing you towards
higher cost like mad
Target
estimate hey
welcome they should be pushing you to
welcome they should be pushing you to
higher cost like really
higher cost like really
fast
fast
gpyx minus y
nearest well we don't need the REO right
nearest well we don't need the REO right
here
you don't need the reu right here I
you don't need the reu right here I
don't know
don't know
why that would
why that would
matter we don't need the re
there it'll be interesting if it does
there it'll be interesting if it does
matter
though cuz that will also tell us
though cuz that will also tell us
something
let's play around with
this is it also it is possible
this is it also it is possible
technically that I've messed something
technically that I've messed something
up in this code if anybody sees
up in this code if anybody sees
anything um so this is our estimate this
anything um so this is our estimate this
is our Paro optimal cost or the cost of
is our Paro optimal cost or the cost of
our poo optimal
our poo optimal
points wait
what no GPC X yes okay this is the cost
what no GPC X yes okay this is the cost
estimate this is the Paro cost
anything that is more
expensive than our current estimated
expensive than our current estimated
point is not going to get sampled we're
point is not going to get sampled we're
going to take a
going to take a
in we find the index of this plus
difference then we get the Paro
index and we use that as y
nearest okay this R does not seem to
nearest okay this R does not seem to
matter which is what we would expect
I can just tell looking at these numbers
I can just tell looking at these numbers
that this isn't going to
help here is the simplest possible term
instantly instantly jumps to the highend
instantly instantly jumps to the highend
cost so it knows
look at
that okay so when I remove this Baseline
that okay so when I remove this Baseline
turn it instantly jumps up to the top so
turn it instantly jumps up to the top so
I think that there's something might be
I think that there's something might be
wrong in this Cal
wrong in this Cal
because I it shouldn't it should not
because I it shouldn't it should not
make a difference like that
okay so here are your parito
costs and here's your G
PCX
PCX
GPC
X okay
cost
cost
difference this is
difference this is
like 7 which is correct and
like 7 which is correct and
then
then
S 4 so these look good
so then let me see if for 1.7 it's
so then let me see if for 1.7 it's
probably going to
be wait
be wait
zero
seven
closest
Six well that's fine because these are
Six well that's fine because these are
equivalent
and then what was
and then what was
it 25 for this High
one yeah it's going to be all the way at
one yeah it's going to be all the way at
the
the
end the 08 is zero
that's a little bit of a
that's a little bit of a
quirk but 08 is zero yeah
hey these Pito indices look fine to
me so
right well du
process
me see
this so parito
this so parito
costs
gpyx GP no GP
CX
1.7 goes to here which is
1.49 how's this happen oh wait no it's
1.49 how's this happen oh wait no it's
the other
the other
one yeah 1.49 okay
one yeah 1.49 okay
and this is
10.8 uh
10.8 uh
10.18 which yeah probably did
this
okay
okay
so
GPC and this only predict 7 okay so this
GPC and this only predict 7 okay so this
estimate is just
wrong it has no capability at
wrong it has no capability at
all uh to predict score it
seems or maybe not to predict
cost this difference is right this
cost this difference is right this
formula looks good
I guess this is the issue when you have
I guess this is the issue when you have
two estimates it's directionally
two estimates it's directionally
correct but then you you take two of
correct but then you you take two of
them you put them together as a
ratio it
ratio it
underestimates very severely
Target
estimate if you take gpyx the thing is
the cost estimate ends up being
the cost estimate ends up being
right or more
right so if you divide by
screwed
screwed
man I mean this is tough I have the
man I mean this is tough I have the
right formula here but it's
like we're just getting screwed by by
like we're just getting screwed by by
the uh the inaccuracy of the gaussian
process which must be why our previous
process which must be why our previous
formula works so well because it just
formula works so well because it just
biases you towards absolute performance
biases you towards absolute performance
and to kind of to screw everything else
how do you solve
that you got wine here
nearest it's very weird
those things aren't bothering
us I mean okay let's say that I use my
us I mean okay let's say that I use my
full formula
when referencing the M's and
puffer and assum
puffer and assum
code um not really there are a couple
code um not really there are a couple
contributor M that were before people
contributor M that were before people
were uh before like I had the
were uh before like I had the
opportunity to do a bunch of code
opportunity to do a bunch of code
reviews um so if you start seeing like
reviews um so if you start seeing like
double or triple pointers or Shenanigans
double or triple pointers or Shenanigans
or stuff that looks weird and redundant
or stuff that looks weird and redundant
it prob probably
it prob probably
is the M's that I have personally
is the M's that I have personally
written at this point are MOA Nur Mo
written at this point are MOA Nur Mo
snake and then the new grid that's in
ablations some of Spencer's recent stuff
ablations some of Spencer's recent stuff
is pretty darn good Captain's physics M
is pretty darn good Captain's physics M
is so far pretty good know his TCG stuff
is so far pretty good know his TCG stuff
has been mostly uh pretty good as well
has been mostly uh pretty good as well
his uh additions to
his uh additions to
that I think that the current code in
that I think that the current code in
there just TCG and and um robocode I
there just TCG and and um robocode I
don't think we've merged them yet so I
don't think we've merged them yet so I
think both of those are my prototypes as
well anything that's currently on my
well anything that's currently on my
screen here is not well written code
hang on have I been stupid
no I think
actually no I've been good this is this
actually no I've been good this is this
is smart this is the way to go e
there's the expected Improvement
formula do I need
that e
so
so
weird m
this is towards highest higher
this is towards highest higher
variance don't want to bias towards
variance don't want to bias towards
higher variance
it's annoying because the only reason
it's annoying because the only reason
that it's staying in that zone is the
that it's staying in that zone is the
model sucks
right maybe it is legitimately time to
right maybe it is legitimately time to
rewrite a big chunk of this code because
like I'm looking at this this formula
like I'm looking at this this formula
looks good to
looks good to
me this looks like this should
me this looks like this should
work e
I mean it's
all you want is for this to be a little
all you want is for this to be a little
sparer
right
okay for
there's no oh there is a
great burrito scores is
correct I could do quantal
correct I could do quantal
or my percentile transform I could do
or my percentile transform I could do
that
now e
instar
instar
nope I do the first one
so what you do is you go and you you go
so what you do is you go and you you go
into a bar with a bunch of programmers
into a bar with a bunch of programmers
and you ask that question and then they
and you ask that question and then they
all shoot each other
oops [ __ ] I closed the wrong window I've
oops [ __ ] I closed the wrong window I've
had that all day well I think that was
had that all day well I think that was
mostly converged anyways but
mostly converged anyways but
[ __ ] uh
so this is what we
have really not that much to be gained
we took cost out and it didn't even
we took cost out and it didn't even
help that's the weird
thing so
thing so
weird why near
don't think I've tried this
yet I want to see if it's really just
yet I want to see if it's really just
the Baseline that's somehow screwing it
the Baseline that's somehow screwing it
up I don't think I tried this
no use the damn
Arrow don't do start thing du use the
Arrow don't do start thing du use the
arrow it exists it exists for that
arrow it exists it exists for that
reason
it's three characters all split up
it's three characters all split up
versus two characters
grouped in the same place you'd expect a
grouped in the same place you'd expect a
DOT to be so yes use the arrow
you can see that the cost predictions
you can see that the cost predictions
are actually like mostly kind of
reasonable and the score predictions
reasonable and the score predictions
just
suck nope no AI editors for me thank you
suck nope no AI editors for me thank you
I use super Maven for mostly single line
I use super Maven for mostly single line
complete that is what I want
okay so
okay so
interestingly just this four
term is also
term is also
enough to screw this up
it's also enough to screw this
it's also enough to screw this
up though I think that that one is more
acceptable let's add um let me add
acceptable let's add um let me add
something
so
for
e
e
e e
nice screw up GPT
oh I do pay for super
Maven I it's stupid not to pay for uh
Maven I it's stupid not to pay for uh
tools that improve your productivity you
tools that improve your productivity you
can afford
them
e e
I do this
instead average what the
instead average what the
hell why all these functions
suck
yeah
yeah
um I honestly prefer Python's white
um I honestly prefer Python's white
space and uh lack of curly braces
space and uh lack of curly braces
I honestly
do
e e
okay
good
e e
interesting e
maybe rank transform is all you
need e
I wonder what happens just with this
oh well that's going to be too much
[ __ ] I mean if you divide the entire
[ __ ] I mean if you divide the entire
thing
okay I don't think this is going to do
okay I don't think this is going to do
anything on its
own what the hell break point did I for
own what the hell break point did I for
get this
get this
one go away
I don't think the rank transform on both
I don't think the rank transform on both
of these makes any sense anymore
of these makes any sense anymore
actually for
h
we just mess everything up with that
don't know how that
don't know how that
happened I'm getting pretty tired here
happened I'm getting pretty tired here
though going to have to break for dinner
though going to have to break for dinner
in a minute
all right I got some stuff that I got to
all right I got some stuff that I got to
think about with
this I'm pretty tired for the time
this I'm pretty tired for the time
being I'm uh I'm going to go get some
being I'm uh I'm going to go get some
food do some
food do some
stuff and we will see from
stuff and we will see from
there for folks tuning
there for folks tuning
in it's uh you know some days you get
in it's uh you know some days you get
like ridiculous progress all at once
like ridiculous progress all at once
some days things go a little slower
I think there is something here it's
I think there is something here it's
just going to take a while to get it
just going to take a while to get it
right and we're definitely going to have
right and we're definitely going to have
to rewrite a lot of that code um but if
to rewrite a lot of that code um but if
you're interested in the work and
you're interested in the work and
following all this stuff it's all free
following all this stuff it's all free
it's all open source the only thing we
it's all open source the only thing we
sell are service packages for companies
sell are service packages for companies
trying to do alll a little bit better um
trying to do alll a little bit better um
you can check out the code right here
you can check out the code right here
star the repo really helps us out if you
star the repo really helps us out if you
star the repo it's all on puffer doai
star the repo it's all on puffer doai
and if you're interested in getting
and if you're interested in getting
involved in contributing using the
involved in contributing using the
library and whatnot go in the Discord
library and whatnot go in the Discord
and if you're looking for Cutting Edge
and if you're looking for Cutting Edge
RL RL tutorials anything in between got
RL RL tutorials anything in between got
a Blog right here all the latest stuff
a Blog right here all the latest stuff
and uh if you want even more RL content
and uh if you want even more RL content
you can follow my
you can follow my
ex there's some articles here you can't
ex there's some articles here you can't
find anywhere else as well as a lot of
find anywhere else as well as a lot of
RL posts pretty much all
RL posts pretty much all
RL so thank you and uh I don't know
RL so thank you and uh I don't know
might be back later tonight might
