Kind: captions
Language: en
Okay, we are
Okay, we are
back. Second session of
today. How's the training
going? So
going? So
close. So close to here. All right,
close. So close to here. All right,
we'll let that take a little bit
longer. Instability is kind of
longer. Instability is kind of
silly. We were going to look at clipping
silly. We were going to look at clipping
now,
right? So, I just tried removing
right? So, I just tried removing
clipping from what I thought would be an
clipping from what I thought would be an
end where it wouldn't
end where it wouldn't
matter, and it totally broke.
Hang on. This is with the filtering,
right? Let's rerun it like
right? Let's rerun it like
this. See if it still
this. See if it still
breaks. This should only use each sample
breaks. This should only use each sample
like one time.
Ah, still
problems. So,
problems. So,
um, Whoops.
I uh I guess I was totally wrong
I uh I guess I was totally wrong
there. I didn't think the clipping would
there. I didn't think the clipping would
make that much of a
make that much of a
difference. I figured it would only make
difference. I figured it would only make
a difference if you're running a ton of
a difference if you're running a ton of
epochs.
Oh, the kale exploded here,
Oh, the kale exploded here,
right? Four, five. Yeah, the tail
exploded. Does that make any sense at
exploded. Does that make any sense at
all?
Like, right, hang on. What if we try?
Like, right, hang on. What if we try?
Let me try things
Let me try things
here. We return
clipping or up to eight pots.
clipping or up to eight pots.
You go slow mode.
It's possible this is not strictly
It's possible this is not strictly
better even though it should be,
right?
T. This looks
um fairly similar. maybe a little
better. So, I'll go on record as I
better. So, I'll go on record as I
saying that I thought that I'd looked at
saying that I thought that I'd looked at
the math a couple weeks ago and I
the math a couple weeks ago and I
thought that I'd figured out that for
thought that I'd figured out that for
the most part the on policy off policy
the most part the on policy off policy
distinction was a joke. Uh, it's not. As
distinction was a joke. Uh, it's not. As
it turns out though, the thing that is a
it turns out though, the thing that is a
joke is that
joke is that
um even Rainbow seems to be
um even Rainbow seems to be
kind of breaking its own math
kind of breaking its own math
assumptions. It's not like I is off
assumptions. It's not like I is off
policy is the correct one of the two.
Okay. So, I mean this is
like this is fine,
like this is fine,
right? If you crank up the update
right? If you crank up the update
box, you don't really break anything.
Yeah, it doesn't seem like you really
Yeah, it doesn't seem like you really
break
anything. Let's check the
anything. Let's check the
[Music]
losses. Let's get rid of this one.
One bad grab
spike, but I mean otherwise it tracks
spike, but I mean otherwise it tracks
pretty well and that's a perfect
sol. What happens if you really crank
sol. What happens if you really crank
this up?
I'm just trying to see if clipping is
I'm just trying to see if clipping is
like a band-aid path or if it's
like a band-aid path or if it's
actually
actually
stable the way we would think
I'm really surprised about the clipping
I'm really surprised about the clipping
thing
Yeah, there's clearly stuff going on
Yeah, there's clearly stuff going on
here that I do not understand yet.
So, the thing that I'm not sure here,
So, the thing that I'm not sure here,
right, is like I was kind of looking for
right, is like I was kind of looking for
just chuck all the methods on stuff real
just chuck all the methods on stuff real
quick to see if anything happens.
quick to see if anything happens.
Anything surprising like we did get the
Anything surprising like we did get the
big nice boost from
Muan.
Muan.
Oh, I see.
Uh, I check
something. What do we clip against?
Wait, the PG lost
Wait, the PG lost
is advantage.
New log props minus back.log
props. Oh no, that is good. Yeah, that's
props. Oh no, that is good. Yeah, that's
how clipping should work.
That's
crazy. So,
um, yeah, the PO clipping is not a fix
um, yeah, the PO clipping is not a fix
or off policy. It's a band-aid.
Like you can still go too far off in
Like you can still go too far off in
just one
epoch. Yeah. Look at this thing. Can we
epoch. Yeah. Look at this thing. Can we
even see this in the losses anywhere or
even see this in the losses anywhere or
is it just
bad? Not really. Right.
bad? Not really. Right.
Entropy is
Entropy is
fine. We're not logging
fine. We're not logging
that. Prox K scale is
fine. You do see the value loss quite
fine. You do see the value loss quite
different
here. It's interesting.
clipping is is in fact
clipping is is in fact
essential. It is actually going to keep
essential. It is actually going to keep
training on this and
training on this and
uh it is
uh it is
improving. I mean that definitely
improving. I mean that definitely
damaged it.
That was an unexpected result. I'm
That was an unexpected result. I'm
trying to think where we go from
there. I really didn't think clipping
there. I really didn't think clipping
was going to make a huge difference in
was going to make a huge difference in
like one update epoch training.
Hang on. What if it's an optimizer
Hang on. What if it's an optimizer
issue? Let's Let's roll one other thing
out. Let me just rule one other thing
out. Let me just rule one other thing
out here.
I want to make sure it's not
um like an optimizer
um like an optimizer
issue. So essentially if we still clip
issue. So essentially if we still clip
but we give it like a really big bounce.
Oh.
Huh.
What? So this is literally like some
What? So this is literally like some
dumb optimizer thing, right?
You don't see a spike
anywhere, but
like So we have This thing,
like So we have This thing,
right? Where was
right? Where was
it? Now, where was the run that totally
it? Now, where was the run that totally
exploded? Yeah, this one.
Right. We should actually see this in
Right. We should actually see this in
the
losses. Yeah. So, this thing just blows
losses. Yeah. So, this thing just blows
up instantly.
You do this.
Get rid of this
thing. I mean, it's still like
fine with 1.0
clipping. So
I don't know about you, but that doesn't
I don't know about you, but that doesn't
strike me
strike me
as flipping to keep the thing on policy
as flipping to keep the thing on policy
or whatever. That strikes me as like
or whatever. That strikes me as like
flipping to prevent uh bad update from
flipping to prevent uh bad update from
destroying the policy, right?
Let's try 02 real quick because this was
Let's try 02 real quick because this was
the value used in the paper. Make sure
the value used in the paper. Make sure
there's not something magic going on.
It's crazy that you clip the loss and
It's crazy that you clip the loss and
you clip the gradient.
you clip the gradient.
I figured that clipping the gradient
I figured that clipping the gradient
would be enough to prevent screwy
would be enough to prevent screwy
optimizer stuff from
happening. But I guess
happening. But I guess
not cuz if you get like a bad advantage,
not cuz if you get like a bad advantage,
right? If you get a bad
right? If you get a bad
advantage, you could take like a 0.5
advantage, you could take like a 0.5
gradient step in the completely wrong
gradient step in the completely wrong
direction. How's this
do? Yeah. So, this actually it does a
do? Yeah. So, this actually it does a
little better than 0.1 but it
little better than 0.1 but it
underperforms
0.5. Let me see what these clip
0.5. Let me see what these clip
coefficients actually do.
So these clip coefficients get
applied. Uh yes this like ratio
flipping. Okay. This is directly
on
props. Yeah, this is logic space
clipping. So it doesn't let the old log
clipping. So it doesn't let the old log
props get too far away from the new
props get too far away from the new
ones. Okay.
ones. Okay.
Then the
Then the
gradient there's also the gradient
gradient there's also the gradient
clipping. And then the value function
clipping. And then the value function
clipping
is this one's weird,
is this one's weird,
right? Yeah. This one is just like
right? Yeah. This one is just like
directly in
um you're literally just plus minus a
um you're literally just plus minus a
constant on the loss. That's
crazy. Oh no, it's clipping the value
crazy. Oh no, it's clipping the value
loss to plus
loss to plus
minus this constant.
Yeah.
Okay. Does this thing get
Okay. Does this thing get
normmed? This doesn't get normed at
all. Probably should be normed.
So, where do we go from here?
Um, the fact that this has like such a
Um, the fact that this has like such a
wide
window. Kale's kind of bad at the same
window. Kale's kind of bad at the same
time, isn't
time, isn't
it? Kale's like kind of bad.
it? Kale's like kind of bad.
Let's go back to the old replay
buffer. Set this to
one. Set this to one and see what
one. Set this to one and see what
happens.
So this is now potentially like
So this is now potentially like
massively off policy.
value loss is super
Bye.
Bye.
Okay, let me try one thing.
So now we define this mass. us.
I mess up.
Try
that. Seriously, it's still now.
So now this is not training the value
So now this is not training the value
loss on stale data but still training
loss on stale data but still training
the policy.
Okay. Looks like we still get
stuck. Oh, we still get stuck.
Value still blows
up. Maybe marginally
better. Okay.
better. Okay.
So, let's not do
that. Let's look at B
trades. Okay. So, this this toilet paper
trades. Okay. So, this this toilet paper
is also what introduces
is also what introduces
Brays. They have an ablation within
Brays. They have an ablation within
without
BRAS. Why I read this
thing? Yeah, there's a big difference
thing? Yeah, there's a big difference
here. Big difference.
Uh, and you see with the replays, but
Uh, and you see with the replays, but
this is what gives them access to this
this is what gives them access to this
replay buffer. So, this is kind of
replay buffer. So, this is kind of
exactly what we
want. The Vray
want. The Vray
uh targets are defined by
one. Okay. over
here. All policy learning is important
here. All policy learning is important
in the decoupled distributed actor
in the decoupled distributed actor
learner architecture because the lag
learner architecture because the lag
between actions are generated by the
between actions are generated by the
actors and when the learner estimates
actors and when the learner estimates
the gradient. Yes.
Consider a
trajectory X
trajectory X
a reward from S. So this is perfect.
a reward from S. So this is perfect.
This is exactly what I have.
We define nstep v trace target for the
We define nstep v trace target for the
value
function sum over the trajectory
function sum over the trajectory
segment
segment
gamma some crazy
gamma some crazy
product the
TV. Okay, this is I think the thing that
TV. Okay, this is I think the thing that
is worth investing into right now.
is worth investing into right now.
This is a thing that's worth investing
This is a thing that's worth investing
into.
We also should open up. Let me switch
We also should open up. Let me switch
the
the
view cuz we need
view cuz we need
to This is going to get
heavy. Okay, so here's our current
heavy. Okay, so here's our current
target from generalized advantage
target from generalized advantage
estimation.
some
some
gamma
gamma
delta. They have this product and then
delta. They have this product and then
they have delta and then they have
V. Delta TV is
PT reward
plus. Okay. So this
here is this not
this Xt + 1 - XT
this Xt + 1 - XT
But this is only a one step,
But this is only a one step,
right? This is literally only a one
step. PT is
min. This is
min. This is
average.
Okay. Truncated important sampling
weights. In the on policy
weights. In the on policy
case when pi equals m
And then you get the on policy endstep
And then you get the on policy endstep
element which is
right
right
here which is lambda equal
here which is lambda equal
one. So this thing
one. So this thing
recovers that
on policy endstep bel
update. I also want to use the same
update. I also want to use the same
algorithm for off and on policy data.
truncated important sampling weights CI
truncated important sampling weights CI
and
and
PT. Well, this is what is this row or
PT. Well, this is what is this row or
something. I actually don't
something. I actually don't
remember.
remember.
Hey, I know that pace weird.
Whatever the
Okay, we don't care about the tabular
Okay, we don't care about the tabular
case.
V trace targets can be computed
V trace targets can be computed
recursively.
Did this one come with source
Did this one come with source
code? Yes, this one
did. Though it's probably in TensorFlow.
Oh, they gave
us
us
this. Jeez. Okay, this is actually it's
this. Jeez. Okay, this is actually it's
row. That's what it is. It was
row. That's what it is. It was
row.
Um, they have this and they have a test
Um, they have this and they have a test
for it because this is a very important
for it because this is a very important
thing to get right.
Let me just uh briefly look up if
Let me just uh briefly look up if
there's been any successor to be trace.
basic
policy. People were talking about this,
right? Oh, yeah. This is Showman's
right? Oh, yeah. This is Showman's
follow up to his own stuff.
Oh yeah, this has the weird thing where
Oh yeah, this has the weird thing where
um
So this thing did better for
Choosing the optimal level of sample
Choosing the optimal level of sample
reuse is not straightforward.
value policy and value function trainer
value policy and value function trainer
decoupled. We can train with different
decoupled. We can train with different
levels of sample
levels of sample
reuse. In order to better understand the
reuse. In order to better understand the
impact, we chose to vary the number of
impact, we chose to vary the number of
policy epochs without changing the
policy epochs without changing the
number of value function epochs. That's
number of value function epochs. That's
actually very similar to what I just
actually very similar to what I just
tried to
tried to
do. Get out of here, bot.
do. Get out of here, bot.
I build the bots around
I build the bots around
here.
here.
Um, single policy epoch is almost always
Um, single policy epoch is almost always
optimal or near optimal in
optimal or near optimal in
ppg. This suggests that the PO
ppg. This suggests that the PO
baseline benefits from greater sample
baseline benefits from greater sample
views only because the experts offer
views only because the experts offer
additional value function training.
But isn't the value function data what
But isn't the value function data what
gets off
policy? I mean it is also the supervised
policy? I mean it is also the supervised
one that you can train perfectly, right?
So that doesn't suggest that uh you can
So that doesn't suggest that uh you can
use that you can reuse samples, right?
Oh,
Oh,
okay. These are literally
okay. These are literally
inverted. Okay.
Man, I love when I just have, you know,
Man, I love when I just have, you know,
a whole day to just get as deep into the
a whole day to just get as deep into the
literature as I need to to try to
literature as I need to to try to
understand stuff. This is good.
Too many
Too many
epochs runs the risk of overfitting to
epochs runs the risk of overfitting to
recent
recent
data. Not with a giant batch. Fewer
data. Not with a giant batch. Fewer
epochs will lead to slower training.
epochs will lead to slower training.
1 to
n training with auxiliary epox is
n training with auxiliary epox is
generally
generally
beneficial up to
six. We may expect better train features
six. We may expect better train features
to be shared with the policy or train a
to be shared with the policy or train a
more accurate value function.
The feature sharing appears to play the
The feature sharing appears to play the
more critical
more critical
role. Auxiliary phase
frequency. We next we investigate
frequency. We next we investigate
alternating between policy and auxiliary
alternating between policy and auxiliary
phases at different frequencies.
phases at different frequencies.
We perform each auxiliary phase after
We perform each auxiliary phase after
end policy
updates. Performance suffers when we
updates. Performance suffers when we
perform auxiliary phases too
frequently. As an alternative to
frequently. As an alternative to
clipping that's
clipping that's
me this guy's own paper.
me this guy's own paper.
proposed using an adaptively weighted KL
proposed using an adaptively weighted KL
penalty. We now investigate the use of
penalty. We now investigate the use of
the K penalty in BPG. Instead, choose to
the K penalty in BPG. Instead, choose to
keep the relative weight of this penalty
fixed.
Okay. Fixed KL performs remarkably
Okay. Fixed KL performs remarkably
similarly to clipping when using PPG.
similarly to clipping when using PPG.
Clipping is more important when using
Clipping is more important when using
when rewards are poorly skilled. Great.
when rewards are poorly skilled. Great.
I don't have to do a bunch of math.
Lovely. They want to keep the tail
Lovely. They want to keep the tail
penalty for the theoretical insights
penalty for the theoretical insights
even though it's a pain in the ass. It's
even though it's a pain in the ass. It's
funny.
default implementation of
default implementation of
PPG a single network
PPG a single network
variant that mimics the same training
variant that mimics the same training
dynamics PO for
dynamics PO for
reference PPO PPG PO single they're very
similar ppg comes with an increased
similar ppg comes with an increased
memory footprint since we use disjoint
memory footprint since we use disjoint
policy and value function networks
policy and value function networks
instead of a single unified network we
instead of a single unified network we
can use we use approximately twice as
can use we use approximately twice as
many parameters
many parameters
We can recover this f by using a single
We can recover this f by using a single
network that appropriately detaches the
network that appropriately detaches the
value function gradient.
What during the policy
phase we detach the value function
phase we detach the value function
gradient at the last layer
gradient at the last layer
shared uh preventing the value function
shared uh preventing the value function
from influencing shared parameters.
from influencing shared parameters.
In the auxiliary
In the auxiliary
phase, the value function gradient with
phase, the value function gradient with
respect to all
parameters.
parameters.
Okay. I I think they did some whatever
Okay. I I think they did some whatever
that's
fine. Work
fine. Work
conclusion. Very good.
Do they have algorithm pseudo code
Do they have algorithm pseudo code
somewhere? Yes, they
somewhere? Yes, they
do. Initialize empty
do. Initialize empty
buffer. Perform
rollouts. What's
rollouts. What's
var value function
var value function
target? I missed
this and var computed with generalized
this and var computed with generalized
advantage estimation.
same objective PO. So this is just PO
right. So perform allows compute value
right. So perform allows compute value
function
function
optimize the policy for some
optimize the policy for some
box. Optimize the value for some
epox, right?
Wait. And then there's L joint and L
Wait. And then there's L joint and L
value. Okay. I don't think I
value. Okay. I don't think I
uh I fully understood what the heck
uh I fully understood what the heck
they're doing here then.
We optimize the policy network with a
We optimize the policy network with a
joint
joint
objective that includes an arbitrary
objective that includes an arbitrary
auxiliary
loss and a behavioral cloning
loss. What?
Lox. Hold
Lox. Hold
on. You don't need this.
Wait, let me try the
Wait, let me try the
single. What's the single
one? By default, PPG comes with an
one? By default, PPG comes with an
increased memory
increased memory
footprint. Since we use disjoint policy
footprint. Since we use disjoint policy
and value function networks instead of a
and value function networks instead of a
single unified policy, we can use
single unified policy, we can use
approximately twice as many parameters
approximately twice as many parameters
compared to
compared to
PO. Recover this cost by using a single
PO. Recover this cost by using a single
network that appropriately detaches the
network that appropriately detaches the
value function gradient. During the
value function gradient. During the
policy phase, we detach the value
policy phase, we detach the value
function gradient at the last layer
function gradient at the last layer
share between the policy and the value
heads. Preserving the value function
heads. Preserving the value function
gradient from influencing shared
gradient from influencing shared
parameters.
During the auxiliary
phase, we take the value function
phase, we take the value function
gradient with respect to all
parameters.
parameters.
Huh. Okay. So, that's not
Huh. Okay. So, that's not
quite. But then what does the auxiliary
quite. But then what does the auxiliary
phase
do? Hang
do? Hang
on. So they just do Where's the pseudo
code? Optimize the clipped policy loss.
Optimize the value
loss.
Optimize L
Optimize L
joint. Optimize L
value. Okay. I don't understand this
value. Okay. I don't understand this
thing.
perform
perform
rollouts under current
policy. Clean RL has this, doesn't it?
Clean
our thought they had ppg in here, didn't
our thought they had ppg in here, didn't
they?
they?
PPG pro
jump policy
jump policy
phase. So they collect
phase. So they collect
rollouts, right?
Optimize policy and value
Optimize policy and value
network. There's the policy network
one. Uh, and they do these at the same
one. Uh, and they do these at the same
time because they're separate networks.
Okay. But then there's an auxiliary
Okay. But then there's an auxiliary
phase policy.
need to understand this and need to
need to understand this and need to
understand B trace today. Those are the
understand B trace today. Those are the
two
goals. What's this B as what's this
goals. What's this B as what's this
extra thing?
That's just
That's just
entropy. So in the first
entropy. So in the first
phase, this is just do PO,
phase, this is just do PO,
right? Yeah. This is literally just do
PO with a stop Brad potentially if you
PO with a stop Brad potentially if you
use a shared
network. Okay, so that's super
network. Okay, so that's super
easy and this is not even slower.
easy and this is not even slower.
This is not even slower than PO
This is not even slower than PO
providing you use the shared
providing you use the shared
formula. But then there's this E
O where you
O where you
optimize L
joint and L value.
So what is
So what is
Ljint? We optimize the policy that has
Ljint? We optimize the policy that has
an auxiliary loss and a behavioral
an auxiliary loss and a behavioral
cloning
cloning
loss. So this is the value function
loss. So this is the value function
loss.
Okay. So in the shared version, the
Okay. So in the shared version, the
shared policy version, this is
shared policy version, this is
uh this is literally
uh this is literally
just train the value
just train the value
loss. And then this thing
here, behavioral cloning loss.
KL. This isn't just a KL
term policy
old. Oh. Oh, okay. I understand this.
old. Oh. Oh, okay. I understand this.
So, they're trying to keep this thing
So, they're trying to keep this thing
outputting the exact same actions
outputting the exact same actions
um
um
while training it to match the value
target. I
see that's
see that's
clever. Let me think why that makes
clever. Let me think why that makes
sense though to do it this way.
Let me make sure I got
Let me make sure I got
the this this is
the this this is
V theta
V theta
V. Yeah, this is theta V.
Do you even have to do this? I wonder
Do you even have to do this? I wonder
with the shared formula.
Like this is arguably not even that hard
Like this is arguably not even that hard
to do, right?
This seems orthogonal to be traced
though. Now that I understand this, let
though. Now that I understand this, let
me go back through the experiments
me go back through the experiments
again.
Where's figure
three? We chose to vary the number of
three? We chose to vary the number of
policy epochs without changing the
policy epochs without changing the
number of value function
number of value function
epochs. Results are shown in figure
epochs. Results are shown in figure
three.
Holy, that's
wild. So that's literally just train the
wild. So that's literally just train the
policy
less. Okay.
and then they get literally the opposite
and then they get literally the opposite
with the value function, right?
lagging value function.
Then here is the auxiliary phase
Then here is the auxiliary phase
frequency.
additional epochs during the auxiliary
phase. So, you don't actually have to do
phase. So, you don't actually have to do
this very
often. Wait, now wait, we have
often. Wait, now wait, we have
additional epochs during the auxiliary
additional epochs during the auxiliary
phase.
Oh no, this is the other way around.
Okay, this is the frequency.
E
ox. Hang on. This is e
ox. Hang on. This is e
pi. E
ox. So, they didn't do
ox. So, they didn't do
this. I thought they did do this
this. I thought they did do this
one. So, this one is
one. So, this one is
EI. No, this is not EB. This is Es
and then the
frequency. So, this is not that bad then
frequency. So, this is not that bad then
because you do have to run some extra
because you do have to run some extra
epoch, but you don't do it that often.
This thing
is probably pretty awful though
is probably pretty awful though
for M
for M
speed. You can't really do anything on
speed. You can't really do anything on
the end while you're optimizing that.
the end while you're optimizing that.
That's okay. Okay. So, I understand this
That's okay. Okay. So, I understand this
thing. I think this is going to have no
thing. I think this is going to have no
bearing whatsoever on uh V
trace. So, this is something to
trace. So, this is something to
investigate for
sure. This seems like a
um pretty strict generalization of PO.
Okay, so that's one. Now we go to B
trace, not this one where it said B
trace. And now we try to understand this
trace. And now we try to understand this
V trade Target.
value at
state. This product
S to tus
S to tus
one sum i=
one sum i=
s to t yeah so this is fine this is not
s to t yeah so this is fine this is not
this is actually easy to
this is actually easy to
compute then
ci i over
ci i over
mu okay and then this delta
is just TD.
They have a Q function in here.
see it in here.
21 billion frames per day, 250k
21 billion frames per day, 250k
fps
on 500
CPUs. Oh, and then that's you actually
CPUs. Oh, and then that's you actually
have to divide by four.
Yeah. So, this is actually 60K. Yeah,
Yeah. So, this is actually 60K. Yeah,
that's about what you can do with a
GPU. That's
GPU. That's
funny. That's actually pretty
funny. That's actually pretty
disappointing
disappointing
for looking at what Puffer does these
days because I can give you half of that
days because I can give you half of that
just like out of the box.
uh with like what 16
CPUs.
CPUs.
So okay in order to implement this thing
Let me go check my current
implementation. So that's the current uh
implementation. So that's the current uh
No, it's in
No, it's in
shared. Okay. So J is just
shared. Okay. So J is just
reward plus gamma value minus it's this
reward plus gamma value minus it's this
right this okay this is all that this
is delta plus
I'm covering the screen. Hang on. We'll
I'm covering the screen. Hang on. We'll
do this for
now. If you get rid of this pair,
Then you can literally just do this,
Then you can literally just do this,
right? Yeah, it's just the sum of
right? Yeah, it's just the sum of
these. And then you don't even have the
these. And then you don't even have the
lambda
lambda
term, right?
So really I just need this row term and
So really I just need this row term and
this C
this C
term. So let me see if I can understand
term. So let me see if I can understand
these.
these.
Row and C are truncated important
Row and C are truncated important
sampling
sampling
weights. Where do they
define? So MOU is the original policy.
define? So MOU is the original policy.
So this is pile
What is this?
Min P bar.
Right out the
B P bar as a truncation
B P bar as a truncation
threshold. Oh, is that a constant?
That might just be a constant,
right? That might literally just be a
right? That might literally just be a
constant.
If that's just a constant, this isn't
If that's just a constant, this isn't
that hard to implement.
Where do they get a par bar
though? Are these just parameters that
though? Are these just parameters that
are going to be listed somewhere for us?
Hypers. Any hypers?
Are those not just fixed diapers? Cuz I
Are those not just fixed diapers? Cuz I
don't see them
don't see them
anywhere. Just count.
You
Oh,
one. Okay. So, if that's correct, those
one. Okay. So, if that's correct, those
are literally just one.
Okay. Now, let's go back one more time
Okay. Now, let's go back one more time
and see if I understand why this is the
and see if I understand why this is the
way it is. So, this
is there's no lambda in
is there's no lambda in
here. Let's ignore lambda. So,
discounted. So, we're just doing
discounted. So, we're just doing
Bellman. Okay. No exponential smoothing
Bellman. Okay. No exponential smoothing
with
lambda value plus. So if we ignore this
lambda value plus. So if we ignore this
piece here and this clipping then this
piece here and this clipping then this
gives us Bellman which is this right
gives us Bellman which is this right
here and
here and
Bellman. So what we're saying
here let's do this one first. This one's
here let's do this one first. This one's
easier.
Ah, so we are
clipping we're clipping the
advantage or rather we're scaling the
advantage or rather we're scaling the
advantage based on important
advantage based on important
sampling instead of scaling
sampling instead of scaling
the policy gradient loss directly.
Oh, that also scales the value function
Oh, that also scales the value function
target, doesn't it?
Important sampling is embedded in the B
Important sampling is embedded in the B
trace
target. Corrected value estimate
target. Corrected value estimate
indirectly influences advantage
computation by correcting value function
computation by correcting value function
B trace advantage accounts policy
B trace advantage accounts policy
mismatch is not directly applied to the
mismatch is not directly applied to the
advantage itself. What's the value
advantage itself. What's the value
function that is
adjusted? Uh, what this is? Is this the
adjusted? Uh, what this is? Is this the
advantage function they
advantage function they
use? That seems weird. I got to check
that advantage and value
loss. Clipped objectives with policy
loss. Clipped objectives with policy
loss.
loss.
typically unclipped or L2 loss value
typically unclipped or L2 loss value
function.
function.
Now there is important sampling right.
Now there is important sampling right.
The clip is on
The clip is on
um a logic
ratio on truncated important weights.
Okay. So, you might not need a lambda
print cuz lambda pushes you
towards towards one step, right? If you
towards towards one step, right? If you
add Yeah, because lambda is one default.
add Yeah, because lambda is one default.
So yeah, this just pushes you away.
So yeah, this just pushes you away.
Lambda just pushes you towards
Lambda just pushes you towards
this which is really easy to optimize
this which is really easy to optimize
but this is like a onestep
but this is like a onestep
bootstrap. So this is hard to optimize
bootstrap. So this is hard to optimize
but is correct, right? More
but is correct, right? More
correct. This is the end step or the
correct. This is the end step or the
infinity step. So you might not even
infinity step. So you might not even
need a lambda term.
Okay. It is not It is not more common to
Okay. It is not It is not more common to
see unclipped value loss in PO. That is
see unclipped value loss in PO. That is
not
true. The value function.
true. The value function.
Now, let me just do what they use for
Now, let me just do what they use for
the advantage function here
the advantage function here
because I'm a little suspicious
because I'm a little suspicious
now. So, here's your V trace
now. So, here's your V trace
target, which is what you train the
target, which is what you train the
value function against. And where's your
value function against. And where's your
advantage function?
Just make sure I understand this as
Just make sure I understand this as
well.
Okay, hang on
here. So, this is just the value
here. So, this is just the value
target. Did they give us the advantage?
gradient of the value
function. Whatever
Oh,
what? That's all they
do. Hang on. This is the V trace target,
do. Hang on. This is the V trace target,
right?
Oh, wait. No, this is the same. Isn't
Oh, wait. No, this is the same. Isn't
this the
this the
same?
same?
Because
advantage you
advantage you
add BST to this, right?
You do V st plus
You do V st plus
this which if it's equivalent
this which if it's equivalent
here R + lambda V S T + 1
That's the onestep version,
right? No, it's not because this
right? No, it's not because this
[Music]
[Music]
is reward
is reward
plus
advantage. Yeah. Okay, that's still
advantage. Yeah. Okay, that's still
fine. That's the
fine. That's the
same. So, this is actually a very sort
same. So, this is actually a very sort
of
of
similar
method. What did they benchmark this
method. What did they benchmark this
against?
What is there? No
correction. Is no correction generalized
correction. Is no correction generalized
advantage estimation or no?
No off policy correction. Multiply
No off policy correction. Multiply
advantages by corresponding importance
weight. That gives you very similar
weight. That gives you very similar
performance.
Without replay, it gives you very, very
Without replay, it gives you very, very
similar performance. With replay,
similar performance. With replay,
there's a bit more of a
gap. There's no benchmark of betray
gap. There's no benchmark of betray
generalized advantage estimation.
Probably worth implementing this thing
Probably worth implementing this thing
still.
I say we start on this.
This thing needs values.
This thing needs values.
Rewards
done. This trace
And then this also
needs
importance. And then we need a float.
row load
C. And this is going to be fairly
C. And this is going to be fairly
similar to
J. Let me see this double loop real
J. Let me see this double loop real
quick.
This is t goes from
s to t minus
one. So this product is pretty easy to
one. So this product is pretty easy to
compute, isn't
compute, isn't
it? It's just one term
it? It's just one term
per Okay.
So we do
float C
product. Does this one go backwards?
Wait, does this one go
back? This one can't be computed
back? This one can't be computed
backwards,
right? That's a little bit annoying.
Wait, this is a t and this is a
Wait, this is a t and this is a
i. But this is
i. But this is
ct. So this is the same thing.
Oh, you missed Horizon.
has to be one to
start broad.
This is
clip something like this I believe.
min
min
max
importance you do fraud times equals
that's or actually we can do it the
that's or actually we can do it the
other way
other way
right so we do
right so we do
prod times equals
this and then this
Now you have all your products.
Okay. So, you do
this. You only need this component,
this. You only need this component,
right?
You don't need this extra exponential
smoothing.
Wait, this is rewards of T
Wait, this is rewards of T
next. Is that
right? That seems weird.
Is that a bug in GE? In my
GE. I think that might be a bug in my J.
This index and the negative are the
This index and the negative are the
same. The
values Yeah, that looks like a bug to
values Yeah, that looks like a bug to
me.
We will play with that.
Okay. So cool. I think we can just start
Okay. So cool. I think we can just start
implementing this portion
now to be so careful with
this. We'll start with this delta.
this. We'll start with this delta.
So this delta is just equal to reward at
So this delta is just equal to reward at
I'm going to make this
I'm going to make this
t
values gamma value t
next minus values. Okay, so this delta
next minus values. Okay, so this delta
is fine.
This is just a
This is just a
backwards sum,
right? So I can just do like float
term equals z.
like him.
This is not a T. Next. This is a T,
This is not a T. Next. This is a T,
right? This is
right? This is
T. It's
delta. Okay.
Next non-
terminal that starts out at zero.
So we do accumul
plus gamma
time next non-
terminal times a
terminal times a
cube and then trace of t is equal to a
cube. I think that's it except I missed
cube. I think that's it except I missed
the
the
What's this? The road
What's this? The road
clip. Ah, shoot. You missed the road
clip. Ah, shoot. You missed the road
clip. You click twice like that.
This is not it's not uh this is
This is not it's not uh this is
important
here. Let's just do
row sub
T. Yeah. So this is it here. So this is
T. Yeah. So this is it here. So this is
the clipped row and then this is row
the clipped row and then this is row
clip
delta. And then we can do this one in
delta. And then we can do this one in
line I think.
line I think.
So this one should
be Yeah. So this is
This has to go
This has to go
here. And then this is actually
row
subt. I want to keep it the math as I
subt. I want to keep it the math as I
possibly
can. And then this is
Okay, let me think about this.
is there no value function bootstrap on
is there no value function bootstrap on
this
Oh, there is
Oh, there is
right. Yeah, there
is. So, it's going to be delta Q is zero
is. So, it's going to be delta Q is zero
at the first time. So, cro delta plus
at the first time. So, cro delta plus
gamma times
gamma times
that I think that's correct.
Let's see if I can get this thing to
compile. I'm gonna actually move this.
compile. I'm gonna actually move this.
I'm going to move
I'm going to move
this. I move this
down. Move this
down. I mean, this should basically be
down. I mean, this should basically be
autocompletable. It's so basic, right?
autocompletable. It's so basic, right?
Check on this board is done is
important
steps trace
steps trace
row clip. Yeah, there we
row clip. Yeah, there we
go. So then we just do
Rewards values done importance and trait
Rewards values done importance and trait
should all have the same
should all have the same
dimensions should be two dimensions on
dimensions should be two dimensions on
the same
the same
device in 432 will
not oh you don't pass in
trace my bad you don't pass in phrase
Right. Now we
Right. Now we
do
phrase this.
That's the shared
That's the shared
shared
shared
file. Now we need the
file. Now we need the
bindings. And this is actually, believe
bindings. And this is actually, believe
it or not, this is going to be the C++
it or not, this is going to be the C++
and the CUDA at the same
and the CUDA at the same
time. It' be very
time. It' be very
nice. We'll do the C++ bind first.
V
V
trace values rewards done
trace values rewards done
importance advantages clip clip thumbs
horizon plus horizon beach
horizon plus horizon beach
row importance plus
offset.
offset.
Okay plus offset row
Okay plus offset row
clip horizon. You missed gamma
clip horizon. You missed gamma
completely man. What the heck is wrong
completely man. What the heck is wrong
with you?
And you use it here too, don't
And you use it here too, don't
you? Gamma, what's wrong with
you? Gamma, what's wrong with
you?
Lo, this doesn't need to get checked
Lo, this doesn't need to get checked
though,
though,
right? Like the uh the check on this
right? Like the uh the check on this
doesn't
doesn't
include any of this
crap. You just need this, man.
Just one extra tensor and you're
good. Okay. Now, the actual function
good. Okay. Now, the actual function
here needs
here needs
everything
everything
trace
gamma. All right. And then you do B
gamma. All right. And then you do B
trace
gamma upper lip C.
Is this the right
Is this the right
one? Yeah, I think this is the right
one? Yeah, I think this is the right
one. So, we
need GA AE kernel I
believe. Okay. And then there's compute
believe. Okay. And then there's compute
GA.
GA.
Just do
this
this
trace kernel which takes values rewards
done
done
importance
importance
trace gamma
row clip.
row clip.
Yep. All
Yep. All
that and then it's just V trace with the
that and then it's just V trace with the
new RS.
And then that will give us our
And then that will give us our
kernel. We still have to bind this
kernel. We still have to bind this
thing. So let's actually put this down
here. I don't know why I did this this
here. I don't know why I did this this
way. This should
way. This should
go. What is this thing? Yeah, I don't
go. What is this thing? Yeah, I don't
know why I did it this way. This needs
know why I did it this way. This needs
to go down here.
You go
there. All right. Base is kernel
there. All right. Base is kernel
now just needs a
now just needs a
binding which passes
in everything
in everything
except perfect.
except perfect.
Uh, that might have just one shot it
Uh, that might have just one shot it
because this is super basic. Let's see.
because this is super basic. Let's see.
BR
check.
Yeah.
Yeah.
GMA looks good to
GMA looks good to
me. Just a bunch of boiler
plate. And then we just have to
plate. And then we just have to
export QV trays.
race.
Cool. BR is false.
These are both
one. We do
LF config use feed phrase. We beat
phrase. You miss
gamma. You do need gamma still.
Did uh we get that in the
signature? Yeah, we did get that in the
signature.
Okay. Well, you don't give it
Okay. Well, you don't give it
experience. importance,
right? Yeah. So, this is like
right? Yeah. So, this is like
um what you need to do this needs to be
um what you need to do this needs to be
important and then we have to do
How do we do this?
We need
We need
um old log props and new log props,
um old log props and new log props,
right?
Does this thing prevent you from doing a
Does this thing prevent you from doing a
prioritized
replay? Cuz wait, you need to have
This says that you need to have the
This says that you need to have the
importance
importance
ratio before you compute advantage,
right? How does prioritized experience
right? How does prioritized experience
replay work?
Maybe we can just use something else to
Maybe we can just use something else to
sample it with and then we don't need
sample it with and then we don't need
advantages.
Uh I don't have I don't have mental
Uh I don't have I don't have mental
bandwidth to read another full paper
bandwidth to read another full paper
here right yet. Hang
here right yet. Hang
on. Let me just uh think what we can do
on. Let me just uh think what we can do
in the meantime.
So, oh, I see what we would
So, oh, I see what we would
do. So, we would just do
Vantages equals
ports. Okay. So then we would sample our
back. So this is now
back. So this is now
uniform, right?
Then we
Then we
do if config use V
trace we need to
trace we need to
get we need to compute J for this
get we need to compute J for this
B. So this is going to be B
rewards from the
rewards from the
gamma. Okay. And then we do
gamma. Okay. And then we do
importance
equals is it this ratio right here?
ratio, right?
ratio, right?
That is the important
sample. Oh, I totally messed this up.
sample. Oh, I totally messed this up.
Hang
Hang
on. I just Yeah, I know how to do this
on. I just Yeah, I know how to do this
now.
Okay. So, we do
Okay. So, we do
this manages four
once we go down to
once we go down to
here. We say
All
right. And then this importance is just
ratio.
ratio.
Okay.
Advantage.
Advantage.
Advantage. Or what's
Advantage. Or what's
this? This is not advantage. This is V
this? This is not advantage. This is V
trace,
right?
Var. And what's the V trade paper make
Var. And what's the V trade paper make
me use for the act
me use for the act
policy? What they
policy? What they
use? So this gives you VS.
Okay. Value function is easy,
right? Yeah. Value function
right? Yeah. Value function
is easy. This is just batch returns,
is easy. This is just batch returns,
right?
batch. So that's an easy target. Now
batch. So that's an easy target. Now
what do you do for the only thing I
what do you do for the only thing I
don't know is the
policy. Policy it says you do R
Why is it dst + one? Is that a thing?
Let me just do something to start.
Oh yeah, this log ratio is
Oh yeah, this log ratio is
uh yeah going to be completely
uh yeah going to be completely
completely different,
completely different,
right? So it's going to be
right? So it's going to be
um PG
log new log crawl.
times back
dot gamma batch returns minus batch
dot gamma batch returns minus batch
values. I think this is
it. I think that is it.
it. I think that is it.
And then we have to
say
Mike next
step. All
right. Zero chance that this runs on the
right. Zero chance that this runs on the
first
try. Oh, I also unless I forget to
try. Oh, I also unless I forget to
enable it.
Ah, this is a bad
Ah, this is a bad
ting. There's a bug.
there. Let's see what that
there. Let's see what that
builds. All right. How many bugs we
have? See flip real quick.
Uh, you just straight up forgot to add
Uh, you just straight up forgot to add
this. Yeah, that was
stupid. Gamma lambda.
Not bad though. That will immediately
Not bad though. That will immediately
have a CUDA kernel for
have a CUDA kernel for
it. So if it works, it'll also
it. So if it works, it'll also
immediately be fast.
Can I just do con? Can I like
Can I just do con? Can I like
const this
[Music]
post redefined.
cannot be used.
I just have to get it so that the thing
I just have to get it so that the thing
will let me use this
will let me use this
um for a static array.
Okay. Expression must have a constant
Okay. Expression must have a constant
value.
Guess I can't do
Guess I can't do
that. That's very obnoxious.
Why did I need to do that
Why did I need to do that
again? I absolutely didn't need to do
again? I absolutely didn't need to do
that,
that,
right? No, I do need it
because Oh, that's super obnoxious.
Hang on. What's this define?
Trace
Trace
trace. Is that
it? Come on. It's const all the way
it? Come on. It's const all the way
down.
typing stupid CS 101 to
GR is
I mean,
I can't mal
it. That's a total pain in the ass.
So I I think I can do
something like Yes.
All right, that actually
compiles.
compiles.
No attribute compute vx. Yeah.
This is probably just signature,
right?
right?
Okay. One, two, three, four. Four time
Okay. One, two, three, four. Four time
serves. Three, four, four
serves. Three, four, four
times
times
gamma v trace
gamma v trace
clip. So float
clip. So float
float float in two
float float in two
ins. Uh should not be two
ins trays. You do not need to
ins trays. You do not need to
pass num steps for horizon
pass num steps for horizon
here. These are
computed
computed
steps. That's what you get for
steps. That's what you get for
uh
autofilling. There you go.
use the restroom. I'll be right back.
[Music]
Okay. What did I pass
Okay. What did I pass
wrong? 1D important
trade. Yep.
trade. Yep.
Yep, that's it.
Red cannot
be
graded. Okay. Interesting.
Oh,
dummy. And this is a mismatch as
dummy. And this is a mismatch as
well. Yeah.
shape. Do some shape
shape. Do some shape
fixing. I just want to get this to
run. Oh joy.
CPU fall back
maybe computer v.
Compute V trace was not
Compute V trace was not
declared but yeah I should declare it.
declared but yeah I should declare it.
All
All
right. Done.
right. Done.
Importance.
Importance.
Uh,
Uh,
gamma
gamma
eclipse race
eclipse race
sets done importance
trace
steps. So much binding code, man.
So much stupid binding
code. I mean, it runs.
We've got
We've got
um I'm assuming I'm not timing this
um I'm assuming I'm not timing this
thing correctly.
That's really freaking slow for some
That's really freaking slow for some
reason,
but doesn't train either.
The CUDA one is somehow out of memory.
The CUDA one is somehow out of memory.
Cuda one somehow legal access.
Really? It's the same bloody code. So,
Really? It's the same bloody code. So,
how do you figure
that work the first time?
So eventually this thing uh hurts
somehow and if I don't do this I
somehow and if I don't do this I
still error
No. Then we
train against who knows what we're
train against who knows what we're
training
training
against. So it is this
against. So it is this
thing values rewards
thing values rewards
done
ratio. I need to do this.
No, doesn't
help. Peace.
anything out of bounds. Looking
We got Max right
Yeah, I don't see anything that would be
Yeah, I don't see anything that would be
wrong
there.
Um, retrace
Um, retrace
kernel. Oh, well, this is really stupid,
kernel. Oh, well, this is really stupid,
right? Or no, this is fine actually.
right? Or no, this is fine actually.
J
J
kernel times
kernel times
horizon values
offer camera
and does this use steps.
Use no
steps. Why is not longer than 256,
steps. Why is not longer than 256,
right?
I'm pretty sure it's not
Okay, not
that device stuff.
I mean, we should probably just look at
I mean, we should probably just look at
this very
carefully. Gamma clip. Right. So, we're
carefully. Gamma clip. Right. So, we're
passing these all in the correct order,
passing these all in the correct order,
I'd assume, right?
Num steps
horizon. So then horizon goes to beach
horizon. So then horizon goes to beach
trace
trace
kernel and it goes to beach row
gamma beach
gamma beach
row horizon.
Oh, what the heck is
Oh, what the heck is
this? How' that
this? How' that
happen? Hey, I don't know how that
happened. See if that does it.
No, still found legal
No, still found legal
memory.
Really? Let's see.
Boston horizon.
not recompiling it or
not recompiling it or
something. Should be
Let me clear the cach to be
sure. That's
sure. That's
bizarre. It runs on the CPU. It doesn't
bizarre. It runs on the CPU. It doesn't
run on the GPU.
Why do I think it's going to end up
Why do I think it's going to end up
being something stupid that's like not
being something stupid that's like not
even
even
here? Because I like I don't see
here? Because I like I don't see
anything here,
anything here,
right? You have this buffer. This buffer
right? You have this buffer. This buffer
is always big
is always big
enough. Starts out at one. You go from
enough. Starts out at one. You go from
zero up to
zero up to
horizon which definitely fits in
horizon which definitely fits in
there. Importance is also the same size.
there. Importance is also the same size.
This the search
This the search
checked,
right? This goes to -2.
So this is one which is
fine
t and rewards at
t and rewards at
t values t
next
next
t horizon minus two to
t horizon minus two to
zero still fine.
Race of T. Yeah, I don't see anything
Race of T. Yeah, I don't see anything
wrong with this
wrong with this
thing gives you numbum steps by horizon.
thing gives you numbum steps by horizon.
You've assert
You've assert
checked all of
checked all of
these,
right? Does this give us anything?
That's not going to recompile.
blind debugging is just such a stupid
thing. Yeah, it's just going to give me
thing. Yeah, it's just going to give me
illegal memory address.
There's nothing useful in
There's nothing useful in
here. It's all asynchronous. So, you're
here. It's all asynchronous. So, you're
just screwed.
Does the synchronize even happen?
Let's just make sure there's not
Let's just make sure there's not
something like
ridiculous. We still get a legal memory.
See, I don't understand how it actually
See, I don't understand how it actually
doesn't
happen. Like, we don't actually even hit
happen. Like, we don't actually even hit
it here,
it here,
right? We hit it in one of the freaking
right? We hit it in one of the freaking
profilers. You would think we should hit
profilers. You would think we should hit
it here,
right? Like, how am I not hitting it
right? Like, how am I not hitting it
there?
Are we hitting it in the sample or
something? Like where are we heading
something? Like where are we heading
this?
Samples with
advantages. So, this doesn't get called
advantages. So, this doesn't get called
at all.
at all.
Uh, I get I guess it could be this
Uh, I get I guess it could be this
sample up here, but that would
make that wouldn't make any sense at all
make that wouldn't make any sense at all
to
me. It's still just in a rapper
me. It's still just in a rapper
somewhere.
Okay. What if I do
this? No.
this? No.
So literally somehow because this is not
So literally somehow because this is not
even
even
doing anything
right. I mean I could just
right. I mean I could just
like for the hell of it
clone, right? We could do all
this. No. So, it's literally
like freaking cuda, man.
Dun is float,
Dun is float,
right? I'm sure the Dun is float. I will
right? I'm sure the Dun is float. I will
double
check. We literally pass it. Yeah.
check. We literally pass it. Yeah.
experience.
experience.
Dun just make sure
Yep.
I don't think this is in place,
right? Could be. That could technically
right? Could be. That could technically
be
be
something. Hang
on. But I I don't see how it would be.
on. But I I don't see how it would be.
That doesn't make any sense.
US.
Pretty sure it's not
that. No, it's not that.
How big was the uh the
array? Oh, you idiot. Okay, I know it.
array? Oh, you idiot. Okay, I know it.
I know
I know
it. Damn
it. All right, that's me being dumb. I
it. All right, that's me being dumb. I
should have added a check for that.
Search. Uh, what is it?
Num. Yeah, there you go.
But
this I forgot that uh because I'm
this I forgot that uh because I'm
applying this to sample
applying this to sample
two mini batches
two mini batches
essentially. You can't have as many
threads. So, it was trying to split it.
threads. So, it was trying to split it.
Uh, it didn't have enough data for a
Uh, it didn't have enough data for a
full
block. Yep, there we
block. Yep, there we
go. So
go. So
stupid. All right. Now, we still have
stupid. All right. Now, we still have
the problem of it not
the problem of it not
training, but uh at least now we can
training, but uh at least now we can
debug working
debug working
code. I'm way happier debugging working
code. I'm way happier debugging working
code. We can see the grad variance has
code. We can see the grad variance has
just exploded
catastrophically. That was
catastrophically. That was
obnoxious. That was a silly mistake as
obnoxious. That was a silly mistake as
well.
You would think you would get some kind
You would think you would get some kind
of error for that, but nope, you
of error for that, but nope, you
don't. You just don't. You get out of
don't. You just don't. You get out of
memory, like memory access, whatever.
Okay.
So, the uh the value loss seems pretty
So, the uh the value loss seems pretty
damn easy,
right? Like this seems
right? Like this seems
pretty
basic. Now, this
Potentially this has important sampling
Potentially this has important sampling
on it. I missed this. What's this
row? Important sample,
row? Important sample,
right? So that row is just
ratio. What's the objective?
ratio. What's the objective?
Why don't they write the
Why don't they write the
uh the objective here?
Just want the freaking beach race
Just want the freaking beach race
loss. Where is it?
intensive flow code, man.
Something
here still in
here still in
progress. Literally top is some random
progress. Literally top is some random
repo.
is the
Pro log.
This one's
easy. What's this song?
The hell is this
The hell is this
sum? Where did it get this f this
sum? Where did it get this f this
formula
from? Yeah, that literally makes no
from? Yeah, that literally makes no
sense.
Okay.
Okay.
Um, logic's actions
advantages. So this is
Wait,
Wait,
what? Cross
what? Cross
entropy times advantages.
I like how it's trying to tell me that
I like how it's trying to tell me that
the original in the original
the original in the original
implementation it's this way. When I
implementation it's this way. When I
pasted the original
implementation, I literally pasted that.
Um, assuming that's not like some other
Um, assuming that's not like some other
baseline, right? Doesn't look like it
baseline, right? Doesn't look like it
would be.
B trace returns.pg advantages.
That's it.
According to the paper, that's what it
According to the paper, that's what it
is.
expected.
Is this not the same as
um Hang
on. Is this not just new log prop?
I think we need to
I think we need to
uh to go look at what their V trace
uh to go look at what their V trace
outputs.
Yeah, we need to go look at what their V
Yeah, we need to go look at what their V
trace
trace
outputs. This is not
working. So they literally just
do cross
do cross
entropy
entropy
advantages. And then this
is V trace returns. PG
advantages GG.
Okay. Oh, this is some like crazy scam
Okay. Oh, this is some like crazy scam
thing.
Cool. Says this is their from importance
weights discounts.
They premputee discounts I guess.
B S T +
one. Okay.
[Music]
Words. Turns of
Gamma batch
returns. Add your values.
Not this ratio.
Brad can create only implicitly from
Brad can create only implicitly from
blah blah
blah blah
blah. All right.
really. Okay, you know what? We're going
really. Okay, you know what? We're going
to just
do this is going to be
add whenever this thing logs
Okay. Well, this clearly still doesn't
Okay. Well, this clearly still doesn't
work.
lift PG
Hang
Hang
on. They have like two different things
on. They have like two different things
that this function is returning.
We'll know when we get it because they
We'll know when we get it because they
should at the very least be competitive
should at the very least be competitive
with uh what we have
with uh what we have
before, what we had
before. And it should basically not work
before. And it should basically not work
if it's
wrong. It's so weird how they like
wrong. It's so weird how they like
export
like they've got
VS and
then
concat. Yeah. So they basically they do
concat. Yeah. So they basically they do
one extra bootstrap value at the
end which is such a weird way of doing
end which is such a weird way of doing
it but
whatever but then they apply hang on
whatever but then they apply hang on
it's
it's
clipped PG
rows so I think you need to return two
rows so I think you need to return two
things from this function don't
you? Then you just need to return two
you? Then you just need to return two
things.
clipped PG
clipped PG
rows
rows
equals if clip threshold
rows where's
row the math
Okay, so here are your rows,
Okay, so here are your rows,
right? This is something that we
compute. Yep, this is something that we
compute. Yep, this is something that we
compute.
times. Well, this is just the
times. Well, this is just the
uh this is literally
uh this is literally
just the ratio, isn't it?
I think the easiest thing is just going
I think the easiest thing is just going
to be to put this entire thing into the
to be to put this entire thing into the
kernel.
Right? The thing is you do you miss one
Right? The thing is you do you miss one
at the end with the way that this is
at the end with the way that this is
written. You literally do miss one at
written. You literally do miss one at
the end.
See this clipped PG row is just
See this clipped PG row is just
literally just a
literally just a
clipped clipped row.
There's your
clipping. They do the clipping
clipping. They do the clipping
differently though, don't they?
Why is their clipping like zero to one
Why is their clipping like zero to one
or whatever?
You can't freaking use like I don't even
You can't freaking use like I don't even
know why I open these models when
know why I open these models when
they're wrong in the first
sentence. Like literally just close
sentence. Like literally just close
Twitter. Oh, the models are coming for
Twitter. Oh, the models are coming for
everything. The models
suck. They're all stupid.
This ratio is still what we want though,
This ratio is still what we want though,
isn't
it? Log ratio.
I'm Oh, you can just do max
Okay. So that should give you clipped
Okay. So that should give you clipped
row.
I think that's the
I think that's the
same. And
So it's this advantage gets multiplied
by rewards plus
discount. Discount
discount. Discount
times that returns
Next
Next
minus
values.
So, so I have this
So, so I have this
term, but then they take this against
term, but then they take this against
the soft max.
the soft max.
Maybe I should just do that.
Q with Z
Q with Z
indices which logit with indicates
I'm going to have to find like a good
I'm going to have to find like a good
reference
implementation
logits. Yeah, there's like no way to
logits. Yeah, there's like no way to
freaking do
freaking do
it. We do like pockets.
Effective target size.
I'm just trying to get something to like
I'm just trying to get something to like
match the reference. I think I'm going
match the reference. I think I'm going
to just have to find a reference
to just have to find a reference
implementation cuz the original sucks.
implementation cuz the original sucks.
It's like stupid dated TensorFlow code.
This is not going to magically start
This is not going to magically start
working
right for a second there
right for a second there
maybe, but
no. Okay. Is there an
no. Okay. Is there an
impala Impala pietor
torch mono beast? Uh they actually have
torch mono beast? Uh they actually have
this thing that they use for net hack,
this thing that they use for net hack,
right?
Is the whole trainer in C++ for this
Is the whole trainer in C++ for this
crazy
thing. Okay. So this is similar
Maybe we can do something with
this. Okay. So this is they have V trace
this. Okay. So this is they have V trace
returns and then they call
V trace. So vtrace from
V trace. So vtrace from
logits. Then they do
advantages
this and loss.
Okay. And
Okay. And
then their V trace here.
No. Is there V trace in
Python? No. No, it's not. It's okay.
Python? No. No, it's not. It's okay.
It's like weird torch code.
Fine. Yeah. Okay. Okay. So from logits
here. Okay. Hopefully we can figure it
here. Okay. Hopefully we can figure it
out from
this. What do they do it from? They do
this. What do they do it from? They do
it from logits or whatever.
from
from
logits. It's probably
easier. And yeah, there's also outputs
easier. And yeah, there's also outputs
two
two
things. Okay.
from logits just calls from importance
from logits just calls from importance
pool. So this
pool. So this
does log props and then
subtracts. So this gives you log
subtracts. So this gives you log
rows which literally right here. So this
rows which literally right here. So this
is log
rows. So you give this log rows and then
rows. So you give this log rows and then
it does X. Yeah. So this ratio is
it does X. Yeah. So this ratio is
literally just rows,
right? And then they clamp rows.
and the torch.n starting from this line
actually.
actually.
Okay. Clip the
Okay. Clip the
rows. Append bootstrapped value.
kind of wonky that they do that. I guess
kind of wonky that they do that. I guess
they were matching the reference
they were matching the reference
implementation, but like
fine,
fine,
deltas, flipped
deltas, flipped
rows, rewards plus
rows, rewards plus
discounts. Okay.
That
looks Wait, the
heck? Okay, so here's
their here's their result here, right?
B S minus
B S minus
V and they add
V and they add
values. They do they add the values here
values. They do they add the values here
to get
VS broadcasted bootstrap
values flipped PG rows. I thought we
values flipped PG rows. I thought we
already did
that. Oh, cool. They just had a separate
threshold. And
threshold. And
then here are your
advantages, which is flipped PG rows
advantages, which is flipped PG rows
times rewards.
Okay, I think we should be able to use
Okay, I think we should be able to use
this as a reference. This is like at
this as a reference. This is like at
least way easier to follow.
So we have our row subt.
Okay, so this is row
Okay, so this is row
subt P0
subt P0
uh
uh
times
rewards discount
rewards discount
times BST
next
next
minus hang on that's the
delta okay so this is delta
Right clipped rows. Okay. So we get this
Right clipped rows. Okay. So we get this
delta and then the
accum
accum
deltas of
t plus
t plus
discount times
Delta Crad times
deltas a
deltas a
Is that
Is that
it? I might have this wrong. I'm one
complicated. Don't need too much space
complicated. Don't need too much space
with this. We just need this little
window. Okay.
So they
So they
do this product times this delta,
right? What's
right? What's
actum? So yeah, this is my
accumulator
discounts. Did I do this wrong?
This might
[Music]
be I might have to move this delta right
be I might have to move this delta right
this product. This is C
product like this.
Yes. So this is the backwards iteration
Yes. So this is the backwards iteration
that I'm doing
here.
So deltas of
So deltas of
t which is right here. I just computed
t which is right here. I just computed
delta.
delta.
My delta is the same as
theirs. We might have screwed up the
theirs. We might have screwed up the
last value. But so far this other than
last value. But so far this other than
that this is
fine. Add V to get V
fine. Add V to get V
S. Add V to get V S. What? Add V of X to
S. Add V to get V S. What? Add V of X to
get VS.
Well, what do they use BS on?
baseline
loss. Yeah. So this
loss. Yeah. So this
is value loss.
Let's first modify this thing to do
Let's first modify this thing to do
um to have two
um to have two
inputs. So this should be
inputs. So this should be
VS and advantage.
So let's let's actually modify the
signature
bandages. So this is going to
bandages. So this is going to
be BS
be BS
float
float
bandages. Okay.
bandages. Okay.
And this
And this
[Music]
[Music]
is
important. Uh does C++ do multiple
important. Uh does C++ do multiple
returns?
Nope. In that case then we will just
um we'll move
um we'll move
this to this B trades
check. Oh, you can't return it from
check. Oh, you can't return it from
PyTorch then either, can
you? Yeah, that's super annoying.
I mean, we were probably going to just
I mean, we were probably going to just
eventually pass it in anyways,
right?
right?
[Music]
Bandages. Yes, bandages.
Okay, so we get BS
Okay, so we get BS
advantages and then we pass
advantages and then we pass
in yes advantages to the
kernel. All
right.
bandages ratio.
Let's modify
this. It's in
here. So, this is now
So we get this
So we get this
accum. So we just
accum. So we just
do s of
do s of
t
t
plus values of
t and then the PG
t and then the PG
advantages
advantages
equals. This does not use the
s+. So rotate
times rewards
times rewards
t plus
t plus
gamma. This is not values.
gamma. This is not values.
This is B S + one minus
values. BS at T + one.
Isn't that
Isn't that just you put this
Isn't that just you put this
here,
right? And I just put this
there. Okay, let's get this to compile
there. Okay, let's get this to compile
and then we'll with the uh the
and then we'll with the uh the
formula,
right? We're getting close though. We're
right? We're getting close though. We're
definitely getting close to
this. I hope it
this. I hope it
works. I mean, like, I hope the correct
works. I mean, like, I hope the correct
implementation's actually good.
Okay. V trace
check
importance thumbs
horizon. Yes.
Don't need this
anymore. Air building up.
Uh, and now this is going
Okay,
Okay,
cool. Uh, these chaos are crap. It's
cool. Uh, these chaos are crap. It's
funny this is not immediately
exploded. So, we should actually just be
exploded. So, we should actually just be
pretty chill right now,
pretty chill right now,
right? This should literally just be um
like something like this,
right?
Advantage and then was like bash
returns
maybe new value minus returns.
maybe new value minus returns.
Yeah, that's it.
Do we have to do this like weird shift
Do we have to do this like weird shift
thing
thing
anymore? I don't think we do, right?
anymore? I don't think we do, right?
Can't we literally just do
um it just
this? Yeah. Yeah. Yeah. Yeah.
GT
GT
shape. What's
this
one? Something like this, right?
The first few logics being zero is weird
The first few logics being zero is weird
as
as
hell. We should probably investigate
hell. We should probably investigate
that.
I almost expected that to work.
Almost. So,
um, here are your advantages, right?
your
PS and see how they implemented
it. They had this loss
it. They had this loss
here. So they did as
NL
loss. They summed. That's crazy weird.
this thing
run. Okay, now I have it implemented the
run. Okay, now I have it implemented the
way that they have it
way that they have it
implemented, right?
polish
ingredients and then they sum and
ingredients and then they sum and
there's no negative on this thing,
there's no negative on this thing,
right? Yeah. Okay. So, this is good.
value loss is super
low because there's nothing happening.
low because there's nothing happening.
It's predicting zeros and the kale is
It's predicting zeros and the kale is
blown
blown
up. Presumably something is wrong
there. So, let's see. Uh let's go back
there. So, let's see. Uh let's go back
to this then.
B trace
returns. Let's go through this thing
returns. Let's go through this thing
with
with
a fine tooth comb. Right.
So these C's
So these C's
are clamped to rows,
right? They also did this like shifted
right? They also did this like shifted
array thing. No big
array thing. No big
deal. And their
deal. And their
deltas flipped rows.
Yeah. So this we have this as well,
Yeah. So this we have this as well,
right? And then this
uh this is not how I
read this is not how I read this CS of
read this is not how I read this CS of
T, right?
Is this
correct? That would be a major
correct? That would be a major
discrepancy. Let me
discrepancy. Let me
see.
see.
So they just get rows from this log
So they just get rows from this log
ratio
ratio
here. They clamp them
here. They clamp them
CS make no changes and then yeah here
CS make no changes and then yeah here
they have this.
they have this.
But in the
paper, where'd he
go? Was it
impala? So in the paper they
impala? So in the paper they
have it.
Is this product not
um Hold on. There's a recursive formula
um Hold on. There's a recursive formula
for
it. Uh B trace targets can be computed
recursively. Okay. So this is the
recursively. Okay. So this is the
formula they're using, right?
CS and CS
is okay. Okay, so this is the formula
is okay. Okay, so this is the formula
that they're
that they're
using and let me let me compare theirs
using and let me let me compare theirs
to
this. So they do
this. So they do
B XS
Delta
SV plus discounts
CS times
act is VSP +
act is VSP +
1 minus V of
1 minus V of
T. Oh, because they have this minus
T. Oh, because they have this minus
values here,
values here,
right? Okay, hang on. Let me let me see
right? Okay, hang on. Let me let me see
if I can align it with this then. I
if I can align it with this then. I
think they're doing like several things.
Screwy. So, we're not going to need this
Screwy. So, we're not going to need this
at all.
at all.
Um, we are going to need clip
CS, which is really
CT
CT
float CT.
clamp
clamp
rows and flip rows. Yeah, these can
rows and flip rows. Yeah, these can
technically have different flip
technically have different flip
thresholds, right? They don't, but they
can. So then what we do is we get our
can. So then what we do is we get our
delta.
delta.
we get our deltas is going to
be they said they define this as flipped
rows. Hang on. I want to make sure we
rows. Hang on. I want to make sure we
get the mathematically
get the mathematically
correct definition
here.
Go. Where did I put this freaking PDF?
Little crazy
here. Row equals
Yeah, it is the clipped thing. Okay,
Yeah, it is the clipped thing. Okay,
that's fine.
that's fine.
So
row delta is equal to
row delta is equal to
rotate
times
rewards
rewards
plus
plus
gamma times
gamma times
values times next terminal.
values times next terminal.
minus values.
And
advantages is equal
advantages is equal
to
do this accum first is deltas plus
do this accum first is deltas plus
gamma.
C
C
T next
terminal. So gamma *
terminal. So gamma *
C*
N and then
BS is you add values to
BS is you add values to
this
this
right to accumul
Advantages is row
times rewards plus
discount. This is BT + 1.
discount. This is BT + 1.
minus values of t. I think this is
minus values of t. I think this is
correct. And we'll
correct. And we'll
say think this is
correct. We're going to be able to
correct. We're going to be able to
remove this whole product thing as
well. So this ends up actually being
well. So this ends up actually being
that bad codewise,
right? See, I thought we had
it. The value loss is low. It's the KL
it. The value loss is low. It's the KL
that's screwing.
that's screwing.
So presumably I'm still optimizing that
So presumably I'm still optimizing that
thing wrong.
So policy
policy. Let's go through it
again. You put your importance ratio
again. You put your importance ratio
which is this X, right?
What do you pass
in? You pass in ratio,
in? You pass in ratio,
right?
Ratio values, rewards, dumps,
ratio. Okay.
So you get your rows which are just
So you get your rows which are just
flipped to
flipped to
max. And this is exactly how I was doing
max. And this is exactly how I was doing
the
the
clipping. Wait,
clipping. Wait,
fax. You have this backwards, don't
fax. You have this backwards, don't
you? This is F
you? This is F
min. You don't need to clip. Yep. You're
min. You don't need to clip. Yep. You're
clipping the wrong here.
clipping the wrong here.
This is F
min. Flipping is
wrong. Every time, every bug is a little
wrong. Every time, every bug is a little
closer to having this thing
closer to having this thing
working. All
working. All
right, so
min.
min.
Yeah, that
Yeah, that
one. Now we try again.
Every time it's like, oh, it's
Every time it's like, oh, it's
optimizing.
Nope. You can see from the kales. The
Nope. You can see from the kales. The
kales are all all
screwy. Keep looking at this line by
screwy. Keep looking at this line by
line.
Delta
Delta
row
row
reward
reward
discount next
discount next
value terminal minus values of
value terminal minus values of
T and then a
Q deltas of T delta
Q deltas of T delta
It's
It's
discount
discount
CST
CT non-
terminal. You need next terminal
here. You do,
right? I believe so.
It would make sense from
this. This gives you Vs minus
this. This gives you Vs minus
V. You add
V. You add
value, right? You add the values
here. The Q plus values of T.
And
And
then the advantage here is the main
then the advantage here is the main
thing. Row times rewards of
thing. Row times rewards of
t gamma discount non-term minus
t gamma discount non-term minus
values. And this B S T + one.
So they just took
So they just took
this the last term here is the value
minus. So the last one of
minus. So the last one of
these is going to
be this is zero. Reward minus value.
of
of
t. You're missing a
term. This accum should probably start
term. This accum should probably start
off
as try this.
We still get exploding policy
loss. This has got to be so close though
loss. This has got to be so close though
at this point, right?
Okay. So
here make sure we do this correct. So we
here make sure we do this correct. So we
absolutely know this file is not the
absolutely know this file is not the
issue.
see
this. What's wrong with logic being like
this. What's wrong with logic being like
why are logic
why are logic
zeros? That's super screwy, isn't
it? Why is batch opposite zero?
this not get filled
up. We're missing like a ton of
up. We're missing like a ton of
observations in here, right?
It's
It's
bizarre. I will double check in a second
bizarre. I will double check in a second
that this still trains with
PO
advantages samples.
advantages samples.
You get your
batch.
batch.
Okay. Actions.
Okay, so you have your
Okay, so you have your
advantage mean
advantage mean
of the way that they did. This
was Oh, did you miss a log
was Oh, did you miss a log
softmax? I think you missed a log soft
softmax? I think you missed a log soft
max. Hang on.
Tail's still
broken. Logic should be just what comes
broken. Logic should be just what comes
out of the policy, right?
Yeah.
Target. We'll do target
Make sure there's not like a separate
Make sure there's not like a separate
param or some weird
thing. You can see the kale still
thing. You can see the kale still
screwed up.
just
detach. I don't It's weird that they sum
detach. I don't It's weird that they sum
that,
that,
but it's definitely supposed to be a
but it's definitely supposed to be a
meme.
Well, at least now we know that we have
Well, at least now we know that we have
this correct,
right? This It's got to be
right? This It's got to be
solid. The baseline is going to just be
some torch sum compute baseline
loss. Ah, so this is returns minus your
loss. Ah, so this is returns minus your
value returns equals DS, right?
value returns equals DS, right?
value
loss. So, this is good right
loss. So, this is good right
here. We're happy with
this. Yeah, we are happy with this.
Now be very very carefully go through
Now be very very carefully go through
this one more time.
You clip to a max of row
record
record
min. It gives you a
max
delta
delta
is flipped
is flipped
row reward at time t.
Gamma value at
Gamma value at
time t next t +
one times
terminal minus values. time.
I'm
I'm
going deltas of t.
Wait. Yeah, this is fine. So this is
Wait. Yeah, this is fine. So this is
delta
delta
gamma
C terminal.
They have to reverse their thing because
They have to reverse their thing because
they did it
they did it
weirdly.
VS cumulate plus
VS cumulate plus
values. And then what we did
values. And then what we did
here is make
here is make
sure broadcasted bootstrap values
Is this a different threshold? PG row
Is this a different threshold? PG row
threshold. We should probably check this
threshold. We should probably check this
right now. These are both set to one
here. We get row of t
times
reward
reward
plus
discount accum
values. The last value of this is going
values. The last value of this is going
to be the last
to be the last
value we have here.
We do all this and our policy gradient
We do all this and our policy gradient
still explodes.
It's a reasonable PG loss right there.
It's a reasonable PG loss right there.
Right.
I'm not seeing the explosion happen
I'm not seeing the explosion happen
instantly.
So yeah, this policy loss is
actually like pretty reasonable for
actually like pretty reasonable for
quite a while. It does eventually blow
quite a while. It does eventually blow
up a fair
up a fair
bit. What about the uh was old
KL. So, so it's like 90 whatever%
KL. So, so it's like 90 whatever%
clipping. Is it immediately 90%
clipping? Yes, it's immediately 90%
clipping? Yes, it's immediately 90%
clipping. 60 or whatever.
clipping. 60 or whatever.
So what is this
flip? Okay. So this clamp here
There's no clip that goes on this floss.
There's no clip that goes on this floss.
Stop it.
Try to run PO real quick. See if I broke
Try to run PO real quick. See if I broke
anything. I did.
This is
PO. We see small positive policy
PO. We see small positive policy
loss, very small chaos.
some
flipping. That is what we should
see. Okay. What is the original
Hello YouTube folks. We are hopefully
Hello YouTube folks. We are hopefully
pretty close to having V trace
pretty close to having V trace
implemented into Puffer.
implemented into Puffer.
This would hopefully stabilize the
This would hopefully stabilize the
advantage filtering stuff we've been
advantage filtering stuff we've been
doing and also maybe maybe just allow us
doing and also maybe maybe just allow us
to throw a little bit of off policy
to throw a little bit of off policy
data into the uh the
data into the uh the
buffer once it
works. Okay, so these are tiny tiny tiny
works. Okay, so these are tiny tiny tiny
numbers,
right? Okay, so how is it that
right? Okay, so how is it that
immediately these numbers
immediately these numbers
are messed up?
Because you don't
do Hang on. When do you sample your
back?
Sample. You sample your badge up
here, right?
I do wonder if there's something to
I do wonder if there's something to
this.
Yeah, they shouldn't be diverged that
Yeah, they shouldn't be diverged that
much immediately.
What happens if we
What happens if we
um
Okay. What happens if we do
this? Okay.
this? Okay.
So that actually does change stuff, but
So that actually does change stuff, but
you still learn with
you still learn with
PO,
right? So you start off with these
right? So you start off with these
approx like 0.5 and
2. You end up with a pretty damn high
2. You end up with a pretty damn high
clip
fract. But you still
fract. But you still
learn and uh the tails do not blow up.
learn and uh the tails do not blow up.
Policy loss does not blow up. Value loss
Policy loss does not blow up. Value loss
does not blow up. And it still works.
does not blow up. And it still works.
But there's quite a bit of
clipping. This thing not being clipped
clipping. This thing not being clipped
is the problem.
see you whatever I do it's still it just
see you whatever I do it's still it just
the KLs blow up instantly
It's got to just mean that the
um we can print the values as well,
um we can print the values as well,
right?
These are not
These are not
exploding. Yeah. So, these are not
exploding. Yeah. So, these are not
exploding as far as I can
see. The KL that blows up.
state experience values.
This wait value equal batch valvalues
This wait value equal batch valvalues
button.
Uh that's going to
Uh that's going to
uncip the value
loss, but the V loss is not blowing up
loss, but the V loss is not blowing up
anyways. And that should be the more
anyways. And that should be the more
stable of the losses.
What
next? What we try
next? Maybe we go
next? Maybe we go
check. Let's go back to their code for a
check. Let's go back to their code for a
second. Check a couple things.
They have a couple different flip
coefficients. This is clamped to a
max. It's clamped to a
max. It's clamped to a
max. It's clamped to a max.
So, PG
advantages just a name tubble.
So we
So we
do V trace from
logits
policy. Why do we have two different
policy. Why do we have two different
outputs
here? Oh, this is just for the ratio.
here? Oh, this is just for the ratio.
That's fine.
The
The
rewards the baseline
rewards the baseline
here
value and then they compute policy
value and then they compute policy
gradient
loss. Let's sum these together.
Presumably they have a negative sign as
Presumably they have a negative sign as
well. They're on entropy.
targets. Multiply
targets. Multiply
these. It's already detached.
from logits
from logits
here action log
props log
props log
props
props
difference you get log
difference you get log
rows and you're going immediately exit
entropy times advantage.
I think what we can even do to check
I think what we can even do to check
this
this
because this is identical.
Oh,
His advantages even look good. Let's
see. Let's see if these advantages look
see. Let's see if these advantages look
good.
Okay. Okay. So, this is zero. And
then why does this get bigger and
then why does this get bigger and
bigger? Ah, because you get another one
bigger? Ah, because you get another one
right here. Right.
That looks like
That looks like
pretty
pretty
sane reasonable stuff to me.
All
right. Yeah, actually tomorrow it's
right. Yeah, actually tomorrow it's
good.
trace. Okay.
So, isn't there there's supposed to be a
So, isn't there there's supposed to be a
value bootstrap, isn't
value bootstrap, isn't
there?
Am I
wrong? All right. So,
like that doesn't look quite right to
like that doesn't look quite right to
me, right?
So this gets
So this gets
no reward, but it does have the value on
no reward, but it does have the value on
it,
right? Do you think you need the
right? Do you think you need the
advantage normalization?
It could be that to be
fair. We'll try that in a second. Let me
fair. We'll try that in a second. Let me
just check one
just check one
thing. So if you get no reward, what's
thing. So if you get no reward, what's
the formula supposed to
do? The way they have
do? The way they have
it. Where's
B. All right. So, the way that they have
B. All right. So, the way that they have
this
things I do to fix.
This is going to be
This is going to be
one. This is
zero. This is like.99 whatever.
So it's this minus
So it's this minus
values. So V ST +
values. So V ST +
one. So this should literally end up
one. So this should literally end up
being
like Yeah, it's just going to be like
like Yeah, it's just going to be like
this number minus this number.
this number minus this number.
like that, right?
like that, right?
Yeah. So, a very small number is
Yeah. So, a very small number is
appropriate because it just it ends up
appropriate because it just it ends up
being the value difference between two
being the value difference between two
states. Let me try advantage
normalizing. Try this.
It just like it blows up instantly,
man. See, now the KL's fine, but the
man. See, now the KL's fine, but the
value loss is
screwed. Well, that is probably the
screwed. Well, that is probably the
thing that I broke from before, right?
I just put this down here.
So now the value loss is still exploding
So now the value loss is still exploding
just less
quickly. Make sure that um we still can
quickly. Make sure that um we still can
train PO on this.
Okay. So, this one still
works. Does it work without value
works. Does it work without value
function clipping?
Yeah, that seems fine,
right? Maybe even
right? Maybe even
better. Okay, so that might even be
better. Okay, so that might even be
better.
Really? value loss blowing
up. What was the value loan?
And it's clip to the clip
And it's clip to the clip
coefficient, which is kind of funny.
Yeah. So,
Yeah. So,
um, is the value loss just done now? Is
um, is the value loss just done now? Is
that what's happening?
batch
returns versus new value.
returns versus new value.
So we
So we
have new
value we have batch
returns and set that right
returns and set that right
there to
VS vs advantage.
This thing has just got to be blowing up
This thing has just got to be blowing up
completely,
right? Value
function. Look at this.
I mean, this is the one that was that
I mean, this is the one that was that
should be easy to get
right to add this and then the
values values
Right.
This thing should end up being pretty
This thing should end up being pretty
small.
Yeah, really good.
Go print out the values.
values.
values.
This Yeah, we can do that. Why not?
Interesting. This just
explodes. This thing just gets bigger
explodes. This thing just gets bigger
and bigger.
There's not even a sign difference.
There's not even a sign difference.
Wait, the fact that there's not a sign
difference. Batch
difference. Batch
values, that means that
like the values and the returns actually
like the values and the returns actually
kind of
kind of
match. The returns just make no bloody
match. The returns just make no bloody
sense, right?
Because all you're supposed to do is
Because all you're supposed to do is
just
just
like
this, right?
Maybe I need to do
Maybe I need to do
this. Maybe this feels here.
Just going to blow
up. It's just going to blow
up.
up.
Okay. Why Why is it like this?
It's like
That's cool.
What do we think?
and check against the math. I
and check against the math. I
guess
lambda. Hey captain, been building a
lambda. Hey captain, been building a
planter. I've been implementing V trace
planter. I've been implementing V trace
all day.
I like to be building a planner. That
I like to be building a planner. That
sounds
chill. And maybe this one we can see. So
Delta
plus gamma
C. Does this need a
C. Does this need a
minus?
minus?
No. Right.
Carpenter was showing me a few other
Carpenter was showing me a few other
guys some tricks. TLDDR V trace uh
guys some tricks. TLDDR V trace uh
replacement for generalized advantage
replacement for generalized advantage
estimation that should be better at off
estimation that should be better at off
policy correction. So it should allow us
policy correction. So it should allow us
to get more out of the new experience
to get more out of the new experience
buffer.
similar to to uh generalized advantage
similar to to uh generalized advantage
estimation. It ditches the lambda term
estimation. It ditches the lambda term
and instead it threads a
and instead it threads a
um it threads the important sampling
um it threads the important sampling
term that's used for the advantage
term that's used for the advantage
function uh or used to scale was it to
function uh or used to scale was it to
clip the policy gradient. Yeah. Instead,
clip the policy gradient. Yeah. Instead,
it threads that through the advantage
it threads that through the advantage
computation.
not max brad norm. It's
not max brad norm. It's
um it's a logic ratio of the new policy
um it's a logic ratio of the new policy
to the old policy before you started
to the old policy before you started
optimizing at this
epoch. So that then means that the
epoch. So that then means that the
clipping term also gets applied to the
clipping term also gets applied to the
the clipping term gets like deeply
the clipping term gets like deeply
applied to the value function as well
applied to the value function as well
but not just like as a post hawk thing.
What are you doing?
What are you doing?
I'm currently attempting to implement a
I'm currently attempting to implement a
CUDA kernel for this vtrace off policy
CUDA kernel for this vtrace off policy
correction advantage estimate from
correction advantage estimate from
impala into puffer so that we can uh
impala into puffer so that we can uh
leverage some like slightly off policy
leverage some like slightly off policy
data with our
data with our
implementation abandoning P30 for now or
implementation abandoning P30 for now or
do you think the new will experience
do you think the new will experience
buffer? Uh, so VRC is still very similar
buffer? Uh, so VRC is still very similar
to generalized advantage estimation and
to generalized advantage estimation and
like pretty much anything that I was
like pretty much anything that I was
going to do with
going to do with
P30 on top of J I could do it with V
P30 on top of J I could do it with V
trace as well.
So, Vray still has the annoying lambda
So, Vray still has the annoying lambda
uh the annoying gamma parameter, though
uh the annoying gamma parameter, though
arguably it should be slightly easier to
arguably it should be slightly easier to
deal with because it no longer has the
deal with because it no longer has the
lambda
parameter. So actually I think that
parameter. So actually I think that
probably once I get this thing
probably once I get this thing
implemented fully, this will probably be
implemented fully, this will probably be
a little easier to work with for like P3
a little easier to work with for like P3
style stuff versus
style stuff versus
J. How much testing of the experience
J. How much testing of the experience
buffer? Not a ton, but I mean we have
buffer? Not a ton, but I mean we have
results on we've got results on like
results on we've got results on like
neural MMO stuff. Actual cube. That's
neural MMO stuff. Actual cube. That's
soda right there. So there you go.
soda right there. So there you go.
That's soda with advantage filtering. Um
That's soda with advantage filtering. Um
So, yeah, it's pretty good. The curves
So, yeah, it's pretty good. The curves
aren't quite as clean, though, so we're
aren't quite as clean, though, so we're
we're gonna have to clean some stuff up
we're gonna have to clean some stuff up
there. But, uh, yeah, it's been pretty
there. But, uh, yeah, it's been pretty
decent overall.
doesn't take much of an SPS. It
doesn't take much of an SPS. It
shouldn't be shouldn't change the SPS at
all. There's like one slight thing that
all. There's like one slight thing that
we might have to consider when we do
we might have to consider when we do
filtering with this, but it should be
filtering with this, but it should be
fine.
It only screws over the SPS if you have
It only screws over the SPS if you have
like a two million step per second
like a two million step per second
training run and then you don't
training run and then you don't
implement a CUDA kernel for this. And lo
implement a CUDA kernel for this. And lo
and behold to the colonel.
I
I
mean, we kind of should just go off of
mean, we kind of should just go off of
this reference, right? Where is
it? I'm trying to think what the odds
it? I'm trying to think what the odds
are that I missed. I just missed
are that I missed. I just missed
something in here even though I've
something in here even though I've
looked at it 10 times.
Just rapid fire. Try a couple stupid
things. I think that's needed, but
Oh, that blows up instantly.
Oh, that blows up instantly.
Okay.
So now the KL blows
up. Interesting.
So either way, one of these two blows
up. Where did that freaking
up. Where did that freaking
implementation just go?
+
one. Maybe this is an issue. Hang
one. Maybe this is an issue. Hang
on. So, we
do t + one
do t + one
So this gets all the
deltas.
deltas.
Okay.
Okay.
And this accumulator
The compile times are a little
silly. Okay. So that's interesting,
silly. Okay. So that's interesting,
right? The value loss
The value loss is back to being mostly
same.
Actually, both the losses are kind of
like Yeah, the losses are like kind of
like Yeah, the losses are like kind of
the same until uh I guess the policy
the same until uh I guess the policy
where the KL tends to blow up. That's
where the KL tends to blow up. That's
better
better
though. Okay, then maybe this thing's
though. Okay, then maybe this thing's
just really sensitive. Maybe I have some
just really sensitive. Maybe I have some
like really minor minor details screwed
like really minor minor details screwed
up.
So the delta, you're missing the last
So the delta, you're missing the last
delta, right?
delta, right?
missing the last delta.
EG advantages.
next. Maybe I have this wrong still.
next. Maybe I have this wrong still.
Hang
on. Wait, I do have this wrong, right?
on. Wait, I do have this wrong, right?
I do have this wrong. Hang on. I think I
I do have this wrong. Hang on. I think I
see it. B ST +
one.
Yeah. All right. I got to get the
Yeah. All right. I got to get the
restroom and then I think I can fix
restroom and then I think I can fix
this. I think I see it. Right back.
So here's the thing,
So here's the thing,
right? This
is V S T +
is V S T +
one. This should not be a cube. This
one. This should not be a cube. This
should be V
should be V
S T + 1.
or this one here. I
mean, which is going to start out at
mean, which is going to start out at
zero.
Possibly we're going to have to add um
Possibly we're going to have to add um
the
the
terminal bootstrap value. We'll
see. This thing still immediately blows
see. This thing still immediately blows
up.
How many bugs do I have to fix before
How many bugs do I have to fix before
this thing
behaves? Look at this
behaves? Look at this
chaos. Okay.
Do I need this next non- terminal thing?
probably
probably
not. I don't think this fixes it though,
not. I don't think this fixes it though,
right? I don't I don't think that
right? I don't I don't think that
fixes. I'll run it, but I think we keep
fixes. I'll run it, but I think we keep
looking.
Discounts
discounts t
discounts t
next values of
next values of
t. Yeah, this stupid thing is just going
t. Yeah, this stupid thing is just going
to
to
keep
keep
breaking. Okay, let's think about
breaking. Okay, let's think about
terminal or bootstrap final
terminal or bootstrap final
value.
So, yeah. So, it's it's just the last
So, yeah. So, it's it's just the last
value, right?
value, right?
B ST +
one. See if that does anything.
exploding
algorithm. Okay, I must have missed
algorithm. Okay, I must have missed
something else, right? Like
Start off at zero.
Okay. So,
Okay. So,
delta is going to
delta is going to
grab t next, right?
And then there's a
So this whole thing should be
So this whole thing should be
fine. Oh wait, the only thing is that
fine. Oh wait, the only thing is that
this the last BS is going to be
wrong. Hang on. The last BS is going to
wrong. Hang on. The last BS is going to
be wrong.
be wrong.
What's the last PS supposed to
be? And it's supposed to be bootstrap
be? And it's supposed to be bootstrap
value.
It is.
I just did that,
right? Yeah, I just did that and that
right? Yeah, I just did that and that
didn't make it. Like that didn't fix it
didn't make it. Like that didn't fix it
magically.
This guy has a pretty nice paper. It's
This guy has a pretty nice paper. It's
uh was it beyond the rainbow? It's uh
uh was it beyond the rainbow? It's uh
basically rainbow, which is all the
basically rainbow, which is all the
kitchen sink of tricks that they threw
kitchen sink of tricks that they threw
on DQM. Well, this guy found another
on DQM. Well, this guy found another
kitchen sink worth of tricks to throw on
kitchen sink worth of tricks to throw on
it, and it did a lot better.
This is discord.gg/puffer GG/huffer if
This is discord.gg/puffer GG/huffer if
anyone is interested want to get
anyone is interested want to get
involved.
We don't get a break on this
We don't get a break on this
thing. This still needs to get done
today. I mean, they just used the
today. I mean, they just used the
bootstrap value at the very end,
right? What's the s
then
add
values? No, they do use it there as
values? No, they do use it there as
well, don't
they? They use the bootstrap value at
they? They use the bootstrap value at
the end here as well.
Finally helps increasing action
Finally helps increasing action
gaps and therefore reducing policy
gaps and therefore reducing policy
turning off
policiness. This policy isn't based on
policiness. This policy isn't based on
the value function directly at
the value function directly at
least increasing action gaps.
This guy had some very nice work.
Soft WS.
Am I getting
trolled? I'm not actually getting
trolled? I'm not actually getting
trolled.
Got a bajillion things in here.
Love to see people using the algorithm
Love to see people using the algorithm
as fast as possible.
This would be a very cool guy to get
This would be a very cool guy to get
involved with. Buffer
still have to finish this uh beach race.
still have to finish this uh beach race.
We don't get a pass on
We don't get a pass on
this. I do not know what's wrong with it
though. Hang on. We are messing with
though. Hang on. We are messing with
this uses.
this uses.
I doubt you can get that speed, but
Okay. So
technically technically this BT plus one
technically technically this BT plus one
could be
could be
off. Soda all policy uses fairly long
off. Soda all policy uses fairly long
traces uses 160.
traces uses 160.
Interesting steps. Extremely slow.
Not as good.
No, I got to read that paper for sure. I
No, I got to read that paper for sure. I
think we start with this thing
think we start with this thing
though. What's this paper
though. What's this paper
from? Is this recentish or
no? 2022. Not really.
Each forward pass slows down a lot.
Each forward pass slows down a lot.
do so many poor passes kills.
two weeks for an Atari game. It's crazy.
two weeks for an Atari game. It's crazy.
It's not like one policy and everything,
right? Yeah, it's still one each, I
right? Yeah, it's still one each, I
think.
That's ridiculous.
I I actually think that I do have this
I I actually think that I do have this
right now, don't
right now, don't
I? I
I? I
miss I missed the last delta, I believe.
Yeah, I missed the last
delta. What does that do? Missing the
delta. What does that do? Missing the
last
delta. Do I have like some weird off by
delta. Do I have like some weird off by
one on the
one on the
advantages? Could be.
advantages? Could be.
I don't think so though.
Oh jeez. They actually roll it out like
Oh jeez. They actually roll it out like
160 time steps and then they throw away
160 time steps and then they throw away
159 of
159 of
them. Crazy.
We're updating a single Q value.
I didn't try without normalizing
I didn't try without normalizing
advantages until I since I did this,
advantages until I since I did this,
right? I didn't try without normalizing
right? I didn't try without normalizing
advantages. Maybe that maybe you can't
advantages. Maybe that maybe you can't
normalize advantages in this.
That's one thing we should try.
Did I really just casually do that? Hang
Did I really just casually do that? Hang
on. Is this the
on. Is this the
baseline? No, this is not the
baseline? No, this is not the
baseline. We just got V trace
working. Well, okay then.
See if we uh we can
See if we uh we can
do we can get rid of this
Start. Looks better, right?
off policy or bottlenecks by gradient
off policy or bottlenecks by gradient
steps more than anything
Well, I think we can uh we can actually
start benchmarking a little
start benchmarking a little
bit. That's
awesome. All of a sudden, it
awesome. All of a sudden, it
works. Also, yeah, that's definitely
works. Also, yeah, that's definitely
Soda.
Spencer just got his
Okay, it's not quite on par yet, right?
Okay, it's not quite on par yet, right?
But this is also no advantage filtering
But this is also no advantage filtering
like brand new thing that we just
like brand new thing that we just
implemented.
So next thing on this is going to
be can we
filter? Well, let's make a commit right
filter? Well, let's make a commit right
now.
now.
because
All
right, we're actually going to do some B
right, we're actually going to do some B
trade stuff real quick. So, this guy is
trade stuff real quick. So, this guy is
cool to chat with. Um, I actually I
cool to chat with. Um, I actually I
really wanted to have somebody with off
really wanted to have somebody with off
policy experience for a while, like
policy experience for a while, like
actually like good off policy
actually like good off policy
experience.
experience.
uh doing some stuff around in puffer and
uh doing some stuff around in puffer and
yeah from that manuscript this guy would
yeah from that manuscript this guy would
be perfect for
be perfect for
that. You can check out his paper. It's
that. You can check out his paper. It's
on open review beyond the rainbow. It's
on open review beyond the rainbow. It's
uh pretty nice additions to rainbow DQN
uh pretty nice additions to rainbow DQN
and pretty well done science too.
So I think that all we have to do for
this can we just copy paste
This
like
and two weeks to 12 hours.
and two weeks to 12 hours.
Apex stuff for
Apex stuff for
BTR. Haven't seen Apex in a long
BTR. Haven't seen Apex in a long
time. God damn
it. Distributed architecture.
Okay. So, the plan with this stuff right
Okay. So, the plan with this stuff right
here would
be how do we
compute how do we compute V
compute how do we compute V
trace? I guess all we have to do is add
um log ratio, right?
We massively lower the replay ratio.
We massively lower the replay ratio.
Interesting.
I think all you have to do for
I think all you have to do for
experience, right, is you just have to
experience, right, is you just have to
initialize.
You just have to initialize it to ones,
right?
right?
Because well, it is going to be one to
Because well, it is going to be one to
start with,
start with,
right? Wait, it's literally always going
right? Wait, it's literally always going
to be one.
to start with.
to start with.
No, I guess yeah, initially it should
No, I guess yeah, initially it should
just
just
be we can just do like
would be like experience importance
would be like experience importance
here. Maybe
turns
advantage. A lot of my work focus on
advantage. A lot of my work focus on
fairly slow sim.
5:14 right now. I'm going to try and
5:14 right now. I'm going to try and
implement this oneation. I'm going to
implement this oneation. I'm going to
order dinner. I'll dev for like half
order dinner. I'll dev for like half
hour or whatever. Get dinner and then
hour or whatever. Get dinner and then
I'll come back for another couple hours.
I'll come back for another couple hours.
I think that's the plan.
Well, this is kind of sketchy, isn't
it? So, this ratio
um yeah, you don't actually know what
um yeah, you don't actually know what
this ratio is.
until you do the forward
until you do the forward
pass,
right? Oh, that's not an issue though,
right? Oh, that's not an issue though,
is
is
it? No, that's not an
it? No, that's not an
issue. Cuz then all you just do is you
issue. Cuz then all you just do is you
just do
just do
experience
experience
or ex ratio.
Right. Perfect. So we finish that
Right. Perfect. So we finish that
conversation. We
get this is fine, right?
or is it
not? This should be
fine. And then Yeah, this should be
fine. And then Yeah, this should be
fine. Let's We'll
see. Get some food as well.
The amount of supposedly Asian food in
The amount of supposedly Asian food in
uh Florida is very concerning.
Where's this
batch? Oh. Uh, this is experience.
And this is
uh
returns. Must be 2D.
Okay, I am actually going to order food
Okay, I am actually going to order food
while I fix this because otherwise I'm
while I fix this because otherwise I'm
going to
going to
like
Yeah, I think what I want
Just real
Just real
quick, it'll be quicker if I do this on
quick, it'll be quicker if I do this on
here, assuming that uh I still have my
here, assuming that uh I still have my
credentials in. And then we'll come back
credentials in. And then we'll come back
and we'll let me order dinner and then
and we'll let me order dinner and then
we'll actually we'll get some cool uh
we'll actually we'll get some cool uh
experiments going and I'll actually talk
experiments going and I'll actually talk
through stuff. I was a little distracted
through stuff. I was a little distracted
with that conversation, but
um yeah, we will actually get some cool
um yeah, we will actually get some cool
experiments
in. Got to love how Florida the average
in. Got to love how Florida the average
like distance to get food is like 10
like distance to get food is like 10
miles.
miles.
What in the
heck? It's actually not bad. That's not
heck? It's actually not bad. That's not
a bad
a bad
idea. Okay, local pups got me.
order some
stuff and uh we will run these
stuff and uh we will run these
experiments.
One
One
second. Hey bet.
Wrong thing. One
second. Just
Perfect. Okay, that is me taken care
of. Let's get back to
research. So, Here's the thing with B
research. So, Here's the thing with B
trace, right? Um, it requires an
trace, right? Um, it requires an
importance
importance
ratio. The importance ratio, we're not
ratio. The importance ratio, we're not
going to deal
going to deal
with historical buffers for now,
with historical buffers for now,
but it should always start off as an
but it should always start off as an
important ratio
important ratio
one because it's the ratio of new policy
one because it's the ratio of new policy
logics to old policy logics. And at the
logics to old policy logics. And at the
start of any, new policy equals old
start of any, new policy equals old
policy.
policy.
Um, so we should be able to do something
Um, so we should be able to do something
like
that. Oh yeah, and now we just have to
that. Oh yeah, and now we just have to
debug some shape stuff and we should
debug some shape stuff and we should
actually be able to compare this to what
actually be able to compare this to what
I was doing earlier.
This is the
This is the
issue. I can I Yeah, I just know that's
issue. I can I Yeah, I just know that's
the issue here. Let me just fix that.
the issue here. Let me just fix that.
This needs to
be instit.
value is not shaped. Once you do that
uh we can call this
uh we can call this
advantages. No reason not
advantages. No reason not
to experience
stocks. What is BTR paper?
Where is the It's beyond the rainbow.
Where is the It's beyond the rainbow.
It's this.
Yeah. Yeah, it's this thing. There's uh
Yeah. Yeah, it's this thing. There's uh
I guess it is on archive then. There's
I guess it is on archive then. There's
an archive. I was looking at the open
an archive. I was looking at the open
review version. From the format, it
review version. From the format, it
looks about the same.
So we had author of that is on the
discord. That's the goal eventually is
discord. That's the goal eventually is
just to have all the all the cool RL
just to have all the all the cool RL
people on the
Discord. Whenever I have a question
Discord. Whenever I have a question
about anything, it's just like ah
about anything, it's just like ah
they're all right here.
So,
uh, what we're with at the
uh, what we're with at the
moment is not quite off policy. It is an
moment is not quite off policy. It is an
off policy correction technique applied
off policy correction technique applied
to on policy.
Uh, yeah, that didn't freaking work, now
Uh, yeah, that didn't freaking work, now
did
it?
it?
Batch. Is it like
batchadvantages? PO arguably
batchadvantages? PO arguably
has not except for the dev branch. It
has not except for the dev branch. It
doesn't.
doesn't.
I mean the buffer branch, not even the
I mean the buffer branch, not even the
dev branch, the more more recent
branch. Okay, so we do actually have
branch. Okay, so we do actually have
advantages in the buffer.
Perfect. So now this is actually got to
Perfect. So now this is actually got to
be experience or is it
be experience or is it
back advantages?
Okay, it
runs. Still on random
sampling. It crashes.
sampling. It crashes.
Really?
Okay. Well, I have a
bug. Do I have a
bug. Do I have a
bug? Hang
bug? Hang
on. Am I not being smart about this?
on. Am I not being smart about this?
So every mini batch we recomputee B
trace the experience
importance
right we sample from advantages
And then if we're going to use V
trace
importance no this right it's not ratio
Actually, this shouldn't matter though,
Actually, this shouldn't matter though,
right?
right?
Wait. No, this is
correct. Is equal to ratio. Yeah. Yeah,
correct. Is equal to ratio. Yeah. Yeah,
this is
correct. It should be
correct. It should be
logit actions.
Huh? That's unstable.
Really? All I did was
uh still on random sampling is the
uh still on random sampling is the
thing. So that advantage didn't factor
thing. So that advantage didn't factor
in.
It's really weird that that is uh
It's really weird that that is uh
unstable. Let's just make sure we didn't
unstable. Let's just make sure we didn't
get
get
unlucky. I don't think
so.
So, comput your advantages, right?
No, we didn't get
No, we didn't get
unlucky. The kale blows up.
But it's weird
because do I just roll this
back? Hang on. Do I roll this back or do
back? Hang on. Do I roll this back or do
I just
compare? It's a good thing. And this is
compare? It's a good thing. And this is
why I made a commit.
why I made a commit.
Uh 15 20 minutes before
Uh 15 20 minutes before
food. Getting very
food. Getting very
hungry.
hungry.
Uh okay.
So from
So from
file
file
puffer clean
puffer
puff good time to remind folks you want
puff good time to remind folks you want
to help me out for free just started the
to help me out for free just started the
give almost at 2k stars and it really
give almost at 2k stars and it really
helps.
Not that many RL post with 2K
Not that many RL post with 2K
stars, especially not in like one year
stars, especially not in like one year
of fulltime
dev. Okay. So in this one here, I gave
dev. Okay. So in this one here, I gave
it
it
once
once
and I sampled
and I sampled
But this shouldn't matter because it's
But this shouldn't matter because it's
random sampling
random sampling
anyways, right? And then in
here, V
trace. Yeah. Start of the buffer. Uh you
trace. Yeah. Start of the buffer. Uh you
and then here we get VS vantage.
values. What is the difference between
values. What is the difference between
computing it here and
computing it here and
then elsewhere?
Oh, hang
Oh, hang
on. Yeah. No, you can't do
on. Yeah. No, you can't do
this. The uh the importance ratio gets
this. The uh the importance ratio gets
stale this way,
stale this way,
right? Yeah. The importance ratio gets
stale. Okay. So, I have to go back.
stale. Okay. So, I have to go back.
We go back
to
this. We go back to this and make sure
this. We go back to this and make sure
this still runs. And then what we can do
Let me see how prioritized experience
Let me see how prioritized experience
replays update formula works.
Okay. So they use TD
error and then they update the
error and then they update the
priorities with a new TD error after a
priorities with a new TD error after a
learning step.
So TD error.
Do you have to
um hang on when you prioritize this
thing your value this is going to get
thing your value this is going to get
slightly stale
Okay. But the key is it only gets stale
for Yeah, it's better to make that.
for Yeah, it's better to make that.
Okay, that's fine. Yeah, I see it. I
Okay, that's fine. Yeah, I see it. I
think I see it now, folks. I see it. So,
think I see it now, folks. I see it. So,
we have advantages here.
I guess we just take importance, right?
Okay. So, we do importance. It's going
Okay. So, we do importance. It's going
to start uniform.
This will be
important. And then where is it?
experience or is it just importance?
Um, I think that'll do it. That's not a
Um, I think that'll do it. That's not a
Neptune
log shape
mismatching. Oh yeah, this importance
mismatching. Oh yeah, this importance
doesn't need to be
doesn't need to be
Yeah, this importance needs to be uh
Yeah, this importance needs to be uh
data.experience
rows, right? Yeah, that's the
importance. Holy. Okay.
Oh, I think that it literally it does.
Oh, I think that it literally it does.
Okay. Port one to
like. Okay.
like. Okay.
So, this should still
So, this should still
work like
before. Come on.
It's getting hungry.
Oh, it eats the batch advantages,
Oh, it eats the batch advantages,
doesn't it?
This is just advantages.
Whatever we do
this. Okay, this runs
this. Okay, this runs
right explode instantly. No. Okay, we
right explode instantly. No. Okay, we
track
track
this and we see if we have broken
this and we see if we have broken
anything or not. And if we haven't, the
anything or not. And if we haven't, the
key thing is now we should actually be
key thing is now we should actually be
able to add in
able to add in
uh we should actually be able to add in
uh we should actually be able to add in
the advantage
the advantage
filtering. Here's our
run. This should be like about what this
run. This should be like about what this
yellow curve is.
1 minute 80 milstep training rooms are
1 minute 80 milstep training rooms are
very
nice. Yeah. Okay. This looks like this
nice. Yeah. Okay. This looks like this
is about the yellow, right? It's already
is about the yellow, right? It's already
up to seven. Yeah. So, this is about on
up to seven. Yeah. So, this is about on
par with the uh the yellow curve.
And there's some variability. So it's
fine. Now what we get to
do, delete this line. Rerun it. We
do, delete this line. Rerun it. We
should have important sampling.
We would hope would be better.
At least don't be
worse. Okay,
worse. Okay,
good. Yeah, this is uh this is what we
good. Yeah, this is uh this is what we
want. This is what we wanted to see.
I think it is better. We got to wait for
I think it is better. We got to wait for
the curve. But I think
so. Okay. Yeah, that is better. Perfect.
so. Okay. Yeah, that is better. Perfect.
So um then the question is going to be
right. Oops.
Do I add this to the experience
buffer? Probably, right?
Okay. And then we do
experience and then it's the importance
experience and then it's the importance
gets updated to this. And the only other
gets updated to this. And the only other
thing we have to do is we have to
thing we have to do is we have to
remember to zero it.
or one it I guess reset it either
way rerun so that we make sure we didn't
way rerun so that we make sure we didn't
break anything and then we will run with
Goodbye. This is experience
bros. Food is here in like a few
bros. Food is here in like a few
minutes. more
experiments and then that'll set a
experiments and then that'll set a
decent tempo. I don't expect the off
decent tempo. I don't expect the off
policy to work immediately by the way. I
policy to work immediately by the way. I
don't expect it to work
immediately. I just wanted to have V
immediately. I just wanted to have V
trace and now we have V
trace which technically kills a
trace which technically kills a
hyperparameter.
If it matches in perf, it's better just
If it matches in perf, it's better just
because it kills hyper
parameter. Okay, that looks good. Now we
parameter. Okay, that looks good. Now we
set replay factor equal
one. Set like 0.5.
one. Set like 0.5.
Maybe 0.5 should be plenty.
Okay, last
experiment should
be replay factor 0.5 means it's allowed
be replay factor 0.5 means it's allowed
to keep up to half a batch of data
to keep up to half a batch of data
across EOPS. So it is actually allowed
across EOPS. So it is actually allowed
to use fully stale data.
Yeah, I think it's going to be
Yeah, I think it's going to be
um the same stale result here.
How long is it keeping it? As long as it
How long is it keeping it? As long as it
likes. It's based on um well, it should
likes. It's based on um well, it should
be based on a priority metric, but we
be based on a priority metric, but we
got to improve on that. Okay. So, this
got to improve on that. Okay. So, this
does not uh vrace did not magically let
does not uh vrace did not magically let
us use all policy data yet. We will
us use all policy data yet. We will
continue to work on that. We will get a
continue to work on that. We will get a
actual proper prioritized replay buffer
actual proper prioritized replay buffer
and we will do some other stuff for that
though. Change the 05. Well, that was
though. Change the 05. Well, that was
the point of the test to see whether you
the point of the test to see whether you
could use
could use
Yeah, that's
Yeah, that's
fine. All right, I will be back after
fine. All right, I will be back after
dinner. Um, probably should be back
dinner. Um, probably should be back
dinner for at least like an hour or so.
dinner for at least like an hour or so.
for folks
for folks
watching puffer.ai for all my stuff.
watching puffer.ai for all my stuff.
Reinforcement learning dev live almost
Reinforcement learning dev live almost
every day. Uh start the GitHub to help
every day. Uh start the GitHub to help
us out. It really helps. We've got lots
us out. It really helps. We've got lots
of fun demos on here, lots of different
of fun demos on here, lots of different
games, all with neural net that run in
games, all with neural net that run in
your
your
browser. If you want to get involved
browser. If you want to get involved
with dev, join the discord. Some of our
with dev, join the discord. Some of our
best contributors came in with zero RL
best contributors came in with zero RL
experience.
experience.
And you can follow me on X for more
And you can follow me on X for more
reinforcement learning
reinforcement learning
content. It's all RL all the time.
content. It's all RL all the time.
[Music]

Kind: captions
Language: en
Okay, we are
Okay, we are
back. Second session of
today. How's the training
going? So
going? So
close. So close to here. All right,
close. So close to here. All right,
we'll let that take a little bit
longer. Instability is kind of
longer. Instability is kind of
silly. We were going to look at clipping
silly. We were going to look at clipping
now,
right? So, I just tried removing
right? So, I just tried removing
clipping from what I thought would be an
clipping from what I thought would be an
end where it wouldn't
end where it wouldn't
matter, and it totally broke.
Hang on. This is with the filtering,
right? Let's rerun it like
right? Let's rerun it like
this. See if it still
this. See if it still
breaks. This should only use each sample
breaks. This should only use each sample
like one time.
Ah, still
problems. So,
problems. So,
um, Whoops.
I uh I guess I was totally wrong
I uh I guess I was totally wrong
there. I didn't think the clipping would
there. I didn't think the clipping would
make that much of a
make that much of a
difference. I figured it would only make
difference. I figured it would only make
a difference if you're running a ton of
a difference if you're running a ton of
epochs.
Oh, the kale exploded here,
Oh, the kale exploded here,
right? Four, five. Yeah, the tail
exploded. Does that make any sense at
exploded. Does that make any sense at
all?
Like, right, hang on. What if we try?
Like, right, hang on. What if we try?
Let me try things
Let me try things
here. We return
clipping or up to eight pots.
clipping or up to eight pots.
You go slow mode.
It's possible this is not strictly
It's possible this is not strictly
better even though it should be,
right?
T. This looks
um fairly similar. maybe a little
better. So, I'll go on record as I
better. So, I'll go on record as I
saying that I thought that I'd looked at
saying that I thought that I'd looked at
the math a couple weeks ago and I
the math a couple weeks ago and I
thought that I'd figured out that for
thought that I'd figured out that for
the most part the on policy off policy
the most part the on policy off policy
distinction was a joke. Uh, it's not. As
distinction was a joke. Uh, it's not. As
it turns out though, the thing that is a
it turns out though, the thing that is a
joke is that
joke is that
um even Rainbow seems to be
um even Rainbow seems to be
kind of breaking its own math
kind of breaking its own math
assumptions. It's not like I is off
assumptions. It's not like I is off
policy is the correct one of the two.
Okay. So, I mean this is
like this is fine,
like this is fine,
right? If you crank up the update
right? If you crank up the update
box, you don't really break anything.
Yeah, it doesn't seem like you really
Yeah, it doesn't seem like you really
break
anything. Let's check the
anything. Let's check the
[Music]
losses. Let's get rid of this one.
One bad grab
spike, but I mean otherwise it tracks
spike, but I mean otherwise it tracks
pretty well and that's a perfect
sol. What happens if you really crank
sol. What happens if you really crank
this up?
I'm just trying to see if clipping is
I'm just trying to see if clipping is
like a band-aid path or if it's
like a band-aid path or if it's
actually
actually
stable the way we would think
I'm really surprised about the clipping
I'm really surprised about the clipping
thing
Yeah, there's clearly stuff going on
Yeah, there's clearly stuff going on
here that I do not understand yet.
So, the thing that I'm not sure here,
So, the thing that I'm not sure here,
right, is like I was kind of looking for
right, is like I was kind of looking for
just chuck all the methods on stuff real
just chuck all the methods on stuff real
quick to see if anything happens.
quick to see if anything happens.
Anything surprising like we did get the
Anything surprising like we did get the
big nice boost from
Muan.
Muan.
Oh, I see.
Uh, I check
something. What do we clip against?
Wait, the PG lost
Wait, the PG lost
is advantage.
New log props minus back.log
props. Oh no, that is good. Yeah, that's
props. Oh no, that is good. Yeah, that's
how clipping should work.
That's
crazy. So,
um, yeah, the PO clipping is not a fix
um, yeah, the PO clipping is not a fix
or off policy. It's a band-aid.
Like you can still go too far off in
Like you can still go too far off in
just one
epoch. Yeah. Look at this thing. Can we
epoch. Yeah. Look at this thing. Can we
even see this in the losses anywhere or
even see this in the losses anywhere or
is it just
bad? Not really. Right.
bad? Not really. Right.
Entropy is
Entropy is
fine. We're not logging
fine. We're not logging
that. Prox K scale is
fine. You do see the value loss quite
fine. You do see the value loss quite
different
here. It's interesting.
clipping is is in fact
clipping is is in fact
essential. It is actually going to keep
essential. It is actually going to keep
training on this and
training on this and
uh it is
uh it is
improving. I mean that definitely
improving. I mean that definitely
damaged it.
That was an unexpected result. I'm
That was an unexpected result. I'm
trying to think where we go from
there. I really didn't think clipping
there. I really didn't think clipping
was going to make a huge difference in
was going to make a huge difference in
like one update epoch training.
Hang on. What if it's an optimizer
Hang on. What if it's an optimizer
issue? Let's Let's roll one other thing
out. Let me just rule one other thing
out. Let me just rule one other thing
out here.
I want to make sure it's not
um like an optimizer
um like an optimizer
issue. So essentially if we still clip
issue. So essentially if we still clip
but we give it like a really big bounce.
Oh.
Huh.
What? So this is literally like some
What? So this is literally like some
dumb optimizer thing, right?
You don't see a spike
anywhere, but
like So we have This thing,
like So we have This thing,
right? Where was
right? Where was
it? Now, where was the run that totally
it? Now, where was the run that totally
exploded? Yeah, this one.
Right. We should actually see this in
Right. We should actually see this in
the
losses. Yeah. So, this thing just blows
losses. Yeah. So, this thing just blows
up instantly.
You do this.
Get rid of this
thing. I mean, it's still like
fine with 1.0
clipping. So
I don't know about you, but that doesn't
I don't know about you, but that doesn't
strike me
strike me
as flipping to keep the thing on policy
as flipping to keep the thing on policy
or whatever. That strikes me as like
or whatever. That strikes me as like
flipping to prevent uh bad update from
flipping to prevent uh bad update from
destroying the policy, right?
Let's try 02 real quick because this was
Let's try 02 real quick because this was
the value used in the paper. Make sure
the value used in the paper. Make sure
there's not something magic going on.
It's crazy that you clip the loss and
It's crazy that you clip the loss and
you clip the gradient.
you clip the gradient.
I figured that clipping the gradient
I figured that clipping the gradient
would be enough to prevent screwy
would be enough to prevent screwy
optimizer stuff from
happening. But I guess
happening. But I guess
not cuz if you get like a bad advantage,
not cuz if you get like a bad advantage,
right? If you get a bad
right? If you get a bad
advantage, you could take like a 0.5
advantage, you could take like a 0.5
gradient step in the completely wrong
gradient step in the completely wrong
direction. How's this
do? Yeah. So, this actually it does a
do? Yeah. So, this actually it does a
little better than 0.1 but it
little better than 0.1 but it
underperforms
0.5. Let me see what these clip
0.5. Let me see what these clip
coefficients actually do.
So these clip coefficients get
applied. Uh yes this like ratio
flipping. Okay. This is directly
on
props. Yeah, this is logic space
clipping. So it doesn't let the old log
clipping. So it doesn't let the old log
props get too far away from the new
props get too far away from the new
ones. Okay.
ones. Okay.
Then the
Then the
gradient there's also the gradient
gradient there's also the gradient
clipping. And then the value function
clipping. And then the value function
clipping
is this one's weird,
is this one's weird,
right? Yeah. This one is just like
right? Yeah. This one is just like
directly in
um you're literally just plus minus a
um you're literally just plus minus a
constant on the loss. That's
crazy. Oh no, it's clipping the value
crazy. Oh no, it's clipping the value
loss to plus
loss to plus
minus this constant.
Yeah.
Okay. Does this thing get
Okay. Does this thing get
normmed? This doesn't get normed at
all. Probably should be normed.
So, where do we go from here?
Um, the fact that this has like such a
Um, the fact that this has like such a
wide
window. Kale's kind of bad at the same
window. Kale's kind of bad at the same
time, isn't
time, isn't
it? Kale's like kind of bad.
it? Kale's like kind of bad.
Let's go back to the old replay
buffer. Set this to
one. Set this to one and see what
one. Set this to one and see what
happens.
So this is now potentially like
So this is now potentially like
massively off policy.
value loss is super
Bye.
Bye.
Okay, let me try one thing.
So now we define this mass. us.
I mess up.
Try
that. Seriously, it's still now.
So now this is not training the value
So now this is not training the value
loss on stale data but still training
loss on stale data but still training
the policy.
Okay. Looks like we still get
stuck. Oh, we still get stuck.
Value still blows
up. Maybe marginally
better. Okay.
better. Okay.
So, let's not do
that. Let's look at B
trades. Okay. So, this this toilet paper
trades. Okay. So, this this toilet paper
is also what introduces
is also what introduces
Brays. They have an ablation within
Brays. They have an ablation within
without
BRAS. Why I read this
thing? Yeah, there's a big difference
thing? Yeah, there's a big difference
here. Big difference.
Uh, and you see with the replays, but
Uh, and you see with the replays, but
this is what gives them access to this
this is what gives them access to this
replay buffer. So, this is kind of
replay buffer. So, this is kind of
exactly what we
want. The Vray
want. The Vray
uh targets are defined by
one. Okay. over
here. All policy learning is important
here. All policy learning is important
in the decoupled distributed actor
in the decoupled distributed actor
learner architecture because the lag
learner architecture because the lag
between actions are generated by the
between actions are generated by the
actors and when the learner estimates
actors and when the learner estimates
the gradient. Yes.
Consider a
trajectory X
trajectory X
a reward from S. So this is perfect.
a reward from S. So this is perfect.
This is exactly what I have.
We define nstep v trace target for the
We define nstep v trace target for the
value
function sum over the trajectory
function sum over the trajectory
segment
segment
gamma some crazy
gamma some crazy
product the
TV. Okay, this is I think the thing that
TV. Okay, this is I think the thing that
is worth investing into right now.
is worth investing into right now.
This is a thing that's worth investing
This is a thing that's worth investing
into.
We also should open up. Let me switch
We also should open up. Let me switch
the
the
view cuz we need
view cuz we need
to This is going to get
heavy. Okay, so here's our current
heavy. Okay, so here's our current
target from generalized advantage
target from generalized advantage
estimation.
some
some
gamma
gamma
delta. They have this product and then
delta. They have this product and then
they have delta and then they have
V. Delta TV is
PT reward
plus. Okay. So this
here is this not
this Xt + 1 - XT
this Xt + 1 - XT
But this is only a one step,
But this is only a one step,
right? This is literally only a one
step. PT is
min. This is
min. This is
average.
Okay. Truncated important sampling
weights. In the on policy
weights. In the on policy
case when pi equals m
And then you get the on policy endstep
And then you get the on policy endstep
element which is
right
right
here which is lambda equal
here which is lambda equal
one. So this thing
one. So this thing
recovers that
on policy endstep bel
update. I also want to use the same
update. I also want to use the same
algorithm for off and on policy data.
truncated important sampling weights CI
truncated important sampling weights CI
and
and
PT. Well, this is what is this row or
PT. Well, this is what is this row or
something. I actually don't
something. I actually don't
remember.
remember.
Hey, I know that pace weird.
Whatever the
Okay, we don't care about the tabular
Okay, we don't care about the tabular
case.
V trace targets can be computed
V trace targets can be computed
recursively.
Did this one come with source
Did this one come with source
code? Yes, this one
did. Though it's probably in TensorFlow.
Oh, they gave
us
us
this. Jeez. Okay, this is actually it's
this. Jeez. Okay, this is actually it's
row. That's what it is. It was
row. That's what it is. It was
row.
Um, they have this and they have a test
Um, they have this and they have a test
for it because this is a very important
for it because this is a very important
thing to get right.
Let me just uh briefly look up if
Let me just uh briefly look up if
there's been any successor to be trace.
basic
policy. People were talking about this,
right? Oh, yeah. This is Showman's
right? Oh, yeah. This is Showman's
follow up to his own stuff.
Oh yeah, this has the weird thing where
Oh yeah, this has the weird thing where
um
So this thing did better for
Choosing the optimal level of sample
Choosing the optimal level of sample
reuse is not straightforward.
value policy and value function trainer
value policy and value function trainer
decoupled. We can train with different
decoupled. We can train with different
levels of sample
levels of sample
reuse. In order to better understand the
reuse. In order to better understand the
impact, we chose to vary the number of
impact, we chose to vary the number of
policy epochs without changing the
policy epochs without changing the
number of value function epochs. That's
number of value function epochs. That's
actually very similar to what I just
actually very similar to what I just
tried to
tried to
do. Get out of here, bot.
do. Get out of here, bot.
I build the bots around
I build the bots around
here.
here.
Um, single policy epoch is almost always
Um, single policy epoch is almost always
optimal or near optimal in
optimal or near optimal in
ppg. This suggests that the PO
ppg. This suggests that the PO
baseline benefits from greater sample
baseline benefits from greater sample
views only because the experts offer
views only because the experts offer
additional value function training.
But isn't the value function data what
But isn't the value function data what
gets off
policy? I mean it is also the supervised
policy? I mean it is also the supervised
one that you can train perfectly, right?
So that doesn't suggest that uh you can
So that doesn't suggest that uh you can
use that you can reuse samples, right?
Oh,
Oh,
okay. These are literally
okay. These are literally
inverted. Okay.
Man, I love when I just have, you know,
Man, I love when I just have, you know,
a whole day to just get as deep into the
a whole day to just get as deep into the
literature as I need to to try to
literature as I need to to try to
understand stuff. This is good.
Too many
Too many
epochs runs the risk of overfitting to
epochs runs the risk of overfitting to
recent
recent
data. Not with a giant batch. Fewer
data. Not with a giant batch. Fewer
epochs will lead to slower training.
epochs will lead to slower training.
1 to
n training with auxiliary epox is
n training with auxiliary epox is
generally
generally
beneficial up to
six. We may expect better train features
six. We may expect better train features
to be shared with the policy or train a
to be shared with the policy or train a
more accurate value function.
The feature sharing appears to play the
The feature sharing appears to play the
more critical
more critical
role. Auxiliary phase
frequency. We next we investigate
frequency. We next we investigate
alternating between policy and auxiliary
alternating between policy and auxiliary
phases at different frequencies.
phases at different frequencies.
We perform each auxiliary phase after
We perform each auxiliary phase after
end policy
updates. Performance suffers when we
updates. Performance suffers when we
perform auxiliary phases too
frequently. As an alternative to
frequently. As an alternative to
clipping that's
clipping that's
me this guy's own paper.
me this guy's own paper.
proposed using an adaptively weighted KL
proposed using an adaptively weighted KL
penalty. We now investigate the use of
penalty. We now investigate the use of
the K penalty in BPG. Instead, choose to
the K penalty in BPG. Instead, choose to
keep the relative weight of this penalty
fixed.
Okay. Fixed KL performs remarkably
Okay. Fixed KL performs remarkably
similarly to clipping when using PPG.
similarly to clipping when using PPG.
Clipping is more important when using
Clipping is more important when using
when rewards are poorly skilled. Great.
when rewards are poorly skilled. Great.
I don't have to do a bunch of math.
Lovely. They want to keep the tail
Lovely. They want to keep the tail
penalty for the theoretical insights
penalty for the theoretical insights
even though it's a pain in the ass. It's
even though it's a pain in the ass. It's
funny.
default implementation of
default implementation of
PPG a single network
PPG a single network
variant that mimics the same training
variant that mimics the same training
dynamics PO for
dynamics PO for
reference PPO PPG PO single they're very
similar ppg comes with an increased
similar ppg comes with an increased
memory footprint since we use disjoint
memory footprint since we use disjoint
policy and value function networks
policy and value function networks
instead of a single unified network we
instead of a single unified network we
can use we use approximately twice as
can use we use approximately twice as
many parameters
many parameters
We can recover this f by using a single
We can recover this f by using a single
network that appropriately detaches the
network that appropriately detaches the
value function gradient.
What during the policy
phase we detach the value function
phase we detach the value function
gradient at the last layer
gradient at the last layer
shared uh preventing the value function
shared uh preventing the value function
from influencing shared parameters.
from influencing shared parameters.
In the auxiliary
In the auxiliary
phase, the value function gradient with
phase, the value function gradient with
respect to all
parameters.
parameters.
Okay. I I think they did some whatever
Okay. I I think they did some whatever
that's
fine. Work
fine. Work
conclusion. Very good.
Do they have algorithm pseudo code
Do they have algorithm pseudo code
somewhere? Yes, they
somewhere? Yes, they
do. Initialize empty
do. Initialize empty
buffer. Perform
rollouts. What's
rollouts. What's
var value function
var value function
target? I missed
this and var computed with generalized
this and var computed with generalized
advantage estimation.
same objective PO. So this is just PO
right. So perform allows compute value
right. So perform allows compute value
function
function
optimize the policy for some
optimize the policy for some
box. Optimize the value for some
epox, right?
Wait. And then there's L joint and L
Wait. And then there's L joint and L
value. Okay. I don't think I
value. Okay. I don't think I
uh I fully understood what the heck
uh I fully understood what the heck
they're doing here then.
We optimize the policy network with a
We optimize the policy network with a
joint
joint
objective that includes an arbitrary
objective that includes an arbitrary
auxiliary
loss and a behavioral cloning
loss. What?
Lox. Hold
Lox. Hold
on. You don't need this.
Wait, let me try the
Wait, let me try the
single. What's the single
one? By default, PPG comes with an
one? By default, PPG comes with an
increased memory
increased memory
footprint. Since we use disjoint policy
footprint. Since we use disjoint policy
and value function networks instead of a
and value function networks instead of a
single unified policy, we can use
single unified policy, we can use
approximately twice as many parameters
approximately twice as many parameters
compared to
compared to
PO. Recover this cost by using a single
PO. Recover this cost by using a single
network that appropriately detaches the
network that appropriately detaches the
value function gradient. During the
value function gradient. During the
policy phase, we detach the value
policy phase, we detach the value
function gradient at the last layer
function gradient at the last layer
share between the policy and the value
heads. Preserving the value function
heads. Preserving the value function
gradient from influencing shared
gradient from influencing shared
parameters.
During the auxiliary
phase, we take the value function
phase, we take the value function
gradient with respect to all
parameters.
parameters.
Huh. Okay. So, that's not
Huh. Okay. So, that's not
quite. But then what does the auxiliary
quite. But then what does the auxiliary
phase
do? Hang
do? Hang
on. So they just do Where's the pseudo
code? Optimize the clipped policy loss.
Optimize the value
loss.
Optimize L
Optimize L
joint. Optimize L
value. Okay. I don't understand this
value. Okay. I don't understand this
thing.
perform
perform
rollouts under current
policy. Clean RL has this, doesn't it?
Clean
our thought they had ppg in here, didn't
our thought they had ppg in here, didn't
they?
they?
PPG pro
jump policy
jump policy
phase. So they collect
phase. So they collect
rollouts, right?
Optimize policy and value
Optimize policy and value
network. There's the policy network
one. Uh, and they do these at the same
one. Uh, and they do these at the same
time because they're separate networks.
Okay. But then there's an auxiliary
Okay. But then there's an auxiliary
phase policy.
need to understand this and need to
need to understand this and need to
understand B trace today. Those are the
understand B trace today. Those are the
two
goals. What's this B as what's this
goals. What's this B as what's this
extra thing?
That's just
That's just
entropy. So in the first
entropy. So in the first
phase, this is just do PO,
phase, this is just do PO,
right? Yeah. This is literally just do
PO with a stop Brad potentially if you
PO with a stop Brad potentially if you
use a shared
network. Okay, so that's super
network. Okay, so that's super
easy and this is not even slower.
easy and this is not even slower.
This is not even slower than PO
This is not even slower than PO
providing you use the shared
providing you use the shared
formula. But then there's this E
O where you
O where you
optimize L
joint and L value.
So what is
So what is
Ljint? We optimize the policy that has
Ljint? We optimize the policy that has
an auxiliary loss and a behavioral
an auxiliary loss and a behavioral
cloning
cloning
loss. So this is the value function
loss. So this is the value function
loss.
Okay. So in the shared version, the
Okay. So in the shared version, the
shared policy version, this is
shared policy version, this is
uh this is literally
uh this is literally
just train the value
just train the value
loss. And then this thing
here, behavioral cloning loss.
KL. This isn't just a KL
term policy
old. Oh. Oh, okay. I understand this.
old. Oh. Oh, okay. I understand this.
So, they're trying to keep this thing
So, they're trying to keep this thing
outputting the exact same actions
outputting the exact same actions
um
um
while training it to match the value
target. I
see that's
see that's
clever. Let me think why that makes
clever. Let me think why that makes
sense though to do it this way.
Let me make sure I got
Let me make sure I got
the this this is
the this this is
V theta
V theta
V. Yeah, this is theta V.
Do you even have to do this? I wonder
Do you even have to do this? I wonder
with the shared formula.
Like this is arguably not even that hard
Like this is arguably not even that hard
to do, right?
This seems orthogonal to be traced
though. Now that I understand this, let
though. Now that I understand this, let
me go back through the experiments
me go back through the experiments
again.
Where's figure
three? We chose to vary the number of
three? We chose to vary the number of
policy epochs without changing the
policy epochs without changing the
number of value function
number of value function
epochs. Results are shown in figure
epochs. Results are shown in figure
three.
Holy, that's
wild. So that's literally just train the
wild. So that's literally just train the
policy
less. Okay.
and then they get literally the opposite
and then they get literally the opposite
with the value function, right?
lagging value function.
Then here is the auxiliary phase
Then here is the auxiliary phase
frequency.
additional epochs during the auxiliary
phase. So, you don't actually have to do
phase. So, you don't actually have to do
this very
often. Wait, now wait, we have
often. Wait, now wait, we have
additional epochs during the auxiliary
additional epochs during the auxiliary
phase.
Oh no, this is the other way around.
Okay, this is the frequency.
E
ox. Hang on. This is e
ox. Hang on. This is e
pi. E
ox. So, they didn't do
ox. So, they didn't do
this. I thought they did do this
this. I thought they did do this
one. So, this one is
one. So, this one is
EI. No, this is not EB. This is Es
and then the
frequency. So, this is not that bad then
frequency. So, this is not that bad then
because you do have to run some extra
because you do have to run some extra
epoch, but you don't do it that often.
This thing
is probably pretty awful though
is probably pretty awful though
for M
for M
speed. You can't really do anything on
speed. You can't really do anything on
the end while you're optimizing that.
the end while you're optimizing that.
That's okay. Okay. So, I understand this
That's okay. Okay. So, I understand this
thing. I think this is going to have no
thing. I think this is going to have no
bearing whatsoever on uh V
trace. So, this is something to
trace. So, this is something to
investigate for
sure. This seems like a
um pretty strict generalization of PO.
Okay, so that's one. Now we go to B
trace, not this one where it said B
trace. And now we try to understand this
trace. And now we try to understand this
V trade Target.
value at
state. This product
S to tus
S to tus
one sum i=
one sum i=
s to t yeah so this is fine this is not
s to t yeah so this is fine this is not
this is actually easy to
this is actually easy to
compute then
ci i over
ci i over
mu okay and then this delta
is just TD.
They have a Q function in here.
see it in here.
21 billion frames per day, 250k
21 billion frames per day, 250k
fps
on 500
CPUs. Oh, and then that's you actually
CPUs. Oh, and then that's you actually
have to divide by four.
Yeah. So, this is actually 60K. Yeah,
Yeah. So, this is actually 60K. Yeah,
that's about what you can do with a
GPU. That's
GPU. That's
funny. That's actually pretty
funny. That's actually pretty
disappointing
disappointing
for looking at what Puffer does these
days because I can give you half of that
days because I can give you half of that
just like out of the box.
uh with like what 16
CPUs.
CPUs.
So okay in order to implement this thing
Let me go check my current
implementation. So that's the current uh
implementation. So that's the current uh
No, it's in
No, it's in
shared. Okay. So J is just
shared. Okay. So J is just
reward plus gamma value minus it's this
reward plus gamma value minus it's this
right this okay this is all that this
is delta plus
I'm covering the screen. Hang on. We'll
I'm covering the screen. Hang on. We'll
do this for
now. If you get rid of this pair,
Then you can literally just do this,
Then you can literally just do this,
right? Yeah, it's just the sum of
right? Yeah, it's just the sum of
these. And then you don't even have the
these. And then you don't even have the
lambda
lambda
term, right?
So really I just need this row term and
So really I just need this row term and
this C
this C
term. So let me see if I can understand
term. So let me see if I can understand
these.
these.
Row and C are truncated important
Row and C are truncated important
sampling
sampling
weights. Where do they
define? So MOU is the original policy.
define? So MOU is the original policy.
So this is pile
What is this?
Min P bar.
Right out the
B P bar as a truncation
B P bar as a truncation
threshold. Oh, is that a constant?
That might just be a constant,
right? That might literally just be a
right? That might literally just be a
constant.
If that's just a constant, this isn't
If that's just a constant, this isn't
that hard to implement.
Where do they get a par bar
though? Are these just parameters that
though? Are these just parameters that
are going to be listed somewhere for us?
Hypers. Any hypers?
Are those not just fixed diapers? Cuz I
Are those not just fixed diapers? Cuz I
don't see them
don't see them
anywhere. Just count.
You
Oh,
one. Okay. So, if that's correct, those
one. Okay. So, if that's correct, those
are literally just one.
Okay. Now, let's go back one more time
Okay. Now, let's go back one more time
and see if I understand why this is the
and see if I understand why this is the
way it is. So, this
is there's no lambda in
is there's no lambda in
here. Let's ignore lambda. So,
discounted. So, we're just doing
discounted. So, we're just doing
Bellman. Okay. No exponential smoothing
Bellman. Okay. No exponential smoothing
with
lambda value plus. So if we ignore this
lambda value plus. So if we ignore this
piece here and this clipping then this
piece here and this clipping then this
gives us Bellman which is this right
gives us Bellman which is this right
here and
here and
Bellman. So what we're saying
here let's do this one first. This one's
here let's do this one first. This one's
easier.
Ah, so we are
clipping we're clipping the
advantage or rather we're scaling the
advantage or rather we're scaling the
advantage based on important
advantage based on important
sampling instead of scaling
sampling instead of scaling
the policy gradient loss directly.
Oh, that also scales the value function
Oh, that also scales the value function
target, doesn't it?
Important sampling is embedded in the B
Important sampling is embedded in the B
trace
target. Corrected value estimate
target. Corrected value estimate
indirectly influences advantage
computation by correcting value function
computation by correcting value function
B trace advantage accounts policy
B trace advantage accounts policy
mismatch is not directly applied to the
mismatch is not directly applied to the
advantage itself. What's the value
advantage itself. What's the value
function that is
adjusted? Uh, what this is? Is this the
adjusted? Uh, what this is? Is this the
advantage function they
advantage function they
use? That seems weird. I got to check
that advantage and value
loss. Clipped objectives with policy
loss. Clipped objectives with policy
loss.
loss.
typically unclipped or L2 loss value
typically unclipped or L2 loss value
function.
function.
Now there is important sampling right.
Now there is important sampling right.
The clip is on
The clip is on
um a logic
ratio on truncated important weights.
Okay. So, you might not need a lambda
print cuz lambda pushes you
towards towards one step, right? If you
towards towards one step, right? If you
add Yeah, because lambda is one default.
add Yeah, because lambda is one default.
So yeah, this just pushes you away.
So yeah, this just pushes you away.
Lambda just pushes you towards
Lambda just pushes you towards
this which is really easy to optimize
this which is really easy to optimize
but this is like a onestep
but this is like a onestep
bootstrap. So this is hard to optimize
bootstrap. So this is hard to optimize
but is correct, right? More
but is correct, right? More
correct. This is the end step or the
correct. This is the end step or the
infinity step. So you might not even
infinity step. So you might not even
need a lambda term.
Okay. It is not It is not more common to
Okay. It is not It is not more common to
see unclipped value loss in PO. That is
see unclipped value loss in PO. That is
not
true. The value function.
true. The value function.
Now, let me just do what they use for
Now, let me just do what they use for
the advantage function here
the advantage function here
because I'm a little suspicious
because I'm a little suspicious
now. So, here's your V trace
now. So, here's your V trace
target, which is what you train the
target, which is what you train the
value function against. And where's your
value function against. And where's your
advantage function?
Just make sure I understand this as
Just make sure I understand this as
well.
Okay, hang on
here. So, this is just the value
here. So, this is just the value
target. Did they give us the advantage?
gradient of the value
function. Whatever
Oh,
what? That's all they
do. Hang on. This is the V trace target,
do. Hang on. This is the V trace target,
right?
Oh, wait. No, this is the same. Isn't
Oh, wait. No, this is the same. Isn't
this the
this the
same?
same?
Because
advantage you
advantage you
add BST to this, right?
You do V st plus
You do V st plus
this which if it's equivalent
this which if it's equivalent
here R + lambda V S T + 1
That's the onestep version,
right? No, it's not because this
right? No, it's not because this
[Music]
[Music]
is reward
is reward
plus
advantage. Yeah. Okay, that's still
advantage. Yeah. Okay, that's still
fine. That's the
fine. That's the
same. So, this is actually a very sort
same. So, this is actually a very sort
of
of
similar
method. What did they benchmark this
method. What did they benchmark this
against?
What is there? No
correction. Is no correction generalized
correction. Is no correction generalized
advantage estimation or no?
No off policy correction. Multiply
No off policy correction. Multiply
advantages by corresponding importance
weight. That gives you very similar
weight. That gives you very similar
performance.
Without replay, it gives you very, very
Without replay, it gives you very, very
similar performance. With replay,
similar performance. With replay,
there's a bit more of a
gap. There's no benchmark of betray
gap. There's no benchmark of betray
generalized advantage estimation.
Probably worth implementing this thing
Probably worth implementing this thing
still.
I say we start on this.
This thing needs values.
This thing needs values.
Rewards
done. This trace
And then this also
needs
importance. And then we need a float.
row load
C. And this is going to be fairly
C. And this is going to be fairly
similar to
J. Let me see this double loop real
J. Let me see this double loop real
quick.
This is t goes from
s to t minus
one. So this product is pretty easy to
one. So this product is pretty easy to
compute, isn't
compute, isn't
it? It's just one term
it? It's just one term
per Okay.
So we do
float C
product. Does this one go backwards?
Wait, does this one go
back? This one can't be computed
back? This one can't be computed
backwards,
right? That's a little bit annoying.
Wait, this is a t and this is a
Wait, this is a t and this is a
i. But this is
i. But this is
ct. So this is the same thing.
Oh, you missed Horizon.
has to be one to
start broad.
This is
clip something like this I believe.
min
min
max
importance you do fraud times equals
that's or actually we can do it the
that's or actually we can do it the
other way
other way
right so we do
right so we do
prod times equals
this and then this
Now you have all your products.
Okay. So, you do
this. You only need this component,
this. You only need this component,
right?
You don't need this extra exponential
smoothing.
Wait, this is rewards of T
Wait, this is rewards of T
next. Is that
right? That seems weird.
Is that a bug in GE? In my
GE. I think that might be a bug in my J.
This index and the negative are the
This index and the negative are the
same. The
values Yeah, that looks like a bug to
values Yeah, that looks like a bug to
me.
We will play with that.
Okay. So cool. I think we can just start
Okay. So cool. I think we can just start
implementing this portion
now to be so careful with
this. We'll start with this delta.
this. We'll start with this delta.
So this delta is just equal to reward at
So this delta is just equal to reward at
I'm going to make this
I'm going to make this
t
values gamma value t
next minus values. Okay, so this delta
next minus values. Okay, so this delta
is fine.
This is just a
This is just a
backwards sum,
right? So I can just do like float
term equals z.
like him.
This is not a T. Next. This is a T,
This is not a T. Next. This is a T,
right? This is
right? This is
T. It's
delta. Okay.
Next non-
terminal that starts out at zero.
So we do accumul
plus gamma
time next non-
terminal times a
terminal times a
cube and then trace of t is equal to a
cube. I think that's it except I missed
cube. I think that's it except I missed
the
the
What's this? The road
What's this? The road
clip. Ah, shoot. You missed the road
clip. Ah, shoot. You missed the road
clip. You click twice like that.
This is not it's not uh this is
This is not it's not uh this is
important
here. Let's just do
row sub
T. Yeah. So this is it here. So this is
T. Yeah. So this is it here. So this is
the clipped row and then this is row
the clipped row and then this is row
clip
delta. And then we can do this one in
delta. And then we can do this one in
line I think.
line I think.
So this one should
be Yeah. So this is
This has to go
This has to go
here. And then this is actually
row
subt. I want to keep it the math as I
subt. I want to keep it the math as I
possibly
can. And then this is
Okay, let me think about this.
is there no value function bootstrap on
is there no value function bootstrap on
this
Oh, there is
Oh, there is
right. Yeah, there
is. So, it's going to be delta Q is zero
is. So, it's going to be delta Q is zero
at the first time. So, cro delta plus
at the first time. So, cro delta plus
gamma times
gamma times
that I think that's correct.
Let's see if I can get this thing to
compile. I'm gonna actually move this.
compile. I'm gonna actually move this.
I'm going to move
I'm going to move
this. I move this
down. Move this
down. I mean, this should basically be
down. I mean, this should basically be
autocompletable. It's so basic, right?
autocompletable. It's so basic, right?
Check on this board is done is
important
steps trace
steps trace
row clip. Yeah, there we
row clip. Yeah, there we
go. So then we just do
Rewards values done importance and trait
Rewards values done importance and trait
should all have the same
should all have the same
dimensions should be two dimensions on
dimensions should be two dimensions on
the same
the same
device in 432 will
not oh you don't pass in
trace my bad you don't pass in phrase
Right. Now we
Right. Now we
do
phrase this.
That's the shared
That's the shared
shared
shared
file. Now we need the
file. Now we need the
bindings. And this is actually, believe
bindings. And this is actually, believe
it or not, this is going to be the C++
it or not, this is going to be the C++
and the CUDA at the same
and the CUDA at the same
time. It' be very
time. It' be very
nice. We'll do the C++ bind first.
V
V
trace values rewards done
trace values rewards done
importance advantages clip clip thumbs
horizon plus horizon beach
horizon plus horizon beach
row importance plus
offset.
offset.
Okay plus offset row
Okay plus offset row
clip horizon. You missed gamma
clip horizon. You missed gamma
completely man. What the heck is wrong
completely man. What the heck is wrong
with you?
And you use it here too, don't
And you use it here too, don't
you? Gamma, what's wrong with
you? Gamma, what's wrong with
you?
Lo, this doesn't need to get checked
Lo, this doesn't need to get checked
though,
though,
right? Like the uh the check on this
right? Like the uh the check on this
doesn't
doesn't
include any of this
crap. You just need this, man.
Just one extra tensor and you're
good. Okay. Now, the actual function
good. Okay. Now, the actual function
here needs
here needs
everything
everything
trace
gamma. All right. And then you do B
gamma. All right. And then you do B
trace
gamma upper lip C.
Is this the right
Is this the right
one? Yeah, I think this is the right
one? Yeah, I think this is the right
one. So, we
need GA AE kernel I
believe. Okay. And then there's compute
believe. Okay. And then there's compute
GA.
GA.
Just do
this
this
trace kernel which takes values rewards
done
done
importance
importance
trace gamma
row clip.
row clip.
Yep. All
Yep. All
that and then it's just V trace with the
that and then it's just V trace with the
new RS.
And then that will give us our
And then that will give us our
kernel. We still have to bind this
kernel. We still have to bind this
thing. So let's actually put this down
here. I don't know why I did this this
here. I don't know why I did this this
way. This should
way. This should
go. What is this thing? Yeah, I don't
go. What is this thing? Yeah, I don't
know why I did it this way. This needs
know why I did it this way. This needs
to go down here.
You go
there. All right. Base is kernel
there. All right. Base is kernel
now just needs a
now just needs a
binding which passes
in everything
in everything
except perfect.
except perfect.
Uh, that might have just one shot it
Uh, that might have just one shot it
because this is super basic. Let's see.
because this is super basic. Let's see.
BR
check.
Yeah.
Yeah.
GMA looks good to
GMA looks good to
me. Just a bunch of boiler
plate. And then we just have to
plate. And then we just have to
export QV trays.
race.
Cool. BR is false.
These are both
one. We do
LF config use feed phrase. We beat
phrase. You miss
gamma. You do need gamma still.
Did uh we get that in the
signature? Yeah, we did get that in the
signature.
Okay. Well, you don't give it
Okay. Well, you don't give it
experience. importance,
right? Yeah. So, this is like
right? Yeah. So, this is like
um what you need to do this needs to be
um what you need to do this needs to be
important and then we have to do
How do we do this?
We need
We need
um old log props and new log props,
um old log props and new log props,
right?
Does this thing prevent you from doing a
Does this thing prevent you from doing a
prioritized
replay? Cuz wait, you need to have
This says that you need to have the
This says that you need to have the
importance
importance
ratio before you compute advantage,
right? How does prioritized experience
right? How does prioritized experience
replay work?
Maybe we can just use something else to
Maybe we can just use something else to
sample it with and then we don't need
sample it with and then we don't need
advantages.
Uh I don't have I don't have mental
Uh I don't have I don't have mental
bandwidth to read another full paper
bandwidth to read another full paper
here right yet. Hang
here right yet. Hang
on. Let me just uh think what we can do
on. Let me just uh think what we can do
in the meantime.
So, oh, I see what we would
So, oh, I see what we would
do. So, we would just do
Vantages equals
ports. Okay. So then we would sample our
back. So this is now
back. So this is now
uniform, right?
Then we
Then we
do if config use V
trace we need to
trace we need to
get we need to compute J for this
get we need to compute J for this
B. So this is going to be B
rewards from the
rewards from the
gamma. Okay. And then we do
gamma. Okay. And then we do
importance
equals is it this ratio right here?
ratio, right?
ratio, right?
That is the important
sample. Oh, I totally messed this up.
sample. Oh, I totally messed this up.
Hang
Hang
on. I just Yeah, I know how to do this
on. I just Yeah, I know how to do this
now.
Okay. So, we do
Okay. So, we do
this manages four
once we go down to
once we go down to
here. We say
All
right. And then this importance is just
ratio.
ratio.
Okay.
Advantage.
Advantage.
Advantage. Or what's
Advantage. Or what's
this? This is not advantage. This is V
this? This is not advantage. This is V
trace,
right?
Var. And what's the V trade paper make
Var. And what's the V trade paper make
me use for the act
me use for the act
policy? What they
policy? What they
use? So this gives you VS.
Okay. Value function is easy,
right? Yeah. Value function
right? Yeah. Value function
is easy. This is just batch returns,
is easy. This is just batch returns,
right?
batch. So that's an easy target. Now
batch. So that's an easy target. Now
what do you do for the only thing I
what do you do for the only thing I
don't know is the
policy. Policy it says you do R
Why is it dst + one? Is that a thing?
Let me just do something to start.
Oh yeah, this log ratio is
Oh yeah, this log ratio is
uh yeah going to be completely
uh yeah going to be completely
completely different,
completely different,
right? So it's going to be
right? So it's going to be
um PG
log new log crawl.
times back
dot gamma batch returns minus batch
dot gamma batch returns minus batch
values. I think this is
it. I think that is it.
it. I think that is it.
And then we have to
say
Mike next
step. All
right. Zero chance that this runs on the
right. Zero chance that this runs on the
first
try. Oh, I also unless I forget to
try. Oh, I also unless I forget to
enable it.
Ah, this is a bad
Ah, this is a bad
ting. There's a bug.
there. Let's see what that
there. Let's see what that
builds. All right. How many bugs we
have? See flip real quick.
Uh, you just straight up forgot to add
Uh, you just straight up forgot to add
this. Yeah, that was
stupid. Gamma lambda.
Not bad though. That will immediately
Not bad though. That will immediately
have a CUDA kernel for
have a CUDA kernel for
it. So if it works, it'll also
it. So if it works, it'll also
immediately be fast.
Can I just do con? Can I like
Can I just do con? Can I like
const this
[Music]
post redefined.
cannot be used.
I just have to get it so that the thing
I just have to get it so that the thing
will let me use this
will let me use this
um for a static array.
Okay. Expression must have a constant
Okay. Expression must have a constant
value.
Guess I can't do
Guess I can't do
that. That's very obnoxious.
Why did I need to do that
Why did I need to do that
again? I absolutely didn't need to do
again? I absolutely didn't need to do
that,
that,
right? No, I do need it
because Oh, that's super obnoxious.
Hang on. What's this define?
Trace
Trace
trace. Is that
it? Come on. It's const all the way
it? Come on. It's const all the way
down.
typing stupid CS 101 to
GR is
I mean,
I can't mal
it. That's a total pain in the ass.
So I I think I can do
something like Yes.
All right, that actually
compiles.
compiles.
No attribute compute vx. Yeah.
This is probably just signature,
right?
right?
Okay. One, two, three, four. Four time
Okay. One, two, three, four. Four time
serves. Three, four, four
serves. Three, four, four
times
times
gamma v trace
gamma v trace
clip. So float
clip. So float
float float in two
float float in two
ins. Uh should not be two
ins trays. You do not need to
ins trays. You do not need to
pass num steps for horizon
pass num steps for horizon
here. These are
computed
computed
steps. That's what you get for
steps. That's what you get for
uh
autofilling. There you go.
use the restroom. I'll be right back.
[Music]
Okay. What did I pass
Okay. What did I pass
wrong? 1D important
trade. Yep.
trade. Yep.
Yep, that's it.
Red cannot
be
graded. Okay. Interesting.
Oh,
dummy. And this is a mismatch as
dummy. And this is a mismatch as
well. Yeah.
shape. Do some shape
shape. Do some shape
fixing. I just want to get this to
run. Oh joy.
CPU fall back
maybe computer v.
Compute V trace was not
Compute V trace was not
declared but yeah I should declare it.
declared but yeah I should declare it.
All
All
right. Done.
right. Done.
Importance.
Importance.
Uh,
Uh,
gamma
gamma
eclipse race
eclipse race
sets done importance
trace
steps. So much binding code, man.
So much stupid binding
code. I mean, it runs.
We've got
We've got
um I'm assuming I'm not timing this
um I'm assuming I'm not timing this
thing correctly.
That's really freaking slow for some
That's really freaking slow for some
reason,
but doesn't train either.
The CUDA one is somehow out of memory.
The CUDA one is somehow out of memory.
Cuda one somehow legal access.
Really? It's the same bloody code. So,
Really? It's the same bloody code. So,
how do you figure
that work the first time?
So eventually this thing uh hurts
somehow and if I don't do this I
somehow and if I don't do this I
still error
No. Then we
train against who knows what we're
train against who knows what we're
training
training
against. So it is this
against. So it is this
thing values rewards
thing values rewards
done
ratio. I need to do this.
No, doesn't
help. Peace.
anything out of bounds. Looking
We got Max right
Yeah, I don't see anything that would be
Yeah, I don't see anything that would be
wrong
there.
Um, retrace
Um, retrace
kernel. Oh, well, this is really stupid,
kernel. Oh, well, this is really stupid,
right? Or no, this is fine actually.
right? Or no, this is fine actually.
J
J
kernel times
kernel times
horizon values
offer camera
and does this use steps.
Use no
steps. Why is not longer than 256,
steps. Why is not longer than 256,
right?
I'm pretty sure it's not
Okay, not
that device stuff.
I mean, we should probably just look at
I mean, we should probably just look at
this very
carefully. Gamma clip. Right. So, we're
carefully. Gamma clip. Right. So, we're
passing these all in the correct order,
passing these all in the correct order,
I'd assume, right?
Num steps
horizon. So then horizon goes to beach
horizon. So then horizon goes to beach
trace
trace
kernel and it goes to beach row
gamma beach
gamma beach
row horizon.
Oh, what the heck is
Oh, what the heck is
this? How' that
this? How' that
happen? Hey, I don't know how that
happened. See if that does it.
No, still found legal
No, still found legal
memory.
Really? Let's see.
Boston horizon.
not recompiling it or
not recompiling it or
something. Should be
Let me clear the cach to be
sure. That's
sure. That's
bizarre. It runs on the CPU. It doesn't
bizarre. It runs on the CPU. It doesn't
run on the GPU.
Why do I think it's going to end up
Why do I think it's going to end up
being something stupid that's like not
being something stupid that's like not
even
even
here? Because I like I don't see
here? Because I like I don't see
anything here,
anything here,
right? You have this buffer. This buffer
right? You have this buffer. This buffer
is always big
is always big
enough. Starts out at one. You go from
enough. Starts out at one. You go from
zero up to
zero up to
horizon which definitely fits in
horizon which definitely fits in
there. Importance is also the same size.
there. Importance is also the same size.
This the search
This the search
checked,
right? This goes to -2.
So this is one which is
fine
t and rewards at
t and rewards at
t values t
next
next
t horizon minus two to
t horizon minus two to
zero still fine.
Race of T. Yeah, I don't see anything
Race of T. Yeah, I don't see anything
wrong with this
wrong with this
thing gives you numbum steps by horizon.
thing gives you numbum steps by horizon.
You've assert
You've assert
checked all of
checked all of
these,
right? Does this give us anything?
That's not going to recompile.
blind debugging is just such a stupid
thing. Yeah, it's just going to give me
thing. Yeah, it's just going to give me
illegal memory address.
There's nothing useful in
There's nothing useful in
here. It's all asynchronous. So, you're
here. It's all asynchronous. So, you're
just screwed.
Does the synchronize even happen?
Let's just make sure there's not
Let's just make sure there's not
something like
ridiculous. We still get a legal memory.
See, I don't understand how it actually
See, I don't understand how it actually
doesn't
happen. Like, we don't actually even hit
happen. Like, we don't actually even hit
it here,
it here,
right? We hit it in one of the freaking
right? We hit it in one of the freaking
profilers. You would think we should hit
profilers. You would think we should hit
it here,
right? Like, how am I not hitting it
right? Like, how am I not hitting it
there?
Are we hitting it in the sample or
something? Like where are we heading
something? Like where are we heading
this?
Samples with
advantages. So, this doesn't get called
advantages. So, this doesn't get called
at all.
at all.
Uh, I get I guess it could be this
Uh, I get I guess it could be this
sample up here, but that would
make that wouldn't make any sense at all
make that wouldn't make any sense at all
to
me. It's still just in a rapper
me. It's still just in a rapper
somewhere.
Okay. What if I do
this? No.
this? No.
So literally somehow because this is not
So literally somehow because this is not
even
even
doing anything
right. I mean I could just
right. I mean I could just
like for the hell of it
clone, right? We could do all
this. No. So, it's literally
like freaking cuda, man.
Dun is float,
Dun is float,
right? I'm sure the Dun is float. I will
right? I'm sure the Dun is float. I will
double
check. We literally pass it. Yeah.
check. We literally pass it. Yeah.
experience.
experience.
Dun just make sure
Yep.
I don't think this is in place,
right? Could be. That could technically
right? Could be. That could technically
be
be
something. Hang
on. But I I don't see how it would be.
on. But I I don't see how it would be.
That doesn't make any sense.
US.
Pretty sure it's not
that. No, it's not that.
How big was the uh the
array? Oh, you idiot. Okay, I know it.
array? Oh, you idiot. Okay, I know it.
I know
I know
it. Damn
it. All right, that's me being dumb. I
it. All right, that's me being dumb. I
should have added a check for that.
Search. Uh, what is it?
Num. Yeah, there you go.
But
this I forgot that uh because I'm
this I forgot that uh because I'm
applying this to sample
applying this to sample
two mini batches
two mini batches
essentially. You can't have as many
threads. So, it was trying to split it.
threads. So, it was trying to split it.
Uh, it didn't have enough data for a
Uh, it didn't have enough data for a
full
block. Yep, there we
block. Yep, there we
go. So
go. So
stupid. All right. Now, we still have
stupid. All right. Now, we still have
the problem of it not
the problem of it not
training, but uh at least now we can
training, but uh at least now we can
debug working
debug working
code. I'm way happier debugging working
code. I'm way happier debugging working
code. We can see the grad variance has
code. We can see the grad variance has
just exploded
catastrophically. That was
catastrophically. That was
obnoxious. That was a silly mistake as
obnoxious. That was a silly mistake as
well.
You would think you would get some kind
You would think you would get some kind
of error for that, but nope, you
of error for that, but nope, you
don't. You just don't. You get out of
don't. You just don't. You get out of
memory, like memory access, whatever.
Okay.
So, the uh the value loss seems pretty
So, the uh the value loss seems pretty
damn easy,
right? Like this seems
right? Like this seems
pretty
basic. Now, this
Potentially this has important sampling
Potentially this has important sampling
on it. I missed this. What's this
row? Important sample,
row? Important sample,
right? So that row is just
ratio. What's the objective?
ratio. What's the objective?
Why don't they write the
Why don't they write the
uh the objective here?
Just want the freaking beach race
Just want the freaking beach race
loss. Where is it?
intensive flow code, man.
Something
here still in
here still in
progress. Literally top is some random
progress. Literally top is some random
repo.
is the
Pro log.
This one's
easy. What's this song?
The hell is this
The hell is this
sum? Where did it get this f this
sum? Where did it get this f this
formula
from? Yeah, that literally makes no
from? Yeah, that literally makes no
sense.
Okay.
Okay.
Um, logic's actions
advantages. So this is
Wait,
Wait,
what? Cross
what? Cross
entropy times advantages.
I like how it's trying to tell me that
I like how it's trying to tell me that
the original in the original
the original in the original
implementation it's this way. When I
implementation it's this way. When I
pasted the original
implementation, I literally pasted that.
Um, assuming that's not like some other
Um, assuming that's not like some other
baseline, right? Doesn't look like it
baseline, right? Doesn't look like it
would be.
B trace returns.pg advantages.
That's it.
According to the paper, that's what it
According to the paper, that's what it
is.
expected.
Is this not the same as
um Hang
on. Is this not just new log prop?
I think we need to
I think we need to
uh to go look at what their V trace
uh to go look at what their V trace
outputs.
Yeah, we need to go look at what their V
Yeah, we need to go look at what their V
trace
trace
outputs. This is not
working. So they literally just
do cross
do cross
entropy
entropy
advantages. And then this
is V trace returns. PG
advantages GG.
Okay. Oh, this is some like crazy scam
Okay. Oh, this is some like crazy scam
thing.
Cool. Says this is their from importance
weights discounts.
They premputee discounts I guess.
B S T +
one. Okay.
[Music]
Words. Turns of
Gamma batch
returns. Add your values.
Not this ratio.
Brad can create only implicitly from
Brad can create only implicitly from
blah blah
blah blah
blah. All right.
really. Okay, you know what? We're going
really. Okay, you know what? We're going
to just
do this is going to be
add whenever this thing logs
Okay. Well, this clearly still doesn't
Okay. Well, this clearly still doesn't
work.
lift PG
Hang
Hang
on. They have like two different things
on. They have like two different things
that this function is returning.
We'll know when we get it because they
We'll know when we get it because they
should at the very least be competitive
should at the very least be competitive
with uh what we have
with uh what we have
before, what we had
before. And it should basically not work
before. And it should basically not work
if it's
wrong. It's so weird how they like
wrong. It's so weird how they like
export
like they've got
VS and
then
concat. Yeah. So they basically they do
concat. Yeah. So they basically they do
one extra bootstrap value at the
end which is such a weird way of doing
end which is such a weird way of doing
it but
whatever but then they apply hang on
whatever but then they apply hang on
it's
it's
clipped PG
rows so I think you need to return two
rows so I think you need to return two
things from this function don't
you? Then you just need to return two
you? Then you just need to return two
things.
clipped PG
clipped PG
rows
rows
equals if clip threshold
rows where's
row the math
Okay, so here are your rows,
Okay, so here are your rows,
right? This is something that we
compute. Yep, this is something that we
compute. Yep, this is something that we
compute.
times. Well, this is just the
times. Well, this is just the
uh this is literally
uh this is literally
just the ratio, isn't it?
I think the easiest thing is just going
I think the easiest thing is just going
to be to put this entire thing into the
to be to put this entire thing into the
kernel.
Right? The thing is you do you miss one
Right? The thing is you do you miss one
at the end with the way that this is
at the end with the way that this is
written. You literally do miss one at
written. You literally do miss one at
the end.
See this clipped PG row is just
See this clipped PG row is just
literally just a
literally just a
clipped clipped row.
There's your
clipping. They do the clipping
clipping. They do the clipping
differently though, don't they?
Why is their clipping like zero to one
Why is their clipping like zero to one
or whatever?
You can't freaking use like I don't even
You can't freaking use like I don't even
know why I open these models when
know why I open these models when
they're wrong in the first
sentence. Like literally just close
sentence. Like literally just close
Twitter. Oh, the models are coming for
Twitter. Oh, the models are coming for
everything. The models
suck. They're all stupid.
This ratio is still what we want though,
This ratio is still what we want though,
isn't
it? Log ratio.
I'm Oh, you can just do max
Okay. So that should give you clipped
Okay. So that should give you clipped
row.
I think that's the
I think that's the
same. And
So it's this advantage gets multiplied
by rewards plus
discount. Discount
discount. Discount
times that returns
Next
Next
minus
values.
So, so I have this
So, so I have this
term, but then they take this against
term, but then they take this against
the soft max.
the soft max.
Maybe I should just do that.
Q with Z
Q with Z
indices which logit with indicates
I'm going to have to find like a good
I'm going to have to find like a good
reference
implementation
logits. Yeah, there's like no way to
logits. Yeah, there's like no way to
freaking do
freaking do
it. We do like pockets.
Effective target size.
I'm just trying to get something to like
I'm just trying to get something to like
match the reference. I think I'm going
match the reference. I think I'm going
to just have to find a reference
to just have to find a reference
implementation cuz the original sucks.
implementation cuz the original sucks.
It's like stupid dated TensorFlow code.
This is not going to magically start
This is not going to magically start
working
right for a second there
right for a second there
maybe, but
no. Okay. Is there an
no. Okay. Is there an
impala Impala pietor
torch mono beast? Uh they actually have
torch mono beast? Uh they actually have
this thing that they use for net hack,
this thing that they use for net hack,
right?
Is the whole trainer in C++ for this
Is the whole trainer in C++ for this
crazy
thing. Okay. So this is similar
Maybe we can do something with
this. Okay. So this is they have V trace
this. Okay. So this is they have V trace
returns and then they call
V trace. So vtrace from
V trace. So vtrace from
logits. Then they do
advantages
this and loss.
Okay. And
Okay. And
then their V trace here.
No. Is there V trace in
Python? No. No, it's not. It's okay.
Python? No. No, it's not. It's okay.
It's like weird torch code.
Fine. Yeah. Okay. Okay. So from logits
here. Okay. Hopefully we can figure it
here. Okay. Hopefully we can figure it
out from
this. What do they do it from? They do
this. What do they do it from? They do
it from logits or whatever.
from
from
logits. It's probably
easier. And yeah, there's also outputs
easier. And yeah, there's also outputs
two
two
things. Okay.
from logits just calls from importance
from logits just calls from importance
pool. So this
pool. So this
does log props and then
subtracts. So this gives you log
subtracts. So this gives you log
rows which literally right here. So this
rows which literally right here. So this
is log
rows. So you give this log rows and then
rows. So you give this log rows and then
it does X. Yeah. So this ratio is
it does X. Yeah. So this ratio is
literally just rows,
right? And then they clamp rows.
and the torch.n starting from this line
actually.
actually.
Okay. Clip the
Okay. Clip the
rows. Append bootstrapped value.
kind of wonky that they do that. I guess
kind of wonky that they do that. I guess
they were matching the reference
they were matching the reference
implementation, but like
fine,
fine,
deltas, flipped
deltas, flipped
rows, rewards plus
rows, rewards plus
discounts. Okay.
That
looks Wait, the
heck? Okay, so here's
their here's their result here, right?
B S minus
B S minus
V and they add
V and they add
values. They do they add the values here
values. They do they add the values here
to get
VS broadcasted bootstrap
values flipped PG rows. I thought we
values flipped PG rows. I thought we
already did
that. Oh, cool. They just had a separate
threshold. And
threshold. And
then here are your
advantages, which is flipped PG rows
advantages, which is flipped PG rows
times rewards.
Okay, I think we should be able to use
Okay, I think we should be able to use
this as a reference. This is like at
this as a reference. This is like at
least way easier to follow.
So we have our row subt.
Okay, so this is row
Okay, so this is row
subt P0
subt P0
uh
uh
times
rewards discount
rewards discount
times BST
next
next
minus hang on that's the
delta okay so this is delta
Right clipped rows. Okay. So we get this
Right clipped rows. Okay. So we get this
delta and then the
accum
accum
deltas of
t plus
t plus
discount times
Delta Crad times
deltas a
deltas a
Is that
Is that
it? I might have this wrong. I'm one
complicated. Don't need too much space
complicated. Don't need too much space
with this. We just need this little
window. Okay.
So they
So they
do this product times this delta,
right? What's
right? What's
actum? So yeah, this is my
accumulator
discounts. Did I do this wrong?
This might
[Music]
be I might have to move this delta right
be I might have to move this delta right
this product. This is C
product like this.
Yes. So this is the backwards iteration
Yes. So this is the backwards iteration
that I'm doing
here.
So deltas of
So deltas of
t which is right here. I just computed
t which is right here. I just computed
delta.
delta.
My delta is the same as
theirs. We might have screwed up the
theirs. We might have screwed up the
last value. But so far this other than
last value. But so far this other than
that this is
fine. Add V to get V
fine. Add V to get V
S. Add V to get V S. What? Add V of X to
S. Add V to get V S. What? Add V of X to
get VS.
Well, what do they use BS on?
baseline
loss. Yeah. So this
loss. Yeah. So this
is value loss.
Let's first modify this thing to do
Let's first modify this thing to do
um to have two
um to have two
inputs. So this should be
inputs. So this should be
VS and advantage.
So let's let's actually modify the
signature
bandages. So this is going to
bandages. So this is going to
be BS
be BS
float
float
bandages. Okay.
bandages. Okay.
And this
And this
[Music]
[Music]
is
important. Uh does C++ do multiple
important. Uh does C++ do multiple
returns?
Nope. In that case then we will just
um we'll move
um we'll move
this to this B trades
check. Oh, you can't return it from
check. Oh, you can't return it from
PyTorch then either, can
you? Yeah, that's super annoying.
I mean, we were probably going to just
I mean, we were probably going to just
eventually pass it in anyways,
right?
right?
[Music]
Bandages. Yes, bandages.
Okay, so we get BS
Okay, so we get BS
advantages and then we pass
advantages and then we pass
in yes advantages to the
kernel. All
right.
bandages ratio.
Let's modify
this. It's in
here. So, this is now
So we get this
So we get this
accum. So we just
accum. So we just
do s of
do s of
t
t
plus values of
t and then the PG
t and then the PG
advantages
advantages
equals. This does not use the
s+. So rotate
times rewards
times rewards
t plus
t plus
gamma. This is not values.
gamma. This is not values.
This is B S + one minus
values. BS at T + one.
Isn't that
Isn't that just you put this
Isn't that just you put this
here,
right? And I just put this
there. Okay, let's get this to compile
there. Okay, let's get this to compile
and then we'll with the uh the
and then we'll with the uh the
formula,
right? We're getting close though. We're
right? We're getting close though. We're
definitely getting close to
this. I hope it
this. I hope it
works. I mean, like, I hope the correct
works. I mean, like, I hope the correct
implementation's actually good.
Okay. V trace
check
importance thumbs
horizon. Yes.
Don't need this
anymore. Air building up.
Uh, and now this is going
Okay,
Okay,
cool. Uh, these chaos are crap. It's
cool. Uh, these chaos are crap. It's
funny this is not immediately
exploded. So, we should actually just be
exploded. So, we should actually just be
pretty chill right now,
pretty chill right now,
right? This should literally just be um
like something like this,
right?
Advantage and then was like bash
returns
maybe new value minus returns.
maybe new value minus returns.
Yeah, that's it.
Do we have to do this like weird shift
Do we have to do this like weird shift
thing
thing
anymore? I don't think we do, right?
anymore? I don't think we do, right?
Can't we literally just do
um it just
this? Yeah. Yeah. Yeah. Yeah.
GT
GT
shape. What's
this
one? Something like this, right?
The first few logics being zero is weird
The first few logics being zero is weird
as
as
hell. We should probably investigate
hell. We should probably investigate
that.
I almost expected that to work.
Almost. So,
um, here are your advantages, right?
your
PS and see how they implemented
it. They had this loss
it. They had this loss
here. So they did as
NL
loss. They summed. That's crazy weird.
this thing
run. Okay, now I have it implemented the
run. Okay, now I have it implemented the
way that they have it
way that they have it
implemented, right?
polish
ingredients and then they sum and
ingredients and then they sum and
there's no negative on this thing,
there's no negative on this thing,
right? Yeah. Okay. So, this is good.
value loss is super
low because there's nothing happening.
low because there's nothing happening.
It's predicting zeros and the kale is
It's predicting zeros and the kale is
blown
blown
up. Presumably something is wrong
there. So, let's see. Uh let's go back
there. So, let's see. Uh let's go back
to this then.
B trace
returns. Let's go through this thing
returns. Let's go through this thing
with
with
a fine tooth comb. Right.
So these C's
So these C's
are clamped to rows,
right? They also did this like shifted
right? They also did this like shifted
array thing. No big
array thing. No big
deal. And their
deal. And their
deltas flipped rows.
Yeah. So this we have this as well,
Yeah. So this we have this as well,
right? And then this
uh this is not how I
read this is not how I read this CS of
read this is not how I read this CS of
T, right?
Is this
correct? That would be a major
correct? That would be a major
discrepancy. Let me
discrepancy. Let me
see.
see.
So they just get rows from this log
So they just get rows from this log
ratio
ratio
here. They clamp them
here. They clamp them
CS make no changes and then yeah here
CS make no changes and then yeah here
they have this.
they have this.
But in the
paper, where'd he
go? Was it
impala? So in the paper they
impala? So in the paper they
have it.
Is this product not
um Hold on. There's a recursive formula
um Hold on. There's a recursive formula
for
it. Uh B trace targets can be computed
recursively. Okay. So this is the
recursively. Okay. So this is the
formula they're using, right?
CS and CS
is okay. Okay, so this is the formula
is okay. Okay, so this is the formula
that they're
that they're
using and let me let me compare theirs
using and let me let me compare theirs
to
this. So they do
this. So they do
B XS
Delta
SV plus discounts
CS times
act is VSP +
act is VSP +
1 minus V of
1 minus V of
T. Oh, because they have this minus
T. Oh, because they have this minus
values here,
values here,
right? Okay, hang on. Let me let me see
right? Okay, hang on. Let me let me see
if I can align it with this then. I
if I can align it with this then. I
think they're doing like several things.
Screwy. So, we're not going to need this
Screwy. So, we're not going to need this
at all.
at all.
Um, we are going to need clip
CS, which is really
CT
CT
float CT.
clamp
clamp
rows and flip rows. Yeah, these can
rows and flip rows. Yeah, these can
technically have different flip
technically have different flip
thresholds, right? They don't, but they
can. So then what we do is we get our
can. So then what we do is we get our
delta.
delta.
we get our deltas is going to
be they said they define this as flipped
rows. Hang on. I want to make sure we
rows. Hang on. I want to make sure we
get the mathematically
get the mathematically
correct definition
here.
Go. Where did I put this freaking PDF?
Little crazy
here. Row equals
Yeah, it is the clipped thing. Okay,
Yeah, it is the clipped thing. Okay,
that's fine.
that's fine.
So
row delta is equal to
row delta is equal to
rotate
times
rewards
rewards
plus
plus
gamma times
gamma times
values times next terminal.
values times next terminal.
minus values.
And
advantages is equal
advantages is equal
to
do this accum first is deltas plus
do this accum first is deltas plus
gamma.
C
C
T next
terminal. So gamma *
terminal. So gamma *
C*
N and then
BS is you add values to
BS is you add values to
this
this
right to accumul
Advantages is row
times rewards plus
discount. This is BT + 1.
discount. This is BT + 1.
minus values of t. I think this is
minus values of t. I think this is
correct. And we'll
correct. And we'll
say think this is
correct. We're going to be able to
correct. We're going to be able to
remove this whole product thing as
well. So this ends up actually being
well. So this ends up actually being
that bad codewise,
right? See, I thought we had
it. The value loss is low. It's the KL
it. The value loss is low. It's the KL
that's screwing.
that's screwing.
So presumably I'm still optimizing that
So presumably I'm still optimizing that
thing wrong.
So policy
policy. Let's go through it
again. You put your importance ratio
again. You put your importance ratio
which is this X, right?
What do you pass
in? You pass in ratio,
in? You pass in ratio,
right?
Ratio values, rewards, dumps,
ratio. Okay.
So you get your rows which are just
So you get your rows which are just
flipped to
flipped to
max. And this is exactly how I was doing
max. And this is exactly how I was doing
the
the
clipping. Wait,
clipping. Wait,
fax. You have this backwards, don't
fax. You have this backwards, don't
you? This is F
you? This is F
min. You don't need to clip. Yep. You're
min. You don't need to clip. Yep. You're
clipping the wrong here.
clipping the wrong here.
This is F
min. Flipping is
wrong. Every time, every bug is a little
wrong. Every time, every bug is a little
closer to having this thing
closer to having this thing
working. All
working. All
right, so
min.
min.
Yeah, that
Yeah, that
one. Now we try again.
Every time it's like, oh, it's
Every time it's like, oh, it's
optimizing.
Nope. You can see from the kales. The
Nope. You can see from the kales. The
kales are all all
screwy. Keep looking at this line by
screwy. Keep looking at this line by
line.
Delta
Delta
row
row
reward
reward
discount next
discount next
value terminal minus values of
value terminal minus values of
T and then a
Q deltas of T delta
Q deltas of T delta
It's
It's
discount
discount
CST
CT non-
terminal. You need next terminal
here. You do,
right? I believe so.
It would make sense from
this. This gives you Vs minus
this. This gives you Vs minus
V. You add
V. You add
value, right? You add the values
here. The Q plus values of T.
And
And
then the advantage here is the main
then the advantage here is the main
thing. Row times rewards of
thing. Row times rewards of
t gamma discount non-term minus
t gamma discount non-term minus
values. And this B S T + one.
So they just took
So they just took
this the last term here is the value
minus. So the last one of
minus. So the last one of
these is going to
be this is zero. Reward minus value.
of
of
t. You're missing a
term. This accum should probably start
term. This accum should probably start
off
as try this.
We still get exploding policy
loss. This has got to be so close though
loss. This has got to be so close though
at this point, right?
Okay. So
here make sure we do this correct. So we
here make sure we do this correct. So we
absolutely know this file is not the
absolutely know this file is not the
issue.
see
this. What's wrong with logic being like
this. What's wrong with logic being like
why are logic
why are logic
zeros? That's super screwy, isn't
it? Why is batch opposite zero?
this not get filled
up. We're missing like a ton of
up. We're missing like a ton of
observations in here, right?
It's
It's
bizarre. I will double check in a second
bizarre. I will double check in a second
that this still trains with
PO
advantages samples.
advantages samples.
You get your
batch.
batch.
Okay. Actions.
Okay, so you have your
Okay, so you have your
advantage mean
advantage mean
of the way that they did. This
was Oh, did you miss a log
was Oh, did you miss a log
softmax? I think you missed a log soft
softmax? I think you missed a log soft
max. Hang on.
Tail's still
broken. Logic should be just what comes
broken. Logic should be just what comes
out of the policy, right?
Yeah.
Target. We'll do target
Make sure there's not like a separate
Make sure there's not like a separate
param or some weird
thing. You can see the kale still
thing. You can see the kale still
screwed up.
just
detach. I don't It's weird that they sum
detach. I don't It's weird that they sum
that,
that,
but it's definitely supposed to be a
but it's definitely supposed to be a
meme.
Well, at least now we know that we have
Well, at least now we know that we have
this correct,
right? This It's got to be
right? This It's got to be
solid. The baseline is going to just be
some torch sum compute baseline
loss. Ah, so this is returns minus your
loss. Ah, so this is returns minus your
value returns equals DS, right?
value returns equals DS, right?
value
loss. So, this is good right
loss. So, this is good right
here. We're happy with
this. Yeah, we are happy with this.
Now be very very carefully go through
Now be very very carefully go through
this one more time.
You clip to a max of row
record
record
min. It gives you a
max
delta
delta
is flipped
is flipped
row reward at time t.
Gamma value at
Gamma value at
time t next t +
one times
terminal minus values. time.
I'm
I'm
going deltas of t.
Wait. Yeah, this is fine. So this is
Wait. Yeah, this is fine. So this is
delta
delta
gamma
C terminal.
They have to reverse their thing because
They have to reverse their thing because
they did it
they did it
weirdly.
VS cumulate plus
VS cumulate plus
values. And then what we did
values. And then what we did
here is make
here is make
sure broadcasted bootstrap values
Is this a different threshold? PG row
Is this a different threshold? PG row
threshold. We should probably check this
threshold. We should probably check this
right now. These are both set to one
here. We get row of t
times
reward
reward
plus
discount accum
values. The last value of this is going
values. The last value of this is going
to be the last
to be the last
value we have here.
We do all this and our policy gradient
We do all this and our policy gradient
still explodes.
It's a reasonable PG loss right there.
It's a reasonable PG loss right there.
Right.
I'm not seeing the explosion happen
I'm not seeing the explosion happen
instantly.
So yeah, this policy loss is
actually like pretty reasonable for
actually like pretty reasonable for
quite a while. It does eventually blow
quite a while. It does eventually blow
up a fair
up a fair
bit. What about the uh was old
KL. So, so it's like 90 whatever%
KL. So, so it's like 90 whatever%
clipping. Is it immediately 90%
clipping? Yes, it's immediately 90%
clipping? Yes, it's immediately 90%
clipping. 60 or whatever.
clipping. 60 or whatever.
So what is this
flip? Okay. So this clamp here
There's no clip that goes on this floss.
There's no clip that goes on this floss.
Stop it.
Try to run PO real quick. See if I broke
Try to run PO real quick. See if I broke
anything. I did.
This is
PO. We see small positive policy
PO. We see small positive policy
loss, very small chaos.
some
flipping. That is what we should
see. Okay. What is the original
Hello YouTube folks. We are hopefully
Hello YouTube folks. We are hopefully
pretty close to having V trace
pretty close to having V trace
implemented into Puffer.
implemented into Puffer.
This would hopefully stabilize the
This would hopefully stabilize the
advantage filtering stuff we've been
advantage filtering stuff we've been
doing and also maybe maybe just allow us
doing and also maybe maybe just allow us
to throw a little bit of off policy
to throw a little bit of off policy
data into the uh the
data into the uh the
buffer once it
works. Okay, so these are tiny tiny tiny
works. Okay, so these are tiny tiny tiny
numbers,
right? Okay, so how is it that
right? Okay, so how is it that
immediately these numbers
immediately these numbers
are messed up?
Because you don't
do Hang on. When do you sample your
back?
Sample. You sample your badge up
here, right?
I do wonder if there's something to
I do wonder if there's something to
this.
Yeah, they shouldn't be diverged that
Yeah, they shouldn't be diverged that
much immediately.
What happens if we
What happens if we
um
Okay. What happens if we do
this? Okay.
this? Okay.
So that actually does change stuff, but
So that actually does change stuff, but
you still learn with
you still learn with
PO,
right? So you start off with these
right? So you start off with these
approx like 0.5 and
2. You end up with a pretty damn high
2. You end up with a pretty damn high
clip
fract. But you still
fract. But you still
learn and uh the tails do not blow up.
learn and uh the tails do not blow up.
Policy loss does not blow up. Value loss
Policy loss does not blow up. Value loss
does not blow up. And it still works.
does not blow up. And it still works.
But there's quite a bit of
clipping. This thing not being clipped
clipping. This thing not being clipped
is the problem.
see you whatever I do it's still it just
see you whatever I do it's still it just
the KLs blow up instantly
It's got to just mean that the
um we can print the values as well,
um we can print the values as well,
right?
These are not
These are not
exploding. Yeah. So, these are not
exploding. Yeah. So, these are not
exploding as far as I can
see. The KL that blows up.
state experience values.
This wait value equal batch valvalues
This wait value equal batch valvalues
button.
Uh that's going to
Uh that's going to
uncip the value
loss, but the V loss is not blowing up
loss, but the V loss is not blowing up
anyways. And that should be the more
anyways. And that should be the more
stable of the losses.
What
next? What we try
next? Maybe we go
next? Maybe we go
check. Let's go back to their code for a
check. Let's go back to their code for a
second. Check a couple things.
They have a couple different flip
coefficients. This is clamped to a
max. It's clamped to a
max. It's clamped to a
max. It's clamped to a max.
So, PG
advantages just a name tubble.
So we
So we
do V trace from
logits
policy. Why do we have two different
policy. Why do we have two different
outputs
here? Oh, this is just for the ratio.
here? Oh, this is just for the ratio.
That's fine.
The
The
rewards the baseline
rewards the baseline
here
value and then they compute policy
value and then they compute policy
gradient
loss. Let's sum these together.
Presumably they have a negative sign as
Presumably they have a negative sign as
well. They're on entropy.
targets. Multiply
targets. Multiply
these. It's already detached.
from logits
from logits
here action log
props log
props log
props
props
difference you get log
difference you get log
rows and you're going immediately exit
entropy times advantage.
I think what we can even do to check
I think what we can even do to check
this
this
because this is identical.
Oh,
His advantages even look good. Let's
see. Let's see if these advantages look
see. Let's see if these advantages look
good.
Okay. Okay. So, this is zero. And
then why does this get bigger and
then why does this get bigger and
bigger? Ah, because you get another one
bigger? Ah, because you get another one
right here. Right.
That looks like
That looks like
pretty
pretty
sane reasonable stuff to me.
All
right. Yeah, actually tomorrow it's
right. Yeah, actually tomorrow it's
good.
trace. Okay.
So, isn't there there's supposed to be a
So, isn't there there's supposed to be a
value bootstrap, isn't
value bootstrap, isn't
there?
Am I
wrong? All right. So,
like that doesn't look quite right to
like that doesn't look quite right to
me, right?
So this gets
So this gets
no reward, but it does have the value on
no reward, but it does have the value on
it,
right? Do you think you need the
right? Do you think you need the
advantage normalization?
It could be that to be
fair. We'll try that in a second. Let me
fair. We'll try that in a second. Let me
just check one
just check one
thing. So if you get no reward, what's
thing. So if you get no reward, what's
the formula supposed to
do? The way they have
do? The way they have
it. Where's
B. All right. So, the way that they have
B. All right. So, the way that they have
this
things I do to fix.
This is going to be
This is going to be
one. This is
zero. This is like.99 whatever.
So it's this minus
So it's this minus
values. So V ST +
values. So V ST +
one. So this should literally end up
one. So this should literally end up
being
like Yeah, it's just going to be like
like Yeah, it's just going to be like
this number minus this number.
this number minus this number.
like that, right?
like that, right?
Yeah. So, a very small number is
Yeah. So, a very small number is
appropriate because it just it ends up
appropriate because it just it ends up
being the value difference between two
being the value difference between two
states. Let me try advantage
normalizing. Try this.
It just like it blows up instantly,
man. See, now the KL's fine, but the
man. See, now the KL's fine, but the
value loss is
screwed. Well, that is probably the
screwed. Well, that is probably the
thing that I broke from before, right?
I just put this down here.
So now the value loss is still exploding
So now the value loss is still exploding
just less
quickly. Make sure that um we still can
quickly. Make sure that um we still can
train PO on this.
Okay. So, this one still
works. Does it work without value
works. Does it work without value
function clipping?
Yeah, that seems fine,
right? Maybe even
right? Maybe even
better. Okay, so that might even be
better. Okay, so that might even be
better.
Really? value loss blowing
up. What was the value loan?
And it's clip to the clip
And it's clip to the clip
coefficient, which is kind of funny.
Yeah. So,
Yeah. So,
um, is the value loss just done now? Is
um, is the value loss just done now? Is
that what's happening?
batch
returns versus new value.
returns versus new value.
So we
So we
have new
value we have batch
returns and set that right
returns and set that right
there to
VS vs advantage.
This thing has just got to be blowing up
This thing has just got to be blowing up
completely,
right? Value
function. Look at this.
I mean, this is the one that was that
I mean, this is the one that was that
should be easy to get
right to add this and then the
values values
Right.
This thing should end up being pretty
This thing should end up being pretty
small.
Yeah, really good.
Go print out the values.
values.
values.
This Yeah, we can do that. Why not?
Interesting. This just
explodes. This thing just gets bigger
explodes. This thing just gets bigger
and bigger.
There's not even a sign difference.
There's not even a sign difference.
Wait, the fact that there's not a sign
difference. Batch
difference. Batch
values, that means that
like the values and the returns actually
like the values and the returns actually
kind of
kind of
match. The returns just make no bloody
match. The returns just make no bloody
sense, right?
Because all you're supposed to do is
Because all you're supposed to do is
just
just
like
this, right?
Maybe I need to do
Maybe I need to do
this. Maybe this feels here.
Just going to blow
up. It's just going to blow
up.
up.
Okay. Why Why is it like this?
It's like
That's cool.
What do we think?
and check against the math. I
and check against the math. I
guess
lambda. Hey captain, been building a
lambda. Hey captain, been building a
planter. I've been implementing V trace
planter. I've been implementing V trace
all day.
I like to be building a planner. That
I like to be building a planner. That
sounds
chill. And maybe this one we can see. So
Delta
plus gamma
C. Does this need a
C. Does this need a
minus?
minus?
No. Right.
Carpenter was showing me a few other
Carpenter was showing me a few other
guys some tricks. TLDDR V trace uh
guys some tricks. TLDDR V trace uh
replacement for generalized advantage
replacement for generalized advantage
estimation that should be better at off
estimation that should be better at off
policy correction. So it should allow us
policy correction. So it should allow us
to get more out of the new experience
to get more out of the new experience
buffer.
similar to to uh generalized advantage
similar to to uh generalized advantage
estimation. It ditches the lambda term
estimation. It ditches the lambda term
and instead it threads a
and instead it threads a
um it threads the important sampling
um it threads the important sampling
term that's used for the advantage
term that's used for the advantage
function uh or used to scale was it to
function uh or used to scale was it to
clip the policy gradient. Yeah. Instead,
clip the policy gradient. Yeah. Instead,
it threads that through the advantage
it threads that through the advantage
computation.
not max brad norm. It's
not max brad norm. It's
um it's a logic ratio of the new policy
um it's a logic ratio of the new policy
to the old policy before you started
to the old policy before you started
optimizing at this
epoch. So that then means that the
epoch. So that then means that the
clipping term also gets applied to the
clipping term also gets applied to the
the clipping term gets like deeply
the clipping term gets like deeply
applied to the value function as well
applied to the value function as well
but not just like as a post hawk thing.
What are you doing?
What are you doing?
I'm currently attempting to implement a
I'm currently attempting to implement a
CUDA kernel for this vtrace off policy
CUDA kernel for this vtrace off policy
correction advantage estimate from
correction advantage estimate from
impala into puffer so that we can uh
impala into puffer so that we can uh
leverage some like slightly off policy
leverage some like slightly off policy
data with our
data with our
implementation abandoning P30 for now or
implementation abandoning P30 for now or
do you think the new will experience
do you think the new will experience
buffer? Uh, so VRC is still very similar
buffer? Uh, so VRC is still very similar
to generalized advantage estimation and
to generalized advantage estimation and
like pretty much anything that I was
like pretty much anything that I was
going to do with
going to do with
P30 on top of J I could do it with V
P30 on top of J I could do it with V
trace as well.
So, Vray still has the annoying lambda
So, Vray still has the annoying lambda
uh the annoying gamma parameter, though
uh the annoying gamma parameter, though
arguably it should be slightly easier to
arguably it should be slightly easier to
deal with because it no longer has the
deal with because it no longer has the
lambda
parameter. So actually I think that
parameter. So actually I think that
probably once I get this thing
probably once I get this thing
implemented fully, this will probably be
implemented fully, this will probably be
a little easier to work with for like P3
a little easier to work with for like P3
style stuff versus
style stuff versus
J. How much testing of the experience
J. How much testing of the experience
buffer? Not a ton, but I mean we have
buffer? Not a ton, but I mean we have
results on we've got results on like
results on we've got results on like
neural MMO stuff. Actual cube. That's
neural MMO stuff. Actual cube. That's
soda right there. So there you go.
soda right there. So there you go.
That's soda with advantage filtering. Um
That's soda with advantage filtering. Um
So, yeah, it's pretty good. The curves
So, yeah, it's pretty good. The curves
aren't quite as clean, though, so we're
aren't quite as clean, though, so we're
we're gonna have to clean some stuff up
we're gonna have to clean some stuff up
there. But, uh, yeah, it's been pretty
there. But, uh, yeah, it's been pretty
decent overall.
doesn't take much of an SPS. It
doesn't take much of an SPS. It
shouldn't be shouldn't change the SPS at
all. There's like one slight thing that
all. There's like one slight thing that
we might have to consider when we do
we might have to consider when we do
filtering with this, but it should be
filtering with this, but it should be
fine.
It only screws over the SPS if you have
It only screws over the SPS if you have
like a two million step per second
like a two million step per second
training run and then you don't
training run and then you don't
implement a CUDA kernel for this. And lo
implement a CUDA kernel for this. And lo
and behold to the colonel.
I
I
mean, we kind of should just go off of
mean, we kind of should just go off of
this reference, right? Where is
it? I'm trying to think what the odds
it? I'm trying to think what the odds
are that I missed. I just missed
are that I missed. I just missed
something in here even though I've
something in here even though I've
looked at it 10 times.
Just rapid fire. Try a couple stupid
things. I think that's needed, but
Oh, that blows up instantly.
Oh, that blows up instantly.
Okay.
So now the KL blows
up. Interesting.
So either way, one of these two blows
up. Where did that freaking
up. Where did that freaking
implementation just go?
+
one. Maybe this is an issue. Hang
one. Maybe this is an issue. Hang
on. So, we
do t + one
do t + one
So this gets all the
deltas.
deltas.
Okay.
Okay.
And this accumulator
The compile times are a little
silly. Okay. So that's interesting,
silly. Okay. So that's interesting,
right? The value loss
The value loss is back to being mostly
same.
Actually, both the losses are kind of
like Yeah, the losses are like kind of
like Yeah, the losses are like kind of
the same until uh I guess the policy
the same until uh I guess the policy
where the KL tends to blow up. That's
where the KL tends to blow up. That's
better
better
though. Okay, then maybe this thing's
though. Okay, then maybe this thing's
just really sensitive. Maybe I have some
just really sensitive. Maybe I have some
like really minor minor details screwed
like really minor minor details screwed
up.
So the delta, you're missing the last
So the delta, you're missing the last
delta, right?
delta, right?
missing the last delta.
EG advantages.
next. Maybe I have this wrong still.
next. Maybe I have this wrong still.
Hang
on. Wait, I do have this wrong, right?
on. Wait, I do have this wrong, right?
I do have this wrong. Hang on. I think I
I do have this wrong. Hang on. I think I
see it. B ST +
one.
Yeah. All right. I got to get the
Yeah. All right. I got to get the
restroom and then I think I can fix
restroom and then I think I can fix
this. I think I see it. Right back.
So here's the thing,
So here's the thing,
right? This
is V S T +
is V S T +
one. This should not be a cube. This
one. This should not be a cube. This
should be V
should be V
S T + 1.
or this one here. I
mean, which is going to start out at
mean, which is going to start out at
zero.
Possibly we're going to have to add um
Possibly we're going to have to add um
the
the
terminal bootstrap value. We'll
see. This thing still immediately blows
see. This thing still immediately blows
up.
How many bugs do I have to fix before
How many bugs do I have to fix before
this thing
behaves? Look at this
behaves? Look at this
chaos. Okay.
Do I need this next non- terminal thing?
probably
probably
not. I don't think this fixes it though,
not. I don't think this fixes it though,
right? I don't I don't think that
right? I don't I don't think that
fixes. I'll run it, but I think we keep
fixes. I'll run it, but I think we keep
looking.
Discounts
discounts t
discounts t
next values of
next values of
t. Yeah, this stupid thing is just going
t. Yeah, this stupid thing is just going
to
to
keep
keep
breaking. Okay, let's think about
breaking. Okay, let's think about
terminal or bootstrap final
terminal or bootstrap final
value.
So, yeah. So, it's it's just the last
So, yeah. So, it's it's just the last
value, right?
value, right?
B ST +
one. See if that does anything.
exploding
algorithm. Okay, I must have missed
algorithm. Okay, I must have missed
something else, right? Like
Start off at zero.
Okay. So,
Okay. So,
delta is going to
delta is going to
grab t next, right?
And then there's a
So this whole thing should be
So this whole thing should be
fine. Oh wait, the only thing is that
fine. Oh wait, the only thing is that
this the last BS is going to be
wrong. Hang on. The last BS is going to
wrong. Hang on. The last BS is going to
be wrong.
be wrong.
What's the last PS supposed to
be? And it's supposed to be bootstrap
be? And it's supposed to be bootstrap
value.
It is.
I just did that,
right? Yeah, I just did that and that
right? Yeah, I just did that and that
didn't make it. Like that didn't fix it
didn't make it. Like that didn't fix it
magically.
This guy has a pretty nice paper. It's
This guy has a pretty nice paper. It's
uh was it beyond the rainbow? It's uh
uh was it beyond the rainbow? It's uh
basically rainbow, which is all the
basically rainbow, which is all the
kitchen sink of tricks that they threw
kitchen sink of tricks that they threw
on DQM. Well, this guy found another
on DQM. Well, this guy found another
kitchen sink worth of tricks to throw on
kitchen sink worth of tricks to throw on
it, and it did a lot better.
This is discord.gg/puffer GG/huffer if
This is discord.gg/puffer GG/huffer if
anyone is interested want to get
anyone is interested want to get
involved.
We don't get a break on this
We don't get a break on this
thing. This still needs to get done
today. I mean, they just used the
today. I mean, they just used the
bootstrap value at the very end,
right? What's the s
then
add
values? No, they do use it there as
values? No, they do use it there as
well, don't
they? They use the bootstrap value at
they? They use the bootstrap value at
the end here as well.
Finally helps increasing action
Finally helps increasing action
gaps and therefore reducing policy
gaps and therefore reducing policy
turning off
policiness. This policy isn't based on
policiness. This policy isn't based on
the value function directly at
the value function directly at
least increasing action gaps.
This guy had some very nice work.
Soft WS.
Am I getting
trolled? I'm not actually getting
trolled? I'm not actually getting
trolled.
Got a bajillion things in here.
Love to see people using the algorithm
Love to see people using the algorithm
as fast as possible.
This would be a very cool guy to get
This would be a very cool guy to get
involved with. Buffer
still have to finish this uh beach race.
still have to finish this uh beach race.
We don't get a pass on
We don't get a pass on
this. I do not know what's wrong with it
though. Hang on. We are messing with
though. Hang on. We are messing with
this uses.
this uses.
I doubt you can get that speed, but
Okay. So
technically technically this BT plus one
technically technically this BT plus one
could be
could be
off. Soda all policy uses fairly long
off. Soda all policy uses fairly long
traces uses 160.
traces uses 160.
Interesting steps. Extremely slow.
Not as good.
No, I got to read that paper for sure. I
No, I got to read that paper for sure. I
think we start with this thing
think we start with this thing
though. What's this paper
though. What's this paper
from? Is this recentish or
no? 2022. Not really.
Each forward pass slows down a lot.
Each forward pass slows down a lot.
do so many poor passes kills.
two weeks for an Atari game. It's crazy.
two weeks for an Atari game. It's crazy.
It's not like one policy and everything,
right? Yeah, it's still one each, I
right? Yeah, it's still one each, I
think.
That's ridiculous.
I I actually think that I do have this
I I actually think that I do have this
right now, don't
right now, don't
I? I
I? I
miss I missed the last delta, I believe.
Yeah, I missed the last
delta. What does that do? Missing the
delta. What does that do? Missing the
last
delta. Do I have like some weird off by
delta. Do I have like some weird off by
one on the
one on the
advantages? Could be.
advantages? Could be.
I don't think so though.
Oh jeez. They actually roll it out like
Oh jeez. They actually roll it out like
160 time steps and then they throw away
160 time steps and then they throw away
159 of
159 of
them. Crazy.
We're updating a single Q value.
I didn't try without normalizing
I didn't try without normalizing
advantages until I since I did this,
advantages until I since I did this,
right? I didn't try without normalizing
right? I didn't try without normalizing
advantages. Maybe that maybe you can't
advantages. Maybe that maybe you can't
normalize advantages in this.
That's one thing we should try.
Did I really just casually do that? Hang
Did I really just casually do that? Hang
on. Is this the
on. Is this the
baseline? No, this is not the
baseline? No, this is not the
baseline. We just got V trace
working. Well, okay then.
See if we uh we can
See if we uh we can
do we can get rid of this
Start. Looks better, right?
off policy or bottlenecks by gradient
off policy or bottlenecks by gradient
steps more than anything
Well, I think we can uh we can actually
start benchmarking a little
start benchmarking a little
bit. That's
awesome. All of a sudden, it
awesome. All of a sudden, it
works. Also, yeah, that's definitely
works. Also, yeah, that's definitely
Soda.
Spencer just got his
Okay, it's not quite on par yet, right?
Okay, it's not quite on par yet, right?
But this is also no advantage filtering
But this is also no advantage filtering
like brand new thing that we just
like brand new thing that we just
implemented.
So next thing on this is going to
be can we
filter? Well, let's make a commit right
filter? Well, let's make a commit right
now.
now.
because
All
right, we're actually going to do some B
right, we're actually going to do some B
trade stuff real quick. So, this guy is
trade stuff real quick. So, this guy is
cool to chat with. Um, I actually I
cool to chat with. Um, I actually I
really wanted to have somebody with off
really wanted to have somebody with off
policy experience for a while, like
policy experience for a while, like
actually like good off policy
actually like good off policy
experience.
experience.
uh doing some stuff around in puffer and
uh doing some stuff around in puffer and
yeah from that manuscript this guy would
yeah from that manuscript this guy would
be perfect for
be perfect for
that. You can check out his paper. It's
that. You can check out his paper. It's
on open review beyond the rainbow. It's
on open review beyond the rainbow. It's
uh pretty nice additions to rainbow DQN
uh pretty nice additions to rainbow DQN
and pretty well done science too.
So I think that all we have to do for
this can we just copy paste
This
like
and two weeks to 12 hours.
and two weeks to 12 hours.
Apex stuff for
Apex stuff for
BTR. Haven't seen Apex in a long
BTR. Haven't seen Apex in a long
time. God damn
it. Distributed architecture.
Okay. So, the plan with this stuff right
Okay. So, the plan with this stuff right
here would
be how do we
compute how do we compute V
compute how do we compute V
trace? I guess all we have to do is add
um log ratio, right?
We massively lower the replay ratio.
We massively lower the replay ratio.
Interesting.
I think all you have to do for
I think all you have to do for
experience, right, is you just have to
experience, right, is you just have to
initialize.
You just have to initialize it to ones,
right?
right?
Because well, it is going to be one to
Because well, it is going to be one to
start with,
start with,
right? Wait, it's literally always going
right? Wait, it's literally always going
to be one.
to start with.
to start with.
No, I guess yeah, initially it should
No, I guess yeah, initially it should
just
just
be we can just do like
would be like experience importance
would be like experience importance
here. Maybe
turns
advantage. A lot of my work focus on
advantage. A lot of my work focus on
fairly slow sim.
5:14 right now. I'm going to try and
5:14 right now. I'm going to try and
implement this oneation. I'm going to
implement this oneation. I'm going to
order dinner. I'll dev for like half
order dinner. I'll dev for like half
hour or whatever. Get dinner and then
hour or whatever. Get dinner and then
I'll come back for another couple hours.
I'll come back for another couple hours.
I think that's the plan.
Well, this is kind of sketchy, isn't
it? So, this ratio
um yeah, you don't actually know what
um yeah, you don't actually know what
this ratio is.
until you do the forward
until you do the forward
pass,
right? Oh, that's not an issue though,
right? Oh, that's not an issue though,
is
is
it? No, that's not an
it? No, that's not an
issue. Cuz then all you just do is you
issue. Cuz then all you just do is you
just do
just do
experience
experience
or ex ratio.
Right. Perfect. So we finish that
Right. Perfect. So we finish that
conversation. We
get this is fine, right?
or is it
not? This should be
fine. And then Yeah, this should be
fine. And then Yeah, this should be
fine. Let's We'll
see. Get some food as well.
The amount of supposedly Asian food in
The amount of supposedly Asian food in
uh Florida is very concerning.
Where's this
batch? Oh. Uh, this is experience.
And this is
uh
returns. Must be 2D.
Okay, I am actually going to order food
Okay, I am actually going to order food
while I fix this because otherwise I'm
while I fix this because otherwise I'm
going to
going to
like
Yeah, I think what I want
Just real
Just real
quick, it'll be quicker if I do this on
quick, it'll be quicker if I do this on
here, assuming that uh I still have my
here, assuming that uh I still have my
credentials in. And then we'll come back
credentials in. And then we'll come back
and we'll let me order dinner and then
and we'll let me order dinner and then
we'll actually we'll get some cool uh
we'll actually we'll get some cool uh
experiments going and I'll actually talk
experiments going and I'll actually talk
through stuff. I was a little distracted
through stuff. I was a little distracted
with that conversation, but
um yeah, we will actually get some cool
um yeah, we will actually get some cool
experiments
in. Got to love how Florida the average
in. Got to love how Florida the average
like distance to get food is like 10
like distance to get food is like 10
miles.
miles.
What in the
heck? It's actually not bad. That's not
heck? It's actually not bad. That's not
a bad
a bad
idea. Okay, local pups got me.
order some
stuff and uh we will run these
stuff and uh we will run these
experiments.
One
One
second. Hey bet.
Wrong thing. One
second. Just
Perfect. Okay, that is me taken care
of. Let's get back to
research. So, Here's the thing with B
research. So, Here's the thing with B
trace, right? Um, it requires an
trace, right? Um, it requires an
importance
importance
ratio. The importance ratio, we're not
ratio. The importance ratio, we're not
going to deal
going to deal
with historical buffers for now,
with historical buffers for now,
but it should always start off as an
but it should always start off as an
important ratio
important ratio
one because it's the ratio of new policy
one because it's the ratio of new policy
logics to old policy logics. And at the
logics to old policy logics. And at the
start of any, new policy equals old
start of any, new policy equals old
policy.
policy.
Um, so we should be able to do something
Um, so we should be able to do something
like
that. Oh yeah, and now we just have to
that. Oh yeah, and now we just have to
debug some shape stuff and we should
debug some shape stuff and we should
actually be able to compare this to what
actually be able to compare this to what
I was doing earlier.
This is the
This is the
issue. I can I Yeah, I just know that's
issue. I can I Yeah, I just know that's
the issue here. Let me just fix that.
the issue here. Let me just fix that.
This needs to
be instit.
value is not shaped. Once you do that
uh we can call this
uh we can call this
advantages. No reason not
advantages. No reason not
to experience
stocks. What is BTR paper?
Where is the It's beyond the rainbow.
Where is the It's beyond the rainbow.
It's this.
Yeah. Yeah, it's this thing. There's uh
Yeah. Yeah, it's this thing. There's uh
I guess it is on archive then. There's
I guess it is on archive then. There's
an archive. I was looking at the open
an archive. I was looking at the open
review version. From the format, it
review version. From the format, it
looks about the same.
So we had author of that is on the
discord. That's the goal eventually is
discord. That's the goal eventually is
just to have all the all the cool RL
just to have all the all the cool RL
people on the
Discord. Whenever I have a question
Discord. Whenever I have a question
about anything, it's just like ah
about anything, it's just like ah
they're all right here.
So,
uh, what we're with at the
uh, what we're with at the
moment is not quite off policy. It is an
moment is not quite off policy. It is an
off policy correction technique applied
off policy correction technique applied
to on policy.
Uh, yeah, that didn't freaking work, now
Uh, yeah, that didn't freaking work, now
did
it?
it?
Batch. Is it like
batchadvantages? PO arguably
batchadvantages? PO arguably
has not except for the dev branch. It
has not except for the dev branch. It
doesn't.
doesn't.
I mean the buffer branch, not even the
I mean the buffer branch, not even the
dev branch, the more more recent
branch. Okay, so we do actually have
branch. Okay, so we do actually have
advantages in the buffer.
Perfect. So now this is actually got to
Perfect. So now this is actually got to
be experience or is it
be experience or is it
back advantages?
Okay, it
runs. Still on random
sampling. It crashes.
sampling. It crashes.
Really?
Okay. Well, I have a
bug. Do I have a
bug. Do I have a
bug? Hang
bug? Hang
on. Am I not being smart about this?
on. Am I not being smart about this?
So every mini batch we recomputee B
trace the experience
importance
right we sample from advantages
And then if we're going to use V
trace
importance no this right it's not ratio
Actually, this shouldn't matter though,
Actually, this shouldn't matter though,
right?
right?
Wait. No, this is
correct. Is equal to ratio. Yeah. Yeah,
correct. Is equal to ratio. Yeah. Yeah,
this is
correct. It should be
correct. It should be
logit actions.
Huh? That's unstable.
Really? All I did was
uh still on random sampling is the
uh still on random sampling is the
thing. So that advantage didn't factor
thing. So that advantage didn't factor
in.
It's really weird that that is uh
It's really weird that that is uh
unstable. Let's just make sure we didn't
unstable. Let's just make sure we didn't
get
get
unlucky. I don't think
so.
So, comput your advantages, right?
No, we didn't get
No, we didn't get
unlucky. The kale blows up.
But it's weird
because do I just roll this
back? Hang on. Do I roll this back or do
back? Hang on. Do I roll this back or do
I just
compare? It's a good thing. And this is
compare? It's a good thing. And this is
why I made a commit.
why I made a commit.
Uh 15 20 minutes before
Uh 15 20 minutes before
food. Getting very
food. Getting very
hungry.
hungry.
Uh okay.
So from
So from
file
file
puffer clean
puffer
puff good time to remind folks you want
puff good time to remind folks you want
to help me out for free just started the
to help me out for free just started the
give almost at 2k stars and it really
give almost at 2k stars and it really
helps.
Not that many RL post with 2K
Not that many RL post with 2K
stars, especially not in like one year
stars, especially not in like one year
of fulltime
dev. Okay. So in this one here, I gave
dev. Okay. So in this one here, I gave
it
it
once
once
and I sampled
and I sampled
But this shouldn't matter because it's
But this shouldn't matter because it's
random sampling
random sampling
anyways, right? And then in
here, V
trace. Yeah. Start of the buffer. Uh you
trace. Yeah. Start of the buffer. Uh you
and then here we get VS vantage.
values. What is the difference between
values. What is the difference between
computing it here and
computing it here and
then elsewhere?
Oh, hang
Oh, hang
on. Yeah. No, you can't do
on. Yeah. No, you can't do
this. The uh the importance ratio gets
this. The uh the importance ratio gets
stale this way,
stale this way,
right? Yeah. The importance ratio gets
stale. Okay. So, I have to go back.
stale. Okay. So, I have to go back.
We go back
to
this. We go back to this and make sure
this. We go back to this and make sure
this still runs. And then what we can do
Let me see how prioritized experience
Let me see how prioritized experience
replays update formula works.
Okay. So they use TD
error and then they update the
error and then they update the
priorities with a new TD error after a
priorities with a new TD error after a
learning step.
So TD error.
Do you have to
um hang on when you prioritize this
thing your value this is going to get
thing your value this is going to get
slightly stale
Okay. But the key is it only gets stale
for Yeah, it's better to make that.
for Yeah, it's better to make that.
Okay, that's fine. Yeah, I see it. I
Okay, that's fine. Yeah, I see it. I
think I see it now, folks. I see it. So,
think I see it now, folks. I see it. So,
we have advantages here.
I guess we just take importance, right?
Okay. So, we do importance. It's going
Okay. So, we do importance. It's going
to start uniform.
This will be
important. And then where is it?
experience or is it just importance?
Um, I think that'll do it. That's not a
Um, I think that'll do it. That's not a
Neptune
log shape
mismatching. Oh yeah, this importance
mismatching. Oh yeah, this importance
doesn't need to be
doesn't need to be
Yeah, this importance needs to be uh
Yeah, this importance needs to be uh
data.experience
rows, right? Yeah, that's the
importance. Holy. Okay.
Oh, I think that it literally it does.
Oh, I think that it literally it does.
Okay. Port one to
like. Okay.
like. Okay.
So, this should still
So, this should still
work like
before. Come on.
It's getting hungry.
Oh, it eats the batch advantages,
Oh, it eats the batch advantages,
doesn't it?
This is just advantages.
Whatever we do
this. Okay, this runs
this. Okay, this runs
right explode instantly. No. Okay, we
right explode instantly. No. Okay, we
track
track
this and we see if we have broken
this and we see if we have broken
anything or not. And if we haven't, the
anything or not. And if we haven't, the
key thing is now we should actually be
key thing is now we should actually be
able to add in
able to add in
uh we should actually be able to add in
uh we should actually be able to add in
the advantage
the advantage
filtering. Here's our
run. This should be like about what this
run. This should be like about what this
yellow curve is.
1 minute 80 milstep training rooms are
1 minute 80 milstep training rooms are
very
nice. Yeah. Okay. This looks like this
nice. Yeah. Okay. This looks like this
is about the yellow, right? It's already
is about the yellow, right? It's already
up to seven. Yeah. So, this is about on
up to seven. Yeah. So, this is about on
par with the uh the yellow curve.
And there's some variability. So it's
fine. Now what we get to
do, delete this line. Rerun it. We
do, delete this line. Rerun it. We
should have important sampling.
We would hope would be better.
At least don't be
worse. Okay,
worse. Okay,
good. Yeah, this is uh this is what we
good. Yeah, this is uh this is what we
want. This is what we wanted to see.
I think it is better. We got to wait for
I think it is better. We got to wait for
the curve. But I think
so. Okay. Yeah, that is better. Perfect.
so. Okay. Yeah, that is better. Perfect.
So um then the question is going to be
right. Oops.
Do I add this to the experience
buffer? Probably, right?
Okay. And then we do
experience and then it's the importance
experience and then it's the importance
gets updated to this. And the only other
gets updated to this. And the only other
thing we have to do is we have to
thing we have to do is we have to
remember to zero it.
or one it I guess reset it either
way rerun so that we make sure we didn't
way rerun so that we make sure we didn't
break anything and then we will run with
Goodbye. This is experience
bros. Food is here in like a few
bros. Food is here in like a few
minutes. more
experiments and then that'll set a
experiments and then that'll set a
decent tempo. I don't expect the off
decent tempo. I don't expect the off
policy to work immediately by the way. I
policy to work immediately by the way. I
don't expect it to work
immediately. I just wanted to have V
immediately. I just wanted to have V
trace and now we have V
trace which technically kills a
trace which technically kills a
hyperparameter.
If it matches in perf, it's better just
If it matches in perf, it's better just
because it kills hyper
parameter. Okay, that looks good. Now we
parameter. Okay, that looks good. Now we
set replay factor equal
one. Set like 0.5.
one. Set like 0.5.
Maybe 0.5 should be plenty.
Okay, last
experiment should
be replay factor 0.5 means it's allowed
be replay factor 0.5 means it's allowed
to keep up to half a batch of data
to keep up to half a batch of data
across EOPS. So it is actually allowed
across EOPS. So it is actually allowed
to use fully stale data.
Yeah, I think it's going to be
Yeah, I think it's going to be
um the same stale result here.
How long is it keeping it? As long as it
How long is it keeping it? As long as it
likes. It's based on um well, it should
likes. It's based on um well, it should
be based on a priority metric, but we
be based on a priority metric, but we
got to improve on that. Okay. So, this
got to improve on that. Okay. So, this
does not uh vrace did not magically let
does not uh vrace did not magically let
us use all policy data yet. We will
us use all policy data yet. We will
continue to work on that. We will get a
continue to work on that. We will get a
actual proper prioritized replay buffer
actual proper prioritized replay buffer
and we will do some other stuff for that
though. Change the 05. Well, that was
though. Change the 05. Well, that was
the point of the test to see whether you
the point of the test to see whether you
could use
could use
Yeah, that's
Yeah, that's
fine. All right, I will be back after
fine. All right, I will be back after
dinner. Um, probably should be back
dinner. Um, probably should be back
dinner for at least like an hour or so.
dinner for at least like an hour or so.
for folks
for folks
watching puffer.ai for all my stuff.
watching puffer.ai for all my stuff.
Reinforcement learning dev live almost
Reinforcement learning dev live almost
every day. Uh start the GitHub to help
every day. Uh start the GitHub to help
us out. It really helps. We've got lots
us out. It really helps. We've got lots
of fun demos on here, lots of different
of fun demos on here, lots of different
games, all with neural net that run in
games, all with neural net that run in
your
your
browser. If you want to get involved
browser. If you want to get involved
with dev, join the discord. Some of our
with dev, join the discord. Some of our
best contributors came in with zero RL
best contributors came in with zero RL
experience.
experience.
And you can follow me on X for more
And you can follow me on X for more
reinforcement learning
reinforcement learning
content. It's all RL all the time.
content. It's all RL all the time.
[Music]
