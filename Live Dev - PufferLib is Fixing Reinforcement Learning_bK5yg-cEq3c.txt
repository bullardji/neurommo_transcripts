Kind: captions
Language: en
okay we are
live he didn't update my titles it's
live he didn't update my titles it's
modly
annoying all right well maybe that
annoying all right well maybe that
worked we'll
see cool um um we have a lot of stuff to
see cool um um we have a lot of stuff to
do
today this is still running
somehow let's check the old experiments
somehow let's check the old experiments
I don't have super high hopes for
this I mean that's not even that bad
this I mean that's not even that bad
honestly
that's really not even that
that's really not even that
bad okay so um it's the thing is
bad okay so um it's the thing is
definitely way worse than
definitely way worse than
before um
before um
so we need to do we need to make a
so we need to do we need to make a
modification of this algorithm the plan
modification of this algorithm the plan
for today and hang on let me get this
for today and hang on let me get this
window open and then I'll go through the
window open and then I'll go through the
plan for
plan for
today oh let me also just make sure it
today oh let me also just make sure it
didn't screw up my
didn't screw up my
mic
mic
welcome let me make sure it's using my
welcome let me make sure it's using my
correct mic yes it
correct mic yes it
is okay so here's the plan for
is okay so here's the plan for
today last night I came up
today last night I came up
with some better math
for the algorithm we should open
po oh no it's not this paper actually
po oh no it's not this paper actually
it's the J
paper so arguably then this will be the
paper so arguably then this will be the
first mod uh first Improvement to RL
first mod uh first Improvement to RL
in 10 years if this
in 10 years if this
works or our
works or our
algorithms the formula we care about
algorithms the formula we care about
here I love how the whole AI field is on
here I love how the whole AI field is on
archive is pretty good lately some of
archive is pretty good lately some of
the big Labs haven't been publishing
the big Labs haven't been publishing
enough stuff so but other than that yeah
enough stuff so but other than that yeah
it's all
there all
there all
right yeah this section right here this
right yeah this section right here this
is what we
is what we
need so I think what we're going to do
need so I think what we're going to do
first is we're going to just make this
first is we're going to just make this
modification run some quick tests not
modification run some quick tests not
spend all day on this though because
spend all day on this though because
there is uh there's work to be done in
there is uh there's work to be done in
Exploration so morning session while I'm
Exploration so morning session while I'm
still waking up here I'm just going to
still waking up here I'm just going to
fix this stuff see if we can get some
fix this stuff see if we can get some
stuff running uh some experiments
stuff running uh some experiments
running and then the rest of the today
running and then the rest of the today
will be on diving into exploration
will be on diving into exploration
research and seeing if we can just find
research and seeing if we can just find
any any signal at all in that whole
any any signal at all in that whole
branch and really just if there's any
branch and really just if there's any
there any ideas and exploration that
there any ideas and exploration that
seem to make any
seem to make any
sense I got the postto if I want it
sense I got the postto if I want it
congratulations
yeah don't listen to me on that one
yeah don't listen to me on that one
because my reference is posts in
because my reference is posts in
computer
computer
science and it's a very different deal
science and it's a very different deal
because you kind of don't need to do a
because you kind of don't need to do a
post talk in my field for anything other
post talk in my field for anything other
than becoming a professor and it really
than becoming a professor and it really
doesn't give you access to any resources
doesn't give you access to any resources
either we need someone to get [ __ ]
either we need someone to get [ __ ]
done
done
well we do get [ __ ] done around here we
well we do get [ __ ] done around here we
do get [ __ ] done
I have no idea what a z machine is so
I have no idea what a z machine is so
I'm just going to assume that it uses
I'm just going to assume that it uses
quantum mechanics to alter the
quantum mechanics to alter the
fundamental nature statistics
all
right greatest x-ray Source on
Earth what do you use the what do you
Earth what do you use the what do you
use those for
stockpile nuclear deterrents
I don't understand how but sure
this is the thing that matters right
this is the thing that matters right
here
here
right this
gamma data for radiation s
orang reinforcement learning
x-rays this is going to be a little
x-rays this is going to be a little
tricky actually isn't
it Advantage
scale we just we'll call this
scale we just we'll call this
gamma oops
we're writing we're wri in Cuda Kernels
we're writing we're wri in Cuda Kernels
at the moment
oh actually that's a problem isn't it
oh actually that's a problem isn't it
the way that this
is there's a lot of
applications project coming down
line puffer could be the main library he
line puffer could be the main library he
if you have a use for we got nice RL
tools and the funny thing is we're kind
tools and the funny thing is we're kind
of at the point where it's
of at the point where it's
like you kind of should just use puffer
like you kind of should just use puffer
for whatever cuz I can't even say oh you
for whatever cuz I can't even say oh you
have a big project so you should use
have a big project so you should use
this other bigger
this other bigger
library because like the bigger
library because like the bigger
libraries just don't work all that well
libraries just don't work all that well
we could contract with
we could contract with
puffer we could contract with the allp
puffer we could contract with the allp
publish yeah I mean we uh we are pretty
publish yeah I mean we uh we are pretty
easy to work
easy to work
with we don't put any we don't have any
with we don't put any we don't have any
real IP restrictions on anything we kind
real IP restrictions on anything we kind
of just put every uh well any core
of just put every uh well any core
features that you need just go in puffer
features that you need just go in puffer
that's really easy uh we don't put any
that's really easy uh we don't put any
confidentiality agreements it's like
confidentiality agreements it's like
pretty darn easy to get uh whatever RL
pretty darn easy to get uh whatever RL
stuff you need from us pretty quick as
stuff you need from us pretty quick as
well
I think we'll be starting our third
I think we'll be starting our third
contract um next week or whatever so I
contract um next week or whatever so I
mean puffer's starting to do okay it's
mean puffer's starting to do okay it's
still a very new company I think it's
still a very new company I think it's
March still less than a year since I uh
March still less than a year since I uh
have really started working on this
fulltime progress has been
great the group is here making a website
great the group is here making a website
that's cool yeah I mean the key thing
that's cool yeah I mean the key thing
would be if you have like a
would be if you have like a
concrete RL problem in
concrete RL problem in
mind um I can give you like just the
mind um I can give you like just the
initial does this make sense the way you
initial does this make sense the way you
framed it and if it does then we can go
framed it and if it does then we can go
from
there I definitely want to make sure
there I definitely want to make sure
though whatever you're looking at you
though whatever you're looking at you
actually have an RL problem because this
actually have an RL problem because this
is where like most people lose on on oh
is where like most people lose on on oh
our Sim is too slow or oh we can't
our Sim is too slow or oh we can't
really get the data that we need or like
really get the data that we need or like
oh there's not really like a clear
oh there's not really like a clear
observation to action model or something
observation to action model or something
like that
I see how this is wanted
okay I see how this works now
yeah that's a little
yeah that's a little
trickier do I need to make a third
trickier do I need to make a third
function for
function for
this I think I kind of do for
if Pokemon can be solved I feel most big
if Pokemon can be solved I feel most big
engineering problems being can be
engineering problems being can be
gamified like Pokemon yeah it's not so
gamified like Pokemon yeah it's not so
much the difficulty of the problem it's
much the difficulty of the problem it's
the formulation of the problem and how
the formulation of the problem and how
much data you have access to like
much data you have access to like
Pokemon wouldn't have worked if it were
Pokemon wouldn't have worked if it were
10 times slower just because we would
10 times slower just because we would
have needed 10 times as much compute and
have needed 10 times as much compute and
it would have really made the
it would have really made the
engineering effort
engineering effort
obnoxious Pokemon would have been a lot
obnoxious Pokemon would have been a lot
easier if were 10 times faster we would
easier if were 10 times faster we would
have been able to run 10 times as many
have been able to run 10 times as many
experiments and there wouldn't have been
experiments and there wouldn't have been
as much
as much
Engineering in puffer liid uh most of
Engineering in puffer liid uh most of
our environments are a 100 times faster
our environments are a 100 times faster
than even Pokemon which is why it's so
than even Pokemon which is why it's so
easy for us to keep like making progress
easy for us to keep like making progress
and everything so
quickly grab all this
let me get this done for now this is I
let me get this done for now this is I
really want to make sure this is done
really want to make sure this is done
this morning we don't get behind on
this morning we don't get behind on
schedule
here Advantage
some I don't know why I'm doing this
some I don't know why I'm doing this
actually can just
actually can just
do highlight the important details as
do highlight the important details as
they
they
emerge
emerge
yeah I mean this is very specialized
yeah I mean this is very specialized
what I'm doing right here this is very
what I'm doing right here this is very
specific Dev this is on a specific new
algorithm um that needs a specific Cuda
algorithm um that needs a specific Cuda
kernel
okay so here is these are really gamas
okay so here is these are really gamas
maybe I was right the first time for
maybe I was right the first time for
non-real Time stuff this is where LM
non-real Time stuff this is where LM
agents could come in handy Pokemon is
agents could come in handy Pokemon is
fast enough but there probably some
fast enough but there probably some
problems that are too slow uh yeah there
problems that are too slow uh yeah there
definitely are but with all the stuff
definitely are but with all the stuff
I'm working on like there are already
I'm working on like there are already
thousands of people doing llm stuff like
thousands of people doing llm stuff like
that so specifically what I do in puffer
that so specifically what I do in puffer
lib tries to handle everything except
lib tries to handle everything except
that right cuz there are also a lot of
that right cuz there are also a lot of
cases and I think this is what llm
cases and I think this is what llm
people are forgetting about there are a
people are forgetting about there are a
lot of cases in the real world where you
lot of cases in the real world where you
do have a fast Sim and you have the
do have a fast Sim and you have the
unlimited data and if you just have like
unlimited data and if you just have like
good stable RL that's going to always be
good stable RL that's going to always be
way more productive than having to train
way more productive than having to train
some like gigantic model or like try to
some like gigantic model or like try to
apply some gigantic model
well the thing that's the most
well the thing that's the most
complimentary to the llm stuff is puffer
complimentary to the llm stuff is puffer
is just like a vastly better way of
is just like a vastly better way of
making core RL
making core RL
breakthroughs uh it is and the reason is
breakthroughs uh it is and the reason is
that we can run experiments like 10,000
that we can run experiments like 10,000
times faster at probably way less than a
times faster at probably way less than a
10,000th of the cost so you know who's
10,000th of the cost so you know who's
going to be making breakthroughs faster
going to be making breakthroughs faster
I mean arguably you know they also have
I mean arguably you know they also have
10,000 times more resources than I do so
10,000 times more resources than I do so
maybe you know some stuff comes out of
maybe you know some stuff comes out of
both but like for the investment I'm
both but like for the investment I'm
putting into this a lot coming out of
it all right let me go back to myit I
it all right let me go back to myit I
don't know why I thought not to do this
don't know why I thought not to do this
this is totally fun
I will
Define yeah let me lock in on this for a
Define yeah let me lock in on this for a
bit so this is we've got our
gamas and now this needs to be
I guess I am only using the sigmas
I guess I am only using the sigmas
that's kind of weird
I mean this should work either
way for
okay flatten
advantages
oh and then how would you
oh and then how would you
do the mean
loss h
loss h
probably is just a mean
loss just have to redo the way that I
loss just have to redo the way that I
have these rewards so I think this is
have these rewards so I think this is
still your advantage
function this is
still your
advantages so then you get where the
advantages so then you get where the
advantage yeah here
advantage yeah here
and then this
one you now need to do this against
one you now need to do this against
returns I
believe
yes so
you should just
try no it has to be this
try no it has to be this
gaussian NL doesn't
it oh this is sketchy though isn't it
it oh this is sketchy though isn't it
let me think isn't this sketchy
yeah yeah this is
sketchy I think there is something
sketchy I think there is something
fundamental about training the value
fundamental about training the value
function to predict return instead of
function to predict return instead of
training them to reproduce a sequence of
training them to reproduce a sequence of
rewards right
need to do this more formally
yeah this is not right either so this is
sum
gamma
I
I so this is your discounted return
I think the the way you do value
I think the the way you do value
function is you do
function is you do
like slash math
GT what's what's the expectation
symbol math op math BB
do not need math
do not need math
op it's
op it's
this all
right suggest a fix with AI doesn't
right suggest a fix with AI doesn't
work this is is in some package isn't
it asms
fonts
oops see how they do it
oops see how they do it
why not use
why not use
obsidian I've never used obsidian I
obsidian I've never used obsidian I
don't
don't
know isn't it uh it's kind of nice to
know isn't it uh it's kind of nice to
have the lch for stuff to put around
places I think that's all we have to
import I thought obsidian isn't obsidian
import I thought obsidian isn't obsidian
like the main like note taking and thing
like the main like note taking and thing
that
application you vastly overestimate how
application you vastly overestimate how
organized I am
do this make sense
do this make sense
see we got to move this
down
Vantage it's going to
be for knowledge basis and note taking
be for knowledge basis and note taking
I see that's
cool just render lch I will check that
cool just render lch I will check that
out though I don't want to like I don't
out though I don't want to like I don't
want to recursively chain down while I'm
want to recursively chain down while I'm
doing
this you
this you
know oh we started with the code and
know oh we started with the code and
then I couldn't figure out that so I
then I couldn't figure out that so I
optimize the formula and I couldn't
optimize the formula and I couldn't
figure out how to write the formula
figure out how to write the formula
quickly soon enough I'm going to be
quickly soon enough I'm going to be
compiling the Linux
Tel okay yeah we have let's get rid of
Tel okay yeah we have let's get rid of
this cuz this one is this is the old
one so this is our minus B so t
your diagnostic IDE is
sharp it'll feel much better to be the
sharp it'll feel much better to be the
world expert in said topic
maybe in a few years takes
time I mean this is one of the other
time I mean this is one of the other
cool things about AI
cool things about AI
is you very often can finish your PhD
is you very often can finish your PhD
right and legitimately be the world
right and legitimately be the world
expert in a pretty decently sized area
there's a reason there isn't even
there's a reason there isn't even
anything remotely close to poer Li right
now H I think it should be like
l for
so this is a little tricky
so this is a little tricky
now I don't have any way to define the
now I don't have any way to define the
value loss do I
so I Define this Dynamic discount Factor
so I Define this Dynamic discount Factor
here I can Define the the return
here I can Define the the return
basically the way it is in
po I can Define the value function the
po I can Define the value function the
way it is in PPO I can Define the
way it is in PPO I can Define the
advantage function the way it is in po
advantage function the way it is in po
so I can optimize the policy
so I can optimize the policy
but then I'm stuck I think on the value
but then I'm stuck I think on the value
function because I can no longer apply
function because I can no longer apply
um gaan negative log likelihood loss to
um gaan negative log likelihood loss to
this thing because now I'm optimizing
this thing because now I'm optimizing
returns instead of Rewards
I can't just apply the um
yeah and I can't just put a uh squared
yeah and I can't just put a uh squared
error loss on it either because if you
error loss on it either because if you
do that then it will not
do that then it will not
optimize the sigma term which also needs
optimize the sigma term which also needs
to be
learned e
hang
on what if I
do
e
e e
does this do it
oops
let me see if I can get
let me see if I can get
uh let me see if I can figure something
uh let me see if I can figure something
out e
oops e
hey how's it
hey how's it
going we're doing pretty well here just
going we're doing pretty well here just
uh I'm just going to do something a
uh I'm just going to do something a
little silly with grock just to see if
little silly with grock just to see if
I'm I'm just basically going to sanity
I'm I'm just basically going to sanity
check what I did just did before I go
check what I did just did before I go
implement this and I'm hoping that I
implement this and I'm hoping that I
have this right and that this will be a
have this right and that this will be a
much
much
better uh implementation of the
better uh implementation of the
algorithm I've been trying to do
why do you get gamut in there twice
why do you get gamut in there twice
actually isn't that a little weird
oh yeah but most of these get cancelled
oh yeah but most of these get cancelled
don't
they you have any ex experience
they you have any ex experience
incorporating RL into multi-agent
incorporating RL into multi-agent
systems my thesis is in multi-agent RL
systems my thesis is in multi-agent RL
in fact probably the most most agent RL
in fact probably the most most agent RL
out
out
there
there
here we have right here for you several
here we have right here for you several
multi-agent environments including this
one here's a massively multi-agent
one here's a massively multi-agent
environment like a whole open world
environment like a whole open world
that's populated by hundreds of
that's populated by hundreds of
reinforcement learned agents that are
reinforcement learned agents that are
all interacting with each other and
all interacting with each other and
playing this like pretty fancy
game lots of stuff fr create a closed
game lots of stuff fr create a closed
loop system but it seems a bit ambitious
loop system but it seems a bit ambitious
kind of concerned it depends I have to
kind of concerned it depends I have to
look at the precise problem um if you
look at the precise problem um if you
just want to get like the gist of stuff
just want to get like the gist of stuff
right we've got uh the Discord and you
right we've got uh the Discord and you
can like you know put some stuff in
can like you know put some stuff in
there and I can like spot check it or
there and I can like spot check it or
other people in there can spot check and
other people in there can spot check and
see if it makes sense the way it's set
see if it makes sense the way it's set
up um and I will say well all of our
up um and I will say well all of our
stuff is free and open source for
stuff is free and open source for
companies we also do have a priority
companies we also do have a priority
service where you can just hire us to
service where you can just hire us to
make sure basically that all your RL is
make sure basically that all your RL is
going sane and to integrate our tools to
going sane and to integrate our tools to
make them work for
make them work for
you multi-agent l okay llms which can do
you multi-agent l okay llms which can do
tool calling that's a little bit
tool calling that's a little bit
different um I can still sanity check
different um I can still sanity check
that for you though because if you're
that for you though because if you're
applying RL separately to different
applying RL separately to different
models that does get a bit dicey and the
models that does get a bit dicey and the
reasons are basically the same as they
reasons are basically the same as they
would be in standard RL
the main issue with
the main issue with
um again it depends how you're doing it
um again it depends how you're doing it
if you have say different prompts or
if you have say different prompts or
something and you're using all of the
something and you're using all of the
training data for uh all the models so
training data for uh all the models so
basically you have one model with
basically you have one model with
different prompts and you're using all
different prompts and you're using all
the data on the model that's kind of
the data on the model that's kind of
reasonable but if you have a bunch of
reasonable but if you have a bunch of
different agents and you're splitting
different agents and you're splitting
the data across them that gets more
the data across them that gets more
expensive proportional to the number of
expensive proportional to the number of
age UPS so then the whole benefit of
age UPS so then the whole benefit of
like oh this is nice and scalable goes
like oh this is nice and scalable goes
away because it also gets scalably more
away because it also gets scalably more
expensive
poms doing different tasks and breaking
poms doing different tasks and breaking
them
them
down different promps doing
down different promps doing
different yeah I mean that's just
different yeah I mean that's just
Distributing work like
that's just Distributing work it depends
that's just Distributing work it depends
how you're setting it up
how you're setting it up
so yeah if you have more details on
so yeah if you have more details on
stuff drop in the Discord and our stuff
stuff drop in the Discord and our stuff
our tools are not
our tools are not
specifically made for uh llms in an RL
specifically made for uh llms in an RL
settings uh not that they don't work for
settings uh not that they don't work for
that it's just that uh llms are so slow
that it's just that uh llms are so slow
that the performance optimizations that
that the performance optimizations that
we've done in puffer lib right how
we've done in puffer lib right how
puffer lib is so fast your model is so
puffer lib is so fast your model is so
big that your whole thing is going to
big that your whole thing is going to
run very slowly and be expensive no
run very slowly and be expensive no
matter what I do uh that's the main
matter what I do uh that's the main
issue there you can actually
issue there you can actually
see this is one of the agents that we
see this is one of the agents that we
have this is like a tiny little agent
have this is like a tiny little agent
running on one CPU in your browser
running on one CPU in your browser
that's here like kiting these agents
that's here like kiting these agents
trying not to get
trying not to get
caught ah and he got caught right at the
caught ah and he got caught right at the
end because he didn't see that they were
end because he didn't see that they were
going to be they were both going to hit
going to be they were both going to hit
him at the same
him at the same
time oh yeah there you
time oh yeah there you
go oh that's a bug
go oh that's a bug
yeah it didn't reset the element here
yeah it didn't reset the element here
that hair color should have shifted back
that hair color should have shifted back
to White well there's much more coming
to White well there's much more coming
with neurl Mo soon
anyways maybe I should just throw a
anyways maybe I should just throw a
little llm interface on puffer at some
little llm interface on puffer at some
point because it would be super easy and
point because it would be super easy and
probably people would like
it but it's really not the poor thing I
it but it's really not the poor thing I
want to do with Puffer for
it's more than a norm now Captain so uh
it's more than a norm now Captain so uh
again like basically anything I do now
again like basically anything I do now
still trains but I think that we're
still trains but I think that we're
going to have to
going to have to
do uh we're going to have to do
do uh we're going to have to do
something a little bit fancier
something a little bit fancier
here oh is
this
e
e
e
e e
okay there we go now
I think you can simplify now can't
you
GMA
e
e e
so uh for the folks that just joined on
so uh for the folks that just joined on
YouTube
YouTube
I've come to these formulas here so far
I've come to these formulas here so far
this is um I'm attempting to replace
this is um I'm attempting to replace
generalized Advantage estimation in RL
generalized Advantage estimation in RL
with a learnable form of it that will be
with a learnable form of it that will be
much more flexible and not have two
much more flexible and not have two
really fiddly hyper
parameters so I have these forms and I'm
parameters so I have these forms and I'm
just trying to sanity check and see if
just trying to sanity check and see if
anything simplifies or if there's you
anything simplifies or if there's you
know anything I can do
okay so this is what I was going for
here so I didn't actually give it this
here so I didn't actually give it this
formula and it came up with it so I
formula and it came up with it so I
think that unless I've like steered it
think that unless I've like steered it
into something wrong I should hopefully
into something wrong I should hopefully
be
be
I should hopefully have it correct
here
e e
that's inversely to the variance at yes
and then we have finite
and then we have finite
Horizon waiting
Horizon waiting
yes value
Target then why is open
truncated weight sum RT might be a
truncated weight sum RT might be a
scaler summing over H time St
scaler summing over H time St
yes reducing is reduced to a scaler yes
yes reducing is reduced to a scaler yes
this
is infinite tail that's
is infinite tail that's
fine value loss
fine value loss
modeling
TT this trains B to
predict
R oh I have this slightly off so this
R oh I have this slightly off so this
should
should
be let me fix
be let me fix
this wait do I no this is
correct this
trains let me
see R Plus
VT +
VT +
one yes that's correct it's train to
one yes that's correct it's train to
predict the reward plus the value at the
predict the reward plus the value at the
next step that's
next step that's
correct return and
correct return and
[Music]
[Music]
Advantage subing this we get that no
Advantage subing this we get that no
simplification
yep lack of cumulative gamma products
yep lack of cumulative gamma products
distinguish it well because it's
distinguish it well because it's
adaptive
right okay I think that this
right okay I think that this
is there no glaring errors that I can
is there no glaring errors that I can
see here
right
e
e e
Oh shoot no this is not going
Oh shoot no this is not going
to is this going to work
I don't know if the last thing is
I don't know if the last thing is
correct
negative gamma might be okay we might
negative gamma might be okay we might
have to clip this
have to clip this
yeah I think we do clip it already
yayer return over H
yayer return over H
steps yes
the loss assumes BT minus BT + 1 is
the loss assumes BT minus BT + 1 is
approximately equal to
approximately equal to
RT
RT
yeah this holds if VT predicts One Step
yeah this holds if VT predicts One Step
returns
returns
iteratively but your target is RT this
iteratively but your target is RT this
is a subtle mismatch
is a subtle mismatch
okay if VT targets
okay if VT targets
RT which it
RT which it
does then VT minus VT + 1 = R doesn't
does then VT minus VT + 1 = R doesn't
naturally
follow unless H equal 1 or
B why not
okay so they're saying
okay so they're saying
here to just
make yeah that's
tricky so we we'll go into this is there
tricky so we we'll go into this is there
anything
anything
else so this is the
else so this is the
issue
e
e
e e
you know this is the one thing that I
you know this is the one thing that I
actually
actually
find lm's useful for is just
find lm's useful for is just
troubleshooting troubleshooting stuff
troubleshooting troubleshooting stuff
like
like
this they don't really have great ideas
this they don't really have great ideas
about anything
but just like doing algebra and doing
but just like doing algebra and doing
symbol manipulation actually it's not
symbol manipulation actually it's not
terrible which is kind of ironic because
terrible which is kind of ironic because
this is kind of one of the places you
this is kind of one of the places you
would expect a symbolic system to be
would expect a symbolic system to be
better at um doing the algebra but I
better at um doing the algebra but I
don't know it's kind of
don't know it's kind of
funny it's also just possible though and
funny it's also just possible though and
probably more likely that it's just not
probably more likely that it's just not
all that good and it's just that I'm a
all that good and it's just that I'm a
much better programmer than I am a
much better programmer than I am a
mathematician so like this thing's
mathematician so like this thing's
really really dumb when it comes to
really really dumb when it comes to
programming to me but much better when
programming to me but much better when
it comes to math because my math
it comes to math because my math
sucks but hey it gets
done e
doesn't match your multistep intent R
doesn't match your multistep intent R
plus gamma
plus gamma
VT plus one doesn't that look familiar
it's much harder to estimate the
it's much harder to estimate the
uh standard
uh standard
deviation of returns isn't it
Bing to the return feels more
coherent okay
so I mean we have a couple different
so I mean we have a couple different
options here
right uh I suppose the other one would
be what is actually the
difference just r
I think I need to understand this better
this satisfies the Bellman equation
right gamma vs
yeah gamma is one horizon is finite just
yeah gamma is one horizon is finite just
expected return yes okay
you want V to estimate weighted return
you want V to estimate weighted return
over H depths
over H depths
yes trains VT minus t +1 to match
yes trains VT minus t +1 to match
RT suggesting that VT is approximately
RT suggesting that VT is approximately
equal
equal
to r + BT + 1
to r + BT + 1
yes a one-step
prediction VT estimates the immediate
prediction VT estimates the immediate
reward plus the value of the next
state if VT is equal to
state if VT is equal to
[Music]
[Music]
RT is equal to the sum
it's the return from T to t plus
it's the return from T to t plus
h
right this equals R only if gamma equals
right this equals R only if gamma equals
oh so they're just saying that there is
oh so they're just saying that there is
a
a
um a gamma term
on they're just saying that there's a
on they're just saying that there's a
gamma term on the first one
right yeah this just says that there's
right yeah this just says that there's
because there's a gamma term on the
because there's a gamma term on the
first
one e
multistep
multistep
return VT consistent with
return VT consistent with
rt it shift Sigma to standard deviation
rt it shift Sigma to standard deviation
of
returns well you could just divide by
returns well you could just divide by
gamma T
right for
was a difference wait what how's this
was a difference wait what how's this
happen
how the hell did I not wait what how did
how the hell did I not wait what how did
this not work
e
e
minus BT +
1 + H + 1 e
okay you could also do this
okay you could also do this
cool
td0 wait your suggestion adjust the TD
td0 wait your suggestion adjust the TD
residual by scaling with L by gamma
residual by scaling with L by gamma
T So value is equal
to gamma RT okay well that's is that
to gamma RT okay well that's is that
what we
want that is what we want isn't
it e
gamma HT plus r it's got to be equal to
gamma HT plus r it's got to be equal to
zero
One Step a turn waited
by well this is kind of zero isn't it
the turn is equal
to gamma
RT plus gamma t + 1 VT
oh yeah now this is be this is going to
oh yeah now this is be this is going to
become a problem as well isn't
[Music]
[Music]
it no we can't do this because
yeah we can't do
this because the whole point is to take
this because the whole point is to take
into account the gamas as the waiting
into account the gamas as the waiting
Factor
true or do a learnable g this is
true okay so let me think about this
then
then
value squared
this is going to be our standard
this is going to be our standard
deviation like
this and these depend on each other
this and these depend on each other
great
lovely e
hold on maybe not
the standard deviation of returns
the standard deviation of returns
now no these depend on each other
for
e e
gamma must reflect the variance the
gamma must reflect the variance the
returns not the
reward to find the Baseline is the
reward to find the Baseline is the
variance of the uninformed
variance of the uninformed
return assume RI has variance R st R
return assume RI has variance R st R
STD
STD
squared uninformed predictor weight all
squared uninformed predictor weight all
terms
equally variance of R
uninformed but since gamma
uninformed but since gamma
varied let's tie the Baseline to the
varied let's tie the Baseline to the
expected variance of R under the current
expected variance of R under the current
gamma
this is still circular
lovely with Decay we're not adding
Decay or you boot strap
Sigma rstd equals the sum of rftd
squ this thing's a
squ this thing's a
mess this thing is a mess
what the hell is this that it's giv me
what the hell is this that it's giv me
this mess
yeah I don't know what it's doing
there is there something we can do with
there is there something we can do with
this instead
this instead
let
so
e
e e
see what if we write this
out
e
e e
did I do that
right
e
e e
the
the
hell oh no wait this is
hell oh no wait this is
gamma this is supposed to
gamma this is supposed to
be R yeah yeah
yeah e
oh I am confused I will be back in a bit
oh I am confused I will be back in a bit
uh I'm going to just walk around do a
uh I'm going to just walk around do a
couple things clear my head I'll be back
couple things clear my head I'll be back
after I don't know 10 minutes or
after I don't know 10 minutes or
something and then uh we will see if I
something and then uh we will see if I
can finish this I don't want to spend
can finish this I don't want to spend
all day on this but like we are making
all day on this but like we are making
some progress so it's good I'll be
back
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e
we are back
oh wait what the heck did I do
oh wait what the heck did I do
here I uh yeah that's not how math Works
here I uh yeah that's not how math Works
what did I
do yeah that's not how math works at all
do yeah that's not how math works at all
I don't know why I did that
there we go
yeah that doesn't really help me
well I could do
yeah okay that's just bad math um I
yeah okay that's just bad math um I
could replace
RI
e
e
e e
what squared
yeah e
I'm so confused by
this
yeah wait
what one over
what the [ __ ] does this
mean no bad why did it do this
ah okay this will mess up I
see
e
e e
this Baseline term is going to be a pain
this Baseline term is going to be a pain
in the ass
I don't know I still don't think that
I don't know I still don't think that
this works though
yeah this is getting
confused
e e
why is this so hard what is different
why is this so hard what is different
about this from before
before the value function was just
before the value function was just
predicting the next
reward so it was easy to just sub in the
reward so it was easy to just sub in the
variance of the reward as a
baseline now the value function
is now the value function is predicting
the the discounted sum of returns
hang on I think I know how to do
it
e e
what's up here
what do we have here we
have
e e
picky I just need a good Baseline on
this
this
okay I think this works
VT is a
scalar VT outputs a vector of
scalar VT outputs a vector of
predictions yes it
does use Sigma squ T consistently what
does use Sigma squ T consistently what
did I do
what is
thisal T I don't know why it would be T
thisal T I don't know why it would be T
here
Gan or Sigma squ T
Gan or Sigma squ T
equals what the
heck e
scaler predicting RT okay let me I have
scaler predicting RT okay let me I have
to just clarify this
actually does it even make sense as a
actually does it even make sense as a
vector anymore let me
think what are we using we're using
think what are we using we're using
[Music]
[Music]
Sigma Sigma squar
okay
e
e
e
e e
I think I should try this
now wait
what no this doesn't make any sense
anymore I need to start like actually
anymore I need to start like actually
doing this stuff this is going to drive
doing this stuff this is going to drive
me crazy this thing just keeps like
me crazy this thing just keeps like
forgetting the way that symbols are
defined
e e
I think you can just do it in here now
right e
hey
e e
no this goes at the very
no this goes at the very
end this goes at the very end
right or does it
not no actually so complicated so
not no actually so complicated so
freaking
complicated e
there do that
and what the heck do we use for the um
damn it what are we for
return that's the buffer
yeah it
is so I guess we have to go backwards
is so I guess we have to go backwards
over this
yeah we do
okay
e
e e
what is it vstd
minus
minus
[Music]
r there
right does that do anything though hang
on [ __ ] no this is still freaking wrong
on [ __ ] no this is still freaking wrong
isn't it
this is so incredibly
this is so incredibly
confusing I'm getting confused here
confusing I'm getting confused here
because I think I need I need the
because I think I need I need the
standard
standard
deviation the estimate of the standard
deviation the estimate of the standard
deviation of the return don't
deviation of the return don't
I so hang on hang on maybe
it's I don't know how you do
it's I don't know how you do
that the math is just tricky
here e
how does this even make
sense wait maybe it does a telescope
damn it this is so obnoxious
what the reason I can't use the original
what the reason I can't use the original
G
G
here there a
reason yeah the reason is that the Decay
reason yeah the reason is that the Decay
Factor doesn't go to zero
Factor doesn't go to zero
right yeah the Decay Factor goes to uh
the def the Decay Factor goes to what
the def the Decay Factor goes to what
does it even go to that's the
problem you've got a chicken and egg
problem you've got a chicken and egg
problem here
is this resolv just by subtracting the
is this resolv just by subtracting the
mean I don't think so right
so if I write it without this value here
so if I write it without this value here
for a second right
that's not what it is is
it hang
on maybe it
is for
isn't this it right like
this it's just the sum
of reward over value standard
deviation and then possibly there's a
deviation and then possibly there's a
term at the end that we account
term at the end that we account
for but this is pretty well
for but this is pretty well
it
it
okay so then what we need
is the problem is this vstd doesn't go
is the problem is this vstd doesn't go
to zero
right actually you definitely need the
right actually you definitely need the
bootstrap term here otherwise you're
bootstrap term here otherwise you're
screwed aren't you
oh maybe that is the Baseline that you
oh maybe that is the Baseline that you
need though hold on
do you subtract values
not
not
easy not easy at
all the tricky thing here right is just
all the tricky thing here right is just
understand
the difference
the difference
between how standard deviations would be
between how standard deviations would be
predicted for uh for rewards versus for
predicted for uh for rewards versus for
returns which is sum of
returns which is sum of
discounted return uh sum of discounted
discounted return uh sum of discounted
rewards so
yeah this definitely needs to get
normed is this D deviation predicting
maybe I can just do it
now this is like very very complicated
now this is like very very complicated
and annoying here
um so this is what I
proposed standard deviation of
VT
okay BT is a scalar predicting the full
okay BT is a scalar predicting the full
return from T to t +
H sum of Sigma
I but R uses this per time step
yes
e
e
e e
certainty
yeah R is a
scaler predicts each
scaler predicts each
term and be
some
e
e e
if I can get this right then we will
if I can get this right then we will
basically have a a
basically have a a
generalized what do we call it
generalized what do we call it
generalized generalized Advantage
estimation
GGA but I have to get the formula right
GGA but I have to get the formula right
and this is driving me
nuts for
maybe we
have t is a
have t is a
vector no
RT
turn for
I think that makes
sense Z
minus
zero e
let's see if it gets the formulas
let's see if it gets the formulas
here the way that I have
them
okay so we have this
okay so we have this
formula RT of
k t +
k t +
k t + k + hus one sure
squ
yes does it
hold when Sigma is equal to
R but this is the tricky thing here
the tricky part that I can't figure out
the tricky part that I can't figure out
here is that this where is it this
here is that this where is it this
is an estimate
of of the return
right e
It's Tricky here because like I'm trying
It's Tricky here because like I'm trying
to hit a moving
Target the what is the standard
Target the what is the standard
deviation of the
return so if you don't discount it that
return so if you don't discount it that
goes to Infinity
goes to Infinity
right
yeah
yeah
I damn I don't think this
works do I have to go all the way back
works do I have to go all the way back
to
to
the I mean I had a form of this that
the I mean I had a form of this that
made a lot more sense to me
I think that one actually still didn't
I think that one actually still didn't
work right
now you do want to estimate returns is
now you do want to estimate returns is
the
thing you do need to estimate returns
thing you do need to estimate returns
and I think the reason that you need to
and I think the reason that you need to
estimate returns right is you're not
estimate returns right is you're not
going to ever really know exactly when
going to ever really know exactly when
you're going to get a reward many steps
you're going to get a reward many steps
out you're just going to roughly know
out you're just going to roughly know
that you're going to get a reward so the
that you're going to get a reward so the
single step bootstrap one doesn't work
single step bootstrap one doesn't work
very
very
well what I was trying to do
well what I was trying to do
before do I still have it here yeah I
before do I still have it here yeah I
was trying to do this
but I don't actually think that this
works well because then this has to be a
works well because then this has to be a
prediction of the reward variance right
does that make sense hang
on now I think that what this
on now I think that what this
says I don't know you can correct me if
says I don't know you can correct me if
I'm wrong here if anybody knows any math
I'm wrong here if anybody knows any math
but so this portion of the formula is
but so this portion of the formula is
kind of clever because this allows you
kind of clever because this allows you
to predict the full returns which should
to predict the full returns which should
be much easier to learn uh discounted
be much easier to learn uh discounted
return should be easier to learn than
return should be easier to learn than
direct prediction of value because you
direct prediction of value because you
only need to predict how much you're
only need to predict how much you're
going to get total in the future you
going to get total in the future you
don't have to predict like exactly when
don't have to predict like exactly when
you're going to get each each
you're going to get each each
amount and it's
amount and it's
discounted uh but then you still end up
discounted uh but then you still end up
with the sigma squar is the sigma squar
with the sigma squar is the sigma squar
of this term
of this term
which is basically an estimate of reward
which is basically an estimate of reward
so you're still
predicting the uncertainty of each
predicting the uncertainty of each
individual reward okay so it actually it
individual reward okay so it actually it
does make more sense to just cut this
does make more sense to just cut this
term out and have VT RT Sigma squ uh
term out and have VT RT Sigma squ uh
Sigma t^
Sigma t^
squ but then if you do
that I mean we're stuck at the same
that I mean we're stuck at the same
problem the problem is this term right
problem the problem is this term right
here how to Baseline it when you just
here how to Baseline it when you just
had uh when you were predicting the
had uh when you were predicting the
rewards instead of the returns it's very
rewards instead of the returns it's very
easy to get the standard
easy to get the standard
deviation of the uh of the reward it's
deviation of the uh of the reward it's
not easy to get the standard deviation
not easy to get the standard deviation
of the
of the
return
now e
subtract what would be a thing we can
subtract what would be a thing we can
subtract from
this if we subtract
mean gamma R mean over our standard
mean gamma R mean over our standard
deviation
how do we estimate
the the mean
return is it just gamma R
mean does that do it
instead of the standard deviation we get
instead of the standard deviation we get
the mean reward does that do it
I don't think that does
it I'm just getting hung up on basic
it I'm just getting hung up on basic
stats
is there anything
here now there's nothing there I have to
here now there's nothing there I have to
just figure it out
squared one over Sigma squ is going to
squared one over Sigma squ is going to
be the waiting right
hang on maybe I do
know maybe I have an idea
let's do this one
let's do this one
here gamma
here gamma
I so this is just going to be
I so this is just going to be
right I
right I
over
Sigma Sigma eyes squar or Sigma is it I
Sigma Sigma eyes squar or Sigma is it I
yes I
yes I
squared and
then minus sum
I mess up here
I mess up here
[Music]
the heck happened up
here okay r i over Sigma IUS
H minus
I going on a small
I going on a small
run
enjoy alumni rug rugby game good
enjoy alumni rug rugby game good
luck will you be deving tonight I'll
luck will you be deving tonight I'll
probably be deving all
day depends how much math I get stuck
day depends how much math I get stuck
doing
providing the funds for the
providing the funds for the
party good
luck e
maybe it is an
armu yeah cuz it's supposed to be okay
the thing is this is still the stand
the thing is this is still the stand
deviation of the reward right not the
return I'm I think I'm also just
return I'm I think I'm also just
missing the value bootstrap
right because this needs to
be
e e
if we just cover up this last CH
here then this sum
explodes
e e
what's the end goal are you trying to
what's the end goal are you trying to
make this algorithm no this is a new
make this algorithm no this is a new
algorithm um the issue I'm having at the
algorithm um the issue I'm having at the
moment this is a new algorithm that
moment this is a new algorithm that
replaces generalized Advantage
replaces generalized Advantage
estimation but the issue that I'm having
estimation but the issue that I'm having
is that
is that
um the in generalized Advantage
um the in generalized Advantage
estimation the series eventually uh
estimation the series eventually uh
converges to zero so it doesn't explode
converges to zero so it doesn't explode
this one converges to the variance of
this one converges to the variance of
the underlying distribution so it will
the underlying distribution so it will
explode if I don't subtract the variance
explode if I don't subtract the variance
but I need the variance to compute the
but I need the variance to compute the
formula and I need the formula to
formula and I need the formula to
compute the variance so I'm trying to
compute the variance so I'm trying to
figure out how I can Baseline this
thing e
you think it will that it will well this
you think it will that it will well this
is new like it's not going to work the
is new like it's not going to work the
way that I have it written at the moment
way that I have it written at the moment
um the thing that happened that's
um the thing that happened that's
annoying is I was doing single reward
annoying is I was doing single reward
predictions before and I'm trying to
predictions before and I'm trying to
make it predict returns now when you're
make it predict returns now when you're
doing single reward predictions you can
doing single reward predictions you can
just take the variance of the reward
just take the variance of the reward
signal like the reward and the data um
signal like the reward and the data um
oh hey car how's it
oh hey car how's it
going uh you can just take the variance
going uh you can just take the variance
of the reward in the data for that but
of the reward in the data for that but
when you have the full return estimates
when you have the full return estimates
there's not really a way to get the
there's not really a way to get the
estimate of the variance of the returns
estimate of the variance of the returns
so yeah I'm having a tough time
so yeah I'm having a tough time
baselining this this is the thing that's
baselining this this is the thing that's
annoying
this should explode shouldn't
it yeah Sigma I is because the thing of
it yeah Sigma I is because the thing of
Sigma
Sigma
I um this needs to be Baseline because
I um this needs to be Baseline because
this is going to converge to the
this is going to converge to the
variance of the discounted
variance of the discounted
[Music]
return maybe I'm still not understanding
return maybe I'm still not understanding
this
this
correctly so
okay let's say that I
okay let's say that I
compute Let's ignore and I'm actually
compute Let's ignore and I'm actually
going to take this term off for a second
going to take this term off for a second
because it's just it's confusing me even
because it's just it's confusing me even
to have to look at this
to have to look at this
thing okay is
thing okay is
it is it percent yeah there we
it is it percent yeah there we
go
go
so if I compute this over every term
this converges to
this converges to
a does this converge to a constant what
a does this converge to a constant what
is
this value function
this value function
right so reward over Sigma
right so reward over Sigma
squ and the value function is pret is
squ and the value function is pret is
trained to predict this
trained to predict this
is trying to predict this
right this works in
right this works in
J because this goes to
J because this goes to
zero it does I
zero it does I
don't it's very difficult to think about
don't it's very difficult to think about
this though it's it's very difficult to
this though it's it's very difficult to
think about this for some
reason CU this isn't a constant it's a
reason CU this isn't a constant it's a
learnable parameter so I can't really
learnable parameter so I can't really
tell how it's going to shift
this made so much more sense when Sigma
this made so much more sense when Sigma
was an estimate of the a standard
was an estimate of the a standard
deviation of the reward instead of the
deviation of the reward instead of the
return
If This Were a fixed gamma how does it
If This Were a fixed gamma how does it
change right if this were a fixed
scamma it actually does change quite a
scamma it actually does change quite a
bit now I'm looking at
it oh well that's an interesting
it oh well that's an interesting
Discovery hang
Discovery hang
on
on
huh I think think I just figured out why
huh I think think I just figured out why
um why learning rate and uh well why
um why learning rate and uh well why
gamma and Lambda feel like such screwy
gamma and Lambda feel like such screwy
parameters because gamma doesn't
parameters because gamma doesn't
actually Decay to zero over the horizons
actually Decay to zero over the horizons
that you're looking
that you're looking
at so when you like when you just change
at so when you like when you just change
the effective
the effective
Horizon
Horizon
um yeah when you just change the
um yeah when you just change the
effective Horizon that they're computed
effective Horizon that they're computed
over does that make sense
God I'm just confusing the hell out of
God I'm just confusing the hell out of
myself today
yeah I don't think any of this has
yeah I don't think any of this has
solved the problem at all
solved the problem at all
if not one
if not one
bit I don't even know if there is a
bit I don't even know if there is a
solution to
this e
okay so yeah this is what we have
yeah but this doesn't do it I don't
yeah but this doesn't do it I don't
think right
yeah so this this would work but then
yeah so this this would work but then
this fundamentally
changes for
we could just try it and see what
happens
e e
I'm going to just try the original thing
I'm going to just try the original thing
because this is driving me nuts
crazy I mean the issue here is I know
crazy I mean the issue here is I know
how to do this with one step predictions
how to do this with one step predictions
I know how to do this bootstrapped it's
I know how to do this bootstrapped it's
very
very
easy I don't think it works anywhere
easy I don't think it works anywhere
near as well
near as well
though because I the key Insight here is
though because I the key Insight here is
it is very difficult to
it is very difficult to
predict um it's very difficult to
predict um it's very difficult to
predict values and time steps ahead
predict values and time steps ahead
exactly but if you you predict the sum
exactly but if you you predict the sum
then it doesn't matter quite where they
then it doesn't matter quite where they
appear it just it's just like it's the
appear it just it's just like it's the
difference of saying hey uh when are you
difference of saying hey uh when are you
going to score points exactly in the
going to score points exactly in the
next 32 steps versus how many points you
next 32 steps versus how many points you
going to
score to be fair I don't think I've
score to be fair I don't think I've
tried the
tried the
uh the original
well the original is also screwy for
well the original is also screwy for
other reasons isn't
it or is it
not this predicts variance of this
well let's finish this
first e
okay R minus value means to
okay R minus value means to
zero something like
this reward block your value
this reward block your value
in value
variance just try to get this to
run e
all so this should run hopefully
what's wrong
what's wrong
here illegal member access
here illegal member access
really what' I do
I times
fren oh that's not
right
e e
it would be great if this thing would
it would be great if this thing would
tell me
where see what code I had before
jez okay index is defined right
reward block
mask
mask
[Music]
advantages of I right
the heck I don't see anywhere where I'm
the heck I don't see anywhere where I'm
indexing
indexing
badly I don't see anywhere
hold
hold
on
K J L
Horizon I + J
that looks like out of bounds to
me well this is minus
minus for
be about this me
hey Spen sir
hey Spen sir
going freaking crazy
going freaking crazy
here over
here over
uh bunch of confusing map always fun
I'm tempted to just do it the other way
I'm tempted to just do it the other way
that I know should actually work
that yeah I have no idea what this is
that yeah I have no idea what this is
doing clearly not
working okay I think what I'm going to
working okay I think what I'm going to
do because this is going to drive me
do because this is going to drive me
nuts I'm going to keep this
nuts I'm going to keep this
uh but I'm going
uh but I'm going
to copy
this yeah I think GP I think
this yeah I think GP I think
uh as always I think that grock or the
uh as always I think that grock or the
llm LED me down a really really freaking
llm LED me down a really really freaking
stupid path cuz I just there's no way to
stupid path cuz I just there's no way to
get a baseline for this thing it's just
get a baseline for this thing it's just
going to explode there's absolutely no
going to explode there's absolutely no
way I can see to get a baseline for
way I can see to get a baseline for
this so if we go back to this one
here it's much easier isn't
it
e for
that pisses me off
okay yes so this is where we
started and and now I
think advantages equals
Advantage Advantage Plus equal to
this one should be easier to figure out
this one should be easier to figure out
by far
now it's just some
of some of rewards
over over Sigma
squar yeah hang on this is easier now
right you do need to
right you do need to
store the
discounted yeah this now becomes the
returns
right these d 10 towards zero
now index isal to the scale
so it's just going to
be buffer of index
lock
Index this is now return
okay I think that's
okay I think that's
it because now you're Computing
it because now you're Computing
discounted
discounted
return
right and if you have Sigma squar is the
right and if you have Sigma squar is the
sigma of of the reward not the return
sigma of of the reward not the return
which I think it can
do then I should just have to go to
do then I should just have to go to
this this gets new value
this this gets new value
mean w block
mean w block
variance uh but I this now needs to be
variance uh but I this now needs to be
slightly slightly
altered which project is the most
altered which project is the most
interesting and
interesting and
useful out of what
um 3D
um 3D
reconstruction and
reconstruction and
editing continual
editing continual
learning 3DC understanding zero shot
learning I mean you've just given me a
learning I mean you've just given me a
you've given me a whole bunch of very
you've given me a whole bunch of very
broad
topics continue learning and 3D scene
topics continue learning and 3D scene
understanding are like very disjoint
understanding are like very disjoint
zero shot has a ton of comp uh a ton of
zero shot has a ton of comp uh a ton of
different areas compositional zero shot
different areas compositional zero shot
I don't even know what that means 3D
I don't even know what that means 3D
reconstruction is the only one of those
reconstruction is the only one of those
that's like a somewhat narrowly defined
that's like a somewhat narrowly defined
area
yeah but like continual learning and 3DC
yeah but like continual learning and 3DC
and understanding
and understanding
aren't like yeah you're like you give
aren't like yeah you're like you give
um those are like huge you have like
um those are like huge you have like
huge areas
huge areas
here these aren't projects you specified
here these aren't projects you specified
like sub fields
I also don't even know what zero shot
I also don't even know what zero shot
learning is because it's like zero shot
learning is because it's like zero shot
implies not learning
continual human pose
continual human pose
estimation key points and pose
variations
variations
oh yeah well that's not what continual
oh yeah well that's not what continual
learning is at
learning is at
all um if you're going to do that
all um if you're going to do that
specifically then
no key
points CL
estimation H es yeah I
don't I think you're going to have to
don't I think you're going to have to
narrow some stuff
narrow some stuff
because things are IL defined mind here
because things are IL defined mind here
so is based
so is based
on continual human pose
on continual human pose
estimation okay I know what POS
estimation okay I know what POS
estimation is I don't know what
estimation is I don't know what
continual pose estimation means
continual pose estimation means
right for incremental integration of key
right for incremental integration of key
points and POS isn't that just posst
points and POS isn't that just posst
estimation
estimation
like yeah some stuff seems IL defined
like yeah some stuff seems IL defined
here
just shift
these I think I do just shift these
these I think I do just shift these
don't I
BT other way around
does this do anything for us
we will
see whole bunch of notifications
compositional zero shot
compositional zero shot
the model trained on individual
the model trained on individual
components and then recognize novel
components and then recognize novel
combination of these components at test
time so
like oh when did they do
like oh when did they do
this March oh they just did this how did
this March oh they just did this how did
I not see this we build large scale
I not see this we build large scale
multitask for in context r
multitask for in context r
it took 50,000 hours to collect this
it took 50,000 hours to collect this
okay well that's kind of
okay well that's kind of
sad it's funny they're still doing
that but yeah this
that but yeah this
one oh not this
one oh not this
one yeah this
one yeah this
one you'd probably look at this
okay so this totally failed as
well let's see what's going wrong
and I'm going to have to get some food
and I'm going to have to get some food
soon because my brain is not working
soon because my brain is not working
very well right
now that actually kind of gives you
now that actually kind of gives you
something reasonable right
something reasonable right
standard deviation is
reasonable that seems
suspicious yeah so this thing's exploded
oh hang on is this
oh hang on is this
not yeah you can't use this this has to
be have you worked with diffusion models
be have you worked with diffusion models
before
nope when I did CV um when I was in CV
nope when I did CV um when I was in CV
it was before diffusion models became a
thing
e e
that's it
stupid thing hate want to just break my
stupid thing hate want to just break my
uh my
shell field changes really
shell field changes really
fast well there hasn't really been a
fast well there hasn't really been a
major Improvement to uh core RL
major Improvement to uh core RL
algorithm since 2017 arguably 2015 so
algorithm since 2017 arguably 2015 so
that's what I'm trying to do I guess
that's what I'm trying to do I guess
2017 it would be
okay so here you have your advantages
interesting this is very weirdly
interesting this is very weirdly
scaled but this is roughly what you like
scaled but this is roughly what you like
you want something that looks like this
you want something that looks like this
it's just very weirdly
scaled uh this as well
yeah so this is
exploding well this is irritating um the
exploding well this is irritating um the
problem is I can't just be doing this
problem is I can't just be doing this
all day all day because I have tons of
all day all day because I have tons of
other stuff to be getting on to as well
other stuff to be getting on to as well
I think I'm going to go get some lunch
I think I'm going to go get some lunch
and then I'm going to think about this
and then I'm going to think about this
and I might just have to to go on to
and I might just have to to go on to
exploration side research for a
exploration side research for a
bit um this is going to take a while and
bit um this is going to take a while and
this is going to drive me
nuts Yeah It's just
tough so I suppose I will think about
tough so I suppose I will think about
this for a bit I'm going to get some
this for a bit I'm going to get some
food clear my head a little bit maybe
food clear my head a little bit maybe
and then I will be back
and then I will be back
afterwards and uh we'll be on research
afterwards and uh we'll be on research
for the whole rest of the day so uh for
for the whole rest of the day so uh for
folks watching now all my stuff at
folks watching now all my stuff at
puffer a are you still a PhD candidate
puffer a are you still a PhD candidate
nope I've graduated I've been working on
nope I've graduated I've been working on
puffer full-time for almost a year now
puffer full-time for almost a year now
made a lot of good
made a lot of good
progress um all my stuff's at puffer
progress um all my stuff's at puffer
doai you can check out what I've been
doai you can check out what I've been
doing lately just St the repo here go in
doing lately just St the repo here go in
Discord if you want to get involved with
Discord if you want to get involved with
Dev and uh follow on X for more
Dev and uh follow on X for more
content thank you and

Kind: captions
Language: en
okay we are
live he didn't update my titles it's
live he didn't update my titles it's
modly
annoying all right well maybe that
annoying all right well maybe that
worked we'll
see cool um um we have a lot of stuff to
see cool um um we have a lot of stuff to
do
today this is still running
somehow let's check the old experiments
somehow let's check the old experiments
I don't have super high hopes for
this I mean that's not even that bad
this I mean that's not even that bad
honestly
that's really not even that
that's really not even that
bad okay so um it's the thing is
bad okay so um it's the thing is
definitely way worse than
definitely way worse than
before um
before um
so we need to do we need to make a
so we need to do we need to make a
modification of this algorithm the plan
modification of this algorithm the plan
for today and hang on let me get this
for today and hang on let me get this
window open and then I'll go through the
window open and then I'll go through the
plan for
plan for
today oh let me also just make sure it
today oh let me also just make sure it
didn't screw up my
didn't screw up my
mic
mic
welcome let me make sure it's using my
welcome let me make sure it's using my
correct mic yes it
correct mic yes it
is okay so here's the plan for
is okay so here's the plan for
today last night I came up
today last night I came up
with some better math
for the algorithm we should open
po oh no it's not this paper actually
po oh no it's not this paper actually
it's the J
paper so arguably then this will be the
paper so arguably then this will be the
first mod uh first Improvement to RL
first mod uh first Improvement to RL
in 10 years if this
in 10 years if this
works or our
works or our
algorithms the formula we care about
algorithms the formula we care about
here I love how the whole AI field is on
here I love how the whole AI field is on
archive is pretty good lately some of
archive is pretty good lately some of
the big Labs haven't been publishing
the big Labs haven't been publishing
enough stuff so but other than that yeah
enough stuff so but other than that yeah
it's all
there all
there all
right yeah this section right here this
right yeah this section right here this
is what we
is what we
need so I think what we're going to do
need so I think what we're going to do
first is we're going to just make this
first is we're going to just make this
modification run some quick tests not
modification run some quick tests not
spend all day on this though because
spend all day on this though because
there is uh there's work to be done in
there is uh there's work to be done in
Exploration so morning session while I'm
Exploration so morning session while I'm
still waking up here I'm just going to
still waking up here I'm just going to
fix this stuff see if we can get some
fix this stuff see if we can get some
stuff running uh some experiments
stuff running uh some experiments
running and then the rest of the today
running and then the rest of the today
will be on diving into exploration
will be on diving into exploration
research and seeing if we can just find
research and seeing if we can just find
any any signal at all in that whole
any any signal at all in that whole
branch and really just if there's any
branch and really just if there's any
there any ideas and exploration that
there any ideas and exploration that
seem to make any
seem to make any
sense I got the postto if I want it
sense I got the postto if I want it
congratulations
yeah don't listen to me on that one
yeah don't listen to me on that one
because my reference is posts in
because my reference is posts in
computer
computer
science and it's a very different deal
science and it's a very different deal
because you kind of don't need to do a
because you kind of don't need to do a
post talk in my field for anything other
post talk in my field for anything other
than becoming a professor and it really
than becoming a professor and it really
doesn't give you access to any resources
doesn't give you access to any resources
either we need someone to get [ __ ]
either we need someone to get [ __ ]
done
done
well we do get [ __ ] done around here we
well we do get [ __ ] done around here we
do get [ __ ] done
I have no idea what a z machine is so
I have no idea what a z machine is so
I'm just going to assume that it uses
I'm just going to assume that it uses
quantum mechanics to alter the
quantum mechanics to alter the
fundamental nature statistics
all
right greatest x-ray Source on
Earth what do you use the what do you
Earth what do you use the what do you
use those for
stockpile nuclear deterrents
I don't understand how but sure
this is the thing that matters right
this is the thing that matters right
here
here
right this
gamma data for radiation s
orang reinforcement learning
x-rays this is going to be a little
x-rays this is going to be a little
tricky actually isn't
it Advantage
scale we just we'll call this
scale we just we'll call this
gamma oops
we're writing we're wri in Cuda Kernels
we're writing we're wri in Cuda Kernels
at the moment
oh actually that's a problem isn't it
oh actually that's a problem isn't it
the way that this
is there's a lot of
applications project coming down
line puffer could be the main library he
line puffer could be the main library he
if you have a use for we got nice RL
tools and the funny thing is we're kind
tools and the funny thing is we're kind
of at the point where it's
of at the point where it's
like you kind of should just use puffer
like you kind of should just use puffer
for whatever cuz I can't even say oh you
for whatever cuz I can't even say oh you
have a big project so you should use
have a big project so you should use
this other bigger
this other bigger
library because like the bigger
library because like the bigger
libraries just don't work all that well
libraries just don't work all that well
we could contract with
we could contract with
puffer we could contract with the allp
puffer we could contract with the allp
publish yeah I mean we uh we are pretty
publish yeah I mean we uh we are pretty
easy to work
easy to work
with we don't put any we don't have any
with we don't put any we don't have any
real IP restrictions on anything we kind
real IP restrictions on anything we kind
of just put every uh well any core
of just put every uh well any core
features that you need just go in puffer
features that you need just go in puffer
that's really easy uh we don't put any
that's really easy uh we don't put any
confidentiality agreements it's like
confidentiality agreements it's like
pretty darn easy to get uh whatever RL
pretty darn easy to get uh whatever RL
stuff you need from us pretty quick as
stuff you need from us pretty quick as
well
I think we'll be starting our third
I think we'll be starting our third
contract um next week or whatever so I
contract um next week or whatever so I
mean puffer's starting to do okay it's
mean puffer's starting to do okay it's
still a very new company I think it's
still a very new company I think it's
March still less than a year since I uh
March still less than a year since I uh
have really started working on this
fulltime progress has been
great the group is here making a website
great the group is here making a website
that's cool yeah I mean the key thing
that's cool yeah I mean the key thing
would be if you have like a
would be if you have like a
concrete RL problem in
concrete RL problem in
mind um I can give you like just the
mind um I can give you like just the
initial does this make sense the way you
initial does this make sense the way you
framed it and if it does then we can go
framed it and if it does then we can go
from
there I definitely want to make sure
there I definitely want to make sure
though whatever you're looking at you
though whatever you're looking at you
actually have an RL problem because this
actually have an RL problem because this
is where like most people lose on on oh
is where like most people lose on on oh
our Sim is too slow or oh we can't
our Sim is too slow or oh we can't
really get the data that we need or like
really get the data that we need or like
oh there's not really like a clear
oh there's not really like a clear
observation to action model or something
observation to action model or something
like that
I see how this is wanted
okay I see how this works now
yeah that's a little
yeah that's a little
trickier do I need to make a third
trickier do I need to make a third
function for
function for
this I think I kind of do for
if Pokemon can be solved I feel most big
if Pokemon can be solved I feel most big
engineering problems being can be
engineering problems being can be
gamified like Pokemon yeah it's not so
gamified like Pokemon yeah it's not so
much the difficulty of the problem it's
much the difficulty of the problem it's
the formulation of the problem and how
the formulation of the problem and how
much data you have access to like
much data you have access to like
Pokemon wouldn't have worked if it were
Pokemon wouldn't have worked if it were
10 times slower just because we would
10 times slower just because we would
have needed 10 times as much compute and
have needed 10 times as much compute and
it would have really made the
it would have really made the
engineering effort
engineering effort
obnoxious Pokemon would have been a lot
obnoxious Pokemon would have been a lot
easier if were 10 times faster we would
easier if were 10 times faster we would
have been able to run 10 times as many
have been able to run 10 times as many
experiments and there wouldn't have been
experiments and there wouldn't have been
as much
as much
Engineering in puffer liid uh most of
Engineering in puffer liid uh most of
our environments are a 100 times faster
our environments are a 100 times faster
than even Pokemon which is why it's so
than even Pokemon which is why it's so
easy for us to keep like making progress
easy for us to keep like making progress
and everything so
quickly grab all this
let me get this done for now this is I
let me get this done for now this is I
really want to make sure this is done
really want to make sure this is done
this morning we don't get behind on
this morning we don't get behind on
schedule
here Advantage
some I don't know why I'm doing this
some I don't know why I'm doing this
actually can just
actually can just
do highlight the important details as
do highlight the important details as
they
they
emerge
emerge
yeah I mean this is very specialized
yeah I mean this is very specialized
what I'm doing right here this is very
what I'm doing right here this is very
specific Dev this is on a specific new
algorithm um that needs a specific Cuda
algorithm um that needs a specific Cuda
kernel
okay so here is these are really gamas
okay so here is these are really gamas
maybe I was right the first time for
maybe I was right the first time for
non-real Time stuff this is where LM
non-real Time stuff this is where LM
agents could come in handy Pokemon is
agents could come in handy Pokemon is
fast enough but there probably some
fast enough but there probably some
problems that are too slow uh yeah there
problems that are too slow uh yeah there
definitely are but with all the stuff
definitely are but with all the stuff
I'm working on like there are already
I'm working on like there are already
thousands of people doing llm stuff like
thousands of people doing llm stuff like
that so specifically what I do in puffer
that so specifically what I do in puffer
lib tries to handle everything except
lib tries to handle everything except
that right cuz there are also a lot of
that right cuz there are also a lot of
cases and I think this is what llm
cases and I think this is what llm
people are forgetting about there are a
people are forgetting about there are a
lot of cases in the real world where you
lot of cases in the real world where you
do have a fast Sim and you have the
do have a fast Sim and you have the
unlimited data and if you just have like
unlimited data and if you just have like
good stable RL that's going to always be
good stable RL that's going to always be
way more productive than having to train
way more productive than having to train
some like gigantic model or like try to
some like gigantic model or like try to
apply some gigantic model
well the thing that's the most
well the thing that's the most
complimentary to the llm stuff is puffer
complimentary to the llm stuff is puffer
is just like a vastly better way of
is just like a vastly better way of
making core RL
making core RL
breakthroughs uh it is and the reason is
breakthroughs uh it is and the reason is
that we can run experiments like 10,000
that we can run experiments like 10,000
times faster at probably way less than a
times faster at probably way less than a
10,000th of the cost so you know who's
10,000th of the cost so you know who's
going to be making breakthroughs faster
going to be making breakthroughs faster
I mean arguably you know they also have
I mean arguably you know they also have
10,000 times more resources than I do so
10,000 times more resources than I do so
maybe you know some stuff comes out of
maybe you know some stuff comes out of
both but like for the investment I'm
both but like for the investment I'm
putting into this a lot coming out of
it all right let me go back to myit I
it all right let me go back to myit I
don't know why I thought not to do this
don't know why I thought not to do this
this is totally fun
I will
Define yeah let me lock in on this for a
Define yeah let me lock in on this for a
bit so this is we've got our
gamas and now this needs to be
I guess I am only using the sigmas
I guess I am only using the sigmas
that's kind of weird
I mean this should work either
way for
okay flatten
advantages
oh and then how would you
oh and then how would you
do the mean
loss h
loss h
probably is just a mean
loss just have to redo the way that I
loss just have to redo the way that I
have these rewards so I think this is
have these rewards so I think this is
still your advantage
function this is
still your
advantages so then you get where the
advantages so then you get where the
advantage yeah here
advantage yeah here
and then this
one you now need to do this against
one you now need to do this against
returns I
believe
yes so
you should just
try no it has to be this
try no it has to be this
gaussian NL doesn't
it oh this is sketchy though isn't it
it oh this is sketchy though isn't it
let me think isn't this sketchy
yeah yeah this is
sketchy I think there is something
sketchy I think there is something
fundamental about training the value
fundamental about training the value
function to predict return instead of
function to predict return instead of
training them to reproduce a sequence of
training them to reproduce a sequence of
rewards right
need to do this more formally
yeah this is not right either so this is
sum
gamma
I
I so this is your discounted return
I think the the way you do value
I think the the way you do value
function is you do
function is you do
like slash math
GT what's what's the expectation
symbol math op math BB
do not need math
do not need math
op it's
op it's
this all
right suggest a fix with AI doesn't
right suggest a fix with AI doesn't
work this is is in some package isn't
it asms
fonts
oops see how they do it
oops see how they do it
why not use
why not use
obsidian I've never used obsidian I
obsidian I've never used obsidian I
don't
don't
know isn't it uh it's kind of nice to
know isn't it uh it's kind of nice to
have the lch for stuff to put around
places I think that's all we have to
import I thought obsidian isn't obsidian
import I thought obsidian isn't obsidian
like the main like note taking and thing
like the main like note taking and thing
that
application you vastly overestimate how
application you vastly overestimate how
organized I am
do this make sense
do this make sense
see we got to move this
down
Vantage it's going to
be for knowledge basis and note taking
be for knowledge basis and note taking
I see that's
cool just render lch I will check that
cool just render lch I will check that
out though I don't want to like I don't
out though I don't want to like I don't
want to recursively chain down while I'm
want to recursively chain down while I'm
doing
this you
this you
know oh we started with the code and
know oh we started with the code and
then I couldn't figure out that so I
then I couldn't figure out that so I
optimize the formula and I couldn't
optimize the formula and I couldn't
figure out how to write the formula
figure out how to write the formula
quickly soon enough I'm going to be
quickly soon enough I'm going to be
compiling the Linux
Tel okay yeah we have let's get rid of
Tel okay yeah we have let's get rid of
this cuz this one is this is the old
one so this is our minus B so t
your diagnostic IDE is
sharp it'll feel much better to be the
sharp it'll feel much better to be the
world expert in said topic
maybe in a few years takes
time I mean this is one of the other
time I mean this is one of the other
cool things about AI
cool things about AI
is you very often can finish your PhD
is you very often can finish your PhD
right and legitimately be the world
right and legitimately be the world
expert in a pretty decently sized area
there's a reason there isn't even
there's a reason there isn't even
anything remotely close to poer Li right
now H I think it should be like
l for
so this is a little tricky
so this is a little tricky
now I don't have any way to define the
now I don't have any way to define the
value loss do I
so I Define this Dynamic discount Factor
so I Define this Dynamic discount Factor
here I can Define the the return
here I can Define the the return
basically the way it is in
po I can Define the value function the
po I can Define the value function the
way it is in PPO I can Define the
way it is in PPO I can Define the
advantage function the way it is in po
advantage function the way it is in po
so I can optimize the policy
so I can optimize the policy
but then I'm stuck I think on the value
but then I'm stuck I think on the value
function because I can no longer apply
function because I can no longer apply
um gaan negative log likelihood loss to
um gaan negative log likelihood loss to
this thing because now I'm optimizing
this thing because now I'm optimizing
returns instead of Rewards
I can't just apply the um
yeah and I can't just put a uh squared
yeah and I can't just put a uh squared
error loss on it either because if you
error loss on it either because if you
do that then it will not
do that then it will not
optimize the sigma term which also needs
optimize the sigma term which also needs
to be
learned e
hang
on what if I
do
e
e e
does this do it
oops
let me see if I can get
let me see if I can get
uh let me see if I can figure something
uh let me see if I can figure something
out e
oops e
hey how's it
hey how's it
going we're doing pretty well here just
going we're doing pretty well here just
uh I'm just going to do something a
uh I'm just going to do something a
little silly with grock just to see if
little silly with grock just to see if
I'm I'm just basically going to sanity
I'm I'm just basically going to sanity
check what I did just did before I go
check what I did just did before I go
implement this and I'm hoping that I
implement this and I'm hoping that I
have this right and that this will be a
have this right and that this will be a
much
much
better uh implementation of the
better uh implementation of the
algorithm I've been trying to do
why do you get gamut in there twice
why do you get gamut in there twice
actually isn't that a little weird
oh yeah but most of these get cancelled
oh yeah but most of these get cancelled
don't
they you have any ex experience
they you have any ex experience
incorporating RL into multi-agent
incorporating RL into multi-agent
systems my thesis is in multi-agent RL
systems my thesis is in multi-agent RL
in fact probably the most most agent RL
in fact probably the most most agent RL
out
out
there
there
here we have right here for you several
here we have right here for you several
multi-agent environments including this
one here's a massively multi-agent
one here's a massively multi-agent
environment like a whole open world
environment like a whole open world
that's populated by hundreds of
that's populated by hundreds of
reinforcement learned agents that are
reinforcement learned agents that are
all interacting with each other and
all interacting with each other and
playing this like pretty fancy
game lots of stuff fr create a closed
game lots of stuff fr create a closed
loop system but it seems a bit ambitious
loop system but it seems a bit ambitious
kind of concerned it depends I have to
kind of concerned it depends I have to
look at the precise problem um if you
look at the precise problem um if you
just want to get like the gist of stuff
just want to get like the gist of stuff
right we've got uh the Discord and you
right we've got uh the Discord and you
can like you know put some stuff in
can like you know put some stuff in
there and I can like spot check it or
there and I can like spot check it or
other people in there can spot check and
other people in there can spot check and
see if it makes sense the way it's set
see if it makes sense the way it's set
up um and I will say well all of our
up um and I will say well all of our
stuff is free and open source for
stuff is free and open source for
companies we also do have a priority
companies we also do have a priority
service where you can just hire us to
service where you can just hire us to
make sure basically that all your RL is
make sure basically that all your RL is
going sane and to integrate our tools to
going sane and to integrate our tools to
make them work for
make them work for
you multi-agent l okay llms which can do
you multi-agent l okay llms which can do
tool calling that's a little bit
tool calling that's a little bit
different um I can still sanity check
different um I can still sanity check
that for you though because if you're
that for you though because if you're
applying RL separately to different
applying RL separately to different
models that does get a bit dicey and the
models that does get a bit dicey and the
reasons are basically the same as they
reasons are basically the same as they
would be in standard RL
the main issue with
the main issue with
um again it depends how you're doing it
um again it depends how you're doing it
if you have say different prompts or
if you have say different prompts or
something and you're using all of the
something and you're using all of the
training data for uh all the models so
training data for uh all the models so
basically you have one model with
basically you have one model with
different prompts and you're using all
different prompts and you're using all
the data on the model that's kind of
the data on the model that's kind of
reasonable but if you have a bunch of
reasonable but if you have a bunch of
different agents and you're splitting
different agents and you're splitting
the data across them that gets more
the data across them that gets more
expensive proportional to the number of
expensive proportional to the number of
age UPS so then the whole benefit of
age UPS so then the whole benefit of
like oh this is nice and scalable goes
like oh this is nice and scalable goes
away because it also gets scalably more
away because it also gets scalably more
expensive
poms doing different tasks and breaking
poms doing different tasks and breaking
them
them
down different promps doing
down different promps doing
different yeah I mean that's just
different yeah I mean that's just
Distributing work like
that's just Distributing work it depends
that's just Distributing work it depends
how you're setting it up
how you're setting it up
so yeah if you have more details on
so yeah if you have more details on
stuff drop in the Discord and our stuff
stuff drop in the Discord and our stuff
our tools are not
our tools are not
specifically made for uh llms in an RL
specifically made for uh llms in an RL
settings uh not that they don't work for
settings uh not that they don't work for
that it's just that uh llms are so slow
that it's just that uh llms are so slow
that the performance optimizations that
that the performance optimizations that
we've done in puffer lib right how
we've done in puffer lib right how
puffer lib is so fast your model is so
puffer lib is so fast your model is so
big that your whole thing is going to
big that your whole thing is going to
run very slowly and be expensive no
run very slowly and be expensive no
matter what I do uh that's the main
matter what I do uh that's the main
issue there you can actually
issue there you can actually
see this is one of the agents that we
see this is one of the agents that we
have this is like a tiny little agent
have this is like a tiny little agent
running on one CPU in your browser
running on one CPU in your browser
that's here like kiting these agents
that's here like kiting these agents
trying not to get
trying not to get
caught ah and he got caught right at the
caught ah and he got caught right at the
end because he didn't see that they were
end because he didn't see that they were
going to be they were both going to hit
going to be they were both going to hit
him at the same
him at the same
time oh yeah there you
time oh yeah there you
go oh that's a bug
go oh that's a bug
yeah it didn't reset the element here
yeah it didn't reset the element here
that hair color should have shifted back
that hair color should have shifted back
to White well there's much more coming
to White well there's much more coming
with neurl Mo soon
anyways maybe I should just throw a
anyways maybe I should just throw a
little llm interface on puffer at some
little llm interface on puffer at some
point because it would be super easy and
point because it would be super easy and
probably people would like
it but it's really not the poor thing I
it but it's really not the poor thing I
want to do with Puffer for
it's more than a norm now Captain so uh
it's more than a norm now Captain so uh
again like basically anything I do now
again like basically anything I do now
still trains but I think that we're
still trains but I think that we're
going to have to
going to have to
do uh we're going to have to do
do uh we're going to have to do
something a little bit fancier
something a little bit fancier
here oh is
this
e
e
e
e e
okay there we go now
I think you can simplify now can't
you
GMA
e
e e
so uh for the folks that just joined on
so uh for the folks that just joined on
YouTube
YouTube
I've come to these formulas here so far
I've come to these formulas here so far
this is um I'm attempting to replace
this is um I'm attempting to replace
generalized Advantage estimation in RL
generalized Advantage estimation in RL
with a learnable form of it that will be
with a learnable form of it that will be
much more flexible and not have two
much more flexible and not have two
really fiddly hyper
parameters so I have these forms and I'm
parameters so I have these forms and I'm
just trying to sanity check and see if
just trying to sanity check and see if
anything simplifies or if there's you
anything simplifies or if there's you
know anything I can do
okay so this is what I was going for
here so I didn't actually give it this
here so I didn't actually give it this
formula and it came up with it so I
formula and it came up with it so I
think that unless I've like steered it
think that unless I've like steered it
into something wrong I should hopefully
into something wrong I should hopefully
be
be
I should hopefully have it correct
here
e e
that's inversely to the variance at yes
and then we have finite
and then we have finite
Horizon waiting
Horizon waiting
yes value
Target then why is open
truncated weight sum RT might be a
truncated weight sum RT might be a
scaler summing over H time St
scaler summing over H time St
yes reducing is reduced to a scaler yes
yes reducing is reduced to a scaler yes
this
is infinite tail that's
is infinite tail that's
fine value loss
fine value loss
modeling
TT this trains B to
predict
R oh I have this slightly off so this
R oh I have this slightly off so this
should
should
be let me fix
be let me fix
this wait do I no this is
correct this
trains let me
see R Plus
VT +
VT +
one yes that's correct it's train to
one yes that's correct it's train to
predict the reward plus the value at the
predict the reward plus the value at the
next step that's
next step that's
correct return and
correct return and
[Music]
[Music]
Advantage subing this we get that no
Advantage subing this we get that no
simplification
yep lack of cumulative gamma products
yep lack of cumulative gamma products
distinguish it well because it's
distinguish it well because it's
adaptive
right okay I think that this
right okay I think that this
is there no glaring errors that I can
is there no glaring errors that I can
see here
right
e
e e
Oh shoot no this is not going
Oh shoot no this is not going
to is this going to work
I don't know if the last thing is
I don't know if the last thing is
correct
negative gamma might be okay we might
negative gamma might be okay we might
have to clip this
have to clip this
yeah I think we do clip it already
yayer return over H
yayer return over H
steps yes
the loss assumes BT minus BT + 1 is
the loss assumes BT minus BT + 1 is
approximately equal to
approximately equal to
RT
RT
yeah this holds if VT predicts One Step
yeah this holds if VT predicts One Step
returns
returns
iteratively but your target is RT this
iteratively but your target is RT this
is a subtle mismatch
is a subtle mismatch
okay if VT targets
okay if VT targets
RT which it
RT which it
does then VT minus VT + 1 = R doesn't
does then VT minus VT + 1 = R doesn't
naturally
follow unless H equal 1 or
B why not
okay so they're saying
okay so they're saying
here to just
make yeah that's
tricky so we we'll go into this is there
tricky so we we'll go into this is there
anything
anything
else so this is the
else so this is the
issue
e
e
e e
you know this is the one thing that I
you know this is the one thing that I
actually
actually
find lm's useful for is just
find lm's useful for is just
troubleshooting troubleshooting stuff
troubleshooting troubleshooting stuff
like
like
this they don't really have great ideas
this they don't really have great ideas
about anything
but just like doing algebra and doing
but just like doing algebra and doing
symbol manipulation actually it's not
symbol manipulation actually it's not
terrible which is kind of ironic because
terrible which is kind of ironic because
this is kind of one of the places you
this is kind of one of the places you
would expect a symbolic system to be
would expect a symbolic system to be
better at um doing the algebra but I
better at um doing the algebra but I
don't know it's kind of
don't know it's kind of
funny it's also just possible though and
funny it's also just possible though and
probably more likely that it's just not
probably more likely that it's just not
all that good and it's just that I'm a
all that good and it's just that I'm a
much better programmer than I am a
much better programmer than I am a
mathematician so like this thing's
mathematician so like this thing's
really really dumb when it comes to
really really dumb when it comes to
programming to me but much better when
programming to me but much better when
it comes to math because my math
it comes to math because my math
sucks but hey it gets
done e
doesn't match your multistep intent R
doesn't match your multistep intent R
plus gamma
plus gamma
VT plus one doesn't that look familiar
it's much harder to estimate the
it's much harder to estimate the
uh standard
uh standard
deviation of returns isn't it
Bing to the return feels more
coherent okay
so I mean we have a couple different
so I mean we have a couple different
options here
right uh I suppose the other one would
be what is actually the
difference just r
I think I need to understand this better
this satisfies the Bellman equation
right gamma vs
yeah gamma is one horizon is finite just
yeah gamma is one horizon is finite just
expected return yes okay
you want V to estimate weighted return
you want V to estimate weighted return
over H depths
over H depths
yes trains VT minus t +1 to match
yes trains VT minus t +1 to match
RT suggesting that VT is approximately
RT suggesting that VT is approximately
equal
equal
to r + BT + 1
to r + BT + 1
yes a one-step
prediction VT estimates the immediate
prediction VT estimates the immediate
reward plus the value of the next
state if VT is equal to
state if VT is equal to
[Music]
[Music]
RT is equal to the sum
it's the return from T to t plus
it's the return from T to t plus
h
right this equals R only if gamma equals
right this equals R only if gamma equals
oh so they're just saying that there is
oh so they're just saying that there is
a
a
um a gamma term
on they're just saying that there's a
on they're just saying that there's a
gamma term on the first one
right yeah this just says that there's
right yeah this just says that there's
because there's a gamma term on the
because there's a gamma term on the
first
one e
multistep
multistep
return VT consistent with
return VT consistent with
rt it shift Sigma to standard deviation
rt it shift Sigma to standard deviation
of
returns well you could just divide by
returns well you could just divide by
gamma T
right for
was a difference wait what how's this
was a difference wait what how's this
happen
how the hell did I not wait what how did
how the hell did I not wait what how did
this not work
e
e
minus BT +
1 + H + 1 e
okay you could also do this
okay you could also do this
cool
td0 wait your suggestion adjust the TD
td0 wait your suggestion adjust the TD
residual by scaling with L by gamma
residual by scaling with L by gamma
T So value is equal
to gamma RT okay well that's is that
to gamma RT okay well that's is that
what we
want that is what we want isn't
it e
gamma HT plus r it's got to be equal to
gamma HT plus r it's got to be equal to
zero
One Step a turn waited
by well this is kind of zero isn't it
the turn is equal
to gamma
RT plus gamma t + 1 VT
oh yeah now this is be this is going to
oh yeah now this is be this is going to
become a problem as well isn't
[Music]
[Music]
it no we can't do this because
yeah we can't do
this because the whole point is to take
this because the whole point is to take
into account the gamas as the waiting
into account the gamas as the waiting
Factor
true or do a learnable g this is
true okay so let me think about this
then
then
value squared
this is going to be our standard
this is going to be our standard
deviation like
this and these depend on each other
this and these depend on each other
great
lovely e
hold on maybe not
the standard deviation of returns
the standard deviation of returns
now no these depend on each other
for
e e
gamma must reflect the variance the
gamma must reflect the variance the
returns not the
reward to find the Baseline is the
reward to find the Baseline is the
variance of the uninformed
variance of the uninformed
return assume RI has variance R st R
return assume RI has variance R st R
STD
STD
squared uninformed predictor weight all
squared uninformed predictor weight all
terms
equally variance of R
uninformed but since gamma
uninformed but since gamma
varied let's tie the Baseline to the
varied let's tie the Baseline to the
expected variance of R under the current
expected variance of R under the current
gamma
this is still circular
lovely with Decay we're not adding
Decay or you boot strap
Sigma rstd equals the sum of rftd
squ this thing's a
squ this thing's a
mess this thing is a mess
what the hell is this that it's giv me
what the hell is this that it's giv me
this mess
yeah I don't know what it's doing
there is there something we can do with
there is there something we can do with
this instead
this instead
let
so
e
e e
see what if we write this
out
e
e e
did I do that
right
e
e e
the
the
hell oh no wait this is
hell oh no wait this is
gamma this is supposed to
gamma this is supposed to
be R yeah yeah
yeah e
oh I am confused I will be back in a bit
oh I am confused I will be back in a bit
uh I'm going to just walk around do a
uh I'm going to just walk around do a
couple things clear my head I'll be back
couple things clear my head I'll be back
after I don't know 10 minutes or
after I don't know 10 minutes or
something and then uh we will see if I
something and then uh we will see if I
can finish this I don't want to spend
can finish this I don't want to spend
all day on this but like we are making
all day on this but like we are making
some progress so it's good I'll be
back
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e
we are back
oh wait what the heck did I do
oh wait what the heck did I do
here I uh yeah that's not how math Works
here I uh yeah that's not how math Works
what did I
do yeah that's not how math works at all
do yeah that's not how math works at all
I don't know why I did that
there we go
yeah that doesn't really help me
well I could do
yeah okay that's just bad math um I
yeah okay that's just bad math um I
could replace
RI
e
e
e e
what squared
yeah e
I'm so confused by
this
yeah wait
what one over
what the [ __ ] does this
mean no bad why did it do this
ah okay this will mess up I
see
e
e e
this Baseline term is going to be a pain
this Baseline term is going to be a pain
in the ass
I don't know I still don't think that
I don't know I still don't think that
this works though
yeah this is getting
confused
e e
why is this so hard what is different
why is this so hard what is different
about this from before
before the value function was just
before the value function was just
predicting the next
reward so it was easy to just sub in the
reward so it was easy to just sub in the
variance of the reward as a
baseline now the value function
is now the value function is predicting
the the discounted sum of returns
hang on I think I know how to do
it
e e
what's up here
what do we have here we
have
e e
picky I just need a good Baseline on
this
this
okay I think this works
VT is a
scalar VT outputs a vector of
scalar VT outputs a vector of
predictions yes it
does use Sigma squ T consistently what
does use Sigma squ T consistently what
did I do
what is
thisal T I don't know why it would be T
thisal T I don't know why it would be T
here
Gan or Sigma squ T
Gan or Sigma squ T
equals what the
heck e
scaler predicting RT okay let me I have
scaler predicting RT okay let me I have
to just clarify this
actually does it even make sense as a
actually does it even make sense as a
vector anymore let me
think what are we using we're using
think what are we using we're using
[Music]
[Music]
Sigma Sigma squar
okay
e
e
e
e e
I think I should try this
now wait
what no this doesn't make any sense
anymore I need to start like actually
anymore I need to start like actually
doing this stuff this is going to drive
doing this stuff this is going to drive
me crazy this thing just keeps like
me crazy this thing just keeps like
forgetting the way that symbols are
defined
e e
I think you can just do it in here now
right e
hey
e e
no this goes at the very
no this goes at the very
end this goes at the very end
right or does it
not no actually so complicated so
not no actually so complicated so
freaking
complicated e
there do that
and what the heck do we use for the um
damn it what are we for
return that's the buffer
yeah it
is so I guess we have to go backwards
is so I guess we have to go backwards
over this
yeah we do
okay
e
e e
what is it vstd
minus
minus
[Music]
r there
right does that do anything though hang
on [ __ ] no this is still freaking wrong
on [ __ ] no this is still freaking wrong
isn't it
this is so incredibly
this is so incredibly
confusing I'm getting confused here
confusing I'm getting confused here
because I think I need I need the
because I think I need I need the
standard
standard
deviation the estimate of the standard
deviation the estimate of the standard
deviation of the return don't
deviation of the return don't
I so hang on hang on maybe
it's I don't know how you do
it's I don't know how you do
that the math is just tricky
here e
how does this even make
sense wait maybe it does a telescope
damn it this is so obnoxious
what the reason I can't use the original
what the reason I can't use the original
G
G
here there a
reason yeah the reason is that the Decay
reason yeah the reason is that the Decay
Factor doesn't go to zero
Factor doesn't go to zero
right yeah the Decay Factor goes to uh
the def the Decay Factor goes to what
the def the Decay Factor goes to what
does it even go to that's the
problem you've got a chicken and egg
problem you've got a chicken and egg
problem here
is this resolv just by subtracting the
is this resolv just by subtracting the
mean I don't think so right
so if I write it without this value here
so if I write it without this value here
for a second right
that's not what it is is
it hang
on maybe it
is for
isn't this it right like
this it's just the sum
of reward over value standard
deviation and then possibly there's a
deviation and then possibly there's a
term at the end that we account
term at the end that we account
for but this is pretty well
for but this is pretty well
it
it
okay so then what we need
is the problem is this vstd doesn't go
is the problem is this vstd doesn't go
to zero
right actually you definitely need the
right actually you definitely need the
bootstrap term here otherwise you're
bootstrap term here otherwise you're
screwed aren't you
oh maybe that is the Baseline that you
oh maybe that is the Baseline that you
need though hold on
do you subtract values
not
not
easy not easy at
all the tricky thing here right is just
all the tricky thing here right is just
understand
the difference
the difference
between how standard deviations would be
between how standard deviations would be
predicted for uh for rewards versus for
predicted for uh for rewards versus for
returns which is sum of
returns which is sum of
discounted return uh sum of discounted
discounted return uh sum of discounted
rewards so
yeah this definitely needs to get
normed is this D deviation predicting
maybe I can just do it
now this is like very very complicated
now this is like very very complicated
and annoying here
um so this is what I
proposed standard deviation of
VT
okay BT is a scalar predicting the full
okay BT is a scalar predicting the full
return from T to t +
H sum of Sigma
I but R uses this per time step
yes
e
e
e e
certainty
yeah R is a
scaler predicts each
scaler predicts each
term and be
some
e
e e
if I can get this right then we will
if I can get this right then we will
basically have a a
basically have a a
generalized what do we call it
generalized what do we call it
generalized generalized Advantage
estimation
GGA but I have to get the formula right
GGA but I have to get the formula right
and this is driving me
nuts for
maybe we
have t is a
have t is a
vector no
RT
turn for
I think that makes
sense Z
minus
zero e
let's see if it gets the formulas
let's see if it gets the formulas
here the way that I have
them
okay so we have this
okay so we have this
formula RT of
k t +
k t +
k t + k + hus one sure
squ
yes does it
hold when Sigma is equal to
R but this is the tricky thing here
the tricky part that I can't figure out
the tricky part that I can't figure out
here is that this where is it this
here is that this where is it this
is an estimate
of of the return
right e
It's Tricky here because like I'm trying
It's Tricky here because like I'm trying
to hit a moving
Target the what is the standard
Target the what is the standard
deviation of the
return so if you don't discount it that
return so if you don't discount it that
goes to Infinity
goes to Infinity
right
yeah
yeah
I damn I don't think this
works do I have to go all the way back
works do I have to go all the way back
to
to
the I mean I had a form of this that
the I mean I had a form of this that
made a lot more sense to me
I think that one actually still didn't
I think that one actually still didn't
work right
now you do want to estimate returns is
now you do want to estimate returns is
the
thing you do need to estimate returns
thing you do need to estimate returns
and I think the reason that you need to
and I think the reason that you need to
estimate returns right is you're not
estimate returns right is you're not
going to ever really know exactly when
going to ever really know exactly when
you're going to get a reward many steps
you're going to get a reward many steps
out you're just going to roughly know
out you're just going to roughly know
that you're going to get a reward so the
that you're going to get a reward so the
single step bootstrap one doesn't work
single step bootstrap one doesn't work
very
very
well what I was trying to do
well what I was trying to do
before do I still have it here yeah I
before do I still have it here yeah I
was trying to do this
but I don't actually think that this
works well because then this has to be a
works well because then this has to be a
prediction of the reward variance right
does that make sense hang
on now I think that what this
on now I think that what this
says I don't know you can correct me if
says I don't know you can correct me if
I'm wrong here if anybody knows any math
I'm wrong here if anybody knows any math
but so this portion of the formula is
but so this portion of the formula is
kind of clever because this allows you
kind of clever because this allows you
to predict the full returns which should
to predict the full returns which should
be much easier to learn uh discounted
be much easier to learn uh discounted
return should be easier to learn than
return should be easier to learn than
direct prediction of value because you
direct prediction of value because you
only need to predict how much you're
only need to predict how much you're
going to get total in the future you
going to get total in the future you
don't have to predict like exactly when
don't have to predict like exactly when
you're going to get each each
you're going to get each each
amount and it's
amount and it's
discounted uh but then you still end up
discounted uh but then you still end up
with the sigma squar is the sigma squar
with the sigma squar is the sigma squar
of this term
of this term
which is basically an estimate of reward
which is basically an estimate of reward
so you're still
predicting the uncertainty of each
predicting the uncertainty of each
individual reward okay so it actually it
individual reward okay so it actually it
does make more sense to just cut this
does make more sense to just cut this
term out and have VT RT Sigma squ uh
term out and have VT RT Sigma squ uh
Sigma t^
Sigma t^
squ but then if you do
that I mean we're stuck at the same
that I mean we're stuck at the same
problem the problem is this term right
problem the problem is this term right
here how to Baseline it when you just
here how to Baseline it when you just
had uh when you were predicting the
had uh when you were predicting the
rewards instead of the returns it's very
rewards instead of the returns it's very
easy to get the standard
easy to get the standard
deviation of the uh of the reward it's
deviation of the uh of the reward it's
not easy to get the standard deviation
not easy to get the standard deviation
of the
of the
return
now e
subtract what would be a thing we can
subtract what would be a thing we can
subtract from
this if we subtract
mean gamma R mean over our standard
mean gamma R mean over our standard
deviation
how do we estimate
the the mean
return is it just gamma R
mean does that do it
instead of the standard deviation we get
instead of the standard deviation we get
the mean reward does that do it
I don't think that does
it I'm just getting hung up on basic
it I'm just getting hung up on basic
stats
is there anything
here now there's nothing there I have to
here now there's nothing there I have to
just figure it out
squared one over Sigma squ is going to
squared one over Sigma squ is going to
be the waiting right
hang on maybe I do
know maybe I have an idea
let's do this one
let's do this one
here gamma
here gamma
I so this is just going to be
I so this is just going to be
right I
right I
over
Sigma Sigma eyes squar or Sigma is it I
Sigma Sigma eyes squar or Sigma is it I
yes I
yes I
squared and
then minus sum
I mess up here
I mess up here
[Music]
the heck happened up
here okay r i over Sigma IUS
H minus
I going on a small
I going on a small
run
enjoy alumni rug rugby game good
enjoy alumni rug rugby game good
luck will you be deving tonight I'll
luck will you be deving tonight I'll
probably be deving all
day depends how much math I get stuck
day depends how much math I get stuck
doing
providing the funds for the
providing the funds for the
party good
luck e
maybe it is an
armu yeah cuz it's supposed to be okay
the thing is this is still the stand
the thing is this is still the stand
deviation of the reward right not the
return I'm I think I'm also just
return I'm I think I'm also just
missing the value bootstrap
right because this needs to
be
e e
if we just cover up this last CH
here then this sum
explodes
e e
what's the end goal are you trying to
what's the end goal are you trying to
make this algorithm no this is a new
make this algorithm no this is a new
algorithm um the issue I'm having at the
algorithm um the issue I'm having at the
moment this is a new algorithm that
moment this is a new algorithm that
replaces generalized Advantage
replaces generalized Advantage
estimation but the issue that I'm having
estimation but the issue that I'm having
is that
is that
um the in generalized Advantage
um the in generalized Advantage
estimation the series eventually uh
estimation the series eventually uh
converges to zero so it doesn't explode
converges to zero so it doesn't explode
this one converges to the variance of
this one converges to the variance of
the underlying distribution so it will
the underlying distribution so it will
explode if I don't subtract the variance
explode if I don't subtract the variance
but I need the variance to compute the
but I need the variance to compute the
formula and I need the formula to
formula and I need the formula to
compute the variance so I'm trying to
compute the variance so I'm trying to
figure out how I can Baseline this
thing e
you think it will that it will well this
you think it will that it will well this
is new like it's not going to work the
is new like it's not going to work the
way that I have it written at the moment
way that I have it written at the moment
um the thing that happened that's
um the thing that happened that's
annoying is I was doing single reward
annoying is I was doing single reward
predictions before and I'm trying to
predictions before and I'm trying to
make it predict returns now when you're
make it predict returns now when you're
doing single reward predictions you can
doing single reward predictions you can
just take the variance of the reward
just take the variance of the reward
signal like the reward and the data um
signal like the reward and the data um
oh hey car how's it
oh hey car how's it
going uh you can just take the variance
going uh you can just take the variance
of the reward in the data for that but
of the reward in the data for that but
when you have the full return estimates
when you have the full return estimates
there's not really a way to get the
there's not really a way to get the
estimate of the variance of the returns
estimate of the variance of the returns
so yeah I'm having a tough time
so yeah I'm having a tough time
baselining this this is the thing that's
baselining this this is the thing that's
annoying
this should explode shouldn't
it yeah Sigma I is because the thing of
it yeah Sigma I is because the thing of
Sigma
Sigma
I um this needs to be Baseline because
I um this needs to be Baseline because
this is going to converge to the
this is going to converge to the
variance of the discounted
variance of the discounted
[Music]
return maybe I'm still not understanding
return maybe I'm still not understanding
this
this
correctly so
okay let's say that I
okay let's say that I
compute Let's ignore and I'm actually
compute Let's ignore and I'm actually
going to take this term off for a second
going to take this term off for a second
because it's just it's confusing me even
because it's just it's confusing me even
to have to look at this
to have to look at this
thing okay is
thing okay is
it is it percent yeah there we
it is it percent yeah there we
go
go
so if I compute this over every term
this converges to
this converges to
a does this converge to a constant what
a does this converge to a constant what
is
this value function
this value function
right so reward over Sigma
right so reward over Sigma
squ and the value function is pret is
squ and the value function is pret is
trained to predict this
trained to predict this
is trying to predict this
right this works in
right this works in
J because this goes to
J because this goes to
zero it does I
zero it does I
don't it's very difficult to think about
don't it's very difficult to think about
this though it's it's very difficult to
this though it's it's very difficult to
think about this for some
reason CU this isn't a constant it's a
reason CU this isn't a constant it's a
learnable parameter so I can't really
learnable parameter so I can't really
tell how it's going to shift
this made so much more sense when Sigma
this made so much more sense when Sigma
was an estimate of the a standard
was an estimate of the a standard
deviation of the reward instead of the
deviation of the reward instead of the
return
If This Were a fixed gamma how does it
If This Were a fixed gamma how does it
change right if this were a fixed
scamma it actually does change quite a
scamma it actually does change quite a
bit now I'm looking at
it oh well that's an interesting
it oh well that's an interesting
Discovery hang
Discovery hang
on
on
huh I think think I just figured out why
huh I think think I just figured out why
um why learning rate and uh well why
um why learning rate and uh well why
gamma and Lambda feel like such screwy
gamma and Lambda feel like such screwy
parameters because gamma doesn't
parameters because gamma doesn't
actually Decay to zero over the horizons
actually Decay to zero over the horizons
that you're looking
that you're looking
at so when you like when you just change
at so when you like when you just change
the effective
the effective
Horizon
Horizon
um yeah when you just change the
um yeah when you just change the
effective Horizon that they're computed
effective Horizon that they're computed
over does that make sense
God I'm just confusing the hell out of
God I'm just confusing the hell out of
myself today
yeah I don't think any of this has
yeah I don't think any of this has
solved the problem at all
solved the problem at all
if not one
if not one
bit I don't even know if there is a
bit I don't even know if there is a
solution to
this e
okay so yeah this is what we have
yeah but this doesn't do it I don't
yeah but this doesn't do it I don't
think right
yeah so this this would work but then
yeah so this this would work but then
this fundamentally
changes for
we could just try it and see what
happens
e e
I'm going to just try the original thing
I'm going to just try the original thing
because this is driving me nuts
crazy I mean the issue here is I know
crazy I mean the issue here is I know
how to do this with one step predictions
how to do this with one step predictions
I know how to do this bootstrapped it's
I know how to do this bootstrapped it's
very
very
easy I don't think it works anywhere
easy I don't think it works anywhere
near as well
near as well
though because I the key Insight here is
though because I the key Insight here is
it is very difficult to
it is very difficult to
predict um it's very difficult to
predict um it's very difficult to
predict values and time steps ahead
predict values and time steps ahead
exactly but if you you predict the sum
exactly but if you you predict the sum
then it doesn't matter quite where they
then it doesn't matter quite where they
appear it just it's just like it's the
appear it just it's just like it's the
difference of saying hey uh when are you
difference of saying hey uh when are you
going to score points exactly in the
going to score points exactly in the
next 32 steps versus how many points you
next 32 steps versus how many points you
going to
score to be fair I don't think I've
score to be fair I don't think I've
tried the
tried the
uh the original
well the original is also screwy for
well the original is also screwy for
other reasons isn't
it or is it
not this predicts variance of this
well let's finish this
first e
okay R minus value means to
okay R minus value means to
zero something like
this reward block your value
this reward block your value
in value
variance just try to get this to
run e
all so this should run hopefully
what's wrong
what's wrong
here illegal member access
here illegal member access
really what' I do
I times
fren oh that's not
right
e e
it would be great if this thing would
it would be great if this thing would
tell me
where see what code I had before
jez okay index is defined right
reward block
mask
mask
[Music]
advantages of I right
the heck I don't see anywhere where I'm
the heck I don't see anywhere where I'm
indexing
indexing
badly I don't see anywhere
hold
hold
on
K J L
Horizon I + J
that looks like out of bounds to
me well this is minus
minus for
be about this me
hey Spen sir
hey Spen sir
going freaking crazy
going freaking crazy
here over
here over
uh bunch of confusing map always fun
I'm tempted to just do it the other way
I'm tempted to just do it the other way
that I know should actually work
that yeah I have no idea what this is
that yeah I have no idea what this is
doing clearly not
working okay I think what I'm going to
working okay I think what I'm going to
do because this is going to drive me
do because this is going to drive me
nuts I'm going to keep this
nuts I'm going to keep this
uh but I'm going
uh but I'm going
to copy
this yeah I think GP I think
this yeah I think GP I think
uh as always I think that grock or the
uh as always I think that grock or the
llm LED me down a really really freaking
llm LED me down a really really freaking
stupid path cuz I just there's no way to
stupid path cuz I just there's no way to
get a baseline for this thing it's just
get a baseline for this thing it's just
going to explode there's absolutely no
going to explode there's absolutely no
way I can see to get a baseline for
way I can see to get a baseline for
this so if we go back to this one
here it's much easier isn't
it
e for
that pisses me off
okay yes so this is where we
started and and now I
think advantages equals
Advantage Advantage Plus equal to
this one should be easier to figure out
this one should be easier to figure out
by far
now it's just some
of some of rewards
over over Sigma
squar yeah hang on this is easier now
right you do need to
right you do need to
store the
discounted yeah this now becomes the
returns
right these d 10 towards zero
now index isal to the scale
so it's just going to
be buffer of index
lock
Index this is now return
okay I think that's
okay I think that's
it because now you're Computing
it because now you're Computing
discounted
discounted
return
right and if you have Sigma squar is the
right and if you have Sigma squar is the
sigma of of the reward not the return
sigma of of the reward not the return
which I think it can
do then I should just have to go to
do then I should just have to go to
this this gets new value
this this gets new value
mean w block
mean w block
variance uh but I this now needs to be
variance uh but I this now needs to be
slightly slightly
altered which project is the most
altered which project is the most
interesting and
interesting and
useful out of what
um 3D
um 3D
reconstruction and
reconstruction and
editing continual
editing continual
learning 3DC understanding zero shot
learning I mean you've just given me a
learning I mean you've just given me a
you've given me a whole bunch of very
you've given me a whole bunch of very
broad
topics continue learning and 3D scene
topics continue learning and 3D scene
understanding are like very disjoint
understanding are like very disjoint
zero shot has a ton of comp uh a ton of
zero shot has a ton of comp uh a ton of
different areas compositional zero shot
different areas compositional zero shot
I don't even know what that means 3D
I don't even know what that means 3D
reconstruction is the only one of those
reconstruction is the only one of those
that's like a somewhat narrowly defined
that's like a somewhat narrowly defined
area
yeah but like continual learning and 3DC
yeah but like continual learning and 3DC
and understanding
and understanding
aren't like yeah you're like you give
aren't like yeah you're like you give
um those are like huge you have like
um those are like huge you have like
huge areas
huge areas
here these aren't projects you specified
here these aren't projects you specified
like sub fields
I also don't even know what zero shot
I also don't even know what zero shot
learning is because it's like zero shot
learning is because it's like zero shot
implies not learning
continual human pose
continual human pose
estimation key points and pose
variations
variations
oh yeah well that's not what continual
oh yeah well that's not what continual
learning is at
learning is at
all um if you're going to do that
all um if you're going to do that
specifically then
no key
points CL
estimation H es yeah I
don't I think you're going to have to
don't I think you're going to have to
narrow some stuff
narrow some stuff
because things are IL defined mind here
because things are IL defined mind here
so is based
so is based
on continual human pose
on continual human pose
estimation okay I know what POS
estimation okay I know what POS
estimation is I don't know what
estimation is I don't know what
continual pose estimation means
continual pose estimation means
right for incremental integration of key
right for incremental integration of key
points and POS isn't that just posst
points and POS isn't that just posst
estimation
estimation
like yeah some stuff seems IL defined
like yeah some stuff seems IL defined
here
just shift
these I think I do just shift these
these I think I do just shift these
don't I
BT other way around
does this do anything for us
we will
see whole bunch of notifications
compositional zero shot
compositional zero shot
the model trained on individual
the model trained on individual
components and then recognize novel
components and then recognize novel
combination of these components at test
time so
like oh when did they do
like oh when did they do
this March oh they just did this how did
this March oh they just did this how did
I not see this we build large scale
I not see this we build large scale
multitask for in context r
multitask for in context r
it took 50,000 hours to collect this
it took 50,000 hours to collect this
okay well that's kind of
okay well that's kind of
sad it's funny they're still doing
that but yeah this
that but yeah this
one oh not this
one oh not this
one yeah this
one yeah this
one you'd probably look at this
okay so this totally failed as
well let's see what's going wrong
and I'm going to have to get some food
and I'm going to have to get some food
soon because my brain is not working
soon because my brain is not working
very well right
now that actually kind of gives you
now that actually kind of gives you
something reasonable right
something reasonable right
standard deviation is
reasonable that seems
suspicious yeah so this thing's exploded
oh hang on is this
oh hang on is this
not yeah you can't use this this has to
be have you worked with diffusion models
be have you worked with diffusion models
before
nope when I did CV um when I was in CV
nope when I did CV um when I was in CV
it was before diffusion models became a
thing
e e
that's it
stupid thing hate want to just break my
stupid thing hate want to just break my
uh my
shell field changes really
shell field changes really
fast well there hasn't really been a
fast well there hasn't really been a
major Improvement to uh core RL
major Improvement to uh core RL
algorithm since 2017 arguably 2015 so
algorithm since 2017 arguably 2015 so
that's what I'm trying to do I guess
that's what I'm trying to do I guess
2017 it would be
okay so here you have your advantages
interesting this is very weirdly
interesting this is very weirdly
scaled but this is roughly what you like
scaled but this is roughly what you like
you want something that looks like this
you want something that looks like this
it's just very weirdly
scaled uh this as well
yeah so this is
exploding well this is irritating um the
exploding well this is irritating um the
problem is I can't just be doing this
problem is I can't just be doing this
all day all day because I have tons of
all day all day because I have tons of
other stuff to be getting on to as well
other stuff to be getting on to as well
I think I'm going to go get some lunch
I think I'm going to go get some lunch
and then I'm going to think about this
and then I'm going to think about this
and I might just have to to go on to
and I might just have to to go on to
exploration side research for a
exploration side research for a
bit um this is going to take a while and
bit um this is going to take a while and
this is going to drive me
nuts Yeah It's just
tough so I suppose I will think about
tough so I suppose I will think about
this for a bit I'm going to get some
this for a bit I'm going to get some
food clear my head a little bit maybe
food clear my head a little bit maybe
and then I will be back
and then I will be back
afterwards and uh we'll be on research
afterwards and uh we'll be on research
for the whole rest of the day so uh for
for the whole rest of the day so uh for
folks watching now all my stuff at
folks watching now all my stuff at
puffer a are you still a PhD candidate
puffer a are you still a PhD candidate
nope I've graduated I've been working on
nope I've graduated I've been working on
puffer full-time for almost a year now
puffer full-time for almost a year now
made a lot of good
made a lot of good
progress um all my stuff's at puffer
progress um all my stuff's at puffer
doai you can check out what I've been
doai you can check out what I've been
doing lately just St the repo here go in
doing lately just St the repo here go in
Discord if you want to get involved with
Discord if you want to get involved with
Dev and uh follow on X for more
Dev and uh follow on X for more
content thank you and
