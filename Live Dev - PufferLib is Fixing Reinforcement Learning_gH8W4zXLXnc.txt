Kind: captions
Language: en
pack
low. I did a little bit of thinking and
low. I did a little bit of thinking and
I think we're
I think we're
fine. So the problem that I sort of left
fine. So the problem that I sort of left
I should provide some context here since
I should provide some context here since
this is start of stream and therefore
this is start of stream and therefore
start of baud. Uh we left off working on
start of baud. Uh we left off working on
prioritized experience
prioritized experience
uh buffer for
uh buffer for
PO
PO
and I thought that I've got this sort of
and I thought that I've got this sort of
this working idea that on policy verse
this working idea that on policy verse
off policy it's kind of all a lie. Um oh
off policy it's kind of all a lie. Um oh
no the day is off policy really doesn't
no the day is off policy really doesn't
matter that much. And I found one spot
matter that much. And I found one spot
that kind of throws a wrench into it but
that kind of throws a wrench into it but
I think it's still fine. So the idea and
I think it's still fine. So the idea and
there will be an article on
there will be an article on
this is that in
this is that in
PO or I mean policy gradient methods in
PO or I mean policy gradient methods in
general you
take I guess the formula is not going to
take I guess the formula is not going to
be in here.
Um, basically the formula doesn't depend
Um, basically the formula doesn't depend
on the old policy at
on the old policy at
all because you're computing new
all because you're computing new
logics. So there's actually if you look
logics. So there's actually if you look
at the math there's nowhere that the old
at the math there's nowhere that the old
policy is even remotely involved. Uh the
policy is even remotely involved. Uh the
one thing that is implicitly involved is
one thing that is implicitly involved is
that the value function gets stale. So
that the value function gets stale. So
the value function is trained to
the value function is trained to
predict well one form of it is trained
predict well one form of it is trained
to predict discounted rewards under the
to predict discounted rewards under the
current policy. So the value function is
current policy. So the value function is
just going to undershoot a little bit.
just going to undershoot a little bit.
Um it's just going to undersshoot the
Um it's just going to undersshoot the
current policy a bit. I don't think that
current policy a bit. I don't think that
should be a problem. It's also it's
should be a problem. It's also it's
fixed. The amount that it's going to
fixed. The amount that it's going to
undersshoot by is fixed because the
undersshoot by is fixed because the
policy can only improve so much from one
policy can only improve so much from one
batch of data, right? it can't
batch of data, right? it can't
infinitely improve. The value function
infinitely improve. The value function
cannot become infinitely stale. So I
cannot become infinitely stale. So I
think this is all fine and basically uh
think this is all fine and basically uh
the results that we're seeing where this
the results that we're seeing where this
isn't immediately working out of the
isn't immediately working out of the
box. Welcome to reinforcement learning.
box. Welcome to reinforcement learning.
You know things don't always work
You know things don't always work
immediately out of the
immediately out of the
box. Uh I think it's fine. We're kind of
box. Uh I think it's fine. We're kind of
seeing what we expect to see here. And
seeing what we expect to see here. And
we just have to tweak a few things is
all. So this is
1024. All
right. So what I want to do here
I want to change the replay buffer to
I want to change the replay buffer to
only be half of the batch
size and we will go from
there. Uh, let's actually log as
there. Uh, let's actually log as
well. But we're not going to run
well. But we're not going to run
experiments all day today. That's
experiments all day today. That's
really, really boring and I just don't
really, really boring and I just don't
feel like doing it. So, I mostly want to
feel like doing it. So, I mostly want to
uh work on getting this file nice and
uh work on getting this file nice and
clean
today. And there are lots of things that
today. And there are lots of things that
we can do with that.
We can definitely improve the kernel
today. I don't want to think about
today. I don't want to think about
putting this into a class or not yet.
putting this into a class or not yet.
We're going to punt on that for
today. We can get E3B and diversity all
today. We can get E3B and diversity all
you need running on the new
you need running on the new
codebase. That would probably be useful.
We can look at a little bit of the
We can look at a little bit of the
logging stats and
stuff and maybe at like some of the
stuff and maybe at like some of the
aggregation
aggregation
code. I think that will probably be
code. I think that will probably be
enough to
enough to
get the codebase into a good state.
should be enough to get the code base
should be enough to get the code base
into a good
state. There are a couple quick
state. There are a couple quick
experiments though that we can run and
experiments though that we can run and
if we get stuck we can kind of just like
if we get stuck we can kind of just like
hunt. So, if I just turn replay
off, I should just be able to disable
off, I should just be able to disable
this thing,
this thing,
right? Reprioritize the
right? Reprioritize the
experience. Yeah, you don't need this.
Okay, cuz this might not even be like
Okay, cuz this might not even be like
the onoff whatever policy This
the onoff whatever policy This
could just be something way simpler.
F
uses data. F
uses. Let's just do this.
Okay, there we go. So, this is now no
Okay, there we go. So, this is now no
stale
stale
data but uh prioritized
data but uh prioritized
experience or what is it? Advantage
experience or what is it? Advantage
filtering basically prioritized
filtering basically prioritized
advantage
filtering. And this is the baseline
filtering. And this is the baseline
which is
random. So far so good actually.
That's not
That's not
bad. Do you need to
bad. Do you need to
use That's all we're going to really
use That's all we're going to really
need to do on the experiment side
need to do on the experiment side
depending on this result for now. We'll
depending on this result for now. We'll
leave this and then we'll look at CUDA.
leave this and then we'll look at CUDA.
I think we can look at a bit of
I think we can look at a bit of
CUDA. CUDA's fun.
Oh yeah, there we
go. So what is this pi bind torch
go. So what is this pi bind torch
extension
extension
name? Do you need to use this?
I don't think you need to use this, do
I don't think you need to use this, do
you? Oh, wait. You have this like
weird. Okay. The one thing I will use
weird. Okay. The one thing I will use
Grock a little bit for is like
Grock a little bit for is like
new new libraries and binds.
Um because I'm I have not done really
Um because I'm I have not done really
much at all with pi bind and I don't
much at all with pi bind and I don't
know if it's necessary.
I'd be surprised if it's
I see. So you can do
it. Is this that bad?
Oh, wait. You can
Oh, wait. You can
[Music]
[Music]
use PyTorch build
tools.
tools.
Okay, so you can actually get CUDA
Okay, so you can actually get CUDA
extensions here.
PyTorch implicitly uses PIBind to create
PyTorch implicitly uses PIBind to create
the Python
the Python
bindings. You don't manually define
bindings. You don't manually define
PIBind
11. So you just define this. So that's
11. So you just define this. So that's
basically the same thing.
Is that
Is that
better or is that the same bloody
better or is that the same bloody
thing? Oh, also
thing? Oh, also
um yeah, there we go. So, uh advantage
um yeah, there we go. So, uh advantage
filtering nice and clean. Questionable
filtering nice and clean. Questionable
whether you get anything else from um
whether you get anything else from um
the prioritize replay. That seems to
the prioritize replay. That seems to
break stuff, but that's fine. Okay, now
break stuff, but that's fine. Okay, now
we do we do some CUDA stuff for a
bit.
Mdeaf. This is like the same bloody
Mdeaf. This is like the same bloody
code, isn't it?
Yeah, there's the same exact
Yeah, there's the same exact
code. How do I have this defined right
now? I guess I don't have it defined in
now? I guess I don't have it defined in
here.
load. Okay, this is the thing. I have it
load. Okay, this is the thing. I have it
right now just in here instead of in the
right now just in here instead of in the
setup.py.
I
I
wonder up
How hard is
that? Pre-ompile for specific platforms
that? Pre-ompile for specific platforms
and CUDA versions.
Yeah. So, that's a little
annoying. Distributing these is a bit
annoying. Distributing these is a bit
annoying.
So, we're not going to deal with that
So, we're not going to deal with that
just at the
moment. But that makes
moment. But that makes
sense. Not missing anything,
right? All
right. The one thing I think we can do
right. The one thing I think we can do
here is
move we can move all this code
move we can move all this code
from Python into
from Python into
C or C++ whatever it is.
If you allocate the tensor and torch, is
If you allocate the tensor and torch, is
it just fine or
no? Compute advantages compute J. So
no? Compute advantages compute J. So
right
here we can kind of do some stuff,
here we can kind of do some stuff,
right? So we do
in
steps size probably and
then I'm going to assume it's
then I'm going to assume it's
autocompleting torch API here and we'll
autocompleting torch API here and we'll
fix it if it's wrong.
then it's what is it? It synchronize at
then it's what is it? It synchronize at
the
end. I don't know if that's automatic or
end. I don't know if that's automatic or
not. What's the plan with
not. What's the plan with
J? Uh so since we're shipping our own
J? Uh so since we're shipping our own
implementation, we need to ideally first
implementation, we need to ideally first
of all I don't want to have a big
of all I don't want to have a big
annoying binding in the Python file. So
annoying binding in the Python file. So
we're shipping a
we're shipping a
pufferlib.cu or
pufferlib.cu or
whatever. I don't know why it's a CU
whatever. I don't know why it's a CU
honestly for this because it's a binding
honestly for this because it's a binding
file. That's kind of weird. Uh anyways,
file. That's kind of weird. Uh anyways,
we need to figure out how to make uh the
we need to figure out how to make uh the
thing that Torch does essentially where
thing that Torch does essentially where
you have a C version and a CUDA version
you have a C version and a CUDA version
and then you know it falls back to the C
and then you know it falls back to the C
version if you don't have
version if you don't have
CUDA. So we're going to figure that out
CUDA. So we're going to figure that out
today. That'll clean up the uh clean
today. That'll clean up the uh clean
puffer file a little bit and also will
puffer file a little bit and also will
let us run all this nice new stuff in
let us run all this nice new stuff in
CPU
CPU
mode. All this is is figuring out
mode. All this is is figuring out
bindings like the actual code. You
bindings like the actual code. You
literally paste the CPU code versus the
literally paste the CPU code versus the
GPU. It's the same thing. So that's
GPU. It's the same thing. So that's
easy. It's literally just uh binding
easy. It's literally just uh binding
help. The only tricky
thing figuring out how to do this will
thing figuring out how to do this will
also be useful though for um P30 because
also be useful though for um P30 because
we're going to need to do the same thing
we're going to need to do the same thing
down the line.
down the line.
Basically, this will all get you reused.
I don't think you need to call
I don't think you need to call
synchronize.
I also don't know how this works with
I also don't know how this works with
AMP.
I'm just going to paste this into gro
I'm just going to paste this into gro
for API
check. Why is there a death in here,
check. Why is there a death in here,
dummy?
having a lot of connectivity issues on
having a lot of connectivity issues on
box
box
two. Running sweeps and offline manually
two. Running sweeps and offline manually
syncing is bandage now. Okay. Also, make
syncing is bandage now. Okay. Also, make
sure when you log in there's a disk
sure when you log in there's a disk
space usage. make sure you're not
space usage. make sure you're not
getting up to 100% utilization. Uh
getting up to 100% utilization. Uh
there's like a torch bug that's causing
there's like a torch bug that's causing
the model checkpoints to blow up. So, I
the model checkpoints to blow up. So, I
had that happen. I'm going to have to
had that happen. I'm going to have to
take a few hours at some point soon to
take a few hours at some point soon to
like go through all the machines and see
like go through all the machines and see
what's going on. I think it's straight
what's going on. I think it's straight
up just ISP. It could be that a cable
up just ISP. It could be that a cable
got damaged as well, but I really don't
got damaged as well, but I really don't
know.
It's a total pain in the ass.
47%. Yeah, it should probably be like
47%. Yeah, it should probably be like
4%. So, my guess is just the uh the
4%. So, my guess is just the uh the
checkpoints. The checkpoints are huge
checkpoints. The checkpoints are huge
for some reason. I was getting like 2
for some reason. I was getting like 2
gigabytes per checkpoint or whatever
gigabytes per checkpoint or whatever
with Neural MMO 3. They should be like
megabytes.
Okay. Replace
Okay. Replace
this with this.
device. What's the D type of
device? Oh, I actually don't like the
device? Oh, I actually don't like the
way it wrote this at all.
You don't need to worry about manual
You don't need to worry about manual
manually manual memory management
manually manual memory management
because smart pointers blah blah blah
full. That's fine.
It launch
there. Okay. I hate C++. This code
there. Okay. I hate C++. This code
sucks. Like what the
sucks. Like what the
Tensor
Tensor
options.dtype device. I programmer who
options.dtype device. I programmer who
decided this is a good way to set
decided this is a good way to set
options needs to be
shot. Good thing this is only the
shot. Good thing this is only the
binding file.
binding file.
Holy hell, this is
horrible. Uh, I don't think that this is
horrible. Uh, I don't think that this is
a float, is it?
How do I make
Do you actually have to do AI dispatch?
Do you actually have to do AI dispatch?
What the hell?
at
dispatch. But now the Cuda kernel isn't
dispatch. But now the Cuda kernel isn't
going to be
isn't going to be good,
right? Maybe we back up on this
Yeah, let's do
that. Okay. So, does this now work?
I should no longer need this.
What's wrong with
it? Yesesh, this thing like fails on
it? Yesesh, this thing like fails on
const.
Really? That's really strict.
All right. Well, we're just going to
All right. Well, we're just going to
accept that this code's going to be
accept that this code's going to be
awful. Uh there's no real way
awful. Uh there's no real way
around having that binding code be awful
around having that binding code be awful
because it's PyTorch C++
because it's PyTorch C++
API. It's written in in idiomatic C++.
API. It's written in in idiomatic C++.
So if you want to interact at all with
So if you want to interact at all with
torch,
torch,
um yeah, the only alternative would be
um yeah, the only alternative would be
to do all this checks on Python side,
to do all this checks on Python side,
but then you end up adding more annoying
Python. At least now we get this DT type
Python. At least now we get this DT type
stuff, right?
What's
idiomatic? Well, it depends on if you
idiomatic? Well, it depends on if you
like C++. I personally hate C++ and
like C++. I personally hate C++ and
think it's a disgusting language. So,
think it's a disgusting language. So,
pretty much anytime I have to interact
pretty much anytime I have to interact
with it at all, it's just like throw up
with it at all, it's just like throw up
my hands and there's no way I can make
my hands and there's no way I can make
this code nice.
There just so many things about C++ that
There just so many things about C++ that
are just horrible horrible software
are just horrible horrible software
design. Hey Joe, what do you think of
design. Hey Joe, what do you think of
the new llama
the new llama
models? I it surpris This may surprise
models? I it surpris This may surprise
some people, but I do not keep up with a
some people, but I do not keep up with a
lot of the new model releases like at
lot of the new model releases like at
all. It really doesn't affect me one
all. It really doesn't affect me one
bit.
bit.
like they're like the new models are
like they're like the new models are
kind of doing some simple RL
kind of doing some simple RL
now in a way that doesn't affect
now in a way that doesn't affect
anything that I'm doing. And like like I
anything that I'm doing. And like like I
say, all the major labs are just going
say, all the major labs are just going
to like keep leaprogging each other by a
to like keep leaprogging each other by a
few ELO points for the next like every
few ELO points for the next like every
couple months for the next who knows how
long. It really doesn't change any of
long. It really doesn't change any of
the way that I do my work. It doesn't
the way that I do my work. It doesn't
really change much of how I think about
really change much of how I think about
anything, honestly.
I don't know. If Grock ever gets to be
I don't know. If Grock ever gets to be
like so far stale behind the other ones
like so far stale behind the other ones
that I would like get some benefit out
that I would like get some benefit out
of switching, then I'll switch to
of switching, then I'll switch to
whatever the best one is at the moment.
whatever the best one is at the moment.
But other than that, there's not really
But other than that, there's not really
like a big reason for any of it.
I pretty much use Brock to
I pretty much use Brock to
like semivet research ideas, but not
like semivet research ideas, but not
really because the paper search isn't
really because the paper search isn't
great. I use it to quickly convert ideas
great. I use it to quickly convert ideas
into a text, so I don't have to type it
into a text, so I don't have to type it
up. And I use it essentially for like
up. And I use it essentially for like
APIs and things I'm not familiar with
APIs and things I'm not familiar with
yet. That's about it.
Uh, why is
this why is this not float?
Oh,
because QJ. Yeah, you don't need
because QJ. Yeah, you don't need
advantages
now. Yeah, this goes
away. Get rid of this.
advantages for you and return advantage.
Uh welcome
Uh welcome
folks there few across YouTube and
folks there few across YouTube and
Twitch.
Twitch.
Now what we're doing right now uh we are
Now what we're doing right now uh we are
working on the
working on the
new prioritized experience replay buffer
new prioritized experience replay buffer
for PO that required us to write a CUDA
for PO that required us to write a CUDA
kernel for JE because JA gets called way
kernel for JE because JA gets called way
more often now with this new code. And
more often now with this new code. And
uh we are currently cleaning up and
uh we are currently cleaning up and
getting the CUDA kernel to work nicely,
getting the CUDA kernel to work nicely,
get it nicely bound. And then we're also
get it nicely bound. And then we're also
going to have to do the CPU fallback,
going to have to do the CPU fallback,
which is a uh C implementation. It's
which is a uh C implementation. It's
actually the same code. We're going to
actually the same code. We're going to
be able to do that very cleanly, but
be able to do that very cleanly, but
that's what we're doing at the moment.
that's what we're doing at the moment.
Just jamming some low-level dev for
Just jamming some low-level dev for
Puffer. Generally having a good
time. Values, rewards, stuns.
float
float
in. Ah, you don't need this
either. Steps horizon.
you mentioned yesterday on versus off
you mentioned yesterday on versus off
policy is a myth. Can you explain that?
policy is a myth. Can you explain that?
Confused as I've read on policy priority
Confused as I've read on policy priority
replay doesn't work or is tough for a
replay doesn't work or is tough for a
while. Well, I could be wrong still, but
um in what way does off policy break
um in what way does off policy break
this?
I don't see anywhere where off policy
I don't see anywhere where off policy
breaks this
right.
So it's not easy to see from this math
So it's not easy to see from this math
definition. It's a lot easier to see
definition. It's a lot easier to see
from the code. Let me look at the let me
from the code. Let me look at the let me
show you the code.
because they use like fancy they use
because they use like fancy they use
fancy ass
fancy ass
notation. Okay,
notation. Okay,
so here's the policy gradient loss,
so here's the policy gradient loss,
right? PG
right? PG
loss. Okay, so it's advantage. Advantage
loss. Okay, so it's advantage. Advantage
doesn't get a
doesn't get a
gradient. So advantage is like
gradient. So advantage is like
essentially a
essentially a
constant times
constant times
ratio. Ratio is
ratio. Ratio is
log
log
ratio.x log ratio
ratio.x log ratio
is new log props
is new log props
uh minus log
props.reshape. Oh, hang on. Maybe there
props.reshape. Oh, hang on. Maybe there
is a PO specific thing. This is not in
is a PO specific thing. This is not in
vanilla J. I mean
vanilla J. I mean
vanilla gradient
estimation. Uh no because this is still
estimation. Uh no because this is still
clipped. Hang
clipped. Hang
on. That's only for a prop
on. That's only for a prop
scale
scale
ratio. God damn it. Now I'm confusing
myself. Oh, because this is a constant.
I went through this the other day and I
I went through this the other day and I
found I found I found something with it.
found I found I found something with it.
But now I'm trying to find where the
But now I'm trying to find where the
freak like where was the nice clean
freak like where was the nice clean
version that I was looking at. I'm
version that I was looking at. I'm
trying to remember if I looked at the J
trying to remember if I looked at the J
paper, the PO paper. It was the PO
paper, the PO paper. It was the PO
paper.
Yeah. Okay. So,
So this is just the clipping term. The
So this is just the clipping term. The
old policy is only used for the clipping
old policy is only used for the clipping
term and the new
term and the new
policy is used on the state and the uh
policy is used on the state and the uh
conditioned on the actions. So this is
conditioned on the actions. So this is
getting new logics using the new
policy. So this clipping term is
policy. So this clipping term is
irrelevant then because this is
irrelevant then because this is
essentially a constant. This advantage
essentially a constant. This advantage
term doesn't get a gradient. So this is
term doesn't get a gradient. So this is
not back propagated through at
all. So this doesn't depend on stale
all. So this doesn't depend on stale
date like this doesn't depend on
date like this doesn't depend on
anything stale at
anything stale at
all. Right?
The reasoning that I went through a
The reasoning that I went through a
while ago was it's it's a like the usual
while ago was it's it's a like the usual
reason given for like oh it's stale like
reason given for like oh it's stale like
stale data is bad. It's like the data
stale data is bad. It's like the data
comes from the old policy. But the thing
comes from the old policy. But the thing
is you don't use the logits from the old
is you don't use the logits from the old
policy here. You use new
policy here. You use new
logics. You use updated logics that you
logics. You use updated logics that you
compute from running the forward pass
compute from running the forward pass
again.
So basically, I don't see anywhere where
So basically, I don't see anywhere where
off policy data would screw you up. And
off policy data would screw you up. And
as a counterpoint, PO is immediately off
as a counterpoint, PO is immediately off
policy anyways after the first mini
policy anyways after the first mini
batch.
I was even starting to wonder if you can
I was even starting to wonder if you can
delete the clip term. Anyways,
like I'm suspicious of the thing
like I'm suspicious of the thing
completely crashing without clipping.
Okay. Oh yeah, it's dominated by one
Okay. Oh yeah, it's dominated by one
experiment.
What was I doing
here? Do not know why it formats my code
here? Do not know why it formats my code
this
this
way. Horrible code of
formatting.
formatting.
Yes. Okay, that runs now. Yay.
Now, how do we get this to be a
Now, how do we get this to be a
fallback? We have to get this to be a
fallback? We have to get this to be a
fallback.
Use conditional compilation in the
binding.
J.CP. Let's
see.
See, the problem with getting stuff like
See, the problem with getting stuff like
this from Grock is I don't know if the
this from Grock is I don't know if the
code is just generally if the state of
code is just generally if the state of
like binding code is genuinely this
like binding code is genuinely this
garbage or if it's just Grock being bad
garbage or if it's just Grock being bad
at coding.
CPU
CPU
implementation. Okay. So they have it's
implementation. Okay. So they have it's
funny it implemented
J. Worse than my version though.
J if death with CUDA
uh that's kind of
off. Oh, it's still doing the garbage.
Um, it's still doing that garbage
Um, it's still doing that garbage
like AMP thing.
Okay. So, compute
Okay. So, compute
J is still the
same. Is there Do you have to freaking
same. Is there Do you have to freaking
macro it? That's like disgusting.
Okay, so here's
your J Cuda kernel.
Okay, this is potentially better.
rely on the build system to include or
rely on the build system to include or
exclude the CUDA file.
J.H.
So this is attemp this is uh attempting
So this is attemp this is uh attempting
to
be
weird. It just has this structured
weird. It just has this structured
weirdly. It's going to take me a few
weirdly. It's going to take me a few
minutes to figure this out.
And then what is this?
Extensions. Oh, but this still doesn't
Extensions. Oh, but this still doesn't
work.
Oh, no. It's It does
So, let me see what this is. So, this is
So, let me see what this is. So, this is
the binding
the binding
file. Declare
file. Declare
both. Oh, but then they're using like
both. Oh, but then they're using like
repeated declaration. That's really
repeated declaration. That's really
obnoxious.
That's really obnoxious.
It's funny how it's always tool chain
It's funny how it's always tool chain
crap. That's
crap. That's
like the hard part,
like the hard part,
right? Writing the kernel is super easy.
If death with CUDA
Oh, wait. Does it need for
declarations? It does need four
declarations? It does need four
declarations, doesn't
it? That's annoying. That's really
it? That's annoying. That's really
annoying.
Is there a good way? I mean, this has
Is there a good way? I mean, this has
got to be a solved problem, right? This
got to be a solved problem, right? This
is just like, is there a good way to do
is just like, is there a good way to do
CUDA with CPU fallbacks without having
CUDA with CPU fallbacks without having
to do a bunch of shitty forward
to do a bunch of shitty forward
declarations
declarations
uh with conditional comp like
uh with conditional comp like
conditional
conditional
compilation, right?
compilation, right?
like the thing I'm trying to do is very
like the thing I'm trying to do is very
very simple
very simple
here. So is it
here. So is it
advantage? All right. So this is the
advantage? All right. So this is the
CUDA file. Really it's just this piece
CUDA file. Really it's just this piece
at the bottom. Uh the plan is I'm going
at the bottom. Uh the plan is I'm going
to take this and I'm going to just paste
to take this and I'm going to just paste
this into C. And then literally all this
this into C. And then literally all this
is going to do is call uh is call the
is going to do is call uh is call the
function, right? Because this is just
function, right? Because this is just
ge.
ge.
So this is
So this is
tiny and then this thing all this needs
tiny and then this thing all this needs
to do is export one binding for the CUDA
to do is export one binding for the CUDA
version and one binding for this uh the
version and one binding for this uh the
C
C
version. So I should be able to end up
version. So I should be able to end up
with like
a is this a C a CUDA file or C C++ file?
a is this a C a CUDA file or C C++ file?
This isn't a CUDA file. This is just
This isn't a CUDA file. This is just
named pufferlib.cu. This should be a C++
named pufferlib.cu. This should be a C++
file. So I should end up with a
file. So I should end up with a
pufferlib.h H I should end up with a
pufferlib.h H I should end up with a
pufferlib.cu and a pufferlib. C++
basically or not. Yeah, a pufferlib.h
basically or not. Yeah, a pufferlib.h
that contains the C
that contains the C
implementation, right? I end up with a
implementation, right? I end up with a
pufferlib.cu that contains the CUDA
pufferlib.cu that contains the CUDA
implementation and I end up with uh with
implementation and I end up with uh with
this
this
thing as well, this binding file.
But the problem is you have to compile
But the problem is you have to compile
the CUDA. Like if you're trying to use
the CUDA. Like if you're trying to use
the CPU version, then the CUDA is not
the CPU version, then the CUDA is not
going to compile if you don't have a
GPU. How does Torch do
GPU. How does Torch do
this? Is there like a clean piece of
this? Is there like a clean piece of
Torch code I could look at as a
Torch code I could look at as a
reference for this or no?
Yeah. So, this is exactly what I want to
Yeah. So, this is exactly what I want to
do like this, right?
I
mean, could I conditionally import
instead? Okay, wait. Here's an idea,
instead? Okay, wait. Here's an idea,
right? I could conditionally import
right? I could conditionally import
instead.
So I
have I could end up with two files
have I could end up with two files
instead. Right. I could
instead. Right. I could
have
have
a puffer lip CUDA and a puffer lip
a puffer lip CUDA and a puffer lip
CPU.
Right. That should make
sense. The CUDA one can import the CPU
sense. The CUDA one can import the CPU
one as well.
one as well.
Let me
Let me
commit. Well, no, because it doesn't
commit. Well, no, because it doesn't
build in the current state,
right? Does it build in the current
state? If it builds in the current
state? If it builds in the current
state, then we
will we'll commit this up and then I
will we'll commit this up and then I
will mess with it. And I think I think I
will mess with it. And I think I think I
have a pretty clean way in mind of
have a pretty clean way in mind of
making this all work.
making this all work.
Welcome YouTube folks since we have some
Welcome YouTube folks since we have some
folks uh that just joined. This is
folks uh that just joined. This is
currently getting a generalized
currently getting a generalized
advantage estimation CUDA kernel working
advantage estimation CUDA kernel working
with a graceful CPU fallback for puffer
with a graceful CPU fallback for puffer
lib uh because puffer lib is so fast
lib uh because puffer lib is so fast
that generalized advantage can become a
that generalized advantage can become a
bottleneck. So we I made it I wrote it
bottleneck. So we I made it I wrote it
in C first and now it's actually become
in C first and now it's actually become
so it actually has to now call that so
so it actually has to now call that so
often that the C is the bottleneck. So
often that the C is the bottleneck. So
now I have to have a CUDA implementation
now I have to have a CUDA implementation
of it and we're trying to get all this
of it and we're trying to get all this
working cleanly in puffer lib so you
working cleanly in puffer lib so you
basically never have to worry about
it. Yeah. Okay. So this works. Let's
it. Yeah. Okay. So this works. Let's
commit this up and then we will uh we
commit this up and then we will uh we
will implement the thing that I have in
will implement the thing that I have in
mind
mind
topl and then
popl.cu
addex. This is also a good time to
addex. This is also a good time to
remind folks to please start puffer liib
remind folks to please start puffer liib
on GitHub. It really helps us out. It's
on GitHub. It really helps us out. It's
free. We're almost at 2K.
free. We're almost at 2K.
back to
code. Okay, so this
code. Okay, so this
puffer.cu should be
puffer.cu should be
uh
uh
pufferlib
cuda.cpp. And let me make sure that that
cuda.cpp. And let me make sure that that
still works.
Okay. And then the idea is going to be
Okay. And then the idea is going to be
that we take most of these checks go
that we take most of these checks go
into the uh
into the uh
CPP. Oops. What did I do
wrong? Global does not have a type name.
wrong? Global does not have a type name.
Does this have to be a CUDA file?
Oh, CUDA is the super. Okay. Yeah. So,
Oh, CUDA is the super. Okay. Yeah. So,
this does have to be a CUDA file. That's
this does have to be a CUDA file. That's
fine. So, then that's that's even easier
fine. So, then that's that's even easier
actually because then this is
actually because then this is
pufferlib.cu.
And then it's pufferlib.cpp.
And then it's pufferlib.cpp.
Easy. And then we make
um copy
um copy
pufferlib
pufferlib
cuerlib
cuerlib
cpp. Okay. And the cpp is going to
cpp. Okay. And the cpp is going to
contain basically everything that
contain basically everything that
doesn't have to do with cuda. And the
doesn't have to do with cuda. And the
idea is we're going to reuse uh as much
idea is we're going to reuse uh as much
of this as
of this as
possible. And in order to do that, we
possible. And in order to do that, we
need we need the CUDA
code. Yeah, we need the CUDA code. So,
code. Yeah, we need the CUDA code. So,
puffer
[Music]
[Music]
lib.cu. Yeah, this this code down here.
lib.cu. Yeah, this this code down here.
We need this
So this is a CUDA function. How are we
So this is a CUDA function. How are we
going to call a CUDA function from C++?
going to call a CUDA function from C++?
We're not. We're going to do the
We're not. We're going to do the
opposite. What we're going to do here
uh we are going to
do this is
void
ge. This takes
a 1D
a 1D
tensor
horizon. Same signature,
horizon. Same signature,
right? Uh, this needs to know horizon,
right? Uh, this needs to know horizon,
but no longer needs to know num
but no longer needs to know num
steps. It's not going to have access to
steps. It's not going to have access to
row anymore.
So, idx is going to be
row time horizon. That's
nothing. We can actually just delete
nothing. We can actually just delete
this line. And all we need is t next is
this line. And all we need is t next is
going to be t + one. So, this is
going to be t + one. So, this is
actually even going to clean up the code
actually even going to clean up the code
a little bit. All right. And then you're
a little bit. All right. And then you're
going to see that this should work with
going to see that this should work with
both CUDA and uh C++ without without us
both CUDA and uh C++ without without us
having to redo all the
code. I mean, without us having to
code. I mean, without us having to
duplicate all the
code. Let me see if I missed
anything. Yeah, that should be fine. So,
anything. Yeah, that should be fine. So,
this is now J operating over a single
this is now J operating over a single
row of the tensor. So if we pass in the
row of the tensor. So if we pass in the
pointer to a specific row, this will
pointer to a specific row, this will
compute J for that
row. Okay.
And I think that autocomplete might have
And I think that autocomplete might have
even just done it for
even just done it for
us. So
us. So
values plus row times horizon.
Yeah. I think we want to do
pointer.
[Music]
Pointer. And this is offset, not
Pointer. And this is offset, not
pointer.
That should be slightly better as
well. Okay. Gamma, lambda, and horizon.
well. Okay. Gamma, lambda, and horizon.
Perfect.
I hate how it like expands this code to
I hate how it like expands this code to
take up a mill bajillion lines for no
take up a mill bajillion lines for no
reason as
well. Yeah, this
So let's just clean this up so it
So let's just clean this up so it
doesn't take up a million
doesn't take up a million
lines for what is really a very simple
lines for what is really a very simple
function. And then you'll see that this
function. And then you'll see that this
is very compact. And then the idea is
is very compact. And then the idea is
going to be that we will call the uh J
going to be that we will call the uh J
row is just going to get called from
row is just going to get called from
CUDA and C++ or C. It's technically C
CUDA and C++ or C. It's technically C
code.
code.
Uh so that'll get called from
Uh so that'll get called from
both and in this case we do the loop
both and in this case we do the loop
ourselves and for CUDA you know CUDA is
ourselves and for CUDA you know CUDA is
just parallel C so it'll loop over the
just parallel C so it'll loop over the
rows for us nicely with very minimal
rows for us nicely with very minimal
additional setup
required. That's fine.
and uh we should hope that the compiler
and uh we should hope that the compiler
inlines this for us. We might have to
inlines this for us. We might have to
give it some encouragement, but it's
give it some encouragement, but it's
probably not going to matter even if it
probably not going to matter even if it
doesn't
realistically. Vantages
So that's it. These are the functions
So that's it. These are the functions
that we need. This is not going to
that we need. This is not going to
include
include
uh C
uh C
advantage. We do need torch extension I
advantage. We do need torch extension I
believe.
And let's actually just finish this
And let's actually just finish this
binding right now.
Uh, we're not really doing P30 yet,
Uh, we're not really doing P30 yet,
right? I'm going to get rid of this for
right? I'm going to get rid of this for
now. We'll leave the CUDA version. We're
now. We'll leave the CUDA version. We're
doing
J. And I think we're going to clean up
J. And I think we're going to clean up
the way that this function is defined as
the way that this function is defined as
well. It's kind of silly the way it
is. So, we have
is. So, we have
device. We do all our
device. We do all our
checks. In fact, I think we're going to
checks. In fact, I think we're going to
do uh
check check tensor or whatever.
So we can do like
JHF
JHF
answer
check and it needs to
check and it needs to
know that's it actually. That's all it
know that's it actually. That's all it
needs to
needs to
know. Let me just paste all this
stuff. Okay, so we're down to a very
stuff. Okay, so we're down to a very
small amount of code here, right?
small amount of code here, right?
We no longer need threads per block or
We no longer need threads per block or
blocks because now we just call
blocks because now we just call
J which is the CPU version
here on values data pointer
float words data pointer
float words data pointer
float data pointer
float data pointer
float and
float and
horizon like
so and then we don't need CUDA errors
so and then we don't need CUDA errors
because this is not
CUDA.
CUDA.
Okay. So, it's going to look something
Okay. So, it's going to look something
like this. Now these module defs I might
like this. Now these module defs I might
have to fiddle
have to fiddle
with. Uh but the goal is now going to be
with. Uh but the goal is now going to be
we import this file from the other file.
So this is puffer lip
cu
CPP. Uh and then this C advantage. I
CPP. Uh and then this C advantage. I
don't even need
don't even need
this cuz this CUDA
file and just go at the top here.
Now you have your
kernel and uh this J kernel should
kernel and uh this J kernel should
become
simpler because this kernel is actually
simpler because this kernel is actually
really just a
really just a
row.
So you have access to all the data but
So you have access to all the data but
you only are given a row.
I should be able to rewrite this
I should be able to rewrite this
function. We use
that. This should be J row
Oh, that works, doesn't
it? I think that does
work.
work.
Offset. We'll do offset road times
horizon
set. Not
set. Not
bad. And now we can get rid of
bad. And now we can get rid of
this. This is the whole code.
because this is row
because this is row
parallel. So, uh we just need to call
parallel. So, uh we just need to call
from here. We literally just call the
from here. We literally just call the
one row function and we're done. And as
one row function and we're done. And as
soon as we make these signatures not
soon as we make these signatures not
take up 10 pages of code,
take up 10 pages of code,
uh this will actually be pretty concise.
And we don't need J row because it's
And we don't need J row because it's
included. So this is our whole kernel.
included. So this is our whole kernel.
This is
it. And we will leave the compute P301
it. And we will leave the compute P301
alone there.
alone there.
Uh we will add this compute J. We have
Uh we will add this compute J. We have
the validate tensors thing right.
See
See
validate check J check it
is G A check. Yeah like
this tensor advantages J check. Yep.
this tensor advantages J check. Yep.
Get rid of
Get rid of
this. And then we have our
this. And then we have our
kernel, right? And this is how it gets
kernel, right? And this is how it gets
called. And then we do have the CUDA
called. And then we do have the CUDA
track, but that's
it. So that is substantially more
it. So that is substantially more
compact. Now things I'm going to run
compact. Now things I'm going to run
into here.
into here.
Um I don't know about that module depth.
torch extension name
M. Yeah, I might end up in like with
M. Yeah, I might end up in like with
like dual declaration or whatever
like dual declaration or whatever
problems, but let's let's see if we can
problems, but let's let's see if we can
get this thing to compile.
Some steps
undefined,
right? So this check has to
get steps.
That way we don't duplicate
this. Okay. And then we'll do the same
thing. All
right, that
works. The only thing I don't know how
works. The only thing I don't know how
to get around is potentially the uh
to get around is potentially the uh
double module deaths. We'll have to
double module deaths. We'll have to
think about a second.
The rest of this should just be a little
The rest of this should just be a little
bit of standard debugging to get this
bit of standard debugging to get this
running. Okay. Num steps undefined. I
running. Okay. Num steps undefined. I
forgot to include
forgot to include
uh type
Oh, we don't need to include a CUD to
Oh, we don't need to include a CUD to
check as well. That'll screw up the C++.
check as well. That'll screw up the C++.
But we should put it
here. Get out of here, bot.
I build the box around
here. Expected and
here. Expected and
identifier. Yep. Can mess that
up. Nice.
Okay, perfect.
Okay, perfect.
So, we're down to only uh module defaf
So, we're down to only uh module defaf
errors. So, I have to understand how
errors. So, I have to understand how
this module defaf works
this module defaf works
and how to not duplicate it potentially.
Can we
defer? This thing did something that I
defer? This thing did something that I
liked before with uh with the module
liked before with uh with the module
defs.
defs.
I don't know if it solves the problem,
I don't know if it solves the problem,
but it should clean it up a little
bit. It's above this
I asked about
the Python API in
here. Was that in a previous message?
Or was it after
this? That must have been
this? That must have been
before. Yeah, it was over
here. Okay. So here uh you can take the
here. Okay. So here uh you can take the
functions out of
this which is worth
this which is worth
doing
right. You can take the wrapper
right. You can take the wrapper
functions out but you still end up with
functions out but you still end up with
this dev problem where you have declared
this dev problem where you have declared
the module twice.
I mean, I could pretty easily just
like put the shared stuff in a shared
like put the shared stuff in a shared
file, right?
So then that's three files instead of
So then that's three files instead of
two. But I can solve the problem with
two. But I can solve the problem with
that. Let's try that before I do
that. Let's try that before I do
anything
anything
silly. So puffer lib C++ puffer lib
silly. So puffer lib C++ puffer lib
cu.
cu.
Um what do I name it?
Um what do I name it?
J check.
It's annoying because I actually want to
It's annoying because I actually want to
share like the some of the C++ utils I
guess even though this is just See?
Let's do uh what is it? Void or torch
Let's do uh what is it? Void or torch
tensor.
I think for now I just I name it
I think for now I just I name it
something and then we'll figure out how
something and then we'll figure out how
to organize this after.
to organize this after.
So, um we'll just do this and
shared.
Okay. Okay. So, this is now in
shared
shared
share.cpp. So, now I have this binding
share.cpp. So, now I have this binding
here.
You need the
extension and then I need to define
extension and then I need to define
these things because this is really
these things because this is really
silly. So this is
void
P3L. Just put all this
P3L. Just put all this
code through
here. Okay, there's compute P30 and
this sand.
just
do comput
do comput
J and then we just take all this
J and then we just take all this
code put this
There. All right.
So, we now
So, we now
have a fair bit of
have a fair bit of
stuff. I've been shared. This doesn't
stuff. I've been shared. This doesn't
need to go
unshared. This can go
here. And uh now we should actually be
here. And uh now we should actually be
able to compile this if I have not made
able to compile this if I have not made
any mistakes.
and we will be able to conditionally
and we will be able to conditionally
load whatever
load whatever
extension. Maybe not the best solution,
extension. Maybe not the best solution,
but this is a relatively simple
but this is a relatively simple
one that doesn't require us doing a
one that doesn't require us doing a
bunch of crazy forward
declarations. J row is undefined in
declarations. J row is undefined in
device code.
Do these need to be marked underscore
Do these need to be marked underscore
global or some shenanigans.
QJ
J
J
kernel then this calls J
row. What if I do that?
global function call must be
global function call must be
configured. What does that mean?
You must specify how many threats will
You must specify how many threats will
be executed. This is the launch
be executed. This is the launch
configuration. Okay, so we don't want to
configuration. Okay, so we don't want to
do
do
that because we just want to call
it. So this should just be a C function.
post
post
function runs on CPU and can only be
function runs on CPU and can only be
called from other CPU code.
Lovely. Global
Lovely. Global
function runs on GPU.
This is very
silly of
underscore_host
Nice. That's very silly.
Okay, this now runs
Okay, this now runs
good and it seems to be training
good and it seems to be training
correctly as
correctly as
well.
well.
So now we see if we can import the CPU
So now we see if we can import the CPU
version and also try that as a fallback.
It's
device.
Check
that course.
so this can stay up
top sources equals
See you.
There we
There we
go. And then I just need to
do this is compute
J. Let's see if that works.
Okay, this one still
runs. Now we have to see if the CPU one
runs. Train device
CPU and we get some
error. Post does not name a
type. Yeah, that's really obnoxious.
Let's
see. CPU.
Yeah. So, obviously this isn't
available. Yep. Okay.
I think I can do this for
now. But when does the CUDA arc get
now. But when does the CUDA arc get
defined? I guess it gets this add this
defined? I guess it gets this add this
gets added by NVCC probably.
Find better way to
Find better way to
condition. And that's the wrong comment
condition. And that's the wrong comment
line.
Okay, cool. So, we now have this
Okay, cool. So, we now have this
training on
training on
CPU. We'll see uh we should at least let
CPU. We'll see uh we should at least let
it run a little bit to make sure that
it run a little bit to make sure that
the results are the same. Uh but they
the results are the same. Uh but they
should be since it's now literally
should be since it's now literally
running the same code.
And that looks pretty
And that looks pretty
good. And it's very funny to me that
good. And it's very funny to me that
this feels slow because it's running on
this feels slow because it's running on
CPU, but this is running like 20 times
CPU, but this is running like 20 times
faster generously than uh the majority
faster generously than uh the majority
of RL
of RL
research. So there we go. We have uh
research. So there we go. We have uh
CPU, we've got GPU fallback. Well, C, we
CPU, we've got GPU fallback. Well, C, we
have GPU code with CPU
have GPU code with CPU
fallback. That's not
fallback. That's not
bad. We'll have to clean this up over
bad. We'll have to clean this up over
time obviously, but at least we now have
time obviously, but at least we now have
this for debugging purposes. Yeah. And
this for debugging purposes. Yeah. And
this is matching the learn purpose well,
this is matching the learn purpose well,
so we're good. And if I run this one,
so we're good. And if I run this one,
hopefully this still
works. See you.
GPU kernel plus CPU
GPU kernel plus CPU
all
the
right. Uh so that nicely cleaned some
right. Uh so that nicely cleaned some
stuff up for
stuff up for
us. So, we can just delete this now
us. So, we can just delete this now
because we no longer need
because we no longer need
this.
Gone. And
um I'm going to just go ahead and put
um I'm going to just go ahead and put
this
into pufferlib.cu as a comment.
All right, I'm just going to put this in
All right, I'm just going to put this in
here as a
comment. Work from
Now we no longer need
this. So that will uh that will get this
this. So that will uh that will get this
substantially
shorter. This doesn't need to be global
shorter. This doesn't need to be global
either. I don't know why it's there.
So, we're now down to 1075 lines in um
So, we're now down to 1075 lines in um
clean puff RL with
clean puff RL with
that. Not bad. Not bad at
that. Not bad. Not bad at
all. Yes, we still use default dick. We
all. Yes, we still use default dick. We
still use a few other things, but it is
still use a few other things, but it is
substantially better.
It's 1:27. I'm going to use the restroom
It's 1:27. I'm going to use the restroom
real quick. I'm going to grab a new cup
real quick. I'm going to grab a new cup
of
of
tea. Maybe just get some hot water for
tea. Maybe just get some hot water for
this. And uh we're going to continue on
this. And uh we're going to continue on
and keep cleaning up this file. And then
and keep cleaning up this file. And then
this should be in a pretty good spot.
this should be in a pretty good spot.
Be right back.
Okay, we are
Okay, we are
back. Did it kick me offline or is the
back. Did it kick me offline or is the
UI just
broken? I think the UI is just broken.
broken? I think the UI is just broken.
The heck? Oh, yeah. There we're good.
Okay. So, I think I realized what's
Okay. So, I think I realized what's
happening with the uh the replay buffer
happening with the uh the replay buffer
by the way. So, it's kind of an onoff
by the way. So, it's kind of an onoff
policy
policy
issue. It depends how you define it. The
issue. It depends how you define it. The
problem is the value function though,
problem is the value function though,
not the policy. It's not that the policy
not the policy. It's not that the policy
is stale. The policy is fine. Uh it's
is stale. The policy is fine. Uh it's
the value function.
the value function.
So the issue
So the issue
is the value function is going to get
is the value function is going to get
newer. Hang on. Does that make
newer. Hang on. Does that make
sense? Yeah. So the value function is
sense? Yeah. So the value function is
trying to predict
trying to predict
uh what's going to
uh what's going to
happen at each time step basically when
happen at each time step basically when
the policy does its
the policy does its
thing. Uh but when the policy gets
thing. Uh but when the policy gets
fresher, the value function hasn't
fresher, the value function hasn't
caught up yet.
So, like the policy optimize is just
So, like the policy optimize is just
fine on this, but I think it screws up
fine on this, but I think it screws up
the value function. I think that's
the value function. I think that's
what's going on. I'll have to really
what's going on. I'll have to really
work through the math details more, but
work through the math details more, but
it's fine. Um, what we're doing with uh
it's fine. Um, what we're doing with uh
experience filtering a
experience filtering a
okay. In fact, yeah, what we're doing
okay. In fact, yeah, what we're doing
with experience filtering is a okay. uh
with experience filtering is a okay. uh
the replay across
the replay across
epochs not so
epochs not so
much but uh like we can keep it like
much but uh like we can keep it like
this for now. This should be this should
this for now. This should be this should
be basically
fine and then we will I will start
fine and then we will I will start
thinking about what modifications we
thinking about what modifications we
would need to make to make the uh the
would need to make to make the uh the
full experience replay work. But we have
full experience replay work. But we have
it implemented now and we can do it. So
it implemented now and we can do it. So
that's solid.
that's solid.
Um I think now we just we take a look at
Um I think now we just we take a look at
this at a high level again and we look
this at a high level again and we look
for more things to improve on
for more things to improve on
and then we probably we fix a few of the
and then we probably we fix a few of the
like annoying bad pieces of code that
like annoying bad pieces of code that
are in here and then we fix uh like E3B
are in here and then we fix uh like E3B
diversity all you need. We fix those
diversity all you need. We fix those
algorithms and then we should be good.
the losses we have are kind of screwy
the losses we have are kind of screwy
here,
right? Loss is kind of screwy.
episode
buffer. Maybe there are a few lines we
buffer. Maybe there are a few lines we
can cut out overall, but this is like
can cut out overall, but this is like
pretty
dense. E3B diversity all you need. Yep,
dense. E3B diversity all you need. Yep,
we'll need
we'll need
those. E30 also
those. E30 also
that needs to get updated.
learning rate schedulers
learning rate schedulers
[Music]
[Music]
am profile dashboard this crazy thing
am profile dashboard this crazy thing
again we're still going to punt on the
again we're still going to punt on the
structure of this file whether this goes
structure of this file whether this goes
in a class or whether we have some
in a class or whether we have some
better way of doing this we'll
see why not on Twitch I am on Twitch I'm
see why not on Twitch I am on Twitch I'm
on X YouTube and
Twitch. I believe it's neural MMO on
Twitch. I believe it's neural MMO on
YouTube because that just that's the
YouTube because that just that's the
channel that blew up. Yeah, it's on
channel that blew up. Yeah, it's on
Twitch X and
Twitch X and
YouTube. Uh Twitch username is the same
YouTube. Uh Twitch username is the same
as the X username. The YouTube one is
as the X username. The YouTube one is
Neural
Neural
MMO because that's where I posted my
MMO because that's where I posted my
thesis. So, that's the channel that blew
thesis. So, that's the channel that blew
up.
All
right. Little bit of shenanigans here
right. Little bit of shenanigans here
with how we're managing X. I mean, uh,
with how we're managing X. I mean, uh,
not X, how we're managing H&C, LSTM
not X, how we're managing H&C, LSTM
state. Little bit
state. Little bit
verbose. Little bit verbose with how
verbose. Little bit verbose with how
we're doing N
we're doing N
byD. A little bit of screwiness with the
byD. A little bit of screwiness with the
masking and whatnot.
Nothing like major that I see
though. We've got the
state. Yeah, it's mostly just DI a and
state. Yeah, it's mostly just DI a and
E3B being screwy.
And cool. Then the store
And cool. Then the store
happens. Info log
happens. And this is kind of fine.
And we have this
And we have this
stuff which occasionally people send us
stuff which occasionally people send us
like numpy arrays and shenanigans. It's
like numpy arrays and shenanigans. It's
kind of fine either
way
way
stats
stats and then this stuff for zeroing
stats and then this stuff for zeroing
the experience buffer.
I don't think you need to do this here.
I don't think you need to do this here.
I think we can
do config data
configures, right?
Okay. Zero the losses
out back up to the
top. Is there not like a cross entropy?
cross. So this is what's
cross. So this is what's
it
forge.n
forge.n
functional cross
entropy that can go there. We no longer
entropy that can go there. We no longer
need that
need that
mass. Then we have our total mini
mass. Then we have our total mini
batches
batches
here. Accumulate mini bags and just end
samples. So this is P30. The PO1 is one
samples. So this is P30. The PO1 is one
line. All this will eventually get moved
line. All this will eventually get moved
into the kernel most likely or whatever.
into the kernel most likely or whatever.
So this won't be a big deal.
So this won't be a big deal.
Then you get your
Then you get your
sample your
sample your
batch action dispatch. LSTMH
LSTMC. Then you do
train. You update the values.
LSTMH
LSTMH
LSTMC. I don't think we need these
anymore. Yeah, we don't need these
anymore. Yeah, we don't need these
updated
updated
anymore because we're only doing single
segments. Maybe an issue, maybe not.
and get our log
ratio clipping. This is algorithmic
ratio clipping. This is algorithmic
stuff that we need to
stuff that we need to
ablate. We have P3 loss. We have clipped
ablate. We have P3 loss. We have clipped
V
V
loss and then we have uh normal V loss.
Have entropy
lost. This diversity all you need
lost. This diversity all you need
shenanigans
thing. Scaler backwards. Few extra
thing. Scaler backwards. Few extra
metrics. Brad
metrics. Brad
flipping. Uh, this
one. Is there anything better we can do
one. Is there anything better we can do
here?
I can't see a better way of doing this
actually, which is quite
annoying. And here's the KL target.
rep prioritized
rep prioritized
experience. It's fine other than being
experience. It's fine other than being
really way too long of a line.
It's going to
It's going to
be
data policy
rows. Got your learning
rateuler. You got this
thing. Got mean and
log. Then you've got your losses
log. Then you've got your losses
[Music]
[Music]
zeroed and
zeroed and
checkpointing. Okay. Then we have the
checkpointing. Okay. Then we have the
storage function. We already went over
storage function. We already went over
this quite a bit. There are some small
this quite a bit. There are some small
tweaks needed, but quite small at this
tweaks needed, but quite small at this
point. Go over the sampler. Sampler has
point. Go over the sampler. Sampler has
a bunch of different options that we
a bunch of different options that we
don't know which ones to keep
don't know which ones to keep
yet. There's some logging that we went
yet. There's some logging that we went
over. Some artifact stuff that hasn't
over. Some artifact stuff that hasn't
been updated in a
been updated in a
while. Uh profilers kind of difficult to
while. Uh profilers kind of difficult to
get around some of this
get around some of this
mess, but these aren't a big deal.
I think I probably want my checkpointing
I think I probably want my checkpointing
code above, don't
I? Yeah, I kind of want all this code
I? Yeah, I kind of want all this code
like above where it is
now. Yeah. Utilization profile
close. Yeah, that's a better spot for
it. Okay, 1073 lines.
rollout needs obviously to be cleaned
rollout needs obviously to be cleaned
up, but the vast majority of messy stuff
up, but the vast majority of messy stuff
in this file is now being caused by um
in this file is now being caused by um
like algorithm hangers on that we
like algorithm hangers on that we
haven't finished evaluating yet. Mainly
haven't finished evaluating yet. Mainly
E3B diversity is all you need and then
E3B diversity is all you need and then
the P30 stuff. So when we end up keeping
the P30 stuff. So when we end up keeping
like one path, one code path, it will be
like one path, one code path, it will be
shorter and substantially cleaner.
Let's go double
Let's go double
check. Let's go double check. Uh
check. Let's go double check. Uh
breakout overall now. Hang
on. Break
out. No.
Let's get a breakout
curve.
Uhhuh. How is this suddenly broken?
Is this uh not commented?
Oh,
Oh,
sorry. I just missed
Okay, that does
work. 1.4
work. 1.4
mil. We will
see
score. That looks good to
score. That looks good to
me. 52 million
me. 52 million
steps solved in basically 60. Yep.
steps solved in basically 60. Yep.
That's pretty well on par with the
That's pretty well on par with the
previous
best. So we're very happy to see
best. So we're very happy to see
that. And then
uses so max usages of a sample it got
uses so max usages of a sample it got
used like 10
times. Average sample gets used once.
So that is basically just advantage
So that is basically just advantage
filtering. Yeah, the prioritized
filtering. Yeah, the prioritized
experience in this case just becomes
experience in this case just becomes
advantage
filtering. Let's make sure really quick
filtering. Let's make sure really quick
that the samplebased
that the samplebased
metric is in fact
better. I believe it will be like way
better. I believe it will be like way
better, but we will uh we'll just make
better, but we will uh we'll just make
sure real quick.
So this is now selecting
So this is now selecting
um selecting the best samples just
um selecting the best samples just
sorting them and taking the top ones
sorting them and taking the top ones
instead of assigning a probability
instead of assigning a probability
distribution.
And I believe this still runs, but uh
And I believe this still runs, but uh
it's not as
good. I believe that was the
conclusion. Is there any reason that
conclusion. Is there any reason that
this would
this would
be absum absum?
Yeah. So, this is a good comparison and
Yeah. So, this is a good comparison and
it's
uh it's much better to sample
uh it's much better to sample
apparently. We should also get a rand
apparently. We should also get a rand
one for
one for
comparison. So, this should just be uh
comparison. So, this should just be uh
what is it?
what is it?
Random. And I think that it's going to
Random. And I think that it's going to
be similar to the uh the prioritized for
be similar to the uh the prioritized for
this environment.
But we should definitely
check random will be very close to the
check random will be very close to the
original PTO. It's just instead of going
original PTO. It's just instead of going
over
over
uh chunks of samples, you're randomly
uh chunks of samples, you're randomly
picking your
chunks. There is a bit of copy over it.
I think it's very similar in
perfect. Yeah. So, prioritize not
perfect. Yeah. So, prioritize not
prioritize very very
similar axises of seven.
I don't know about
that. Oh, it is random sampling. Okay.
that. Oh, it is random sampling. Okay.
So, yeah, it is possible like yeah,
So, yeah, it is possible like yeah,
that's reasonable. So, there's not a
that's reasonable. So, there's not a
huge difference
huge difference
then. You know, it's not really using
then. You know, it's not really using
it's not really deciding to reuse
it's not really deciding to reuse
samples way more often than it would
samples way more often than it would
randomly. So, makes sense. These are two
randomly. So, makes sense. These are two
are pretty comparable.
We leave it now. We leave it for
We leave it now. We leave it for
now. So I guess then the next thing is
now. So I guess then the next thing is
just going to be to start
just going to be to start
fixing
fixing
E3B diversity all you need. Fix those up
E3B diversity all you need. Fix those up
and uh we can start running real
and uh we can start running real
benchmarks,
benchmarks,
right? Fix all the
right? Fix all the
machines and get this running on stuff.
Uh, there is also PF stuff to deal with
though. There is also PF stuff to deal
though. There is also PF stuff to deal
with. Hang on.
Let's see the copy overhead on
this. Okay. So, copy overhead.
is very
is very
low for
training. We have quite a bit of copy
training. We have quite a bit of copy
overhead now in Eval that wasn't there
overhead now in Eval that wasn't there
before we did this
stuff.
Yeah. Make sure this is getting measured
Yeah. Make sure this is getting measured
correctly. Okay. So this is eval copy
correctly. Okay. So this is eval copy
right
right
here. That's
store. It's going to be these copies
store. It's going to be these copies
right here.
I mean, I looked at a ton of different
I mean, I looked at a ton of different
ways to do this. I don't think there's a
ways to do this. I don't think there's a
good way around doing it this way,
good way around doing it this way,
though.
If I store it the other way, then uh
If I store it the other way, then uh
it'll be slower when I go to access
it. Well, how bad is it on like a real
it. Well, how bad is it on like a real
lens, right?
Okay, we
Okay, we
are pretty much all in
learn with copy creeping up a little
250k. Let me try a couple quick things
here. I think this might not be
here. I think this might not be
optimized because this is half the speed
optimized because this is half the speed
it should be.
Yeah, something's true with the way this
Yeah, something's true with the way this
is being
sampled. You see what the batches
are 2048 by 16
You end
eight. Uh that is how I had it
eight. Uh that is how I had it
before. Wait. 2048.
before. Wait. 2048.
I'm
I'm
16. That's weird though. That's bigger
16. That's weird though. That's bigger
than it's supposed to be allowed to
be.
Max is 16k.
This is data mini batch
size. Okay, that's better.
Is this uh does this account for the
Is this uh does this account for the
difference though? I don't think it
does. Oh, there you
does. Oh, there you
go. There's your perf
back. And then we'll uh we'll see what
back. And then we'll uh we'll see what
copy converges at.
Yeah, I think that's not a bad haircut
Yeah, I think that's not a bad haircut
to perf.
to perf.
Overall, there's definitely copy
Overall, there's definitely copy
bandwidth to optimize, but
um I think we got out of this pretty
um I think we got out of this pretty
well.
65 and
65 and
33, but we're not capturing um you know,
33, but we're not capturing um you know,
something's not getting profiled
something's not getting profiled
because these numbers don't add up, I'm
realizing. So, the SPS is roughly fine,
realizing. So, the SPS is roughly fine,
but these numbers don't add up, which is
but these numbers don't add up, which is
not good.
Neither of them add up at all
Neither of them add up at all
actually. So there's something that's
actually. So there's something that's
taking time and isn't getting profiled
taking time and isn't getting profiled
in both of
these. Let's see if we can find it in
these. Let's see if we can find it in
eel first.
eel first.
Okay. So, we
Okay. So, we
got MISK eval
forward eval copy
forward eval copy
eval. Uh, yeah, this entire function is
eval. Uh, yeah, this entire function is
inside an eval.
inside an eval.
So, I don't actually know what it is
So, I don't actually know what it is
that's taking
up taking up the
up taking up the
time, right?
Oh, it's probably the Hang
on. Where do we do the uh the
on. Where do we do the uh the
synchronize
call? So, context
enterp context
enterp context
exit. Okay. So, this has to go up
top. This has to go
here. Synchronize. Yeah, it's got to be
here. Synchronize. Yeah, it's got to be
like bookended.
And that should give us better
And that should give us better
counts. We will
counts. We will
see. That looks like it adds up
now. Yeah, that adds up now.
10% copy
10% copy
overhead, 18%
overhead, 18%
forward. At least we have good stats. At
forward. At least we have good stats. At
least we have good stats
now. Real quick, just for
now. Real quick, just for
fun, if I don't do any of this, does
fun, if I don't do any of this, does
this actually affect
this actually affect
Perf? We're at 428.
Oh yeah, that takes
Oh yeah, that takes
uh that takes a little bite out of the
uh that takes a little bite out of the
perf, doesn't
it? So, we have to actually optimize the
it? So, we have to actually optimize the
profiling a bit and we get some of our
profiling a bit and we get some of our
time back.
time back.
about
5%. That'll be an easy PF win.
And that's most of the fur back already
And that's most of the fur back already
just from that if we do that
just from that if we do that
correctly. Uh let me think how we would
correctly. Uh let me think how we would
do
that. I think we can't do that until we
that. I think we can't do that until we
update the dashboard a little
update the dashboard a little
bit. But we know that that is a 5enter
bit. But we know that that is a 5enter
waiting on this
So perf not
So perf not
terrible. I think we fixed
terrible. I think we fixed
E3B. We fixed diversity all you
E3B. We fixed diversity all you
need and uh
need and uh
[Music]
[Music]
we probably we're okay to start running
we probably we're okay to start running
experiments.
It does concern me somewhat the
uh ah well no never mind. We need to
uh ah well no never mind. We need to
rerun those experiments before we can
rerun those experiments before we can
make any conclusions.
make any conclusions.
So let's
So let's
do let's just get E3B and all like the
do let's just get E3B and all like the
other algorithms working for
now. Okay. So we get some errors.
I think I know what we're going to do to
I think I know what we're going to do to
clean this mess up as well.
have to move some data
have to move some data
around and we'll see if I get stuck on
around and we'll see if I get stuck on
anything, but seems pretty simple.
All right. So, that runs with a massive
All right. So, that runs with a massive
hit to Perf.
We'll check
We'll check
uh what this
uh what this
does. Is that everything though?
does. Is that everything though?
Really? I didn't have to mess with it
Really? I didn't have to mess with it
and train it
all. Oh, no. E3B is the easy one because
all. Oh, no. E3B is the easy one because
it's only uh it only adds rewards.
Yeah, right there it adds the reward.
Perfect. Crazy amount of overhead this
Perfect. Crazy amount of overhead this
thing adds though.
I will be interested to see how it
I will be interested to see how it
scales if it's uh bad with bigger
scales if it's uh bad with bigger
policies as well.
Doesn't hurt the curve that much, but
Doesn't hurt the curve that much, but
doesn't really do much for you either.
66% forward time to
E3B. Let's see how it scales.
Cuda runs out of memory. That's
funny.
funny.
Um, yeah. What do we do about that? Uh,
Um, yeah. What do we do about that? Uh,
like I didn't plan for
that. I guess we go to like two ends or
that. I guess we go to like two ends or
something.
Holy that is slow.
Let's just make sure I didn't slow it
Let's just make sure I didn't slow it
down a ton because of the two
M's. Yeah. Now
Send a message real quick.
Let me actually just put that uh in the
Let me actually just put that uh in the
Discord
quick. Where's the
quick. Where's the
uh
uh
M? I think it's this one.
512 x 512
Here we
Here we
go. All right. So, E3B not too
go. All right. So, E3B not too
optimistic about. We can get the other
optimistic about. We can get the other
algorithm working though because that
algorithm working though because that
one I actually think that there's
one I actually think that there's
something to
it. Probably E3B. There is a way to make
it. Probably E3B. There is a way to make
it work, but it's like
it work, but it's like
Very
Very
slow. Very very slow.
Okay, these are going to
Okay, these are going to
be archive skills
be archive skills
batch. Archive skills
batch. And the code's not going to look
batch. And the code's not going to look
like this for uh forever. But this is
like this for uh forever. But this is
for the initial
port. Huh.
port. Huh.
I guess we have uh
archive. Uh this
parameters. I think that'll do it.
Does this go into
there? This is size total
agents. This is
size experience. So, this one probably
size experience. So, this one probably
does go in there.
I need
skills then batch.
I could probably just do it like
um
zero skills batch and the archive is the
zero skills batch and the archive is the
only thing that needs to go into
only thing that needs to go into
data. So this is
Eat expected batch.
and get these shapes to match up.
8192. Yeah, this is fine. This is a flat
8192. Yeah, this is fine. This is a flat
state hidden.
Oh, wait. No, that's not
Oh, wait. No, that's not
fine. You want this to be
fine. You want this to be
um you want this to be batched as well.
really want this to be batched as
well. Oh, we can just do this,
well. Oh, we can just do this,
right? Not have to do this.
Bash
time. Sample logs.
Why would that affect lot just one
This happened on like second batch or
This happened on like second batch or
something weird.
No, that happened
immediately. Okay, we have
logits got batch actions.
Why are these suddenly not
Why are these suddenly not
um not
flattened? That's very
weird. Oh, cuz you did this stupid
There we
go. So, we got Q, we've got Z indices.
How did this get added to the
How did this get added to the
observation
though? Where'd that even happen? The
though? Where'd that even happen? The
indices up
top. Batch dot
That's only 128 of
them. I guess this is state zindices,
them. I guess this is state zindices,
right?
allocates
discriminator. Where's the
discriminator. Where's the
uh the extra observation though?
encode. Where's the extra
observation? I thought we add to the obs
observation? I thought we add to the obs
space, don't we?
confused
now. Yeah. You pass here
now. Yeah. You pass here
to to that, right? You pass in
Did I screw this up
somehow? I thought we were passing the
somehow? I thought we were passing the
input of this to the
policy. Let me go double check how I had
policy. Let me go double check how I had
this in depth.
So, how close are we to
2K? 1918. Pretty
2K? 1918. Pretty
close. Pretty
close. Pretty
close. Star puffer helps us out.
Okay.
So just add this somehow.
I don't see anywhere where I'm adding
I don't see anywhere where I'm adding
this as an observation at all.
see how I was doing it in
see how I was doing it in
here. Also, the dev branch of this is
here. Also, the dev branch of this is
1,200 and uh this one is
1,200 and uh this one is
six that
six that
right. Yeah, this has gotten way too
right. Yeah, this has gotten way too
long. 400 lines extra.
long. 400 lines extra.
So we definitely need to cut this
down. Okay. So here is where we compute
down. Okay. So here is where we compute
observations. Here's the
indices. This doesn't go into state at
indices. This doesn't go into state at
all. Oh wait, no. Here it does. does go
all. Oh wait, no. Here it does. does go
into state right
into state right
there. And then
policy. Does this not augment the
policy. Does this not augment the
observations?
I could have sworn that's how it
worked. Oh, you know, maybe it's in the
um Maybe it's in the LSTM or
something. It's not in here
either.
either.
Well, let's at least get it to run for
Well, let's at least get it to run for
now.
No
How do you unsqueeze a dimension like
How do you unsqueeze a dimension like
that?
Yeah, that'll expand.
Okay.
I just want this to run for now.
Okay, there we go. This still
runs.
runs.
Cool. Yes, this still runs. We have all
Cool. Yes, this still runs. We have all
these uh these things ported and
these uh these things ported and
running. Uh there's still P30 to do.
I am going to need to still do some work
I am going to need to still do some work
on
that, but this feels like enough porting
that, but this feels like enough porting
for now.
Okay.
Next, we get to run two
experiments. I like this.
Okay, good. We don't have any weird
Okay, good. We don't have any weird
stuff in
here. The first one of these is going to
here. The first one of these is going to
be with
be with
uh prioritized or what is
uh prioritized or what is
it? Prioritize filtering. Then the next
it? Prioritize filtering. Then the next
one will be without
one will be without
That'll give us a good
That'll give us a good
eval. And then we'll do a couple other
eval. And then we'll do a couple other
things with this just to make sure
things with this just to make sure
things are
things are
working. And then I think we'll do some
working. And then I think we'll do some
thinking about
algorithms. Yeah. All right. I'll let
algorithms. Yeah. All right. I'll let
this run. I'll be right back.
Okay. Scores here.
Okay. Scores here.
We let this do 100
mil and then we do the other one which
mil and then we do the other one which
will
be
method which will be the comparison run
method which will be the comparison run
for random sampling versus prioritize
for random sampling versus prioritize
filter.
This is one of those methods I would
This is one of those methods I would
expect. I will
say so it will be surprising if this
say so it will be surprising if this
doesn't at least maintain Perf.
That's a bit
disappointing. Not clearly better.
Unless it pulls
ahead. Uh there are a couple variants we
ahead. Uh there are a couple variants we
should try as well.
Low absolute
Low absolute
advantage. So I did do this correctly.
advantage. So I did do this correctly.
Low absolute
advantage. Appendix C for details.
Okay, so they're still using
Okay, so they're still using
Atom, but we can beat that straight up
Atom, but we can beat that straight up
as
as
is. And then advantage filtering. You
is. And then advantage filtering. You
collect
collect
rollouts, you compute
advantages, and
advantages, and
then max absolute advantage.
ax
equals. Okay, so they're doing a like a
equals. Okay, so they're doing a like a
direct filtering thing instead of doing
direct filtering thing instead of doing
um we have a sample based thing, they
um we have a sample based thing, they
have a filterbased
thing. Those should both be fine.
thing. Those should both be fine.
Oh, did this do something? Hey, it did
Oh, did this do something? Hey, it did
something. Look at
something. Look at
that. It did
something.
Yay. That's
cool. So, on the harder task where stuff
cool. So, on the harder task where stuff
is like
is like
No, stuff gets sparse for overtime and
No, stuff gets sparse for overtime and
whatnot. It actually does
whatnot. It actually does
something. We're happy with that result.
something. We're happy with that result.
We take that. That's a
We take that. That's a
win. That's a pretty big win, actually.
Look at that. I wonder if that'll help
Look at that. I wonder if that'll help
on neural
MMO. I'll send this over.
go just link somebody one paper.
Okay, we are happy with this. This is a
Okay, we are happy with this. This is a
win. This is a solid win. In fact, I'm
win. This is a solid win. In fact, I'm
going to uh me send this to Aaron.
solid. Now, I think what I want to do, I
solid. Now, I think what I want to do, I
want to do something a little bit out
want to do something a little bit out
there.
there.
I kind of want to just take some time to
I kind of want to just take some time to
gather my
gather my
thoughts, kind of go through a bunch of
thoughts, kind of go through a bunch of
the things on the algorithm side that
the things on the algorithm side that
have
have
been floating around my
been floating around my
head, just write down a bunch of random
head, just write down a bunch of random
throwaway
throwaway
notes, and
notes, and
uh basically just see where I think we
uh basically just see where I think we
should go next with all this because
should go next with all this because
this is going pretty well so far.
What are some things that we know to be
What are some things that we know to be
true?
Generalized vantage
Generalized vantage
estimation locks you into
perm search for
perm search for
gamma lambda appears be a bit more
stable filtering
We
We
win
prioritized experience
replay.
Um, it follows Was it the value function
Um, it follows Was it the value function
lags? The value function
lags. No, it's the policy lags behind.
No. Oh, yeah. Because the
Word up by an old policy.
policy prevents you from reusing
policy prevents you from reusing
old data to
old data to
train
train
the policy
the policy
though. So it's really it's the value
though. So it's really it's the value
function that gets screwed up.
Okay, so these are a couple things.
Um
Um
noisebased
exploration methods don't
exploration methods don't
work.
work.
Countbased exploration methods
Countbased exploration methods
work are
specific specific. And
specific specific. And
then
then
E3B might be able to work, but it's
slow can result in
visually behavior.
Um,
Um,
discriminator trained
discriminator trained
on
on
lockets prone
to
jiggle
jiggle
jiggle. Actually, is there a stop grad
jiggle. Actually, is there a stop grad
on that? Hang on. Is there a stop grab?
on that? Hang on. Is there a stop grab?
If there's a stop credit, it wouldn't
be. I think I tried it with a stop bra
be. I think I tried it with a stop bra
though and I couldn't get it to work.
Yeah, you can't do it with a stop
Yeah, you can't do it with a stop
grant because uh you have to be able to
grant because uh you have to be able to
influence the policy actions. That's the
influence the policy actions. That's the
whole point. So yeah, the formulation is
whole point. So yeah, the formulation is
weird, but the the method is good. Okay.
Dreamer V3
Dreamer V3
tricks are
tricks are
bogus. Something in world modeling lets
bogus. Something in world modeling lets
you scale big
policies. These are things I know to be
policies. These are things I know to be
true.
P3L was not a clear win.
Self play can be an easy
Self play can be an easy
curriculum, auto
curriculum, auto
curriculum.
curriculum.
Competitive self play is
Competitive self play is
prone to policy
prone to policy
collapse.
Historical self play can
help training
multiple. So what I'm kind of looking
multiple. So what I'm kind of looking
for
for
is a way to tie all this stuff
together. These are these are what I
together. These are these are what I
believe to be facts.
What is the minimal change to PO that I
What is the minimal change to PO that I
could make that would make prioritized
could make that would make prioritized
experience replay work?
All
right. Recline a little bit
here. Thinking
mode. There we go.
So the issue right
now when you compute generalized
now when you compute generalized
advantage estimation,
right? That
right? That
advantage you can look at the value
advantage you can look at the value
function is essentially learning to
function is essentially learning to
predict discounted return. That's not
predict discounted return. That's not
quite accurate. That's only true for
quite accurate. That's only true for
lambda equals 1, but it's about that.
and it's discounted return under the old
and it's discounted return under the old
policy. That is the
policy. That is the
problem. So there's actually nothing
problem. So there's actually nothing
preventing you from if you like just
preventing you from if you like just
ignore the value function, right? If we
ignore the value function, right? If we
just look at
just look at
reinforce, let's look at
reinforce. Where's the spinning up one?
Yeah. Okay. So, actually you already
Yeah. Okay. So, actually you already
have a value function in vanilla
have a value function in vanilla
reinforce it seems.
Do you? That seems weird. I thought
Do you? That seems weird. I thought
original reinforced didn't have a value
baseline. Yeah, they updated this. This
baseline. Yeah, they updated this. This
is not vanilla policy gradient. This is
like Is this really vanilla?
like Is this really vanilla?
I thought the vanilla you just used the
I thought the vanilla you just used the
returns
returns
directly without a
directly without a
baseline because if you use the returns
baseline because if you use the returns
directly without a baseline, let me zoom
directly without a baseline, let me zoom
this thing
this thing
in. If you use the returns directly
in. If you use the returns directly
without a
baseline, there's nothing that prevents
baseline, there's nothing that prevents
you from reusing the samples as much as
you from reusing the samples as much as
you want, right?
You're not reusing logits. You're
You're not reusing logits. You're
recmputing the forward pass logits every
time. So when you get into trouble is I
time. So when you get into trouble is I
think with the value
function. It's kind of backwards. It's
function. It's kind of backwards. It's
the problem isn't the current value
the problem isn't the current value
function. It's the current policy.
function. It's the current policy.
So you got the trajectories under a
So you got the trajectories under a
specific
specific
policy and
then the value
function. This is a little tricky to
function. This is a little tricky to
think about.
The value function attempts to predict
The value function attempts to predict
discounted return under the current
policy. So it's if you're in this state
policy. So it's if you're in this state
and you act with the current
and you act with the current
policy, what are you going to
policy, what are you going to
get? Uh that's going to undersshoot
get? Uh that's going to undersshoot
though because as soon as you start
though because as soon as you start
updating the
updating the
policy right that
policy right that
data is
data is
uh is now from an old
uh is now from an old
policy. Now crucially like you can keep
policy. Now crucially like you can keep
using that to train the policy. Nothing
using that to train the policy. Nothing
bad should happen there.
The issue is that the value function is
The issue is that the value function is
going to be
going to be
off because it's your you can make you
off because it's your you can make you
can get like an optimal value function
can get like an optimal value function
for the old policy not for the new one.
for the old policy not for the new one.
Now it shouldn't change that much right
Now it shouldn't change that much right
from one batch of data like you can only
from one batch of data like you can only
update the policy as much as is allowed
update the policy as much as is allowed
by a batch of
by a batch of
data. But if you start using like
data. But if you start using like
historical
historical
buffers that is no longer the case.
I don't think replacing the value
I don't think replacing the value
function with a Q function helps you
function with a Q function helps you
either. that just gives you one
either. that just gives you one
additional step, right?
additional step, right?
So a Q function
So a Q function
says it just gives you uh the value for
says it just gives you uh the value for
all
all
actions. But then it assumes that after
actions. But then it assumes that after
you take this action that you follow the
you take this action that you follow the
uh you follow like the best policy or
uh you follow like the best policy or
whatever.
So what happens if your policy if you
So what happens if your policy if you
let's say you have a Q function for your
let's say you have a Q function for your
policy right so no value function the
policy right so no value function the
policy just is a Q function we'll ignore
policy just is a Q function we'll ignore
continuous for
Does it magically
work? Well, you go to the very end.
work? Well, you go to the very end.
No, it shouldn't magically
No, it shouldn't magically
work because the last step you say,
work because the last step you say,
okay, you train the Q function with the
okay, you train the Q function with the
action that you took. That's correct.
action that you took. That's correct.
That it gets the reward. But as soon as
That it gets the reward. But as soon as
you go back one additional step,
you go back one additional step,
um I think you're off
again. How does this work in like in DQN
again. How does this work in like in DQN
for instance?
for instance?
I mean, they don't have trajectories.
I mean, they don't have trajectories.
Here is the thing. I don't think this is
Here is the thing. I don't think this is
going to be
going to be
helpful. I think it's way simpler
here. How sad is it that like there are
here. How sad is it that like there are
so many things and there's not even the
so many things and there's not even the
article, the archive
paper. There we go.
I mean this gives you implicitly
I mean this gives you implicitly
discounted returns.
doesn't include
doesn't include
J. What if we look at
J. What if we look at
like what if we look at something that
like what if we look at something that
actually
works? I see how they handle
this.
Actually, the code's probably in clean
Actually, the code's probably in clean
for this, isn't it?
though this probably doesn't have
though this probably doesn't have
recurrent
recurrent
uh recurrence in
it. How long is this? It's only 300
it. How long is this? It's only 300
lines.
Okay. So they
Okay. So they
have QET and then they have the actor
They make Oh, do they
They make Oh, do they
use Where's this replay buffer come
from? Oh, they use SP3's replay buffer.
from? Oh, they use SP3's replay buffer.
Come on. That's not clean or quality for
Come on. That's not clean or quality for
you.
Good training.
I think I really just need to understand
I think I really just need to understand
exactly where it is. Where's the line
exactly where it is. Where's the line
that lets you go off policy?
large continuous domains.
large continuous domains.
Okay. So this
Okay. So this
is part of this is handling
continuous soft value function is trying
continuous soft value function is trying
to measure minimize squared residual
to measure minimize squared residual
error. We have
that policy parameters.
I don't think I need to go this far to
I don't think I need to go this far to
figure it
figure it
out, but I should I should figure out
out, but I should I should figure out
this as
this as
well. Essentially the question I have
well. Essentially the question I have
right now right is
right now right is
just I found that the off policiness of
just I found that the off policiness of
PO isn't quite a problem in the way that
PO isn't quite a problem in the way that
it's usually thought about though there
it's usually thought about though there
is still a problem with the value
function. I'm trying to figure out
function. I'm trying to figure out
how you get around that
here. Also, these things don't have J,
here. Also, these things don't have J,
do they?
do they?
They don't have generalized advantage
They don't have generalized advantage
estimation.
Yeah, I don't trust these baselines
Yeah, I don't trust these baselines
either versus
either versus
uh I don't trust these experiments
uh I don't trust these experiments
either is the
either is the
thing. Who was this paper from
thing. Who was this paper from
originally? Sergey's
group. How many experiments do they
group. How many experiments do they
have?
Yeah, not really anywhere near as many
Yeah, not really anywhere near as many
experiments as you would want.
I think that the
I think that the
way in which off policy algorithms do
way in which off policy algorithms do
discounting is linked to what I'm
discounting is linked to what I'm
looking for.
I don't see a discounting factor in
I don't see a discounting factor in
here. Do
here. Do
you? Where's the discounting
factor? Is there no discounting factor
factor? Is there no discounting factor
in here?
There is no discounting factor
There is no discounting factor
in soft actor
in soft actor
critic. That immediately makes me
critic. That immediately makes me
suspicious.
Oh, wait. Args. Gamma. What's
gamma? Ah, there is a discount factor.
gamma? Ah, there is a discount factor.
Good. Okay, I'm not completely insane.
Where is data collection?
Okay, you step a whole bunch,
right? And you
right? And you
add to the rollout
add to the rollout
buffer and then okay, they're doing Yep.
buffer and then okay, they're doing Yep.
So, this is the collect and then here's
So, this is the collect and then here's
the step. You
the step. You
sample, you train the
sample, you train the
critic and the objective of the
critic. There's no loss being computed
critic. There's no loss being computed
here. This is no bread.
So basically it's just this chaining
So basically it's just this chaining
forward
thing. All right, I'm use the restroom.
thing. All right, I'm use the restroom.
I'll be right back. Keep thinking about
I'll be right back. Keep thinking about
this.
Okay. The key thing that I'm not sure
Okay. The key thing that I'm not sure
about here,
about here,
right, we're trying to replace J
right, we're trying to replace J
ultimately,
ultimately,
um, J is really
obnoxious because it requires that you
obnoxious because it requires that you
roll
roll
forward. Fix the chair.
So G is really obnoxious because it
So G is really obnoxious because it
requires that you roll forward
requires that you roll forward
uh trajectories. You can't really train
uh trajectories. You can't really train
until you get to the
end and that seems
essential. Where is SACE been used
essential. Where is SACE been used
though? Has anybody done like SACE on
Atari? Random paper.
So this is versus
So this is versus
rainbow actually. Maybe that would be
rainbow actually. Maybe that would be
better a
comparison. I just don't trust any of
comparison. I just don't trust any of
these papers is the problem.
There's this monstrosity.
So difficult to compare stuff like this
So difficult to compare stuff like this
though.
multistep
returns and this is Atari as well which
returns and this is Atari as well which
is not
So the thing that's difficult here,
So the thing that's difficult here,
right, generalized advantage estimation
right, generalized advantage estimation
works really, really well. In fact, the
works really, really well. In fact, the
more stuff you do with PO, the more it's
more stuff you do with PO, the more it's
like generalized advantage estimation is
like generalized advantage estimation is
most of
PO, but then off policy learning doesn't
PO, but then off policy learning doesn't
use generalized advantage
estimation. They really only have one
estimation. They really only have one
discounting factor as far as I can
discounting factor as far as I can
see. So,
They kind of do like this onestep
thing. This is
multi-step for view. Multi-step targets
multi-step for view. Multi-step targets
can be
can be
used. Okay.
They have their multistep thing, but I
They have their multistep thing, but I
think it's like three steps,
think it's like three steps,
right? Yeah. Three steps forward is all
right? Yeah. Three steps forward is all
they
go. So sketchy.
I might really have to go through a
I might really have to go through a
bunch of literature in order to figure
bunch of literature in order to figure
this type of stuff out.
I think what I'm going to do is I'm
I think what I'm going to do is I'm
going to just spend it's
going to just spend it's
336. So I'm going to spend a little time
336. So I'm going to spend a little time
on this now. I'm going to see if
on this now. I'm going to see if
anything sticks out to me. And if not, I
anything sticks out to me. And if not, I
think we will focus on trying to finish
think we will focus on trying to finish
and ship this update with muon with
and ship this update with muon with
cosign and kneeling with all the perf
cosign and kneeling with all the perf
enhancements with advantage filtering
enhancements with advantage filtering
with good experiments and baselines all
with good experiments and baselines all
of that. And then the next update will
of that. And then the next update will
be really seeing if we can fix algo side
That's going to be like an ordeal,
That's going to be like an ordeal,
though.
Obviously, I'm going to have to go
Obviously, I'm going to have to go
through all this in
through all this in
details, but
um summary
Splitting actions and action selection
Splitting actions and action selection
and value
estimation. Okay, so this is kind of
estimation. Okay, so this is kind of
analogous to the shared critic
thing. And this had evidently made a
thing. And this had evidently made a
very large
difference. The largest in
fact okay. Okay. What about
distributional full probability
distribution discretized into bins.
Okay.
Okay.
So, this is kind of the biggest one
So, this is kind of the biggest one
then, right?
This thing should have OCR, right?
Do they do drop
Do they do drop
ones? They did do drop ones. So, no
ones? They did do drop ones. So, no
distribution hurts a
distribution hurts a
lot. Oh, actually, yeah, it's a little
lot. Oh, actually, yeah, it's a little
unintuitive here. A lot of these things
unintuitive here. A lot of these things
hurt a lot.
Dueling doesn't
matter. Double doesn't matter is very
matter. Double doesn't matter is very
weird. I guess it's covered by the other
weird. I guess it's covered by the other
ones already.
So the biggest things
So the biggest things
are
are
noisy multi-step distributional
noisy multi-step distributional
prioritization. Yeah.
the distributional thing matters a lot.
This thing can't
see. Wrong line, buddy.
Yeah. So distributional seems to be the
Yeah. So distributional seems to be the
big chunk
there. Multistep also makes a big
there. Multistep also makes a big
difference but only for like three steps
difference but only for like three steps
or whatever they tuned it.
I was looking for a specific paper.
really need to be able to replace J is
really need to be able to replace J is
the
thing. Well, hang on.
The heck does this even work?
DQN you train
fors. I really just the thing is I need
fors. I really just the thing is I need
to understand very very
to understand very very
precisely what it is that lets you do.
um that lets you do like off policy
um that lets you do like off policy
samples versus
not. And the the tricky thing with this
not. And the the tricky thing with this
is
like none of these use an
like none of these use an
LSTM. Does that matter
LSTM. Does that matter
though? No, because you still end up
though? No, because you still end up
with segments in PO even if you don't
with segments in PO even if you don't
have an LSTM, right? you still end up
have an LSTM, right? you still end up
with segments. You can sample them in
with segments. You can sample them in
whatever order you like, but you still
whatever order you like, but you still
end up with segments because of the GA
end up with segments because of the GA
computation,
right? Okay.
So what prevents you from
doing unlimited
doing unlimited
sampling on that in that case?
Let's see.
So you collect your
samples. You have like
pi of obs.
you get your probability over your
you get your probability over your
action distribution.
you have a value
you have a value
function
ops I don't know v subo or
whatever off policy you Q.
I don't know what notation I'm doing at
I don't know what notation I'm doing at
all
all
here,
but we're going to keep going with this.
When you train on
policy, you train value to approximately
equal
like gamma.
R subi something like
this. Whereas with the Q function, you
this. Whereas with the Q function, you
kind of do this implicit thing, right?
and the
equation. I want to make sure I get the
equation. I want to make sure I get the
sign
right. Yeah, there it is. So, it's
R + gamma
Q. And then here the value is equal
to R
to R
I
Q S
T
minus
minus
T one something like this
Right. And then there's this is a
Right. And then there's this is a
distribution. So
presumably which action do you use here
presumably which action do you use here
though?
Oh, this is max sub
Oh, this is max sub
a. So I see. So this is the
maximum and this this one's
maximum and this this one's
max. So maybe this is it.
How is this not still dependent on the
How is this not still dependent on the
policy?
I think they should still be dependent.
I think they should still be dependent.
No, cuz when you get to the next state,
No, cuz when you get to the next state,
what action do you use?
This a has to be the action that you
This a has to be the action that you
actually took,
right? Oh, wait. Okay, I have this
wrong. So, I have this wrong. I haven't
wrong. So, I have this wrong. I haven't
done DQN in a long
time. Let's just erase this this
time. Let's just erase this this
portion.
So this is going to be
Q
X
Q
S. Then this is minus the one that you
S. Then this is minus the one that you
actually
actually
took. A
Okay, so this is the difference between
Okay, so this is the difference between
taking the best action under your
taking the best action under your
current value
current value
function and or your current Q function
function and or your current Q function
whatever and the action that you
whatever and the action that you
actually
took. So this is the thing I think that
took. So this is the thing I think that
gives it to you.
but the thing I don't get here,
right? The Q function just tells you
right? The Q function just tells you
it's like a one-step difference.
The presumption is that you follow
The presumption is that you follow
either the best policy or the sample
either the best policy or the sample
policy, whatever you're training
policy, whatever you're training
against.
had a multi-step return to work with
had a multi-step return to work with
uh with
uh with
Q-learning. They saw that in
Rainbow and it was very
Rainbow and it was very
important in Rainbow.
Oh, so this is just straight up skipping
Oh, so this is just straight up skipping
steps,
steps,
right? So you do discounted return kind
right? So you do discounted return kind
of but only for a few
steps. And then this
steps. And then this
is gamma at t +
n. Doesn't that look familiar from
J? That looks familiar.
Isn't it?
This is this component in the
This is this component in the
middle. Then this is
middle. Then this is
gamma to the k.
gamma to the k.
Yeah. So, this is
identical. This is the endstep advantage
identical. This is the endstep advantage
right
here. Or yeah, this is the endstep
advantage. There's no lambda in here
advantage. There's no lambda in here
yet, though.
So this is before you introduce lambda
So this is before you introduce lambda
at all.
This somehow disappeared when they
This somehow disappeared when they
summed it though.
Oh, this is under the max action though
Oh, this is under the max action though
here, right?
here, right?
Look. So this is different.
Yeah, but I think this is still
Yeah, but I think this is still
broken
broken
because how you get from ST to T + N is
because how you get from ST to T + N is
dependent on the current policy,
right? I think that's still broken.
That seems like a half.
Maybe this is why they have such short
Maybe this is why they have such short
returns.
They only do it for a few
steps. I think your actions can
steps. I think your actions can
diverge. And this is a huge part as
diverge. And this is a huge part as
well.
well.
Look, where' the ablation
Look, where' the ablation
go? No multistep is
go? No multistep is
down all the way there.
Let me make sure I understand this
Let me make sure I understand this
correctly.
correctly.
So this is your Q
So this is your Q
function with the current set of
parameters state and action.
And then you have the max over the Q
function and steps
ahead. That's the value function
ahead. That's the value function
basically, right?
Whenever you take max over actions,
Whenever you take max over actions,
that's just the value
function or something similar to it. It
function or something similar to it. It
depends on whether you take max or
depends on whether you take max or
whether you sample in your value
whether you sample in your value
function but very
similar. And then this
similar. And then this
one is also value
function. Slightly better I guess
function. Slightly better I guess
because you get to know what action
because you get to know what action
you're going to take for one
step. But yeah, it breaks in the middle,
step. But yeah, it breaks in the middle,
right?
Now, I guess the difference here, right,
Now, I guess the difference here, right,
is you only have like three steps to go
is you only have like three steps to go
wrong and then if every sample looks
wrong and then if every sample looks
like n steps
like n steps
ahead, they sort of get chained
ahead, they sort of get chained
together,
together,
right? And you're never off by in three
steps. Is that how that works?
I understand I think now the the one
I understand I think now the the one
step ahead case right so the one step
step ahead case right so the one step
ahead case is this the DQN yeah it is
ahead case is this the DQN yeah it is
this DQN so the one step ahead
case you have what's this gamma next
case you have what's this gamma next
state minus current
state minus current
Okay. So this is the value
function. Uh but in this one you get the
function. Uh but in this one you get the
action condition value function.
So
So
basically in
basically in
um if you don't have a Q function,
um if you don't have a Q function,
right, if you're just using a value
right, if you're just using a value
function here like uh like PO does,
function here like uh like PO does,
then the action that the policy is going
then the action that the policy is going
to take can
to take can
change and then the trajectory changes.
change and then the trajectory changes.
So you're optimizing the value function
So you're optimizing the value function
over a trajectory that the policyy's not
over a trajectory that the policyy's not
actually going to take. But here you say
actually going to take. But here you say
explicitly
explicitly
uh if the policy takes this action right
uh if the policy takes this action right
this will be the value
this will be the value
function. So it's not dependent on the
function. So it's not dependent on the
actual action that the policy
takes. Now this still
takes. Now this still
is this is just unconditioned value
is this is just unconditioned value
function
function
um but this is only one step
um but this is only one step
ahead. And then the idea here is that
ahead. And then the idea here is that
you train this transition and then if
you train this transition and then if
you train the next transition, well, the
you train the next transition, well, the
next transition also has this. So if
next transition also has this. So if
you're training all these transitions
you're training all these transitions
individually, they all chain together.
individually, they all chain together.
Then it works out.
Now, this multi-step
thing. I think if you make n large here,
thing. I think if you make n large here,
this
breaks cuz you have the exact same
breaks cuz you have the exact same
problem as
problem as
uh on
policy, but they kind of just punt on
policy, but they kind of just punt on
it.
I kind of just punt on it
anyways. And it only gets a few steps
anyways. And it only gets a few steps
ahead.
Can you just do this with J?
You can technically just do this with J,
You can technically just do this with J,
right? But the thing is the segment
right? But the thing is the segment
lengths are still really long.
Hang
on. Why do we need such long segments in
on. Why do we need such long segments in
J but not here is the
J but not here is the
question. Why does online need long
question. Why does online need long
segments and this doesn't?
Okay. As this goes to
Okay. As this goes to
infinity, yeah, you get
this. Did they cut out a term here?
I think they cut out a
I think they cut out a
term. They assume this goes to infinity,
term. They assume this goes to infinity,
don't they?
Yeah, I think that this case with one
Yeah, I think that this case with one
actually goes
actually goes
to they missed this term, don't
to they missed this term, don't
they? They just assume that it's going
they? They just assume that it's going
to be
infinite. Yeah. See, they're going to
infinite. Yeah. See, they're going to
infinity
here. So, if you don't go to infinity,
here. So, if you don't go to infinity,
gamma one
I think you recover
I think you recover
this
BST plus. Yeah.
BST plus. Yeah.
So you get
So you get
this. So this term they dropped and this
this. So this term they dropped and this
is exactly what
is exactly what
uh this is exactly what DQN is going to
uh this is exactly what DQN is going to
use.
and they keep this
and they keep this
term only like three ahead or whatever.
term only like three ahead or whatever.
They keep this term as
They keep this term as
is. That's what the max over actions
is. That's what the max over actions
does. Essentially, it turns your Q
does. Essentially, it turns your Q
function into a value
function. And then this one's kept as a
function. And then this one's kept as a
Q function and it's conditioned on the
Q function and it's conditioned on the
action that you actually took here.
So if this is one step look
So if this is one step look
ahead which is this
one. Oh yeah this is the onestep one.
one. Oh yeah this is the onestep one.
Okay so yeah now if you take this one
Okay so yeah now if you take this one
here
here
then this is value next state. this Q
then this is value next state. this Q
function and then this gives you
function and then this gives you
perfect off policy training. This gives
perfect off policy training. This gives
you perfect off policy
you perfect off policy
training because
training because
um if you go one step
ahead, this value is going to be
ahead, this value is going to be
conditioned on the action that you
conditioned on the action that you
actually took.
actually took.
So you're no longer relying on the
So you're no longer relying on the
policy to take this action because
policy to take this action because
you're explicitly conditioning the value
function. I think that's correct.
Do you just straight up sub in a Q
Do you just straight up sub in a Q
function for a value function and have
function for a value function and have
it do better then?
Because you kind of already get
Because you kind of already get
double double DQN out of this, don't
double double DQN out of this, don't
you? Let me see.
Where is that
summary? Splitting action selection and
summary? Splitting action selection and
value estimation between two networks.
Main network selects the best action in
Main network selects the best action in
the next
state and evaluates that action's Q
state and evaluates that action's Q
value using a target
network to select the best
action. Let me see how they set that up.
signing each experience randomly to
signing each experience randomly to
update one of the two value
functions. One set of weights is used to
functions. One set of weights is used to
determine the greedy policy and the
determine the greedy policy and the
other to determine its value.
I think we can just replace
I think we can just replace
the value head with a Q head, right?
Oh, got a got a thing to do pretty
Oh, got a got a thing to do pretty
soon. I think we can just replace the
soon. I think we can just replace the
value head with a Q head though. I think
value head with a Q head though. I think
that's the
that's the
finding. We can do this for
finding. We can do this for
all all discrete
ms,
ms,
right? Yeah, we can do this for all
right? Yeah, we can do this for all
discrete
ms. I think that'll be a nice thing to
ms. I think that'll be a nice thing to
try.
I don't necessarily know if we're going
I don't necessarily know if we're going
to do this immediately.
to do this immediately.
Um, it might be better to like get some
Um, it might be better to like get some
experiments in on the current thing
experiments in on the current thing
because this should let us do
because this should let us do
prioritized
prioritized
replay,
replay,
but there
is I think there is a big
tradeoff. Also, let's take a moment to
tradeoff. Also, let's take a moment to
appreciate the one week of training for
appreciate the one week of training for
200 million frames where we currently
200 million frames where we currently
for 200 million
for 200 million
frames states I guess on our
frames states I guess on our
re-implementations it takes
re-implementations it takes
about I don't know 3
minutes. So that's pretty cool.
minutes. So that's pretty cool.
four minutes
maybe. So I think that's the finding. I
maybe. So I think that's the finding. I
think the finding
is you should just be able to
is you should just be able to
replace the value function with a Q
replace the value function with a Q
function. Um does that ever hurt
function. Um does that ever hurt
you? I think it never hurts. It just it
you? I think it never hurts. It just it
makes learning maybe a bit more
complicated. I mean like the the code a
complicated. I mean like the the code a
little more
complicated. The thing that's tricky
complicated. The thing that's tricky
though is
like you only really can do off policy
like you only really can do off policy
when the multi-step returns are pretty
short. I guess the question is why does
short. I guess the question is why does
rainbow work so well then
right? Yeah, you use the baselining that
right? Yeah, you use the baselining that
happens that requires V and not
happens that requires V and not
Q. V is just equal to
Q. V is just equal to
um V is just Q of max
um V is just Q of max
action,
action,
right? Q is just action condition value
right? Q is just action condition value
function. You can recover B. You can
function. You can recover B. You can
recover B by taking the action of the
recover B by taking the action of the
policy. And that's you Aaron. Hey
All
All
right, we got a chat, right? So, I'm
right, we got a chat, right? So, I'm
gonna I'll be back in a couple minutes.
gonna I'll be back in a couple minutes.
Use the restroom, grab a drink, all
Use the restroom, grab a drink, all
that, and
that, and
uh maybe grab a half sandwich real
uh maybe grab a half sandwich real
quick and I'll be right back in a few
quick and I'll be right back in a few
minutes to chat. So, for the other folks
minutes to chat. So, for the other folks
watching, I will probably be back for an
watching, I will probably be back for an
evening session. I don't know if I'm
evening session. I don't know if I'm
going to implement this stuff
going to implement this stuff
immediately or if we're just going to
immediately or if we're just going to
try to get everything into a better
try to get everything into a better
stable state. Um,
stable state. Um,
but all my stuff's at
but all my stuff's at
puffer.ai. Closing in on 2K stars.
puffer.ai. Closing in on 2K stars.
Please go ahead and start the repo.
Please go ahead and start the repo.
Really helps us out.
Really helps us out.
And you can join the Discord to get
And you can join the Discord to get
involved with dev. It's not all staring
involved with dev. It's not all staring
at math all day. There's plenty of cool
at math all day. There's plenty of cool
dev to do.

Kind: captions
Language: en
pack
low. I did a little bit of thinking and
low. I did a little bit of thinking and
I think we're
I think we're
fine. So the problem that I sort of left
fine. So the problem that I sort of left
I should provide some context here since
I should provide some context here since
this is start of stream and therefore
this is start of stream and therefore
start of baud. Uh we left off working on
start of baud. Uh we left off working on
prioritized experience
prioritized experience
uh buffer for
uh buffer for
PO
PO
and I thought that I've got this sort of
and I thought that I've got this sort of
this working idea that on policy verse
this working idea that on policy verse
off policy it's kind of all a lie. Um oh
off policy it's kind of all a lie. Um oh
no the day is off policy really doesn't
no the day is off policy really doesn't
matter that much. And I found one spot
matter that much. And I found one spot
that kind of throws a wrench into it but
that kind of throws a wrench into it but
I think it's still fine. So the idea and
I think it's still fine. So the idea and
there will be an article on
there will be an article on
this is that in
this is that in
PO or I mean policy gradient methods in
PO or I mean policy gradient methods in
general you
take I guess the formula is not going to
take I guess the formula is not going to
be in here.
Um, basically the formula doesn't depend
Um, basically the formula doesn't depend
on the old policy at
on the old policy at
all because you're computing new
all because you're computing new
logics. So there's actually if you look
logics. So there's actually if you look
at the math there's nowhere that the old
at the math there's nowhere that the old
policy is even remotely involved. Uh the
policy is even remotely involved. Uh the
one thing that is implicitly involved is
one thing that is implicitly involved is
that the value function gets stale. So
that the value function gets stale. So
the value function is trained to
the value function is trained to
predict well one form of it is trained
predict well one form of it is trained
to predict discounted rewards under the
to predict discounted rewards under the
current policy. So the value function is
current policy. So the value function is
just going to undershoot a little bit.
just going to undershoot a little bit.
Um it's just going to undersshoot the
Um it's just going to undersshoot the
current policy a bit. I don't think that
current policy a bit. I don't think that
should be a problem. It's also it's
should be a problem. It's also it's
fixed. The amount that it's going to
fixed. The amount that it's going to
undersshoot by is fixed because the
undersshoot by is fixed because the
policy can only improve so much from one
policy can only improve so much from one
batch of data, right? it can't
batch of data, right? it can't
infinitely improve. The value function
infinitely improve. The value function
cannot become infinitely stale. So I
cannot become infinitely stale. So I
think this is all fine and basically uh
think this is all fine and basically uh
the results that we're seeing where this
the results that we're seeing where this
isn't immediately working out of the
isn't immediately working out of the
box. Welcome to reinforcement learning.
box. Welcome to reinforcement learning.
You know things don't always work
You know things don't always work
immediately out of the
immediately out of the
box. Uh I think it's fine. We're kind of
box. Uh I think it's fine. We're kind of
seeing what we expect to see here. And
seeing what we expect to see here. And
we just have to tweak a few things is
all. So this is
1024. All
right. So what I want to do here
I want to change the replay buffer to
I want to change the replay buffer to
only be half of the batch
size and we will go from
there. Uh, let's actually log as
there. Uh, let's actually log as
well. But we're not going to run
well. But we're not going to run
experiments all day today. That's
experiments all day today. That's
really, really boring and I just don't
really, really boring and I just don't
feel like doing it. So, I mostly want to
feel like doing it. So, I mostly want to
uh work on getting this file nice and
uh work on getting this file nice and
clean
today. And there are lots of things that
today. And there are lots of things that
we can do with that.
We can definitely improve the kernel
today. I don't want to think about
today. I don't want to think about
putting this into a class or not yet.
putting this into a class or not yet.
We're going to punt on that for
today. We can get E3B and diversity all
today. We can get E3B and diversity all
you need running on the new
you need running on the new
codebase. That would probably be useful.
We can look at a little bit of the
We can look at a little bit of the
logging stats and
stuff and maybe at like some of the
stuff and maybe at like some of the
aggregation
aggregation
code. I think that will probably be
code. I think that will probably be
enough to
enough to
get the codebase into a good state.
should be enough to get the code base
should be enough to get the code base
into a good
state. There are a couple quick
state. There are a couple quick
experiments though that we can run and
experiments though that we can run and
if we get stuck we can kind of just like
if we get stuck we can kind of just like
hunt. So, if I just turn replay
off, I should just be able to disable
off, I should just be able to disable
this thing,
this thing,
right? Reprioritize the
right? Reprioritize the
experience. Yeah, you don't need this.
Okay, cuz this might not even be like
Okay, cuz this might not even be like
the onoff whatever policy This
the onoff whatever policy This
could just be something way simpler.
F
uses data. F
uses. Let's just do this.
Okay, there we go. So, this is now no
Okay, there we go. So, this is now no
stale
stale
data but uh prioritized
data but uh prioritized
experience or what is it? Advantage
experience or what is it? Advantage
filtering basically prioritized
filtering basically prioritized
advantage
filtering. And this is the baseline
filtering. And this is the baseline
which is
random. So far so good actually.
That's not
That's not
bad. Do you need to
bad. Do you need to
use That's all we're going to really
use That's all we're going to really
need to do on the experiment side
need to do on the experiment side
depending on this result for now. We'll
depending on this result for now. We'll
leave this and then we'll look at CUDA.
leave this and then we'll look at CUDA.
I think we can look at a bit of
I think we can look at a bit of
CUDA. CUDA's fun.
Oh yeah, there we
go. So what is this pi bind torch
go. So what is this pi bind torch
extension
extension
name? Do you need to use this?
I don't think you need to use this, do
I don't think you need to use this, do
you? Oh, wait. You have this like
weird. Okay. The one thing I will use
weird. Okay. The one thing I will use
Grock a little bit for is like
Grock a little bit for is like
new new libraries and binds.
Um because I'm I have not done really
Um because I'm I have not done really
much at all with pi bind and I don't
much at all with pi bind and I don't
know if it's necessary.
I'd be surprised if it's
I see. So you can do
it. Is this that bad?
Oh, wait. You can
Oh, wait. You can
[Music]
[Music]
use PyTorch build
tools.
tools.
Okay, so you can actually get CUDA
Okay, so you can actually get CUDA
extensions here.
PyTorch implicitly uses PIBind to create
PyTorch implicitly uses PIBind to create
the Python
the Python
bindings. You don't manually define
bindings. You don't manually define
PIBind
11. So you just define this. So that's
11. So you just define this. So that's
basically the same thing.
Is that
Is that
better or is that the same bloody
better or is that the same bloody
thing? Oh, also
thing? Oh, also
um yeah, there we go. So, uh advantage
um yeah, there we go. So, uh advantage
filtering nice and clean. Questionable
filtering nice and clean. Questionable
whether you get anything else from um
whether you get anything else from um
the prioritize replay. That seems to
the prioritize replay. That seems to
break stuff, but that's fine. Okay, now
break stuff, but that's fine. Okay, now
we do we do some CUDA stuff for a
bit.
Mdeaf. This is like the same bloody
Mdeaf. This is like the same bloody
code, isn't it?
Yeah, there's the same exact
Yeah, there's the same exact
code. How do I have this defined right
now? I guess I don't have it defined in
now? I guess I don't have it defined in
here.
load. Okay, this is the thing. I have it
load. Okay, this is the thing. I have it
right now just in here instead of in the
right now just in here instead of in the
setup.py.
I
I
wonder up
How hard is
that? Pre-ompile for specific platforms
that? Pre-ompile for specific platforms
and CUDA versions.
Yeah. So, that's a little
annoying. Distributing these is a bit
annoying. Distributing these is a bit
annoying.
So, we're not going to deal with that
So, we're not going to deal with that
just at the
moment. But that makes
moment. But that makes
sense. Not missing anything,
right? All
right. The one thing I think we can do
right. The one thing I think we can do
here is
move we can move all this code
move we can move all this code
from Python into
from Python into
C or C++ whatever it is.
If you allocate the tensor and torch, is
If you allocate the tensor and torch, is
it just fine or
no? Compute advantages compute J. So
no? Compute advantages compute J. So
right
here we can kind of do some stuff,
here we can kind of do some stuff,
right? So we do
in
steps size probably and
then I'm going to assume it's
then I'm going to assume it's
autocompleting torch API here and we'll
autocompleting torch API here and we'll
fix it if it's wrong.
then it's what is it? It synchronize at
then it's what is it? It synchronize at
the
end. I don't know if that's automatic or
end. I don't know if that's automatic or
not. What's the plan with
not. What's the plan with
J? Uh so since we're shipping our own
J? Uh so since we're shipping our own
implementation, we need to ideally first
implementation, we need to ideally first
of all I don't want to have a big
of all I don't want to have a big
annoying binding in the Python file. So
annoying binding in the Python file. So
we're shipping a
we're shipping a
pufferlib.cu or
pufferlib.cu or
whatever. I don't know why it's a CU
whatever. I don't know why it's a CU
honestly for this because it's a binding
honestly for this because it's a binding
file. That's kind of weird. Uh anyways,
file. That's kind of weird. Uh anyways,
we need to figure out how to make uh the
we need to figure out how to make uh the
thing that Torch does essentially where
thing that Torch does essentially where
you have a C version and a CUDA version
you have a C version and a CUDA version
and then you know it falls back to the C
and then you know it falls back to the C
version if you don't have
version if you don't have
CUDA. So we're going to figure that out
CUDA. So we're going to figure that out
today. That'll clean up the uh clean
today. That'll clean up the uh clean
puffer file a little bit and also will
puffer file a little bit and also will
let us run all this nice new stuff in
let us run all this nice new stuff in
CPU
CPU
mode. All this is is figuring out
mode. All this is is figuring out
bindings like the actual code. You
bindings like the actual code. You
literally paste the CPU code versus the
literally paste the CPU code versus the
GPU. It's the same thing. So that's
GPU. It's the same thing. So that's
easy. It's literally just uh binding
easy. It's literally just uh binding
help. The only tricky
thing figuring out how to do this will
thing figuring out how to do this will
also be useful though for um P30 because
also be useful though for um P30 because
we're going to need to do the same thing
we're going to need to do the same thing
down the line.
down the line.
Basically, this will all get you reused.
I don't think you need to call
I don't think you need to call
synchronize.
I also don't know how this works with
I also don't know how this works with
AMP.
I'm just going to paste this into gro
I'm just going to paste this into gro
for API
check. Why is there a death in here,
check. Why is there a death in here,
dummy?
having a lot of connectivity issues on
having a lot of connectivity issues on
box
box
two. Running sweeps and offline manually
two. Running sweeps and offline manually
syncing is bandage now. Okay. Also, make
syncing is bandage now. Okay. Also, make
sure when you log in there's a disk
sure when you log in there's a disk
space usage. make sure you're not
space usage. make sure you're not
getting up to 100% utilization. Uh
getting up to 100% utilization. Uh
there's like a torch bug that's causing
there's like a torch bug that's causing
the model checkpoints to blow up. So, I
the model checkpoints to blow up. So, I
had that happen. I'm going to have to
had that happen. I'm going to have to
take a few hours at some point soon to
take a few hours at some point soon to
like go through all the machines and see
like go through all the machines and see
what's going on. I think it's straight
what's going on. I think it's straight
up just ISP. It could be that a cable
up just ISP. It could be that a cable
got damaged as well, but I really don't
got damaged as well, but I really don't
know.
It's a total pain in the ass.
47%. Yeah, it should probably be like
47%. Yeah, it should probably be like
4%. So, my guess is just the uh the
4%. So, my guess is just the uh the
checkpoints. The checkpoints are huge
checkpoints. The checkpoints are huge
for some reason. I was getting like 2
for some reason. I was getting like 2
gigabytes per checkpoint or whatever
gigabytes per checkpoint or whatever
with Neural MMO 3. They should be like
megabytes.
Okay. Replace
Okay. Replace
this with this.
device. What's the D type of
device? Oh, I actually don't like the
device? Oh, I actually don't like the
way it wrote this at all.
You don't need to worry about manual
You don't need to worry about manual
manually manual memory management
manually manual memory management
because smart pointers blah blah blah
full. That's fine.
It launch
there. Okay. I hate C++. This code
there. Okay. I hate C++. This code
sucks. Like what the
sucks. Like what the
Tensor
Tensor
options.dtype device. I programmer who
options.dtype device. I programmer who
decided this is a good way to set
decided this is a good way to set
options needs to be
shot. Good thing this is only the
shot. Good thing this is only the
binding file.
binding file.
Holy hell, this is
horrible. Uh, I don't think that this is
horrible. Uh, I don't think that this is
a float, is it?
How do I make
Do you actually have to do AI dispatch?
Do you actually have to do AI dispatch?
What the hell?
at
dispatch. But now the Cuda kernel isn't
dispatch. But now the Cuda kernel isn't
going to be
isn't going to be good,
right? Maybe we back up on this
Yeah, let's do
that. Okay. So, does this now work?
I should no longer need this.
What's wrong with
it? Yesesh, this thing like fails on
it? Yesesh, this thing like fails on
const.
Really? That's really strict.
All right. Well, we're just going to
All right. Well, we're just going to
accept that this code's going to be
accept that this code's going to be
awful. Uh there's no real way
awful. Uh there's no real way
around having that binding code be awful
around having that binding code be awful
because it's PyTorch C++
because it's PyTorch C++
API. It's written in in idiomatic C++.
API. It's written in in idiomatic C++.
So if you want to interact at all with
So if you want to interact at all with
torch,
torch,
um yeah, the only alternative would be
um yeah, the only alternative would be
to do all this checks on Python side,
to do all this checks on Python side,
but then you end up adding more annoying
Python. At least now we get this DT type
Python. At least now we get this DT type
stuff, right?
What's
idiomatic? Well, it depends on if you
idiomatic? Well, it depends on if you
like C++. I personally hate C++ and
like C++. I personally hate C++ and
think it's a disgusting language. So,
think it's a disgusting language. So,
pretty much anytime I have to interact
pretty much anytime I have to interact
with it at all, it's just like throw up
with it at all, it's just like throw up
my hands and there's no way I can make
my hands and there's no way I can make
this code nice.
There just so many things about C++ that
There just so many things about C++ that
are just horrible horrible software
are just horrible horrible software
design. Hey Joe, what do you think of
design. Hey Joe, what do you think of
the new llama
the new llama
models? I it surpris This may surprise
models? I it surpris This may surprise
some people, but I do not keep up with a
some people, but I do not keep up with a
lot of the new model releases like at
lot of the new model releases like at
all. It really doesn't affect me one
all. It really doesn't affect me one
bit.
bit.
like they're like the new models are
like they're like the new models are
kind of doing some simple RL
kind of doing some simple RL
now in a way that doesn't affect
now in a way that doesn't affect
anything that I'm doing. And like like I
anything that I'm doing. And like like I
say, all the major labs are just going
say, all the major labs are just going
to like keep leaprogging each other by a
to like keep leaprogging each other by a
few ELO points for the next like every
few ELO points for the next like every
couple months for the next who knows how
long. It really doesn't change any of
long. It really doesn't change any of
the way that I do my work. It doesn't
the way that I do my work. It doesn't
really change much of how I think about
really change much of how I think about
anything, honestly.
I don't know. If Grock ever gets to be
I don't know. If Grock ever gets to be
like so far stale behind the other ones
like so far stale behind the other ones
that I would like get some benefit out
that I would like get some benefit out
of switching, then I'll switch to
of switching, then I'll switch to
whatever the best one is at the moment.
whatever the best one is at the moment.
But other than that, there's not really
But other than that, there's not really
like a big reason for any of it.
I pretty much use Brock to
I pretty much use Brock to
like semivet research ideas, but not
like semivet research ideas, but not
really because the paper search isn't
really because the paper search isn't
great. I use it to quickly convert ideas
great. I use it to quickly convert ideas
into a text, so I don't have to type it
into a text, so I don't have to type it
up. And I use it essentially for like
up. And I use it essentially for like
APIs and things I'm not familiar with
APIs and things I'm not familiar with
yet. That's about it.
Uh, why is
this why is this not float?
Oh,
because QJ. Yeah, you don't need
because QJ. Yeah, you don't need
advantages
now. Yeah, this goes
away. Get rid of this.
advantages for you and return advantage.
Uh welcome
Uh welcome
folks there few across YouTube and
folks there few across YouTube and
Twitch.
Twitch.
Now what we're doing right now uh we are
Now what we're doing right now uh we are
working on the
working on the
new prioritized experience replay buffer
new prioritized experience replay buffer
for PO that required us to write a CUDA
for PO that required us to write a CUDA
kernel for JE because JA gets called way
kernel for JE because JA gets called way
more often now with this new code. And
more often now with this new code. And
uh we are currently cleaning up and
uh we are currently cleaning up and
getting the CUDA kernel to work nicely,
getting the CUDA kernel to work nicely,
get it nicely bound. And then we're also
get it nicely bound. And then we're also
going to have to do the CPU fallback,
going to have to do the CPU fallback,
which is a uh C implementation. It's
which is a uh C implementation. It's
actually the same code. We're going to
actually the same code. We're going to
be able to do that very cleanly, but
be able to do that very cleanly, but
that's what we're doing at the moment.
that's what we're doing at the moment.
Just jamming some low-level dev for
Just jamming some low-level dev for
Puffer. Generally having a good
time. Values, rewards, stuns.
float
float
in. Ah, you don't need this
either. Steps horizon.
you mentioned yesterday on versus off
you mentioned yesterday on versus off
policy is a myth. Can you explain that?
policy is a myth. Can you explain that?
Confused as I've read on policy priority
Confused as I've read on policy priority
replay doesn't work or is tough for a
replay doesn't work or is tough for a
while. Well, I could be wrong still, but
um in what way does off policy break
um in what way does off policy break
this?
I don't see anywhere where off policy
I don't see anywhere where off policy
breaks this
right.
So it's not easy to see from this math
So it's not easy to see from this math
definition. It's a lot easier to see
definition. It's a lot easier to see
from the code. Let me look at the let me
from the code. Let me look at the let me
show you the code.
because they use like fancy they use
because they use like fancy they use
fancy ass
fancy ass
notation. Okay,
notation. Okay,
so here's the policy gradient loss,
so here's the policy gradient loss,
right? PG
right? PG
loss. Okay, so it's advantage. Advantage
loss. Okay, so it's advantage. Advantage
doesn't get a
doesn't get a
gradient. So advantage is like
gradient. So advantage is like
essentially a
essentially a
constant times
constant times
ratio. Ratio is
ratio. Ratio is
log
log
ratio.x log ratio
ratio.x log ratio
is new log props
is new log props
uh minus log
props.reshape. Oh, hang on. Maybe there
props.reshape. Oh, hang on. Maybe there
is a PO specific thing. This is not in
is a PO specific thing. This is not in
vanilla J. I mean
vanilla J. I mean
vanilla gradient
estimation. Uh no because this is still
estimation. Uh no because this is still
clipped. Hang
clipped. Hang
on. That's only for a prop
on. That's only for a prop
scale
scale
ratio. God damn it. Now I'm confusing
myself. Oh, because this is a constant.
I went through this the other day and I
I went through this the other day and I
found I found I found something with it.
found I found I found something with it.
But now I'm trying to find where the
But now I'm trying to find where the
freak like where was the nice clean
freak like where was the nice clean
version that I was looking at. I'm
version that I was looking at. I'm
trying to remember if I looked at the J
trying to remember if I looked at the J
paper, the PO paper. It was the PO
paper, the PO paper. It was the PO
paper.
Yeah. Okay. So,
So this is just the clipping term. The
So this is just the clipping term. The
old policy is only used for the clipping
old policy is only used for the clipping
term and the new
term and the new
policy is used on the state and the uh
policy is used on the state and the uh
conditioned on the actions. So this is
conditioned on the actions. So this is
getting new logics using the new
policy. So this clipping term is
policy. So this clipping term is
irrelevant then because this is
irrelevant then because this is
essentially a constant. This advantage
essentially a constant. This advantage
term doesn't get a gradient. So this is
term doesn't get a gradient. So this is
not back propagated through at
all. So this doesn't depend on stale
all. So this doesn't depend on stale
date like this doesn't depend on
date like this doesn't depend on
anything stale at
anything stale at
all. Right?
The reasoning that I went through a
The reasoning that I went through a
while ago was it's it's a like the usual
while ago was it's it's a like the usual
reason given for like oh it's stale like
reason given for like oh it's stale like
stale data is bad. It's like the data
stale data is bad. It's like the data
comes from the old policy. But the thing
comes from the old policy. But the thing
is you don't use the logits from the old
is you don't use the logits from the old
policy here. You use new
policy here. You use new
logics. You use updated logics that you
logics. You use updated logics that you
compute from running the forward pass
compute from running the forward pass
again.
So basically, I don't see anywhere where
So basically, I don't see anywhere where
off policy data would screw you up. And
off policy data would screw you up. And
as a counterpoint, PO is immediately off
as a counterpoint, PO is immediately off
policy anyways after the first mini
policy anyways after the first mini
batch.
I was even starting to wonder if you can
I was even starting to wonder if you can
delete the clip term. Anyways,
like I'm suspicious of the thing
like I'm suspicious of the thing
completely crashing without clipping.
Okay. Oh yeah, it's dominated by one
Okay. Oh yeah, it's dominated by one
experiment.
What was I doing
here? Do not know why it formats my code
here? Do not know why it formats my code
this
this
way. Horrible code of
formatting.
formatting.
Yes. Okay, that runs now. Yay.
Now, how do we get this to be a
Now, how do we get this to be a
fallback? We have to get this to be a
fallback? We have to get this to be a
fallback.
Use conditional compilation in the
binding.
J.CP. Let's
see.
See, the problem with getting stuff like
See, the problem with getting stuff like
this from Grock is I don't know if the
this from Grock is I don't know if the
code is just generally if the state of
code is just generally if the state of
like binding code is genuinely this
like binding code is genuinely this
garbage or if it's just Grock being bad
garbage or if it's just Grock being bad
at coding.
CPU
CPU
implementation. Okay. So they have it's
implementation. Okay. So they have it's
funny it implemented
J. Worse than my version though.
J if death with CUDA
uh that's kind of
off. Oh, it's still doing the garbage.
Um, it's still doing that garbage
Um, it's still doing that garbage
like AMP thing.
Okay. So, compute
Okay. So, compute
J is still the
same. Is there Do you have to freaking
same. Is there Do you have to freaking
macro it? That's like disgusting.
Okay, so here's
your J Cuda kernel.
Okay, this is potentially better.
rely on the build system to include or
rely on the build system to include or
exclude the CUDA file.
J.H.
So this is attemp this is uh attempting
So this is attemp this is uh attempting
to
be
weird. It just has this structured
weird. It just has this structured
weirdly. It's going to take me a few
weirdly. It's going to take me a few
minutes to figure this out.
And then what is this?
Extensions. Oh, but this still doesn't
Extensions. Oh, but this still doesn't
work.
Oh, no. It's It does
So, let me see what this is. So, this is
So, let me see what this is. So, this is
the binding
the binding
file. Declare
file. Declare
both. Oh, but then they're using like
both. Oh, but then they're using like
repeated declaration. That's really
repeated declaration. That's really
obnoxious.
That's really obnoxious.
It's funny how it's always tool chain
It's funny how it's always tool chain
crap. That's
crap. That's
like the hard part,
like the hard part,
right? Writing the kernel is super easy.
If death with CUDA
Oh, wait. Does it need for
declarations? It does need four
declarations? It does need four
declarations, doesn't
it? That's annoying. That's really
it? That's annoying. That's really
annoying.
Is there a good way? I mean, this has
Is there a good way? I mean, this has
got to be a solved problem, right? This
got to be a solved problem, right? This
is just like, is there a good way to do
is just like, is there a good way to do
CUDA with CPU fallbacks without having
CUDA with CPU fallbacks without having
to do a bunch of shitty forward
to do a bunch of shitty forward
declarations
declarations
uh with conditional comp like
uh with conditional comp like
conditional
conditional
compilation, right?
compilation, right?
like the thing I'm trying to do is very
like the thing I'm trying to do is very
very simple
very simple
here. So is it
here. So is it
advantage? All right. So this is the
advantage? All right. So this is the
CUDA file. Really it's just this piece
CUDA file. Really it's just this piece
at the bottom. Uh the plan is I'm going
at the bottom. Uh the plan is I'm going
to take this and I'm going to just paste
to take this and I'm going to just paste
this into C. And then literally all this
this into C. And then literally all this
is going to do is call uh is call the
is going to do is call uh is call the
function, right? Because this is just
function, right? Because this is just
ge.
ge.
So this is
So this is
tiny and then this thing all this needs
tiny and then this thing all this needs
to do is export one binding for the CUDA
to do is export one binding for the CUDA
version and one binding for this uh the
version and one binding for this uh the
C
C
version. So I should be able to end up
version. So I should be able to end up
with like
a is this a C a CUDA file or C C++ file?
a is this a C a CUDA file or C C++ file?
This isn't a CUDA file. This is just
This isn't a CUDA file. This is just
named pufferlib.cu. This should be a C++
named pufferlib.cu. This should be a C++
file. So I should end up with a
file. So I should end up with a
pufferlib.h H I should end up with a
pufferlib.h H I should end up with a
pufferlib.cu and a pufferlib. C++
basically or not. Yeah, a pufferlib.h
basically or not. Yeah, a pufferlib.h
that contains the C
that contains the C
implementation, right? I end up with a
implementation, right? I end up with a
pufferlib.cu that contains the CUDA
pufferlib.cu that contains the CUDA
implementation and I end up with uh with
implementation and I end up with uh with
this
this
thing as well, this binding file.
But the problem is you have to compile
But the problem is you have to compile
the CUDA. Like if you're trying to use
the CUDA. Like if you're trying to use
the CPU version, then the CUDA is not
the CPU version, then the CUDA is not
going to compile if you don't have a
GPU. How does Torch do
GPU. How does Torch do
this? Is there like a clean piece of
this? Is there like a clean piece of
Torch code I could look at as a
Torch code I could look at as a
reference for this or no?
Yeah. So, this is exactly what I want to
Yeah. So, this is exactly what I want to
do like this, right?
I
mean, could I conditionally import
instead? Okay, wait. Here's an idea,
instead? Okay, wait. Here's an idea,
right? I could conditionally import
right? I could conditionally import
instead.
So I
have I could end up with two files
have I could end up with two files
instead. Right. I could
instead. Right. I could
have
have
a puffer lip CUDA and a puffer lip
a puffer lip CUDA and a puffer lip
CPU.
Right. That should make
sense. The CUDA one can import the CPU
sense. The CUDA one can import the CPU
one as well.
one as well.
Let me
Let me
commit. Well, no, because it doesn't
commit. Well, no, because it doesn't
build in the current state,
right? Does it build in the current
state? If it builds in the current
state? If it builds in the current
state, then we
will we'll commit this up and then I
will we'll commit this up and then I
will mess with it. And I think I think I
will mess with it. And I think I think I
have a pretty clean way in mind of
have a pretty clean way in mind of
making this all work.
making this all work.
Welcome YouTube folks since we have some
Welcome YouTube folks since we have some
folks uh that just joined. This is
folks uh that just joined. This is
currently getting a generalized
currently getting a generalized
advantage estimation CUDA kernel working
advantage estimation CUDA kernel working
with a graceful CPU fallback for puffer
with a graceful CPU fallback for puffer
lib uh because puffer lib is so fast
lib uh because puffer lib is so fast
that generalized advantage can become a
that generalized advantage can become a
bottleneck. So we I made it I wrote it
bottleneck. So we I made it I wrote it
in C first and now it's actually become
in C first and now it's actually become
so it actually has to now call that so
so it actually has to now call that so
often that the C is the bottleneck. So
often that the C is the bottleneck. So
now I have to have a CUDA implementation
now I have to have a CUDA implementation
of it and we're trying to get all this
of it and we're trying to get all this
working cleanly in puffer lib so you
working cleanly in puffer lib so you
basically never have to worry about
it. Yeah. Okay. So this works. Let's
it. Yeah. Okay. So this works. Let's
commit this up and then we will uh we
commit this up and then we will uh we
will implement the thing that I have in
will implement the thing that I have in
mind
mind
topl and then
popl.cu
addex. This is also a good time to
addex. This is also a good time to
remind folks to please start puffer liib
remind folks to please start puffer liib
on GitHub. It really helps us out. It's
on GitHub. It really helps us out. It's
free. We're almost at 2K.
free. We're almost at 2K.
back to
code. Okay, so this
code. Okay, so this
puffer.cu should be
puffer.cu should be
uh
uh
pufferlib
cuda.cpp. And let me make sure that that
cuda.cpp. And let me make sure that that
still works.
Okay. And then the idea is going to be
Okay. And then the idea is going to be
that we take most of these checks go
that we take most of these checks go
into the uh
into the uh
CPP. Oops. What did I do
wrong? Global does not have a type name.
wrong? Global does not have a type name.
Does this have to be a CUDA file?
Oh, CUDA is the super. Okay. Yeah. So,
Oh, CUDA is the super. Okay. Yeah. So,
this does have to be a CUDA file. That's
this does have to be a CUDA file. That's
fine. So, then that's that's even easier
fine. So, then that's that's even easier
actually because then this is
actually because then this is
pufferlib.cu.
And then it's pufferlib.cpp.
And then it's pufferlib.cpp.
Easy. And then we make
um copy
um copy
pufferlib
pufferlib
cuerlib
cuerlib
cpp. Okay. And the cpp is going to
cpp. Okay. And the cpp is going to
contain basically everything that
contain basically everything that
doesn't have to do with cuda. And the
doesn't have to do with cuda. And the
idea is we're going to reuse uh as much
idea is we're going to reuse uh as much
of this as
of this as
possible. And in order to do that, we
possible. And in order to do that, we
need we need the CUDA
code. Yeah, we need the CUDA code. So,
code. Yeah, we need the CUDA code. So,
puffer
[Music]
[Music]
lib.cu. Yeah, this this code down here.
lib.cu. Yeah, this this code down here.
We need this
So this is a CUDA function. How are we
So this is a CUDA function. How are we
going to call a CUDA function from C++?
going to call a CUDA function from C++?
We're not. We're going to do the
We're not. We're going to do the
opposite. What we're going to do here
uh we are going to
do this is
void
ge. This takes
a 1D
a 1D
tensor
horizon. Same signature,
horizon. Same signature,
right? Uh, this needs to know horizon,
right? Uh, this needs to know horizon,
but no longer needs to know num
but no longer needs to know num
steps. It's not going to have access to
steps. It's not going to have access to
row anymore.
So, idx is going to be
row time horizon. That's
nothing. We can actually just delete
nothing. We can actually just delete
this line. And all we need is t next is
this line. And all we need is t next is
going to be t + one. So, this is
going to be t + one. So, this is
actually even going to clean up the code
actually even going to clean up the code
a little bit. All right. And then you're
a little bit. All right. And then you're
going to see that this should work with
going to see that this should work with
both CUDA and uh C++ without without us
both CUDA and uh C++ without without us
having to redo all the
code. I mean, without us having to
code. I mean, without us having to
duplicate all the
code. Let me see if I missed
anything. Yeah, that should be fine. So,
anything. Yeah, that should be fine. So,
this is now J operating over a single
this is now J operating over a single
row of the tensor. So if we pass in the
row of the tensor. So if we pass in the
pointer to a specific row, this will
pointer to a specific row, this will
compute J for that
row. Okay.
And I think that autocomplete might have
And I think that autocomplete might have
even just done it for
even just done it for
us. So
us. So
values plus row times horizon.
Yeah. I think we want to do
pointer.
[Music]
Pointer. And this is offset, not
Pointer. And this is offset, not
pointer.
That should be slightly better as
well. Okay. Gamma, lambda, and horizon.
well. Okay. Gamma, lambda, and horizon.
Perfect.
I hate how it like expands this code to
I hate how it like expands this code to
take up a mill bajillion lines for no
take up a mill bajillion lines for no
reason as
well. Yeah, this
So let's just clean this up so it
So let's just clean this up so it
doesn't take up a million
doesn't take up a million
lines for what is really a very simple
lines for what is really a very simple
function. And then you'll see that this
function. And then you'll see that this
is very compact. And then the idea is
is very compact. And then the idea is
going to be that we will call the uh J
going to be that we will call the uh J
row is just going to get called from
row is just going to get called from
CUDA and C++ or C. It's technically C
CUDA and C++ or C. It's technically C
code.
code.
Uh so that'll get called from
Uh so that'll get called from
both and in this case we do the loop
both and in this case we do the loop
ourselves and for CUDA you know CUDA is
ourselves and for CUDA you know CUDA is
just parallel C so it'll loop over the
just parallel C so it'll loop over the
rows for us nicely with very minimal
rows for us nicely with very minimal
additional setup
required. That's fine.
and uh we should hope that the compiler
and uh we should hope that the compiler
inlines this for us. We might have to
inlines this for us. We might have to
give it some encouragement, but it's
give it some encouragement, but it's
probably not going to matter even if it
probably not going to matter even if it
doesn't
realistically. Vantages
So that's it. These are the functions
So that's it. These are the functions
that we need. This is not going to
that we need. This is not going to
include
include
uh C
uh C
advantage. We do need torch extension I
advantage. We do need torch extension I
believe.
And let's actually just finish this
And let's actually just finish this
binding right now.
Uh, we're not really doing P30 yet,
Uh, we're not really doing P30 yet,
right? I'm going to get rid of this for
right? I'm going to get rid of this for
now. We'll leave the CUDA version. We're
now. We'll leave the CUDA version. We're
doing
J. And I think we're going to clean up
J. And I think we're going to clean up
the way that this function is defined as
the way that this function is defined as
well. It's kind of silly the way it
is. So, we have
is. So, we have
device. We do all our
device. We do all our
checks. In fact, I think we're going to
checks. In fact, I think we're going to
do uh
check check tensor or whatever.
So we can do like
JHF
JHF
answer
check and it needs to
check and it needs to
know that's it actually. That's all it
know that's it actually. That's all it
needs to
needs to
know. Let me just paste all this
stuff. Okay, so we're down to a very
stuff. Okay, so we're down to a very
small amount of code here, right?
small amount of code here, right?
We no longer need threads per block or
We no longer need threads per block or
blocks because now we just call
blocks because now we just call
J which is the CPU version
here on values data pointer
float words data pointer
float words data pointer
float data pointer
float data pointer
float and
float and
horizon like
so and then we don't need CUDA errors
so and then we don't need CUDA errors
because this is not
CUDA.
CUDA.
Okay. So, it's going to look something
Okay. So, it's going to look something
like this. Now these module defs I might
like this. Now these module defs I might
have to fiddle
have to fiddle
with. Uh but the goal is now going to be
with. Uh but the goal is now going to be
we import this file from the other file.
So this is puffer lip
cu
CPP. Uh and then this C advantage. I
CPP. Uh and then this C advantage. I
don't even need
don't even need
this cuz this CUDA
file and just go at the top here.
Now you have your
kernel and uh this J kernel should
kernel and uh this J kernel should
become
simpler because this kernel is actually
simpler because this kernel is actually
really just a
really just a
row.
So you have access to all the data but
So you have access to all the data but
you only are given a row.
I should be able to rewrite this
I should be able to rewrite this
function. We use
that. This should be J row
Oh, that works, doesn't
it? I think that does
work.
work.
Offset. We'll do offset road times
horizon
set. Not
set. Not
bad. And now we can get rid of
bad. And now we can get rid of
this. This is the whole code.
because this is row
because this is row
parallel. So, uh we just need to call
parallel. So, uh we just need to call
from here. We literally just call the
from here. We literally just call the
one row function and we're done. And as
one row function and we're done. And as
soon as we make these signatures not
soon as we make these signatures not
take up 10 pages of code,
take up 10 pages of code,
uh this will actually be pretty concise.
And we don't need J row because it's
And we don't need J row because it's
included. So this is our whole kernel.
included. So this is our whole kernel.
This is
it. And we will leave the compute P301
it. And we will leave the compute P301
alone there.
alone there.
Uh we will add this compute J. We have
Uh we will add this compute J. We have
the validate tensors thing right.
See
See
validate check J check it
is G A check. Yeah like
this tensor advantages J check. Yep.
this tensor advantages J check. Yep.
Get rid of
Get rid of
this. And then we have our
this. And then we have our
kernel, right? And this is how it gets
kernel, right? And this is how it gets
called. And then we do have the CUDA
called. And then we do have the CUDA
track, but that's
it. So that is substantially more
it. So that is substantially more
compact. Now things I'm going to run
compact. Now things I'm going to run
into here.
into here.
Um I don't know about that module depth.
torch extension name
M. Yeah, I might end up in like with
M. Yeah, I might end up in like with
like dual declaration or whatever
like dual declaration or whatever
problems, but let's let's see if we can
problems, but let's let's see if we can
get this thing to compile.
Some steps
undefined,
right? So this check has to
get steps.
That way we don't duplicate
this. Okay. And then we'll do the same
thing. All
right, that
works. The only thing I don't know how
works. The only thing I don't know how
to get around is potentially the uh
to get around is potentially the uh
double module deaths. We'll have to
double module deaths. We'll have to
think about a second.
The rest of this should just be a little
The rest of this should just be a little
bit of standard debugging to get this
bit of standard debugging to get this
running. Okay. Num steps undefined. I
running. Okay. Num steps undefined. I
forgot to include
forgot to include
uh type
Oh, we don't need to include a CUD to
Oh, we don't need to include a CUD to
check as well. That'll screw up the C++.
check as well. That'll screw up the C++.
But we should put it
here. Get out of here, bot.
I build the box around
here. Expected and
here. Expected and
identifier. Yep. Can mess that
up. Nice.
Okay, perfect.
Okay, perfect.
So, we're down to only uh module defaf
So, we're down to only uh module defaf
errors. So, I have to understand how
errors. So, I have to understand how
this module defaf works
this module defaf works
and how to not duplicate it potentially.
Can we
defer? This thing did something that I
defer? This thing did something that I
liked before with uh with the module
liked before with uh with the module
defs.
defs.
I don't know if it solves the problem,
I don't know if it solves the problem,
but it should clean it up a little
bit. It's above this
I asked about
the Python API in
here. Was that in a previous message?
Or was it after
this? That must have been
this? That must have been
before. Yeah, it was over
here. Okay. So here uh you can take the
here. Okay. So here uh you can take the
functions out of
this which is worth
this which is worth
doing
right. You can take the wrapper
right. You can take the wrapper
functions out but you still end up with
functions out but you still end up with
this dev problem where you have declared
this dev problem where you have declared
the module twice.
I mean, I could pretty easily just
like put the shared stuff in a shared
like put the shared stuff in a shared
file, right?
So then that's three files instead of
So then that's three files instead of
two. But I can solve the problem with
two. But I can solve the problem with
that. Let's try that before I do
that. Let's try that before I do
anything
anything
silly. So puffer lib C++ puffer lib
silly. So puffer lib C++ puffer lib
cu.
cu.
Um what do I name it?
Um what do I name it?
J check.
It's annoying because I actually want to
It's annoying because I actually want to
share like the some of the C++ utils I
guess even though this is just See?
Let's do uh what is it? Void or torch
Let's do uh what is it? Void or torch
tensor.
I think for now I just I name it
I think for now I just I name it
something and then we'll figure out how
something and then we'll figure out how
to organize this after.
to organize this after.
So, um we'll just do this and
shared.
Okay. Okay. So, this is now in
shared
shared
share.cpp. So, now I have this binding
share.cpp. So, now I have this binding
here.
You need the
extension and then I need to define
extension and then I need to define
these things because this is really
these things because this is really
silly. So this is
void
P3L. Just put all this
P3L. Just put all this
code through
here. Okay, there's compute P30 and
this sand.
just
do comput
do comput
J and then we just take all this
J and then we just take all this
code put this
There. All right.
So, we now
So, we now
have a fair bit of
have a fair bit of
stuff. I've been shared. This doesn't
stuff. I've been shared. This doesn't
need to go
unshared. This can go
here. And uh now we should actually be
here. And uh now we should actually be
able to compile this if I have not made
able to compile this if I have not made
any mistakes.
and we will be able to conditionally
and we will be able to conditionally
load whatever
load whatever
extension. Maybe not the best solution,
extension. Maybe not the best solution,
but this is a relatively simple
but this is a relatively simple
one that doesn't require us doing a
one that doesn't require us doing a
bunch of crazy forward
declarations. J row is undefined in
declarations. J row is undefined in
device code.
Do these need to be marked underscore
Do these need to be marked underscore
global or some shenanigans.
QJ
J
J
kernel then this calls J
row. What if I do that?
global function call must be
global function call must be
configured. What does that mean?
You must specify how many threats will
You must specify how many threats will
be executed. This is the launch
be executed. This is the launch
configuration. Okay, so we don't want to
configuration. Okay, so we don't want to
do
do
that because we just want to call
it. So this should just be a C function.
post
post
function runs on CPU and can only be
function runs on CPU and can only be
called from other CPU code.
Lovely. Global
Lovely. Global
function runs on GPU.
This is very
silly of
underscore_host
Nice. That's very silly.
Okay, this now runs
Okay, this now runs
good and it seems to be training
good and it seems to be training
correctly as
correctly as
well.
well.
So now we see if we can import the CPU
So now we see if we can import the CPU
version and also try that as a fallback.
It's
device.
Check
that course.
so this can stay up
top sources equals
See you.
There we
There we
go. And then I just need to
do this is compute
J. Let's see if that works.
Okay, this one still
runs. Now we have to see if the CPU one
runs. Train device
CPU and we get some
error. Post does not name a
type. Yeah, that's really obnoxious.
Let's
see. CPU.
Yeah. So, obviously this isn't
available. Yep. Okay.
I think I can do this for
now. But when does the CUDA arc get
now. But when does the CUDA arc get
defined? I guess it gets this add this
defined? I guess it gets this add this
gets added by NVCC probably.
Find better way to
Find better way to
condition. And that's the wrong comment
condition. And that's the wrong comment
line.
Okay, cool. So, we now have this
Okay, cool. So, we now have this
training on
training on
CPU. We'll see uh we should at least let
CPU. We'll see uh we should at least let
it run a little bit to make sure that
it run a little bit to make sure that
the results are the same. Uh but they
the results are the same. Uh but they
should be since it's now literally
should be since it's now literally
running the same code.
And that looks pretty
And that looks pretty
good. And it's very funny to me that
good. And it's very funny to me that
this feels slow because it's running on
this feels slow because it's running on
CPU, but this is running like 20 times
CPU, but this is running like 20 times
faster generously than uh the majority
faster generously than uh the majority
of RL
of RL
research. So there we go. We have uh
research. So there we go. We have uh
CPU, we've got GPU fallback. Well, C, we
CPU, we've got GPU fallback. Well, C, we
have GPU code with CPU
have GPU code with CPU
fallback. That's not
fallback. That's not
bad. We'll have to clean this up over
bad. We'll have to clean this up over
time obviously, but at least we now have
time obviously, but at least we now have
this for debugging purposes. Yeah. And
this for debugging purposes. Yeah. And
this is matching the learn purpose well,
this is matching the learn purpose well,
so we're good. And if I run this one,
so we're good. And if I run this one,
hopefully this still
works. See you.
GPU kernel plus CPU
GPU kernel plus CPU
all
the
right. Uh so that nicely cleaned some
right. Uh so that nicely cleaned some
stuff up for
stuff up for
us. So, we can just delete this now
us. So, we can just delete this now
because we no longer need
because we no longer need
this.
Gone. And
um I'm going to just go ahead and put
um I'm going to just go ahead and put
this
into pufferlib.cu as a comment.
All right, I'm just going to put this in
All right, I'm just going to put this in
here as a
comment. Work from
Now we no longer need
this. So that will uh that will get this
this. So that will uh that will get this
substantially
shorter. This doesn't need to be global
shorter. This doesn't need to be global
either. I don't know why it's there.
So, we're now down to 1075 lines in um
So, we're now down to 1075 lines in um
clean puff RL with
clean puff RL with
that. Not bad. Not bad at
that. Not bad. Not bad at
all. Yes, we still use default dick. We
all. Yes, we still use default dick. We
still use a few other things, but it is
still use a few other things, but it is
substantially better.
It's 1:27. I'm going to use the restroom
It's 1:27. I'm going to use the restroom
real quick. I'm going to grab a new cup
real quick. I'm going to grab a new cup
of
of
tea. Maybe just get some hot water for
tea. Maybe just get some hot water for
this. And uh we're going to continue on
this. And uh we're going to continue on
and keep cleaning up this file. And then
and keep cleaning up this file. And then
this should be in a pretty good spot.
this should be in a pretty good spot.
Be right back.
Okay, we are
Okay, we are
back. Did it kick me offline or is the
back. Did it kick me offline or is the
UI just
broken? I think the UI is just broken.
broken? I think the UI is just broken.
The heck? Oh, yeah. There we're good.
Okay. So, I think I realized what's
Okay. So, I think I realized what's
happening with the uh the replay buffer
happening with the uh the replay buffer
by the way. So, it's kind of an onoff
by the way. So, it's kind of an onoff
policy
policy
issue. It depends how you define it. The
issue. It depends how you define it. The
problem is the value function though,
problem is the value function though,
not the policy. It's not that the policy
not the policy. It's not that the policy
is stale. The policy is fine. Uh it's
is stale. The policy is fine. Uh it's
the value function.
the value function.
So the issue
So the issue
is the value function is going to get
is the value function is going to get
newer. Hang on. Does that make
newer. Hang on. Does that make
sense? Yeah. So the value function is
sense? Yeah. So the value function is
trying to predict
trying to predict
uh what's going to
uh what's going to
happen at each time step basically when
happen at each time step basically when
the policy does its
the policy does its
thing. Uh but when the policy gets
thing. Uh but when the policy gets
fresher, the value function hasn't
fresher, the value function hasn't
caught up yet.
So, like the policy optimize is just
So, like the policy optimize is just
fine on this, but I think it screws up
fine on this, but I think it screws up
the value function. I think that's
the value function. I think that's
what's going on. I'll have to really
what's going on. I'll have to really
work through the math details more, but
work through the math details more, but
it's fine. Um, what we're doing with uh
it's fine. Um, what we're doing with uh
experience filtering a
experience filtering a
okay. In fact, yeah, what we're doing
okay. In fact, yeah, what we're doing
with experience filtering is a okay. uh
with experience filtering is a okay. uh
the replay across
the replay across
epochs not so
epochs not so
much but uh like we can keep it like
much but uh like we can keep it like
this for now. This should be this should
this for now. This should be this should
be basically
fine and then we will I will start
fine and then we will I will start
thinking about what modifications we
thinking about what modifications we
would need to make to make the uh the
would need to make to make the uh the
full experience replay work. But we have
full experience replay work. But we have
it implemented now and we can do it. So
it implemented now and we can do it. So
that's solid.
that's solid.
Um I think now we just we take a look at
Um I think now we just we take a look at
this at a high level again and we look
this at a high level again and we look
for more things to improve on
for more things to improve on
and then we probably we fix a few of the
and then we probably we fix a few of the
like annoying bad pieces of code that
like annoying bad pieces of code that
are in here and then we fix uh like E3B
are in here and then we fix uh like E3B
diversity all you need. We fix those
diversity all you need. We fix those
algorithms and then we should be good.
the losses we have are kind of screwy
the losses we have are kind of screwy
here,
right? Loss is kind of screwy.
episode
buffer. Maybe there are a few lines we
buffer. Maybe there are a few lines we
can cut out overall, but this is like
can cut out overall, but this is like
pretty
dense. E3B diversity all you need. Yep,
dense. E3B diversity all you need. Yep,
we'll need
we'll need
those. E30 also
those. E30 also
that needs to get updated.
learning rate schedulers
learning rate schedulers
[Music]
[Music]
am profile dashboard this crazy thing
am profile dashboard this crazy thing
again we're still going to punt on the
again we're still going to punt on the
structure of this file whether this goes
structure of this file whether this goes
in a class or whether we have some
in a class or whether we have some
better way of doing this we'll
see why not on Twitch I am on Twitch I'm
see why not on Twitch I am on Twitch I'm
on X YouTube and
Twitch. I believe it's neural MMO on
Twitch. I believe it's neural MMO on
YouTube because that just that's the
YouTube because that just that's the
channel that blew up. Yeah, it's on
channel that blew up. Yeah, it's on
Twitch X and
Twitch X and
YouTube. Uh Twitch username is the same
YouTube. Uh Twitch username is the same
as the X username. The YouTube one is
as the X username. The YouTube one is
Neural
Neural
MMO because that's where I posted my
MMO because that's where I posted my
thesis. So, that's the channel that blew
thesis. So, that's the channel that blew
up.
All
right. Little bit of shenanigans here
right. Little bit of shenanigans here
with how we're managing X. I mean, uh,
with how we're managing X. I mean, uh,
not X, how we're managing H&C, LSTM
not X, how we're managing H&C, LSTM
state. Little bit
state. Little bit
verbose. Little bit verbose with how
verbose. Little bit verbose with how
we're doing N
we're doing N
byD. A little bit of screwiness with the
byD. A little bit of screwiness with the
masking and whatnot.
Nothing like major that I see
though. We've got the
state. Yeah, it's mostly just DI a and
state. Yeah, it's mostly just DI a and
E3B being screwy.
And cool. Then the store
And cool. Then the store
happens. Info log
happens. And this is kind of fine.
And we have this
And we have this
stuff which occasionally people send us
stuff which occasionally people send us
like numpy arrays and shenanigans. It's
like numpy arrays and shenanigans. It's
kind of fine either
way
way
stats
stats and then this stuff for zeroing
stats and then this stuff for zeroing
the experience buffer.
I don't think you need to do this here.
I don't think you need to do this here.
I think we can
do config data
configures, right?
Okay. Zero the losses
out back up to the
top. Is there not like a cross entropy?
cross. So this is what's
cross. So this is what's
it
forge.n
forge.n
functional cross
entropy that can go there. We no longer
entropy that can go there. We no longer
need that
need that
mass. Then we have our total mini
mass. Then we have our total mini
batches
batches
here. Accumulate mini bags and just end
samples. So this is P30. The PO1 is one
samples. So this is P30. The PO1 is one
line. All this will eventually get moved
line. All this will eventually get moved
into the kernel most likely or whatever.
into the kernel most likely or whatever.
So this won't be a big deal.
So this won't be a big deal.
Then you get your
Then you get your
sample your
sample your
batch action dispatch. LSTMH
LSTMC. Then you do
train. You update the values.
LSTMH
LSTMH
LSTMC. I don't think we need these
anymore. Yeah, we don't need these
anymore. Yeah, we don't need these
updated
updated
anymore because we're only doing single
segments. Maybe an issue, maybe not.
and get our log
ratio clipping. This is algorithmic
ratio clipping. This is algorithmic
stuff that we need to
stuff that we need to
ablate. We have P3 loss. We have clipped
ablate. We have P3 loss. We have clipped
V
V
loss and then we have uh normal V loss.
Have entropy
lost. This diversity all you need
lost. This diversity all you need
shenanigans
thing. Scaler backwards. Few extra
thing. Scaler backwards. Few extra
metrics. Brad
metrics. Brad
flipping. Uh, this
one. Is there anything better we can do
one. Is there anything better we can do
here?
I can't see a better way of doing this
actually, which is quite
annoying. And here's the KL target.
rep prioritized
rep prioritized
experience. It's fine other than being
experience. It's fine other than being
really way too long of a line.
It's going to
It's going to
be
data policy
rows. Got your learning
rateuler. You got this
thing. Got mean and
log. Then you've got your losses
log. Then you've got your losses
[Music]
[Music]
zeroed and
zeroed and
checkpointing. Okay. Then we have the
checkpointing. Okay. Then we have the
storage function. We already went over
storage function. We already went over
this quite a bit. There are some small
this quite a bit. There are some small
tweaks needed, but quite small at this
tweaks needed, but quite small at this
point. Go over the sampler. Sampler has
point. Go over the sampler. Sampler has
a bunch of different options that we
a bunch of different options that we
don't know which ones to keep
don't know which ones to keep
yet. There's some logging that we went
yet. There's some logging that we went
over. Some artifact stuff that hasn't
over. Some artifact stuff that hasn't
been updated in a
been updated in a
while. Uh profilers kind of difficult to
while. Uh profilers kind of difficult to
get around some of this
get around some of this
mess, but these aren't a big deal.
I think I probably want my checkpointing
I think I probably want my checkpointing
code above, don't
I? Yeah, I kind of want all this code
I? Yeah, I kind of want all this code
like above where it is
now. Yeah. Utilization profile
close. Yeah, that's a better spot for
it. Okay, 1073 lines.
rollout needs obviously to be cleaned
rollout needs obviously to be cleaned
up, but the vast majority of messy stuff
up, but the vast majority of messy stuff
in this file is now being caused by um
in this file is now being caused by um
like algorithm hangers on that we
like algorithm hangers on that we
haven't finished evaluating yet. Mainly
haven't finished evaluating yet. Mainly
E3B diversity is all you need and then
E3B diversity is all you need and then
the P30 stuff. So when we end up keeping
the P30 stuff. So when we end up keeping
like one path, one code path, it will be
like one path, one code path, it will be
shorter and substantially cleaner.
Let's go double
Let's go double
check. Let's go double check. Uh
check. Let's go double check. Uh
breakout overall now. Hang
on. Break
out. No.
Let's get a breakout
curve.
Uhhuh. How is this suddenly broken?
Is this uh not commented?
Oh,
Oh,
sorry. I just missed
Okay, that does
work. 1.4
work. 1.4
mil. We will
see
score. That looks good to
score. That looks good to
me. 52 million
me. 52 million
steps solved in basically 60. Yep.
steps solved in basically 60. Yep.
That's pretty well on par with the
That's pretty well on par with the
previous
best. So we're very happy to see
best. So we're very happy to see
that. And then
uses so max usages of a sample it got
uses so max usages of a sample it got
used like 10
times. Average sample gets used once.
So that is basically just advantage
So that is basically just advantage
filtering. Yeah, the prioritized
filtering. Yeah, the prioritized
experience in this case just becomes
experience in this case just becomes
advantage
filtering. Let's make sure really quick
filtering. Let's make sure really quick
that the samplebased
that the samplebased
metric is in fact
better. I believe it will be like way
better. I believe it will be like way
better, but we will uh we'll just make
better, but we will uh we'll just make
sure real quick.
So this is now selecting
So this is now selecting
um selecting the best samples just
um selecting the best samples just
sorting them and taking the top ones
sorting them and taking the top ones
instead of assigning a probability
instead of assigning a probability
distribution.
And I believe this still runs, but uh
And I believe this still runs, but uh
it's not as
good. I believe that was the
conclusion. Is there any reason that
conclusion. Is there any reason that
this would
this would
be absum absum?
Yeah. So, this is a good comparison and
Yeah. So, this is a good comparison and
it's
uh it's much better to sample
uh it's much better to sample
apparently. We should also get a rand
apparently. We should also get a rand
one for
one for
comparison. So, this should just be uh
comparison. So, this should just be uh
what is it?
what is it?
Random. And I think that it's going to
Random. And I think that it's going to
be similar to the uh the prioritized for
be similar to the uh the prioritized for
this environment.
But we should definitely
check random will be very close to the
check random will be very close to the
original PTO. It's just instead of going
original PTO. It's just instead of going
over
over
uh chunks of samples, you're randomly
uh chunks of samples, you're randomly
picking your
chunks. There is a bit of copy over it.
I think it's very similar in
perfect. Yeah. So, prioritize not
perfect. Yeah. So, prioritize not
prioritize very very
similar axises of seven.
I don't know about
that. Oh, it is random sampling. Okay.
that. Oh, it is random sampling. Okay.
So, yeah, it is possible like yeah,
So, yeah, it is possible like yeah,
that's reasonable. So, there's not a
that's reasonable. So, there's not a
huge difference
huge difference
then. You know, it's not really using
then. You know, it's not really using
it's not really deciding to reuse
it's not really deciding to reuse
samples way more often than it would
samples way more often than it would
randomly. So, makes sense. These are two
randomly. So, makes sense. These are two
are pretty comparable.
We leave it now. We leave it for
We leave it now. We leave it for
now. So I guess then the next thing is
now. So I guess then the next thing is
just going to be to start
just going to be to start
fixing
fixing
E3B diversity all you need. Fix those up
E3B diversity all you need. Fix those up
and uh we can start running real
and uh we can start running real
benchmarks,
benchmarks,
right? Fix all the
right? Fix all the
machines and get this running on stuff.
Uh, there is also PF stuff to deal with
though. There is also PF stuff to deal
though. There is also PF stuff to deal
with. Hang on.
Let's see the copy overhead on
this. Okay. So, copy overhead.
is very
is very
low for
training. We have quite a bit of copy
training. We have quite a bit of copy
overhead now in Eval that wasn't there
overhead now in Eval that wasn't there
before we did this
stuff.
Yeah. Make sure this is getting measured
Yeah. Make sure this is getting measured
correctly. Okay. So this is eval copy
correctly. Okay. So this is eval copy
right
right
here. That's
store. It's going to be these copies
store. It's going to be these copies
right here.
I mean, I looked at a ton of different
I mean, I looked at a ton of different
ways to do this. I don't think there's a
ways to do this. I don't think there's a
good way around doing it this way,
good way around doing it this way,
though.
If I store it the other way, then uh
If I store it the other way, then uh
it'll be slower when I go to access
it. Well, how bad is it on like a real
it. Well, how bad is it on like a real
lens, right?
Okay, we
Okay, we
are pretty much all in
learn with copy creeping up a little
250k. Let me try a couple quick things
here. I think this might not be
here. I think this might not be
optimized because this is half the speed
optimized because this is half the speed
it should be.
Yeah, something's true with the way this
Yeah, something's true with the way this
is being
sampled. You see what the batches
are 2048 by 16
You end
eight. Uh that is how I had it
eight. Uh that is how I had it
before. Wait. 2048.
before. Wait. 2048.
I'm
I'm
16. That's weird though. That's bigger
16. That's weird though. That's bigger
than it's supposed to be allowed to
be.
Max is 16k.
This is data mini batch
size. Okay, that's better.
Is this uh does this account for the
Is this uh does this account for the
difference though? I don't think it
does. Oh, there you
does. Oh, there you
go. There's your perf
back. And then we'll uh we'll see what
back. And then we'll uh we'll see what
copy converges at.
Yeah, I think that's not a bad haircut
Yeah, I think that's not a bad haircut
to perf.
to perf.
Overall, there's definitely copy
Overall, there's definitely copy
bandwidth to optimize, but
um I think we got out of this pretty
um I think we got out of this pretty
well.
65 and
65 and
33, but we're not capturing um you know,
33, but we're not capturing um you know,
something's not getting profiled
something's not getting profiled
because these numbers don't add up, I'm
realizing. So, the SPS is roughly fine,
realizing. So, the SPS is roughly fine,
but these numbers don't add up, which is
but these numbers don't add up, which is
not good.
Neither of them add up at all
Neither of them add up at all
actually. So there's something that's
actually. So there's something that's
taking time and isn't getting profiled
taking time and isn't getting profiled
in both of
these. Let's see if we can find it in
these. Let's see if we can find it in
eel first.
eel first.
Okay. So, we
Okay. So, we
got MISK eval
forward eval copy
forward eval copy
eval. Uh, yeah, this entire function is
eval. Uh, yeah, this entire function is
inside an eval.
inside an eval.
So, I don't actually know what it is
So, I don't actually know what it is
that's taking
up taking up the
up taking up the
time, right?
Oh, it's probably the Hang
on. Where do we do the uh the
on. Where do we do the uh the
synchronize
call? So, context
enterp context
enterp context
exit. Okay. So, this has to go up
top. This has to go
here. Synchronize. Yeah, it's got to be
here. Synchronize. Yeah, it's got to be
like bookended.
And that should give us better
And that should give us better
counts. We will
counts. We will
see. That looks like it adds up
now. Yeah, that adds up now.
10% copy
10% copy
overhead, 18%
overhead, 18%
forward. At least we have good stats. At
forward. At least we have good stats. At
least we have good stats
now. Real quick, just for
now. Real quick, just for
fun, if I don't do any of this, does
fun, if I don't do any of this, does
this actually affect
this actually affect
Perf? We're at 428.
Oh yeah, that takes
Oh yeah, that takes
uh that takes a little bite out of the
uh that takes a little bite out of the
perf, doesn't
it? So, we have to actually optimize the
it? So, we have to actually optimize the
profiling a bit and we get some of our
profiling a bit and we get some of our
time back.
time back.
about
5%. That'll be an easy PF win.
And that's most of the fur back already
And that's most of the fur back already
just from that if we do that
just from that if we do that
correctly. Uh let me think how we would
correctly. Uh let me think how we would
do
that. I think we can't do that until we
that. I think we can't do that until we
update the dashboard a little
update the dashboard a little
bit. But we know that that is a 5enter
bit. But we know that that is a 5enter
waiting on this
So perf not
So perf not
terrible. I think we fixed
terrible. I think we fixed
E3B. We fixed diversity all you
E3B. We fixed diversity all you
need and uh
need and uh
[Music]
[Music]
we probably we're okay to start running
we probably we're okay to start running
experiments.
It does concern me somewhat the
uh ah well no never mind. We need to
uh ah well no never mind. We need to
rerun those experiments before we can
rerun those experiments before we can
make any conclusions.
make any conclusions.
So let's
So let's
do let's just get E3B and all like the
do let's just get E3B and all like the
other algorithms working for
now. Okay. So we get some errors.
I think I know what we're going to do to
I think I know what we're going to do to
clean this mess up as well.
have to move some data
have to move some data
around and we'll see if I get stuck on
around and we'll see if I get stuck on
anything, but seems pretty simple.
All right. So, that runs with a massive
All right. So, that runs with a massive
hit to Perf.
We'll check
We'll check
uh what this
uh what this
does. Is that everything though?
does. Is that everything though?
Really? I didn't have to mess with it
Really? I didn't have to mess with it
and train it
all. Oh, no. E3B is the easy one because
all. Oh, no. E3B is the easy one because
it's only uh it only adds rewards.
Yeah, right there it adds the reward.
Perfect. Crazy amount of overhead this
Perfect. Crazy amount of overhead this
thing adds though.
I will be interested to see how it
I will be interested to see how it
scales if it's uh bad with bigger
scales if it's uh bad with bigger
policies as well.
Doesn't hurt the curve that much, but
Doesn't hurt the curve that much, but
doesn't really do much for you either.
66% forward time to
E3B. Let's see how it scales.
Cuda runs out of memory. That's
funny.
funny.
Um, yeah. What do we do about that? Uh,
Um, yeah. What do we do about that? Uh,
like I didn't plan for
that. I guess we go to like two ends or
that. I guess we go to like two ends or
something.
Holy that is slow.
Let's just make sure I didn't slow it
Let's just make sure I didn't slow it
down a ton because of the two
M's. Yeah. Now
Send a message real quick.
Let me actually just put that uh in the
Let me actually just put that uh in the
Discord
quick. Where's the
quick. Where's the
uh
uh
M? I think it's this one.
512 x 512
Here we
Here we
go. All right. So, E3B not too
go. All right. So, E3B not too
optimistic about. We can get the other
optimistic about. We can get the other
algorithm working though because that
algorithm working though because that
one I actually think that there's
one I actually think that there's
something to
it. Probably E3B. There is a way to make
it. Probably E3B. There is a way to make
it work, but it's like
it work, but it's like
Very
Very
slow. Very very slow.
Okay, these are going to
Okay, these are going to
be archive skills
be archive skills
batch. Archive skills
batch. And the code's not going to look
batch. And the code's not going to look
like this for uh forever. But this is
like this for uh forever. But this is
for the initial
port. Huh.
port. Huh.
I guess we have uh
archive. Uh this
parameters. I think that'll do it.
Does this go into
there? This is size total
agents. This is
size experience. So, this one probably
size experience. So, this one probably
does go in there.
I need
skills then batch.
I could probably just do it like
um
zero skills batch and the archive is the
zero skills batch and the archive is the
only thing that needs to go into
only thing that needs to go into
data. So this is
Eat expected batch.
and get these shapes to match up.
8192. Yeah, this is fine. This is a flat
8192. Yeah, this is fine. This is a flat
state hidden.
Oh, wait. No, that's not
Oh, wait. No, that's not
fine. You want this to be
fine. You want this to be
um you want this to be batched as well.
really want this to be batched as
well. Oh, we can just do this,
well. Oh, we can just do this,
right? Not have to do this.
Bash
time. Sample logs.
Why would that affect lot just one
This happened on like second batch or
This happened on like second batch or
something weird.
No, that happened
immediately. Okay, we have
logits got batch actions.
Why are these suddenly not
Why are these suddenly not
um not
flattened? That's very
weird. Oh, cuz you did this stupid
There we
go. So, we got Q, we've got Z indices.
How did this get added to the
How did this get added to the
observation
though? Where'd that even happen? The
though? Where'd that even happen? The
indices up
top. Batch dot
That's only 128 of
them. I guess this is state zindices,
them. I guess this is state zindices,
right?
allocates
discriminator. Where's the
discriminator. Where's the
uh the extra observation though?
encode. Where's the extra
observation? I thought we add to the obs
observation? I thought we add to the obs
space, don't we?
confused
now. Yeah. You pass here
now. Yeah. You pass here
to to that, right? You pass in
Did I screw this up
somehow? I thought we were passing the
somehow? I thought we were passing the
input of this to the
policy. Let me go double check how I had
policy. Let me go double check how I had
this in depth.
So, how close are we to
2K? 1918. Pretty
2K? 1918. Pretty
close. Pretty
close. Pretty
close. Star puffer helps us out.
Okay.
So just add this somehow.
I don't see anywhere where I'm adding
I don't see anywhere where I'm adding
this as an observation at all.
see how I was doing it in
see how I was doing it in
here. Also, the dev branch of this is
here. Also, the dev branch of this is
1,200 and uh this one is
1,200 and uh this one is
six that
six that
right. Yeah, this has gotten way too
right. Yeah, this has gotten way too
long. 400 lines extra.
long. 400 lines extra.
So we definitely need to cut this
down. Okay. So here is where we compute
down. Okay. So here is where we compute
observations. Here's the
indices. This doesn't go into state at
indices. This doesn't go into state at
all. Oh wait, no. Here it does. does go
all. Oh wait, no. Here it does. does go
into state right
into state right
there. And then
policy. Does this not augment the
policy. Does this not augment the
observations?
I could have sworn that's how it
worked. Oh, you know, maybe it's in the
um Maybe it's in the LSTM or
something. It's not in here
either.
either.
Well, let's at least get it to run for
Well, let's at least get it to run for
now.
No
How do you unsqueeze a dimension like
How do you unsqueeze a dimension like
that?
Yeah, that'll expand.
Okay.
I just want this to run for now.
Okay, there we go. This still
runs.
runs.
Cool. Yes, this still runs. We have all
Cool. Yes, this still runs. We have all
these uh these things ported and
these uh these things ported and
running. Uh there's still P30 to do.
I am going to need to still do some work
I am going to need to still do some work
on
that, but this feels like enough porting
that, but this feels like enough porting
for now.
Okay.
Next, we get to run two
experiments. I like this.
Okay, good. We don't have any weird
Okay, good. We don't have any weird
stuff in
here. The first one of these is going to
here. The first one of these is going to
be with
be with
uh prioritized or what is
uh prioritized or what is
it? Prioritize filtering. Then the next
it? Prioritize filtering. Then the next
one will be without
one will be without
That'll give us a good
That'll give us a good
eval. And then we'll do a couple other
eval. And then we'll do a couple other
things with this just to make sure
things with this just to make sure
things are
things are
working. And then I think we'll do some
working. And then I think we'll do some
thinking about
algorithms. Yeah. All right. I'll let
algorithms. Yeah. All right. I'll let
this run. I'll be right back.
Okay. Scores here.
Okay. Scores here.
We let this do 100
mil and then we do the other one which
mil and then we do the other one which
will
be
method which will be the comparison run
method which will be the comparison run
for random sampling versus prioritize
for random sampling versus prioritize
filter.
This is one of those methods I would
This is one of those methods I would
expect. I will
say so it will be surprising if this
say so it will be surprising if this
doesn't at least maintain Perf.
That's a bit
disappointing. Not clearly better.
Unless it pulls
ahead. Uh there are a couple variants we
ahead. Uh there are a couple variants we
should try as well.
Low absolute
Low absolute
advantage. So I did do this correctly.
advantage. So I did do this correctly.
Low absolute
advantage. Appendix C for details.
Okay, so they're still using
Okay, so they're still using
Atom, but we can beat that straight up
Atom, but we can beat that straight up
as
as
is. And then advantage filtering. You
is. And then advantage filtering. You
collect
collect
rollouts, you compute
advantages, and
advantages, and
then max absolute advantage.
ax
equals. Okay, so they're doing a like a
equals. Okay, so they're doing a like a
direct filtering thing instead of doing
direct filtering thing instead of doing
um we have a sample based thing, they
um we have a sample based thing, they
have a filterbased
thing. Those should both be fine.
thing. Those should both be fine.
Oh, did this do something? Hey, it did
Oh, did this do something? Hey, it did
something. Look at
something. Look at
that. It did
something.
Yay. That's
cool. So, on the harder task where stuff
cool. So, on the harder task where stuff
is like
is like
No, stuff gets sparse for overtime and
No, stuff gets sparse for overtime and
whatnot. It actually does
whatnot. It actually does
something. We're happy with that result.
something. We're happy with that result.
We take that. That's a
We take that. That's a
win. That's a pretty big win, actually.
Look at that. I wonder if that'll help
Look at that. I wonder if that'll help
on neural
MMO. I'll send this over.
go just link somebody one paper.
Okay, we are happy with this. This is a
Okay, we are happy with this. This is a
win. This is a solid win. In fact, I'm
win. This is a solid win. In fact, I'm
going to uh me send this to Aaron.
solid. Now, I think what I want to do, I
solid. Now, I think what I want to do, I
want to do something a little bit out
want to do something a little bit out
there.
there.
I kind of want to just take some time to
I kind of want to just take some time to
gather my
gather my
thoughts, kind of go through a bunch of
thoughts, kind of go through a bunch of
the things on the algorithm side that
the things on the algorithm side that
have
have
been floating around my
been floating around my
head, just write down a bunch of random
head, just write down a bunch of random
throwaway
throwaway
notes, and
notes, and
uh basically just see where I think we
uh basically just see where I think we
should go next with all this because
should go next with all this because
this is going pretty well so far.
What are some things that we know to be
What are some things that we know to be
true?
Generalized vantage
Generalized vantage
estimation locks you into
perm search for
perm search for
gamma lambda appears be a bit more
stable filtering
We
We
win
prioritized experience
replay.
Um, it follows Was it the value function
Um, it follows Was it the value function
lags? The value function
lags. No, it's the policy lags behind.
No. Oh, yeah. Because the
Word up by an old policy.
policy prevents you from reusing
policy prevents you from reusing
old data to
old data to
train
train
the policy
the policy
though. So it's really it's the value
though. So it's really it's the value
function that gets screwed up.
Okay, so these are a couple things.
Um
Um
noisebased
exploration methods don't
exploration methods don't
work.
work.
Countbased exploration methods
Countbased exploration methods
work are
specific specific. And
specific specific. And
then
then
E3B might be able to work, but it's
slow can result in
visually behavior.
Um,
Um,
discriminator trained
discriminator trained
on
on
lockets prone
to
jiggle
jiggle
jiggle. Actually, is there a stop grad
jiggle. Actually, is there a stop grad
on that? Hang on. Is there a stop grab?
on that? Hang on. Is there a stop grab?
If there's a stop credit, it wouldn't
be. I think I tried it with a stop bra
be. I think I tried it with a stop bra
though and I couldn't get it to work.
Yeah, you can't do it with a stop
Yeah, you can't do it with a stop
grant because uh you have to be able to
grant because uh you have to be able to
influence the policy actions. That's the
influence the policy actions. That's the
whole point. So yeah, the formulation is
whole point. So yeah, the formulation is
weird, but the the method is good. Okay.
Dreamer V3
Dreamer V3
tricks are
tricks are
bogus. Something in world modeling lets
bogus. Something in world modeling lets
you scale big
policies. These are things I know to be
policies. These are things I know to be
true.
P3L was not a clear win.
Self play can be an easy
Self play can be an easy
curriculum, auto
curriculum, auto
curriculum.
curriculum.
Competitive self play is
Competitive self play is
prone to policy
prone to policy
collapse.
Historical self play can
help training
multiple. So what I'm kind of looking
multiple. So what I'm kind of looking
for
for
is a way to tie all this stuff
together. These are these are what I
together. These are these are what I
believe to be facts.
What is the minimal change to PO that I
What is the minimal change to PO that I
could make that would make prioritized
could make that would make prioritized
experience replay work?
All
right. Recline a little bit
here. Thinking
mode. There we go.
So the issue right
now when you compute generalized
now when you compute generalized
advantage estimation,
right? That
right? That
advantage you can look at the value
advantage you can look at the value
function is essentially learning to
function is essentially learning to
predict discounted return. That's not
predict discounted return. That's not
quite accurate. That's only true for
quite accurate. That's only true for
lambda equals 1, but it's about that.
and it's discounted return under the old
and it's discounted return under the old
policy. That is the
policy. That is the
problem. So there's actually nothing
problem. So there's actually nothing
preventing you from if you like just
preventing you from if you like just
ignore the value function, right? If we
ignore the value function, right? If we
just look at
just look at
reinforce, let's look at
reinforce. Where's the spinning up one?
Yeah. Okay. So, actually you already
Yeah. Okay. So, actually you already
have a value function in vanilla
have a value function in vanilla
reinforce it seems.
Do you? That seems weird. I thought
Do you? That seems weird. I thought
original reinforced didn't have a value
baseline. Yeah, they updated this. This
baseline. Yeah, they updated this. This
is not vanilla policy gradient. This is
like Is this really vanilla?
like Is this really vanilla?
I thought the vanilla you just used the
I thought the vanilla you just used the
returns
returns
directly without a
directly without a
baseline because if you use the returns
baseline because if you use the returns
directly without a baseline, let me zoom
directly without a baseline, let me zoom
this thing
this thing
in. If you use the returns directly
in. If you use the returns directly
without a
baseline, there's nothing that prevents
baseline, there's nothing that prevents
you from reusing the samples as much as
you from reusing the samples as much as
you want, right?
You're not reusing logits. You're
You're not reusing logits. You're
recmputing the forward pass logits every
time. So when you get into trouble is I
time. So when you get into trouble is I
think with the value
function. It's kind of backwards. It's
function. It's kind of backwards. It's
the problem isn't the current value
the problem isn't the current value
function. It's the current policy.
function. It's the current policy.
So you got the trajectories under a
So you got the trajectories under a
specific
specific
policy and
then the value
function. This is a little tricky to
function. This is a little tricky to
think about.
The value function attempts to predict
The value function attempts to predict
discounted return under the current
policy. So it's if you're in this state
policy. So it's if you're in this state
and you act with the current
and you act with the current
policy, what are you going to
policy, what are you going to
get? Uh that's going to undersshoot
get? Uh that's going to undersshoot
though because as soon as you start
though because as soon as you start
updating the
updating the
policy right that
policy right that
data is
data is
uh is now from an old
uh is now from an old
policy. Now crucially like you can keep
policy. Now crucially like you can keep
using that to train the policy. Nothing
using that to train the policy. Nothing
bad should happen there.
The issue is that the value function is
The issue is that the value function is
going to be
going to be
off because it's your you can make you
off because it's your you can make you
can get like an optimal value function
can get like an optimal value function
for the old policy not for the new one.
for the old policy not for the new one.
Now it shouldn't change that much right
Now it shouldn't change that much right
from one batch of data like you can only
from one batch of data like you can only
update the policy as much as is allowed
update the policy as much as is allowed
by a batch of
by a batch of
data. But if you start using like
data. But if you start using like
historical
historical
buffers that is no longer the case.
I don't think replacing the value
I don't think replacing the value
function with a Q function helps you
function with a Q function helps you
either. that just gives you one
either. that just gives you one
additional step, right?
additional step, right?
So a Q function
So a Q function
says it just gives you uh the value for
says it just gives you uh the value for
all
all
actions. But then it assumes that after
actions. But then it assumes that after
you take this action that you follow the
you take this action that you follow the
uh you follow like the best policy or
uh you follow like the best policy or
whatever.
So what happens if your policy if you
So what happens if your policy if you
let's say you have a Q function for your
let's say you have a Q function for your
policy right so no value function the
policy right so no value function the
policy just is a Q function we'll ignore
policy just is a Q function we'll ignore
continuous for
Does it magically
work? Well, you go to the very end.
work? Well, you go to the very end.
No, it shouldn't magically
No, it shouldn't magically
work because the last step you say,
work because the last step you say,
okay, you train the Q function with the
okay, you train the Q function with the
action that you took. That's correct.
action that you took. That's correct.
That it gets the reward. But as soon as
That it gets the reward. But as soon as
you go back one additional step,
you go back one additional step,
um I think you're off
again. How does this work in like in DQN
again. How does this work in like in DQN
for instance?
for instance?
I mean, they don't have trajectories.
I mean, they don't have trajectories.
Here is the thing. I don't think this is
Here is the thing. I don't think this is
going to be
going to be
helpful. I think it's way simpler
here. How sad is it that like there are
here. How sad is it that like there are
so many things and there's not even the
so many things and there's not even the
article, the archive
paper. There we go.
I mean this gives you implicitly
I mean this gives you implicitly
discounted returns.
doesn't include
doesn't include
J. What if we look at
J. What if we look at
like what if we look at something that
like what if we look at something that
actually
works? I see how they handle
this.
Actually, the code's probably in clean
Actually, the code's probably in clean
for this, isn't it?
though this probably doesn't have
though this probably doesn't have
recurrent
recurrent
uh recurrence in
it. How long is this? It's only 300
it. How long is this? It's only 300
lines.
Okay. So they
Okay. So they
have QET and then they have the actor
They make Oh, do they
They make Oh, do they
use Where's this replay buffer come
from? Oh, they use SP3's replay buffer.
from? Oh, they use SP3's replay buffer.
Come on. That's not clean or quality for
Come on. That's not clean or quality for
you.
Good training.
I think I really just need to understand
I think I really just need to understand
exactly where it is. Where's the line
exactly where it is. Where's the line
that lets you go off policy?
large continuous domains.
large continuous domains.
Okay. So this
Okay. So this
is part of this is handling
continuous soft value function is trying
continuous soft value function is trying
to measure minimize squared residual
to measure minimize squared residual
error. We have
that policy parameters.
I don't think I need to go this far to
I don't think I need to go this far to
figure it
figure it
out, but I should I should figure out
out, but I should I should figure out
this as
this as
well. Essentially the question I have
well. Essentially the question I have
right now right is
right now right is
just I found that the off policiness of
just I found that the off policiness of
PO isn't quite a problem in the way that
PO isn't quite a problem in the way that
it's usually thought about though there
it's usually thought about though there
is still a problem with the value
function. I'm trying to figure out
function. I'm trying to figure out
how you get around that
here. Also, these things don't have J,
here. Also, these things don't have J,
do they?
do they?
They don't have generalized advantage
They don't have generalized advantage
estimation.
Yeah, I don't trust these baselines
Yeah, I don't trust these baselines
either versus
either versus
uh I don't trust these experiments
uh I don't trust these experiments
either is the
either is the
thing. Who was this paper from
thing. Who was this paper from
originally? Sergey's
group. How many experiments do they
group. How many experiments do they
have?
Yeah, not really anywhere near as many
Yeah, not really anywhere near as many
experiments as you would want.
I think that the
I think that the
way in which off policy algorithms do
way in which off policy algorithms do
discounting is linked to what I'm
discounting is linked to what I'm
looking for.
I don't see a discounting factor in
I don't see a discounting factor in
here. Do
here. Do
you? Where's the discounting
factor? Is there no discounting factor
factor? Is there no discounting factor
in here?
There is no discounting factor
There is no discounting factor
in soft actor
in soft actor
critic. That immediately makes me
critic. That immediately makes me
suspicious.
Oh, wait. Args. Gamma. What's
gamma? Ah, there is a discount factor.
gamma? Ah, there is a discount factor.
Good. Okay, I'm not completely insane.
Where is data collection?
Okay, you step a whole bunch,
right? And you
right? And you
add to the rollout
add to the rollout
buffer and then okay, they're doing Yep.
buffer and then okay, they're doing Yep.
So, this is the collect and then here's
So, this is the collect and then here's
the step. You
the step. You
sample, you train the
sample, you train the
critic and the objective of the
critic. There's no loss being computed
critic. There's no loss being computed
here. This is no bread.
So basically it's just this chaining
So basically it's just this chaining
forward
thing. All right, I'm use the restroom.
thing. All right, I'm use the restroom.
I'll be right back. Keep thinking about
I'll be right back. Keep thinking about
this.
Okay. The key thing that I'm not sure
Okay. The key thing that I'm not sure
about here,
about here,
right, we're trying to replace J
right, we're trying to replace J
ultimately,
ultimately,
um, J is really
obnoxious because it requires that you
obnoxious because it requires that you
roll
roll
forward. Fix the chair.
So G is really obnoxious because it
So G is really obnoxious because it
requires that you roll forward
requires that you roll forward
uh trajectories. You can't really train
uh trajectories. You can't really train
until you get to the
end and that seems
essential. Where is SACE been used
essential. Where is SACE been used
though? Has anybody done like SACE on
Atari? Random paper.
So this is versus
So this is versus
rainbow actually. Maybe that would be
rainbow actually. Maybe that would be
better a
comparison. I just don't trust any of
comparison. I just don't trust any of
these papers is the problem.
There's this monstrosity.
So difficult to compare stuff like this
So difficult to compare stuff like this
though.
multistep
returns and this is Atari as well which
returns and this is Atari as well which
is not
So the thing that's difficult here,
So the thing that's difficult here,
right, generalized advantage estimation
right, generalized advantage estimation
works really, really well. In fact, the
works really, really well. In fact, the
more stuff you do with PO, the more it's
more stuff you do with PO, the more it's
like generalized advantage estimation is
like generalized advantage estimation is
most of
PO, but then off policy learning doesn't
PO, but then off policy learning doesn't
use generalized advantage
estimation. They really only have one
estimation. They really only have one
discounting factor as far as I can
discounting factor as far as I can
see. So,
They kind of do like this onestep
thing. This is
multi-step for view. Multi-step targets
multi-step for view. Multi-step targets
can be
can be
used. Okay.
They have their multistep thing, but I
They have their multistep thing, but I
think it's like three steps,
think it's like three steps,
right? Yeah. Three steps forward is all
right? Yeah. Three steps forward is all
they
go. So sketchy.
I might really have to go through a
I might really have to go through a
bunch of literature in order to figure
bunch of literature in order to figure
this type of stuff out.
I think what I'm going to do is I'm
I think what I'm going to do is I'm
going to just spend it's
going to just spend it's
336. So I'm going to spend a little time
336. So I'm going to spend a little time
on this now. I'm going to see if
on this now. I'm going to see if
anything sticks out to me. And if not, I
anything sticks out to me. And if not, I
think we will focus on trying to finish
think we will focus on trying to finish
and ship this update with muon with
and ship this update with muon with
cosign and kneeling with all the perf
cosign and kneeling with all the perf
enhancements with advantage filtering
enhancements with advantage filtering
with good experiments and baselines all
with good experiments and baselines all
of that. And then the next update will
of that. And then the next update will
be really seeing if we can fix algo side
That's going to be like an ordeal,
That's going to be like an ordeal,
though.
Obviously, I'm going to have to go
Obviously, I'm going to have to go
through all this in
through all this in
details, but
um summary
Splitting actions and action selection
Splitting actions and action selection
and value
estimation. Okay, so this is kind of
estimation. Okay, so this is kind of
analogous to the shared critic
thing. And this had evidently made a
thing. And this had evidently made a
very large
difference. The largest in
fact okay. Okay. What about
distributional full probability
distribution discretized into bins.
Okay.
Okay.
So, this is kind of the biggest one
So, this is kind of the biggest one
then, right?
This thing should have OCR, right?
Do they do drop
Do they do drop
ones? They did do drop ones. So, no
ones? They did do drop ones. So, no
distribution hurts a
distribution hurts a
lot. Oh, actually, yeah, it's a little
lot. Oh, actually, yeah, it's a little
unintuitive here. A lot of these things
unintuitive here. A lot of these things
hurt a lot.
Dueling doesn't
matter. Double doesn't matter is very
matter. Double doesn't matter is very
weird. I guess it's covered by the other
weird. I guess it's covered by the other
ones already.
So the biggest things
So the biggest things
are
are
noisy multi-step distributional
noisy multi-step distributional
prioritization. Yeah.
the distributional thing matters a lot.
This thing can't
see. Wrong line, buddy.
Yeah. So distributional seems to be the
Yeah. So distributional seems to be the
big chunk
there. Multistep also makes a big
there. Multistep also makes a big
difference but only for like three steps
difference but only for like three steps
or whatever they tuned it.
I was looking for a specific paper.
really need to be able to replace J is
really need to be able to replace J is
the
thing. Well, hang on.
The heck does this even work?
DQN you train
fors. I really just the thing is I need
fors. I really just the thing is I need
to understand very very
to understand very very
precisely what it is that lets you do.
um that lets you do like off policy
um that lets you do like off policy
samples versus
not. And the the tricky thing with this
not. And the the tricky thing with this
is
like none of these use an
like none of these use an
LSTM. Does that matter
LSTM. Does that matter
though? No, because you still end up
though? No, because you still end up
with segments in PO even if you don't
with segments in PO even if you don't
have an LSTM, right? you still end up
have an LSTM, right? you still end up
with segments. You can sample them in
with segments. You can sample them in
whatever order you like, but you still
whatever order you like, but you still
end up with segments because of the GA
end up with segments because of the GA
computation,
right? Okay.
So what prevents you from
doing unlimited
doing unlimited
sampling on that in that case?
Let's see.
So you collect your
samples. You have like
pi of obs.
you get your probability over your
you get your probability over your
action distribution.
you have a value
you have a value
function
ops I don't know v subo or
whatever off policy you Q.
I don't know what notation I'm doing at
I don't know what notation I'm doing at
all
all
here,
but we're going to keep going with this.
When you train on
policy, you train value to approximately
equal
like gamma.
R subi something like
this. Whereas with the Q function, you
this. Whereas with the Q function, you
kind of do this implicit thing, right?
and the
equation. I want to make sure I get the
equation. I want to make sure I get the
sign
right. Yeah, there it is. So, it's
R + gamma
Q. And then here the value is equal
to R
to R
I
Q S
T
minus
minus
T one something like this
Right. And then there's this is a
Right. And then there's this is a
distribution. So
presumably which action do you use here
presumably which action do you use here
though?
Oh, this is max sub
Oh, this is max sub
a. So I see. So this is the
maximum and this this one's
maximum and this this one's
max. So maybe this is it.
How is this not still dependent on the
How is this not still dependent on the
policy?
I think they should still be dependent.
I think they should still be dependent.
No, cuz when you get to the next state,
No, cuz when you get to the next state,
what action do you use?
This a has to be the action that you
This a has to be the action that you
actually took,
right? Oh, wait. Okay, I have this
wrong. So, I have this wrong. I haven't
wrong. So, I have this wrong. I haven't
done DQN in a long
time. Let's just erase this this
time. Let's just erase this this
portion.
So this is going to be
Q
X
Q
S. Then this is minus the one that you
S. Then this is minus the one that you
actually
actually
took. A
Okay, so this is the difference between
Okay, so this is the difference between
taking the best action under your
taking the best action under your
current value
current value
function and or your current Q function
function and or your current Q function
whatever and the action that you
whatever and the action that you
actually
took. So this is the thing I think that
took. So this is the thing I think that
gives it to you.
but the thing I don't get here,
right? The Q function just tells you
right? The Q function just tells you
it's like a one-step difference.
The presumption is that you follow
The presumption is that you follow
either the best policy or the sample
either the best policy or the sample
policy, whatever you're training
policy, whatever you're training
against.
had a multi-step return to work with
had a multi-step return to work with
uh with
uh with
Q-learning. They saw that in
Rainbow and it was very
Rainbow and it was very
important in Rainbow.
Oh, so this is just straight up skipping
Oh, so this is just straight up skipping
steps,
steps,
right? So you do discounted return kind
right? So you do discounted return kind
of but only for a few
steps. And then this
steps. And then this
is gamma at t +
n. Doesn't that look familiar from
J? That looks familiar.
Isn't it?
This is this component in the
This is this component in the
middle. Then this is
middle. Then this is
gamma to the k.
gamma to the k.
Yeah. So, this is
identical. This is the endstep advantage
identical. This is the endstep advantage
right
here. Or yeah, this is the endstep
advantage. There's no lambda in here
advantage. There's no lambda in here
yet, though.
So this is before you introduce lambda
So this is before you introduce lambda
at all.
This somehow disappeared when they
This somehow disappeared when they
summed it though.
Oh, this is under the max action though
Oh, this is under the max action though
here, right?
here, right?
Look. So this is different.
Yeah, but I think this is still
Yeah, but I think this is still
broken
broken
because how you get from ST to T + N is
because how you get from ST to T + N is
dependent on the current policy,
right? I think that's still broken.
That seems like a half.
Maybe this is why they have such short
Maybe this is why they have such short
returns.
They only do it for a few
steps. I think your actions can
steps. I think your actions can
diverge. And this is a huge part as
diverge. And this is a huge part as
well.
well.
Look, where' the ablation
Look, where' the ablation
go? No multistep is
go? No multistep is
down all the way there.
Let me make sure I understand this
Let me make sure I understand this
correctly.
correctly.
So this is your Q
So this is your Q
function with the current set of
parameters state and action.
And then you have the max over the Q
function and steps
ahead. That's the value function
ahead. That's the value function
basically, right?
Whenever you take max over actions,
Whenever you take max over actions,
that's just the value
function or something similar to it. It
function or something similar to it. It
depends on whether you take max or
depends on whether you take max or
whether you sample in your value
whether you sample in your value
function but very
similar. And then this
similar. And then this
one is also value
function. Slightly better I guess
function. Slightly better I guess
because you get to know what action
because you get to know what action
you're going to take for one
step. But yeah, it breaks in the middle,
step. But yeah, it breaks in the middle,
right?
Now, I guess the difference here, right,
Now, I guess the difference here, right,
is you only have like three steps to go
is you only have like three steps to go
wrong and then if every sample looks
wrong and then if every sample looks
like n steps
like n steps
ahead, they sort of get chained
ahead, they sort of get chained
together,
together,
right? And you're never off by in three
steps. Is that how that works?
I understand I think now the the one
I understand I think now the the one
step ahead case right so the one step
step ahead case right so the one step
ahead case is this the DQN yeah it is
ahead case is this the DQN yeah it is
this DQN so the one step ahead
case you have what's this gamma next
case you have what's this gamma next
state minus current
state minus current
Okay. So this is the value
function. Uh but in this one you get the
function. Uh but in this one you get the
action condition value function.
So
So
basically in
basically in
um if you don't have a Q function,
um if you don't have a Q function,
right, if you're just using a value
right, if you're just using a value
function here like uh like PO does,
function here like uh like PO does,
then the action that the policy is going
then the action that the policy is going
to take can
to take can
change and then the trajectory changes.
change and then the trajectory changes.
So you're optimizing the value function
So you're optimizing the value function
over a trajectory that the policyy's not
over a trajectory that the policyy's not
actually going to take. But here you say
actually going to take. But here you say
explicitly
explicitly
uh if the policy takes this action right
uh if the policy takes this action right
this will be the value
this will be the value
function. So it's not dependent on the
function. So it's not dependent on the
actual action that the policy
takes. Now this still
takes. Now this still
is this is just unconditioned value
is this is just unconditioned value
function
function
um but this is only one step
um but this is only one step
ahead. And then the idea here is that
ahead. And then the idea here is that
you train this transition and then if
you train this transition and then if
you train the next transition, well, the
you train the next transition, well, the
next transition also has this. So if
next transition also has this. So if
you're training all these transitions
you're training all these transitions
individually, they all chain together.
individually, they all chain together.
Then it works out.
Now, this multi-step
thing. I think if you make n large here,
thing. I think if you make n large here,
this
breaks cuz you have the exact same
breaks cuz you have the exact same
problem as
problem as
uh on
policy, but they kind of just punt on
policy, but they kind of just punt on
it.
I kind of just punt on it
anyways. And it only gets a few steps
anyways. And it only gets a few steps
ahead.
Can you just do this with J?
You can technically just do this with J,
You can technically just do this with J,
right? But the thing is the segment
right? But the thing is the segment
lengths are still really long.
Hang
on. Why do we need such long segments in
on. Why do we need such long segments in
J but not here is the
J but not here is the
question. Why does online need long
question. Why does online need long
segments and this doesn't?
Okay. As this goes to
Okay. As this goes to
infinity, yeah, you get
this. Did they cut out a term here?
I think they cut out a
I think they cut out a
term. They assume this goes to infinity,
term. They assume this goes to infinity,
don't they?
Yeah, I think that this case with one
Yeah, I think that this case with one
actually goes
actually goes
to they missed this term, don't
to they missed this term, don't
they? They just assume that it's going
they? They just assume that it's going
to be
infinite. Yeah. See, they're going to
infinite. Yeah. See, they're going to
infinity
here. So, if you don't go to infinity,
here. So, if you don't go to infinity,
gamma one
I think you recover
I think you recover
this
BST plus. Yeah.
BST plus. Yeah.
So you get
So you get
this. So this term they dropped and this
this. So this term they dropped and this
is exactly what
is exactly what
uh this is exactly what DQN is going to
uh this is exactly what DQN is going to
use.
and they keep this
and they keep this
term only like three ahead or whatever.
term only like three ahead or whatever.
They keep this term as
They keep this term as
is. That's what the max over actions
is. That's what the max over actions
does. Essentially, it turns your Q
does. Essentially, it turns your Q
function into a value
function. And then this one's kept as a
function. And then this one's kept as a
Q function and it's conditioned on the
Q function and it's conditioned on the
action that you actually took here.
So if this is one step look
So if this is one step look
ahead which is this
one. Oh yeah this is the onestep one.
one. Oh yeah this is the onestep one.
Okay so yeah now if you take this one
Okay so yeah now if you take this one
here
here
then this is value next state. this Q
then this is value next state. this Q
function and then this gives you
function and then this gives you
perfect off policy training. This gives
perfect off policy training. This gives
you perfect off policy
you perfect off policy
training because
training because
um if you go one step
ahead, this value is going to be
ahead, this value is going to be
conditioned on the action that you
conditioned on the action that you
actually took.
actually took.
So you're no longer relying on the
So you're no longer relying on the
policy to take this action because
policy to take this action because
you're explicitly conditioning the value
function. I think that's correct.
Do you just straight up sub in a Q
Do you just straight up sub in a Q
function for a value function and have
function for a value function and have
it do better then?
Because you kind of already get
Because you kind of already get
double double DQN out of this, don't
double double DQN out of this, don't
you? Let me see.
Where is that
summary? Splitting action selection and
summary? Splitting action selection and
value estimation between two networks.
Main network selects the best action in
Main network selects the best action in
the next
state and evaluates that action's Q
state and evaluates that action's Q
value using a target
network to select the best
action. Let me see how they set that up.
signing each experience randomly to
signing each experience randomly to
update one of the two value
functions. One set of weights is used to
functions. One set of weights is used to
determine the greedy policy and the
determine the greedy policy and the
other to determine its value.
I think we can just replace
I think we can just replace
the value head with a Q head, right?
Oh, got a got a thing to do pretty
Oh, got a got a thing to do pretty
soon. I think we can just replace the
soon. I think we can just replace the
value head with a Q head though. I think
value head with a Q head though. I think
that's the
that's the
finding. We can do this for
finding. We can do this for
all all discrete
ms,
ms,
right? Yeah, we can do this for all
right? Yeah, we can do this for all
discrete
ms. I think that'll be a nice thing to
ms. I think that'll be a nice thing to
try.
I don't necessarily know if we're going
I don't necessarily know if we're going
to do this immediately.
to do this immediately.
Um, it might be better to like get some
Um, it might be better to like get some
experiments in on the current thing
experiments in on the current thing
because this should let us do
because this should let us do
prioritized
prioritized
replay,
replay,
but there
is I think there is a big
tradeoff. Also, let's take a moment to
tradeoff. Also, let's take a moment to
appreciate the one week of training for
appreciate the one week of training for
200 million frames where we currently
200 million frames where we currently
for 200 million
for 200 million
frames states I guess on our
frames states I guess on our
re-implementations it takes
re-implementations it takes
about I don't know 3
minutes. So that's pretty cool.
minutes. So that's pretty cool.
four minutes
maybe. So I think that's the finding. I
maybe. So I think that's the finding. I
think the finding
is you should just be able to
is you should just be able to
replace the value function with a Q
replace the value function with a Q
function. Um does that ever hurt
function. Um does that ever hurt
you? I think it never hurts. It just it
you? I think it never hurts. It just it
makes learning maybe a bit more
complicated. I mean like the the code a
complicated. I mean like the the code a
little more
complicated. The thing that's tricky
complicated. The thing that's tricky
though is
like you only really can do off policy
like you only really can do off policy
when the multi-step returns are pretty
short. I guess the question is why does
short. I guess the question is why does
rainbow work so well then
right? Yeah, you use the baselining that
right? Yeah, you use the baselining that
happens that requires V and not
happens that requires V and not
Q. V is just equal to
Q. V is just equal to
um V is just Q of max
um V is just Q of max
action,
action,
right? Q is just action condition value
right? Q is just action condition value
function. You can recover B. You can
function. You can recover B. You can
recover B by taking the action of the
recover B by taking the action of the
policy. And that's you Aaron. Hey
All
All
right, we got a chat, right? So, I'm
right, we got a chat, right? So, I'm
gonna I'll be back in a couple minutes.
gonna I'll be back in a couple minutes.
Use the restroom, grab a drink, all
Use the restroom, grab a drink, all
that, and
that, and
uh maybe grab a half sandwich real
uh maybe grab a half sandwich real
quick and I'll be right back in a few
quick and I'll be right back in a few
minutes to chat. So, for the other folks
minutes to chat. So, for the other folks
watching, I will probably be back for an
watching, I will probably be back for an
evening session. I don't know if I'm
evening session. I don't know if I'm
going to implement this stuff
going to implement this stuff
immediately or if we're just going to
immediately or if we're just going to
try to get everything into a better
try to get everything into a better
stable state. Um,
stable state. Um,
but all my stuff's at
but all my stuff's at
puffer.ai. Closing in on 2K stars.
puffer.ai. Closing in on 2K stars.
Please go ahead and start the repo.
Please go ahead and start the repo.
Really helps us out.
Really helps us out.
And you can join the Discord to get
And you can join the Discord to get
involved with dev. It's not all staring
involved with dev. It's not all staring
at math all day. There's plenty of cool
at math all day. There's plenty of cool
dev to do.
