Kind: captions
Language: en
Okay, we are back live.
Hi. I think the uh objective
Hi. I think the uh objective
for right now
for right now
is just going to be to get a few more of
is just going to be to get a few more of
these environments running with uh new
these environments running with uh new
ultra fast
ultra fast
pars. Quite a few of them.
That's Does this actually need 800k time
That's Does this actually need 800k time
steps? 800 mil time
steps. I can't believe that it does,
steps. I can't believe that it does,
right?
Uh, that does not work. I guess it needs
Uh, that does not work. I guess it needs
to be
recompiled. Oh, wrong Python.
3.8
mil. Did I come up with a Perf metric
mil. Did I come up with a Perf metric
for this?
Player score over max
score. Five times player max
lives. Yeah, bet. Whatever the heck is
lives. Yeah, bet. Whatever the heck is
happening here is
happening here is
not not working.
Go back on
that purple.
I mean, it's kind of ridiculous to try
I mean, it's kind of ridiculous to try
to max heart pull, isn't
to max heart pull, isn't
it? I do I do just want to see if it
it? I do I do just want to see if it
works,
though.
though.
Freaking This is like nuking a
fly. Nuclear weapons for flies.
This actually will be a reasonable test
This actually will be a reasonable test
as well to see
as well to see
like if we totally crush sample
efficiency. Okay. Trains in 7 seconds.
efficiency. Okay. Trains in 7 seconds.
Can't
Can't
complain. Trains in seven seconds. Can't
complain. Trains in seven seconds. Can't
complain.
Connect four.
Maybe let's get a baseline on connect
Maybe let's get a baseline on connect
four. Actually, this one kind of
matters. Believe 30 million steps. But
matters. Believe 30 million steps. But
I'd like to know the curve shape.
Quite slow
Quite slow
training. Oh no, never mind.
training. Oh no, never mind.
400k hard bottlenecked by the
400k hard bottlenecked by the
environment is not
amazing. It's because of the scripted
amazing. It's because of the scripted
opponent.
Okay. 15 mil for
95, 20 mil for
99. And then it stabilizes from there.
So, I don't expect this one to be
So, I don't expect this one to be
ridiculously fast because the end is
slow. Uh, it probably can't actually go
slow. Uh, it probably can't actually go
very much higher than 480 to be honest
very much higher than 480 to be honest
because 65% m
because 65% m
time. This should just push it to like
time. This should just push it to like
90% time or
whatever. Oh, hang on. 512* 8
whatever. Oh, hang on. 512* 8
It should be 1024.
It should be 1024.
Not okay. Maybe it'll only push it to
Not okay. Maybe it'll only push it to
like 80% of
time. 80% of the time.
time. 80% of the time.
Spot
on and we'll see how the training curve
is. We're not far off of the original.
is. We're not far off of the original.
Oh yeah, that actually we match the
Oh yeah, that actually we match the
original perfectly
original perfectly
fine. That is solid. And this has the
fine. That is solid. And this has the
potential to be like 5x faster if uh if
potential to be like 5x faster if uh if
we improve n per. So this is
good. I don't know if we have continuous
good. I don't know if we have continuous
working. Do we have this end? Does this
working. Do we have this end? Does this
end actually
run? No, we do not. It's
fine. Okay. So, we did connect four
fine. Okay. So, we did connect four
enduro
go. Uh, grid is the
maiden. I'm actually interested in this
maiden. I'm actually interested in this
one because this is we we use this one
one because this is we we use this one
quite a bit actively in a lot of
quite a bit actively in a lot of
research.
This is 800k train right
This is 800k train right
now. This can likely
now. This can likely
go see numbum m is 2048. So this can
go see numbum m is 2048. So this can
probably be made maybe 2x faster.
But then the question is going to be
But then the question is going to be
whether we mess up anything else in the
process. Oh no, actually we can get this
process. Oh no, actually we can get this
uh quite a bit better than
uh quite a bit better than
that. I didn't see that we have it on
that. I didn't see that we have it on
one end copy.
So we have learning rate mini badge
So we have learning rate mini badge
size. It's going to do all
size. It's going to do all
this. We'll leave the rest of them
this. We'll leave the rest of them
alone. And then this is going to be
4096. Should be on our next test.
We should get substantially better wall
We should get substantially better wall
clock. We'll see if it drains well
though. Yeah, I like this end because
though. Yeah, I like this end because
it's
it's
like it's kind of a pure exploration
like it's kind of a pure exploration
task. So if we just look at the slow end
task. So if we just look at the slow end
here, you get this very nice log
here, you get this very nice log
curve. It's a pure exploration test.
solving
mazes training is very
mazes training is very
stable. It's very configurable in
stable. It's very configurable in
difficulty. It's a pretty solid
difficulty. It's a pretty solid
benchmark.
It's interesting that it actually does
It's interesting that it actually does
continue to make progress like this the
continue to make progress like this the
whole
whole
time. It's just a very clean log curve.
time. It's just a very clean log curve.
I guess that any smooth environment
I guess that any smooth environment
should look like this, right?
Well, not really. It depends on the
Well, not really. It depends on the
dynamics of the task
dynamics of the task
itself, but uh yeah, I mean, you can
see 98% act
see 98% act
probably maybe 97. This is just
probably maybe 97. This is just
consistent with what we got before.
consistent with what we got before.
And now we run the exact same thing on
And now we run the exact same thing on
optim and we see if we totally crash
optim and we see if we totally crash
learning proof or
not. Uh I'm actually I'm seeing here
not. Uh I'm actually I'm seeing here
that this is a bigger model as
well. Do we leave learning rate as is or
well. Do we leave learning rate as is or
do we use the
do we use the
higher? I think we start with the higher
higher? I think we start with the higher
one and then we see
Oh, this is going to 100%
crash. If this somehow optimizes, I will
crash. If this somehow optimizes, I will
be
be
astounded, but I highly highly doubt it.
astounded, but I highly highly doubt it.
Yeah, it's just thrashing at point
2. Okay,
2. Okay,
so we had
025. Let's try
05. This
05. This
doubled. This may not be optimal.
This trains at about double the speed of
This trains at about double the speed of
before. So this is a 2.2 mil perm model
before. So this is a 2.2 mil perm model
that is training at a million and a half
that is training at a million and a half
steps per second.
steps per second.
That actually suggests to me that we can
That actually suggests to me that we can
make the neural MMO one substantially
make the neural MMO one substantially
faster. This is the same hidden size as
faster. This is the same hidden size as
is used for neural
MMO. How's the curve look?
curve matches so
far. That's kind of just a free win
far. That's kind of just a free win
then,
then,
right? You just make it twice as fast
right? You just make it twice as fast
with no
downsides. I actually didn't expect to
downsides. I actually didn't expect to
be able to train
be able to train
um a model that's that size so quickly.
um a model that's that size so quickly.
kind of just goes to show we should
kind of just goes to show we should
spend more time optimizing our uh our
spend more time optimizing our uh our
architecture to be
fast. So these curves match, right? But
fast. So these curves match, right? But
Watch
this huge time
difference. Okay, we're quite happy with
that. Leave this as is.
So there is impulse worth referring to
So there is impulse worth referring to
come back
come back
to what's neural 3
actually. It's kind of already pretty
actually. It's kind of already pretty
close to what we would
close to what we would
expect optimal to be
right. I think the only thing is maybe
right. I think the only thing is maybe
the BPT horizon could change.
a little bit. Batch size could
a little bit. Batch size could
change. We'll run that ablation
change. We'll run that ablation
though. We'll run that
ablation. We can't run that one now.
ablation. We can't run that one now.
That's the end that takes several days.
That's the end that takes several days.
uh
uh
squared are aware.
There I think was decently fast to begin
with. Yeah.
with. Yeah.
1.6. I think this will go to like 25 or
1.6. I think this will go to like 25 or
something. Maybe three.
So the key here is reducing the episode
So the key here is reducing the episode
length. That's the solve time. So 45 is
baseline and
then ah hang on each has multiple agents
Okay. Four agents. Yeah. So, you only
Okay. Four agents. Yeah. So, you only
want 1024
M is 4096
agents. Holy 3.5 mill steps per
second match
second match
PF. Look at episode
PF. Look at episode
length. Matches what we had
length. Matches what we had
before. We just made that twice as fast.
before. We just made that twice as fast.
35 mills a second.
We did snake
We did snake
already. Let's do
square. I think this one still runs,
right? This is like really
right? This is like really
simple. This
simple. This
sanity. My car pull
is. So this is just fully
is. So this is just fully
solved in seconds.
solve in
solve in
uh like 5 seconds or something
uh like 5 seconds or something
ridiculous
ridiculous
now. Or does it
break? That's ridiculous.
break? That's ridiculous.
mil. We did tower climb,
mil. We did tower climb,
right? We need uh track pick up next.
Okay. Yeah, this doesn't need 100 mil,
Okay. Yeah, this doesn't need 100 mil,
right?
This thing kind of just insta
solves. Uh how many? It's four agents
solves. Uh how many? It's four agents
each. So this is 1024.
each. So this is 1024.
So I think
So I think
it's this
maybe we're looking for salt within 30
maybe we're looking for salt within 30
mil.
Yeah, that's totally fine.
and reduce this to 50 mil.
solves. Triple triad, I believe, is the
solves. Triple triad, I believe, is the
last
last
one. And all these can get moved into
one. And all these can get moved into
default soon, but I don't want to mess
default soon, but I don't want to mess
stuff up for Aaron, who's also working
stuff up for Aaron, who's also working
on these.
I say this has worked out very
well testing. Yeah, captain. It seems
well testing. Yeah, captain. It seems
like pretty much everything is stable on
like pretty much everything is stable on
the higher mini batch size I'm learning
the higher mini batch size I'm learning
right
right
now. So, um, all the simple MS have gone
now. So, um, all the simple MS have gone
up from like 500K to 1.5 mil up to like
up from like 500K to 1.5 mil up to like
uh 2 something to uh 4 mil. So, pretty
uh 2 something to uh 4 mil. So, pretty
much everything's doubled at least.
Triple triad seems to
Triple triad seems to
take a while to learn. Oh, I think we
take a while to learn. Oh, I think we
just never recorded this metric. This is
just never recorded this metric. This is
the win
the win
rate. Wall time has decreased. Yeah,
rate. Wall time has decreased. Yeah,
wall time decreases. Like sample
wall time decreases. Like sample
efficiency is roughly the same for most
efficiency is roughly the same for most
M's. A bit lower for some of them.
M's. A bit lower for some of them.
Um it kind of we just win. We just win
Um it kind of we just win. We just win
really
hard. So this is uh this is the maze end
hard. So this is uh this is the maze end
which is a really good exploration test.
which is a really good exploration test.
So these are the two train
curves and then these are the two train
curves and then these are the two train
curves when you go wall clock. So same
curves when you go wall clock. So same
exact train curve but then it's just
exact train curve but then it's just
fast.
Okay, so we have this as our
Okay, so we have this as our
baseline and now triple triad.
stable training with large mini batches
stable training with large mini batches
isn't something normally
isn't something normally
done. Well, it sure as hell didn't work
before. I can tell you that much.
Like sometimes you take a small hit to
Like sometimes you take a small hit to
sample efficiency, right? You can see
sample efficiency, right? You can see
the curves don't exactly
the curves don't exactly
match, but they're very
match, but they're very
close. And now this is training at 3
close. And now this is training at 3
million steps a second. So
like
right the coolest thing about this my
right the coolest thing about this my
larger mini batch size now close to one
larger mini batch size now close to one
quarter
uh 1/8 I believe default mini batch is
uh 1/8 I believe default mini batch is
500k so it's 500k mini batch 32k mini uh
500k so it's 500k mini batch 32k mini uh
500k batch, 32k mini
batch. Yeah. So, this just matched Perf,
batch. Yeah. So, this just matched Perf,
right? This just matched on
right? This just matched on
Perf and then triple triad.
This is what we like to see. Yeah, it's
This is what we like to see. Yeah, it's
pretty good. Let me send one message on
pretty good. Let me send one message on
that.
All of
All of
them. That is literally all the
ends. Impulse wars.
Maybe. How quickly will I see things
Maybe. How quickly will I see things
happen here?
You also don't have um total time step
You also don't have um total time step
set in
here. Not going to be as big for bigger
here. Not going to be as big for bigger
models. The speed ups are way more for
models. The speed ups are way more for
small models. So, the cool thing with
small models. So, the cool thing with
small models, right, is that they're way
small models, right, is that they're way
harder to optimize. Well, like it's way
harder to optimize. Well, like it's way
harder to get small models to train
harder to get small models to train
really, really, really fast. Um,
really, really, really fast. Um,
optimizing stuff gets easier as models
optimizing stuff gets easier as models
get bigger until you have to do multiGPU
get bigger until you have to do multiGPU
stuff. Then it gets hard again.
and just do
this. You're a hard bottleneck by end of
anyways. Well, not according to this
Then
300k. What is it? Drone zero reward I
300k. What is it? Drone zero reward I
think is the thing we're trying to get.
I just need a curve that goes
I just need a curve that goes
up.
up.
So I mean this curve is fine,
right? Train curve looks fine,
right? Train curve looks fine,
right? Is this what you saw before?
Winds look lower than
normal. Uh, did I change any
hypers? Possible.
I mean, let me fiddle with it a few
runs. It gets 0.2 versus the scripted
runs. It gets 0.2 versus the scripted
bot. Or is this sitting
duck? It is sitting duck.
Okay. No, this is sitting duck
Okay. No, this is sitting duck
apparently.
This is such a cool environment. Like,
This is such a cool environment. Like,
we've got to make the training for this
we've got to make the training for this
thing reasonably fast at
least. Is the new box 2D release any
least. Is the new box 2D release any
faster? I saw uh on X Aaron Kata
faster? I saw uh on X Aaron Kata
released uh 3.1.
I don't know if you've tried it
yet marginally using
yet marginally using
31. All
right. Can we get some magic compilation
right. Can we get some magic compilation
settings to like make it
settings to like make it
fast? I don't
know. pretty much been using 31
already. Are the perf improvements
already. Are the perf improvements
buried somewhere in the 10,000 lines of
buried somewhere in the 10,000 lines of
extra dependency files.
racing compiler link time optimization
racing compiler link time optimization
is the biggest
is the biggest
win. So you get link time optimization
win. So you get link time optimization
with the new puffer bindings
with the new puffer bindings
automatically. We should probably do um
automatically. We should probably do um
should try the new puffer bindings at
should try the new puffer bindings at
some point. I think Syon is usually
some point. I think Syon is usually
fast, but sometimes it generates garbage
fast, but sometimes it generates garbage
for um you know complex
stuff. But you've timed the C as well,
stuff. But you've timed the C as well,
right? Oh, and there was a big PF dip
right? Oh, and there was a big PF dip
compared to the C, I
think. Didn't we do timing the other
think. Didn't we do timing the other
day? Didn't we time the C versus the
day? Didn't we time the C versus the
Python and it was like slower in the
Python or am I making that
up? Yeah, I think so. Well, I'm not
up? Yeah, I think so. Well, I'm not
saying the new bindings are for sure
saying the new bindings are for sure
going to make it faster, but it's worth
going to make it faster, but it's worth
trying. Plus, the new bindings are just
trying. Plus, the new bindings are just
nice to work with.
You no longer have to deal with Syon.
You no longer have to deal with Syon.
You can debug, see through
You can debug, see through
Python. It doesn't generate 30,000 lines
Python. It doesn't generate 30,000 lines
of crazy like Syon
shenanigans. Yeah. So, you don't need
shenanigans. Yeah. So, you don't need
that anymore because there's just no
that anymore because there's just no
Syon.
Syon.
So that's kind of my solution to stuff.
So that's kind of my solution to stuff.
Make it
simpler. I mean, this does pretty well.
simpler. I mean, this does pretty well.
This is like linearly increasing
This is like linearly increasing
curve,
right? It's only 100 million steps. I
right? It's only 100 million steps. I
mean, this should be one minute. It's
mean, this should be one minute. It's
just
just
like stuff is not perfwise where it
like stuff is not perfwise where it
should be.
Let me apply some new settings. See what
happens. Also, I guess it just likes to
happens. Also, I guess it just likes to
take forever on
take forever on
eB. So
it's win rate's
20%. Oh, I was looking at reward. Reward
20%. Oh, I was looking at reward. Reward
is linearly
increasing. So is win rate.
M's 16 409. Wait, hang on. This gives
M's 16 409. Wait, hang on. This gives
you 4096,
you 4096,
right? Isn't it 512?
And then you have default train settings
And then you have default train settings
for a pretty big network.
Um, let's try like
this. 05
Yeah, I want to see if this does
Yeah, I want to see if this does
anything out of the box first.
Um, like technically it should, but then
Um, like technically it should, but then
the thing is like the end is going to
the thing is like the end is going to
bottleneck super hard.
Yeah. See, half your time's in the end
Yeah. See, half your time's in the end
here.
I mean, that's just
end. Like we have a few other MS that
end. Like we have a few other MS that
um are like this, but
This is definitely on the slower
side. I mean, we can use bigger
side. I mean, we can use bigger
policies, I
guess. You know, I don't think that
guess. You know, I don't think that
there's probably like
What was the speed
What was the speed
per like the end speed per
again? It could be there's some binding
again? It could be there's some binding
overhead because it seems really weird
overhead because it seems really weird
that you only get 300k with 16
cores. Wasn't fast.
Why is this not getting
logged? We're 0.9 at
50. According to this, we're doing
50. According to this, we're doing
better, but it's not showing up on the
better, but it's not showing up on the
graph for some
reason. I don't know why this isn't
reason. I don't know why this isn't
logging. It's
weird. But look, we're at 1.2
weird. But look, we're at 1.2
already. 27.
already. 27.
So, all I did is I slotted in some of
So, all I did is I slotted in some of
our latest stuff and I improved
Perf. You can link a
Perf. You can link a
sweep and compare.
One
One
for two
for two
agents
agents
[Music]
[Music]
is what's that
number? 2.5 million
number? 2.5 million
steps in 20 seconds.
steps in 20 seconds.
So that's like 5 million agent steps in
So that's like 5 million agent steps in
20 seconds,
20 seconds,
right? So is that
250k 250,000 step agent steps per
250k 250,000 step agent steps per
second,
second,
right? Yeah. So 250k *
right? Yeah. So 250k *
16 equals we should be at least getting
16 equals we should be at least getting
something decent out of this.
So there's definitely some
overhead. So this just doubled the win
overhead. So this just doubled the win
rate versus the previous and the reward.
I'm now at
I'm now at
044 in 100
mil. Not bad,
right? I don't know if it's bad to be
right? I don't know if it's bad to be
fair, but better.
Let's throw our latest params at this
Let's throw our latest params at this
thing.
the batch
the batch
size. See if this
works.
works.
19 1.9 mil. So there is some overhead
19 1.9 mil. So there is some overhead
but then I think that there's probably
but then I think that there's probably
more stuff with the vectorzation to look
more stuff with the vectorzation to look
at
right the final capital
3. Yeah, it should be like nowhere near
3. Yeah, it should be like nowhere near
that.
Let me see if they've replied.
I guess I can look at reward on this
I guess I can look at reward on this
thing. Let's get this on
Neptune. I was trying to run some
Neptune. I was trying to run some
baselines for some guys on
this. Switching the bindings. It
this. Switching the bindings. It
shouldn't be that hard.
shouldn't be that hard.
And if you're if you're not doing
And if you're if you're not doing
anything too crazy, like you can look at
anything too crazy, like you can look at
breakout, you can look at neural MMO,
breakout, you can look at neural MMO,
like there are a couple different
like there are a couple different
options for you depending on if you need
options for you depending on if you need
to pass like PM params or
There's mean reward I
There's mean reward I
guess reward.
Okay, this increases
We'll run this for them.
So this random Python end is
So this random Python end is
200k for some
context. It's got a recurrent policy on
context. It's got a recurrent policy on
it.
said gamma 0.95. Yeah, this should
said gamma 0.95. Yeah, this should
totally work then.
totally work then.
I think that uh we're kind of at the
I think that uh we're kind of at the
point where if this doesn't work, I can
point where if this doesn't work, I can
confidently say, hey, the end might have
confidently say, hey, the end might have
uh some issues in
it. How should I handle dynamic log
it. How should I handle dynamic log
entries? Depending on the number of
entries? Depending on the number of
drones, there may be more or less log
drones, there may be more or less log
entries. So, if if you look at the way
entries. So, if if you look at the way
we do it in neural MMO. Um, you log you
we do it in neural MMO. Um, you log you
add a log when an agent dies or end of
add a log when an agent dies or end of
game, I
game, I
guess. So, per agent, you add
guess. So, per agent, you add
one. Have you Let me show you how this
one. Have you Let me show you how this
works real
quick.
Oops. Because it's like it's really
Oops. Because it's like it's really
nice. So, you just have this log strct,
nice. So, you just have this log strct,
right?
You want separate logs for each agent.
You want separate logs for each agent.
Why? It's the same
policy,
right? Okay. I mean, the stats could be
right? Okay. I mean, the stats could be
one minus the other, but then if you
one minus the other, but then if you
really want it to be that
really want it to be that
way. Um, you just
like you don't have that many stats,
like you don't have that many stats,
right? Do you? You could just do like
right? Do you? You could just do like
drone zero whatever, drone zero this,
drone zero whatever, drone zero this,
and then drone one X, drone one, Y. If
and then drone one X, drone one, Y. If
you have two agents, you want to do
you have two agents, you want to do
that.
Then just do
Then just do
that. It's the same thing. It's just you
that. It's the same thing. It's just you
see there's no more like aggregation
see there's no more like aggregation
code in
here. Yeah, we'll solve that when we get
here. Yeah, we'll solve that when we get
there, Captain. No big deal.
Um okay then here
Um okay then here
look actually it's very
easy I guess technically I have to give
easy I guess technically I have to give
you access to the end here
you access to the end here
maybe unless you're reporting like a num
maybe unless you're reporting like a num
variable in the log but I would probably
variable in the log but I would probably
just make it like I'll give you access
just make it like I'll give you access
to the end in this function and then you
to the end in this function and then you
can just conditionally aggregate logs,
right? So, we'll be able to do that just
right? So, we'll be able to do that just
fine. Do it the way you're do it the way
fine. Do it the way you're do it the way
like just with zero and one for now and
like just with zero and one for now and
then we'll do that in the
then we'll do that in the
future. Like we've given ourselves so
future. Like we've given ourselves so
many ways to fix stuff at this point. It
many ways to fix stuff at this point. It
should be pretty easy.
I think it internally should reset,
right? Yeah, internally does reset
right? Yeah, internally does reset
because the episode length is correct.
Uh, my Chrome window just closed for no
reason. We still
live. That's never happened before. It
live. That's never happened before. It
literally just closed Chrome for no
reason. Oh, it tried to update itself.
Auto updates suck.
Audio and video are desc. It tips
all I think.
Hang on. This thing's got to be alarming
Hang on. This thing's got to be alarming
me or
something. Are we good now?
They've got to have a memory leak or
They've got to have a memory leak or
something. Does this record system
stats? Does this thing record system
stats? Does this thing record system
stats? Because I think that they just
stats? Because I think that they just
like I think this M is just
like I think this M is just
like not implemented correctly at
like not implemented correctly at
all.
Yep. Probably some mem leak or
something. Yeah, it wasn't taxing GPU at
all. All
all. All
right.
right.
Well, let me uh let me reopen everything
Well, let me uh let me reopen everything
here.
here.
And I will tell the authors of this that
And I will tell the authors of this that
uh they might want to disable crash your
uh they might want to disable crash your
uh crash your PC
uh crash your PC
mode on the M.
All right, that's their
All right, that's their
report. That's kind of cool though to be
report. That's kind of cool though to be
able to do that because like hey, this
able to do that because like hey, this
works on every other
environment book,
environment book,
right? If I didn't have this set of
right? If I didn't have this set of
parameters working on every other end, I
parameters working on every other end, I
couldn't say that I wasn't like I
couldn't say that I wasn't like I
haven't screwed something up. But this
haven't screwed something up. But this
is like very very high confidence in uh
is like very very high confidence in uh
this result which is exactly the point
this result which is exactly the point
of puffer
lip. Look how easy that was. I just take
lip. Look how easy that was. I just take
the latest version of puffer. I run it
the latest version of puffer. I run it
on this M and we get what is almost
on this M and we get what is almost
certainly a conclusive result.
Get them.
pretty good.
They're telling me now that I have a
They're telling me now that I have a
dated version of the end because they
dated version of the end because they
moved debit to some other branch. Okay.
I'm pretty sure I have it right. Is this
I'm pretty sure I have it right. Is this
client
MV? It's open source. Feel like
How much memory is this thing taking
up? Oh, yeah.
I'm just rerunning it it like real quick
I'm just rerunning it it like real quick
to make
to make
sure. Looks like the same
result. It's 100% the same. Not dated
result. It's 100% the same. Not dated
end, just doesn't
end, just doesn't
run. Okay.
run. Okay.
Result looks good to
me. Let me give them one
me. Let me give them one
more. Let me see one more thing if
more. Let me see one more thing if
they're going to respond to this. And
they're going to respond to this. And
then if not, I will grab a drink and
then if not, I will grab a drink and
then we will move on to next dev
then we will move on to next dev
segment, whatever that may be.
segment, whatever that may be.
I have one thing that Aaron asked for
I have one thing that Aaron asked for
that I can do real quick.
Want to check the model
size.
Crazy.
Crazy model.
I'm pretty sure they told me these
I'm pretty sure they told me these
values were all continuous.
Got to love Twitch bots. Get out of
Got to love Twitch bots. Get out of
here. Run out of block list just on
here. Run out of block list just on
bots.
set config
config. Oh, is this in here? Hang on.
I think this is just a sim link,
right? Ah, this is the issue.
Okay.
Uh, what the
heck? Yeah.
Okay. Why is this imported?
[ __ ]
Dude, how the [ __ ] is it that they have
Dude, how the [ __ ] is it that they have
everything importing everything
everything importing everything
else? I get rid of this
How the hell is it that everything that
How the hell is it that everything that
they imports everything else in the
they imports everything else in the
entire [ __ ] library?
Okay. Is this in the requirements file?
Okay. Is this in the requirements file?
Let me just check to make sure I'm not
Let me just check to make sure I'm not
the one who's a dummy
here. Okay, so they have it, but they
here. Okay, so they have it, but they
didn't set it up correctly.
config
three. Okay, now I just need the
three. Okay, now I just need the
file which they just added me to.
Let's see where they added me to
this. So technically the only private
this. So technically the only private
thing here is this one file which is a
thing here is this one file which is a
bunch of data. There's no way to leak
bunch of data. There's no way to leak
this by
this by
mistake. All the code's open source.
No such
file. Why do they not have this such
file. Why do they not have this such
that it works?
data
historical data path.
too many values to unpack.
What's this thing
What's this thing
return
return
features?
Ah, okay.
Config
Config
states,
states,
prices. Just config
prices. Just config
states, prices, attributes, timestamps.
spices.
Data
reader. I'll get some ppg stuff to look
reader. I'll get some ppg stuff to look
at. Oh, does it do anything? Cool. Let
at. Oh, does it do anything? Cool. Let
me fix this and then gladly
The [ __ ] is this thing? Oh, I need to
The [ __ ] is this thing? Oh, I need to
thread this
through. Does this
through. Does this
matter? Let's do this.
Oops.

Kind: captions
Language: en
Okay, we are back live.
Hi. I think the uh objective
Hi. I think the uh objective
for right now
for right now
is just going to be to get a few more of
is just going to be to get a few more of
these environments running with uh new
these environments running with uh new
ultra fast
ultra fast
pars. Quite a few of them.
That's Does this actually need 800k time
That's Does this actually need 800k time
steps? 800 mil time
steps. I can't believe that it does,
steps. I can't believe that it does,
right?
Uh, that does not work. I guess it needs
Uh, that does not work. I guess it needs
to be
recompiled. Oh, wrong Python.
3.8
mil. Did I come up with a Perf metric
mil. Did I come up with a Perf metric
for this?
Player score over max
score. Five times player max
lives. Yeah, bet. Whatever the heck is
lives. Yeah, bet. Whatever the heck is
happening here is
happening here is
not not working.
Go back on
that purple.
I mean, it's kind of ridiculous to try
I mean, it's kind of ridiculous to try
to max heart pull, isn't
to max heart pull, isn't
it? I do I do just want to see if it
it? I do I do just want to see if it
works,
though.
though.
Freaking This is like nuking a
fly. Nuclear weapons for flies.
This actually will be a reasonable test
This actually will be a reasonable test
as well to see
as well to see
like if we totally crush sample
efficiency. Okay. Trains in 7 seconds.
efficiency. Okay. Trains in 7 seconds.
Can't
Can't
complain. Trains in seven seconds. Can't
complain. Trains in seven seconds. Can't
complain.
Connect four.
Maybe let's get a baseline on connect
Maybe let's get a baseline on connect
four. Actually, this one kind of
matters. Believe 30 million steps. But
matters. Believe 30 million steps. But
I'd like to know the curve shape.
Quite slow
Quite slow
training. Oh no, never mind.
training. Oh no, never mind.
400k hard bottlenecked by the
400k hard bottlenecked by the
environment is not
amazing. It's because of the scripted
amazing. It's because of the scripted
opponent.
Okay. 15 mil for
95, 20 mil for
99. And then it stabilizes from there.
So, I don't expect this one to be
So, I don't expect this one to be
ridiculously fast because the end is
slow. Uh, it probably can't actually go
slow. Uh, it probably can't actually go
very much higher than 480 to be honest
very much higher than 480 to be honest
because 65% m
because 65% m
time. This should just push it to like
time. This should just push it to like
90% time or
whatever. Oh, hang on. 512* 8
whatever. Oh, hang on. 512* 8
It should be 1024.
It should be 1024.
Not okay. Maybe it'll only push it to
Not okay. Maybe it'll only push it to
like 80% of
time. 80% of the time.
time. 80% of the time.
Spot
on and we'll see how the training curve
is. We're not far off of the original.
is. We're not far off of the original.
Oh yeah, that actually we match the
Oh yeah, that actually we match the
original perfectly
original perfectly
fine. That is solid. And this has the
fine. That is solid. And this has the
potential to be like 5x faster if uh if
potential to be like 5x faster if uh if
we improve n per. So this is
good. I don't know if we have continuous
good. I don't know if we have continuous
working. Do we have this end? Does this
working. Do we have this end? Does this
end actually
run? No, we do not. It's
fine. Okay. So, we did connect four
fine. Okay. So, we did connect four
enduro
go. Uh, grid is the
maiden. I'm actually interested in this
maiden. I'm actually interested in this
one because this is we we use this one
one because this is we we use this one
quite a bit actively in a lot of
quite a bit actively in a lot of
research.
This is 800k train right
This is 800k train right
now. This can likely
now. This can likely
go see numbum m is 2048. So this can
go see numbum m is 2048. So this can
probably be made maybe 2x faster.
But then the question is going to be
But then the question is going to be
whether we mess up anything else in the
process. Oh no, actually we can get this
process. Oh no, actually we can get this
uh quite a bit better than
uh quite a bit better than
that. I didn't see that we have it on
that. I didn't see that we have it on
one end copy.
So we have learning rate mini badge
So we have learning rate mini badge
size. It's going to do all
size. It's going to do all
this. We'll leave the rest of them
this. We'll leave the rest of them
alone. And then this is going to be
4096. Should be on our next test.
We should get substantially better wall
We should get substantially better wall
clock. We'll see if it drains well
though. Yeah, I like this end because
though. Yeah, I like this end because
it's
it's
like it's kind of a pure exploration
like it's kind of a pure exploration
task. So if we just look at the slow end
task. So if we just look at the slow end
here, you get this very nice log
here, you get this very nice log
curve. It's a pure exploration test.
solving
mazes training is very
mazes training is very
stable. It's very configurable in
stable. It's very configurable in
difficulty. It's a pretty solid
difficulty. It's a pretty solid
benchmark.
It's interesting that it actually does
It's interesting that it actually does
continue to make progress like this the
continue to make progress like this the
whole
whole
time. It's just a very clean log curve.
time. It's just a very clean log curve.
I guess that any smooth environment
I guess that any smooth environment
should look like this, right?
Well, not really. It depends on the
Well, not really. It depends on the
dynamics of the task
dynamics of the task
itself, but uh yeah, I mean, you can
see 98% act
see 98% act
probably maybe 97. This is just
probably maybe 97. This is just
consistent with what we got before.
consistent with what we got before.
And now we run the exact same thing on
And now we run the exact same thing on
optim and we see if we totally crash
optim and we see if we totally crash
learning proof or
not. Uh I'm actually I'm seeing here
not. Uh I'm actually I'm seeing here
that this is a bigger model as
well. Do we leave learning rate as is or
well. Do we leave learning rate as is or
do we use the
do we use the
higher? I think we start with the higher
higher? I think we start with the higher
one and then we see
Oh, this is going to 100%
crash. If this somehow optimizes, I will
crash. If this somehow optimizes, I will
be
be
astounded, but I highly highly doubt it.
astounded, but I highly highly doubt it.
Yeah, it's just thrashing at point
2. Okay,
2. Okay,
so we had
025. Let's try
05. This
05. This
doubled. This may not be optimal.
This trains at about double the speed of
This trains at about double the speed of
before. So this is a 2.2 mil perm model
before. So this is a 2.2 mil perm model
that is training at a million and a half
that is training at a million and a half
steps per second.
steps per second.
That actually suggests to me that we can
That actually suggests to me that we can
make the neural MMO one substantially
make the neural MMO one substantially
faster. This is the same hidden size as
faster. This is the same hidden size as
is used for neural
MMO. How's the curve look?
curve matches so
far. That's kind of just a free win
far. That's kind of just a free win
then,
then,
right? You just make it twice as fast
right? You just make it twice as fast
with no
downsides. I actually didn't expect to
downsides. I actually didn't expect to
be able to train
be able to train
um a model that's that size so quickly.
um a model that's that size so quickly.
kind of just goes to show we should
kind of just goes to show we should
spend more time optimizing our uh our
spend more time optimizing our uh our
architecture to be
fast. So these curves match, right? But
fast. So these curves match, right? But
Watch
this huge time
difference. Okay, we're quite happy with
that. Leave this as is.
So there is impulse worth referring to
So there is impulse worth referring to
come back
come back
to what's neural 3
actually. It's kind of already pretty
actually. It's kind of already pretty
close to what we would
close to what we would
expect optimal to be
right. I think the only thing is maybe
right. I think the only thing is maybe
the BPT horizon could change.
a little bit. Batch size could
a little bit. Batch size could
change. We'll run that ablation
change. We'll run that ablation
though. We'll run that
ablation. We can't run that one now.
ablation. We can't run that one now.
That's the end that takes several days.
That's the end that takes several days.
uh
uh
squared are aware.
There I think was decently fast to begin
with. Yeah.
with. Yeah.
1.6. I think this will go to like 25 or
1.6. I think this will go to like 25 or
something. Maybe three.
So the key here is reducing the episode
So the key here is reducing the episode
length. That's the solve time. So 45 is
baseline and
then ah hang on each has multiple agents
Okay. Four agents. Yeah. So, you only
Okay. Four agents. Yeah. So, you only
want 1024
M is 4096
agents. Holy 3.5 mill steps per
second match
second match
PF. Look at episode
PF. Look at episode
length. Matches what we had
length. Matches what we had
before. We just made that twice as fast.
before. We just made that twice as fast.
35 mills a second.
We did snake
We did snake
already. Let's do
square. I think this one still runs,
right? This is like really
right? This is like really
simple. This
simple. This
sanity. My car pull
is. So this is just fully
is. So this is just fully
solved in seconds.
solve in
solve in
uh like 5 seconds or something
uh like 5 seconds or something
ridiculous
ridiculous
now. Or does it
break? That's ridiculous.
break? That's ridiculous.
mil. We did tower climb,
mil. We did tower climb,
right? We need uh track pick up next.
Okay. Yeah, this doesn't need 100 mil,
Okay. Yeah, this doesn't need 100 mil,
right?
This thing kind of just insta
solves. Uh how many? It's four agents
solves. Uh how many? It's four agents
each. So this is 1024.
each. So this is 1024.
So I think
So I think
it's this
maybe we're looking for salt within 30
maybe we're looking for salt within 30
mil.
Yeah, that's totally fine.
and reduce this to 50 mil.
solves. Triple triad, I believe, is the
solves. Triple triad, I believe, is the
last
last
one. And all these can get moved into
one. And all these can get moved into
default soon, but I don't want to mess
default soon, but I don't want to mess
stuff up for Aaron, who's also working
stuff up for Aaron, who's also working
on these.
I say this has worked out very
well testing. Yeah, captain. It seems
well testing. Yeah, captain. It seems
like pretty much everything is stable on
like pretty much everything is stable on
the higher mini batch size I'm learning
the higher mini batch size I'm learning
right
right
now. So, um, all the simple MS have gone
now. So, um, all the simple MS have gone
up from like 500K to 1.5 mil up to like
up from like 500K to 1.5 mil up to like
uh 2 something to uh 4 mil. So, pretty
uh 2 something to uh 4 mil. So, pretty
much everything's doubled at least.
Triple triad seems to
Triple triad seems to
take a while to learn. Oh, I think we
take a while to learn. Oh, I think we
just never recorded this metric. This is
just never recorded this metric. This is
the win
the win
rate. Wall time has decreased. Yeah,
rate. Wall time has decreased. Yeah,
wall time decreases. Like sample
wall time decreases. Like sample
efficiency is roughly the same for most
efficiency is roughly the same for most
M's. A bit lower for some of them.
M's. A bit lower for some of them.
Um it kind of we just win. We just win
Um it kind of we just win. We just win
really
hard. So this is uh this is the maze end
hard. So this is uh this is the maze end
which is a really good exploration test.
which is a really good exploration test.
So these are the two train
curves and then these are the two train
curves and then these are the two train
curves when you go wall clock. So same
curves when you go wall clock. So same
exact train curve but then it's just
exact train curve but then it's just
fast.
Okay, so we have this as our
Okay, so we have this as our
baseline and now triple triad.
stable training with large mini batches
stable training with large mini batches
isn't something normally
isn't something normally
done. Well, it sure as hell didn't work
before. I can tell you that much.
Like sometimes you take a small hit to
Like sometimes you take a small hit to
sample efficiency, right? You can see
sample efficiency, right? You can see
the curves don't exactly
the curves don't exactly
match, but they're very
match, but they're very
close. And now this is training at 3
close. And now this is training at 3
million steps a second. So
like
right the coolest thing about this my
right the coolest thing about this my
larger mini batch size now close to one
larger mini batch size now close to one
quarter
uh 1/8 I believe default mini batch is
uh 1/8 I believe default mini batch is
500k so it's 500k mini batch 32k mini uh
500k so it's 500k mini batch 32k mini uh
500k batch, 32k mini
batch. Yeah. So, this just matched Perf,
batch. Yeah. So, this just matched Perf,
right? This just matched on
right? This just matched on
Perf and then triple triad.
This is what we like to see. Yeah, it's
This is what we like to see. Yeah, it's
pretty good. Let me send one message on
pretty good. Let me send one message on
that.
All of
All of
them. That is literally all the
ends. Impulse wars.
Maybe. How quickly will I see things
Maybe. How quickly will I see things
happen here?
You also don't have um total time step
You also don't have um total time step
set in
here. Not going to be as big for bigger
here. Not going to be as big for bigger
models. The speed ups are way more for
models. The speed ups are way more for
small models. So, the cool thing with
small models. So, the cool thing with
small models, right, is that they're way
small models, right, is that they're way
harder to optimize. Well, like it's way
harder to optimize. Well, like it's way
harder to get small models to train
harder to get small models to train
really, really, really fast. Um,
really, really, really fast. Um,
optimizing stuff gets easier as models
optimizing stuff gets easier as models
get bigger until you have to do multiGPU
get bigger until you have to do multiGPU
stuff. Then it gets hard again.
and just do
this. You're a hard bottleneck by end of
anyways. Well, not according to this
Then
300k. What is it? Drone zero reward I
300k. What is it? Drone zero reward I
think is the thing we're trying to get.
I just need a curve that goes
I just need a curve that goes
up.
up.
So I mean this curve is fine,
right? Train curve looks fine,
right? Train curve looks fine,
right? Is this what you saw before?
Winds look lower than
normal. Uh, did I change any
hypers? Possible.
I mean, let me fiddle with it a few
runs. It gets 0.2 versus the scripted
runs. It gets 0.2 versus the scripted
bot. Or is this sitting
duck? It is sitting duck.
Okay. No, this is sitting duck
Okay. No, this is sitting duck
apparently.
This is such a cool environment. Like,
This is such a cool environment. Like,
we've got to make the training for this
we've got to make the training for this
thing reasonably fast at
least. Is the new box 2D release any
least. Is the new box 2D release any
faster? I saw uh on X Aaron Kata
faster? I saw uh on X Aaron Kata
released uh 3.1.
I don't know if you've tried it
yet marginally using
yet marginally using
31. All
right. Can we get some magic compilation
right. Can we get some magic compilation
settings to like make it
settings to like make it
fast? I don't
know. pretty much been using 31
already. Are the perf improvements
already. Are the perf improvements
buried somewhere in the 10,000 lines of
buried somewhere in the 10,000 lines of
extra dependency files.
racing compiler link time optimization
racing compiler link time optimization
is the biggest
is the biggest
win. So you get link time optimization
win. So you get link time optimization
with the new puffer bindings
with the new puffer bindings
automatically. We should probably do um
automatically. We should probably do um
should try the new puffer bindings at
should try the new puffer bindings at
some point. I think Syon is usually
some point. I think Syon is usually
fast, but sometimes it generates garbage
fast, but sometimes it generates garbage
for um you know complex
stuff. But you've timed the C as well,
stuff. But you've timed the C as well,
right? Oh, and there was a big PF dip
right? Oh, and there was a big PF dip
compared to the C, I
think. Didn't we do timing the other
think. Didn't we do timing the other
day? Didn't we time the C versus the
day? Didn't we time the C versus the
Python and it was like slower in the
Python or am I making that
up? Yeah, I think so. Well, I'm not
up? Yeah, I think so. Well, I'm not
saying the new bindings are for sure
saying the new bindings are for sure
going to make it faster, but it's worth
going to make it faster, but it's worth
trying. Plus, the new bindings are just
trying. Plus, the new bindings are just
nice to work with.
You no longer have to deal with Syon.
You no longer have to deal with Syon.
You can debug, see through
You can debug, see through
Python. It doesn't generate 30,000 lines
Python. It doesn't generate 30,000 lines
of crazy like Syon
shenanigans. Yeah. So, you don't need
shenanigans. Yeah. So, you don't need
that anymore because there's just no
that anymore because there's just no
Syon.
Syon.
So that's kind of my solution to stuff.
So that's kind of my solution to stuff.
Make it
simpler. I mean, this does pretty well.
simpler. I mean, this does pretty well.
This is like linearly increasing
This is like linearly increasing
curve,
right? It's only 100 million steps. I
right? It's only 100 million steps. I
mean, this should be one minute. It's
mean, this should be one minute. It's
just
just
like stuff is not perfwise where it
like stuff is not perfwise where it
should be.
Let me apply some new settings. See what
happens. Also, I guess it just likes to
happens. Also, I guess it just likes to
take forever on
take forever on
eB. So
it's win rate's
20%. Oh, I was looking at reward. Reward
20%. Oh, I was looking at reward. Reward
is linearly
increasing. So is win rate.
M's 16 409. Wait, hang on. This gives
M's 16 409. Wait, hang on. This gives
you 4096,
you 4096,
right? Isn't it 512?
And then you have default train settings
And then you have default train settings
for a pretty big network.
Um, let's try like
this. 05
Yeah, I want to see if this does
Yeah, I want to see if this does
anything out of the box first.
Um, like technically it should, but then
Um, like technically it should, but then
the thing is like the end is going to
the thing is like the end is going to
bottleneck super hard.
Yeah. See, half your time's in the end
Yeah. See, half your time's in the end
here.
I mean, that's just
end. Like we have a few other MS that
end. Like we have a few other MS that
um are like this, but
This is definitely on the slower
side. I mean, we can use bigger
side. I mean, we can use bigger
policies, I
guess. You know, I don't think that
guess. You know, I don't think that
there's probably like
What was the speed
What was the speed
per like the end speed per
again? It could be there's some binding
again? It could be there's some binding
overhead because it seems really weird
overhead because it seems really weird
that you only get 300k with 16
cores. Wasn't fast.
Why is this not getting
logged? We're 0.9 at
50. According to this, we're doing
50. According to this, we're doing
better, but it's not showing up on the
better, but it's not showing up on the
graph for some
reason. I don't know why this isn't
reason. I don't know why this isn't
logging. It's
weird. But look, we're at 1.2
weird. But look, we're at 1.2
already. 27.
already. 27.
So, all I did is I slotted in some of
So, all I did is I slotted in some of
our latest stuff and I improved
Perf. You can link a
Perf. You can link a
sweep and compare.
One
One
for two
for two
agents
agents
[Music]
[Music]
is what's that
number? 2.5 million
number? 2.5 million
steps in 20 seconds.
steps in 20 seconds.
So that's like 5 million agent steps in
So that's like 5 million agent steps in
20 seconds,
20 seconds,
right? So is that
250k 250,000 step agent steps per
250k 250,000 step agent steps per
second,
second,
right? Yeah. So 250k *
right? Yeah. So 250k *
16 equals we should be at least getting
16 equals we should be at least getting
something decent out of this.
So there's definitely some
overhead. So this just doubled the win
overhead. So this just doubled the win
rate versus the previous and the reward.
I'm now at
I'm now at
044 in 100
mil. Not bad,
right? I don't know if it's bad to be
right? I don't know if it's bad to be
fair, but better.
Let's throw our latest params at this
Let's throw our latest params at this
thing.
the batch
the batch
size. See if this
works.
works.
19 1.9 mil. So there is some overhead
19 1.9 mil. So there is some overhead
but then I think that there's probably
but then I think that there's probably
more stuff with the vectorzation to look
more stuff with the vectorzation to look
at
right the final capital
3. Yeah, it should be like nowhere near
3. Yeah, it should be like nowhere near
that.
Let me see if they've replied.
I guess I can look at reward on this
I guess I can look at reward on this
thing. Let's get this on
Neptune. I was trying to run some
Neptune. I was trying to run some
baselines for some guys on
this. Switching the bindings. It
this. Switching the bindings. It
shouldn't be that hard.
shouldn't be that hard.
And if you're if you're not doing
And if you're if you're not doing
anything too crazy, like you can look at
anything too crazy, like you can look at
breakout, you can look at neural MMO,
breakout, you can look at neural MMO,
like there are a couple different
like there are a couple different
options for you depending on if you need
options for you depending on if you need
to pass like PM params or
There's mean reward I
There's mean reward I
guess reward.
Okay, this increases
We'll run this for them.
So this random Python end is
So this random Python end is
200k for some
context. It's got a recurrent policy on
context. It's got a recurrent policy on
it.
said gamma 0.95. Yeah, this should
said gamma 0.95. Yeah, this should
totally work then.
totally work then.
I think that uh we're kind of at the
I think that uh we're kind of at the
point where if this doesn't work, I can
point where if this doesn't work, I can
confidently say, hey, the end might have
confidently say, hey, the end might have
uh some issues in
it. How should I handle dynamic log
it. How should I handle dynamic log
entries? Depending on the number of
entries? Depending on the number of
drones, there may be more or less log
drones, there may be more or less log
entries. So, if if you look at the way
entries. So, if if you look at the way
we do it in neural MMO. Um, you log you
we do it in neural MMO. Um, you log you
add a log when an agent dies or end of
add a log when an agent dies or end of
game, I
game, I
guess. So, per agent, you add
guess. So, per agent, you add
one. Have you Let me show you how this
one. Have you Let me show you how this
works real
quick.
Oops. Because it's like it's really
Oops. Because it's like it's really
nice. So, you just have this log strct,
nice. So, you just have this log strct,
right?
You want separate logs for each agent.
You want separate logs for each agent.
Why? It's the same
policy,
right? Okay. I mean, the stats could be
right? Okay. I mean, the stats could be
one minus the other, but then if you
one minus the other, but then if you
really want it to be that
really want it to be that
way. Um, you just
like you don't have that many stats,
like you don't have that many stats,
right? Do you? You could just do like
right? Do you? You could just do like
drone zero whatever, drone zero this,
drone zero whatever, drone zero this,
and then drone one X, drone one, Y. If
and then drone one X, drone one, Y. If
you have two agents, you want to do
you have two agents, you want to do
that.
Then just do
Then just do
that. It's the same thing. It's just you
that. It's the same thing. It's just you
see there's no more like aggregation
see there's no more like aggregation
code in
here. Yeah, we'll solve that when we get
here. Yeah, we'll solve that when we get
there, Captain. No big deal.
Um okay then here
Um okay then here
look actually it's very
easy I guess technically I have to give
easy I guess technically I have to give
you access to the end here
you access to the end here
maybe unless you're reporting like a num
maybe unless you're reporting like a num
variable in the log but I would probably
variable in the log but I would probably
just make it like I'll give you access
just make it like I'll give you access
to the end in this function and then you
to the end in this function and then you
can just conditionally aggregate logs,
right? So, we'll be able to do that just
right? So, we'll be able to do that just
fine. Do it the way you're do it the way
fine. Do it the way you're do it the way
like just with zero and one for now and
like just with zero and one for now and
then we'll do that in the
then we'll do that in the
future. Like we've given ourselves so
future. Like we've given ourselves so
many ways to fix stuff at this point. It
many ways to fix stuff at this point. It
should be pretty easy.
I think it internally should reset,
right? Yeah, internally does reset
right? Yeah, internally does reset
because the episode length is correct.
Uh, my Chrome window just closed for no
reason. We still
live. That's never happened before. It
live. That's never happened before. It
literally just closed Chrome for no
reason. Oh, it tried to update itself.
Auto updates suck.
Audio and video are desc. It tips
all I think.
Hang on. This thing's got to be alarming
Hang on. This thing's got to be alarming
me or
something. Are we good now?
They've got to have a memory leak or
They've got to have a memory leak or
something. Does this record system
stats? Does this thing record system
stats? Does this thing record system
stats? Because I think that they just
stats? Because I think that they just
like I think this M is just
like I think this M is just
like not implemented correctly at
like not implemented correctly at
all.
Yep. Probably some mem leak or
something. Yeah, it wasn't taxing GPU at
all. All
all. All
right.
right.
Well, let me uh let me reopen everything
Well, let me uh let me reopen everything
here.
here.
And I will tell the authors of this that
And I will tell the authors of this that
uh they might want to disable crash your
uh they might want to disable crash your
uh crash your PC
uh crash your PC
mode on the M.
All right, that's their
All right, that's their
report. That's kind of cool though to be
report. That's kind of cool though to be
able to do that because like hey, this
able to do that because like hey, this
works on every other
environment book,
environment book,
right? If I didn't have this set of
right? If I didn't have this set of
parameters working on every other end, I
parameters working on every other end, I
couldn't say that I wasn't like I
couldn't say that I wasn't like I
haven't screwed something up. But this
haven't screwed something up. But this
is like very very high confidence in uh
is like very very high confidence in uh
this result which is exactly the point
this result which is exactly the point
of puffer
lip. Look how easy that was. I just take
lip. Look how easy that was. I just take
the latest version of puffer. I run it
the latest version of puffer. I run it
on this M and we get what is almost
on this M and we get what is almost
certainly a conclusive result.
Get them.
pretty good.
They're telling me now that I have a
They're telling me now that I have a
dated version of the end because they
dated version of the end because they
moved debit to some other branch. Okay.
I'm pretty sure I have it right. Is this
I'm pretty sure I have it right. Is this
client
MV? It's open source. Feel like
How much memory is this thing taking
up? Oh, yeah.
I'm just rerunning it it like real quick
I'm just rerunning it it like real quick
to make
to make
sure. Looks like the same
result. It's 100% the same. Not dated
result. It's 100% the same. Not dated
end, just doesn't
end, just doesn't
run. Okay.
run. Okay.
Result looks good to
me. Let me give them one
me. Let me give them one
more. Let me see one more thing if
more. Let me see one more thing if
they're going to respond to this. And
they're going to respond to this. And
then if not, I will grab a drink and
then if not, I will grab a drink and
then we will move on to next dev
then we will move on to next dev
segment, whatever that may be.
segment, whatever that may be.
I have one thing that Aaron asked for
I have one thing that Aaron asked for
that I can do real quick.
Want to check the model
size.
Crazy.
Crazy model.
I'm pretty sure they told me these
I'm pretty sure they told me these
values were all continuous.
Got to love Twitch bots. Get out of
Got to love Twitch bots. Get out of
here. Run out of block list just on
here. Run out of block list just on
bots.
set config
config. Oh, is this in here? Hang on.
I think this is just a sim link,
right? Ah, this is the issue.
Okay.
Uh, what the
heck? Yeah.
Okay. Why is this imported?
[ __ ]
Dude, how the [ __ ] is it that they have
Dude, how the [ __ ] is it that they have
everything importing everything
everything importing everything
else? I get rid of this
How the hell is it that everything that
How the hell is it that everything that
they imports everything else in the
they imports everything else in the
entire [ __ ] library?
Okay. Is this in the requirements file?
Okay. Is this in the requirements file?
Let me just check to make sure I'm not
Let me just check to make sure I'm not
the one who's a dummy
here. Okay, so they have it, but they
here. Okay, so they have it, but they
didn't set it up correctly.
config
three. Okay, now I just need the
three. Okay, now I just need the
file which they just added me to.
Let's see where they added me to
this. So technically the only private
this. So technically the only private
thing here is this one file which is a
thing here is this one file which is a
bunch of data. There's no way to leak
bunch of data. There's no way to leak
this by
this by
mistake. All the code's open source.
No such
file. Why do they not have this such
file. Why do they not have this such
that it works?
data
historical data path.
too many values to unpack.
What's this thing
What's this thing
return
return
features?
Ah, okay.
Config
Config
states,
states,
prices. Just config
prices. Just config
states, prices, attributes, timestamps.
spices.
Data
reader. I'll get some ppg stuff to look
reader. I'll get some ppg stuff to look
at. Oh, does it do anything? Cool. Let
at. Oh, does it do anything? Cool. Let
me fix this and then gladly
The [ __ ] is this thing? Oh, I need to
The [ __ ] is this thing? Oh, I need to
thread this
through. Does this
through. Does this
matter? Let's do this.
Oops.
