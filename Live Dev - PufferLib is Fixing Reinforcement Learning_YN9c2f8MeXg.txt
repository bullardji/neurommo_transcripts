Kind: captions
Language: en
We are live.
We are live.
Hi. I'm just going to go make a live
Hi. I'm just going to go make a live
stream announcement real quick and then
stream announcement real quick and then
we are going to get to
dev. We have a full day planned today.
Okay, so here's the uh the plan for
Okay, so here's the uh the plan for
today. We're going to get this
today. We're going to get this
experience buffer working. We're going
experience buffer working. We're going
to go through a few different
to go through a few different
permutations of how this could be
permutations of how this could be
implemented. Um I started on this
implemented. Um I started on this
yesterday. I was absolutely exhausted.
yesterday. I was absolutely exhausted.
Uh, I went to bed at 700 p.m. and slept
Uh, I went to bed at 700 p.m. and slept
till 8 a.m. So, I'm refreshed and we're
till 8 a.m. So, I'm refreshed and we're
going to get all this
going to get all this
stuff going here. Um, so I realized that
stuff going here. Um, so I realized that
I was pretty much correct in the way
I was pretty much correct in the way
that I was doing this before already.
that I was doing this before already.
Uh, there are a few small changes that I
Uh, there are a few small changes that I
am going to want to make to this
though. And now the question is going to
though. And now the question is going to
be, do I make them now or do I get the
be, do I make them now or do I get the
current version to run
first? I think it's better to just make
first? I think it's better to just make
the changes now and get it working
the changes now and get it working
once.
Okay, let me make sure I'm actually
Okay, let me make sure I'm actually
correct with how I was going to do this.
correct with how I was going to do this.
So the uh the crux of it here is the uh
So the uh the crux of it here is the uh
the buffer shape.
the buffer shape.
You see it is numbum rows by BPT
You see it is numbum rows by BPT
horizon like
horizon like
so. Uh what I was going to
do not quite this
rows
rows
bash it's going to be the number of
bash it's going to be the number of
rows. What is it?
No, it's still
numeros. It was going to be something
numeros. It was going to be something
like a trajectory
segment
numbum something like this. And then it
numbum something like this. And then it
would be number of rows
by any
batches length something like this. I
batches length something like this. I
don't think this is yet perfect either.
Yeah, the data structure for this is a
Yeah, the data structure for this is a
little bit tricky. So technically right
little bit tricky. So technically right
you can have any number of trajectories
you can have any number of trajectories
any number any trajectory length any
any number any trajectory length any
segment horizon for BPT
segment horizon for BPT
uh
BPTT there's a lot that you need to add
BPTT there's a lot that you need to add
here
I guess the two cases that we want to
I guess the two cases that we want to
optimize for are like your standard old
optimize for are like your standard old
environments which are not vectorzed and
environments which are not vectorzed and
you're not going to be running very many
you're not going to be running very many
copies of because they're slow and then
copies of because they're slow and then
the vectorzed case where everything is
the vectorzed case where everything is
very fast and you can assume that you're
very fast and you can assume that you're
going to get no uh equal chunks of
going to get no uh equal chunks of
experience from each subprocess.
So instead of doing numbum
rows, it's like mini batches, micro
rows, it's like mini batches, micro
batches or something, isn't it?
So if I really wanted to do this, it
So if I really wanted to do this, it
would be like num micro
would be like num micro
batches micro batch
batches micro batch
size. But that's annoying, right?
size. But that's annoying, right?
Because now we have this 5D
Because now we have this 5D
tensor. Yeah, we have a 5D tensor.
There's nothing wrong with that
There's nothing wrong with that
fundamentally. It's just very silly to
fundamentally. It's just very silly to
work with, right? Because then you can
work with, right? Because then you can
imagine that all these tensors up top,
imagine that all these tensors up top,
log prop, rewards, done, truncated, all
log prop, rewards, done, truncated, all
these, uh, now we're going to have to do
these, uh, now we're going to have to do
the same with
the same with
these. That's annoying.
Yeah. So in this case, right, you'd loop
Yeah. So in this case, right, you'd loop
over mini battress and then you would
over mini battress and then you would
loop over micro batches with the same
loop over micro batches with the same
LSTM
LSTM
state and then you would go uh you know
state and then you would go uh you know
take all the data from these
take all the data from these
get micro batch by trib length and this
get micro batch by trib length and this
is your data by obsh
shape. So that
works. I don't like this though. I
works. I don't like this though. I
really don't like this.
And the reason that
um are you ever going to have
uh are you ever going to have more m
uh are you ever going to have more m
than micro batch size? You are right.
Yeah. So, this is tricky because like
Yeah. So, this is tricky because like
what ends up happening here, right?
what ends up happening here, right?
Let's say you have 8192 MS or 8192
Let's say you have 8192 MS or 8192
agents. You're not going to be able to
agents. You're not going to be able to
run
run
optim over a batch of that size most
optim over a batch of that size most
likely.
That's tricky. That was very
tricky. It's almost one of those things,
tricky. It's almost one of those things,
right, where you have to build it as
right, where you have to build it as
generally as possible. Um, not because,
generally as possible. Um, not because,
you know, necessarily it needs to be
you know, necessarily it needs to be
that way, but because you don't know
that way, but because you don't know
what the correct specific build is,
what the correct specific build is,
right?
right?
like we might be able to assume that
like we might be able to assume that
you're always going to have more m than
you're always going to have more m than
backros or something like that. And then
backros or something like that. And then
we can kill one of these dimensions
Guess we could time some stuff as
well. The expectation I have is that the
well. The expectation I have is that the
LSTM, the number of rows in the LSTM
LSTM, the number of rows in the LSTM
probably has to
probably has to
be reasonably large.
Actually, it is a
Actually, it is a
fused It is a fused CUDA
fused It is a fused CUDA
kernel. Maybe it doesn't have to
kernel. Maybe it doesn't have to
be. Should we just test that? Does that
be. Should we just test that? Does that
is that going to help us
though? I don't actually know what that
though? I don't actually know what that
would tell us.
This isn't easy to write to either,
This isn't easy to write to either,
right? Yeah, because you need to
index. Okay, this is quite difficult
index. Okay, this is quite difficult
actually to design
actually to design
this. Maybe I should start drawing this
this. Maybe I should start drawing this
so people can see. No, cuz this is like
so people can see. No, cuz this is like
very abstract. Maybe I should just start
very abstract. Maybe I should just start
drawing this so people can see what I'm
drawing this so people can see what I'm
talking about. And maybe I'll figure it
talking about. And maybe I'll figure it
out that
way. I'm going just download this as a
way. I'm going just download this as a
PDF in case I need
PDF in case I need
that. All right. So, here's the issue,
that. All right. So, here's the issue,
right?
uh the what the thing that we're looking
uh the what the thing that we're looking
at
at
now you have experience it comes in from
now you have experience it comes in from
the environments
right so each of these is an
agent and then you're probably going to
agent and then you're probably going to
get something like this being like a an
get something like this being like a an
Mstep
So you're going to get some group of
So you're going to get some group of
uh of observations for some of the
uh of observations for some of the
agents and then what your job is to do
agents and then what your job is to do
uh your job is to turn this into a batch
uh your job is to turn this into a batch
for
for
training. Now a batch for training is
training. Now a batch for training is
probably each mini batch rather is going
probably each mini batch rather is going
to
to
contain ideally unpadded fixed length
It's going to be like some number of
It's going to be like some number of
steps. So this is like sag
length and then this is
length and then this is
your mini
batch. So what's wrong with this, right?
batch. So what's wrong with this, right?
Why can't you just grab like this and
Why can't you just grab like this and
like this? Well, this segment length is
like this? Well, this segment length is
not necessarily the same as this length
not necessarily the same as this length
here, right? You might want to chop this
here, right? You might want to chop this
up. And if you do chop it up, then you
up. And if you do chop it up, then you
have to keep track of which M's this
have to keep track of which M's this
belongs to. Because this next batch,
belongs to. Because this next batch,
right, that's going to come in like
right, that's going to come in like
this. You have to keep track of the LSTM
state because ideally you'd like to
state because ideally you'd like to
preserve the LSTM state um across
preserve the LSTM state um across
boundaries. And it's also not
boundaries. And it's also not
necessarily the case that these are
necessarily the case that these are
going to be dense, right? some steps
going to be dense, right? some steps
might get more experience uh than
might get more experience uh than
others, right? So like this one here,
others, right? So like this one here,
maybe you're missing a little bit of
maybe you're missing a little bit of
experience. What do you do about that?
experience. What do you do about that?
There are lots and lots of
There are lots and lots of
considerations
here. Now complicating this further,
here. Now complicating this further,
uh there's also the puffer m
uh there's also the puffer m
case.
case.
So we have two different main cases,
So we have two different main cases,
right? In one case, we're basically
right? In one case, we're basically
going to get random data from like
going to get random data from like
random agents. We're not going to have
random agents. We're not going to have
that big of a batch size. Um, it's not
that big of a batch size. Um, it's not
going to be super fast so that we don't
going to be super fast so that we don't
have to worry about overhead. That's
have to worry about overhead. That's
like working with Atari or working with
like working with Atari or working with
any classic. I almost should start
any classic. I almost should start
calling them legacy with how fast Puffer
calling them legacy with how fast Puffer
is getting. But the other case is
is getting. But the other case is
working with Puffer amps. And in Puffer
working with Puffer amps. And in Puffer
amps, you get lots of nice things for
amps, you get lots of nice things for
free. So in buffer
free. So in buffer
ends, I can basically guarantee you that
ends, I can basically guarantee you that
let's say that you have this buffer and
let's say that you have this buffer and
we just like look at this block and this
we just like look at this block and this
block here. You're going to get a block
block here. You're going to get a block
of observations here and then you're
of observations here and then you're
going to get a block of observations
going to get a block of observations
here and then
here and then
here and then here. And these are going
here and then here. And these are going
to fill up, you know, these are going to
to fill up, you know, these are going to
fill up
fill up
perfectly. And you're not going to have
perfectly. And you're not going to have
weird like agents indexed where batches
weird like agents indexed where batches
are split all across this buffer. And
are split all across this buffer. And
things are going to be relatively
things are going to be relatively
nice. You're going to have relatively
nice. You're going to have relatively
large batch sizes. Uh you're not going
large batch sizes. Uh you're not going
to have to deal with padding. You're not
to have to deal with padding. You're not
going to have to deal with weird
going to have to deal with weird
indexing. Things are just nice. But this
indexing. Things are just nice. But this
doesn't hold for the other case. So your
doesn't hold for the other case. So your
goal here is whatever I build, I want
goal here is whatever I build, I want
this thing to be very nicely indexed
this thing to be very nicely indexed
such that we get minimal possible copy
such that we get minimal possible copy
overhead and such. And then the jank
overhead and such. And then the jank
buffer, right? This is like what an
buffer, right? This is like what an
Atari buffer might look like where it's
Atari buffer might look like where it's
like stuff's all over the place, right?
like stuff's all over the place, right?
This one's allowed to be slower because
This one's allowed to be slower because
a small amount of overhead here. you're
a small amount of overhead here. you're
not going to notice it because the M's
not going to notice it because the M's
are slow anyways and they're going to be
are slow anyways and they're going to be
by definition. They're not really
by definition. They're not really
properly
vectorzed. So you need to optimize for
vectorzed. So you need to optimize for
both of these things. Now I'd also like
both of these things. Now I'd also like
to optimize this such that it is
to optimize this such that it is
relatively efficient to use an
relatively efficient to use an
experience buffer of some sort.
I should clarify that to use a
I should clarify that to use a
prioritized experience buffer of some
sort. This is also tricky.
sort. This is also tricky.
So, a prioritized experience
buffer basically by
definition is going to require you to uh
definition is going to require you to uh
to do additional copies.
It's because you don't know what the
It's because you don't know what the
priority is of each of these samples
priority is of each of these samples
until you have the whole sample
until you have the whole sample
collected.
Ah, there is an optimization I can make
Ah, there is an optimization I can make
here. If I know that I'm going to have
here. If I know that I'm going to have
to copy the data
anyways, I
can I can keep it. I can keep the full
can I can keep it. I can keep the full
segment, right? So, we'll just have this
segment, right? So, we'll just have this
be a full trash.
be a full trash.
or full
segment. Okay, I was worried about this
segment. Okay, I was worried about this
because technically you can have BPT
because technically you can have BPT
horizon lower than trajectory segment
horizon lower than trajectory segment
and then you'd have to like turn this
and then you'd have to like turn this
into 3D and then you know there are
into 3D and then you know there are
other things that make you turn it into
other things that make you turn it into
4D. So we can actually keep this
4D. So we can actually keep this
relatively
relatively
simple. That actually simplifies a lot.
simple. That actually simplifies a lot.
We do incur an extra
copy, but there's no way around that
copy, but there's no way around that
extra copy, I'm pretty
extra copy, I'm pretty
sure.
Okay, now the experience buffer is going
Okay, now the experience buffer is going
to have to work a little
differently. Is there any reason
differently. Is there any reason
to have a lower BPT horizon versus a
to have a lower BPT horizon versus a
higher
one. I used to think it was O², but it's
one. I used to think it was O², but it's
not. It's O
N. Does anybody know? Would there ever
N. Does anybody know? Would there ever
be a reason that we need BPT horizon
be a reason that we need BPT horizon
like 16 instead of
128? Like I'm thinking here, right?
And it seems very unlikely to
And it seems very unlikely to
me that we're really going to ever want
me that we're really going to ever want
to scale to giant giant batch
to scale to giant giant batch
sizes because of hardware
sizes because of hardware
efficiency. Well, that's not true. To
efficiency. Well, that's not true. To
giant unroll
giant unroll
lengths. I don't know. At least for
lengths. I don't know. At least for
Impulse, a higher BPT always seems to do
Impulse, a higher BPT always seems to do
better. Maybe if your episodes are super
better. Maybe if your episodes are super
short.
short.
Uh, in this case, if the episodes were
Uh, in this case, if the episodes were
super short, then it would just end up
super short, then it would just end up
packing multiple episodes into a single
packing multiple episodes into a single
buffer length, and that's not really a
problem. The only thing that happens is
problem. The only thing that happens is
that uh you don't get to reset your LSTM
that uh you don't get to reset your LSTM
state between episodes, and that really
state between episodes, and that really
doesn't
matter.
Also, welcome
cat. I'm back for the next three weeks
cat. I'm back for the next three weeks
or whatever. So, I will be around to
or whatever. So, I will be around to
look at any code as is need as is
needed. this week. I think most of this
needed. this week. I think most of this
week I'm going to spend really trying to
week I'm going to spend really trying to
clean up our trainer so that we can have
clean up our trainer so that we can have
basically so that we get close to
basically so that we get close to
something in the releasable state.
something in the releasable state.
Um yeah, we're going to have like a nice
Um yeah, we're going to have like a nice
stable dev branch so we can like finish
stable dev branch so we can like finish
up all the experiments and start
up all the experiments and start
thinking about a release. A lot of work
thinking about a release. A lot of work
to do for
that. I'm trying to think if this is
that. I'm trying to think if this is
ever going to bite me.
ever going to bite me.
I think the only time you end up with
I think the only time you end up with
shorter seg like shorter BPT horizon is
shorter seg like shorter BPT horizon is
when you're collecting shorter segments.
when you're collecting shorter segments.
So I don't think this ever really bites
you. Design for prioritize. Yeah. So,
you. Design for prioritize. Yeah. So,
we're redoing the experience buffer, and
we're redoing the experience buffer, and
the goal is to co-design the experience
the goal is to co-design the experience
buffer with prioritized experience
buffer with prioritized experience
replay so that uh we end up with a
replay so that uh we end up with a
simpler, faster, lower memo, like a
simpler, faster, lower memo, like a
lower mem copy overhead
lower mem copy overhead
buffer. Basically, we're trying to build
buffer. Basically, we're trying to build
a god buffer like everybody is everybody
a god buffer like everybody is everybody
always has built buffers. We're trying
always has built buffers. We're trying
to build the best
buffer. Any ideas on Oh, giant paragraph
buffer. Any ideas on Oh, giant paragraph
from Spencer there. Any ideas on speed
from Spencer there. Any ideas on speed
improvement for network design
improvement for network design
below.
Okay. Wait. When I have features encoded
Okay. Wait. When I have features encoded
like a channel versus flat, I go from
like a channel versus flat, I go from
1.1 mil to 550K. Currently with point
1.1 mil to 550K. Currently with point
and quarter 550K with flat features
1.1. Let me see what you did.
Let's help you out
Let's help you out
here and get Tokyo Puff
going. We get a 1.1
Miller. Uh, hang on. Where's this thing?
Okay.
So, so this is the flat one presumably.
So, so this is the flat one presumably.
Yeah, this is flat.
Let me think about that for a second for
Let me think about that for a second for
you.
resume flat. Well, if you think about
resume flat. Well, if you think about
it, flat's a little tricky. Um, so the
it, flat's a little tricky. Um, so the
flat encoder, it has
flat encoder, it has
no, it has to learn basically the same
no, it has to learn basically the same
representation for each point.
representation for each point.
You're basically you're asking it to
You're basically you're asking it to
learn the same set of weights for each
learn the same set of weights for each
point
point
almost, which is like you may as well
almost, which is like you may as well
just have it learn those weights once.
just have it learn those weights once.
The tricky thing though, right, is it's
The tricky thing though, right, is it's
only
only
doing it's like four to hidden state, I
doing it's like four to hidden state, I
guess.
guess.
Wait, hang on. Shouldn't that be
Wait, hang on. Shouldn't that be
strictly
better? What's the projection dimension
better? What's the projection dimension
here? So you
here? So you
do ego encoder
Is this the road
encoder? So this is linear relu
linear but in this
linear but in this
case this
case this
is 5x
64. This goes to CNN channel which is
64. This goes to CNN channel which is
64.
And then this one
And then this one
is Yeah. Okay. I see. How many weights
is Yeah. Okay. I see. How many weights
is that? Let me see. So, it's it's five
is that? Let me see. So, it's it's five
by 5*
64. Let's figure this out.
So this should be 5 * 64. This is this
So this should be 5 * 64. This is this
one
one
here. Uh and then that goes to 64 I
here. Uh and then that goes to 64 I
guess. Okay. And then the other one
is 5 * 64.
Well, it's five is the input and then 64
Well, it's five is the input and then 64
is the
is the
output. 64. It's the same number of
output. 64. It's the same number of
weights on the
input. The input should be the same
input. The input should be the same
speed, shouldn't
speed, shouldn't
it? Oh, I see the issue. Uh, one of
it? Oh, I see the issue. Uh, one of
these networks is way bigger than the
these networks is way bigger than the
other.
other.
Spencer is or not networks. One of these
Spencer is or not networks. One of these
one of these is using way more flops
one of these is using way more flops
than the
than the
other.
So I draw this for
So I draw this for
you. So you have this is the flat one,
you. So you have this is the flat one,
right? And you do like I guess to this
right? And you do like I guess to this
size here. So you do like
size here. So you do like
this. Okay. And then then this other one
this. Okay. And then then this other one
you have
like or no like like this and then it
like or no like like this and then it
does like
does like
this.
Bam. So I'm pretty sure these are the
Bam. So I'm pretty sure these are the
same sizes though, right? So the first
same sizes though, right? So the first
layer it's
layer it's
either smaller uh yeah but they're
either smaller uh yeah but they're
processed separately in parallel
processed separately in parallel
right it's separately but in
right it's separately but in
parallel and I think it's the same size
parallel and I think it's the same size
because this is this is like 5 by 64
because this is this is like 5 by 64
right and then this goes to 64 and then
right and then this goes to 64 and then
this is five and it goes to 64 but you
this is five and it goes to 64 but you
do it 64 times
Right. Yeah. So, this should be the
same. Unless I'm wrong about Hang on.
same. Unless I'm wrong about Hang on.
Does that is that how matrix multipliers
Does that is that how matrix multipliers
work? Let me
think. That might not be how matrix
think. That might not be how matrix
multiplies work. Let me think here.
multiplies work. Let me think here.
because it's technically
it's what's the squared dimension on a
it's what's the squared dimension on a
matrix
multiply cuz it's cubic
multiply cuz it's cubic
right but not for but that's only for
right but not for but that's only for
square matrices
square matrices
uh let me think so it's
input each input gets applied to
Okay, I'm going to look this up cuz the
Okay, I'm going to look this up cuz the
brain not working right now for
this
this
matrix like
matrix like
ational lexity with or actually no I'm
ational lexity with or actually no I'm
stupid. It's it's matrix vector right
stupid. It's it's matrix vector right
that's why I'm getting
confused. So uh vector
matrixity
matrixity
vector size
vector size
matrix
size what is the weight
size? It should
size? It should
be This is going to be m squ n, isn't
it? I just want to make sure I don't get
it? I just want to make sure I don't get
this
wrong. Ah, see, I'm stupid. There you
wrong. Ah, see, I'm stupid. There you
go. What's up, man? There's no squared
go. What's up, man? There's no squared
term. Why would there be?
Yeah. So if there's no squared term then
Yeah. So if there's no squared term then
then I'm right then these two should be
then I'm right then these two should be
the same number of flops. But the thing
the same number of flops. But the thing
that you missed here is the second
that you missed here is the second
layer. The second layer is a different
layer. The second layer is a different
number of flops because in the first
number of flops because in the first
case you're doing 64 by 64 and then the
case you're doing 64 by 64 and then the
second case you're doing 64 times right
second case you're doing 64 times right
uh 6 or what however many road points
uh 6 or what however many road points
there are assuming 64 road points 64
there are assuming 64 road points 64
time 64 *
time 64 *
64. So that's what you're missing.
So what I would suggest that you do for
So what I would suggest that you do for
now and this will take you all five
now and this will take you all five
minutes is um put the
max put the max like after this instead
max put the max like after this instead
and then
and then
rettime. put the max after one
layer. So you did right here there's a
layer. So you did right here there's a
discrepancy. Yeah. See you like these
discrepancy. Yeah. See you like these
two are these two networks are not
two are these two networks are not
analogous because what you did here
analogous because what you did here
right is if you look at it this first
right is if you look at it this first
layer combines all the information of
layer combines all the information of
all the points
all the points
right this first layer combines all the
right this first layer combines all the
information of all the points but here
information of all the points but here
you gave it an extra layer. So, this is
you gave it an extra layer. So, this is
actually this should be a stronger
actually this should be a stronger
network than this one here, but it's
network than this one here, but it's
going to be
going to be
slower because there's a huge additional
slower because there's a huge additional
uh ask of flops right here. This is a
uh ask of flops right here. This is a
lot of extra flops. So, I would do the
lot of extra flops. So, I would do the
max here. And if this makes the network
max here. And if this makes the network
way weaker, then we go back to something
way weaker, then we go back to something
like this. But at the very least, like
like this. But at the very least, like
this is not apples to apples. So the
this is not apples to apples. So the
apples to apples here, you would have to
apples to apples here, you would have to
make this you'd have to make this
make this you'd have to make this
dimension here way bigger or
something like this would have to be a
something like this would have to be a
bigger linear layer so that there's it
bigger linear layer so that there's it
doesn't have to compress everything on
doesn't have to compress everything on
the first
the first
layer. And that's still not quite
layer. And that's still not quite
perfect, but that's as close as you'd be
perfect, but that's as close as you'd be
able to get it.
50% slower but acting
50% slower but acting
max 50% slower but adding max there
max 50% slower but adding max there
reduces
operations. Wait 50 I don't understand
operations. Wait 50 I don't understand
the comment. So max here is slower than
the comment. So max here is slower than
max here because you're doing 64 times
max here because you're doing 64 times
the number of forward passes on this
the number of forward passes on this
layer.
Right. Difference between the two
Right. Difference between the two
versions right now is 50% slower. I'd
versions right now is 50% slower. I'd
have to look at the rest of the network
have to look at the rest of the network
to see if I think 50% is reasonable. I
to see if I think 50% is reasonable. I
mean, first principles, it could it
mean, first principles, it could it
could very well be because this is a
could very well be because this is a
tiny network and you're basically doing
tiny network and you're basically doing
64 times the number of forward passes in
64 times the number of forward passes in
this right now.
And this is a big increase in
complexity. Just like run it, right?
complexity. Just like run it, right?
It'll take you five minutes to just flip
It'll take you five minutes to just flip
it around and run
it. And if it's still slower, then come
it. And if it's still slower, then come
back and we'll chat about it and we'll
back and we'll chat about it and we'll
figure out what we can
figure out what we can
do. But that is kind of the standard
do. But that is kind of the standard
entity encoder style
network. Okay.
network. Okay.
Back to back to experience
buffers. How's max there changed things?
buffers. How's max there changed things?
Sorry, still lost. Okay, let me see if I
Sorry, still lost. Okay, let me see if I
can explain this. So, what you did,
can explain this. So, what you did,
right, in the first
right, in the first
one, yeah, I'll I'll explain this real
one, yeah, I'll I'll explain this real
quick. So, in the first one, right, you
quick. So, in the first one, right, you
take your encoder, you have all these
take your encoder, you have all these
all these big observations and you
all these big observations and you
encode them to 60, where is it? to 64 by
encode them to 60, where is it? to 64 by
64. So or 64. So this is
64. So or 64. So this is
64. All right. And then you do 64 x
64. All right. And then you do 64 x
64. Right? So if you have one sample,
64. Right? So if you have one sample,
this is now a 64 vector to a 64 64 uh
this is now a 64 vector to a 64 64 uh
vector matrix.
vector matrix.
Right? So it looks like this.
Okay. But what happens on this other one
Okay. But what happens on this other one
where you do all these little encodings?
where you do all these little encodings?
This is a batch,
This is a batch,
right? So you do like all these little
right? So you do like all these little
encodings. What you end up with here is
encodings. What you end up with here is
not a 64dimensional vector. Uh this is
not a 64dimensional vector. Uh this is
excluding the batch dimension. So we're
excluding the batch dimension. So we're
not even talking about the batch
not even talking about the batch
dimension. This is now 64
dimension. This is now 64
by
by
64. Okay. And then you're doing a 64x 64
multiply. So this is
multiply. So this is
n^2. This is n
cubed because you still have all the
cubed because you still have all the
road points, right? You haven't maxed
road points, right? You haven't maxed
over them yet.
doing the max on n2. Well, you do the
doing the max on n2. Well, you do the
max after this, right? And then you you
max after this, right? And then you you
do max
do max
here. So the output of this, hang on. So
here. So the output of this, hang on. So
the output of this here, this is going
the output of this here, this is going
to give you
to give you
64 or hang on. This
64 or hang on. This
is this is
is this is
64. The output of this is 64x
64. And then you do max here.
64. And then you do max here.
right to get
64. So this operation here is 64 times
64. So this operation here is 64 times
more expensive than the operation
more expensive than the operation
here. So placing it earlier achieves a
here. So placing it earlier achieves a
similar effect but less multiplies. So
similar effect but less multiplies. So
placing it earlier
placing it earlier
essentially the networks become the same
essentially the networks become the same
after the first
after the first
layer. So you do your combine operation
layer. So you do your combine operation
in the first layer because this does the
in the first layer because this does the
combine in the first layer. Right? This
combine in the first layer. Right? This
top network combines all the information
top network combines all the information
from all these points in the first
from all these points in the first
layer. Once you go here, you can no
layer. Once you go here, you can no
longer distinguish between points. In
longer distinguish between points. In
your network here, you can distinguish
your network here, you can distinguish
between points until you do this max
between points until you do this max
over here.
I'm not saying that this is necessarily
I'm not saying that this is necessarily
even a bad architecture. It might be
even a bad architecture. It might be
that this this architecture could end up
that this this architecture could end up
being essential even and this is what
being essential even and this is what
you would do if you wanted to scale your
you would do if you wanted to scale your
network up a little bit without
network up a little bit without
increasing parameter count or whatever.
increasing parameter count or whatever.
But it's 64 times as
But it's 64 times as
expensive this layer is. Now this layer
expensive this layer is. Now this layer
is only a small part of your overall
is only a small part of your overall
network. So it works out to be about 50%
network. So it works out to be about 50%
slower. And you also kind of get economy
slower. And you also kind of get economy
as a scale because GPU go burr. But um
as a scale because GPU go burr. But um
that's what you're doing right
that's what you're doing right
here. You
see if you max here
see if you max here
instead if you max right
here. I don't know why I wrote name
here. I don't know why I wrote name
instead of max.
instead of max.
Then you get a 64dimensional
Then you get a 64dimensional
vector. And now this is exactly the same
vector. And now this is exactly the same
as up here.
Right? That's the
key. Do you see how I think about this
key. Do you see how I think about this
now though? It's not even that your
now though? It's not even that your
network is bad. It's just that this is
network is bad. It's just that this is
going to be way more flops than the top
going to be way more flops than the top
one. Quick followup. GPU D group does
one. Quick followup. GPU D group does
tanh instead of RLU and add normalized
tanh instead of RLU and add normalized
layer between linear. Any reason for me
layer between linear. Any reason for me
to do
to do
that? I've seen that in robotic stuff
that? I've seen that in robotic stuff
before. There's something to tanh over
before. There's something to tanh over
relu because there's like this big
relu because there's like this big
control study where for some reason it
control study where for some reason it
was better for control problems with
was better for control problems with
continuous outputs.
Um, I have no idea why that would be
Um, I have no idea why that would be
better and it's kind of sucky because
better and it's kind of sucky because
like tanh is just a shitty activation.
like tanh is just a shitty activation.
This is what we used back in like 2015
This is what we used back in like 2015
or before 2015. We used tanh just cuz we
or before 2015. We used tanh just cuz we
didn't know better and then the nor the
didn't know better and then the nor the
layer norms can be very
layer norms can be very
slow. So they said that they were really
slow. So they said that they were really
needed uh in order to make their stuff
needed uh in order to make their stuff
work. Do I trust that it wasn't just bad
work. Do I trust that it wasn't just bad
hyperparameters? So, like this was
hyperparameters? So, like this was
band-aid patch for bad hyperparameters.
band-aid patch for bad hyperparameters.
No, it could very well have just been
No, it could very well have just been
like band-aid patch for bad hypers and
like band-aid patch for bad hypers and
whatever, which would be fine if it
whatever, which would be fine if it
weren't slow. And I think it is going to
weren't slow. And I think it is going to
be slow. You can try it, but if it's
be slow. You can try it, but if it's
slow, that'll be expected. Can it
slow, that'll be expected. Can it
shouldn't really be any slower. But um
yeah, I'm also I'm also going to be
yeah, I'm also I'm also going to be
interested to see if like there's a big
interested to see if like there's a big
difference in perf now with
Muan cuz Muan should help smooth some of
Muan cuz Muan should help smooth some of
the perks that we were getting
the perks that we were getting
before. Okay, are we good on this
before. Okay, are we good on this
though? Just ban that bot.
Sweep with Muan. Yes. Good. Okay. Solid.
Sweep with Muan. Yes. Good. Okay. Solid.
I'm So, my goal for this week, Spencer,
I'm So, my goal for this week, Spencer,
is um I'm going to try to get this
is um I'm going to try to get this
experience buffer and the training code
experience buffer and the training code
into a spot where it's not insane. So,
into a spot where it's not insane. So,
like you'll actually be able to use to
like you'll actually be able to use to
do stable work on it. That's the goal.
do stable work on it. That's the goal.
Mind taking a quick look at
Mind taking a quick look at
multi-iset multi-discreet actions.
Oh, yeah. Also, I think I think you both
Oh, yeah. Also, I think I think you both
probably saw, but I posted in the
probably saw, but I posted in the
uh the
uh the
Discord one thing.
Oops. Sorry, I just had answer
Oops. Sorry, I just had answer
something. And I think you both saw, but
something. And I think you both saw, but
this is what I was looking at over the
weekend. This is the capacity right here
weekend. This is the capacity right here
for new
for new
machines. We're going to have two foot
machines. We're going to have two foot
like just storage racks in front of
like just storage racks in front of
these for desktops.
these for desktops.
40 ports and then uh there's going to be
40 ports and then uh there's going to be
a 42U rack sitting right here where we
a 42U rack sitting right here where we
can stick any server grade equipment uh
can stick any server grade equipment uh
if we get any we can stick right in
if we get any we can stick right in
front of
there and it'll all be on a UPS so we
there and it'll all be on a UPS so we
won't have screwy like power out drops
won't have screwy like power out drops
screwing us over anymore. We might still
screwing us over anymore. We might still
have internet because we cannot
have internet because we cannot
get a 10 gig line uh to the facility.
get a 10 gig line uh to the facility.
So, we're going to have just a one gig
So, we're going to have just a one gig
line to start with and then uh you know,
line to start with and then uh you know,
I might just get like extra Starlink
I might just get like extra Starlink
backups because that's not going to be
backups because that's not going to be
enough
enough
bandwidth. Uh it is it's a pretty huge
bandwidth. Uh it is it's a pretty huge
UPS. It's like it's a big
UPS. It's like it's a big
box. Yeah. So, we get this and then all
box. Yeah. So, we get this and then all
the space that is not being used by
the space that is not being used by
servers gets used by for a gym
servers gets used by for a gym
equipment. So, facility also gets to be
equipment. So, facility also gets to be
my
my
gym. It'll be fun. I'm having a good
gym. It'll be fun. I'm having a good
time going through and like figuring out
time going through and like figuring out
what to order for
it. And this will be ready in about 3
it. And this will be ready in about 3
weeks. Don't drop weights on the
weeks. Don't drop weights on the
servers. The entire floor is going to be
servers. The entire floor is going to be
3/4 inch rubber mat. We're getting the
3/4 inch rubber mat. We're getting the
mats installed in probably two to 3
mats installed in probably two to 3
weeks.
So like that's solid that's a solid
So like that's solid that's a solid
chunk of
rubber. Yeah. 3/4 inch. Yeah. So I went
rubber. Yeah. 3/4 inch. Yeah. So I went
I went extra heavy on it just to be
I went extra heavy on it just to be
absolutely sure that we will be good
there. I might even have to do something
there. I might even have to do something
thicker for the uh deadlift area. We'll
thicker for the uh deadlift area. We'll
but we'll figure that out when we get
but we'll figure that out when we get
there.
there.
Um, I'm not really deadlifting any
Um, I'm not really deadlifting any
reasonable amount of weight at the
reasonable amount of weight at the
moment. So, no problem for a
moment. So, no problem for a
bit. I'm just trying to get my body
bit. I'm just trying to get my body
weight
back. But yeah, that's what I've been
back. But yeah, that's what I've been
doing. Let's take a look at your
sweep. Okay, it's a cool
sweep. Okay, it's a cool
sweep. This is impulse force. Looks like
sweep. This is impulse force. Looks like
it.
very high
gamma. That's somewhat expected. We
gamma. That's somewhat expected. We
might even want increase maximum gamma
might even want increase maximum gamma
for you because it's this
high
high
lambda. Yeah. Lowish
lambda. Oh jeez, you really cranked this
lambda. Oh jeez, you really cranked this
thing. Does this even work? You have to
thing. Does this even work? You have to
have a really large batch size for this
have a really large batch size for this
to even H. That's
interesting. Yeah, I know. I I did the
interesting. Yeah, I know. I I did the
both I did both of those dashboards
both I did both of those dashboards
right in my setup. I'm looking at this
right in my setup. I'm looking at this
first.
This actually Yeah, this appears to keep
This actually Yeah, this appears to keep
doing
better.
better.
M. Very small number of M's though. Is
M. Very small number of M's though. Is
this the uh You probably have several
this the uh You probably have several
cores here, right? So, this is probably
cores here, right? So, this is probably
like M's per core and you probably have
like M's per core and you probably have
like several cores, right?
Mini batch size
8192 cores. Yeah. So that's a good
8192 cores. Yeah. So that's a good
number of environments then. That's what
number of environments then. That's what
is that uh 20 48 ms for this. That's
is that uh 20 48 ms for this. That's
reasonable.
Hey, you can share your absolutely
Hey, you can share your absolutely
god-awful learning curves and uh win a
god-awful learning curves and uh win a
gift box.
So, we just submit them our entire
So, we just submit them our entire
sweeps on
everything. It's like, yeah, this is
everything. It's like, yeah, this is
just daily life for us. Thank
you. See, stuff like this kind of
you. See, stuff like this kind of
bothers me.
Stuff like this kind of bothers me. It
Stuff like this kind of bothers me. It
really shouldn't be this sensitive to a
really shouldn't be this sensitive to a
little bit of
little bit of
entropy,
right? So, it doesn't even bother me
right? So, it doesn't even bother me
that it
that it
does like I don't even like expect to do
does like I don't even like expect to do
better with some entropy, but like it
better with some entropy, but like it
shouldn't crash learning with entropy.
shouldn't crash learning with entropy.
Like you should expect this to kind of
Like you should expect this to kind of
be flat right here is what I would
be flat right here is what I would
expect. I would expect some runs with
expect. I would expect some runs with
like decent PF at slightly higher
like decent PF at slightly higher
entropy. It's very sensitive to that
entropy. It's very sensitive to that
which is
concerning. This is like better, right?
concerning. This is like better, right?
This is kind of like a flat region here.
This is kind of like a flat region here.
You can see kind of like you want to
You can see kind of like you want to
increase your value function
increase your value function
coefficient, but it's kind of stable
coefficient, but it's kind of stable
over here.
Oh, and then we're still at like baby
Oh, and then we're still at like baby
levels of time steps
levels of time steps
here. 600 million. Get out of here. I
here. 600 million. Get out of here. I
did a five billion step neural MMO sweep
did a five billion step neural MMO sweep
over the weekend. Each run was 5 billion
steps with bigger networks as well.
multi-discreet without torch compile.
multi-discreet without torch compile.
So, we'll have to fix that. I thought I
So, we'll have to fix that. I thought I
merged that PR from that guy. Did that
merged that PR from that guy. Did that
not
help? This is still going up though. You
help? This is still going up though. You
can just keep this
going. 60 to 80%. Okay. So, that's weird
going. 60 to 80%. Okay. So, that's weird
because I don't get anything out of
because I don't get anything out of
torch compile in my in my setups. So,
torch compile in my in my setups. So,
something screwy is going on
there. I don't have continuous actions.
there. I don't have continuous actions.
I have discrete actions and I don't get
I have discrete actions and I don't get
much from compile at
much from compile at
all. It's often slower. In fact,
Looks like a very reasonable sweep. Was
Looks like a very reasonable sweep. Was
there anything in particular that uh you
there anything in particular that uh you
were wondering about? Because this looks
were wondering about? Because this looks
I mean what you want to do is you want
I mean what you want to do is you want
to go over to
to go over to
uh to here and then what does it
win? Little concerning how shallow. Some
win? Little concerning how shallow. Some
of these curves
are not on dev. Okay. So, I probably
are not on dev. Okay. So, I probably
just probably just something broken
just probably just something broken
then. We'll have to sit down and find
then. We'll have to sit down and find
that at some point. Like I said, I'm
that at some point. Like I said, I'm
around all week. I have or next couple
around all week. I have or next couple
weeks I've got pretty much nothing
weeks I've got pretty much nothing
planned than just solid dev. And then I
planned than just solid dev. And then I
go to the warehouse and then I have
go to the warehouse and then I have
absolutely nothing planned but just
absolutely nothing planned but just
solid dev when I get
there. Hang on a second.
Oops. Okay. Had to answer a
Oops. Okay. Had to answer a
thing. Mostly I was curious why ENT was
thing. Mostly I was curious why ENT was
so low and if I should do a new sweep
so low and if I should do a new sweep
with increased back prop horizon. Best
with increased back prop horizon. Best
runs mini batch
runs mini batch
seemed low to me. Yeah, that'll happen.
seemed low to me. Yeah, that'll happen.
You just use I think that the best thing
You just use I think that the best thing
you can do captain is you just you use
you can do captain is you just you use
the parameters that the sweep tells you
the parameters that the sweep tells you
and you keep improving infrastructure. I
and you keep improving infrastructure. I
have noticed as you improve
have noticed as you improve
infrastructure and things that are fast
infrastructure and things that are fast
uh that should be fast become fast the
uh that should be fast become fast the
hyperparameters get better because you
hyperparameters get better because you
have to remember the access that it's
have to remember the access that it's
controlling is wall clock. So like some
controlling is wall clock. So like some
part being slow that's not supposed to
part being slow that's not supposed to
be slow can actually change optimal
be slow can actually change optimal
hypers and make things unintuitive.
I am a little suspicious how shallow
I am a little suspicious how shallow
these graphs are. Oh yeah. What's your
these graphs are. Oh yeah. What's your
SPS
at? 100 to
at? 100 to
250k. My
250k. My
guy. O, that is rough.
I'm pretty sure I'm running like I'm
I'm pretty sure I'm running like I'm
running three million parameter necks.
running three million parameter necks.
Um twice as fast as your fastest run
here. You must be hard m bottlenecked,
here. You must be hard m bottlenecked,
right? Or you are you net bottleneck or
right? Or you are you net bottleneck or
m bottleneck? What's the dashboard show
m bottleneck? What's the dashboard show
you?
320. That's still
low. So, you've
low. So, you've
got 500 seconds M
got 500 seconds M
time. You have a crazy amount of copy
time. You have a crazy amount of copy
bandwidth right here.
Your M actually is not big. Like this is
Your M actually is not big. Like this is
not Yeah, your M time. You're not even
not Yeah, your M time. You're not even
bottlenecked by your N here. Look,
bottlenecked by your N here. Look,
you're getting screwed over by
you're getting screwed over by
copies. I'm going to improve that a
copies. I'm going to improve that a
little bit for you, but you also need to
little bit for you, but you also need to
check the size of your uh your data
check the size of your uh your data
packets there. So, this is probably
packets there. So, this is probably
what's that 20% of your eval time spent
what's that 20% of your eval time spent
in that. And then let's see, MISK is
in that. And then let's see, MISK is
good. 63. Yep. That's still not crazy
good. 63. Yep. That's still not crazy
though. Oh, you have like zero learn
time. Wait,
what? Nah, this is there's I don't know
what? Nah, this is there's I don't know
what the heck the reason the solution is
what the heck the reason the solution is
here, but something's just trolling
here, but something's just trolling
here. This is like hard trolling.
train
train
and 63
and 63
eval. So more than 80% time spent in
eval. So more than 80% time spent in
Eval is hard
Eval is hard
trolling. Yeah, that is hard trolling.
trolling. Yeah, that is hard trolling.
Um, my guess
Um, my guess
is I just pick a random one of
these. There's something wrong if
these. There's something wrong if
multi-discrete actions are that slow.
multi-discrete actions are that slow.
I mean that's like a trivial
operation. Let me check something real
quick. 4 m batch
size. Wait 16
amps. you have this quad
buffered. So that's only
buffered. So that's only
512. You only have 512 batch size in the
512. You only have 512 batch size in the
forward
forward
pass. You're definitely not going to
pass. You're definitely not going to
load the GPU enough with
load the GPU enough with
that. I'm guessing that you have it quad
that. I'm guessing that you have it quad
buffered because when you double buffer
buffered because when you double buffer
it, the uh the M percentage gets to be
it, the uh the M percentage gets to be
quite large or something.
300k is is still not considered
300k is is still not considered
reasonable, captain. Like we we really
reasonable, captain. Like we we really
should have this thing at 500 plus. If I
should have this thing at 500 plus. If I
can have neural MMO at 500 plus, we
can have neural MMO at 500 plus, we
should be able to have this 500
should be able to have this 500
plus. Like here, I'll show you
my because I have a bigger net as well.
I guess this thing crashed at some
I guess this thing crashed at some
point, but we got 24 runs in
here. Couple of these actually did
okay.
Couple runs did
okay. What was my
okay. What was my
SPS? So, pretty much all these are at
500k. Eval doesn't know up. I think I
500k. Eval doesn't know up. I think I
just get 500k. Well, then the re then
just get 500k. Well, then the re then
the question is going to be why? Because
the question is going to be why? Because
um you have 16 cores.
um you have 16 cores.
With Puffer, you should be able to get
With Puffer, you should be able to get
pretty damn close to linear speed up.
pretty damn close to linear speed up.
So, unless the MV is running at like
So, unless the MV is running at like
30k, you should be able to get faster
30k, you should be able to get faster
than
that, right?
And this is again this is with a big
And this is again this is with a big
network as well like I think this is
with hidden
size. Yes. So, this is with a 512 512 R
LSTM. Ford pass is the big one. MS speed
LSTM. Ford pass is the big one. MS speed
was not your biggest uh your biggest
was not your biggest uh your biggest
line item at all. I checked your PF,
line item at all. I checked your PF,
right? Your line uh your M speed really
right? Your line uh your M speed really
wasn't that big of a deal.
It was a pretty small
It was a pretty small
fraction. Your copy time was bigger than
fraction. Your copy time was bigger than
your M time
your M time
even. So, I'll help you play with
even. So, I'll help you play with
that. But, uh, definitely we need to do
that. But, uh, definitely we need to do
some stuff on that.
some stuff on that.
I think what we're going to probably do
I think what we're going to probably do
is I'll do
is I'll do
um if people aren't free in the
um if people aren't free in the
evenings, I'll do some polls, you know,
evenings, I'll do some polls, you know,
during the day. But I'll try to make
during the day. But I'll try to make
myself more available than I've been
myself more available than I've been
doing in the evenings, you know, to just
doing in the evenings, you know, to just
to chat uh to chat through
to chat uh to chat through
stuff, you know, over dinner and then
stuff, you know, over dinner and then
after
after
dinner. Continuous actions though,
dinner. Continuous actions though,
that's more accurate to when torch
that's more accurate to when torch
compile works with multi-discreet
compile works with multi-discreet
actions.
The thing is that we should just be able
The thing is that we should just be able
to make multi-discreet fast. There's no
to make multi-discreet fast. There's no
reason that should be slow. So I'm going
reason that should be slow. So I'm going
I'm believing you that it is slow,
I'm believing you that it is slow,
right? But there's no reason that it
right? But there's no reason that it
should be. So we should just be able to
should be. So we should just be able to
make that
fast.
300k.
Yeah. Anything else I can do right now
Yeah. Anything else I can do right now
for you, Captain? Like that's a
for you, Captain? Like that's a
reasonable sweep. I would just yep keep
reasonable sweep. I would just yep keep
running that stuff.
Um, how like what else do you want to
Um, how like what else do you want to
do? I we can save this for like a later
do? I we can save this for like a later
conversation, but like start thinking
conversation, but like start thinking
about like what you want to do uh with
about like what you want to do uh with
impulse wars before you release it,
impulse wars before you release it,
right? Is the goal is there more content
right? Is the goal is there more content
that has to go in? Is there UI stuff
that has to go in? Is there UI stuff
that needs to be cleaned up? Is there
that needs to be cleaned up? Is there
perf stuff to do? Or do you at this
perf stuff to do? Or do you at this
point just want a really strong
point just want a really strong
baseline?
That's what I would encourage you to do.
That's what I would encourage you to do.
Like think of how you want to think of
Like think of how you want to think of
like what you want to do for the release
like what you want to do for the release
of this thing.
these small things for 3D graphics and
these small things for 3D graphics and
then baseline. Gotcha. So, I mean, I can
then baseline. Gotcha. So, I mean, I can
show you some of the stuff I've done for
show you some of the stuff I've done for
the neural MMO baseline.
the neural MMO baseline.
Um, are you on the latest code with Muan
Um, are you on the latest code with Muan
and everything at this point or no?
Okay, good. So, if you're on the latest
Okay, good. So, if you're on the latest
code using Muon, then um you should be
code using Muon, then um you should be
able to get some stable runs and the
able to get some stable runs and the
only thing that's really hamstringing
only thing that's really hamstringing
you should be the M speed because like
you should be the M speed because like
you know billions, not mill not hundreds
you know billions, not mill not hundreds
of millions. Old bindings bindings
of millions. Old bindings bindings
aren't really going to give you any
aren't really going to give you any
speed. Bindings are just going to
speed. Bindings are just going to
simplify things.
simplify things.
I could be wrong, but I haven't seen
I could be wrong, but I haven't seen
major speed ups with
bindings. Neural MMO got like 5 10%
bindings. Neural MMO got like 5 10%
faster and then like pawn got a little
faster and then like pawn got a little
bit slower. I think neural MMO is
bit slower. I think neural MMO is
probably the better comparison. So you
probably the better comparison. So you
might get a little bit of
speed. I think you get link time
speed. I think you get link time
optimization now as well. You don't
optimization now as well. You don't
screw up link time optimization anymore.
my next sweep. Yeah, you can just bump
my next sweep. Yeah, you can just bump
it up. Um, you can bump that up. It's
it up. Um, you can bump that up. It's
fine. But like honestly, I just I did a
fine. But like honestly, I just I did a
fixed five bill sweep lately for neural
fixed five bill sweep lately for neural
MMO that did okay. And then I also do I
MMO that did okay. And then I also do I
start doing like some individual
start doing like some individual
experiments. So like if do you have a
experiments. So like if do you have a
256 hidden dim already or do you have
256 hidden dim already or do you have
128? Because 128's probably too small
128? Because 128's probably too small
for that problem.
for that problem.
I got a little bit of fur out of doing
I got a little bit of fur out of doing
even
even
512 56. Okay, good. I bumped neural MMO
512 56. Okay, good. I bumped neural MMO
up to 512 and the network really wasn't
up to 512 and the network really wasn't
that much slower and I got a little bit
that much slower and I got a little bit
of improvement out of that.
of improvement out of that.
Um, you got to make sure that you're not
Um, you got to make sure that you're not
bottlenecking your network by a very
bottlenecking your network by a very
small like number of con filters or
small like number of con filters or
something. You got to make sure that
something. You got to make sure that
whatever is coming out of the CNN isn't
whatever is coming out of the CNN isn't
tiny. I got a little bit of pf out of
tiny. I got a little bit of pf out of
dealing with that.
Um, you might need to look at your
Um, you might need to look at your
seating. I got a little bit of perf out
seating. I got a little bit of perf out
of fixing my
of fixing my
seating. I sweep over hidden dim. I
seating. I sweep over hidden dim. I
don't know if you'd sweep over hidden
don't know if you'd sweep over hidden
dim because
dim because
like I technically have it in place that
like I technically have it in place that
you should be able to, but um there's
you should be able to, but um there's
still some screwy things with that,
still some screwy things with that,
especially around memory usage. So, I
especially around memory usage. So, I
probably wouldn't do that. You can kind
probably wouldn't do that. You can kind
of just run one sweep at higher hitting
of just run one sweep at higher hitting
them and one at
lower chatting with someone on the side
lower chatting with someone on the side
who's done similar experience buffer
who's done similar experience buffer
Heat. Heat.
512 hidden demise from CNN. What
512 hidden demise from CNN. What
else? You can look at some of my latest
else? You can look at some of my latest
um neural MMO three sweeps, I think. But
um neural MMO three sweeps, I think. But
I like those were the big ones. The only
I like those were the big ones. The only
other thing that I had to deal with was
other thing that I had to deal with was
uh seeding. seeding actually made a
uh seeding. seeding actually made a
decent difference. I just added that
decent difference. I just added that
into the new binding code. So, you're
into the new binding code. So, you're
not going to have access to that yet.
not going to have access to that yet.
Um, if you really want to do something
Um, if you really want to do something
short-term with that, then you probably
short-term with that, then you probably
just need to find like, and I didn't do
just need to find like, and I didn't do
this because it's slightly annoying, but
this because it's slightly annoying, but
if you get the clock time in nanoseconds
if you get the clock time in nanoseconds
instead of in seconds, you should be
instead of in seconds, you should be
able to guarantee unique seeds for all
able to guarantee unique seeds for all
the M's. Um, I think basically what
the M's. Um, I think basically what
happened before is that because I was
happened before is that because I was
using time in seconds, uh, all the maps
using time in seconds, uh, all the maps
were getting the same seed. You got to
were getting the same seed. You got to
be careful with that. That made a decent
difference. If you're using a random
difference. If you're using a random
seed for every process, you still need a
seed for every process, you still need a
random seed for every environment,
random seed for every environment,
right?
I guess it kind of
I guess it kind of
depends. Incremental. Okay. If you're
depends. Incremental. Okay. If you're
already seating, then that should be
already seating, then that should be
fine. Yeah. If you're already seating,
fine. Yeah. If you're already seating,
then that should be
fine. Well, this is a lot of the science
fine. Well, this is a lot of the science
side stuff, right? These are the harder
side stuff, right? These are the harder
M's. They're going to take a little bit
M's. They're going to take a little bit
of work to get to be uh you know nice
of work to get to be uh you know nice
and functional. And um I mean kind of in
and functional. And um I mean kind of in
my head though once we have if we can
my head though once we have if we can
like get clean
like get clean
uh clean RL like just clean simple RL
uh clean RL like just clean simple RL
working on your end on neural MMO 3
working on your end on neural MMO 3
maybe on a couple others maybe on mobile
maybe on a couple others maybe on mobile
with selfplay not just against shitty
with selfplay not just against shitty
scripted bots that's kind of like good
scripted bots that's kind of like good
for release and probably already better
for release and probably already better
than uh a lot of the problems in
than uh a lot of the problems in
industry like
industry like
Frankly, there are a lot of valuable
Frankly, there are a lot of valuable
problems in industry that are just flat
problems in industry that are just flat
out simpler than Impulse
out simpler than Impulse
Wars. They don't look like games, right?
Wars. They don't look like games, right?
But the actual learning dynamics are way
But the actual learning dynamics are way
simpler than your problem. So, I think
simpler than your problem. So, I think
that'll be uh that'll be the next
that'll be uh that'll be the next
milestone goal, right? Is we have solid
milestone goal, right? Is we have solid
simple RL working on your amp, working
simple RL working on your amp, working
on my amp. We ship, you know, we ship
on my amp. We ship, you know, we ship
your AMP, we ship my amp, we ship some
your AMP, we ship my amp, we ship some
of the stuff Spencer's been doing. And
of the stuff Spencer's been doing. And
uh that's the next release. Then we'll
uh that's the next release. Then we'll
see what we do from there. From there,
see what we do from there. From there,
it depends on how much stuff is left to
it depends on how much stuff is left to
fix versus how much I'd like say, "Okay,
fix versus how much I'd like say, "Okay,
let's scale this now. Get some industry
let's scale this now. Get some industry
contracts
contracts
going." Sucks because continuous actions
going." Sucks because continuous actions
are much faster. Well, we'll just fix
are much faster. Well, we'll just fix
that, Captain. Because they shouldn't
be. We'll just fix
be. We'll just fix
that. Uh, is this is this going to be
that. Uh, is this is this going to be
the Neoim tweet? Am I terminally online
the Neoim tweet? Am I terminally online
and know the post? Oh, no. This is
and know the post? Oh, no. This is
another one of his memes.
Yeah.
Yeah. The funny thing, Tyler, is like
Yeah. The funny thing, Tyler, is like
I'm way happier and way more productive
I'm way happier and way more productive
when I just turn off X and I just stop
when I just turn off X and I just stop
interacting with shitty programmers with
interacting with shitty programmers with
absolutely terrible tech
absolutely terrible tech
takes. That's like literally that's all
takes. That's like literally that's all
there is to it. Like they post all this
there is to it. Like they post all this
progress, but it's all just it's all
progress, but it's all just it's all
smoke and mirrors. Like of course they
smoke and mirrors. Like of course they
haven't built anything because they
haven't built anything because they
don't know how to build anything. It
don't know how to build anything. It
it's all lies.
it's all lies.
Like bothers the hell out of me. But you
Like bothers the hell out of me. But you
can just turn it off and you're not
can just turn it off and you're not
missing
missing
anything. Impulse word better not be
anything. Impulse word better not be
100% vibe coded. Captain
Yeah, exactly. I'd
Yeah, exactly. I'd
know cuz I'd have criticisms of your
know cuz I'd have criticisms of your
code, not just the uh the Box 2D.
code, not just the uh the Box 2D.
I still think, by the way, that because
I still think, by the way, that because
of the way box 2D is designed, there's
of the way box 2D is designed, there's
probably twice as much code as there
probably twice as much code as there
needs to be in uh in Impulse Wars.
needs to be in uh in Impulse Wars.
There's like a massive amount of boiler
There's like a massive amount of boiler
plate there with the way that physics
plate there with the way that physics
engine is set
engine is set
up. So, I still think that's the
case. I think the box 2D internals are
case. I think the box 2D internals are
very good. Um, no. Aaron seems to know
very good. Um, no. Aaron seems to know
what he's doing on the internals, but
what he's doing on the internals, but
uh, definitely the interface of it is a
uh, definitely the interface of it is a
little
little
bit it's a little sus. It's like a
bit it's a little sus. It's like a
suspiciously highle interface for a
suspiciously highle interface for a
low-level
library. All
right. I
right. I
mean, I don't know.
mean, I don't know.
I know I get very pedantic over like um
I know I get very pedantic over like um
code length and code quality stuff, but
code length and code quality stuff, but
it really makes a difference.
it really makes a difference.
Like it it's kind of like the recurring
Like it it's kind of like the recurring
cost of your library is the number of
cost of your library is the number of
lines of code or like I guess the
lines of code or like I guess the
complexity of your code is the recurring
complexity of your code is the recurring
cost of like maintenance and changing
cost of like maintenance and changing
stuff and what it's like if you can
stuff and what it's like if you can
solve a problem that would be like that.
solve a problem that would be like that.
The fang people would take a million
The fang people would take a million
lines of code and you solve it in 50,000
lines of code and you solve it in 50,000
lines. Like you don't have the same
lines. Like you don't have the same
thing, right? Yours is infinitely more
valuable is you can actually do stuff
valuable is you can actually do stuff
with
it. Plus, you got to have some
it. Plus, you got to have some
selfrespect. Okay. Does anybody have any
selfrespect. Okay. Does anybody have any
other things for me to review right now
other things for me to review right now
or can I get back to doing the
or can I get back to doing the
experience
buffer makes sense to only keep pure RL
buffer makes sense to only keep pure RL
eel training and move other stuff to
eel training and move other stuff to
other module. I was thinking about doing
other module. I was thinking about doing
the opposite. I was thinking about
the opposite. I was thinking about
combining the demo file with clean puff
combining the demo file with clean puff
RL and then just working really hard to
RL and then just working really hard to
make that file
shorter. Hiding code in other files
shorter. Hiding code in other files
doesn't make it shorter.
logging for
logging for
example.
example.
So the reason that there's nothing
So the reason that there's nothing
fundamentally good about putting all the
fundamentally good about putting all the
code in one file, right? Like it removes
code in one file, right? Like it removes
a few lines of imports. That's it. Um,
a few lines of imports. That's it. Um,
the reason I tend to do it is because
the reason I tend to do it is because
when you have all your code in one
when you have all your code in one
place, it makes it easier to see that
place, it makes it easier to see that
you're being stupid when you've missed
you're being stupid when you've missed
like co-optimizations where like if you
like co-optimizations where like if you
have module A and module B, it needs to
have module A and module B, it needs to
be a lot of code, but if you just
be a lot of code, but if you just
redesign them into one thing, it's
redesign them into one thing, it's
shorter. That's why I do it.
Um, yeah, that's why I do it.
Um, yeah, that's why I do it.
I'm going to take a pass at like redoing
I'm going to take a pass at like redoing
a whole bunch of the stuff anyways and
a whole bunch of the stuff anyways and
then we'll see where we end up. I'm
then we'll see where we end up. I'm
hoping we can just make a lot of it
hoping we can just make a lot of it
shorter anyways. But I don't see why
shorter anyways. But I don't see why
like I don't know why we would put stuff
like I don't know why we would put stuff
that's only ever going to be useful for
that's only ever going to be useful for
clean puffl that's been designed
clean puffl that's been designed
specifically for that trainer into the
specifically for that trainer into the
internals of puffer
lab. Heck, there's a lot of stuff in
lab. Heck, there's a lot of stuff in
puffer I just like to straight up
puffer I just like to straight up
delete, right? I'd really love it if we
delete, right? I'd really love it if we
got to a spot where like puffer core is
got to a spot where like puffer core is
a few thousand lines and like a very
a few thousand lines and like a very
small number of useful tools and then
small number of useful tools and then
all the work kind of starts being more
all the work kind of starts being more
more and more uh low-level stuff in C
more and more uh low-level stuff in C
more like shipping our own CUDA modules
more like shipping our own CUDA modules
more stuff like
that aside from the obvious algorithmic
All
All
right. Anybody have any other stuff or
right. Anybody have any other stuff or
do I go back to experience
do I go back to experience
buffer? Cuz experience buffer is going
buffer? Cuz experience buffer is going
to be hard and I want to actually be
to be hard and I want to actually be
able to focus on it for a few solid
hours. Okay.
hours. Okay.
likely we go back to experience
likely we go back to experience
buffer.
buffer.
So, let me
So, let me
think. We've kind of figured out. Let me
think. We've kind of figured out. Let me
just read this one
just read this one
message. This other guy can't really
message. This other guy can't really
tell me that much about what he's done,
tell me that much about what he's done,
but somebody did a
but somebody did a
buffer. Before you start back, has
buffer. Before you start back, has
anybody got priority experience buffer
anybody got priority experience buffer
working well with on policy? I don't
working well with on policy? I don't
know if they have in the way that I'm
know if they have in the way that I'm
doing it. Um, but basically one of the
doing it. Um, but basically one of the
things I figured out over the weekend,
things I figured out over the weekend,
yeah, let's just say that there's going
yeah, let's just say that there's going
to be an article titled on policy
to be an article titled on policy
learning is a lie coming out pretty
learning is a lie coming out pretty
soon. Let's just say
that I looked at the math a bit and uh
that I looked at the math a bit and uh
it don't make
sense. So, that's going to be fun.
vententral protein. There'll be some
vententral protein. There'll be some
articles. I don't mind writing blog
articles. I don't mind writing blog
posts now and again. I just hate writing
posts now and again. I just hate writing
papers. So, there will be my guess is
papers. So, there will be my guess is
there going to be like a series of blog
there going to be like a series of blog
posts with the 2.5 release.
Plus, you know that it's actually useful
Plus, you know that it's actually useful
whenever I write like long form content
whenever I write like long form content
on X. A good chunk of the time it blows
on X. A good chunk of the time it blows
up. Like I didn't even do an article,
up. Like I didn't even do an article,
but I did uh I got really annoyed by
but I did uh I got really annoyed by
this insane doomer like blog post. It
this insane doomer like blog post. It
was literally kind of like a hit piece
was literally kind of like a hit piece
by like I I don't know what the hell it
by like I I don't know what the hell it
was. So, I wrote a big thread on it and
was. So, I wrote a big thread on it and
it's gotten 70 77,000 views and like
it's gotten 70 77,000 views and like
3,000 people actually read the entire 25
3,000 people actually read the entire 25
tweet thread that I wrote. So, that was
tweet thread that I wrote. So, that was
kind of cool. So long form content
good. Okay, back to experience
buffer. Collect segments.
Now for the prioritize
Now for the prioritize
buffer. So each of these are going to
buffer. So each of these are going to
get a score which is going to be based
get a score which is going to be based
on the advantage.
Is it even worth looking at reference
Is it even worth looking at reference
implementations for an experience
implementations for an experience
buffer? Because I'm pretty sure they all
buffer? Because I'm pretty sure they all
like I'm pretty sure they all suck.
Hang on. The advantage
function. Can you use the advantage
function. Can you use the advantage
function for experience prioritization?
Okay, so we have this and we have
this. I guess the torch one might be
this. I guess the torch one might be
half
decent.
See, so this is the original experience
buffer. How much prioritization is
buffer. How much prioritization is
used? Important sampling negative
used? Important sampling negative
exponent added to priority
exponent added to priority
is
storage list storage will be used. Well,
storage list storage will be used. Well,
that's
horrifying. That's like the worst thing
horrifying. That's like the worst thing
ever.
ever.
Pin
Pin
memory
memory
prefetch number of next batches to be
prefetch number of next batches to be
preset fetched using
multi-threading. That's useless if your
multi-threading. That's useless if your
memor is on the GPU,
memor is on the GPU,
right? Yeah, that's useless if your
right? Yeah, that's useless if your
memor is on the
memor is on the
GPU. Transform to be
GPU. Transform to be
executed. Okay, that's horrible. Don't
executed. Okay, that's horrible. Don't
do that ever.
do that ever.
Uh batch
Uh batch
flies dim
extend.
Yeah. Let's see if their code's any
Yeah. Let's see if their code's any
good.
Oh, wait. Is there more
docks? No, it's just
docks? No, it's just
this replay buffer.
Um, none of this is in C++, right? I
Um, none of this is in C++, right? I
guess it's relying on tensor
guess it's relying on tensor
deck, but that's still not
great. Only loops over dim. So far, I
great. Only loops over dim. So far, I
don't see loops over elements.
What's this? Prioritize replay
buffer. OG is remote buffer.
And that's not good. That's slow as
hell. I don't see anything in here that
hell. I don't see anything in here that
makes me think I should like pay any
makes me think I should like pay any
attention to
attention to
this. So, let's go to the actual
paper. It just doesn't look very well
paper. It just doesn't look very well
built.
Where's the math for
this prioritizing with TD
error expected learning
error expected learning
progress magnitude of TD
progress magnitude of TD
error indicates how surprising or
error indicates how surprising or
unexpected the transition is.
unexpected the transition is.
So this would be very closely related to
So this would be very closely related to
what I was considering which is using
what I was considering which is using
advantage. Um
because in PO we have some advantage
because in PO we have some advantage
function
function
right and where is it?
Yeah, this here this is TD error,
right? So for gamma equals 0 or it's at
right? So for gamma equals 0 or it's at
lambda equals zero, uh generalized
lambda equals zero, uh generalized
advantage estimation just gives you TD
advantage estimation just gives you TD
error and then for one it gives
error and then for one it gives
you value baseline discounted returns.
you value baseline discounted returns.
Now the thing I'm wondering
here this is only really based on the
here this is only really based on the
value function
right? Oh, but wait. The value function
right? Oh, but wait. The value function
is in a sense conditioned on the policy,
is in a sense conditioned on the policy,
right? Because the value function is not
right? Because the value function is not
just telling
just telling
you Yeah. Yeah. Yeah. Okay. So, what I
you Yeah. Yeah. Yeah. Okay. So, what I
was concerned about is that if I use the
was concerned about is that if I use the
advantage function
advantage function
uh in order to prioritize experience,
uh in order to prioritize experience,
then it could be the case that the value
then it could be the case that the value
function learns the correct uh the
function learns the correct uh the
correct return out of it, but the policy
correct return out of it, but the policy
hasn't actually done anything. So that's
hasn't actually done anything. So that's
not the case because the value
not the case because the value
function well first of all the reward
function well first of all the reward
collected is actually the word collected
collected is actually the word collected
by the behavior of the policy. So the
by the behavior of the policy. So the
policy is correct in a sense the policy
policy is correct in a sense the policy
is always correct because that's the
is always correct because that's the
only sample that you see um is the
only sample that you see um is the
sample that you get. So the only thing
sample that you get. So the only thing
that can be wrong is the value function
that can be wrong is the value function
here. So you are in a sense just
here. So you are in a sense just
training the value function and the
training the value function and the
value function depends on the policy
value function depends on the policy
anyways because that's what collects the
anyways because that's what collects the
data that it sees. Okay, I'm happy that
data that it sees. Okay, I'm happy that
you can use this for prioritization.
you can use this for prioritization.
That is fine. So then what you could do
That is fine. So then what you could do
is you can rep prioritize every time you
is you can rep prioritize every time you
do an update to the policy. You can rep
do an update to the policy. You can rep
prioritize the advantage
buffer. That would make sense.
Yes, that would make
sense. The reshape for this
sense. The reshape for this
thing. Well, hang on. Getting ahead of
thing. Well, hang on. Getting ahead of
ourselves. We collect data in
ourselves. We collect data in
batch. Now we need prioritize data.
How do you want to sample from this
thing? Is there ever a reason that you
thing? Is there ever a reason that you
would want to
would want to
train
train
on you never want to train on low
on you never want to train on low
priority samples, right?
So the data in this buffer is going to
So the data in this buffer is going to
have to be constantly shifting around
I think you're screwed on memory
I think you're screwed on memory
contiguity here,
though. I think you're screwed on memory
though. I think you're screwed on memory
contiguity.
Yeah. So the issue here, right, is you
Yeah. So the issue here, right, is you
get these samples on
get these samples on
policy. You've got like some other
policy. You've got like some other
samples
samples
here that are off
policy. And then these got to get
blended into like some
blended into like some
sample some
sample some
samples. We'll call this
samples. We'll call this
a
z. Okay. So here's your
z. Okay. So here's your
buffer, but uh you don't just iterate
buffer, but uh you don't just iterate
over this
buffer. You actually you want to sample
buffer. You actually you want to sample
from it the top n. So you actually need
from it the top n. So you actually need
top
K and this is going to be a
K and this is going to be a
non-ontiguous
non-ontiguous
sample. And even if you were to sort
sample. And even if you were to sort
this by top K then you have to
this by top K then you have to
uh rep
prioritize, right?
prioritize, right?
You have to rep prioritize.
Okay, hang
Okay, hang
on. We can save a
on. We can save a
copy. There is a way for us to save a
copy. There is a way for us to save a
copy.
If we just make one big buffer, right?
If we just make one big buffer, right?
We just connect these
two and then at the end of a whole
two and then at the end of a whole
training
training
loop, we just make sure that we have uh
loop, we just make sure that we have uh
these samples at the
these samples at the
end. Then we're good.
If you want constantly correct
If you want constantly correct
prioritized samples, there's no way
prioritized samples, there's no way
around
this. Oh Wait, it's harder than
this. Oh Wait, it's harder than
this. Every time you do a sample
this. Every time you do a sample
step, you have to rep prioritize
step, you have to rep prioritize
everything, don't
you? How bad is that? So every time you
you? How bad is that? So every time you
do a mini batch, you have to rep
do a mini batch, you have to rep
prioritize all the
samples. I think I can make that fast.
If I do a CUDA kernel for this, I think
If I do a CUDA kernel for this, I think
I can make it
I can make it
fast. So, you just need CUDA
fast. So, you just need CUDA
GA. It doesn't have to cross segment
boundaries. I can definitely make that
fast. Okay.
Well, we might find out that I'm crazy
Well, we might find out that I'm crazy
with um some of the assumptions I've
with um some of the assumptions I've
made here. There are several ways at a
made here. There are several ways at a
hardware level that this can go
hardware level that this can go
wrong, but if it's if everything works
wrong, but if it's if everything works
out, this should be pretty simple.
out, this should be pretty simple.
This should be pretty
This should be pretty
simple. And then this would give us
simple. And then this would give us
experience prioritized experience
experience prioritized experience
replay. There will
replay. There will
be copy
overhead. The copy
overhead is at a trajectory
overhead is at a trajectory
level which is nowhere near as bad as at
level which is nowhere near as bad as at
a sample level. Previously we were doing
a sample level. Previously we were doing
sample level indexing. So we actually
sample level indexing. So we actually
have more efficient by a factor of
have more efficient by a factor of
segment length indexing.
segment length indexing.
Now I think this works. This gets us 2D
Now I think this works. This gets us 2D
tensors
tensors
uh for
uh for
everything. 2D tensors for
everything. This gets us prioritized
everything. This gets us prioritized
experience replay.
experience replay.
Um, this gets
Um, this gets
us non-p prioritized advantage filtering
us non-p prioritized advantage filtering
with the same code very
with the same code very
easily. This gets us a
easily. This gets us a
lot. This gets us a lot, I'd say.
And actually I
And actually I
think for anyone watching with RL
think for anyone watching with RL
experience if you know whether uh you
experience if you know whether uh you
use both high advantage and low
use both high advantage and low
advantage trajectories in experience
advantage trajectories in experience
replay. I'm actually not
replay. I'm actually not
sure. I would think that you would take
sure. I would think that you would take
absolute value of the advantage or
absolute value of the advantage or
whatever. You would want to
whatever. You would want to
have you would want the anything that's
have you would want the anything that's
surprising you would want to learn on.
surprising you would want to learn on.
So you'd learn on really low advantage
So you'd learn on really low advantage
and really high advantage samples
and really high advantage samples
because
because
advantage yeah advantage is not uh you
advantage yeah advantage is not uh you
don't take absolute value by default. So
don't take absolute value by default. So
we would want to take absolute
value.
value.
Okay I think we start on this. I think
Okay I think we start on this. I think
we start on this. Uh this is not going
we start on this. Uh this is not going
to be implementing this unless I've
to be implementing this unless I've
miscalculated shouldn't be that bad. And
miscalculated shouldn't be that bad. And
This is actually going to even be a
This is actually going to even be a
little simpler than our current
little simpler than our current
implementation and will be much much
implementation and will be much much
much better if I'm correct on all the
much better if I'm correct on all the
assumptions. So we don't have to do any
assumptions. So we don't have to do any
of this micro batch mini batch
of this micro batch mini batch
shenanigans.
shenanigans.
Uh, all we need to do
is do batch size, replay
size. Uh, we need to add one parameter.
size. Uh, we need to add one parameter.
I
I
believe we need to add just one
believe we need to add just one
parameter.
And I think we want to make it a factor.
We play
We play
size factor times batch size. So num
size factor times batch size. So num
rows is going to
rows is going to
be batch size
plus I don't know why I did minus one.
Hey, welcome
Hrian. Currently, we are coding up
Hrian. Currently, we are coding up
prioritized experience replay for
prioritized experience replay for
PO, which will be a fun thing to
PO, which will be a fun thing to
do. Well, not really, but hopefully the
do. Well, not really, but hopefully the
results will be
fun. Need this. Don't need this. Don't
fun. Need this. Don't need this. Don't
need this.
What do you
What do you
want? What do you
mean? I'm currently on one big cup of
mean? I'm currently on one big cup of
coffee, if that's what you mean.
I guess this does limit uh nonLSTM
I guess this does limit uh nonLSTM
models the way we have it here a little
bit. What are you working on? Yeah.
bit. What are you working on? Yeah.
Yeah. Yeah. Experience replay buffer for
Yeah. Yeah. Experience replay buffer for
uh PO. Prioritize exp beer
uh PO. Prioritize exp beer
buffer. Okay. The only bad thing that I
buffer. Okay. The only bad thing that I
can see with the way that I have uh I
can see with the way that I have uh I
suggested or I have this implementation
suggested or I have this implementation
suggested is that technically right now
suggested is that technically right now
you can only prioritize segments. You
you can only prioritize segments. You
cannot prioritize
cannot prioritize
uh individual
samples. Do I care?
How bad is
that? I probably really don't care that
that? I probably really don't care that
much,
right? Like, yeah, technically theory uh
right? Like, yeah, technically theory uh
that could make a huge difference for
that could make a huge difference for
non-recurrent models, but any problem
non-recurrent models, but any problem
that you can solve without memory like
that you can solve without memory like
just as well or better is probably a
just as well or better is probably a
really stupid
problem. I can't think of like any big
problem. I can't think of like any big
problems that have been solved without
problems that have been solved without
an LSTM.
an LSTM.
It's usually people like not using an
It's usually people like not using an
LSTM on Atari or something because like
LSTM on Atari or something because like
they don't know what they're doing and
they don't know what they're doing and
their LSTM implementation
their LSTM implementation
sucks. So I think that we don't care
sucks. So I think that we don't care
about this. I think we're
about this. I think we're
good. I think we just proceed with
plan. You need episode length
still. You still need episode lengths.
still. You still need episode lengths.
You still need episode indices. You
You still need episode indices. You
still need all these variables, but it
still need all these variables, but it
gets a little
gets a little
easier. Yeah, it's a little
easier. And then I need to change before
easier. And then I need to change before
I forget the full
function on policy rows.
function on policy rows.
We'll just do on policy rows.
Here we
go. So now we have
everything
everything
and this full function works as before.
I have to edit the siphon a tiny
bit. This is now
zero. This gets commented.
So we no longer need to bootstrap across
So we no longer need to bootstrap across
segments like
crazy. I got to tell you, it makes a
crazy. I got to tell you, it makes a
world of difference coding on 13 hours
world of difference coding on 13 hours
of sleep versus 4 hours of sleep.
of sleep versus 4 hours of sleep.
Everything is so much easier today.
I say as I make a stupid
error. I pretty much crashed at like 700
error. I pretty much crashed at like 700
p.m. last night and didn't wake up until
p.m. last night and didn't wake up until
8:00
8:00
a.m.
Exhausted. All
right. Now all we have to change is
right. Now all we have to change is
really the sampling,
really the sampling,
right? When we do like this flatten
right? When we do like this flatten
batch
batch
thing, like this is no longer even uh a
thing, like this is no longer even uh a
necessity. Like this literally doesn't
necessity. Like this literally doesn't
matter anymore,
right? I guess we pack everything into a
right? I guess we pack everything into a
namespace or whatever.
Okay. So, advantages
Okay. So, advantages
here. I need to do weighted choice,
here. I need to do weighted choice,
right?
Is it weighted random
sample utils data weighted random sample
is there
is there
um a weighted is there just like a
um a weighted is there just like a
weighted sample somewhere in here that
weighted sample somewhere in here that
we can just
apply or does it only work on a PyTorch
apply or does it only work on a PyTorch
data
set. Oh no, this just gives
set. Oh no, this just gives
you Yeah, this just gives you indices.
you Yeah, this just gives you indices.
So this is perfect, right?
Wait, is this this is a function, right?
Wait, is this this is a function, right?
The way that they're calling
The way that they're calling
this. Okay, I don't know why it's named
this. Okay, I don't know why it's named
in uppercase like this, but this is just
in uppercase like this, but this is just
a function. So, this is what we
need. And I don't think you need to give
need. And I don't think you need to give
it probabilities either, right? Yeah,
it probabilities either, right? Yeah,
these don't have to be probabilities.
these don't have to be probabilities.
So this is
So this is
advantages
advantages
and replacement false should be
and replacement false should be
faster and then generator none should be
faster and then generator none should be
default. Oh wait no replacement true
default. Oh wait no replacement true
should be faster.
should be faster.
Yes. Yeah. There you go. That's just
Yes. Yeah. There you go. That's just
this. So then you just do
weights. All right, this is
indices. And
indices. And
now all we have to do
now all we have to do
is Is it that
is Is it that
easy? I think it actually is that easy.
easy? I think it actually is that easy.
That's kind of embarrassing.
So we need uh
advantages
advantages
x
actions log props.
actions log props.
uh
dun obs.
Then you need values and
returns which is B advantages plus B
returns which is B advantages plus B
values.
and just plus
and just plus
values. Yeah. So it's something like
values. Yeah. So it's something like
this. I
this. I
think that's it.
Yeah. So, this is going to have to
Yeah. So, this is going to have to
change
change
massively.
massively.
Massively because
You no longer even have update epochs
You no longer even have update epochs
with this,
right? I guess you just do it
here. You literally just do it here,
here. You literally just do it here,
don't
don't
you? Yeah.
So you got to have experience values.
So you got to have experience values.
You got to have all this
stuff and then you compute the
stuff and then you compute the
J to
So now we have all this data.
This feels wrong. Like the amount of
This feels wrong. Like the amount of
changes we're about to make to this
trainer. It's going to get a lot simpler
though. Let me make sure that that width
though. Let me make sure that that width
statement is lined
up. It's not. Right. This needs to be
up. It's not. Right. This needs to be
tabbed in.
Not many
batches. Oh, no. It's That's correct.
batches. Oh, no. It's That's correct.
That
That
is No, it it needs to be tapped in one.
is No, it it needs to be tapped in one.
It doesn't need to be tapped in
one. There we go.
This total mess of code that you're
This total mess of code that you're
seeing here is not what this is going to
seeing here is not what this is going to
look like when we ship this. By the way,
look like when we ship this. By the way,
if you look at our current trainer in
if you look at our current trainer in
2.0, the code is way better than this.
2.0, the code is way better than this.
This is what happens with like 3 months
This is what happens with like 3 months
of me trying a dozen different methods
of me trying a dozen different methods
to improve various things in RL. And um
to improve various things in RL. And um
we're not even going to keep all these
we're not even going to keep all these
flags in. We're just going to drop
flags in. We're just going to drop
everything that doesn't immediately work
everything that doesn't immediately work
well. And you know, we're going to have
well. And you know, we're going to have
puffer lip should be optimized for like
puffer lip should be optimized for like
the defaults are good,
right? There will have to be like extra
right? There will have to be like extra
features that aren't on by default will
features that aren't on by default will
have to have a damn good reason for
have to have a damn good reason for
being there. Basically,
Are you still planning on trying to fl
Are you still planning on trying to fl
out P3 at some point? Yes, that is. So,
out P3 at some point? Yes, that is. So,
to give you an
to give you an
idea, the main thing in PO right now
idea, the main thing in PO right now
that is holding RL back is J.
that is holding RL back is J.
We'll see whether I got it right with
We'll see whether I got it right with
P30 as the
P30 as the
successor, but that thing needs to be
successor, but that thing needs to be
fixed. RL will be stuck until we fix
fixed. RL will be stuck until we fix
that
thing. I'm a little iffy on whether I'm
thing. I'm a little iffy on whether I'm
close or not with P30, though. It might
close or not with P30, though. It might
be that I went a little off base with
be that I went a little off base with
it. We might have to do a little bit
it. We might have to do a little bit
more work to get that to function, but
more work to get that to function, but
it's fine. We'll deal with
it. All
right, I think we are almost ready to
right, I think we are almost ready to
start debugging this.
explained variance has been yeah I took
explained variance has been yeah I took
it out of logging because I there's
it out of logging because I there's
another thing that I broke relating to
another thing that I broke relating to
that it's not that explained variance is
that it's not that explained variance is
wrong it's that the log is not being
wrong it's that the log is not being
populated I also never look at that
populated I also never look at that
log maybe I should but I never look at
log maybe I should but I never look at
that log
Okay, this is an indexing error.
Okay.
This is 2,000
This is 2,000
things and this is giving me 64
things. And L is out of out of index.
So, this needs to just go down, doesn't
So, this needs to just go down, doesn't
it? Until here, maybe.
Okay. And then we get the same error up
Okay. And then we get the same error up
top.
Um, I notice we don't
reset. That's got to get reset right
there. Maybe not.
Okay. So we have full indices
here
here
16k at lengths 4096 f indices.
So uh yeah this is not full
So uh yeah this is not full
indices right.
Or is
it? Maybe it
it? Maybe it
is. Maybe this needs to be LSTM on Total
is. Maybe this needs to be LSTM on Total
Agents. Maybe I have the wrong size. I
Agents. Maybe I have the wrong size. I
think I do. Morning. How's it going?
think I do. Morning. How's it going?
Doing pretty well. Back from little
Doing pretty well. Back from little
trip. Um, got a bunch of good rest in.
trip. Um, got a bunch of good rest in.
Pretty well recharged. And this week we
Pretty well recharged. And this week we
are going to get prioritized experience
are going to get prioritized experience
replay into Puffer. We're going to get
replay into Puffer. We're going to get
this horrendous dev branch code cleaned
this horrendous dev branch code cleaned
up. Um, cleaned up dramatically, I would
up. Um, cleaned up dramatically, I would
hope. And we're going to keep running
hope. And we're going to keep running
experiments on all the more complex MS
experiments on all the more complex MS
that we have and hopefully get some more
that we have and hopefully get some more
soda results. I've got pretty well. I
soda results. I've got pretty well. I
have the next like many weeks of my
have the next like many weeks of my
schedule blocked nicely, so I should be
schedule blocked nicely, so I should be
able to focus uh very well on work.
able to focus uh very well on work.
Where did I go? Went to tour the new
Where did I go? Went to tour the new
puffer
facility. I will be there in person in a
facility. I will be there in person in a
few weeks again. But in the
few weeks again. But in the
meanwhile, this is the uh this is where
meanwhile, this is the uh this is where
all the machines are going.
We've got electrical done for
it. Haven't bought all the machines yet.
it. Haven't bought all the machines yet.
We'll move some of the ones that we have
We'll move some of the ones that we have
now here in the short term, but you
now here in the short term, but you
know, we will be buying a lot more
know, we will be buying a lot more
machines. We now have capacity for more
machines. We now have capacity for more
machines. We will no longer have to deal
machines. We will no longer have to deal
with power fluctuations and other
with power fluctuations and other
And uh we should be able to
And uh we should be able to
power a lot of good RL research.
majority of that cluster is going to be
majority of that cluster is going to be
like the vast majority of that cluster
like the vast majority of that cluster
is going to be just for all the people
is going to be just for all the people
we have on open source work doing more
we have on open source work doing more
open source work. So hopefully we'll
open source work. So hopefully we'll
even be able to grow Puffer to a larger
even be able to grow Puffer to a larger
open source project with this because we
open source project with this because we
will have more machines to give people
will have more machines to give people
access to which will be kind of cool,
access to which will be kind of cool,
right? You know, before it was like if
right? You know, before it was like if
you're one of the top contributors, you
you're one of the top contributors, you
get access to a box. Uh and then once we
get access to a box. Uh and then once we
have the new machines, it'll be like
have the new machines, it'll be like
pretty much all the contributors get a
pretty much all the contributors get a
box and the top contributors have access
box and the top contributors have access
to several machines for running
to several machines for running
experiments. That's the goal.
It will be expensive
though. Donor. There's no
though. Donor. There's no
donor. There is no donor, my friend.
donor. There is no donor, my friend.
This is
This is
I will be covering all the
machines. Buffer is officially a company
machines. Buffer is officially a company
after all. It's not a not for profofit
after all. It's not a not for profofit
even though all our stuff is open
even though all our stuff is open
source. So uh the way we make money is
source. So uh the way we make money is
just by selling service packages to
just by selling service packages to
people using our stuff. You want us on
people using our stuff. You want us on
your side to make your RL fast and easy?
your side to make your RL fast and easy?
You can do that for a reasonable price.
You can do that for a reasonable price.
Starts at 10K a
month. So like a quarter of an engineer
month. So like a quarter of an engineer
to have your RL not be insane and
cursed. Pretty good
cursed. Pretty good
deal. If any of you watching have
deal. If any of you watching have
uh industry problems and need RL to work
uh industry problems and need RL to work
for you, give us a ring.
Well, it should be one D but given away
Well, it should be one D but given away
top
top
shape.
Okay. What shape do we have?
Do we just sum
Do we just sum
these? I think we just sum these,
right? I am going to save RL. This is
right? I am going to save RL. This is
why I'm doing this. Literally like if I
why I'm doing this. Literally like if I
thought I it lit if I thought that all I
thought I it lit if I thought that all I
could do is maybe make like RL 10%
could do is maybe make like RL 10%
better, I wouldn't be doing this, right?
better, I wouldn't be doing this, right?
I wouldn't be spending the rest of my
I wouldn't be spending the rest of my
20s on like some crazy mission to make
20s on like some crazy mission to make
RL 10% better, right? The goal of this
RL 10% better, right? The goal of this
is I think that we can fix the whole
is I think that we can fix the whole
field of RL. Um the progress in the last
field of RL. Um the progress in the last
year has been crazy. I think by the end
year has been crazy. I think by the end
of this year, you're going to see it
of this year, you're going to see it
pretty broadly applicable to a bunch of
pretty broadly applicable to a bunch of
industries and we're going to actually
industries and we're going to actually
start scaling, you know, some contract
start scaling, you know, some contract
work out so we can get Puffer going, you
work out so we can get Puffer going, you
know, at a reasonable scale. And then
know, at a reasonable scale. And then
the year after that, by then we
the year after that, by then we
hopefully we just have everything solved
hopefully we just have everything solved
and we get like some much larger
and we get like some much larger
contracts and we are just like a nice
contracts and we are just like a nice
stable midsized industry lab. That's the
stable midsized industry lab. That's the
hope.
And the key is to do all of it
And the key is to do all of it
bootstrapped so that we don't end up
bootstrapped so that we don't end up
with strings attached on what we can
with strings attached on what we can
open source and what we can't because we
open source and what we can't because we
kind of just want to open source
everything. What's wrong with this?
What?
What?
Um. Huh?
I don't want to do list to
this. Does anybody know how this stupid
this. Does anybody know how this stupid
thing works? This weighted random
sampler? Maybe we just go read the
sampler? Maybe we just go read the
source
code. Oh, it's just
multinnomial. So, we don't need this
multinnomial. So, we don't need this
thing at all. It's just multinnomial.
Even
better. And uh we need to do abs of this
better. And uh we need to do abs of this
anyways.
Oh, that was
sketchy. I'm a Cali CC student
sketchy. I'm a Cali CC student
transferring now. Got to most UC's.
transferring now. Got to most UC's.
Congratulations for applied math and
Congratulations for applied math and
rejected for CS. Do I think I can make
rejected for CS. Do I think I can make
it useful if I take the right class or
it useful if I take the right class or
does it suck if I want to do software
does it suck if I want to do software
engineering? Uh, the software
engineering? Uh, the software
engineering courses suck anyways. So,
engineering courses suck anyways. So,
you should be fine. I uh I don't know
you should be fine. I uh I don't know
how the courses work there because
how the courses work there because
uh it depends on your university whether
uh it depends on your university whether
you apply to a major or not. like
you apply to a major or not. like
Stanford, you do not apply for a major
Stanford, you do not apply for a major
and a lot of schools, you do not apply
and a lot of schools, you do not apply
for a major and you can just take
for a major and you can just take
whatever you want and including stuff
whatever you want and including stuff
outside of your major once you declare
outside of your major once you declare
it. So, I don't know if that means you
it. So, I don't know if that means you
can take zero courses or whatever.
can take zero courses or whatever.
Hopefully, you can still take a couple
Hopefully, you can still take a couple
courses, but usually outside of the
courses, but usually outside of the
intro CS courses, which are just there
intro CS courses, which are just there
to make sure that like you've heard of
to make sure that like you've heard of
all the basics, um CS is very much a
all the basics, um CS is very much a
self-taught discipline. like the courses
self-taught discipline. like the courses
suck. So, you're not really at a
suck. So, you're not really at a
disadvantage for not having access to
disadvantage for not having access to
those
those
courses. If you're a math heavy person
courses. If you're a math heavy person
and you want to lean into the math,
and you want to lean into the math,
that's cool. I didn't. I'm more engine
that's cool. I didn't. I'm more engine
I'm more engineering heavy, but that
I'm more engineering heavy, but that
doesn't mean that I spent my time taking
doesn't mean that I spent my time taking
a bunch of CS courses, right? It means I
a bunch of CS courses, right? It means I
spent my time doing a lot of projects
spent my time doing a lot of projects
and a lot of
and a lot of
research. Our expanding RL Algo pool,
research. Our expanding RL Algo pool,
what's coming next? So, our goal is not
what's coming next? So, our goal is not
to have a pool of algorithms. It's
to have a pool of algorithms. It's
really to just always have the one best
really to just always have the one best
algorithm. So, right now I'm adding on
algorithm. So, right now I'm adding on
to PO an a prioritized replay buffer
to PO an a prioritized replay buffer
that should make quite a large
that should make quite a large
difference. Uh we're going to test it
difference. Uh we're going to test it
obviously we're going to test it on all
obviously we're going to test it on all
the different puffer ms and actually as
the different puffer ms and actually as
a fallback. There's a way to use to uh
a fallback. There's a way to use to uh
do training without a uh replay buffer
do training without a uh replay buffer
as well. But the goal is not to like
as well. But the goal is not to like
keep expanding this thing horizontally
keep expanding this thing horizontally
with more algorithms. The goal is to
with more algorithms. The goal is to
just get the best algorithm, make sure
just get the best algorithm, make sure
we have the best algorithm, and just
we have the best algorithm, and just
keep pushing that
keep pushing that
way. So, right now, we're currently just
way. So, right now, we're currently just
attempting to improve the best
attempting to improve the best
algorithm. Thanks. Appreciate your
algorithm. Thanks. Appreciate your
insight. We'll just keep self-learning
insight. We'll just keep self-learning
projects to go and be fine. Yeah. And if
projects to go and be fine. Yeah. And if
you're looking at RL specifically,
you're looking at RL specifically,
there's a quick start guide on my
there's a quick start guide on my
website that does not assume you have a
website that does not assume you have a
ton of math experience. Um, it links you
ton of math experience. Um, it links you
a very manageable number of papers and
a very manageable number of papers and
blog posts to read to get you up to
blog posts to read to get you up to
speed on most of the basics. This is
speed on most of the basics. This is
what I've given to all our new
what I've given to all our new
contributors. Most of them came in with
contributors. Most of them came in with
zero RL experience. And now I would put
zero RL experience. And now I would put
like I bet on many of my contributors uh
like I bet on many of my contributors uh
over like top CS PhD students in order
over like top CS PhD students in order
to get RL stuff done. So yeah, it's you
to get RL stuff done. So yeah, it's you
can learn stuff. The thing that's
can learn stuff. The thing that's
hardest honestly is getting the good
hardest honestly is getting the good
engineering base. But the courses don't
engineering base. But the courses don't
help you. Like becoming a good
help you. Like becoming a good
programmer is very very very hard. It's
programmer is very very very hard. It's
much harder than basically anything
much harder than basically anything
else. I consider it harder than like
else. I consider it harder than like
getting good at AI even.
getting good at AI even.
Um but the courses aren't going to help
Um but the courses aren't going to help
you there anyways.
So yeah, definitely don't fall for like
So yeah, definitely don't fall for like
the oh no coders next year or whatever.
the oh no coders next year or whatever.
Like no, the coding is the hardest part.
Like no, the coding is the hardest part.
It's the hardest
part. Everything else is pretty easy.
part. Everything else is pretty easy.
Actually, the AI stuff is not that
Actually, the AI stuff is not that
hard. Executing on everything with good
hard. Executing on everything with good
engineering is the hard part.
So that looks good.
I have lots of programming tips as well
I have lots of programming tips as well
around if you just sort of see with this
couple has no attribute flat.
values. How'd that
happen? Screwed that up.
Heck happened
There.
There.
Oh, yeah. That's going to screw you up,
Oh, yeah. That's going to screw you up,
isn't it?
some do
some do
this. I'll do it.
Um, we're going to comment this for
now. Accumulate mini
now. Accumulate mini
batches. What do we do
this? I guess this is epoch then, right?
no attribute B returns. Oops.
Yeah. So, that's going to be a
problem. Um, I guess this is just like
problem. Um, I guess this is just like
advantages
advantages
plus values or
plus values or
something. Want a new dev? I want to dev
something. Want a new dev? I want to dev
a new M using your C or Python API.
a new M using your C or Python API.
Which M has the most straightforward
Which M has the most straightforward
template to follow? Square does. And
template to follow? Square does. And
there's a little bit of uh
there's a little bit of uh
so there's a little bit of subtlety here
so there's a little bit of subtlety here
because we're in between two end finding
because we're in between two end finding
generations. So we're in between
generations. So we're in between
generations of the way that we plan to
generations of the way that we plan to
do this stuff. So if you just go to 2.0
do this stuff. So if you just go to 2.0
O puffer lib ocean
O puffer lib ocean
squared. There is a full python
squared. There is a full python
implementation of squared somewhere pi
implementation of squared somewhere pi
squared. So this is the whole n written
squared. So this is the whole n written
in python very simple n and then there
in python very simple n and then there
is the exact same
is the exact same
environment written with just a
environment written with just a
binding through syon
binding through syon
syonbind and then the whole logic is in
syonbind and then the whole logic is in
the.h right here. So you have both as
the.h right here. So you have both as
examples. Now in the new dev branch
examples. Now in the new dev branch
which is not done yet uh we have found a
which is not done yet uh we have found a
way to eliminate Syon and substantially
way to eliminate Syon and substantially
improve the ease with which you can bind
improve the ease with which you can bind
stuff to C. But that's not quite ready
stuff to C. But that's not quite ready
yet. You can look at it in the dev
yet. You can look at it in the dev
branch. In fact, I'll show you real
branch. In fact, I'll show you real
quick.
quick.
So instead of writing a Syon file, this
So instead of writing a Syon file, this
is what you write instead of Syon now.
is what you write instead of Syon now.
And the rest of the code is just about
And the rest of the code is just about
the same. But um aside from
the same. But um aside from
that, all the stuff in 2.0 is stable and
that, all the stuff in 2.0 is stable and
easy enough to
use and a lot of this unstable stuff is
use and a lot of this unstable stuff is
going to get stable really quick. I had
going to get stable really quick. I had
like a bunch of stuff going on in the
like a bunch of stuff going on in the
last few weeks and you know my family
last few weeks and you know my family
was around so you know there was time
was around so you know there was time
loss in between doing stuff. I should be
loss in between doing stuff. I should be
able to get some really solid hours in
able to get some really solid hours in
over the next few weeks. So, a lot of
over the next few weeks. So, a lot of
this stuff should just get stable
this stuff should just get stable
fast. Really, like most stuff in Puffer
fast. Really, like most stuff in Puffer
Lib is just straight up bottlenecked by
Lib is just straight up bottlenecked by
my dev
hours. Advantages plus values. Yeah.
and
and
probably we failed to increment global
probably we failed to increment global
step or Nothing.
Ah, this is what happened. Yeah, this
Ah, this is what happened. Yeah, this
didn't get reset.
So, so this has to go into
So, so this has to go into
sample.
Actually, not into sample. It should go
into I guess it goes here for now.
Boom. Cool. 2.1 million SPS. Probably
Boom. Cool. 2.1 million SPS. Probably
it's not doing enough stuff in training,
it's not doing enough stuff in training,
but
but
uh doesn't immediately
uh doesn't immediately
crash. Pretty cool.
crash. Pretty cool.
It's not going to train well, but that's
It's not going to train well, but that's
not bad for a first
not bad for a first
draft. Not bad at
draft. Not bad at
all. So, next thing is going to
all. So, next thing is going to
be experience prioritization probably,
right? Experience prioritization.
Notice obsctions and
Notice obsctions and
rewards are in UN8 int and float. Yeah,
rewards are in UN8 int and float. Yeah,
you can have them be different d types.
you can have them be different d types.
So that specific environment
So that specific environment
um the reward is I mean in that specific
um the reward is I mean in that specific
in that specific environment it is
in that specific environment it is
better to have U and 8 because it's
better to have U and 8 because it's
lower bandwidth other environments have
lower bandwidth other environments have
different D types you can define
different D types you can define
whatever DT type you want right does
whatever DT type you want right does
puffer handle that
puffer handle that
automatically well when you're writing
automatically well when you're writing
your environment if you're writing it in
your environment if you're writing it in
C it's going to have a DT type
C it's going to have a DT type
Right. And then the only thing that you
Right. And then the only thing that you
have to be careful with which Syon
have to be careful with which Syon
should tell you if you're doing it wrong
should tell you if you're doing it wrong
and you know that our new code should
and you know that our new code should
also tell you if you're doing it wrong.
also tell you if you're doing it wrong.
Uh you just have to make sure that the
Uh you just have to make sure that the
numpy array that you pass in matches the
numpy array that you pass in matches the
type that you've assigned in uh in C
type that you've assigned in uh in C
which all you do for that is puffer will
which all you do for that is puffer will
take care of that for you provided that
take care of that for you provided that
the observation space right when you
the observation space right when you
define observation space you define the
define observation space you define the
correct dype there.
correct dype there.
So if you look at the other ends, we
So if you look at the other ends, we
have ops that are imps, we have ops that
have ops that are imps, we have ops that
are floats, we have
are floats, we have
whatever. And actions can also be floats
whatever. And actions can also be floats
if they are continuous because we do
if they are continuous because we do
have the continuous support as well.
Okay. How do we want to populate
Okay. How do we want to populate
the prioritize
buffer? I guess all we
buffer? I guess all we
do, we take this little piece of
code. We have to copy everything over
code. We have to copy everything over
though. That's kind of gross.
What if I just do like
prioritize? Well, I'm going to be able
prioritize? Well, I'm going to be able
to make this simpler anyways. So, I
to make this simpler anyways. So, I
think I just do it for now and I don't
think I just do it for now and I don't
worry too much.
worry too much.
So, we do
So, we do
this bit of
this bit of
code
here. Oh, you don't need a weighted
here. Oh, you don't need a weighted
sample. I'm
sample. I'm
dumb. I don't think you need a weighted
dumb. I don't think you need a weighted
sample at all. We'll do it like this for
sample at all. We'll do it like this for
now.
But to
But to
learn
learn
do we
do we
try
try
experience. So now we have
advantages compute J
Then we have to do
So this is a mess, but it's not going to
So this is a mess, but it's not going to
remain a
mess. No set all policy rows.
name space has
no issues.
No attribute rewards.
Log
props actions log props done ops
props actions log props done ops
advantages values return for
terms. So it's these these things right
terms. So it's these these things right
obsctions log props
uh guns values
returns something like
and I guess these don't get talked
either. They need to get talked though,
either. They need to get talked though,
don't they?
Yeah. So, we'll just add this to the uh
Yeah. So, we'll just add this to the uh
the
the
sampler, right?
Okay. So, experience values is bigger
Okay. So, experience values is bigger
than advantages
than advantages
here. Uh let me see why that is the
here. Uh let me see why that is the
case.
Dun. Why is Dun's
smaller? Experience. Duns.
smaller? Experience. Duns.
This should not
This should not
be
be
smaller
numbers BPT
numbers BPT
horizon. So it looks to me like these
horizon. So it looks to me like these
are getting these shapes are getting
are getting these shapes are getting
messed with.
So, it says that Dun is actually it's
So, it says that Dun is actually it's
supposed to be the bigger of the two
Yes, this is
16k. Still
correct.
advantages and
values. Very
values. Very
odd. Very odd indeed.
Ah, hang on. Dun equals xdons. That will
Ah, hang on. Dun equals xdons. That will
screw you
up. Yeah, you don't need that anywhere
up. Yeah, you don't need that anywhere
else. I think that's it. I think I just
else. I think that's it. I think I just
had a little bug. I'm going to have lots
had a little bug. I'm going to have lots
of bugs to fix with the code being this
of bugs to fix with the code being this
messy.
But we're almost to the point where we
But we're almost to the point where we
can start cleaning it
can start cleaning it
up. There we
up. There we
go. Okay. So this is probably not
go. Okay. So this is probably not
working, probably super buggy, probably
working, probably super buggy, probably
very bloated, but this is an
very bloated, but this is an
implementation of prioritized experience
implementation of prioritized experience
replay in puffer lib with PO running at
replay in puffer lib with PO running at
2 million steps per
2 million steps per
second. Um, that's very
good.
Now, there's so many things that I'm
Now, there's so many things that I'm
going to have to think about on how to
going to have to think about on how to
fix this.
I guess I can start by just removing
I guess I can start by just removing
stuff that's no longer
needed. This is gone.
We still want to be able to do this with
P3. It shouldn't be that hard to like
P3. It shouldn't be that hard to like
make it work with P30 either, but we're
make it work with P30 either, but we're
not going to fix that just yet.
What about episode indices, episode
What about episode indices, episode
lengths and all of
lengths and all of
that? We still need
that? We still need
these, I believe. So, I believe we still
these, I believe. So, I believe we still
need the
need the
indices. Easier than before, but we
indices. Easier than before, but we
still need
still need
them. So, what was it? Compute
Jun's values, rewards, sword
indices. See if this gets simplified at
all. No longer need stored indices.
We no longer need
this. Eat stuns, values, rewards, gamut,
this. Eat stuns, values, rewards, gamut,
and
should be
should be
good. So if we're going to store indices
good. So if we're going to store indices
from
here see what I did wrong here.
See if this
See if this
works.
works.
Yes. Now we have this
Yes. Now we have this
running. See if that still trains or
running. See if that still trains or
still runs. Not going to train just
yet. One, two, three. Oh yeah, I forgot
yet. One, two, three. Oh yeah, I forgot
to fix it in the other spot.
Okay. So, this still now this runs
Okay. So, this still now this runs
again.
I don't think that there is any way
I don't think that there is any way
around all these indents
unfortunately. I guess I could combine
them. That doesn't look good.
This is
gone. What do we think it's going to
gone. What do we think it's going to
take to get this thing to train?
That would be the most useful is if I
That would be the most useful is if I
could actually have it training. So then
could actually have it training. So then
as I refactor stuff, I know I don't
as I refactor stuff, I know I don't
break it
anymore. Let's go take a look.
Okay. So this is the current
observation
8192. Uh mini batch size is 8192 samples
8192. Uh mini batch size is 8192 samples
not segments.
samples. This should be mini bat.
Yeah. So now it's 64 by
Yeah. So now it's 64 by
118 or was it 128x 64 by 118? That's way
118 or was it 128x 64 by 118? That's way
better.
And now instead of update
And now instead of update
epoch, it's no longer update
epoch. Okay, that's a good question.
epoch. Okay, that's a good question.
Now, how do we want to rewrite this
Now, how do we want to rewrite this
parameter?
train
steps. It's no longer update epochs.
steps. It's no longer update epochs.
It's
updates. Do I want to have it as a
updates. Do I want to have it as a
factor?
I guess I can punt on it for now and
I guess I can punt on it for now and
just adjust the parameter the way I know
just adjust the parameter the way I know
it needs to be,
right? Because right now, so it's BPT or
right? Because right now, so it's BPT or
was it mini batch size 8192 with one
was it mini batch size 8192 with one
update
update
epoch? Whoop. So this is going to
epoch? Whoop. So this is going to
be 51 was it
52488 over 8192.
64. So it's technically 64 update
64. So it's technically 64 update
epochs. Really 64 updates, not epochs.
epochs. Really 64 updates, not epochs.
It's a total number of updates that you
It's a total number of updates that you
run. See what this
does. It's a lot closer to the original
does. It's a lot closer to the original
speed.
There's no way this thing is just going
There's no way this thing is just going
to train already,
right? I mean, I'm not complaining.
Did I seriously just do this whole thing
Did I seriously just do this whole thing
zero shot without making any
mistakes? Looks like
mistakes? Looks like
it looks like it did.
It's pretty close to the original
It's pretty close to the original
learning curve as well.
Okay. Okay.
Okay. Okay.
So, we have a working uh a working
So, we have a working uh a working
demo. At this point, I should make a
demo. At this point, I should make a
commit or we break it.
I guess it's mostly going to be cleanup
I guess it's mostly going to be cleanup
time now, right?
time now, right?
Like the main issue with this file is
Like the main issue with this file is
it's a mess and all the stuff I do want
it's a mess and all the stuff I do want
to do with experience buffer like I
to do with experience buffer like I
don't have much confidence in this file
don't have much confidence in this file
right now like that everything is
right now like that everything is
correct. So that's going to be the next
correct. So that's going to be the next
stage. Um yeah that's going to
stage. Um yeah that's going to
definitely be the next
definitely be the next
stage.
Okay. I'm going to take a couple quick
Okay. I'm going to take a couple quick
minutes. I'm going to grab a drink, do
minutes. I'm going to grab a drink, do
whatever, and then I will be back here,
whatever, and then I will be back here,
and we will figure out if there's
and we will figure out if there's
anything we can do with all these crazy
anything we can do with all these crazy
messy
messy
rappers.
rappers.
Um, how much of this like redundant
Um, how much of this like redundant
stuff we can delete, if there's a good
stuff we can delete, if there's a good
way to clean up the experience buffer,
way to clean up the experience buffer,
you know, we'll figure out all of
you know, we'll figure out all of
that. Okay, I'll be right back.
Okay, good progress so far.
There kind of a couple stages to this
There kind of a couple stages to this
cleanup,
cleanup,
right? The first one is going to be the
right? The first one is going to be the
big stuff like can we not have
big stuff like can we not have
everything indented under four layers of
everything indented under four layers of
width statements because of
width statements because of
profiling.
profiling.
Um, can we clean up the
Um, can we clean up the
way that we have the experience
way that we have the experience
buffer all that sort of
stuff and then the second was going to
stuff and then the second was going to
be like the
be like the
smaller line by line stuff.
Yeah. That's going to be the main
Yeah. That's going to be the main
thing. So I think for now what we do go
thing. So I think for now what we do go
a little
a little
slower. We look at how indented the code
slower. We look at how indented the code
is and we see if there's anything we can
is and we see if there's anything we can
do to not have it be like that.
So the first one is the AM
context. Can that be a decorator
context. Can that be a decorator
somehow? I don't really like
somehow? I don't really like
decorators to be honest.
decorators to be honest.
It's kind of what I did up here,
It's kind of what I did up here,
right? This is like the overall profile
decorator. Maybe we can do that.
How to turn
How to turn
PyTorch
am I don't know if I'm going to want to
am I don't know if I'm going to want to
do
So you do something like this, right?
But isn't the autocast doesn't have a DT
type? Well, I guess we can make it up
type? Well, I guess we can make it up
here,
right? So, you can make like little
wrapper. Do I want this mixed with my
wrapper. Do I want this mixed with my
profiler
though? Okay, we'll keep that in mind.
though? Okay, we'll keep that in mind.
So this is hiding one with statement
So this is hiding one with statement
here. This is another
here. This is another
one. And then aside from
one. And then aside from
that, there's one additional layer,
that, there's one additional layer,
right? Maximum of one additional layer
right? Maximum of one additional layer
because you never have nested
because you never have nested
profilers, but there are a lot of them.
profilers, but there are a lot of them.
You
You
see now if we move the CUDA synchronize
see now if we move the CUDA synchronize
into the
profiler that gets rid of all these
profiler that gets rid of all these
little
little
statements. We could do that and we can
statements. We could do that and we can
remove it as well dynamically which is
remove it as well dynamically which is
easy. So that gets rid
easy. So that gets rid
of a little bit of code.
what do we think about this
what do we think about this
generally? I mean like tabbing this
generally? I mean like tabbing this
under a width statement is the correct
under a width statement is the correct
thing to do. It's
thing to do. It's
just it's not so nice, you know.
What's our profiler code look
like? Pretty garbage,
right? Pretty garbage.
We also have utilization in here
We also have utilization in here
somewhere,
right? This isn't so
bad. So, if we uh we kind of just like
bad. So, if we uh we kind of just like
scroll up through this, this big chunk
scroll up through this, this big chunk
of code is the dashboard.
of code is the dashboard.
Uh, this is actually pretty compact for
Uh, this is actually pretty compact for
what it is already. The dashboard is
what it is already. The dashboard is
just it's a pretty complicated object.
just it's a pretty complicated object.
It's kind of manually laid
out. This is a seating function.
We've got this roll out function which
We've got this roll out function which
will get a little bit
simplified. We've got checkpoint loading
simplified. We've got checkpoint loading
and
saving. We've got this
utilization. Experience buffer is a big
utilization. Experience buffer is a big
one. Uh we actually will be able to
one. Uh we actually will be able to
remove at least for now we'll be able to
remove at least for now we'll be able to
remove
this. We'll have to put it back in with
P30. I think actually I don't want to
P30. I think actually I don't want to
remove it just left yet because I want
remove it just left yet because I want
to be able to
to be able to
um to port that stuff
up. Creation here of this can be
up. Creation here of this can be
simplified a little bit maybe.
Oh yeah, this can be
simplified at least a
bit. And
bit. And
then yeah, profiler is pretty bad.
log
log
distributed. This doesn't get used.
distributed. This doesn't get used.
We'll be we were considering doing stuff
We'll be we were considering doing stuff
with this
with this
though. Definitely some just stale code
though. Definitely some just stale code
we'll be able to just straight up delete
we'll be able to just straight up delete
with no
with no
issues. Stuff like this, you know,
issues. Stuff like this, you know,
instead of writing all these out, we
instead of writing all these out, we
probably can do something with
it. And then a lot of this is also like
it. And then a lot of this is also like
algorithm options. Let's look at the
algorithm options. Let's look at the
indents in here. So here we have
indents in here. So here we have
train
train
epoch. Are we missing the AM context
epoch. Are we missing the AM context
here? I think we're missing Oh, no. Here
here? I think we're missing Oh, no. Here
it is. Here's the ammp context. This can
it is. Here's the ammp context. This can
just go around the whole thing. I
just go around the whole thing. I
think just go around the whole function.
I wonder if there's a way to just do AM
globally. Uh because you don't want to
globally. Uh because you don't want to
have it globally, right?
M and V
forward. So you actually you
can't you can't just put this to the
can't you can't just put this to the
top. It has to go into this profile
top. It has to go into this profile
wrapper.
Okay. So
then I think no matter how I do this I
then I think no matter how I do this I
have one layer of wrappers right which
have one layer of wrappers right which
will be the profile wrappers
All these little if statements are going
All these little if statements are going
to get compressed anyways.
I could start with this.
I could start with this.
changing the
changing the
profiler and I can see where I end
profiler and I can see where I end
up. I think that's not a terrible idea.
How the heck does this even work?
ah each of these is actually a
ah each of these is actually a
popular.youutil.profiler I see.
So wait, when you
do Yeah. So it's with proper lab utils
do Yeah. So it's with proper lab utils
pro I say
Then you just have them all written out.
So I guess the initial thing that would
So I guess the initial thing that would
be very You see?
Okay. So now this should
Okay. So now this should
synchronize CUDA
everywhere and then we can do
on
text. Yes.
session. Guess we just do this.
So now how do we
do how do we do like start and end on a
do how do we do like start and end on a
context like this?
Okay. So now that's the context handled,
Okay. So now that's the context handled,
right?
and then we want to
pass ammp
context. All
right. So, I guess we'll figure out
where figure out which
But this should let us fix a whole bunch
But this should let us fix a whole bunch
of
stuff because
now we can just unindent all of
now we can just unindent all of
this,
this,
right? Unindent all this.
right? Unindent all this.
And I can also
And I can also
do also do
this. That one might still be needed.
But all these that are train related,
But all these that are train related,
you do not need do not need these.
Okay.
Mask. Okay. So, most of that should now
Mask. Okay. So, most of that should now
be
be
good. If we can get this to run.
No attributes include
No attributes include
the
same. Okay, this runs.
Looks like it still trains as
Looks like it still trains as
well. So that cleans up all the context
well. So that cleans up all the context
about as much as we can expect, right?
Like we're not going to be able to get
Like we're not going to be able to get
rid of these because we need some way of
rid of these because we need some way of
timing individual blocks of code. And it
timing individual blocks of code. And it
is cleaner to just indent one level than
is cleaner to just indent one level than
it is to try to keep track of like start
it is to try to keep track of like start
and stop. Right? So the only thing we
and stop. Right? So the only thing we
can kind of do is we can maybe optimize
can kind of do is we can maybe optimize
like where these blocks are getting
like where these blocks are getting
declared a little
bit. Maybe we can optimize like a little
bit. Maybe we can optimize like a little
bit
bit
there. But I'm looking at this eval
there. But I'm looking at this eval
forward.
This can go
here.
Yeah. That's pretty much as compact as
Yeah. That's pretty much as compact as
it's going to get.
Some of these if statements are again
Some of these if statements are again
these are going to get compressed so
these are going to get compressed so
don't worry about those just worry about
don't worry about those just worry about
like the overall indentation of it.
like the overall indentation of it.
This is min indent
This is min indent
level. One indent level for decorators
level. One indent level for decorators
like
this.
Um that we maybe can fix.
Okay, this is already
Okay, this is already
a I think the next biggest improvement
a I think the next biggest improvement
is going to come from the experience
is going to come from the experience
buffer
buffer
refactor. So
let's let's look at
that. How much does tensor
that. How much does tensor
dict do for us here?
We also need to look at this free
We also need to look at this free
function.
This is like kind of fine, right?
I make the uh the end it a little
I make the uh the end it a little
easier
easier
if I put this into a pass.
man. It's so hard to capture like the
man. It's so hard to capture like the
clean RL vibes, you
clean RL vibes, you
know? Costa is just really the
goat. It's like every time you try to
goat. It's like every time you try to
think of adding some
structure. I guess he did eventually go
structure. I guess he did eventually go
with a data class for args, but that's
with a data class for args, but that's
literally just
literally just
arg there's a class for agent and then
arg there's a class for agent and then
the rest of it's all just this
loop. Man, this is like such good code.
loop. Man, this is like such good code.
How do we like get back to
this, you know?
this, you know?
How do we get back to this?
Every additional piece of modularity
Every additional piece of modularity
that I add brings us farther and farther
that I add brings us farther and farther
away from this beautifully simple code.
100 lines of
code. What do we have that's adding to
code. What do we have that's adding to
this?
like does experience buffer even need to
like does experience buffer even need to
be a
class? Well, there are a few things that
class? Well, there are a few things that
we had to do here, right? Like that make
we had to do here, right? Like that make
this a little harder. So the first thing
this a little harder. So the first thing
was you need to be able to call eval and
was you need to be able to call eval and
train separately, right?
Now you need to be able to call evaluate
Now you need to be able to call evaluate
and
and
train separately.
right now I have three separate
right now I have three separate
functions for this. I've
functions for this. I've
got essentially a class without a
got essentially a class without a
class and I have eval and train but then
class and I have eval and train but then
I also have this separate rollout
I also have this separate rollout
function
Every feature that we add makes this
Every feature that we add makes this
harder. But what's even harder than
harder. But what's even harder than
adding a feature is adding an optional
adding a feature is adding an optional
feature,
right? Unless it's just like a smooth
right? Unless it's just like a smooth
parameterization where zero is off.
How I design this is going to depend
How I design this is going to depend
heavily
heavily
on how many features I expect Puffer Lib
on how many features I expect Puffer Lib
to
have. A few other things.
I really really want to get back to
I really really want to get back to
something closer to this though.
I'm going to sit here and agonize over
I'm going to sit here and agonize over
this as long as it takes for me to find
this as long as it takes for me to find
something some way to get closer back to
something some way to get closer back to
this. But I don't have to sit here and
this. But I don't have to sit here and
agonize all at once. That would be very
agonize all at once. That would be very
boring. Doesn't have to be all at once.
boring. Doesn't have to be all at once.
Um, I think for now what I do is I avoid
Um, I think for now what I do is I avoid
going more in the direction I don't want
going more in the direction I don't want
to go. So for now I don't make this a
to go. So for now I don't make this a
class. I leave this as
class. I leave this as
is. Um, I just see if I can locally
is. Um, I just see if I can locally
clean up the experience buffer a little
clean up the experience buffer a little
bit. And then we clean up and prune code
bit. And then we clean up and prune code
that doesn't need to be there. And then
that doesn't need to be there. And then
maybe that'll give us some additional
maybe that'll give us some additional
clarity.
clarity.
Maybe that'll give us additional
clarity with the knowledge that a lot of
clarity with the knowledge that a lot of
these conditionals are going to get
these conditionals are going to get
deleted. Hey YouTube
deleted. Hey YouTube
folks, this
folks, this
is currently me adding prioritized
is currently me adding prioritized
experience replay to puffer lips PO
experience replay to puffer lips PO
implementation. That is the goal here.
implementation. That is the goal here.
We have an initial implementation that
We have an initial implementation that
seems like it works. And
seems like it works. And
um now the goal is to clean up the dev
um now the goal is to clean up the dev
branch code so it's actually
usable with the new experience
buffer. I mean pretty much all these
buffer. I mean pretty much all these
tensors
tensors
are like these are almost all the same
are like these are almost all the same
size,
size,
right? So I don't necessarily have to go
right? So I don't necessarily have to go
to a tensor dict
to a tensor dict
immediately. But
um it would seem like we should go to a
um it would seem like we should go to a
name space,
right, that we can iterate
Maybe we can go part way.
So this is going to add you know some
So this is going to add you know some
redundant
code. We don't need all
this ops actions log props rewards done
chunks. Okay.
chunks. Okay.
So now we do store
And just values return for
terms. They miss
terms. They miss
advantages and values.
Yeah. Should these even need to be in
here? I don't think the advantages
here? I don't think the advantages
should need to be in here.
What about returns?
Iterating over these keys seems worse to
Iterating over these keys seems worse to
me even
somehow. It can't be though,
right? Just do it for now and then we'll
right? Just do it for now and then we'll
see how it looks.
Okay, let's just port up this P30 stuff
Okay, let's just port up this P30 stuff
as Oh,
The returns was advantages plus
values. All right, that's not
values. All right, that's not
bad. I think that's it, right? Returns
bad. I think that's it, right? Returns
mean standard deviation. All these
mean standard deviation. All these
stupid transposes go
stupid transposes go
away and we should be
away and we should be
good. Oh. Oh, diversity. All you need as
good. Oh. Oh, diversity. All you need as
well, this
one. But that's not bad either.
goes
goes
away. I don't like the dict indexing,
away. I don't like the dict indexing,
but we will uh you know, we'll deal with
that. I think I can do well, we'll get
that. I think I can do well, we'll get
to
to
it. That's sample cleaned
it. That's sample cleaned
up if it
runs. Few advantages.
See if that still works.
If
so, we should be able to clean up the
so, we should be able to clean up the
rest of it.
Right. That's
next name space. No attribute. Name
next name space. No attribute. Name
space.
Okay. See if that breaks
anything. Do we think we're ready for
anything. Do we think we're ready for
CUDA? CUDA J.
It would be simpler and a bit faster.
I think we just chill for now.
How much complexity are we left with
here? A lot of it is just from all these
here? A lot of it is just from all these
various different choices, isn't it?
Okay.
Yeah, I think we're on the stage where
Yeah, I think we're on the stage where
it's just got to be a lot of fiddly line
it's just got to be a lot of fiddly line
by line.
Lot of just fiddly line by
Lot of just fiddly line by
line
edits. Try not to break anything. Try to
edits. Try not to break anything. Try to
shorten the code. All of that.
Is there any way
Is there any way
um we can avoid having to recall
um we can avoid having to recall
the advantage code at the
end? I think not. But I think that we
end? I think not. But I think that we
would have like we could simplify it if
would have like we could simplify it if
J were part of the
buffer. Does that make sense
buffer. Does that make sense
even to have J part of the buffer?
It bothers
It bothers
me. Definitely bothers
me. How much stuff does this buffer have
me. How much stuff does this buffer have
that we don't even need? Right.
So, could all this stuff just be part of
So, could all this stuff just be part of
create, you know? I think it could be.
That's an interesting
That's an interesting
idea. So, what if we didn't have an
idea. So, what if we didn't have an
experience buffer class, but did kind of
experience buffer class, but did kind of
have like a trainer
have like a trainer
class? Well, we don't have to make the
class? Well, we don't have to make the
decision on a trainer class. We can just
decision on a trainer class. We can just
stick it in data for now, right?
That seems
That seems
better. No experience
component. That seems quite nice to me.
seems very good to me
seems very good to me
actually. Let's just take all this
code. Let's just move
code. Let's just move
it right here.
Ops D type action DT
type. Does this even get used lower down
anywhere? I don't think it even gets to
anywhere? I don't think it even gets to
used. Yeah. No, it doesn't.
You get the D types
here. See this next row.
It's not
bad. Experience equal upper li name
space obs equals
Come here.
And what I break
here LSTM total agents is just total
here LSTM total agents is just total
agents.
And then you just need all this mess,
And then you just need all this mess,
right?
Where's hidden size?
archive skills and
archive skills and
batch config
archive. I know what this
is. It totally screwed that up, didn't
is. It totally screwed that up, didn't
it? Don't ever press the auto
complete then skills is rand
complete then skills is rand
archive agents. Yep, that looks good.
archive agents. Yep, that looks good.
of
of
zeros. This
is
size. Something like that,
size. Something like that,
right? Something like that.
And what is this batch size thing here?
config. Just do batch size config batch
config. Just do batch size config batch
size and
size and
device.
device.
Yeah, just for now. And we'll we'll keep
Yeah, just for now. And we'll we'll keep
cleaning this up.
That takes care of it pretty
That takes care of it pretty
well now. You just need all these
well now. You just need all these
checks. All
right. Config mini batch and then config
right. Config mini batch and then config
max mini batch.
and we do the same
thing. And uh that's pretty much all
thing. And uh that's pretty much all
there is in there. Mini batch size, mini
there is in there. Mini batch size, mini
batch rows, device
batch rows, device
Pointer and
step. Do we even use pointer
anymore? We'll set him just in
case.
Cool. So now
Cool. So now
um we take these
um we take these
functions and put these in a reasonable
functions and put these in a reasonable
spot which will probably
spot which will probably
be below
be below
eval and train.
be a good spot.
experience
experience
X. We're going to end up typing this a
X. We're going to end up typing this a
lot.
This is going to be long
uh long iterative
process.
Actually, hang
Actually, hang
on. It's not. Let's not screw this
up. This is actually
data. Be data.
where GPU winds just
where GPU winds just
go. Oh, it was never uh Okay, that's
fine. And almost this is no longer data.
fine. And almost this is no longer data.
This is
X. It's going to get messier before it
X. It's going to get messier before it
gets
gets
simpler, but it is going to get simpler
Let's go back up to
Let's go back up to
experience. Make sure we're not doing
experience. Make sure we're not doing
stupid
things. Delete that massive
things. Delete that massive
mess.
mess.
Um, yeah. See, not all these things
Um, yeah. See, not all these things
belong here.
belong here.
All these things, these go to
sell. These we can punt on for
sell. These we can punt on for
now. This is definitely data
now. This is definitely data
though. What is it? LSTMH.
Okay. So now all these things need to go
Okay. So now all these things need to go
into here which is the annoying bit
into here which is the annoying bit
about how we have this right now.
what else? Mini batch rows, num mini
what else? Mini batch rows, num mini
batches, any other
batches, any other
stuff? Yeah, there should be a bunch of
stuff? Yeah, there should be a bunch of
things. So, stored indices, app lengths,
things. So, stored indices, app lengths,
app indices, free index,
name
space. This is data and this is data.
Yep. So there's on policy and off policy
Yep. So there's on policy and off policy
rows.
Like I say, it's going to get it's going
Like I say, it's going to get it's going
to get weird before it gets easier.
Ah, you left trailing commas on
Ah, you left trailing commas on
literally everything.
name space no attribute
name space no attribute
device
device
really. Well, we'll just add it for
now. Making a huge mess of this,
now. Making a huge mess of this,
but this is necessary for now in order
but this is necessary for now in order
to get it to a point where it's
cleaner. We have config here.
Not have config.
Now my mini batches.
is I thought that we had this.
Oh, now
Oh, now
data. This gets us at least closer to
data. This gets us at least closer to
what I want, which is not a whole bunch
what I want, which is not a whole bunch
of separate little utility classes.
And this is a
sample. Speed up.
Okay. into object
I probably left some stuff in here
I probably left some stuff in here
that's not supposed to be in
there. Pointer and step. That's not
there. Pointer and step. That's not
supposed to be in there.
broke my terminal.
Okay. Sample
data experience
items. We'll see how that goes.
Just a whole bunch of renaming of stuff
Just a whole bunch of renaming of stuff
required for
required for
this. Hopefully we didn't break
this. Hopefully we didn't break
anything. We'll
see. Got run the same speed as before
see. Got run the same speed as before
and hopefully nothing broken.
So now we have the entire big experience
So now we have the entire big experience
buffer inside this create function as
buffer inside this create function as
this massive
this massive
like not super massive honestly. Where
like not super massive honestly. Where
is
it? It's like a screen and a half of
it? It's like a screen and a half of
code. And mind you, a lot of that is for
code. And mind you, a lot of that is for
the different algorithms. So, this is
the different algorithms. So, this is
going to end up being like less than a
going to end up being like less than a
screen of code for the experience once
screen of code for the experience once
we get rid of extras that don't uh that
we get rid of extras that don't uh that
are not
are not
needed. That's not bad. I'm still going
needed. That's not bad. I'm still going
to be stubborn for now about not making
to be stubborn for now about not making
this a class. So, we'll just endure
this a class. So, we'll just endure
having this stupid name space for now.
But uh hopefully this allows us to
But uh hopefully this allows us to
co-optimize a few
co-optimize a few
things since we no longer have multiple
things since we no longer have multiple
different strus hanging
out. I think the next thing will be to
out. I think the next thing will be to
approximately fix all the algorithm
approximately fix all the algorithm
add-ons like these like E3B diversity
add-ons like these like E3B diversity
all you need all this stuff. um these
all you need all this stuff. um these
code blocks are no longer in the correct
code blocks are no longer in the correct
place after we've made these
place after we've made these
changes. So, we should probably fix
changes. So, we should probably fix
that. And then we can play around with
that. And then we can play around with
the experience buffer itself and how we
the experience buffer itself and how we
want that to
work. Probably will need some more
work. Probably will need some more
logging. I mean, there's going to be a
logging. I mean, there's going to be a
lot of cleanup here,
lot of cleanup here,
right? This file is just massively too
right? This file is just massively too
long. This is a 1200 line file. This
long. This is a 1200 line file. This
needs to be a three three-digit line
needs to be a three three-digit line
file for
sure. I know how to fix
that. Uh we also have the option to do
that. Uh we also have the option to do
CUDA module
CUDA module
for generalized advantage estimation.
for generalized advantage estimation.
That would probably be a good idea.
I think we'll time it first to make sure
I think we'll time it first to make sure
that it is actually an issue
that it is actually an issue
because well, it's probably not getting
because well, it's probably not getting
profiles correctly yet. All
profiles correctly yet. All
right, I'm going to use a restroom. I
right, I'm going to use a restroom. I
will uh be right back. I got to do one
will uh be right back. I got to do one
or two other quick things and then we'll
or two other quick things and then we'll
be back in a few and uh we
will we'll continue cleaning this up and
will we'll continue cleaning this up and
hopefully we at least have a reasonable
hopefully we at least have a reasonable
working prototype of this today with
working prototype of this today with
additional cleanup to continue tomorrow
additional cleanup to continue tomorrow
and then by midweek we should already
and then by midweek we should already
have no end of midweek we should already
have no end of midweek we should already
have something that looks pretty good.
have something that looks pretty good.
Right back few
All
All
right, I just have to put in a food
right, I just have to put in a food
order and
order and
um then we'll be back. The plan is going
um then we'll be back. The plan is going
to be I do super early dinner today. So,
to be I do super early dinner today. So,
I work on this for the
I work on this for the
next hour or so, and then I grab super
next hour or so, and then I grab super
early dinner. I get a little exercise
early dinner. I get a little exercise
in, and then I should be able to come
in, and then I should be able to come
back for a short evening session as
well. Just figure out what I'm getting.
do that.
Don't really need cars. All right.
Do we want to do CUDA advantage
Do we want to do CUDA advantage
already? Let's see how hard it would be.
Doesn't look like it would be that hard,
Doesn't look like it would be that hard,
right? This is what I had for
um
um
E30. So, we should pretty easily just be
E30. So, we should pretty easily just be
able to do
um do this
um do this
for PO, right?
Okay. So, this is going to
Okay. So, this is going to
take float star
take float star
values. This is going to
take
orbs. Oh, it also needs dumps.
it like this maybe values rewards
it like this maybe values rewards
dumps float it
uh float
uh float
gamma lambda and then we also need
gamma lambda and then we also need
non-steps and horizon.
Okay.
This is going to be row parallel, right?
Yeah, this is one
row. Literally, it's just this operation
row. Literally, it's just this operation
right here. Oh, you probably need the
right here. Oh, you probably need the
advantage buffer output as well.
Yes.
X row time horizon plus
T was it
T was it
road X +
So this is idx
So this is idx
next. This is
next. This is
idx
idx
next. Oops. I only want the single term
next. Oops. I only want the single term
autocomplete so I can make sure it
autocomplete so I can make sure it
doesn't mess anything up.
Delta rewards
Delta rewards
next. This is fine.
next. This is fine.
And then advantages of idx like
this. Okay. So this is roughly what our
this. Okay. So this is roughly what our
kernel should look
like. Was it C
like. Was it C
advantage? Where's the
binding?
C. Not this.
Can't be this
either. Okay, hang on. Where's this
load? Okay,
load? Okay,
load
load
pufferlib.cu. That's what I did.
So yeah, this is what I
did. I just need to get the signature
did. I just need to get the signature
for this and we'll be able to fix it.
This is now called P30
This is now called P30
kernel.
P3
kernel. Now this is
kernel. Now this is
P30
kernel. All right. And now this one
kernel. All right. And now this one
here which
here which
tenser
values
rewards
advantages. Okay.
And this is going to
And this is going to
be J
be J
kernel. This is J
kernel. Very nice.
kernel. Very nice.
We do a little cuda now and again, you
We do a little cuda now and again, you
know, we do a little bit of cuda now and
again. Start with
this. Uh we will just like redefine this
this. Uh we will just like redefine this
thing because we'll need this
thing because we'll need this
later.
later.
So this is
So this is
comput. And I'd love to figure out how
comput. And I'd love to figure out how
uh to reduce the amount of binding
uh to reduce the amount of binding
nonsense that you need for this, but
nonsense that you need for this, but
um yeah, for now we're just going to do
um yeah, for now we're just going to do
it this way because it'll still be
it this way because it'll still be
faster.
This
is where is it? I copied the wrong one.
is where is it? I copied the wrong one.
Stupid. Need the
signature. You know, I really haven't
signature. You know, I really haven't
written much CUDA, but it turns out if
written much CUDA, but it turns out if
you're used to writing C, CUDA is just C
you're used to writing C, CUDA is just C
that runs on the GPU.
kind of no big
deal. Honestly, dealing with the stupid
deal. Honestly, dealing with the stupid
binding module is more of a pain than
binding module is more of a pain than
writing the actual CUDA, assuming you're
writing the actual CUDA, assuming you're
doing, you know, embarrassingly easy
doing, you know, embarrassingly easy
stuff if you're just doing row parallel
stuff if you're just doing row parallel
or whatever. Obviously, they're like
or whatever. Obviously, they're like
complex block parallel screams. We don't
complex block parallel screams. We don't
get into that, but you don't need to
get into that, but you don't need to
most of the
most of the
time. Okay.
time. Okay.
Just got
that
steps signature.
How the hell did it cut a parameter
How the hell did it cut a parameter
off? Well, I wasn't looking to cut a
off? Well, I wasn't looking to cut a
parameter off. You see that?
All right.
And num steps in horizon is
actually launch.
Now we get to actually use the torch
Now we get to actually use the torch
tensor for this,
tensor for this,
right?
So we just do comput
J rewards, dons, advantages, gamma, and
J rewards, dons, advantages, gamma, and
lambda. Isn't that
nice? Then we do the same thing right
there. Sure.
What did we screw
up? There's some stuff in the cuda.
You forgot semicolons on literally
everything. What a
everything. What a
noob. And didn't you even forget? Yeah,
noob. And didn't you even forget? Yeah,
you forgot this too, you noob.
you forgot this too, you noob.
Okay, that's what I get for coding with
Okay, that's what I get for coding with
no LSP or anything.
I probably should have an
LSP in num
steps. Forgot this comma.
expected opening
brace
brace
block. This is just this is added for
block. This is just this is added for
you, isn't
you, isn't
it? Oh, duh.
it? Oh, duh.
All
right. This gun
Wait. I messed it up here as well. Yes,
Wait. I messed it up here as well. Yes,
I did.
J kernel's already been
defined. Forgot to delete this nonsense.
Uh, okay. That looks
worse. Def J kernel has already been
defined. Does this have to be
defined. Does this have to be
um a different
name? Oh, this should be compute.
name? Oh, this should be compute.
This should be compute P3L,
This should be compute P3L,
right? And this can be compute
right? And this can be compute
J. That way it doesn't conflict with the
J. That way it doesn't conflict with the
kernel
name. Still
name. Still
no J kernel already been defined.
Uh, I only see it once.
this
compute. I don't know which one of these
compute. I don't know which one of these
you
load. Pionet compute J has already been
defined. Port extension
name. What would that be processed by?
name. What would that be processed by?
Hang on. Do I get it for both of them
now? Oh, it's a single M.Daf, isn't
it? It's probably a single
M. Can I just do this?
That's probably better,
right?
Cool. So now that actually compiles and
Cool. So now that actually compiles and
the only thing that I get
is this warning which we can just
is this warning which we can just
comment this variable.
Then was it cannot access X.
I don't know how I do that. I just
I don't know how I do that. I just
accidentally delete a massive chunk of
code. All
right. Does it
right. Does it
work? Does it work first
work? Does it work first
try? Shall see.
You need uh advantages,
right?
Yeah. Let's do it this way.
Oh, that's way faster, isn't
Oh, that's way faster, isn't
it? Yeah, that's way faster. So, I don't
it? Yeah, that's way faster. So, I don't
even have to check if the uh the C
even have to check if the uh the C
implementation was slower or whatever
implementation was slower or whatever
because it obviously
is. That's good to know.
And yeah, we kind of immediately just
And yeah, we kind of immediately just
went with
that cuda very fast.
Okay, so we still saw a breakout now
Okay, so we still saw a breakout now
with
with
this good time to make a commit.
There you go. Chrome done.
So now it's mostly just going to be a
So now it's mostly just going to be a
matter of cleaning up the rest of the
matter of cleaning up the rest of the
code and uh then actually like figuring
code and uh then actually like figuring
out how uh the replay buffer interacts
out how uh the replay buffer interacts
with all this.
Right. It's
3:22. Okay. I'm probably gonna have
3:22. Okay. I'm probably gonna have
dinner. I'm having a really early dinner
dinner. I'm having a really early dinner
today. like probably within the next
today. like probably within the next
half hour. So, I'm just going to keep
half hour. So, I'm just going to keep
working on that until then and get
working on that until then and get
dinner, probably get a little bit of
dinner, probably get a little bit of
exercise in, and then I'll probably come
exercise in, and then I'll probably come
back for a evening
back for a evening
session. It's the goal. So, I think for
session. It's the goal. So, I think for
now what we do is we just start cleaning
now what we do is we just start cleaning
this stuff
up. So, this bit here. Whoops.
This goes up to
here. We indent this.
So this is
now values. Yeah. Yeah. Yeah. Okay. This
now values. Yeah. Yeah. Yeah. Okay. This
gets
gets
easier. So this is
easier. So this is
now
now
data.mmassblock.0
data.buff.0 data.rewardb reward
data.buff.0 data.rewardb reward
block. This is now
experience.rewards.
experience.rewards.
Experience
Experience
this. And
this. And
then
then
advantages is not stored here. I
guessbounce.0. Get rid of this.
guessbounce.0. Get rid of this.
What did I just
What did I just
do? Double
dot. It's probably something something
dot. It's probably something something
around like this, right? Don't need
around like this, right? Don't need
these anymore.
This
is I don't even necessarily want this to
is I don't even necessarily want this to
run just yet.
Uh, we don't need this flatten batch
either. Okay. Well, I don't even
either. Okay. Well, I don't even
necessarily want this stuff to run just
yet.
yet.
Um, yeah, I just want this to be like
Um, yeah, I just want this to be like
roughly in the right spot, in the right
roughly in the right spot, in the right
size.
And we don't need any of this.
This is not
This is not
experience. This is back
values.plat. Okay. Let me make sure this
values.plat. Okay. Let me make sure this
still
still
works. Slowly but surely, we are cutting
works. Slowly but surely, we are cutting
off lots and lots of code.
I don't know why this is
here. This can just be
Okay, that still works. Let's try this
Okay, that still works. Let's try this
now.
I don't know why this is a separate
function. Oh, it's because I see it gets
function. Oh, it's because I see it gets
used in both places. Fine.
used in both places. Fine.
We'll leave it alone for
now. But we are going to get rid of
that. Okay. Are we ready to take a look
that. Okay. Are we ready to take a look
now through this code and see uh see how
now through this code and see uh see how
this is overall?
this is overall?
So, we have our bindings. The bindings
So, we have our bindings. The bindings
are messy. The bindings will improve.
are messy. The bindings will improve.
We've got this create
function. It's got a giant block of
function. It's got a giant block of
stuff for allocating
stuff for allocating
experience. Most of it is like extra
experience. Most of it is like extra
algo specific. So, a lot of these things
algo specific. So, a lot of these things
will be gotten rid
of. Here's the rest of it.
of. Here's the rest of it.
We have this crazy messy namespace thing
We have this crazy messy namespace thing
to think
to think
about that's lower on the priority
about that's lower on the priority
list. What I really want to see is like
list. What I really want to see is like
evaluate and stuff like
this. So profile E is
this. So profile E is
val profile the M. Get
that. Here's the first little snippet
that. Here's the first little snippet
from diversity. All you
need
need
[Music]
copy, LSTMH indexing,
copy, LSTMH indexing,
forward with the state
tensor. Okay, so most of the like extra
tensor. Okay, so most of the like extra
crap here is just from the new algorithm
crap here is just from the new algorithm
like
like
components. So these will be
uh the goal will be we will finish
uh the goal will be we will finish
testing these and then we will either
testing these and then we will either
keep them or delete them.
And then actually the loop will be
And then actually the loop will be
pretty clean with possibly some small
pretty clean with possibly some small
you know line by line fixes that we need
you know line by line fixes that we need
to
to
make. Then we have train.
Do we use
this? No, we don't use any of
these. Log stuff will have to be
these. Log stuff will have to be
adjusted. We do
adjusted. We do
P3 and there's
PO.
Yep. Here's your train forward function.
So, we're pretty much back to
So, we're pretty much back to
like some line by line changes plus
like some line by line changes plus
the main thing being
um yeah, the main thing being the
um yeah, the main thing being the
algorithm head-ons. Okay, this is good
algorithm head-ons. Okay, this is good
progress. That might be dinner. Let me
progress. That might be dinner. Let me
go check on
something. All
right, that's dinner bell. Um, I will be
right, that's dinner bell. Um, I will be
back probably later tonight after
back probably later tonight after
dinner to continue working on this
dinner to continue working on this
thing. If you're interested in my stuff
thing. If you're interested in my stuff
or RL generally, I've got a pretty
or RL generally, I've got a pretty
pretty full streaming schedule the next
pretty full streaming schedule the next
full week. It's going to be streaming a
full week. It's going to be streaming a
lot of dev. So, puffer.ai start the
lot of dev. So, puffer.ai start the
GitHub repo to help us out. That really
GitHub repo to help us out. That really
helps us out. It's free. Just do
helps us out. It's free. Just do
it. Can join the Discord to get involved
it. Can join the Discord to get involved
with dev. A lot of our best contributors
with dev. A lot of our best contributors
came in with zero RL experience prior.
came in with zero RL experience prior.
It's a great way to learn. And it can
It's a great way to learn. And it can
follow me on X for more RL content.

Kind: captions
Language: en
We are live.
We are live.
Hi. I'm just going to go make a live
Hi. I'm just going to go make a live
stream announcement real quick and then
stream announcement real quick and then
we are going to get to
dev. We have a full day planned today.
Okay, so here's the uh the plan for
Okay, so here's the uh the plan for
today. We're going to get this
today. We're going to get this
experience buffer working. We're going
experience buffer working. We're going
to go through a few different
to go through a few different
permutations of how this could be
permutations of how this could be
implemented. Um I started on this
implemented. Um I started on this
yesterday. I was absolutely exhausted.
yesterday. I was absolutely exhausted.
Uh, I went to bed at 700 p.m. and slept
Uh, I went to bed at 700 p.m. and slept
till 8 a.m. So, I'm refreshed and we're
till 8 a.m. So, I'm refreshed and we're
going to get all this
going to get all this
stuff going here. Um, so I realized that
stuff going here. Um, so I realized that
I was pretty much correct in the way
I was pretty much correct in the way
that I was doing this before already.
that I was doing this before already.
Uh, there are a few small changes that I
Uh, there are a few small changes that I
am going to want to make to this
though. And now the question is going to
though. And now the question is going to
be, do I make them now or do I get the
be, do I make them now or do I get the
current version to run
first? I think it's better to just make
first? I think it's better to just make
the changes now and get it working
the changes now and get it working
once.
Okay, let me make sure I'm actually
Okay, let me make sure I'm actually
correct with how I was going to do this.
correct with how I was going to do this.
So the uh the crux of it here is the uh
So the uh the crux of it here is the uh
the buffer shape.
the buffer shape.
You see it is numbum rows by BPT
You see it is numbum rows by BPT
horizon like
horizon like
so. Uh what I was going to
do not quite this
rows
rows
bash it's going to be the number of
bash it's going to be the number of
rows. What is it?
No, it's still
numeros. It was going to be something
numeros. It was going to be something
like a trajectory
segment
numbum something like this. And then it
numbum something like this. And then it
would be number of rows
by any
batches length something like this. I
batches length something like this. I
don't think this is yet perfect either.
Yeah, the data structure for this is a
Yeah, the data structure for this is a
little bit tricky. So technically right
little bit tricky. So technically right
you can have any number of trajectories
you can have any number of trajectories
any number any trajectory length any
any number any trajectory length any
segment horizon for BPT
segment horizon for BPT
uh
BPTT there's a lot that you need to add
BPTT there's a lot that you need to add
here
I guess the two cases that we want to
I guess the two cases that we want to
optimize for are like your standard old
optimize for are like your standard old
environments which are not vectorzed and
environments which are not vectorzed and
you're not going to be running very many
you're not going to be running very many
copies of because they're slow and then
copies of because they're slow and then
the vectorzed case where everything is
the vectorzed case where everything is
very fast and you can assume that you're
very fast and you can assume that you're
going to get no uh equal chunks of
going to get no uh equal chunks of
experience from each subprocess.
So instead of doing numbum
rows, it's like mini batches, micro
rows, it's like mini batches, micro
batches or something, isn't it?
So if I really wanted to do this, it
So if I really wanted to do this, it
would be like num micro
would be like num micro
batches micro batch
batches micro batch
size. But that's annoying, right?
size. But that's annoying, right?
Because now we have this 5D
Because now we have this 5D
tensor. Yeah, we have a 5D tensor.
There's nothing wrong with that
There's nothing wrong with that
fundamentally. It's just very silly to
fundamentally. It's just very silly to
work with, right? Because then you can
work with, right? Because then you can
imagine that all these tensors up top,
imagine that all these tensors up top,
log prop, rewards, done, truncated, all
log prop, rewards, done, truncated, all
these, uh, now we're going to have to do
these, uh, now we're going to have to do
the same with
the same with
these. That's annoying.
Yeah. So in this case, right, you'd loop
Yeah. So in this case, right, you'd loop
over mini battress and then you would
over mini battress and then you would
loop over micro batches with the same
loop over micro batches with the same
LSTM
LSTM
state and then you would go uh you know
state and then you would go uh you know
take all the data from these
take all the data from these
get micro batch by trib length and this
get micro batch by trib length and this
is your data by obsh
shape. So that
works. I don't like this though. I
works. I don't like this though. I
really don't like this.
And the reason that
um are you ever going to have
uh are you ever going to have more m
uh are you ever going to have more m
than micro batch size? You are right.
Yeah. So, this is tricky because like
Yeah. So, this is tricky because like
what ends up happening here, right?
what ends up happening here, right?
Let's say you have 8192 MS or 8192
Let's say you have 8192 MS or 8192
agents. You're not going to be able to
agents. You're not going to be able to
run
run
optim over a batch of that size most
optim over a batch of that size most
likely.
That's tricky. That was very
tricky. It's almost one of those things,
tricky. It's almost one of those things,
right, where you have to build it as
right, where you have to build it as
generally as possible. Um, not because,
generally as possible. Um, not because,
you know, necessarily it needs to be
you know, necessarily it needs to be
that way, but because you don't know
that way, but because you don't know
what the correct specific build is,
what the correct specific build is,
right?
right?
like we might be able to assume that
like we might be able to assume that
you're always going to have more m than
you're always going to have more m than
backros or something like that. And then
backros or something like that. And then
we can kill one of these dimensions
Guess we could time some stuff as
well. The expectation I have is that the
well. The expectation I have is that the
LSTM, the number of rows in the LSTM
LSTM, the number of rows in the LSTM
probably has to
probably has to
be reasonably large.
Actually, it is a
Actually, it is a
fused It is a fused CUDA
fused It is a fused CUDA
kernel. Maybe it doesn't have to
kernel. Maybe it doesn't have to
be. Should we just test that? Does that
be. Should we just test that? Does that
is that going to help us
though? I don't actually know what that
though? I don't actually know what that
would tell us.
This isn't easy to write to either,
This isn't easy to write to either,
right? Yeah, because you need to
index. Okay, this is quite difficult
index. Okay, this is quite difficult
actually to design
actually to design
this. Maybe I should start drawing this
this. Maybe I should start drawing this
so people can see. No, cuz this is like
so people can see. No, cuz this is like
very abstract. Maybe I should just start
very abstract. Maybe I should just start
drawing this so people can see what I'm
drawing this so people can see what I'm
talking about. And maybe I'll figure it
talking about. And maybe I'll figure it
out that
way. I'm going just download this as a
way. I'm going just download this as a
PDF in case I need
PDF in case I need
that. All right. So, here's the issue,
that. All right. So, here's the issue,
right?
uh the what the thing that we're looking
uh the what the thing that we're looking
at
at
now you have experience it comes in from
now you have experience it comes in from
the environments
right so each of these is an
agent and then you're probably going to
agent and then you're probably going to
get something like this being like a an
get something like this being like a an
Mstep
So you're going to get some group of
So you're going to get some group of
uh of observations for some of the
uh of observations for some of the
agents and then what your job is to do
agents and then what your job is to do
uh your job is to turn this into a batch
uh your job is to turn this into a batch
for
for
training. Now a batch for training is
training. Now a batch for training is
probably each mini batch rather is going
probably each mini batch rather is going
to
to
contain ideally unpadded fixed length
It's going to be like some number of
It's going to be like some number of
steps. So this is like sag
length and then this is
length and then this is
your mini
batch. So what's wrong with this, right?
batch. So what's wrong with this, right?
Why can't you just grab like this and
Why can't you just grab like this and
like this? Well, this segment length is
like this? Well, this segment length is
not necessarily the same as this length
not necessarily the same as this length
here, right? You might want to chop this
here, right? You might want to chop this
up. And if you do chop it up, then you
up. And if you do chop it up, then you
have to keep track of which M's this
have to keep track of which M's this
belongs to. Because this next batch,
belongs to. Because this next batch,
right, that's going to come in like
right, that's going to come in like
this. You have to keep track of the LSTM
state because ideally you'd like to
state because ideally you'd like to
preserve the LSTM state um across
preserve the LSTM state um across
boundaries. And it's also not
boundaries. And it's also not
necessarily the case that these are
necessarily the case that these are
going to be dense, right? some steps
going to be dense, right? some steps
might get more experience uh than
might get more experience uh than
others, right? So like this one here,
others, right? So like this one here,
maybe you're missing a little bit of
maybe you're missing a little bit of
experience. What do you do about that?
experience. What do you do about that?
There are lots and lots of
There are lots and lots of
considerations
here. Now complicating this further,
here. Now complicating this further,
uh there's also the puffer m
uh there's also the puffer m
case.
case.
So we have two different main cases,
So we have two different main cases,
right? In one case, we're basically
right? In one case, we're basically
going to get random data from like
going to get random data from like
random agents. We're not going to have
random agents. We're not going to have
that big of a batch size. Um, it's not
that big of a batch size. Um, it's not
going to be super fast so that we don't
going to be super fast so that we don't
have to worry about overhead. That's
have to worry about overhead. That's
like working with Atari or working with
like working with Atari or working with
any classic. I almost should start
any classic. I almost should start
calling them legacy with how fast Puffer
calling them legacy with how fast Puffer
is getting. But the other case is
is getting. But the other case is
working with Puffer amps. And in Puffer
working with Puffer amps. And in Puffer
amps, you get lots of nice things for
amps, you get lots of nice things for
free. So in buffer
free. So in buffer
ends, I can basically guarantee you that
ends, I can basically guarantee you that
let's say that you have this buffer and
let's say that you have this buffer and
we just like look at this block and this
we just like look at this block and this
block here. You're going to get a block
block here. You're going to get a block
of observations here and then you're
of observations here and then you're
going to get a block of observations
going to get a block of observations
here and then
here and then
here and then here. And these are going
here and then here. And these are going
to fill up, you know, these are going to
to fill up, you know, these are going to
fill up
fill up
perfectly. And you're not going to have
perfectly. And you're not going to have
weird like agents indexed where batches
weird like agents indexed where batches
are split all across this buffer. And
are split all across this buffer. And
things are going to be relatively
things are going to be relatively
nice. You're going to have relatively
nice. You're going to have relatively
large batch sizes. Uh you're not going
large batch sizes. Uh you're not going
to have to deal with padding. You're not
to have to deal with padding. You're not
going to have to deal with weird
going to have to deal with weird
indexing. Things are just nice. But this
indexing. Things are just nice. But this
doesn't hold for the other case. So your
doesn't hold for the other case. So your
goal here is whatever I build, I want
goal here is whatever I build, I want
this thing to be very nicely indexed
this thing to be very nicely indexed
such that we get minimal possible copy
such that we get minimal possible copy
overhead and such. And then the jank
overhead and such. And then the jank
buffer, right? This is like what an
buffer, right? This is like what an
Atari buffer might look like where it's
Atari buffer might look like where it's
like stuff's all over the place, right?
like stuff's all over the place, right?
This one's allowed to be slower because
This one's allowed to be slower because
a small amount of overhead here. you're
a small amount of overhead here. you're
not going to notice it because the M's
not going to notice it because the M's
are slow anyways and they're going to be
are slow anyways and they're going to be
by definition. They're not really
by definition. They're not really
properly
vectorzed. So you need to optimize for
vectorzed. So you need to optimize for
both of these things. Now I'd also like
both of these things. Now I'd also like
to optimize this such that it is
to optimize this such that it is
relatively efficient to use an
relatively efficient to use an
experience buffer of some sort.
I should clarify that to use a
I should clarify that to use a
prioritized experience buffer of some
sort. This is also tricky.
sort. This is also tricky.
So, a prioritized experience
buffer basically by
definition is going to require you to uh
definition is going to require you to uh
to do additional copies.
It's because you don't know what the
It's because you don't know what the
priority is of each of these samples
priority is of each of these samples
until you have the whole sample
until you have the whole sample
collected.
Ah, there is an optimization I can make
Ah, there is an optimization I can make
here. If I know that I'm going to have
here. If I know that I'm going to have
to copy the data
anyways, I
can I can keep it. I can keep the full
can I can keep it. I can keep the full
segment, right? So, we'll just have this
segment, right? So, we'll just have this
be a full trash.
be a full trash.
or full
segment. Okay, I was worried about this
segment. Okay, I was worried about this
because technically you can have BPT
because technically you can have BPT
horizon lower than trajectory segment
horizon lower than trajectory segment
and then you'd have to like turn this
and then you'd have to like turn this
into 3D and then you know there are
into 3D and then you know there are
other things that make you turn it into
other things that make you turn it into
4D. So we can actually keep this
4D. So we can actually keep this
relatively
relatively
simple. That actually simplifies a lot.
simple. That actually simplifies a lot.
We do incur an extra
copy, but there's no way around that
copy, but there's no way around that
extra copy, I'm pretty
extra copy, I'm pretty
sure.
Okay, now the experience buffer is going
Okay, now the experience buffer is going
to have to work a little
differently. Is there any reason
differently. Is there any reason
to have a lower BPT horizon versus a
to have a lower BPT horizon versus a
higher
one. I used to think it was O², but it's
one. I used to think it was O², but it's
not. It's O
N. Does anybody know? Would there ever
N. Does anybody know? Would there ever
be a reason that we need BPT horizon
be a reason that we need BPT horizon
like 16 instead of
128? Like I'm thinking here, right?
And it seems very unlikely to
And it seems very unlikely to
me that we're really going to ever want
me that we're really going to ever want
to scale to giant giant batch
to scale to giant giant batch
sizes because of hardware
sizes because of hardware
efficiency. Well, that's not true. To
efficiency. Well, that's not true. To
giant unroll
giant unroll
lengths. I don't know. At least for
lengths. I don't know. At least for
Impulse, a higher BPT always seems to do
Impulse, a higher BPT always seems to do
better. Maybe if your episodes are super
better. Maybe if your episodes are super
short.
short.
Uh, in this case, if the episodes were
Uh, in this case, if the episodes were
super short, then it would just end up
super short, then it would just end up
packing multiple episodes into a single
packing multiple episodes into a single
buffer length, and that's not really a
problem. The only thing that happens is
problem. The only thing that happens is
that uh you don't get to reset your LSTM
that uh you don't get to reset your LSTM
state between episodes, and that really
state between episodes, and that really
doesn't
matter.
Also, welcome
cat. I'm back for the next three weeks
cat. I'm back for the next three weeks
or whatever. So, I will be around to
or whatever. So, I will be around to
look at any code as is need as is
needed. this week. I think most of this
needed. this week. I think most of this
week I'm going to spend really trying to
week I'm going to spend really trying to
clean up our trainer so that we can have
clean up our trainer so that we can have
basically so that we get close to
basically so that we get close to
something in the releasable state.
something in the releasable state.
Um yeah, we're going to have like a nice
Um yeah, we're going to have like a nice
stable dev branch so we can like finish
stable dev branch so we can like finish
up all the experiments and start
up all the experiments and start
thinking about a release. A lot of work
thinking about a release. A lot of work
to do for
that. I'm trying to think if this is
that. I'm trying to think if this is
ever going to bite me.
ever going to bite me.
I think the only time you end up with
I think the only time you end up with
shorter seg like shorter BPT horizon is
shorter seg like shorter BPT horizon is
when you're collecting shorter segments.
when you're collecting shorter segments.
So I don't think this ever really bites
you. Design for prioritize. Yeah. So,
you. Design for prioritize. Yeah. So,
we're redoing the experience buffer, and
we're redoing the experience buffer, and
the goal is to co-design the experience
the goal is to co-design the experience
buffer with prioritized experience
buffer with prioritized experience
replay so that uh we end up with a
replay so that uh we end up with a
simpler, faster, lower memo, like a
simpler, faster, lower memo, like a
lower mem copy overhead
lower mem copy overhead
buffer. Basically, we're trying to build
buffer. Basically, we're trying to build
a god buffer like everybody is everybody
a god buffer like everybody is everybody
always has built buffers. We're trying
always has built buffers. We're trying
to build the best
buffer. Any ideas on Oh, giant paragraph
buffer. Any ideas on Oh, giant paragraph
from Spencer there. Any ideas on speed
from Spencer there. Any ideas on speed
improvement for network design
improvement for network design
below.
Okay. Wait. When I have features encoded
Okay. Wait. When I have features encoded
like a channel versus flat, I go from
like a channel versus flat, I go from
1.1 mil to 550K. Currently with point
1.1 mil to 550K. Currently with point
and quarter 550K with flat features
1.1. Let me see what you did.
Let's help you out
Let's help you out
here and get Tokyo Puff
going. We get a 1.1
Miller. Uh, hang on. Where's this thing?
Okay.
So, so this is the flat one presumably.
So, so this is the flat one presumably.
Yeah, this is flat.
Let me think about that for a second for
Let me think about that for a second for
you.
resume flat. Well, if you think about
resume flat. Well, if you think about
it, flat's a little tricky. Um, so the
it, flat's a little tricky. Um, so the
flat encoder, it has
flat encoder, it has
no, it has to learn basically the same
no, it has to learn basically the same
representation for each point.
representation for each point.
You're basically you're asking it to
You're basically you're asking it to
learn the same set of weights for each
learn the same set of weights for each
point
point
almost, which is like you may as well
almost, which is like you may as well
just have it learn those weights once.
just have it learn those weights once.
The tricky thing though, right, is it's
The tricky thing though, right, is it's
only
only
doing it's like four to hidden state, I
doing it's like four to hidden state, I
guess.
guess.
Wait, hang on. Shouldn't that be
Wait, hang on. Shouldn't that be
strictly
better? What's the projection dimension
better? What's the projection dimension
here? So you
here? So you
do ego encoder
Is this the road
encoder? So this is linear relu
linear but in this
linear but in this
case this
case this
is 5x
64. This goes to CNN channel which is
64. This goes to CNN channel which is
64.
And then this one
And then this one
is Yeah. Okay. I see. How many weights
is Yeah. Okay. I see. How many weights
is that? Let me see. So, it's it's five
is that? Let me see. So, it's it's five
by 5*
64. Let's figure this out.
So this should be 5 * 64. This is this
So this should be 5 * 64. This is this
one
one
here. Uh and then that goes to 64 I
here. Uh and then that goes to 64 I
guess. Okay. And then the other one
is 5 * 64.
Well, it's five is the input and then 64
Well, it's five is the input and then 64
is the
is the
output. 64. It's the same number of
output. 64. It's the same number of
weights on the
input. The input should be the same
input. The input should be the same
speed, shouldn't
speed, shouldn't
it? Oh, I see the issue. Uh, one of
it? Oh, I see the issue. Uh, one of
these networks is way bigger than the
these networks is way bigger than the
other.
other.
Spencer is or not networks. One of these
Spencer is or not networks. One of these
one of these is using way more flops
one of these is using way more flops
than the
than the
other.
So I draw this for
So I draw this for
you. So you have this is the flat one,
you. So you have this is the flat one,
right? And you do like I guess to this
right? And you do like I guess to this
size here. So you do like
size here. So you do like
this. Okay. And then then this other one
this. Okay. And then then this other one
you have
like or no like like this and then it
like or no like like this and then it
does like
does like
this.
Bam. So I'm pretty sure these are the
Bam. So I'm pretty sure these are the
same sizes though, right? So the first
same sizes though, right? So the first
layer it's
layer it's
either smaller uh yeah but they're
either smaller uh yeah but they're
processed separately in parallel
processed separately in parallel
right it's separately but in
right it's separately but in
parallel and I think it's the same size
parallel and I think it's the same size
because this is this is like 5 by 64
because this is this is like 5 by 64
right and then this goes to 64 and then
right and then this goes to 64 and then
this is five and it goes to 64 but you
this is five and it goes to 64 but you
do it 64 times
Right. Yeah. So, this should be the
same. Unless I'm wrong about Hang on.
same. Unless I'm wrong about Hang on.
Does that is that how matrix multipliers
Does that is that how matrix multipliers
work? Let me
think. That might not be how matrix
think. That might not be how matrix
multiplies work. Let me think here.
multiplies work. Let me think here.
because it's technically
it's what's the squared dimension on a
it's what's the squared dimension on a
matrix
multiply cuz it's cubic
multiply cuz it's cubic
right but not for but that's only for
right but not for but that's only for
square matrices
square matrices
uh let me think so it's
input each input gets applied to
Okay, I'm going to look this up cuz the
Okay, I'm going to look this up cuz the
brain not working right now for
this
this
matrix like
matrix like
ational lexity with or actually no I'm
ational lexity with or actually no I'm
stupid. It's it's matrix vector right
stupid. It's it's matrix vector right
that's why I'm getting
confused. So uh vector
matrixity
matrixity
vector size
vector size
matrix
size what is the weight
size? It should
size? It should
be This is going to be m squ n, isn't
it? I just want to make sure I don't get
it? I just want to make sure I don't get
this
wrong. Ah, see, I'm stupid. There you
wrong. Ah, see, I'm stupid. There you
go. What's up, man? There's no squared
go. What's up, man? There's no squared
term. Why would there be?
Yeah. So if there's no squared term then
Yeah. So if there's no squared term then
then I'm right then these two should be
then I'm right then these two should be
the same number of flops. But the thing
the same number of flops. But the thing
that you missed here is the second
that you missed here is the second
layer. The second layer is a different
layer. The second layer is a different
number of flops because in the first
number of flops because in the first
case you're doing 64 by 64 and then the
case you're doing 64 by 64 and then the
second case you're doing 64 times right
second case you're doing 64 times right
uh 6 or what however many road points
uh 6 or what however many road points
there are assuming 64 road points 64
there are assuming 64 road points 64
time 64 *
time 64 *
64. So that's what you're missing.
So what I would suggest that you do for
So what I would suggest that you do for
now and this will take you all five
now and this will take you all five
minutes is um put the
max put the max like after this instead
max put the max like after this instead
and then
and then
rettime. put the max after one
layer. So you did right here there's a
layer. So you did right here there's a
discrepancy. Yeah. See you like these
discrepancy. Yeah. See you like these
two are these two networks are not
two are these two networks are not
analogous because what you did here
analogous because what you did here
right is if you look at it this first
right is if you look at it this first
layer combines all the information of
layer combines all the information of
all the points
all the points
right this first layer combines all the
right this first layer combines all the
information of all the points but here
information of all the points but here
you gave it an extra layer. So, this is
you gave it an extra layer. So, this is
actually this should be a stronger
actually this should be a stronger
network than this one here, but it's
network than this one here, but it's
going to be
going to be
slower because there's a huge additional
slower because there's a huge additional
uh ask of flops right here. This is a
uh ask of flops right here. This is a
lot of extra flops. So, I would do the
lot of extra flops. So, I would do the
max here. And if this makes the network
max here. And if this makes the network
way weaker, then we go back to something
way weaker, then we go back to something
like this. But at the very least, like
like this. But at the very least, like
this is not apples to apples. So the
this is not apples to apples. So the
apples to apples here, you would have to
apples to apples here, you would have to
make this you'd have to make this
make this you'd have to make this
dimension here way bigger or
something like this would have to be a
something like this would have to be a
bigger linear layer so that there's it
bigger linear layer so that there's it
doesn't have to compress everything on
doesn't have to compress everything on
the first
the first
layer. And that's still not quite
layer. And that's still not quite
perfect, but that's as close as you'd be
perfect, but that's as close as you'd be
able to get it.
50% slower but acting
50% slower but acting
max 50% slower but adding max there
max 50% slower but adding max there
reduces
operations. Wait 50 I don't understand
operations. Wait 50 I don't understand
the comment. So max here is slower than
the comment. So max here is slower than
max here because you're doing 64 times
max here because you're doing 64 times
the number of forward passes on this
the number of forward passes on this
layer.
Right. Difference between the two
Right. Difference between the two
versions right now is 50% slower. I'd
versions right now is 50% slower. I'd
have to look at the rest of the network
have to look at the rest of the network
to see if I think 50% is reasonable. I
to see if I think 50% is reasonable. I
mean, first principles, it could it
mean, first principles, it could it
could very well be because this is a
could very well be because this is a
tiny network and you're basically doing
tiny network and you're basically doing
64 times the number of forward passes in
64 times the number of forward passes in
this right now.
And this is a big increase in
complexity. Just like run it, right?
complexity. Just like run it, right?
It'll take you five minutes to just flip
It'll take you five minutes to just flip
it around and run
it. And if it's still slower, then come
it. And if it's still slower, then come
back and we'll chat about it and we'll
back and we'll chat about it and we'll
figure out what we can
figure out what we can
do. But that is kind of the standard
do. But that is kind of the standard
entity encoder style
network. Okay.
network. Okay.
Back to back to experience
buffers. How's max there changed things?
buffers. How's max there changed things?
Sorry, still lost. Okay, let me see if I
Sorry, still lost. Okay, let me see if I
can explain this. So, what you did,
can explain this. So, what you did,
right, in the first
right, in the first
one, yeah, I'll I'll explain this real
one, yeah, I'll I'll explain this real
quick. So, in the first one, right, you
quick. So, in the first one, right, you
take your encoder, you have all these
take your encoder, you have all these
all these big observations and you
all these big observations and you
encode them to 60, where is it? to 64 by
encode them to 60, where is it? to 64 by
64. So or 64. So this is
64. So or 64. So this is
64. All right. And then you do 64 x
64. All right. And then you do 64 x
64. Right? So if you have one sample,
64. Right? So if you have one sample,
this is now a 64 vector to a 64 64 uh
this is now a 64 vector to a 64 64 uh
vector matrix.
vector matrix.
Right? So it looks like this.
Okay. But what happens on this other one
Okay. But what happens on this other one
where you do all these little encodings?
where you do all these little encodings?
This is a batch,
This is a batch,
right? So you do like all these little
right? So you do like all these little
encodings. What you end up with here is
encodings. What you end up with here is
not a 64dimensional vector. Uh this is
not a 64dimensional vector. Uh this is
excluding the batch dimension. So we're
excluding the batch dimension. So we're
not even talking about the batch
not even talking about the batch
dimension. This is now 64
dimension. This is now 64
by
by
64. Okay. And then you're doing a 64x 64
multiply. So this is
multiply. So this is
n^2. This is n
cubed because you still have all the
cubed because you still have all the
road points, right? You haven't maxed
road points, right? You haven't maxed
over them yet.
doing the max on n2. Well, you do the
doing the max on n2. Well, you do the
max after this, right? And then you you
max after this, right? And then you you
do max
do max
here. So the output of this, hang on. So
here. So the output of this, hang on. So
the output of this here, this is going
the output of this here, this is going
to give you
to give you
64 or hang on. This
64 or hang on. This
is this is
is this is
64. The output of this is 64x
64. And then you do max here.
64. And then you do max here.
right to get
64. So this operation here is 64 times
64. So this operation here is 64 times
more expensive than the operation
more expensive than the operation
here. So placing it earlier achieves a
here. So placing it earlier achieves a
similar effect but less multiplies. So
similar effect but less multiplies. So
placing it earlier
placing it earlier
essentially the networks become the same
essentially the networks become the same
after the first
after the first
layer. So you do your combine operation
layer. So you do your combine operation
in the first layer because this does the
in the first layer because this does the
combine in the first layer. Right? This
combine in the first layer. Right? This
top network combines all the information
top network combines all the information
from all these points in the first
from all these points in the first
layer. Once you go here, you can no
layer. Once you go here, you can no
longer distinguish between points. In
longer distinguish between points. In
your network here, you can distinguish
your network here, you can distinguish
between points until you do this max
between points until you do this max
over here.
I'm not saying that this is necessarily
I'm not saying that this is necessarily
even a bad architecture. It might be
even a bad architecture. It might be
that this this architecture could end up
that this this architecture could end up
being essential even and this is what
being essential even and this is what
you would do if you wanted to scale your
you would do if you wanted to scale your
network up a little bit without
network up a little bit without
increasing parameter count or whatever.
increasing parameter count or whatever.
But it's 64 times as
But it's 64 times as
expensive this layer is. Now this layer
expensive this layer is. Now this layer
is only a small part of your overall
is only a small part of your overall
network. So it works out to be about 50%
network. So it works out to be about 50%
slower. And you also kind of get economy
slower. And you also kind of get economy
as a scale because GPU go burr. But um
as a scale because GPU go burr. But um
that's what you're doing right
that's what you're doing right
here. You
see if you max here
see if you max here
instead if you max right
here. I don't know why I wrote name
here. I don't know why I wrote name
instead of max.
instead of max.
Then you get a 64dimensional
Then you get a 64dimensional
vector. And now this is exactly the same
vector. And now this is exactly the same
as up here.
Right? That's the
key. Do you see how I think about this
key. Do you see how I think about this
now though? It's not even that your
now though? It's not even that your
network is bad. It's just that this is
network is bad. It's just that this is
going to be way more flops than the top
going to be way more flops than the top
one. Quick followup. GPU D group does
one. Quick followup. GPU D group does
tanh instead of RLU and add normalized
tanh instead of RLU and add normalized
layer between linear. Any reason for me
layer between linear. Any reason for me
to do
to do
that? I've seen that in robotic stuff
that? I've seen that in robotic stuff
before. There's something to tanh over
before. There's something to tanh over
relu because there's like this big
relu because there's like this big
control study where for some reason it
control study where for some reason it
was better for control problems with
was better for control problems with
continuous outputs.
Um, I have no idea why that would be
Um, I have no idea why that would be
better and it's kind of sucky because
better and it's kind of sucky because
like tanh is just a shitty activation.
like tanh is just a shitty activation.
This is what we used back in like 2015
This is what we used back in like 2015
or before 2015. We used tanh just cuz we
or before 2015. We used tanh just cuz we
didn't know better and then the nor the
didn't know better and then the nor the
layer norms can be very
layer norms can be very
slow. So they said that they were really
slow. So they said that they were really
needed uh in order to make their stuff
needed uh in order to make their stuff
work. Do I trust that it wasn't just bad
work. Do I trust that it wasn't just bad
hyperparameters? So, like this was
hyperparameters? So, like this was
band-aid patch for bad hyperparameters.
band-aid patch for bad hyperparameters.
No, it could very well have just been
No, it could very well have just been
like band-aid patch for bad hypers and
like band-aid patch for bad hypers and
whatever, which would be fine if it
whatever, which would be fine if it
weren't slow. And I think it is going to
weren't slow. And I think it is going to
be slow. You can try it, but if it's
be slow. You can try it, but if it's
slow, that'll be expected. Can it
slow, that'll be expected. Can it
shouldn't really be any slower. But um
yeah, I'm also I'm also going to be
yeah, I'm also I'm also going to be
interested to see if like there's a big
interested to see if like there's a big
difference in perf now with
Muan cuz Muan should help smooth some of
Muan cuz Muan should help smooth some of
the perks that we were getting
the perks that we were getting
before. Okay, are we good on this
before. Okay, are we good on this
though? Just ban that bot.
Sweep with Muan. Yes. Good. Okay. Solid.
Sweep with Muan. Yes. Good. Okay. Solid.
I'm So, my goal for this week, Spencer,
I'm So, my goal for this week, Spencer,
is um I'm going to try to get this
is um I'm going to try to get this
experience buffer and the training code
experience buffer and the training code
into a spot where it's not insane. So,
into a spot where it's not insane. So,
like you'll actually be able to use to
like you'll actually be able to use to
do stable work on it. That's the goal.
do stable work on it. That's the goal.
Mind taking a quick look at
Mind taking a quick look at
multi-iset multi-discreet actions.
Oh, yeah. Also, I think I think you both
Oh, yeah. Also, I think I think you both
probably saw, but I posted in the
probably saw, but I posted in the
uh the
uh the
Discord one thing.
Oops. Sorry, I just had answer
Oops. Sorry, I just had answer
something. And I think you both saw, but
something. And I think you both saw, but
this is what I was looking at over the
weekend. This is the capacity right here
weekend. This is the capacity right here
for new
for new
machines. We're going to have two foot
machines. We're going to have two foot
like just storage racks in front of
like just storage racks in front of
these for desktops.
these for desktops.
40 ports and then uh there's going to be
40 ports and then uh there's going to be
a 42U rack sitting right here where we
a 42U rack sitting right here where we
can stick any server grade equipment uh
can stick any server grade equipment uh
if we get any we can stick right in
if we get any we can stick right in
front of
there and it'll all be on a UPS so we
there and it'll all be on a UPS so we
won't have screwy like power out drops
won't have screwy like power out drops
screwing us over anymore. We might still
screwing us over anymore. We might still
have internet because we cannot
have internet because we cannot
get a 10 gig line uh to the facility.
get a 10 gig line uh to the facility.
So, we're going to have just a one gig
So, we're going to have just a one gig
line to start with and then uh you know,
line to start with and then uh you know,
I might just get like extra Starlink
I might just get like extra Starlink
backups because that's not going to be
backups because that's not going to be
enough
enough
bandwidth. Uh it is it's a pretty huge
bandwidth. Uh it is it's a pretty huge
UPS. It's like it's a big
UPS. It's like it's a big
box. Yeah. So, we get this and then all
box. Yeah. So, we get this and then all
the space that is not being used by
the space that is not being used by
servers gets used by for a gym
servers gets used by for a gym
equipment. So, facility also gets to be
equipment. So, facility also gets to be
my
my
gym. It'll be fun. I'm having a good
gym. It'll be fun. I'm having a good
time going through and like figuring out
time going through and like figuring out
what to order for
it. And this will be ready in about 3
it. And this will be ready in about 3
weeks. Don't drop weights on the
weeks. Don't drop weights on the
servers. The entire floor is going to be
servers. The entire floor is going to be
3/4 inch rubber mat. We're getting the
3/4 inch rubber mat. We're getting the
mats installed in probably two to 3
mats installed in probably two to 3
weeks.
So like that's solid that's a solid
So like that's solid that's a solid
chunk of
rubber. Yeah. 3/4 inch. Yeah. So I went
rubber. Yeah. 3/4 inch. Yeah. So I went
I went extra heavy on it just to be
I went extra heavy on it just to be
absolutely sure that we will be good
there. I might even have to do something
there. I might even have to do something
thicker for the uh deadlift area. We'll
thicker for the uh deadlift area. We'll
but we'll figure that out when we get
but we'll figure that out when we get
there.
there.
Um, I'm not really deadlifting any
Um, I'm not really deadlifting any
reasonable amount of weight at the
reasonable amount of weight at the
moment. So, no problem for a
moment. So, no problem for a
bit. I'm just trying to get my body
bit. I'm just trying to get my body
weight
back. But yeah, that's what I've been
back. But yeah, that's what I've been
doing. Let's take a look at your
sweep. Okay, it's a cool
sweep. Okay, it's a cool
sweep. This is impulse force. Looks like
sweep. This is impulse force. Looks like
it.
very high
gamma. That's somewhat expected. We
gamma. That's somewhat expected. We
might even want increase maximum gamma
might even want increase maximum gamma
for you because it's this
high
high
lambda. Yeah. Lowish
lambda. Oh jeez, you really cranked this
lambda. Oh jeez, you really cranked this
thing. Does this even work? You have to
thing. Does this even work? You have to
have a really large batch size for this
have a really large batch size for this
to even H. That's
interesting. Yeah, I know. I I did the
interesting. Yeah, I know. I I did the
both I did both of those dashboards
both I did both of those dashboards
right in my setup. I'm looking at this
right in my setup. I'm looking at this
first.
This actually Yeah, this appears to keep
This actually Yeah, this appears to keep
doing
better.
better.
M. Very small number of M's though. Is
M. Very small number of M's though. Is
this the uh You probably have several
this the uh You probably have several
cores here, right? So, this is probably
cores here, right? So, this is probably
like M's per core and you probably have
like M's per core and you probably have
like several cores, right?
Mini batch size
8192 cores. Yeah. So that's a good
8192 cores. Yeah. So that's a good
number of environments then. That's what
number of environments then. That's what
is that uh 20 48 ms for this. That's
is that uh 20 48 ms for this. That's
reasonable.
Hey, you can share your absolutely
Hey, you can share your absolutely
god-awful learning curves and uh win a
god-awful learning curves and uh win a
gift box.
So, we just submit them our entire
So, we just submit them our entire
sweeps on
everything. It's like, yeah, this is
everything. It's like, yeah, this is
just daily life for us. Thank
you. See, stuff like this kind of
you. See, stuff like this kind of
bothers me.
Stuff like this kind of bothers me. It
Stuff like this kind of bothers me. It
really shouldn't be this sensitive to a
really shouldn't be this sensitive to a
little bit of
little bit of
entropy,
right? So, it doesn't even bother me
right? So, it doesn't even bother me
that it
that it
does like I don't even like expect to do
does like I don't even like expect to do
better with some entropy, but like it
better with some entropy, but like it
shouldn't crash learning with entropy.
shouldn't crash learning with entropy.
Like you should expect this to kind of
Like you should expect this to kind of
be flat right here is what I would
be flat right here is what I would
expect. I would expect some runs with
expect. I would expect some runs with
like decent PF at slightly higher
like decent PF at slightly higher
entropy. It's very sensitive to that
entropy. It's very sensitive to that
which is
concerning. This is like better, right?
concerning. This is like better, right?
This is kind of like a flat region here.
This is kind of like a flat region here.
You can see kind of like you want to
You can see kind of like you want to
increase your value function
increase your value function
coefficient, but it's kind of stable
coefficient, but it's kind of stable
over here.
Oh, and then we're still at like baby
Oh, and then we're still at like baby
levels of time steps
levels of time steps
here. 600 million. Get out of here. I
here. 600 million. Get out of here. I
did a five billion step neural MMO sweep
did a five billion step neural MMO sweep
over the weekend. Each run was 5 billion
steps with bigger networks as well.
multi-discreet without torch compile.
multi-discreet without torch compile.
So, we'll have to fix that. I thought I
So, we'll have to fix that. I thought I
merged that PR from that guy. Did that
merged that PR from that guy. Did that
not
help? This is still going up though. You
help? This is still going up though. You
can just keep this
going. 60 to 80%. Okay. So, that's weird
going. 60 to 80%. Okay. So, that's weird
because I don't get anything out of
because I don't get anything out of
torch compile in my in my setups. So,
torch compile in my in my setups. So,
something screwy is going on
there. I don't have continuous actions.
there. I don't have continuous actions.
I have discrete actions and I don't get
I have discrete actions and I don't get
much from compile at
much from compile at
all. It's often slower. In fact,
Looks like a very reasonable sweep. Was
Looks like a very reasonable sweep. Was
there anything in particular that uh you
there anything in particular that uh you
were wondering about? Because this looks
were wondering about? Because this looks
I mean what you want to do is you want
I mean what you want to do is you want
to go over to
to go over to
uh to here and then what does it
win? Little concerning how shallow. Some
win? Little concerning how shallow. Some
of these curves
are not on dev. Okay. So, I probably
are not on dev. Okay. So, I probably
just probably just something broken
just probably just something broken
then. We'll have to sit down and find
then. We'll have to sit down and find
that at some point. Like I said, I'm
that at some point. Like I said, I'm
around all week. I have or next couple
around all week. I have or next couple
weeks I've got pretty much nothing
weeks I've got pretty much nothing
planned than just solid dev. And then I
planned than just solid dev. And then I
go to the warehouse and then I have
go to the warehouse and then I have
absolutely nothing planned but just
absolutely nothing planned but just
solid dev when I get
there. Hang on a second.
Oops. Okay. Had to answer a
Oops. Okay. Had to answer a
thing. Mostly I was curious why ENT was
thing. Mostly I was curious why ENT was
so low and if I should do a new sweep
so low and if I should do a new sweep
with increased back prop horizon. Best
with increased back prop horizon. Best
runs mini batch
runs mini batch
seemed low to me. Yeah, that'll happen.
seemed low to me. Yeah, that'll happen.
You just use I think that the best thing
You just use I think that the best thing
you can do captain is you just you use
you can do captain is you just you use
the parameters that the sweep tells you
the parameters that the sweep tells you
and you keep improving infrastructure. I
and you keep improving infrastructure. I
have noticed as you improve
have noticed as you improve
infrastructure and things that are fast
infrastructure and things that are fast
uh that should be fast become fast the
uh that should be fast become fast the
hyperparameters get better because you
hyperparameters get better because you
have to remember the access that it's
have to remember the access that it's
controlling is wall clock. So like some
controlling is wall clock. So like some
part being slow that's not supposed to
part being slow that's not supposed to
be slow can actually change optimal
be slow can actually change optimal
hypers and make things unintuitive.
I am a little suspicious how shallow
I am a little suspicious how shallow
these graphs are. Oh yeah. What's your
these graphs are. Oh yeah. What's your
SPS
at? 100 to
at? 100 to
250k. My
250k. My
guy. O, that is rough.
I'm pretty sure I'm running like I'm
I'm pretty sure I'm running like I'm
running three million parameter necks.
running three million parameter necks.
Um twice as fast as your fastest run
here. You must be hard m bottlenecked,
here. You must be hard m bottlenecked,
right? Or you are you net bottleneck or
right? Or you are you net bottleneck or
m bottleneck? What's the dashboard show
m bottleneck? What's the dashboard show
you?
320. That's still
low. So, you've
low. So, you've
got 500 seconds M
got 500 seconds M
time. You have a crazy amount of copy
time. You have a crazy amount of copy
bandwidth right here.
Your M actually is not big. Like this is
Your M actually is not big. Like this is
not Yeah, your M time. You're not even
not Yeah, your M time. You're not even
bottlenecked by your N here. Look,
bottlenecked by your N here. Look,
you're getting screwed over by
you're getting screwed over by
copies. I'm going to improve that a
copies. I'm going to improve that a
little bit for you, but you also need to
little bit for you, but you also need to
check the size of your uh your data
check the size of your uh your data
packets there. So, this is probably
packets there. So, this is probably
what's that 20% of your eval time spent
what's that 20% of your eval time spent
in that. And then let's see, MISK is
in that. And then let's see, MISK is
good. 63. Yep. That's still not crazy
good. 63. Yep. That's still not crazy
though. Oh, you have like zero learn
time. Wait,
what? Nah, this is there's I don't know
what? Nah, this is there's I don't know
what the heck the reason the solution is
what the heck the reason the solution is
here, but something's just trolling
here, but something's just trolling
here. This is like hard trolling.
train
train
and 63
and 63
eval. So more than 80% time spent in
eval. So more than 80% time spent in
Eval is hard
Eval is hard
trolling. Yeah, that is hard trolling.
trolling. Yeah, that is hard trolling.
Um, my guess
Um, my guess
is I just pick a random one of
these. There's something wrong if
these. There's something wrong if
multi-discrete actions are that slow.
multi-discrete actions are that slow.
I mean that's like a trivial
operation. Let me check something real
quick. 4 m batch
size. Wait 16
amps. you have this quad
buffered. So that's only
buffered. So that's only
512. You only have 512 batch size in the
512. You only have 512 batch size in the
forward
forward
pass. You're definitely not going to
pass. You're definitely not going to
load the GPU enough with
load the GPU enough with
that. I'm guessing that you have it quad
that. I'm guessing that you have it quad
buffered because when you double buffer
buffered because when you double buffer
it, the uh the M percentage gets to be
it, the uh the M percentage gets to be
quite large or something.
300k is is still not considered
300k is is still not considered
reasonable, captain. Like we we really
reasonable, captain. Like we we really
should have this thing at 500 plus. If I
should have this thing at 500 plus. If I
can have neural MMO at 500 plus, we
can have neural MMO at 500 plus, we
should be able to have this 500
should be able to have this 500
plus. Like here, I'll show you
my because I have a bigger net as well.
I guess this thing crashed at some
I guess this thing crashed at some
point, but we got 24 runs in
here. Couple of these actually did
okay.
Couple runs did
okay. What was my
okay. What was my
SPS? So, pretty much all these are at
500k. Eval doesn't know up. I think I
500k. Eval doesn't know up. I think I
just get 500k. Well, then the re then
just get 500k. Well, then the re then
the question is going to be why? Because
the question is going to be why? Because
um you have 16 cores.
um you have 16 cores.
With Puffer, you should be able to get
With Puffer, you should be able to get
pretty damn close to linear speed up.
pretty damn close to linear speed up.
So, unless the MV is running at like
So, unless the MV is running at like
30k, you should be able to get faster
30k, you should be able to get faster
than
that, right?
And this is again this is with a big
And this is again this is with a big
network as well like I think this is
with hidden
size. Yes. So, this is with a 512 512 R
LSTM. Ford pass is the big one. MS speed
LSTM. Ford pass is the big one. MS speed
was not your biggest uh your biggest
was not your biggest uh your biggest
line item at all. I checked your PF,
line item at all. I checked your PF,
right? Your line uh your M speed really
right? Your line uh your M speed really
wasn't that big of a deal.
It was a pretty small
It was a pretty small
fraction. Your copy time was bigger than
fraction. Your copy time was bigger than
your M time
your M time
even. So, I'll help you play with
even. So, I'll help you play with
that. But, uh, definitely we need to do
that. But, uh, definitely we need to do
some stuff on that.
some stuff on that.
I think what we're going to probably do
I think what we're going to probably do
is I'll do
is I'll do
um if people aren't free in the
um if people aren't free in the
evenings, I'll do some polls, you know,
evenings, I'll do some polls, you know,
during the day. But I'll try to make
during the day. But I'll try to make
myself more available than I've been
myself more available than I've been
doing in the evenings, you know, to just
doing in the evenings, you know, to just
to chat uh to chat through
to chat uh to chat through
stuff, you know, over dinner and then
stuff, you know, over dinner and then
after
after
dinner. Continuous actions though,
dinner. Continuous actions though,
that's more accurate to when torch
that's more accurate to when torch
compile works with multi-discreet
compile works with multi-discreet
actions.
The thing is that we should just be able
The thing is that we should just be able
to make multi-discreet fast. There's no
to make multi-discreet fast. There's no
reason that should be slow. So I'm going
reason that should be slow. So I'm going
I'm believing you that it is slow,
I'm believing you that it is slow,
right? But there's no reason that it
right? But there's no reason that it
should be. So we should just be able to
should be. So we should just be able to
make that
fast.
300k.
Yeah. Anything else I can do right now
Yeah. Anything else I can do right now
for you, Captain? Like that's a
for you, Captain? Like that's a
reasonable sweep. I would just yep keep
reasonable sweep. I would just yep keep
running that stuff.
Um, how like what else do you want to
Um, how like what else do you want to
do? I we can save this for like a later
do? I we can save this for like a later
conversation, but like start thinking
conversation, but like start thinking
about like what you want to do uh with
about like what you want to do uh with
impulse wars before you release it,
impulse wars before you release it,
right? Is the goal is there more content
right? Is the goal is there more content
that has to go in? Is there UI stuff
that has to go in? Is there UI stuff
that needs to be cleaned up? Is there
that needs to be cleaned up? Is there
perf stuff to do? Or do you at this
perf stuff to do? Or do you at this
point just want a really strong
point just want a really strong
baseline?
That's what I would encourage you to do.
That's what I would encourage you to do.
Like think of how you want to think of
Like think of how you want to think of
like what you want to do for the release
like what you want to do for the release
of this thing.
these small things for 3D graphics and
these small things for 3D graphics and
then baseline. Gotcha. So, I mean, I can
then baseline. Gotcha. So, I mean, I can
show you some of the stuff I've done for
show you some of the stuff I've done for
the neural MMO baseline.
the neural MMO baseline.
Um, are you on the latest code with Muan
Um, are you on the latest code with Muan
and everything at this point or no?
Okay, good. So, if you're on the latest
Okay, good. So, if you're on the latest
code using Muon, then um you should be
code using Muon, then um you should be
able to get some stable runs and the
able to get some stable runs and the
only thing that's really hamstringing
only thing that's really hamstringing
you should be the M speed because like
you should be the M speed because like
you know billions, not mill not hundreds
you know billions, not mill not hundreds
of millions. Old bindings bindings
of millions. Old bindings bindings
aren't really going to give you any
aren't really going to give you any
speed. Bindings are just going to
speed. Bindings are just going to
simplify things.
simplify things.
I could be wrong, but I haven't seen
I could be wrong, but I haven't seen
major speed ups with
bindings. Neural MMO got like 5 10%
bindings. Neural MMO got like 5 10%
faster and then like pawn got a little
faster and then like pawn got a little
bit slower. I think neural MMO is
bit slower. I think neural MMO is
probably the better comparison. So you
probably the better comparison. So you
might get a little bit of
speed. I think you get link time
speed. I think you get link time
optimization now as well. You don't
optimization now as well. You don't
screw up link time optimization anymore.
my next sweep. Yeah, you can just bump
my next sweep. Yeah, you can just bump
it up. Um, you can bump that up. It's
it up. Um, you can bump that up. It's
fine. But like honestly, I just I did a
fine. But like honestly, I just I did a
fixed five bill sweep lately for neural
fixed five bill sweep lately for neural
MMO that did okay. And then I also do I
MMO that did okay. And then I also do I
start doing like some individual
start doing like some individual
experiments. So like if do you have a
experiments. So like if do you have a
256 hidden dim already or do you have
256 hidden dim already or do you have
128? Because 128's probably too small
128? Because 128's probably too small
for that problem.
for that problem.
I got a little bit of fur out of doing
I got a little bit of fur out of doing
even
even
512 56. Okay, good. I bumped neural MMO
512 56. Okay, good. I bumped neural MMO
up to 512 and the network really wasn't
up to 512 and the network really wasn't
that much slower and I got a little bit
that much slower and I got a little bit
of improvement out of that.
of improvement out of that.
Um, you got to make sure that you're not
Um, you got to make sure that you're not
bottlenecking your network by a very
bottlenecking your network by a very
small like number of con filters or
small like number of con filters or
something. You got to make sure that
something. You got to make sure that
whatever is coming out of the CNN isn't
whatever is coming out of the CNN isn't
tiny. I got a little bit of pf out of
tiny. I got a little bit of pf out of
dealing with that.
Um, you might need to look at your
Um, you might need to look at your
seating. I got a little bit of perf out
seating. I got a little bit of perf out
of fixing my
of fixing my
seating. I sweep over hidden dim. I
seating. I sweep over hidden dim. I
don't know if you'd sweep over hidden
don't know if you'd sweep over hidden
dim because
dim because
like I technically have it in place that
like I technically have it in place that
you should be able to, but um there's
you should be able to, but um there's
still some screwy things with that,
still some screwy things with that,
especially around memory usage. So, I
especially around memory usage. So, I
probably wouldn't do that. You can kind
probably wouldn't do that. You can kind
of just run one sweep at higher hitting
of just run one sweep at higher hitting
them and one at
lower chatting with someone on the side
lower chatting with someone on the side
who's done similar experience buffer
who's done similar experience buffer
Heat. Heat.
512 hidden demise from CNN. What
512 hidden demise from CNN. What
else? You can look at some of my latest
else? You can look at some of my latest
um neural MMO three sweeps, I think. But
um neural MMO three sweeps, I think. But
I like those were the big ones. The only
I like those were the big ones. The only
other thing that I had to deal with was
other thing that I had to deal with was
uh seeding. seeding actually made a
uh seeding. seeding actually made a
decent difference. I just added that
decent difference. I just added that
into the new binding code. So, you're
into the new binding code. So, you're
not going to have access to that yet.
not going to have access to that yet.
Um, if you really want to do something
Um, if you really want to do something
short-term with that, then you probably
short-term with that, then you probably
just need to find like, and I didn't do
just need to find like, and I didn't do
this because it's slightly annoying, but
this because it's slightly annoying, but
if you get the clock time in nanoseconds
if you get the clock time in nanoseconds
instead of in seconds, you should be
instead of in seconds, you should be
able to guarantee unique seeds for all
able to guarantee unique seeds for all
the M's. Um, I think basically what
the M's. Um, I think basically what
happened before is that because I was
happened before is that because I was
using time in seconds, uh, all the maps
using time in seconds, uh, all the maps
were getting the same seed. You got to
were getting the same seed. You got to
be careful with that. That made a decent
difference. If you're using a random
difference. If you're using a random
seed for every process, you still need a
seed for every process, you still need a
random seed for every environment,
random seed for every environment,
right?
I guess it kind of
I guess it kind of
depends. Incremental. Okay. If you're
depends. Incremental. Okay. If you're
already seating, then that should be
already seating, then that should be
fine. Yeah. If you're already seating,
fine. Yeah. If you're already seating,
then that should be
fine. Well, this is a lot of the science
fine. Well, this is a lot of the science
side stuff, right? These are the harder
side stuff, right? These are the harder
M's. They're going to take a little bit
M's. They're going to take a little bit
of work to get to be uh you know nice
of work to get to be uh you know nice
and functional. And um I mean kind of in
and functional. And um I mean kind of in
my head though once we have if we can
my head though once we have if we can
like get clean
like get clean
uh clean RL like just clean simple RL
uh clean RL like just clean simple RL
working on your end on neural MMO 3
working on your end on neural MMO 3
maybe on a couple others maybe on mobile
maybe on a couple others maybe on mobile
with selfplay not just against shitty
with selfplay not just against shitty
scripted bots that's kind of like good
scripted bots that's kind of like good
for release and probably already better
for release and probably already better
than uh a lot of the problems in
than uh a lot of the problems in
industry like
industry like
Frankly, there are a lot of valuable
Frankly, there are a lot of valuable
problems in industry that are just flat
problems in industry that are just flat
out simpler than Impulse
out simpler than Impulse
Wars. They don't look like games, right?
Wars. They don't look like games, right?
But the actual learning dynamics are way
But the actual learning dynamics are way
simpler than your problem. So, I think
simpler than your problem. So, I think
that'll be uh that'll be the next
that'll be uh that'll be the next
milestone goal, right? Is we have solid
milestone goal, right? Is we have solid
simple RL working on your amp, working
simple RL working on your amp, working
on my amp. We ship, you know, we ship
on my amp. We ship, you know, we ship
your AMP, we ship my amp, we ship some
your AMP, we ship my amp, we ship some
of the stuff Spencer's been doing. And
of the stuff Spencer's been doing. And
uh that's the next release. Then we'll
uh that's the next release. Then we'll
see what we do from there. From there,
see what we do from there. From there,
it depends on how much stuff is left to
it depends on how much stuff is left to
fix versus how much I'd like say, "Okay,
fix versus how much I'd like say, "Okay,
let's scale this now. Get some industry
let's scale this now. Get some industry
contracts
contracts
going." Sucks because continuous actions
going." Sucks because continuous actions
are much faster. Well, we'll just fix
are much faster. Well, we'll just fix
that, Captain. Because they shouldn't
be. We'll just fix
be. We'll just fix
that. Uh, is this is this going to be
that. Uh, is this is this going to be
the Neoim tweet? Am I terminally online
the Neoim tweet? Am I terminally online
and know the post? Oh, no. This is
and know the post? Oh, no. This is
another one of his memes.
Yeah.
Yeah. The funny thing, Tyler, is like
Yeah. The funny thing, Tyler, is like
I'm way happier and way more productive
I'm way happier and way more productive
when I just turn off X and I just stop
when I just turn off X and I just stop
interacting with shitty programmers with
interacting with shitty programmers with
absolutely terrible tech
absolutely terrible tech
takes. That's like literally that's all
takes. That's like literally that's all
there is to it. Like they post all this
there is to it. Like they post all this
progress, but it's all just it's all
progress, but it's all just it's all
smoke and mirrors. Like of course they
smoke and mirrors. Like of course they
haven't built anything because they
haven't built anything because they
don't know how to build anything. It
don't know how to build anything. It
it's all lies.
it's all lies.
Like bothers the hell out of me. But you
Like bothers the hell out of me. But you
can just turn it off and you're not
can just turn it off and you're not
missing
missing
anything. Impulse word better not be
anything. Impulse word better not be
100% vibe coded. Captain
Yeah, exactly. I'd
Yeah, exactly. I'd
know cuz I'd have criticisms of your
know cuz I'd have criticisms of your
code, not just the uh the Box 2D.
code, not just the uh the Box 2D.
I still think, by the way, that because
I still think, by the way, that because
of the way box 2D is designed, there's
of the way box 2D is designed, there's
probably twice as much code as there
probably twice as much code as there
needs to be in uh in Impulse Wars.
needs to be in uh in Impulse Wars.
There's like a massive amount of boiler
There's like a massive amount of boiler
plate there with the way that physics
plate there with the way that physics
engine is set
engine is set
up. So, I still think that's the
case. I think the box 2D internals are
case. I think the box 2D internals are
very good. Um, no. Aaron seems to know
very good. Um, no. Aaron seems to know
what he's doing on the internals, but
what he's doing on the internals, but
uh, definitely the interface of it is a
uh, definitely the interface of it is a
little
little
bit it's a little sus. It's like a
bit it's a little sus. It's like a
suspiciously highle interface for a
suspiciously highle interface for a
low-level
library. All
right. I
right. I
mean, I don't know.
mean, I don't know.
I know I get very pedantic over like um
I know I get very pedantic over like um
code length and code quality stuff, but
code length and code quality stuff, but
it really makes a difference.
it really makes a difference.
Like it it's kind of like the recurring
Like it it's kind of like the recurring
cost of your library is the number of
cost of your library is the number of
lines of code or like I guess the
lines of code or like I guess the
complexity of your code is the recurring
complexity of your code is the recurring
cost of like maintenance and changing
cost of like maintenance and changing
stuff and what it's like if you can
stuff and what it's like if you can
solve a problem that would be like that.
solve a problem that would be like that.
The fang people would take a million
The fang people would take a million
lines of code and you solve it in 50,000
lines of code and you solve it in 50,000
lines. Like you don't have the same
lines. Like you don't have the same
thing, right? Yours is infinitely more
valuable is you can actually do stuff
valuable is you can actually do stuff
with
it. Plus, you got to have some
it. Plus, you got to have some
selfrespect. Okay. Does anybody have any
selfrespect. Okay. Does anybody have any
other things for me to review right now
other things for me to review right now
or can I get back to doing the
or can I get back to doing the
experience
buffer makes sense to only keep pure RL
buffer makes sense to only keep pure RL
eel training and move other stuff to
eel training and move other stuff to
other module. I was thinking about doing
other module. I was thinking about doing
the opposite. I was thinking about
the opposite. I was thinking about
combining the demo file with clean puff
combining the demo file with clean puff
RL and then just working really hard to
RL and then just working really hard to
make that file
shorter. Hiding code in other files
shorter. Hiding code in other files
doesn't make it shorter.
logging for
logging for
example.
example.
So the reason that there's nothing
So the reason that there's nothing
fundamentally good about putting all the
fundamentally good about putting all the
code in one file, right? Like it removes
code in one file, right? Like it removes
a few lines of imports. That's it. Um,
a few lines of imports. That's it. Um,
the reason I tend to do it is because
the reason I tend to do it is because
when you have all your code in one
when you have all your code in one
place, it makes it easier to see that
place, it makes it easier to see that
you're being stupid when you've missed
you're being stupid when you've missed
like co-optimizations where like if you
like co-optimizations where like if you
have module A and module B, it needs to
have module A and module B, it needs to
be a lot of code, but if you just
be a lot of code, but if you just
redesign them into one thing, it's
redesign them into one thing, it's
shorter. That's why I do it.
Um, yeah, that's why I do it.
Um, yeah, that's why I do it.
I'm going to take a pass at like redoing
I'm going to take a pass at like redoing
a whole bunch of the stuff anyways and
a whole bunch of the stuff anyways and
then we'll see where we end up. I'm
then we'll see where we end up. I'm
hoping we can just make a lot of it
hoping we can just make a lot of it
shorter anyways. But I don't see why
shorter anyways. But I don't see why
like I don't know why we would put stuff
like I don't know why we would put stuff
that's only ever going to be useful for
that's only ever going to be useful for
clean puffl that's been designed
clean puffl that's been designed
specifically for that trainer into the
specifically for that trainer into the
internals of puffer
lab. Heck, there's a lot of stuff in
lab. Heck, there's a lot of stuff in
puffer I just like to straight up
puffer I just like to straight up
delete, right? I'd really love it if we
delete, right? I'd really love it if we
got to a spot where like puffer core is
got to a spot where like puffer core is
a few thousand lines and like a very
a few thousand lines and like a very
small number of useful tools and then
small number of useful tools and then
all the work kind of starts being more
all the work kind of starts being more
more and more uh low-level stuff in C
more and more uh low-level stuff in C
more like shipping our own CUDA modules
more like shipping our own CUDA modules
more stuff like
that aside from the obvious algorithmic
All
All
right. Anybody have any other stuff or
right. Anybody have any other stuff or
do I go back to experience
do I go back to experience
buffer? Cuz experience buffer is going
buffer? Cuz experience buffer is going
to be hard and I want to actually be
to be hard and I want to actually be
able to focus on it for a few solid
hours. Okay.
hours. Okay.
likely we go back to experience
likely we go back to experience
buffer.
buffer.
So, let me
So, let me
think. We've kind of figured out. Let me
think. We've kind of figured out. Let me
just read this one
just read this one
message. This other guy can't really
message. This other guy can't really
tell me that much about what he's done,
tell me that much about what he's done,
but somebody did a
but somebody did a
buffer. Before you start back, has
buffer. Before you start back, has
anybody got priority experience buffer
anybody got priority experience buffer
working well with on policy? I don't
working well with on policy? I don't
know if they have in the way that I'm
know if they have in the way that I'm
doing it. Um, but basically one of the
doing it. Um, but basically one of the
things I figured out over the weekend,
things I figured out over the weekend,
yeah, let's just say that there's going
yeah, let's just say that there's going
to be an article titled on policy
to be an article titled on policy
learning is a lie coming out pretty
learning is a lie coming out pretty
soon. Let's just say
that I looked at the math a bit and uh
that I looked at the math a bit and uh
it don't make
sense. So, that's going to be fun.
vententral protein. There'll be some
vententral protein. There'll be some
articles. I don't mind writing blog
articles. I don't mind writing blog
posts now and again. I just hate writing
posts now and again. I just hate writing
papers. So, there will be my guess is
papers. So, there will be my guess is
there going to be like a series of blog
there going to be like a series of blog
posts with the 2.5 release.
Plus, you know that it's actually useful
Plus, you know that it's actually useful
whenever I write like long form content
whenever I write like long form content
on X. A good chunk of the time it blows
on X. A good chunk of the time it blows
up. Like I didn't even do an article,
up. Like I didn't even do an article,
but I did uh I got really annoyed by
but I did uh I got really annoyed by
this insane doomer like blog post. It
this insane doomer like blog post. It
was literally kind of like a hit piece
was literally kind of like a hit piece
by like I I don't know what the hell it
by like I I don't know what the hell it
was. So, I wrote a big thread on it and
was. So, I wrote a big thread on it and
it's gotten 70 77,000 views and like
it's gotten 70 77,000 views and like
3,000 people actually read the entire 25
3,000 people actually read the entire 25
tweet thread that I wrote. So, that was
tweet thread that I wrote. So, that was
kind of cool. So long form content
good. Okay, back to experience
buffer. Collect segments.
Now for the prioritize
Now for the prioritize
buffer. So each of these are going to
buffer. So each of these are going to
get a score which is going to be based
get a score which is going to be based
on the advantage.
Is it even worth looking at reference
Is it even worth looking at reference
implementations for an experience
implementations for an experience
buffer? Because I'm pretty sure they all
buffer? Because I'm pretty sure they all
like I'm pretty sure they all suck.
Hang on. The advantage
function. Can you use the advantage
function. Can you use the advantage
function for experience prioritization?
Okay, so we have this and we have
this. I guess the torch one might be
this. I guess the torch one might be
half
decent.
See, so this is the original experience
buffer. How much prioritization is
buffer. How much prioritization is
used? Important sampling negative
used? Important sampling negative
exponent added to priority
exponent added to priority
is
storage list storage will be used. Well,
storage list storage will be used. Well,
that's
horrifying. That's like the worst thing
horrifying. That's like the worst thing
ever.
ever.
Pin
Pin
memory
memory
prefetch number of next batches to be
prefetch number of next batches to be
preset fetched using
multi-threading. That's useless if your
multi-threading. That's useless if your
memor is on the GPU,
memor is on the GPU,
right? Yeah, that's useless if your
right? Yeah, that's useless if your
memor is on the
memor is on the
GPU. Transform to be
GPU. Transform to be
executed. Okay, that's horrible. Don't
executed. Okay, that's horrible. Don't
do that ever.
do that ever.
Uh batch
Uh batch
flies dim
extend.
Yeah. Let's see if their code's any
Yeah. Let's see if their code's any
good.
Oh, wait. Is there more
docks? No, it's just
docks? No, it's just
this replay buffer.
Um, none of this is in C++, right? I
Um, none of this is in C++, right? I
guess it's relying on tensor
guess it's relying on tensor
deck, but that's still not
great. Only loops over dim. So far, I
great. Only loops over dim. So far, I
don't see loops over elements.
What's this? Prioritize replay
buffer. OG is remote buffer.
And that's not good. That's slow as
hell. I don't see anything in here that
hell. I don't see anything in here that
makes me think I should like pay any
makes me think I should like pay any
attention to
attention to
this. So, let's go to the actual
paper. It just doesn't look very well
paper. It just doesn't look very well
built.
Where's the math for
this prioritizing with TD
error expected learning
error expected learning
progress magnitude of TD
progress magnitude of TD
error indicates how surprising or
error indicates how surprising or
unexpected the transition is.
unexpected the transition is.
So this would be very closely related to
So this would be very closely related to
what I was considering which is using
what I was considering which is using
advantage. Um
because in PO we have some advantage
because in PO we have some advantage
function
function
right and where is it?
Yeah, this here this is TD error,
right? So for gamma equals 0 or it's at
right? So for gamma equals 0 or it's at
lambda equals zero, uh generalized
lambda equals zero, uh generalized
advantage estimation just gives you TD
advantage estimation just gives you TD
error and then for one it gives
error and then for one it gives
you value baseline discounted returns.
you value baseline discounted returns.
Now the thing I'm wondering
here this is only really based on the
here this is only really based on the
value function
right? Oh, but wait. The value function
right? Oh, but wait. The value function
is in a sense conditioned on the policy,
is in a sense conditioned on the policy,
right? Because the value function is not
right? Because the value function is not
just telling
just telling
you Yeah. Yeah. Yeah. Okay. So, what I
you Yeah. Yeah. Yeah. Okay. So, what I
was concerned about is that if I use the
was concerned about is that if I use the
advantage function
advantage function
uh in order to prioritize experience,
uh in order to prioritize experience,
then it could be the case that the value
then it could be the case that the value
function learns the correct uh the
function learns the correct uh the
correct return out of it, but the policy
correct return out of it, but the policy
hasn't actually done anything. So that's
hasn't actually done anything. So that's
not the case because the value
not the case because the value
function well first of all the reward
function well first of all the reward
collected is actually the word collected
collected is actually the word collected
by the behavior of the policy. So the
by the behavior of the policy. So the
policy is correct in a sense the policy
policy is correct in a sense the policy
is always correct because that's the
is always correct because that's the
only sample that you see um is the
only sample that you see um is the
sample that you get. So the only thing
sample that you get. So the only thing
that can be wrong is the value function
that can be wrong is the value function
here. So you are in a sense just
here. So you are in a sense just
training the value function and the
training the value function and the
value function depends on the policy
value function depends on the policy
anyways because that's what collects the
anyways because that's what collects the
data that it sees. Okay, I'm happy that
data that it sees. Okay, I'm happy that
you can use this for prioritization.
you can use this for prioritization.
That is fine. So then what you could do
That is fine. So then what you could do
is you can rep prioritize every time you
is you can rep prioritize every time you
do an update to the policy. You can rep
do an update to the policy. You can rep
prioritize the advantage
buffer. That would make sense.
Yes, that would make
sense. The reshape for this
sense. The reshape for this
thing. Well, hang on. Getting ahead of
thing. Well, hang on. Getting ahead of
ourselves. We collect data in
ourselves. We collect data in
batch. Now we need prioritize data.
How do you want to sample from this
thing? Is there ever a reason that you
thing? Is there ever a reason that you
would want to
would want to
train
train
on you never want to train on low
on you never want to train on low
priority samples, right?
So the data in this buffer is going to
So the data in this buffer is going to
have to be constantly shifting around
I think you're screwed on memory
I think you're screwed on memory
contiguity here,
though. I think you're screwed on memory
though. I think you're screwed on memory
contiguity.
Yeah. So the issue here, right, is you
Yeah. So the issue here, right, is you
get these samples on
get these samples on
policy. You've got like some other
policy. You've got like some other
samples
samples
here that are off
policy. And then these got to get
blended into like some
blended into like some
sample some
sample some
samples. We'll call this
samples. We'll call this
a
z. Okay. So here's your
z. Okay. So here's your
buffer, but uh you don't just iterate
buffer, but uh you don't just iterate
over this
buffer. You actually you want to sample
buffer. You actually you want to sample
from it the top n. So you actually need
from it the top n. So you actually need
top
K and this is going to be a
K and this is going to be a
non-ontiguous
non-ontiguous
sample. And even if you were to sort
sample. And even if you were to sort
this by top K then you have to
this by top K then you have to
uh rep
prioritize, right?
prioritize, right?
You have to rep prioritize.
Okay, hang
Okay, hang
on. We can save a
on. We can save a
copy. There is a way for us to save a
copy. There is a way for us to save a
copy.
If we just make one big buffer, right?
If we just make one big buffer, right?
We just connect these
two and then at the end of a whole
two and then at the end of a whole
training
training
loop, we just make sure that we have uh
loop, we just make sure that we have uh
these samples at the
these samples at the
end. Then we're good.
If you want constantly correct
If you want constantly correct
prioritized samples, there's no way
prioritized samples, there's no way
around
this. Oh Wait, it's harder than
this. Oh Wait, it's harder than
this. Every time you do a sample
this. Every time you do a sample
step, you have to rep prioritize
step, you have to rep prioritize
everything, don't
you? How bad is that? So every time you
you? How bad is that? So every time you
do a mini batch, you have to rep
do a mini batch, you have to rep
prioritize all the
samples. I think I can make that fast.
If I do a CUDA kernel for this, I think
If I do a CUDA kernel for this, I think
I can make it
I can make it
fast. So, you just need CUDA
fast. So, you just need CUDA
GA. It doesn't have to cross segment
boundaries. I can definitely make that
fast. Okay.
Well, we might find out that I'm crazy
Well, we might find out that I'm crazy
with um some of the assumptions I've
with um some of the assumptions I've
made here. There are several ways at a
made here. There are several ways at a
hardware level that this can go
hardware level that this can go
wrong, but if it's if everything works
wrong, but if it's if everything works
out, this should be pretty simple.
out, this should be pretty simple.
This should be pretty
This should be pretty
simple. And then this would give us
simple. And then this would give us
experience prioritized experience
experience prioritized experience
replay. There will
replay. There will
be copy
overhead. The copy
overhead is at a trajectory
overhead is at a trajectory
level which is nowhere near as bad as at
level which is nowhere near as bad as at
a sample level. Previously we were doing
a sample level. Previously we were doing
sample level indexing. So we actually
sample level indexing. So we actually
have more efficient by a factor of
have more efficient by a factor of
segment length indexing.
segment length indexing.
Now I think this works. This gets us 2D
Now I think this works. This gets us 2D
tensors
tensors
uh for
uh for
everything. 2D tensors for
everything. This gets us prioritized
everything. This gets us prioritized
experience replay.
experience replay.
Um, this gets
Um, this gets
us non-p prioritized advantage filtering
us non-p prioritized advantage filtering
with the same code very
with the same code very
easily. This gets us a
easily. This gets us a
lot. This gets us a lot, I'd say.
And actually I
And actually I
think for anyone watching with RL
think for anyone watching with RL
experience if you know whether uh you
experience if you know whether uh you
use both high advantage and low
use both high advantage and low
advantage trajectories in experience
advantage trajectories in experience
replay. I'm actually not
replay. I'm actually not
sure. I would think that you would take
sure. I would think that you would take
absolute value of the advantage or
absolute value of the advantage or
whatever. You would want to
whatever. You would want to
have you would want the anything that's
have you would want the anything that's
surprising you would want to learn on.
surprising you would want to learn on.
So you'd learn on really low advantage
So you'd learn on really low advantage
and really high advantage samples
and really high advantage samples
because
because
advantage yeah advantage is not uh you
advantage yeah advantage is not uh you
don't take absolute value by default. So
don't take absolute value by default. So
we would want to take absolute
value.
value.
Okay I think we start on this. I think
Okay I think we start on this. I think
we start on this. Uh this is not going
we start on this. Uh this is not going
to be implementing this unless I've
to be implementing this unless I've
miscalculated shouldn't be that bad. And
miscalculated shouldn't be that bad. And
This is actually going to even be a
This is actually going to even be a
little simpler than our current
little simpler than our current
implementation and will be much much
implementation and will be much much
much better if I'm correct on all the
much better if I'm correct on all the
assumptions. So we don't have to do any
assumptions. So we don't have to do any
of this micro batch mini batch
of this micro batch mini batch
shenanigans.
shenanigans.
Uh, all we need to do
is do batch size, replay
size. Uh, we need to add one parameter.
size. Uh, we need to add one parameter.
I
I
believe we need to add just one
believe we need to add just one
parameter.
And I think we want to make it a factor.
We play
We play
size factor times batch size. So num
size factor times batch size. So num
rows is going to
rows is going to
be batch size
plus I don't know why I did minus one.
Hey, welcome
Hrian. Currently, we are coding up
Hrian. Currently, we are coding up
prioritized experience replay for
prioritized experience replay for
PO, which will be a fun thing to
PO, which will be a fun thing to
do. Well, not really, but hopefully the
do. Well, not really, but hopefully the
results will be
fun. Need this. Don't need this. Don't
fun. Need this. Don't need this. Don't
need this.
What do you
What do you
want? What do you
mean? I'm currently on one big cup of
mean? I'm currently on one big cup of
coffee, if that's what you mean.
I guess this does limit uh nonLSTM
I guess this does limit uh nonLSTM
models the way we have it here a little
bit. What are you working on? Yeah.
bit. What are you working on? Yeah.
Yeah. Yeah. Experience replay buffer for
Yeah. Yeah. Experience replay buffer for
uh PO. Prioritize exp beer
uh PO. Prioritize exp beer
buffer. Okay. The only bad thing that I
buffer. Okay. The only bad thing that I
can see with the way that I have uh I
can see with the way that I have uh I
suggested or I have this implementation
suggested or I have this implementation
suggested is that technically right now
suggested is that technically right now
you can only prioritize segments. You
you can only prioritize segments. You
cannot prioritize
cannot prioritize
uh individual
samples. Do I care?
How bad is
that? I probably really don't care that
that? I probably really don't care that
much,
right? Like, yeah, technically theory uh
right? Like, yeah, technically theory uh
that could make a huge difference for
that could make a huge difference for
non-recurrent models, but any problem
non-recurrent models, but any problem
that you can solve without memory like
that you can solve without memory like
just as well or better is probably a
just as well or better is probably a
really stupid
problem. I can't think of like any big
problem. I can't think of like any big
problems that have been solved without
problems that have been solved without
an LSTM.
an LSTM.
It's usually people like not using an
It's usually people like not using an
LSTM on Atari or something because like
LSTM on Atari or something because like
they don't know what they're doing and
they don't know what they're doing and
their LSTM implementation
their LSTM implementation
sucks. So I think that we don't care
sucks. So I think that we don't care
about this. I think we're
about this. I think we're
good. I think we just proceed with
plan. You need episode length
still. You still need episode lengths.
still. You still need episode lengths.
You still need episode indices. You
You still need episode indices. You
still need all these variables, but it
still need all these variables, but it
gets a little
gets a little
easier. Yeah, it's a little
easier. And then I need to change before
easier. And then I need to change before
I forget the full
function on policy rows.
function on policy rows.
We'll just do on policy rows.
Here we
go. So now we have
everything
everything
and this full function works as before.
I have to edit the siphon a tiny
bit. This is now
zero. This gets commented.
So we no longer need to bootstrap across
So we no longer need to bootstrap across
segments like
crazy. I got to tell you, it makes a
crazy. I got to tell you, it makes a
world of difference coding on 13 hours
world of difference coding on 13 hours
of sleep versus 4 hours of sleep.
of sleep versus 4 hours of sleep.
Everything is so much easier today.
I say as I make a stupid
error. I pretty much crashed at like 700
error. I pretty much crashed at like 700
p.m. last night and didn't wake up until
p.m. last night and didn't wake up until
8:00
8:00
a.m.
Exhausted. All
right. Now all we have to change is
right. Now all we have to change is
really the sampling,
really the sampling,
right? When we do like this flatten
right? When we do like this flatten
batch
batch
thing, like this is no longer even uh a
thing, like this is no longer even uh a
necessity. Like this literally doesn't
necessity. Like this literally doesn't
matter anymore,
right? I guess we pack everything into a
right? I guess we pack everything into a
namespace or whatever.
Okay. So, advantages
Okay. So, advantages
here. I need to do weighted choice,
here. I need to do weighted choice,
right?
Is it weighted random
sample utils data weighted random sample
is there
is there
um a weighted is there just like a
um a weighted is there just like a
weighted sample somewhere in here that
weighted sample somewhere in here that
we can just
apply or does it only work on a PyTorch
apply or does it only work on a PyTorch
data
set. Oh no, this just gives
set. Oh no, this just gives
you Yeah, this just gives you indices.
you Yeah, this just gives you indices.
So this is perfect, right?
Wait, is this this is a function, right?
Wait, is this this is a function, right?
The way that they're calling
The way that they're calling
this. Okay, I don't know why it's named
this. Okay, I don't know why it's named
in uppercase like this, but this is just
in uppercase like this, but this is just
a function. So, this is what we
need. And I don't think you need to give
need. And I don't think you need to give
it probabilities either, right? Yeah,
it probabilities either, right? Yeah,
these don't have to be probabilities.
these don't have to be probabilities.
So this is
So this is
advantages
advantages
and replacement false should be
and replacement false should be
faster and then generator none should be
faster and then generator none should be
default. Oh wait no replacement true
default. Oh wait no replacement true
should be faster.
should be faster.
Yes. Yeah. There you go. That's just
Yes. Yeah. There you go. That's just
this. So then you just do
weights. All right, this is
indices. And
indices. And
now all we have to do
now all we have to do
is Is it that
is Is it that
easy? I think it actually is that easy.
easy? I think it actually is that easy.
That's kind of embarrassing.
So we need uh
advantages
advantages
x
actions log props.
actions log props.
uh
dun obs.
Then you need values and
returns which is B advantages plus B
returns which is B advantages plus B
values.
and just plus
and just plus
values. Yeah. So it's something like
values. Yeah. So it's something like
this. I
this. I
think that's it.
Yeah. So, this is going to have to
Yeah. So, this is going to have to
change
change
massively.
massively.
Massively because
You no longer even have update epochs
You no longer even have update epochs
with this,
right? I guess you just do it
here. You literally just do it here,
here. You literally just do it here,
don't
don't
you? Yeah.
So you got to have experience values.
So you got to have experience values.
You got to have all this
stuff and then you compute the
stuff and then you compute the
J to
So now we have all this data.
This feels wrong. Like the amount of
This feels wrong. Like the amount of
changes we're about to make to this
trainer. It's going to get a lot simpler
though. Let me make sure that that width
though. Let me make sure that that width
statement is lined
up. It's not. Right. This needs to be
up. It's not. Right. This needs to be
tabbed in.
Not many
batches. Oh, no. It's That's correct.
batches. Oh, no. It's That's correct.
That
That
is No, it it needs to be tapped in one.
is No, it it needs to be tapped in one.
It doesn't need to be tapped in
one. There we go.
This total mess of code that you're
This total mess of code that you're
seeing here is not what this is going to
seeing here is not what this is going to
look like when we ship this. By the way,
look like when we ship this. By the way,
if you look at our current trainer in
if you look at our current trainer in
2.0, the code is way better than this.
2.0, the code is way better than this.
This is what happens with like 3 months
This is what happens with like 3 months
of me trying a dozen different methods
of me trying a dozen different methods
to improve various things in RL. And um
to improve various things in RL. And um
we're not even going to keep all these
we're not even going to keep all these
flags in. We're just going to drop
flags in. We're just going to drop
everything that doesn't immediately work
everything that doesn't immediately work
well. And you know, we're going to have
well. And you know, we're going to have
puffer lip should be optimized for like
puffer lip should be optimized for like
the defaults are good,
right? There will have to be like extra
right? There will have to be like extra
features that aren't on by default will
features that aren't on by default will
have to have a damn good reason for
have to have a damn good reason for
being there. Basically,
Are you still planning on trying to fl
Are you still planning on trying to fl
out P3 at some point? Yes, that is. So,
out P3 at some point? Yes, that is. So,
to give you an
to give you an
idea, the main thing in PO right now
idea, the main thing in PO right now
that is holding RL back is J.
that is holding RL back is J.
We'll see whether I got it right with
We'll see whether I got it right with
P30 as the
P30 as the
successor, but that thing needs to be
successor, but that thing needs to be
fixed. RL will be stuck until we fix
fixed. RL will be stuck until we fix
that
thing. I'm a little iffy on whether I'm
thing. I'm a little iffy on whether I'm
close or not with P30, though. It might
close or not with P30, though. It might
be that I went a little off base with
be that I went a little off base with
it. We might have to do a little bit
it. We might have to do a little bit
more work to get that to function, but
more work to get that to function, but
it's fine. We'll deal with
it. All
right, I think we are almost ready to
right, I think we are almost ready to
start debugging this.
explained variance has been yeah I took
explained variance has been yeah I took
it out of logging because I there's
it out of logging because I there's
another thing that I broke relating to
another thing that I broke relating to
that it's not that explained variance is
that it's not that explained variance is
wrong it's that the log is not being
wrong it's that the log is not being
populated I also never look at that
populated I also never look at that
log maybe I should but I never look at
log maybe I should but I never look at
that log
Okay, this is an indexing error.
Okay.
This is 2,000
This is 2,000
things and this is giving me 64
things. And L is out of out of index.
So, this needs to just go down, doesn't
So, this needs to just go down, doesn't
it? Until here, maybe.
Okay. And then we get the same error up
Okay. And then we get the same error up
top.
Um, I notice we don't
reset. That's got to get reset right
there. Maybe not.
Okay. So we have full indices
here
here
16k at lengths 4096 f indices.
So uh yeah this is not full
So uh yeah this is not full
indices right.
Or is
it? Maybe it
it? Maybe it
is. Maybe this needs to be LSTM on Total
is. Maybe this needs to be LSTM on Total
Agents. Maybe I have the wrong size. I
Agents. Maybe I have the wrong size. I
think I do. Morning. How's it going?
think I do. Morning. How's it going?
Doing pretty well. Back from little
Doing pretty well. Back from little
trip. Um, got a bunch of good rest in.
trip. Um, got a bunch of good rest in.
Pretty well recharged. And this week we
Pretty well recharged. And this week we
are going to get prioritized experience
are going to get prioritized experience
replay into Puffer. We're going to get
replay into Puffer. We're going to get
this horrendous dev branch code cleaned
this horrendous dev branch code cleaned
up. Um, cleaned up dramatically, I would
up. Um, cleaned up dramatically, I would
hope. And we're going to keep running
hope. And we're going to keep running
experiments on all the more complex MS
experiments on all the more complex MS
that we have and hopefully get some more
that we have and hopefully get some more
soda results. I've got pretty well. I
soda results. I've got pretty well. I
have the next like many weeks of my
have the next like many weeks of my
schedule blocked nicely, so I should be
schedule blocked nicely, so I should be
able to focus uh very well on work.
able to focus uh very well on work.
Where did I go? Went to tour the new
Where did I go? Went to tour the new
puffer
facility. I will be there in person in a
facility. I will be there in person in a
few weeks again. But in the
few weeks again. But in the
meanwhile, this is the uh this is where
meanwhile, this is the uh this is where
all the machines are going.
We've got electrical done for
it. Haven't bought all the machines yet.
it. Haven't bought all the machines yet.
We'll move some of the ones that we have
We'll move some of the ones that we have
now here in the short term, but you
now here in the short term, but you
know, we will be buying a lot more
know, we will be buying a lot more
machines. We now have capacity for more
machines. We now have capacity for more
machines. We will no longer have to deal
machines. We will no longer have to deal
with power fluctuations and other
with power fluctuations and other
And uh we should be able to
And uh we should be able to
power a lot of good RL research.
majority of that cluster is going to be
majority of that cluster is going to be
like the vast majority of that cluster
like the vast majority of that cluster
is going to be just for all the people
is going to be just for all the people
we have on open source work doing more
we have on open source work doing more
open source work. So hopefully we'll
open source work. So hopefully we'll
even be able to grow Puffer to a larger
even be able to grow Puffer to a larger
open source project with this because we
open source project with this because we
will have more machines to give people
will have more machines to give people
access to which will be kind of cool,
access to which will be kind of cool,
right? You know, before it was like if
right? You know, before it was like if
you're one of the top contributors, you
you're one of the top contributors, you
get access to a box. Uh and then once we
get access to a box. Uh and then once we
have the new machines, it'll be like
have the new machines, it'll be like
pretty much all the contributors get a
pretty much all the contributors get a
box and the top contributors have access
box and the top contributors have access
to several machines for running
to several machines for running
experiments. That's the goal.
It will be expensive
though. Donor. There's no
though. Donor. There's no
donor. There is no donor, my friend.
donor. There is no donor, my friend.
This is
This is
I will be covering all the
machines. Buffer is officially a company
machines. Buffer is officially a company
after all. It's not a not for profofit
after all. It's not a not for profofit
even though all our stuff is open
even though all our stuff is open
source. So uh the way we make money is
source. So uh the way we make money is
just by selling service packages to
just by selling service packages to
people using our stuff. You want us on
people using our stuff. You want us on
your side to make your RL fast and easy?
your side to make your RL fast and easy?
You can do that for a reasonable price.
You can do that for a reasonable price.
Starts at 10K a
month. So like a quarter of an engineer
month. So like a quarter of an engineer
to have your RL not be insane and
cursed. Pretty good
cursed. Pretty good
deal. If any of you watching have
deal. If any of you watching have
uh industry problems and need RL to work
uh industry problems and need RL to work
for you, give us a ring.
Well, it should be one D but given away
Well, it should be one D but given away
top
top
shape.
Okay. What shape do we have?
Do we just sum
Do we just sum
these? I think we just sum these,
right? I am going to save RL. This is
right? I am going to save RL. This is
why I'm doing this. Literally like if I
why I'm doing this. Literally like if I
thought I it lit if I thought that all I
thought I it lit if I thought that all I
could do is maybe make like RL 10%
could do is maybe make like RL 10%
better, I wouldn't be doing this, right?
better, I wouldn't be doing this, right?
I wouldn't be spending the rest of my
I wouldn't be spending the rest of my
20s on like some crazy mission to make
20s on like some crazy mission to make
RL 10% better, right? The goal of this
RL 10% better, right? The goal of this
is I think that we can fix the whole
is I think that we can fix the whole
field of RL. Um the progress in the last
field of RL. Um the progress in the last
year has been crazy. I think by the end
year has been crazy. I think by the end
of this year, you're going to see it
of this year, you're going to see it
pretty broadly applicable to a bunch of
pretty broadly applicable to a bunch of
industries and we're going to actually
industries and we're going to actually
start scaling, you know, some contract
start scaling, you know, some contract
work out so we can get Puffer going, you
work out so we can get Puffer going, you
know, at a reasonable scale. And then
know, at a reasonable scale. And then
the year after that, by then we
the year after that, by then we
hopefully we just have everything solved
hopefully we just have everything solved
and we get like some much larger
and we get like some much larger
contracts and we are just like a nice
contracts and we are just like a nice
stable midsized industry lab. That's the
stable midsized industry lab. That's the
hope.
And the key is to do all of it
And the key is to do all of it
bootstrapped so that we don't end up
bootstrapped so that we don't end up
with strings attached on what we can
with strings attached on what we can
open source and what we can't because we
open source and what we can't because we
kind of just want to open source
everything. What's wrong with this?
What?
What?
Um. Huh?
I don't want to do list to
this. Does anybody know how this stupid
this. Does anybody know how this stupid
thing works? This weighted random
sampler? Maybe we just go read the
sampler? Maybe we just go read the
source
code. Oh, it's just
multinnomial. So, we don't need this
multinnomial. So, we don't need this
thing at all. It's just multinnomial.
Even
better. And uh we need to do abs of this
better. And uh we need to do abs of this
anyways.
Oh, that was
sketchy. I'm a Cali CC student
sketchy. I'm a Cali CC student
transferring now. Got to most UC's.
transferring now. Got to most UC's.
Congratulations for applied math and
Congratulations for applied math and
rejected for CS. Do I think I can make
rejected for CS. Do I think I can make
it useful if I take the right class or
it useful if I take the right class or
does it suck if I want to do software
does it suck if I want to do software
engineering? Uh, the software
engineering? Uh, the software
engineering courses suck anyways. So,
engineering courses suck anyways. So,
you should be fine. I uh I don't know
you should be fine. I uh I don't know
how the courses work there because
how the courses work there because
uh it depends on your university whether
uh it depends on your university whether
you apply to a major or not. like
you apply to a major or not. like
Stanford, you do not apply for a major
Stanford, you do not apply for a major
and a lot of schools, you do not apply
and a lot of schools, you do not apply
for a major and you can just take
for a major and you can just take
whatever you want and including stuff
whatever you want and including stuff
outside of your major once you declare
outside of your major once you declare
it. So, I don't know if that means you
it. So, I don't know if that means you
can take zero courses or whatever.
can take zero courses or whatever.
Hopefully, you can still take a couple
Hopefully, you can still take a couple
courses, but usually outside of the
courses, but usually outside of the
intro CS courses, which are just there
intro CS courses, which are just there
to make sure that like you've heard of
to make sure that like you've heard of
all the basics, um CS is very much a
all the basics, um CS is very much a
self-taught discipline. like the courses
self-taught discipline. like the courses
suck. So, you're not really at a
suck. So, you're not really at a
disadvantage for not having access to
disadvantage for not having access to
those
those
courses. If you're a math heavy person
courses. If you're a math heavy person
and you want to lean into the math,
and you want to lean into the math,
that's cool. I didn't. I'm more engine
that's cool. I didn't. I'm more engine
I'm more engineering heavy, but that
I'm more engineering heavy, but that
doesn't mean that I spent my time taking
doesn't mean that I spent my time taking
a bunch of CS courses, right? It means I
a bunch of CS courses, right? It means I
spent my time doing a lot of projects
spent my time doing a lot of projects
and a lot of
and a lot of
research. Our expanding RL Algo pool,
research. Our expanding RL Algo pool,
what's coming next? So, our goal is not
what's coming next? So, our goal is not
to have a pool of algorithms. It's
to have a pool of algorithms. It's
really to just always have the one best
really to just always have the one best
algorithm. So, right now I'm adding on
algorithm. So, right now I'm adding on
to PO an a prioritized replay buffer
to PO an a prioritized replay buffer
that should make quite a large
that should make quite a large
difference. Uh we're going to test it
difference. Uh we're going to test it
obviously we're going to test it on all
obviously we're going to test it on all
the different puffer ms and actually as
the different puffer ms and actually as
a fallback. There's a way to use to uh
a fallback. There's a way to use to uh
do training without a uh replay buffer
do training without a uh replay buffer
as well. But the goal is not to like
as well. But the goal is not to like
keep expanding this thing horizontally
keep expanding this thing horizontally
with more algorithms. The goal is to
with more algorithms. The goal is to
just get the best algorithm, make sure
just get the best algorithm, make sure
we have the best algorithm, and just
we have the best algorithm, and just
keep pushing that
keep pushing that
way. So, right now, we're currently just
way. So, right now, we're currently just
attempting to improve the best
attempting to improve the best
algorithm. Thanks. Appreciate your
algorithm. Thanks. Appreciate your
insight. We'll just keep self-learning
insight. We'll just keep self-learning
projects to go and be fine. Yeah. And if
projects to go and be fine. Yeah. And if
you're looking at RL specifically,
you're looking at RL specifically,
there's a quick start guide on my
there's a quick start guide on my
website that does not assume you have a
website that does not assume you have a
ton of math experience. Um, it links you
ton of math experience. Um, it links you
a very manageable number of papers and
a very manageable number of papers and
blog posts to read to get you up to
blog posts to read to get you up to
speed on most of the basics. This is
speed on most of the basics. This is
what I've given to all our new
what I've given to all our new
contributors. Most of them came in with
contributors. Most of them came in with
zero RL experience. And now I would put
zero RL experience. And now I would put
like I bet on many of my contributors uh
like I bet on many of my contributors uh
over like top CS PhD students in order
over like top CS PhD students in order
to get RL stuff done. So yeah, it's you
to get RL stuff done. So yeah, it's you
can learn stuff. The thing that's
can learn stuff. The thing that's
hardest honestly is getting the good
hardest honestly is getting the good
engineering base. But the courses don't
engineering base. But the courses don't
help you. Like becoming a good
help you. Like becoming a good
programmer is very very very hard. It's
programmer is very very very hard. It's
much harder than basically anything
much harder than basically anything
else. I consider it harder than like
else. I consider it harder than like
getting good at AI even.
getting good at AI even.
Um but the courses aren't going to help
Um but the courses aren't going to help
you there anyways.
So yeah, definitely don't fall for like
So yeah, definitely don't fall for like
the oh no coders next year or whatever.
the oh no coders next year or whatever.
Like no, the coding is the hardest part.
Like no, the coding is the hardest part.
It's the hardest
part. Everything else is pretty easy.
part. Everything else is pretty easy.
Actually, the AI stuff is not that
Actually, the AI stuff is not that
hard. Executing on everything with good
hard. Executing on everything with good
engineering is the hard part.
So that looks good.
I have lots of programming tips as well
I have lots of programming tips as well
around if you just sort of see with this
couple has no attribute flat.
values. How'd that
happen? Screwed that up.
Heck happened
There.
There.
Oh, yeah. That's going to screw you up,
Oh, yeah. That's going to screw you up,
isn't it?
some do
some do
this. I'll do it.
Um, we're going to comment this for
now. Accumulate mini
now. Accumulate mini
batches. What do we do
this? I guess this is epoch then, right?
no attribute B returns. Oops.
Yeah. So, that's going to be a
problem. Um, I guess this is just like
problem. Um, I guess this is just like
advantages
advantages
plus values or
plus values or
something. Want a new dev? I want to dev
something. Want a new dev? I want to dev
a new M using your C or Python API.
a new M using your C or Python API.
Which M has the most straightforward
Which M has the most straightforward
template to follow? Square does. And
template to follow? Square does. And
there's a little bit of uh
there's a little bit of uh
so there's a little bit of subtlety here
so there's a little bit of subtlety here
because we're in between two end finding
because we're in between two end finding
generations. So we're in between
generations. So we're in between
generations of the way that we plan to
generations of the way that we plan to
do this stuff. So if you just go to 2.0
do this stuff. So if you just go to 2.0
O puffer lib ocean
O puffer lib ocean
squared. There is a full python
squared. There is a full python
implementation of squared somewhere pi
implementation of squared somewhere pi
squared. So this is the whole n written
squared. So this is the whole n written
in python very simple n and then there
in python very simple n and then there
is the exact same
is the exact same
environment written with just a
environment written with just a
binding through syon
binding through syon
syonbind and then the whole logic is in
syonbind and then the whole logic is in
the.h right here. So you have both as
the.h right here. So you have both as
examples. Now in the new dev branch
examples. Now in the new dev branch
which is not done yet uh we have found a
which is not done yet uh we have found a
way to eliminate Syon and substantially
way to eliminate Syon and substantially
improve the ease with which you can bind
improve the ease with which you can bind
stuff to C. But that's not quite ready
stuff to C. But that's not quite ready
yet. You can look at it in the dev
yet. You can look at it in the dev
branch. In fact, I'll show you real
branch. In fact, I'll show you real
quick.
quick.
So instead of writing a Syon file, this
So instead of writing a Syon file, this
is what you write instead of Syon now.
is what you write instead of Syon now.
And the rest of the code is just about
And the rest of the code is just about
the same. But um aside from
the same. But um aside from
that, all the stuff in 2.0 is stable and
that, all the stuff in 2.0 is stable and
easy enough to
use and a lot of this unstable stuff is
use and a lot of this unstable stuff is
going to get stable really quick. I had
going to get stable really quick. I had
like a bunch of stuff going on in the
like a bunch of stuff going on in the
last few weeks and you know my family
last few weeks and you know my family
was around so you know there was time
was around so you know there was time
loss in between doing stuff. I should be
loss in between doing stuff. I should be
able to get some really solid hours in
able to get some really solid hours in
over the next few weeks. So, a lot of
over the next few weeks. So, a lot of
this stuff should just get stable
this stuff should just get stable
fast. Really, like most stuff in Puffer
fast. Really, like most stuff in Puffer
Lib is just straight up bottlenecked by
Lib is just straight up bottlenecked by
my dev
hours. Advantages plus values. Yeah.
and
and
probably we failed to increment global
probably we failed to increment global
step or Nothing.
Ah, this is what happened. Yeah, this
Ah, this is what happened. Yeah, this
didn't get reset.
So, so this has to go into
So, so this has to go into
sample.
Actually, not into sample. It should go
into I guess it goes here for now.
Boom. Cool. 2.1 million SPS. Probably
Boom. Cool. 2.1 million SPS. Probably
it's not doing enough stuff in training,
it's not doing enough stuff in training,
but
but
uh doesn't immediately
uh doesn't immediately
crash. Pretty cool.
crash. Pretty cool.
It's not going to train well, but that's
It's not going to train well, but that's
not bad for a first
not bad for a first
draft. Not bad at
draft. Not bad at
all. So, next thing is going to
all. So, next thing is going to
be experience prioritization probably,
right? Experience prioritization.
Notice obsctions and
Notice obsctions and
rewards are in UN8 int and float. Yeah,
rewards are in UN8 int and float. Yeah,
you can have them be different d types.
you can have them be different d types.
So that specific environment
So that specific environment
um the reward is I mean in that specific
um the reward is I mean in that specific
in that specific environment it is
in that specific environment it is
better to have U and 8 because it's
better to have U and 8 because it's
lower bandwidth other environments have
lower bandwidth other environments have
different D types you can define
different D types you can define
whatever DT type you want right does
whatever DT type you want right does
puffer handle that
puffer handle that
automatically well when you're writing
automatically well when you're writing
your environment if you're writing it in
your environment if you're writing it in
C it's going to have a DT type
C it's going to have a DT type
Right. And then the only thing that you
Right. And then the only thing that you
have to be careful with which Syon
have to be careful with which Syon
should tell you if you're doing it wrong
should tell you if you're doing it wrong
and you know that our new code should
and you know that our new code should
also tell you if you're doing it wrong.
also tell you if you're doing it wrong.
Uh you just have to make sure that the
Uh you just have to make sure that the
numpy array that you pass in matches the
numpy array that you pass in matches the
type that you've assigned in uh in C
type that you've assigned in uh in C
which all you do for that is puffer will
which all you do for that is puffer will
take care of that for you provided that
take care of that for you provided that
the observation space right when you
the observation space right when you
define observation space you define the
define observation space you define the
correct dype there.
correct dype there.
So if you look at the other ends, we
So if you look at the other ends, we
have ops that are imps, we have ops that
have ops that are imps, we have ops that
are floats, we have
are floats, we have
whatever. And actions can also be floats
whatever. And actions can also be floats
if they are continuous because we do
if they are continuous because we do
have the continuous support as well.
Okay. How do we want to populate
Okay. How do we want to populate
the prioritize
buffer? I guess all we
buffer? I guess all we
do, we take this little piece of
code. We have to copy everything over
code. We have to copy everything over
though. That's kind of gross.
What if I just do like
prioritize? Well, I'm going to be able
prioritize? Well, I'm going to be able
to make this simpler anyways. So, I
to make this simpler anyways. So, I
think I just do it for now and I don't
think I just do it for now and I don't
worry too much.
worry too much.
So, we do
So, we do
this bit of
this bit of
code
here. Oh, you don't need a weighted
here. Oh, you don't need a weighted
sample. I'm
sample. I'm
dumb. I don't think you need a weighted
dumb. I don't think you need a weighted
sample at all. We'll do it like this for
sample at all. We'll do it like this for
now.
But to
But to
learn
learn
do we
do we
try
try
experience. So now we have
advantages compute J
Then we have to do
So this is a mess, but it's not going to
So this is a mess, but it's not going to
remain a
mess. No set all policy rows.
name space has
no issues.
No attribute rewards.
Log
props actions log props done ops
props actions log props done ops
advantages values return for
terms. So it's these these things right
terms. So it's these these things right
obsctions log props
uh guns values
returns something like
and I guess these don't get talked
either. They need to get talked though,
either. They need to get talked though,
don't they?
Yeah. So, we'll just add this to the uh
Yeah. So, we'll just add this to the uh
the
the
sampler, right?
Okay. So, experience values is bigger
Okay. So, experience values is bigger
than advantages
than advantages
here. Uh let me see why that is the
here. Uh let me see why that is the
case.
Dun. Why is Dun's
smaller? Experience. Duns.
smaller? Experience. Duns.
This should not
This should not
be
be
smaller
numbers BPT
numbers BPT
horizon. So it looks to me like these
horizon. So it looks to me like these
are getting these shapes are getting
are getting these shapes are getting
messed with.
So, it says that Dun is actually it's
So, it says that Dun is actually it's
supposed to be the bigger of the two
Yes, this is
16k. Still
correct.
advantages and
values. Very
values. Very
odd. Very odd indeed.
Ah, hang on. Dun equals xdons. That will
Ah, hang on. Dun equals xdons. That will
screw you
up. Yeah, you don't need that anywhere
up. Yeah, you don't need that anywhere
else. I think that's it. I think I just
else. I think that's it. I think I just
had a little bug. I'm going to have lots
had a little bug. I'm going to have lots
of bugs to fix with the code being this
of bugs to fix with the code being this
messy.
But we're almost to the point where we
But we're almost to the point where we
can start cleaning it
can start cleaning it
up. There we
up. There we
go. Okay. So this is probably not
go. Okay. So this is probably not
working, probably super buggy, probably
working, probably super buggy, probably
very bloated, but this is an
very bloated, but this is an
implementation of prioritized experience
implementation of prioritized experience
replay in puffer lib with PO running at
replay in puffer lib with PO running at
2 million steps per
2 million steps per
second. Um, that's very
good.
Now, there's so many things that I'm
Now, there's so many things that I'm
going to have to think about on how to
going to have to think about on how to
fix this.
I guess I can start by just removing
I guess I can start by just removing
stuff that's no longer
needed. This is gone.
We still want to be able to do this with
P3. It shouldn't be that hard to like
P3. It shouldn't be that hard to like
make it work with P30 either, but we're
make it work with P30 either, but we're
not going to fix that just yet.
What about episode indices, episode
What about episode indices, episode
lengths and all of
lengths and all of
that? We still need
that? We still need
these, I believe. So, I believe we still
these, I believe. So, I believe we still
need the
need the
indices. Easier than before, but we
indices. Easier than before, but we
still need
still need
them. So, what was it? Compute
Jun's values, rewards, sword
indices. See if this gets simplified at
all. No longer need stored indices.
We no longer need
this. Eat stuns, values, rewards, gamut,
this. Eat stuns, values, rewards, gamut,
and
should be
should be
good. So if we're going to store indices
good. So if we're going to store indices
from
here see what I did wrong here.
See if this
See if this
works.
works.
Yes. Now we have this
Yes. Now we have this
running. See if that still trains or
running. See if that still trains or
still runs. Not going to train just
yet. One, two, three. Oh yeah, I forgot
yet. One, two, three. Oh yeah, I forgot
to fix it in the other spot.
Okay. So, this still now this runs
Okay. So, this still now this runs
again.
I don't think that there is any way
I don't think that there is any way
around all these indents
unfortunately. I guess I could combine
them. That doesn't look good.
This is
gone. What do we think it's going to
gone. What do we think it's going to
take to get this thing to train?
That would be the most useful is if I
That would be the most useful is if I
could actually have it training. So then
could actually have it training. So then
as I refactor stuff, I know I don't
as I refactor stuff, I know I don't
break it
anymore. Let's go take a look.
Okay. So this is the current
observation
8192. Uh mini batch size is 8192 samples
8192. Uh mini batch size is 8192 samples
not segments.
samples. This should be mini bat.
Yeah. So now it's 64 by
Yeah. So now it's 64 by
118 or was it 128x 64 by 118? That's way
118 or was it 128x 64 by 118? That's way
better.
And now instead of update
And now instead of update
epoch, it's no longer update
epoch. Okay, that's a good question.
epoch. Okay, that's a good question.
Now, how do we want to rewrite this
Now, how do we want to rewrite this
parameter?
train
steps. It's no longer update epochs.
steps. It's no longer update epochs.
It's
updates. Do I want to have it as a
updates. Do I want to have it as a
factor?
I guess I can punt on it for now and
I guess I can punt on it for now and
just adjust the parameter the way I know
just adjust the parameter the way I know
it needs to be,
right? Because right now, so it's BPT or
right? Because right now, so it's BPT or
was it mini batch size 8192 with one
was it mini batch size 8192 with one
update
update
epoch? Whoop. So this is going to
epoch? Whoop. So this is going to
be 51 was it
52488 over 8192.
64. So it's technically 64 update
64. So it's technically 64 update
epochs. Really 64 updates, not epochs.
epochs. Really 64 updates, not epochs.
It's a total number of updates that you
It's a total number of updates that you
run. See what this
does. It's a lot closer to the original
does. It's a lot closer to the original
speed.
There's no way this thing is just going
There's no way this thing is just going
to train already,
right? I mean, I'm not complaining.
Did I seriously just do this whole thing
Did I seriously just do this whole thing
zero shot without making any
mistakes? Looks like
mistakes? Looks like
it looks like it did.
It's pretty close to the original
It's pretty close to the original
learning curve as well.
Okay. Okay.
Okay. Okay.
So, we have a working uh a working
So, we have a working uh a working
demo. At this point, I should make a
demo. At this point, I should make a
commit or we break it.
I guess it's mostly going to be cleanup
I guess it's mostly going to be cleanup
time now, right?
time now, right?
Like the main issue with this file is
Like the main issue with this file is
it's a mess and all the stuff I do want
it's a mess and all the stuff I do want
to do with experience buffer like I
to do with experience buffer like I
don't have much confidence in this file
don't have much confidence in this file
right now like that everything is
right now like that everything is
correct. So that's going to be the next
correct. So that's going to be the next
stage. Um yeah that's going to
stage. Um yeah that's going to
definitely be the next
definitely be the next
stage.
Okay. I'm going to take a couple quick
Okay. I'm going to take a couple quick
minutes. I'm going to grab a drink, do
minutes. I'm going to grab a drink, do
whatever, and then I will be back here,
whatever, and then I will be back here,
and we will figure out if there's
and we will figure out if there's
anything we can do with all these crazy
anything we can do with all these crazy
messy
messy
rappers.
rappers.
Um, how much of this like redundant
Um, how much of this like redundant
stuff we can delete, if there's a good
stuff we can delete, if there's a good
way to clean up the experience buffer,
way to clean up the experience buffer,
you know, we'll figure out all of
you know, we'll figure out all of
that. Okay, I'll be right back.
Okay, good progress so far.
There kind of a couple stages to this
There kind of a couple stages to this
cleanup,
cleanup,
right? The first one is going to be the
right? The first one is going to be the
big stuff like can we not have
big stuff like can we not have
everything indented under four layers of
everything indented under four layers of
width statements because of
width statements because of
profiling.
profiling.
Um, can we clean up the
Um, can we clean up the
way that we have the experience
way that we have the experience
buffer all that sort of
stuff and then the second was going to
stuff and then the second was going to
be like the
be like the
smaller line by line stuff.
Yeah. That's going to be the main
Yeah. That's going to be the main
thing. So I think for now what we do go
thing. So I think for now what we do go
a little
a little
slower. We look at how indented the code
slower. We look at how indented the code
is and we see if there's anything we can
is and we see if there's anything we can
do to not have it be like that.
So the first one is the AM
context. Can that be a decorator
context. Can that be a decorator
somehow? I don't really like
somehow? I don't really like
decorators to be honest.
decorators to be honest.
It's kind of what I did up here,
It's kind of what I did up here,
right? This is like the overall profile
decorator. Maybe we can do that.
How to turn
How to turn
PyTorch
am I don't know if I'm going to want to
am I don't know if I'm going to want to
do
So you do something like this, right?
But isn't the autocast doesn't have a DT
type? Well, I guess we can make it up
type? Well, I guess we can make it up
here,
right? So, you can make like little
wrapper. Do I want this mixed with my
wrapper. Do I want this mixed with my
profiler
though? Okay, we'll keep that in mind.
though? Okay, we'll keep that in mind.
So this is hiding one with statement
So this is hiding one with statement
here. This is another
here. This is another
one. And then aside from
one. And then aside from
that, there's one additional layer,
that, there's one additional layer,
right? Maximum of one additional layer
right? Maximum of one additional layer
because you never have nested
because you never have nested
profilers, but there are a lot of them.
profilers, but there are a lot of them.
You
You
see now if we move the CUDA synchronize
see now if we move the CUDA synchronize
into the
profiler that gets rid of all these
profiler that gets rid of all these
little
little
statements. We could do that and we can
statements. We could do that and we can
remove it as well dynamically which is
remove it as well dynamically which is
easy. So that gets rid
easy. So that gets rid
of a little bit of code.
what do we think about this
what do we think about this
generally? I mean like tabbing this
generally? I mean like tabbing this
under a width statement is the correct
under a width statement is the correct
thing to do. It's
thing to do. It's
just it's not so nice, you know.
What's our profiler code look
like? Pretty garbage,
right? Pretty garbage.
We also have utilization in here
We also have utilization in here
somewhere,
right? This isn't so
bad. So, if we uh we kind of just like
bad. So, if we uh we kind of just like
scroll up through this, this big chunk
scroll up through this, this big chunk
of code is the dashboard.
of code is the dashboard.
Uh, this is actually pretty compact for
Uh, this is actually pretty compact for
what it is already. The dashboard is
what it is already. The dashboard is
just it's a pretty complicated object.
just it's a pretty complicated object.
It's kind of manually laid
out. This is a seating function.
We've got this roll out function which
We've got this roll out function which
will get a little bit
simplified. We've got checkpoint loading
simplified. We've got checkpoint loading
and
saving. We've got this
utilization. Experience buffer is a big
utilization. Experience buffer is a big
one. Uh we actually will be able to
one. Uh we actually will be able to
remove at least for now we'll be able to
remove at least for now we'll be able to
remove
this. We'll have to put it back in with
P30. I think actually I don't want to
P30. I think actually I don't want to
remove it just left yet because I want
remove it just left yet because I want
to be able to
to be able to
um to port that stuff
up. Creation here of this can be
up. Creation here of this can be
simplified a little bit maybe.
Oh yeah, this can be
simplified at least a
bit. And
bit. And
then yeah, profiler is pretty bad.
log
log
distributed. This doesn't get used.
distributed. This doesn't get used.
We'll be we were considering doing stuff
We'll be we were considering doing stuff
with this
with this
though. Definitely some just stale code
though. Definitely some just stale code
we'll be able to just straight up delete
we'll be able to just straight up delete
with no
with no
issues. Stuff like this, you know,
issues. Stuff like this, you know,
instead of writing all these out, we
instead of writing all these out, we
probably can do something with
it. And then a lot of this is also like
it. And then a lot of this is also like
algorithm options. Let's look at the
algorithm options. Let's look at the
indents in here. So here we have
indents in here. So here we have
train
train
epoch. Are we missing the AM context
epoch. Are we missing the AM context
here? I think we're missing Oh, no. Here
here? I think we're missing Oh, no. Here
it is. Here's the ammp context. This can
it is. Here's the ammp context. This can
just go around the whole thing. I
just go around the whole thing. I
think just go around the whole function.
I wonder if there's a way to just do AM
globally. Uh because you don't want to
globally. Uh because you don't want to
have it globally, right?
M and V
forward. So you actually you
can't you can't just put this to the
can't you can't just put this to the
top. It has to go into this profile
top. It has to go into this profile
wrapper.
Okay. So
then I think no matter how I do this I
then I think no matter how I do this I
have one layer of wrappers right which
have one layer of wrappers right which
will be the profile wrappers
All these little if statements are going
All these little if statements are going
to get compressed anyways.
I could start with this.
I could start with this.
changing the
changing the
profiler and I can see where I end
profiler and I can see where I end
up. I think that's not a terrible idea.
How the heck does this even work?
ah each of these is actually a
ah each of these is actually a
popular.youutil.profiler I see.
So wait, when you
do Yeah. So it's with proper lab utils
do Yeah. So it's with proper lab utils
pro I say
Then you just have them all written out.
So I guess the initial thing that would
So I guess the initial thing that would
be very You see?
Okay. So now this should
Okay. So now this should
synchronize CUDA
everywhere and then we can do
on
text. Yes.
session. Guess we just do this.
So now how do we
do how do we do like start and end on a
do how do we do like start and end on a
context like this?
Okay. So now that's the context handled,
Okay. So now that's the context handled,
right?
and then we want to
pass ammp
context. All
right. So, I guess we'll figure out
where figure out which
But this should let us fix a whole bunch
But this should let us fix a whole bunch
of
stuff because
now we can just unindent all of
now we can just unindent all of
this,
this,
right? Unindent all this.
right? Unindent all this.
And I can also
And I can also
do also do
this. That one might still be needed.
But all these that are train related,
But all these that are train related,
you do not need do not need these.
Okay.
Mask. Okay. So, most of that should now
Mask. Okay. So, most of that should now
be
be
good. If we can get this to run.
No attributes include
No attributes include
the
same. Okay, this runs.
Looks like it still trains as
Looks like it still trains as
well. So that cleans up all the context
well. So that cleans up all the context
about as much as we can expect, right?
Like we're not going to be able to get
Like we're not going to be able to get
rid of these because we need some way of
rid of these because we need some way of
timing individual blocks of code. And it
timing individual blocks of code. And it
is cleaner to just indent one level than
is cleaner to just indent one level than
it is to try to keep track of like start
it is to try to keep track of like start
and stop. Right? So the only thing we
and stop. Right? So the only thing we
can kind of do is we can maybe optimize
can kind of do is we can maybe optimize
like where these blocks are getting
like where these blocks are getting
declared a little
bit. Maybe we can optimize like a little
bit. Maybe we can optimize like a little
bit
bit
there. But I'm looking at this eval
there. But I'm looking at this eval
forward.
This can go
here.
Yeah. That's pretty much as compact as
Yeah. That's pretty much as compact as
it's going to get.
Some of these if statements are again
Some of these if statements are again
these are going to get compressed so
these are going to get compressed so
don't worry about those just worry about
don't worry about those just worry about
like the overall indentation of it.
like the overall indentation of it.
This is min indent
This is min indent
level. One indent level for decorators
level. One indent level for decorators
like
this.
Um that we maybe can fix.
Okay, this is already
Okay, this is already
a I think the next biggest improvement
a I think the next biggest improvement
is going to come from the experience
is going to come from the experience
buffer
buffer
refactor. So
let's let's look at
that. How much does tensor
that. How much does tensor
dict do for us here?
We also need to look at this free
We also need to look at this free
function.
This is like kind of fine, right?
I make the uh the end it a little
I make the uh the end it a little
easier
easier
if I put this into a pass.
man. It's so hard to capture like the
man. It's so hard to capture like the
clean RL vibes, you
clean RL vibes, you
know? Costa is just really the
goat. It's like every time you try to
goat. It's like every time you try to
think of adding some
structure. I guess he did eventually go
structure. I guess he did eventually go
with a data class for args, but that's
with a data class for args, but that's
literally just
literally just
arg there's a class for agent and then
arg there's a class for agent and then
the rest of it's all just this
loop. Man, this is like such good code.
loop. Man, this is like such good code.
How do we like get back to
this, you know?
this, you know?
How do we get back to this?
Every additional piece of modularity
Every additional piece of modularity
that I add brings us farther and farther
that I add brings us farther and farther
away from this beautifully simple code.
100 lines of
code. What do we have that's adding to
code. What do we have that's adding to
this?
like does experience buffer even need to
like does experience buffer even need to
be a
class? Well, there are a few things that
class? Well, there are a few things that
we had to do here, right? Like that make
we had to do here, right? Like that make
this a little harder. So the first thing
this a little harder. So the first thing
was you need to be able to call eval and
was you need to be able to call eval and
train separately, right?
Now you need to be able to call evaluate
Now you need to be able to call evaluate
and
and
train separately.
right now I have three separate
right now I have three separate
functions for this. I've
functions for this. I've
got essentially a class without a
got essentially a class without a
class and I have eval and train but then
class and I have eval and train but then
I also have this separate rollout
I also have this separate rollout
function
Every feature that we add makes this
Every feature that we add makes this
harder. But what's even harder than
harder. But what's even harder than
adding a feature is adding an optional
adding a feature is adding an optional
feature,
right? Unless it's just like a smooth
right? Unless it's just like a smooth
parameterization where zero is off.
How I design this is going to depend
How I design this is going to depend
heavily
heavily
on how many features I expect Puffer Lib
on how many features I expect Puffer Lib
to
have. A few other things.
I really really want to get back to
I really really want to get back to
something closer to this though.
I'm going to sit here and agonize over
I'm going to sit here and agonize over
this as long as it takes for me to find
this as long as it takes for me to find
something some way to get closer back to
something some way to get closer back to
this. But I don't have to sit here and
this. But I don't have to sit here and
agonize all at once. That would be very
agonize all at once. That would be very
boring. Doesn't have to be all at once.
boring. Doesn't have to be all at once.
Um, I think for now what I do is I avoid
Um, I think for now what I do is I avoid
going more in the direction I don't want
going more in the direction I don't want
to go. So for now I don't make this a
to go. So for now I don't make this a
class. I leave this as
class. I leave this as
is. Um, I just see if I can locally
is. Um, I just see if I can locally
clean up the experience buffer a little
clean up the experience buffer a little
bit. And then we clean up and prune code
bit. And then we clean up and prune code
that doesn't need to be there. And then
that doesn't need to be there. And then
maybe that'll give us some additional
maybe that'll give us some additional
clarity.
clarity.
Maybe that'll give us additional
clarity with the knowledge that a lot of
clarity with the knowledge that a lot of
these conditionals are going to get
these conditionals are going to get
deleted. Hey YouTube
deleted. Hey YouTube
folks, this
folks, this
is currently me adding prioritized
is currently me adding prioritized
experience replay to puffer lips PO
experience replay to puffer lips PO
implementation. That is the goal here.
implementation. That is the goal here.
We have an initial implementation that
We have an initial implementation that
seems like it works. And
seems like it works. And
um now the goal is to clean up the dev
um now the goal is to clean up the dev
branch code so it's actually
usable with the new experience
buffer. I mean pretty much all these
buffer. I mean pretty much all these
tensors
tensors
are like these are almost all the same
are like these are almost all the same
size,
size,
right? So I don't necessarily have to go
right? So I don't necessarily have to go
to a tensor dict
to a tensor dict
immediately. But
um it would seem like we should go to a
um it would seem like we should go to a
name space,
right, that we can iterate
Maybe we can go part way.
So this is going to add you know some
So this is going to add you know some
redundant
code. We don't need all
this ops actions log props rewards done
chunks. Okay.
chunks. Okay.
So now we do store
And just values return for
terms. They miss
terms. They miss
advantages and values.
Yeah. Should these even need to be in
here? I don't think the advantages
here? I don't think the advantages
should need to be in here.
What about returns?
Iterating over these keys seems worse to
Iterating over these keys seems worse to
me even
somehow. It can't be though,
right? Just do it for now and then we'll
right? Just do it for now and then we'll
see how it looks.
Okay, let's just port up this P30 stuff
Okay, let's just port up this P30 stuff
as Oh,
The returns was advantages plus
values. All right, that's not
values. All right, that's not
bad. I think that's it, right? Returns
bad. I think that's it, right? Returns
mean standard deviation. All these
mean standard deviation. All these
stupid transposes go
stupid transposes go
away and we should be
away and we should be
good. Oh. Oh, diversity. All you need as
good. Oh. Oh, diversity. All you need as
well, this
one. But that's not bad either.
goes
goes
away. I don't like the dict indexing,
away. I don't like the dict indexing,
but we will uh you know, we'll deal with
that. I think I can do well, we'll get
that. I think I can do well, we'll get
to
to
it. That's sample cleaned
it. That's sample cleaned
up if it
runs. Few advantages.
See if that still works.
If
so, we should be able to clean up the
so, we should be able to clean up the
rest of it.
Right. That's
next name space. No attribute. Name
next name space. No attribute. Name
space.
Okay. See if that breaks
anything. Do we think we're ready for
anything. Do we think we're ready for
CUDA? CUDA J.
It would be simpler and a bit faster.
I think we just chill for now.
How much complexity are we left with
here? A lot of it is just from all these
here? A lot of it is just from all these
various different choices, isn't it?
Okay.
Yeah, I think we're on the stage where
Yeah, I think we're on the stage where
it's just got to be a lot of fiddly line
it's just got to be a lot of fiddly line
by line.
Lot of just fiddly line by
Lot of just fiddly line by
line
edits. Try not to break anything. Try to
edits. Try not to break anything. Try to
shorten the code. All of that.
Is there any way
Is there any way
um we can avoid having to recall
um we can avoid having to recall
the advantage code at the
end? I think not. But I think that we
end? I think not. But I think that we
would have like we could simplify it if
would have like we could simplify it if
J were part of the
buffer. Does that make sense
buffer. Does that make sense
even to have J part of the buffer?
It bothers
It bothers
me. Definitely bothers
me. How much stuff does this buffer have
me. How much stuff does this buffer have
that we don't even need? Right.
So, could all this stuff just be part of
So, could all this stuff just be part of
create, you know? I think it could be.
That's an interesting
That's an interesting
idea. So, what if we didn't have an
idea. So, what if we didn't have an
experience buffer class, but did kind of
experience buffer class, but did kind of
have like a trainer
have like a trainer
class? Well, we don't have to make the
class? Well, we don't have to make the
decision on a trainer class. We can just
decision on a trainer class. We can just
stick it in data for now, right?
That seems
That seems
better. No experience
component. That seems quite nice to me.
seems very good to me
seems very good to me
actually. Let's just take all this
code. Let's just move
code. Let's just move
it right here.
Ops D type action DT
type. Does this even get used lower down
anywhere? I don't think it even gets to
anywhere? I don't think it even gets to
used. Yeah. No, it doesn't.
You get the D types
here. See this next row.
It's not
bad. Experience equal upper li name
space obs equals
Come here.
And what I break
here LSTM total agents is just total
here LSTM total agents is just total
agents.
And then you just need all this mess,
And then you just need all this mess,
right?
Where's hidden size?
archive skills and
archive skills and
batch config
archive. I know what this
is. It totally screwed that up, didn't
is. It totally screwed that up, didn't
it? Don't ever press the auto
complete then skills is rand
complete then skills is rand
archive agents. Yep, that looks good.
archive agents. Yep, that looks good.
of
of
zeros. This
is
size. Something like that,
size. Something like that,
right? Something like that.
And what is this batch size thing here?
config. Just do batch size config batch
config. Just do batch size config batch
size and
size and
device.
device.
Yeah, just for now. And we'll we'll keep
Yeah, just for now. And we'll we'll keep
cleaning this up.
That takes care of it pretty
That takes care of it pretty
well now. You just need all these
well now. You just need all these
checks. All
right. Config mini batch and then config
right. Config mini batch and then config
max mini batch.
and we do the same
thing. And uh that's pretty much all
thing. And uh that's pretty much all
there is in there. Mini batch size, mini
there is in there. Mini batch size, mini
batch rows, device
batch rows, device
Pointer and
step. Do we even use pointer
anymore? We'll set him just in
case.
Cool. So now
Cool. So now
um we take these
um we take these
functions and put these in a reasonable
functions and put these in a reasonable
spot which will probably
spot which will probably
be below
be below
eval and train.
be a good spot.
experience
experience
X. We're going to end up typing this a
X. We're going to end up typing this a
lot.
This is going to be long
uh long iterative
process.
Actually, hang
Actually, hang
on. It's not. Let's not screw this
up. This is actually
data. Be data.
where GPU winds just
where GPU winds just
go. Oh, it was never uh Okay, that's
fine. And almost this is no longer data.
fine. And almost this is no longer data.
This is
X. It's going to get messier before it
X. It's going to get messier before it
gets
gets
simpler, but it is going to get simpler
Let's go back up to
Let's go back up to
experience. Make sure we're not doing
experience. Make sure we're not doing
stupid
things. Delete that massive
things. Delete that massive
mess.
mess.
Um, yeah. See, not all these things
Um, yeah. See, not all these things
belong here.
belong here.
All these things, these go to
sell. These we can punt on for
sell. These we can punt on for
now. This is definitely data
now. This is definitely data
though. What is it? LSTMH.
Okay. So now all these things need to go
Okay. So now all these things need to go
into here which is the annoying bit
into here which is the annoying bit
about how we have this right now.
what else? Mini batch rows, num mini
what else? Mini batch rows, num mini
batches, any other
batches, any other
stuff? Yeah, there should be a bunch of
stuff? Yeah, there should be a bunch of
things. So, stored indices, app lengths,
things. So, stored indices, app lengths,
app indices, free index,
name
space. This is data and this is data.
Yep. So there's on policy and off policy
Yep. So there's on policy and off policy
rows.
Like I say, it's going to get it's going
Like I say, it's going to get it's going
to get weird before it gets easier.
Ah, you left trailing commas on
Ah, you left trailing commas on
literally everything.
name space no attribute
name space no attribute
device
device
really. Well, we'll just add it for
now. Making a huge mess of this,
now. Making a huge mess of this,
but this is necessary for now in order
but this is necessary for now in order
to get it to a point where it's
cleaner. We have config here.
Not have config.
Now my mini batches.
is I thought that we had this.
Oh, now
Oh, now
data. This gets us at least closer to
data. This gets us at least closer to
what I want, which is not a whole bunch
what I want, which is not a whole bunch
of separate little utility classes.
And this is a
sample. Speed up.
Okay. into object
I probably left some stuff in here
I probably left some stuff in here
that's not supposed to be in
there. Pointer and step. That's not
there. Pointer and step. That's not
supposed to be in there.
broke my terminal.
Okay. Sample
data experience
items. We'll see how that goes.
Just a whole bunch of renaming of stuff
Just a whole bunch of renaming of stuff
required for
required for
this. Hopefully we didn't break
this. Hopefully we didn't break
anything. We'll
see. Got run the same speed as before
see. Got run the same speed as before
and hopefully nothing broken.
So now we have the entire big experience
So now we have the entire big experience
buffer inside this create function as
buffer inside this create function as
this massive
this massive
like not super massive honestly. Where
like not super massive honestly. Where
is
it? It's like a screen and a half of
it? It's like a screen and a half of
code. And mind you, a lot of that is for
code. And mind you, a lot of that is for
the different algorithms. So, this is
the different algorithms. So, this is
going to end up being like less than a
going to end up being like less than a
screen of code for the experience once
screen of code for the experience once
we get rid of extras that don't uh that
we get rid of extras that don't uh that
are not
are not
needed. That's not bad. I'm still going
needed. That's not bad. I'm still going
to be stubborn for now about not making
to be stubborn for now about not making
this a class. So, we'll just endure
this a class. So, we'll just endure
having this stupid name space for now.
But uh hopefully this allows us to
But uh hopefully this allows us to
co-optimize a few
co-optimize a few
things since we no longer have multiple
things since we no longer have multiple
different strus hanging
out. I think the next thing will be to
out. I think the next thing will be to
approximately fix all the algorithm
approximately fix all the algorithm
add-ons like these like E3B diversity
add-ons like these like E3B diversity
all you need all this stuff. um these
all you need all this stuff. um these
code blocks are no longer in the correct
code blocks are no longer in the correct
place after we've made these
place after we've made these
changes. So, we should probably fix
changes. So, we should probably fix
that. And then we can play around with
that. And then we can play around with
the experience buffer itself and how we
the experience buffer itself and how we
want that to
work. Probably will need some more
work. Probably will need some more
logging. I mean, there's going to be a
logging. I mean, there's going to be a
lot of cleanup here,
lot of cleanup here,
right? This file is just massively too
right? This file is just massively too
long. This is a 1200 line file. This
long. This is a 1200 line file. This
needs to be a three three-digit line
needs to be a three three-digit line
file for
sure. I know how to fix
that. Uh we also have the option to do
that. Uh we also have the option to do
CUDA module
CUDA module
for generalized advantage estimation.
for generalized advantage estimation.
That would probably be a good idea.
I think we'll time it first to make sure
I think we'll time it first to make sure
that it is actually an issue
that it is actually an issue
because well, it's probably not getting
because well, it's probably not getting
profiles correctly yet. All
profiles correctly yet. All
right, I'm going to use a restroom. I
right, I'm going to use a restroom. I
will uh be right back. I got to do one
will uh be right back. I got to do one
or two other quick things and then we'll
or two other quick things and then we'll
be back in a few and uh we
will we'll continue cleaning this up and
will we'll continue cleaning this up and
hopefully we at least have a reasonable
hopefully we at least have a reasonable
working prototype of this today with
working prototype of this today with
additional cleanup to continue tomorrow
additional cleanup to continue tomorrow
and then by midweek we should already
and then by midweek we should already
have no end of midweek we should already
have no end of midweek we should already
have something that looks pretty good.
have something that looks pretty good.
Right back few
All
All
right, I just have to put in a food
right, I just have to put in a food
order and
order and
um then we'll be back. The plan is going
um then we'll be back. The plan is going
to be I do super early dinner today. So,
to be I do super early dinner today. So,
I work on this for the
I work on this for the
next hour or so, and then I grab super
next hour or so, and then I grab super
early dinner. I get a little exercise
early dinner. I get a little exercise
in, and then I should be able to come
in, and then I should be able to come
back for a short evening session as
well. Just figure out what I'm getting.
do that.
Don't really need cars. All right.
Do we want to do CUDA advantage
Do we want to do CUDA advantage
already? Let's see how hard it would be.
Doesn't look like it would be that hard,
Doesn't look like it would be that hard,
right? This is what I had for
um
um
E30. So, we should pretty easily just be
E30. So, we should pretty easily just be
able to do
um do this
um do this
for PO, right?
Okay. So, this is going to
Okay. So, this is going to
take float star
take float star
values. This is going to
take
orbs. Oh, it also needs dumps.
it like this maybe values rewards
it like this maybe values rewards
dumps float it
uh float
uh float
gamma lambda and then we also need
gamma lambda and then we also need
non-steps and horizon.
Okay.
This is going to be row parallel, right?
Yeah, this is one
row. Literally, it's just this operation
row. Literally, it's just this operation
right here. Oh, you probably need the
right here. Oh, you probably need the
advantage buffer output as well.
Yes.
X row time horizon plus
T was it
T was it
road X +
So this is idx
So this is idx
next. This is
next. This is
idx
idx
next. Oops. I only want the single term
next. Oops. I only want the single term
autocomplete so I can make sure it
autocomplete so I can make sure it
doesn't mess anything up.
Delta rewards
Delta rewards
next. This is fine.
next. This is fine.
And then advantages of idx like
this. Okay. So this is roughly what our
this. Okay. So this is roughly what our
kernel should look
like. Was it C
like. Was it C
advantage? Where's the
binding?
C. Not this.
Can't be this
either. Okay, hang on. Where's this
load? Okay,
load? Okay,
load
load
pufferlib.cu. That's what I did.
So yeah, this is what I
did. I just need to get the signature
did. I just need to get the signature
for this and we'll be able to fix it.
This is now called P30
This is now called P30
kernel.
P3
kernel. Now this is
kernel. Now this is
P30
kernel. All right. And now this one
kernel. All right. And now this one
here which
here which
tenser
values
rewards
advantages. Okay.
And this is going to
And this is going to
be J
be J
kernel. This is J
kernel. Very nice.
kernel. Very nice.
We do a little cuda now and again, you
We do a little cuda now and again, you
know, we do a little bit of cuda now and
again. Start with
this. Uh we will just like redefine this
this. Uh we will just like redefine this
thing because we'll need this
thing because we'll need this
later.
later.
So this is
So this is
comput. And I'd love to figure out how
comput. And I'd love to figure out how
uh to reduce the amount of binding
uh to reduce the amount of binding
nonsense that you need for this, but
nonsense that you need for this, but
um yeah, for now we're just going to do
um yeah, for now we're just going to do
it this way because it'll still be
it this way because it'll still be
faster.
This
is where is it? I copied the wrong one.
is where is it? I copied the wrong one.
Stupid. Need the
signature. You know, I really haven't
signature. You know, I really haven't
written much CUDA, but it turns out if
written much CUDA, but it turns out if
you're used to writing C, CUDA is just C
you're used to writing C, CUDA is just C
that runs on the GPU.
kind of no big
deal. Honestly, dealing with the stupid
deal. Honestly, dealing with the stupid
binding module is more of a pain than
binding module is more of a pain than
writing the actual CUDA, assuming you're
writing the actual CUDA, assuming you're
doing, you know, embarrassingly easy
doing, you know, embarrassingly easy
stuff if you're just doing row parallel
stuff if you're just doing row parallel
or whatever. Obviously, they're like
or whatever. Obviously, they're like
complex block parallel screams. We don't
complex block parallel screams. We don't
get into that, but you don't need to
get into that, but you don't need to
most of the
most of the
time. Okay.
time. Okay.
Just got
that
steps signature.
How the hell did it cut a parameter
How the hell did it cut a parameter
off? Well, I wasn't looking to cut a
off? Well, I wasn't looking to cut a
parameter off. You see that?
All right.
And num steps in horizon is
actually launch.
Now we get to actually use the torch
Now we get to actually use the torch
tensor for this,
tensor for this,
right?
So we just do comput
J rewards, dons, advantages, gamma, and
J rewards, dons, advantages, gamma, and
lambda. Isn't that
nice? Then we do the same thing right
there. Sure.
What did we screw
up? There's some stuff in the cuda.
You forgot semicolons on literally
everything. What a
everything. What a
noob. And didn't you even forget? Yeah,
noob. And didn't you even forget? Yeah,
you forgot this too, you noob.
you forgot this too, you noob.
Okay, that's what I get for coding with
Okay, that's what I get for coding with
no LSP or anything.
I probably should have an
LSP in num
steps. Forgot this comma.
expected opening
brace
brace
block. This is just this is added for
block. This is just this is added for
you, isn't
you, isn't
it? Oh, duh.
it? Oh, duh.
All
right. This gun
Wait. I messed it up here as well. Yes,
Wait. I messed it up here as well. Yes,
I did.
J kernel's already been
defined. Forgot to delete this nonsense.
Uh, okay. That looks
worse. Def J kernel has already been
defined. Does this have to be
defined. Does this have to be
um a different
name? Oh, this should be compute.
name? Oh, this should be compute.
This should be compute P3L,
This should be compute P3L,
right? And this can be compute
right? And this can be compute
J. That way it doesn't conflict with the
J. That way it doesn't conflict with the
kernel
name. Still
name. Still
no J kernel already been defined.
Uh, I only see it once.
this
compute. I don't know which one of these
compute. I don't know which one of these
you
load. Pionet compute J has already been
defined. Port extension
name. What would that be processed by?
name. What would that be processed by?
Hang on. Do I get it for both of them
now? Oh, it's a single M.Daf, isn't
it? It's probably a single
M. Can I just do this?
That's probably better,
right?
Cool. So now that actually compiles and
Cool. So now that actually compiles and
the only thing that I get
is this warning which we can just
is this warning which we can just
comment this variable.
Then was it cannot access X.
I don't know how I do that. I just
I don't know how I do that. I just
accidentally delete a massive chunk of
code. All
right. Does it
right. Does it
work? Does it work first
work? Does it work first
try? Shall see.
You need uh advantages,
right?
Yeah. Let's do it this way.
Oh, that's way faster, isn't
Oh, that's way faster, isn't
it? Yeah, that's way faster. So, I don't
it? Yeah, that's way faster. So, I don't
even have to check if the uh the C
even have to check if the uh the C
implementation was slower or whatever
implementation was slower or whatever
because it obviously
is. That's good to know.
And yeah, we kind of immediately just
And yeah, we kind of immediately just
went with
that cuda very fast.
Okay, so we still saw a breakout now
Okay, so we still saw a breakout now
with
with
this good time to make a commit.
There you go. Chrome done.
So now it's mostly just going to be a
So now it's mostly just going to be a
matter of cleaning up the rest of the
matter of cleaning up the rest of the
code and uh then actually like figuring
code and uh then actually like figuring
out how uh the replay buffer interacts
out how uh the replay buffer interacts
with all this.
Right. It's
3:22. Okay. I'm probably gonna have
3:22. Okay. I'm probably gonna have
dinner. I'm having a really early dinner
dinner. I'm having a really early dinner
today. like probably within the next
today. like probably within the next
half hour. So, I'm just going to keep
half hour. So, I'm just going to keep
working on that until then and get
working on that until then and get
dinner, probably get a little bit of
dinner, probably get a little bit of
exercise in, and then I'll probably come
exercise in, and then I'll probably come
back for a evening
back for a evening
session. It's the goal. So, I think for
session. It's the goal. So, I think for
now what we do is we just start cleaning
now what we do is we just start cleaning
this stuff
up. So, this bit here. Whoops.
This goes up to
here. We indent this.
So this is
now values. Yeah. Yeah. Yeah. Okay. This
now values. Yeah. Yeah. Yeah. Okay. This
gets
gets
easier. So this is
easier. So this is
now
now
data.mmassblock.0
data.buff.0 data.rewardb reward
data.buff.0 data.rewardb reward
block. This is now
experience.rewards.
experience.rewards.
Experience
Experience
this. And
this. And
then
then
advantages is not stored here. I
guessbounce.0. Get rid of this.
guessbounce.0. Get rid of this.
What did I just
What did I just
do? Double
dot. It's probably something something
dot. It's probably something something
around like this, right? Don't need
around like this, right? Don't need
these anymore.
This
is I don't even necessarily want this to
is I don't even necessarily want this to
run just yet.
Uh, we don't need this flatten batch
either. Okay. Well, I don't even
either. Okay. Well, I don't even
necessarily want this stuff to run just
yet.
yet.
Um, yeah, I just want this to be like
Um, yeah, I just want this to be like
roughly in the right spot, in the right
roughly in the right spot, in the right
size.
And we don't need any of this.
This is not
This is not
experience. This is back
values.plat. Okay. Let me make sure this
values.plat. Okay. Let me make sure this
still
still
works. Slowly but surely, we are cutting
works. Slowly but surely, we are cutting
off lots and lots of code.
I don't know why this is
here. This can just be
Okay, that still works. Let's try this
Okay, that still works. Let's try this
now.
I don't know why this is a separate
function. Oh, it's because I see it gets
function. Oh, it's because I see it gets
used in both places. Fine.
used in both places. Fine.
We'll leave it alone for
now. But we are going to get rid of
that. Okay. Are we ready to take a look
that. Okay. Are we ready to take a look
now through this code and see uh see how
now through this code and see uh see how
this is overall?
this is overall?
So, we have our bindings. The bindings
So, we have our bindings. The bindings
are messy. The bindings will improve.
are messy. The bindings will improve.
We've got this create
function. It's got a giant block of
function. It's got a giant block of
stuff for allocating
stuff for allocating
experience. Most of it is like extra
experience. Most of it is like extra
algo specific. So, a lot of these things
algo specific. So, a lot of these things
will be gotten rid
of. Here's the rest of it.
of. Here's the rest of it.
We have this crazy messy namespace thing
We have this crazy messy namespace thing
to think
to think
about that's lower on the priority
about that's lower on the priority
list. What I really want to see is like
list. What I really want to see is like
evaluate and stuff like
this. So profile E is
this. So profile E is
val profile the M. Get
that. Here's the first little snippet
that. Here's the first little snippet
from diversity. All you
need
need
[Music]
copy, LSTMH indexing,
copy, LSTMH indexing,
forward with the state
tensor. Okay, so most of the like extra
tensor. Okay, so most of the like extra
crap here is just from the new algorithm
crap here is just from the new algorithm
like
like
components. So these will be
uh the goal will be we will finish
uh the goal will be we will finish
testing these and then we will either
testing these and then we will either
keep them or delete them.
And then actually the loop will be
And then actually the loop will be
pretty clean with possibly some small
pretty clean with possibly some small
you know line by line fixes that we need
you know line by line fixes that we need
to
to
make. Then we have train.
Do we use
this? No, we don't use any of
these. Log stuff will have to be
these. Log stuff will have to be
adjusted. We do
adjusted. We do
P3 and there's
PO.
Yep. Here's your train forward function.
So, we're pretty much back to
So, we're pretty much back to
like some line by line changes plus
like some line by line changes plus
the main thing being
um yeah, the main thing being the
um yeah, the main thing being the
algorithm head-ons. Okay, this is good
algorithm head-ons. Okay, this is good
progress. That might be dinner. Let me
progress. That might be dinner. Let me
go check on
something. All
right, that's dinner bell. Um, I will be
right, that's dinner bell. Um, I will be
back probably later tonight after
back probably later tonight after
dinner to continue working on this
dinner to continue working on this
thing. If you're interested in my stuff
thing. If you're interested in my stuff
or RL generally, I've got a pretty
or RL generally, I've got a pretty
pretty full streaming schedule the next
pretty full streaming schedule the next
full week. It's going to be streaming a
full week. It's going to be streaming a
lot of dev. So, puffer.ai start the
lot of dev. So, puffer.ai start the
GitHub repo to help us out. That really
GitHub repo to help us out. That really
helps us out. It's free. Just do
helps us out. It's free. Just do
it. Can join the Discord to get involved
it. Can join the Discord to get involved
with dev. A lot of our best contributors
with dev. A lot of our best contributors
came in with zero RL experience prior.
came in with zero RL experience prior.
It's a great way to learn. And it can
It's a great way to learn. And it can
follow me on X for more RL content.
