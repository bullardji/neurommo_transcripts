Kind: captions
Language: en
We are
We are
live.
Hi. Let me tweet out the uh the stream
Hi. Let me tweet out the uh the stream
and then uh it's a short stream. Just
and then uh it's a short stream. Just
going to be here for an hour and then I
going to be here for an hour and then I
will talk a little bit about uh some of
will talk a little bit about uh some of
the plans for stuff I have going forward
the plans for stuff I have going forward
and uh where my head's at on all the
and uh where my head's at on all the
latest things in RL land.
Also, let me make sure I got the time
Also, let me make sure I got the time
zones right. I'm pretty darn sure that I
zones right. I'm pretty darn sure that I
did, but let me make sure I'm not
did, but let me make sure I'm not
missing this
missing this
other meeting that it is in fact in an
other meeting that it is in fact in an
hour. Pretty darn sure that it was PST.
hour. Pretty darn sure that it was PST.
Yes, it is. Perfect.
Okay, so here's what is going on right
Okay, so here's what is going on right
now. Um, I am flying out to the new
now. Um, I am flying out to the new
facility on Thursday.
facility on Thursday.
I just made a massive Amazon order of
I just made a massive Amazon order of
all of the things that I'm going to need
all of the things that I'm going to need
uh to set up the stream at the new
uh to set up the stream at the new
facility as well as hardware. We got to
facility as well as hardware. We got to
move servers. We got to do a whole bunch
move servers. We got to do a whole bunch
of stuff for that. So, my hope uh is
of stuff for that. So, my hope uh is
that I will get there on Thursday. I
that I will get there on Thursday. I
will maybe be able to stream Friday or
will maybe be able to stream Friday or
Saturday, but definitely the next week I
Saturday, but definitely the next week I
should be there with my whole setup
should be there with my whole setup
nicely working.
nicely working.
Um, and that's going to be really cool.
Um, and that's going to be really cool.
I'm going to have some cool things to
I'm going to have some cool things to
show off with that. We don't have all
show off with that. We don't have all
the new servers yet. We just have the
the new servers yet. We just have the
existing ones until the tariffs go down.
existing ones until the tariffs go down.
But that's what it's going to be. And
But that's what it's going to be. And
when we get the new servers, the
when we get the new servers, the
facility will have capacity for them.
facility will have capacity for them.
And that means that contributors get
And that means that contributors get
access to more compute, which is going
access to more compute, which is going
to be great. Um, but in the
to be great. Um, but in the
meantime, we have Puffer in a pretty
meantime, we have Puffer in a pretty
awesome spot. Uh, the new dev branch is
awesome spot. Uh, the new dev branch is
kind of dissolving everything I throw at
kind of dissolving everything I throw at
it. It seems to be a very nice step up
it. It seems to be a very nice step up
from the current 20 main branch and the
from the current 20 main branch and the
key advant the key things that I've
key advant the key things that I've
added on which will be in a long blog
added on which will be in a long blog
post later but you know you get to hear
post later but you know you get to hear
it here early muon really big cosign and
it here early muon really big cosign and
kneeling seems like it works I need to
kneeling seems like it works I need to
do a little bit more testing on it
do a little bit more testing on it
because it kind of does weird things to
because it kind of does weird things to
the learning rate uh the experience
the learning rate uh the experience
buffer is a big one as well so we have
buffer is a big one as well so we have
this new experience buffer it works on
this new experience buffer it works on
trajectory
trajectory
segments. It's really difficult to
segments. It's really difficult to
design these things well uh in a way
design these things well uh in a way
that's performance. Most of the
that's performance. Most of the
implementations out there that are used
implementations out there that are used
in most RL libraries just suck. So I've
in most RL libraries just suck. So I've
done my best with it. I think it's
done my best with it. I think it's
pretty decent and that's what we're
pretty decent and that's what we're
going to work on in a few minutes is
going to work on in a few minutes is
trying to shore up a couple things with
trying to shore up a couple things with
that. Uh but the thing with that
that. Uh but the thing with that
experience buffer is it lets us do
experience buffer is it lets us do
advantage filtering. Uh so we can
advantage filtering. Uh so we can
actually like look at all the samples
actually like look at all the samples
and we can do like prioritized replay on
and we can do like prioritized replay on
which samples are actually good and
which samples are actually good and
train on those. Uh and that actually
train on those. Uh and that actually
seems to help a fair bit. So we have
seems to help a fair bit. So we have
that. We also have our brand new puffer
that. We also have our brand new puffer
exclusive algorithm. I guess it's uh
exclusive algorithm. I guess it's uh
it's kind of a neat little thing. It
it's kind of a neat little thing. It
combines V trace and generalized
combines V trace and generalized
advantage estimation. And we have a food
advantage estimation. And we have a food
kernel for it. So it's really really
kernel for it. So it's really really
fast. Uh, so between experience buffer
fast. Uh, so between experience buffer
changes, advantage filtering, Kio
changes, advantage filtering, Kio
Replay, all that, that's kind of what is
Replay, all that, that's kind of what is
at the moment looking like it is going
at the moment looking like it is going
into the next update. There a few other
into the next update. There a few other
things I want to play around with more.
things I want to play around with more.
Uh, exploration algorithms, diversity is
Uh, exploration algorithms, diversity is
all you need. That algorithm had like
all you need. That algorithm had like
some promising results. It needs a lot
some promising results. It needs a lot
more experiments. I probably going to
more experiments. I probably going to
have to put together a whole custom
have to put together a whole custom
environment uh just to see if that
environment uh just to see if that
algorithm works the way it should.
algorithm works the way it should.
Protein. Yes, thank you, Captain.
Protein. Yes, thank you, Captain.
Protein is huge. We have that whole
Protein is huge. We have that whole
hyperparameter tuning algo. Really,
hyperparameter tuning algo. Really,
really solid. I'm happy with that. Uh,
really solid. I'm happy with that. Uh,
that's pretty much what is the core algo
that's pretty much what is the core algo
side of the new update. There are some
side of the new update. There are some
fiddly things with the experience
fiddly things with the experience
buffer. There are some perf things with
buffer. There are some perf things with
the experience buffer that we're going
the experience buffer that we're going
to look at for the next hour or so, but
to look at for the next hour or so, but
outside of that, it's pretty darn good.
outside of that, it's pretty darn good.
Um, yeah. So, that's mostly where my
Um, yeah. So, that's mostly where my
mind is at. And my goal at the moment
mind is at. And my goal at the moment
timelinewise is I want to have by
timelinewise is I want to have by
sometime next week I want to have this
sometime next week I want to have this
experience buffer ideally this week but
experience buffer ideally this week but
we'll see you know with all the stuff I
we'll see you know with all the stuff I
have to do to prep for the new facility.
have to do to prep for the new facility.
I want to have the experience buffer uh
I want to have the experience buffer uh
really nice and stable because that's
really nice and stable because that's
the last remaining like annoying thing
the last remaining like annoying thing
that can cause issues. Uh yeah. So I
that can cause issues. Uh yeah. So I
think that is mostly what I'm thinking
think that is mostly what I'm thinking
about. We've got a whole bunch of new
about. We've got a whole bunch of new
M's that I want to get added to Puffer.
M's that I want to get added to Puffer.
Waiting on Captain for the Impulse Force
Waiting on Captain for the Impulse Force
PR so I can run some experiments for
PR so I can run some experiments for
him. Uh the meta stuff is awesome. I
him. Uh the meta stuff is awesome. I
just got decent baselines on that. Uh we
just got decent baselines on that. Uh we
got kind of okay benchmarks on our new
got kind of okay benchmarks on our new
GPU drive port with Spencer yesterday.
GPU drive port with Spencer yesterday.
Uh we're going to see what his hyper
Uh we're going to see what his hyper
sweep does. I don't think he posted it.
sweep does. I don't think he posted it.
Did he? He posted Spencer. Oh, wait. He
Did he? He posted Spencer. Oh, wait. He
did. restarting sweep because time step
did. restarting sweep because time step
still went
beyond. Let's see what he gets. We'll
beyond. Let's see what he gets. We'll
take look at these real
quick. Not amazing. Yeah, not amazing.
quick. Not amazing. Yeah, not amazing.
Um, that actually seems kind of sketchy,
Um, that actually seems kind of sketchy,
doesn't it?
Where's the
score? So, this one is 64, which is
score? So, this one is 64, which is
lower than what I had
lower than what I had
before. And this is like by far the
before. And this is like by far the
cleanest. Now, something's weird if it's
cleanest. Now, something's weird if it's
hard plateaued right here, I think.
hard plateaued right here, I think.
Right.
That seems like something's wrong.
That seems like something's wrong.
Show. Let me send this dispenser.
All
right, there we
right, there we
go. So, Spencer's got this end. Got
go. So, Spencer's got this end. Got
Impulse Wars. Uh, Tower Climb is pretty
Impulse Wars. Uh, Tower Climb is pretty
awesome as well. He's That one's like
awesome as well. He's That one's like
stable and good. So, we have one, two,
stable and good. So, we have one, two,
is it three new big environments plus
is it three new big environments plus
blastar uh bet did blast and cartpole
blastar uh bet did blast and cartpole
simple m plus we got uh the CPR the
simple m plus we got uh the CPR the
common was it CRP was it common pool
common was it CRP was it common pool
resource or something we got that as
resource or something we got that as
well so I think that is a total of is
well so I think that is a total of is
that six new environments yeah six new
that six new environments yeah six new
environments three of which are pretty
environments three of which are pretty
darn big so that's nice and then
darn big so that's nice and then
Spencer's going to be working on more
Spencer's going to be working on more
end stuff but that seems good for update
Still around.
All
right, let's go to the experience
right, let's go to the experience
buffer because I think everything else
buffer because I think everything else
is in a good is in a good uh spot.
is in a good is in a good uh spot.
Um, I guess I will say a couple of the
Um, I guess I will say a couple of the
other things on my mind. So, I need to
other things on my mind. So, I need to
get the servers at some point. I have no
get the servers at some point. I have no
idea when because they're like plus 40%
idea when because they're like plus 40%
price tag right now with the tariffs.
price tag right now with the tariffs.
I hope that these don't stay in place
I hope that these don't stay in place
because it would be unbelievably stupid
because it would be unbelievably stupid
to like have an exemption on assembled
to like have an exemption on assembled
machines but not on components because
machines but not on components because
you're literally incentivizing
you're literally incentivizing
assembling machines in China and then
assembling machines in China and then
shipping them here instead of shipping
shipping them here instead of shipping
the parts and doing the assembly
the parts and doing the assembly
business in America. So that seems like
business in America. So that seems like
just an oversight to me. That doesn't
just an oversight to me. That doesn't
seem intentional. Um yeah, but I don't
seem intentional. Um yeah, but I don't
know. So,
uh, that's one thing. The other thing is
uh, that's one thing. The other thing is
that at some point I'm going to have to
that at some point I'm going to have to
do like a monthlong deep dive into hard
do like a monthlong deep dive into hard
RLT
RLT
area. Yeah. And the reason for that is
area. Yeah. And the reason for that is
that there's like a whole bunch of
that there's like a whole bunch of
algorithmic innovations from DeepMind
algorithmic innovations from DeepMind
that they've like gone this whole other
that they've like gone this whole other
direction with RL. And uh I want to see
direction with RL. And uh I want to see
what's in there and whether any of it's
what's in there and whether any of it's
applicable, but I want to get the new
applicable, but I want to get the new
update out before I do that because
update out before I do that because
otherwise it's like just lag too far
otherwise it's like just lag too far
behind. Okay, let's talk about
behind. Okay, let's talk about
experience buffer. We do real
experience buffer. We do real
engineering.
engineering.
Now, there are a few problems with this.
Now, there are a few problems with this.
Um one, it doesn't have a clean CUDA
Um one, it doesn't have a clean CUDA
fallback. It can run on CPU, but you
fallback. It can run on CPU, but you
still need to have CUDA even to run it
still need to have CUDA even to run it
on CPU the way it's set up. uh there's a
on CPU the way it's set up. uh there's a
threading issue where you can't use like
threading issue where you can't use like
you have to have a certain batch size or
you have to have a certain batch size or
it breaks and then the really big one is
it breaks and then the really big one is
that uh the way the experience buffer
that uh the way the experience buffer
collects data it kind of just breaks if
collects data it kind of just breaks if
you have the wrong number of
you have the wrong number of
environments and I haven't been able to
environments and I haven't been able to
figure out a good workaround for that.
figure out a good workaround for that.
So I think that those are the three
So I think that those are the three
things we're going to fiddle with at the
things we're going to fiddle with at the
moment. We can start with the easy ones
moment. We can start with the easy ones
here though for sure. So uh if I just
go open the CUDA
go open the CUDA
file. I was thinking about
file. I was thinking about
this and I think that
this and I think that
um yeah so this numbum steps
here I don't know if this is allowed to
here I don't know if this is allowed to
be a not a multiple of 128. I think this
be a not a multiple of 128. I think this
number has to
number has to
be but this number doesn't maybe.
be but this number doesn't maybe.
So, let's
do I think it's 256
max steps.
Is this going to compile out or is this
Is this going to compile out or is this
going to actually
going to actually
do? This shouldn't get compiled out,
do? This shouldn't get compiled out,
right?
We'll try
that. And then to test this, we'll just
that. And then to test this, we'll just
get like some simple
get like some simple
N breakout
maybe. We'll recompile kernel and then
maybe. We'll recompile kernel and then
we're going to mess around with the
we're going to mess around with the
number of amps and the batch size a
number of amps and the batch size a
bunch.
It's my 1 hour alarm. I'm good for a
bit. But so we can see here breakout
bit. But so we can see here breakout
trains and it should solve in under a
trains and it should solve in under a
minute. 1.8 8 million steps per second.
minute. 1.8 8 million steps per second.
Very, very fast.
Right. That doesn't look like 1.8
Right. That doesn't look like 1.8
million steps per second to me, though.
million steps per second to me, though.
It seems like it's a bit slower, doesn't
It seems like it's a bit slower, doesn't
it? It seems like it's more like 1
it? It seems like it's more like 1
million. So, the reporting might be
million. So, the reporting might be
wrong, which would make sense. I did
wrong, which would make sense. I did
mess with the profiling.
Looks more like a million. Yeah, we'll
Looks more like a million. Yeah, we'll
fix that though. We should be able to
fix that though. We should be able to
get close to 2
get close to 2
million. But there you go. That's
million. But there you go. That's
breakout
sol. Next, we
sol. Next, we
do let's just mess with this number of
do let's just mess with this number of
M's. Like, let's just do
M's. Like, let's just do
like let's make it some awkward number
like let's make it some awkward number
of like 14 m or
something. Ready to open attempt dirty
something. Ready to open attempt dirty
Impulse Wars PR? Awesome.
Impulse Wars PR? Awesome.
You did. You check that it trains in
You did. You check that it trains in
puffer like that though.
should definitely make sure we got
should definitely make sure we got
baselines like give me something to work
baselines like give me something to work
off of and we will uh merge that. I have
off of and we will uh merge that. I have
a meeting at 11:00 and then probably
a meeting at 11:00 and then probably
like breakfast and stuff or brunch and
like breakfast and stuff or brunch and
stuff and then afternoon I can do merge
stuff and then afternoon I can do merge
for you and uh we can start getting that
for you and uh we can start getting that
yeah into puffer before test
yeah into puffer before test
ends or benchmark
ends or benchmark
ends. Okay, so this does run. It's not
ends. Okay, so this does run. It's not
supposed to be fast. We just want to
supposed to be fast. We just want to
make sure that it doesn't break
make sure that it doesn't break
horribly. If I set the mini batch size
horribly. If I set the mini batch size
to
to
like Well, if I set this to like
512. Okay, it still works.
Lovely. And
then, oh, actually, there's one other
then, oh, actually, there's one other
thing I wanted to test first.
there's like some weirdness in uh with
there's like some weirdness in uh with
one end. I
believe seems like it works here just
believe seems like it works here just
fine.
Just per
batch. I'm trying to repro this bug that
batch. I'm trying to repro this bug that
people hit occasionally when they try to
people hit occasionally when they try to
like use one end or
like use one end or
whatever. Doesn't seem to repro
nicely. I think this was the one that
nicely. I think this was the one that
people were getting the
people were getting the
issue. Was this not a native end?
Maybe that's one
Yeah, let me go check on one other thing
Yeah, let me go check on one other thing
for Spencer as well. I was supposed to
for Spencer as well. I was supposed to
set
set
up We were supposed to set up a meeting.
floating point exception for
floating point exception for
dumped.
Well, that doesn't seem like a MI
Well, that doesn't seem like a MI
problem. I would doubt that that is a MI
problem. I would doubt that that is a MI
problem.
problem.
How about we do config Atari
breakout? Let's see how that
breakout? Let's see how that
does. Native vector effects. It's
does. Native vector effects. It's
multipprocessing. It's
fine. Okay. Okay. So, Atari
has
has
multiprocessing and then we do
multiprocessing and then we do
uh 128
uh 128
m 16 workers 32 and baptize.
This policy is
This policy is
probably a comm policy,
right? Yeah. pump policy, but it doesn't
right? Yeah. pump policy, but it doesn't
have Star Parks.
doesn't need output
doesn't need output
size or no this one doesn't need output
size or no this one doesn't need output
size
size
right this or it doesn't need num layers
anymore just making sure we still have
anymore just making sure we still have
our mms
running we definitely need to remove
running we definitely need to remove
like some of these
like some of these
is continuous blah blah blah.
3.9 mil per is actually not that far off
3.9 mil per is actually not that far off
from what we're training with neural MMO
from what we're training with neural MMO
at like millions uh was it 500k steps
at like millions uh was it 500k steps
per second. So we should actually be
per second. So we should actually be
able to make this fast maybe.
So, what's happened here?
Why is this
Why is this
a You know what this probably
a You know what this probably
is? Hang on. We know what this
is. We don't need this anymore.
is. We don't need this anymore.
That's just API shenanigans like growing
That's just API shenanigans like growing
pain cans puff are getting
pain cans puff are getting
bigger puff are getting simpler but
bigger puff are getting simpler but
um have to refactor things.
We technically probably should just have
We technically probably should just have
like um plugandplay
like um plugandplay
Atari benchmark that we can just
Atari benchmark that we can just
run. It's just so freaking compute
run. It's just so freaking compute
inefficient. It's like
We could do it like 30 minutes max per
We could do it like 30 minutes max per
end or something so that we run the
end or something so that we run the
whole suite in a
day. We probably still get some decent
perf. Okay, the SPS will update in a
perf. Okay, the SPS will update in a
second.
92% in
learn. That seems
sketchy. Doesn't that seem sketchy to
you? Well, let's make it sketchier first
you? Well, let's make it sketchier first
by doing
by doing
One,
One,
one,
one. Certex. Yep, that's the experience
one. Certex. Yep, that's the experience
buff for shenanigans I'm fixing right
buff for shenanigans I'm fixing right
now. Um, that means your batch size
now. Um, that means your batch size
isn't big
isn't big
enough or you don't have enough M's
enough or you don't have enough M's
rather for the I think it means you have
rather for the I think it means you have
too many M's for your batch
size. I'm going to add warnings and
size. I'm going to add warnings and
stuff so that like that's easier to not
stuff so that like that's easier to not
do.
just like either double the batch size
just like either double the batch size
or half the number of M's and see if it
works. Okay, this seems like it's
works. Okay, this seems like it's
broken,
broken,
right? Oh, no. It
right? Oh, no. It
works. It's just really freaking
works. It's just really freaking
slow.
slow.
Works. So, I think I might have fixed
Works. So, I think I might have fixed
that other weird bug like because I'm
that other weird bug like because I'm
not I'm not hitting it
anymore. Let's go back to this
one. Mini batch size of 1024. This is
one. Mini batch size of 1024. This is
probably too small,
right? Or too big or something. What the
right? Or too big or something. What the
hell is this?
Now I'm getting no steps
Now I'm getting no steps
percent. Uh so that's the thing I
percent. Uh so that's the thing I
literally just patched. But now it means
literally just patched. But now it means
that you have too few environments per
batch. Yeah. So this is the jank thing
batch. Yeah. So this is the jank thing
with the uh the new experience buffer
with the uh the new experience buffer
that I'm fixing. It works, but it
that I'm fixing. It works, but it
doesn't like certain param ranges right
doesn't like certain param ranges right
now. So I'm fixing
now. So I'm fixing
that. It's stuff like um Pudinum
that. It's stuff like um Pudinum
threads, right? Like that's like pudinum
threads, right? Like that's like pudinum
threads. I just fixed that. Um and then
threads. I just fixed that. Um and then
there like other related
there like other related
shenanigans
basically. That's good. Now, let's fix
basically. That's good. Now, let's fix
that bug next. Well, not even really a
bug. Let me just send this over to you.
Okay, take this. If I rebase it out, uh,
Okay, take this. If I rebase it out, uh,
well, it'll fix some of your issues and
well, it'll fix some of your issues and
probably cause several
others, I would imagine.
I mean, it's probably worth at least
I mean, it's probably worth at least
trying because then you'll have up to
trying because then you'll have up to
date with this and we'll see if we can
date with this and we'll see if we can
get it to train on
get it to train on
that like on the
latest. The one that is tricky though is
latest. The one that is tricky though is
the one we're going to try to fix right
the one we're going to try to fix right
now. I got to think about the experience
now. I got to think about the experience
buffer the way it
works. I can draw this actually.
If they start putting like product
If they start putting like product
placement stuff into LLMs, I'm just
placement stuff into LLMs, I'm just
ditching them
immediately. It's like half the use case
immediately. It's like half the use case
of what I use it for is like finding
of what I use it for is like finding
stuff. If it's like sponsored chat GPT
stuff. If it's like sponsored chat GPT
suggestion,
nope. All right.
nope. All right.
So, here's the thing with the experience
So, here's the thing with the experience
buffer. The way it used to work was the
buffer. The way it used to work was the
experience buffer looked like
experience buffer looked like
this. And you'd get a block of data in
this. And you'd get a block of data in
like
like
so. And the block of data has lots of uh
so. And the block of data has lots of uh
different agents worth of experience.
different agents worth of experience.
And you just do
this. And then the next block comes
in. You again just do this.
in. You again just do this.
And then when you need to train, when
And then when you need to train, when
you need to
train, what you have to do is you make
train, what you have to do is you make
like a big buffer like
this. Well, you have to like sort it
this. Well, you have to like sort it
first.
first.
So first this is like hang
on you have to like sort it first so
on you have to like sort it first so
that you know this where is it this one
that you know this where is it this one
here has to line up with this one here.
here has to line up with this one here.
So you get like this.
So there's like this complicated sort
So there's like this complicated sort
operation
operation
here where like you need the first step
here where like you need the first step
from each a or you need the step of all
from each a or you need the step of all
the agent one like all of agent one time
the agent one like all of agent one time
steps and then all of ages time steps
steps and then all of ages time steps
and whatever. So you do a sort and then
and whatever. So you do a sort and then
you would reshape it into like this
you would reshape it into like this
block goes here and this block goes here
block goes here and this block goes here
and then this block goes here. That's
and then this block goes here. That's
how it worked.
Um, this is cool, but and this like
Um, this is cool, but and this like
handles variable length data segments
handles variable length data segments
very very well. The issue with
very very well. The issue with
this is that because you have variable
this is that because you have variable
length data segments when you do this
length data segments when you do this
reshape, you don't actually get like
reshape, you don't actually get like
contiguous
contiguous
uh like you're going to get overflow and
uh like you're going to get overflow and
like segments are going to get cut
like segments are going to get cut
across in between these
So, how I currently have the new one
working your new experience buffer,
working your new experience buffer,
right? Um, this goes here, this goes
right? Um, this goes here, this goes
here, this goes here, this goes here,
here, this goes here, this goes here,
and it saves it pretty much in just the
and it saves it pretty much in just the
order that you want to use your data,
order that you want to use your data,
right? So, this is like all ready to go.
right? So, this is like all ready to go.
Um there are a few issues on this that
Um there are a few issues on this that
I've punted on like what happens if m
I've punted on like what happens if m
return very different amounts of data
return very different amounts of data
then like some of these are longer than
then like some of these are longer than
others and you get problems. I have some
others and you get problems. I have some
ways of handling this but it's not
ways of handling this but it's not
ideal. Uh but this does fix this
ideal. Uh but this does fix this
annoyance. So what you can do with this
annoyance. So what you can do with this
is uh you can just
is uh you can just
like filter out which of these segments
like filter out which of these segments
you want and which of these segments you
you want and which of these segments you
don't
want. That's what you can do with this
want. That's what you can do with this
design.
design.
Now the
Now the
issue
issue
is you actually have to start thinking
is you actually have to start thinking
about like the number of rows and stuff
about like the number of rows and stuff
that you
that you
have because this
have because this
is so this is your segment length. So
is so this is your segment length. So
this is like BP
this is like BP
TT
TT
horizon and then this
is this is
is this is
like
bash divided
by and this actually has to be a
by and this actually has to be a
reasonable number right so that's the
reasonable number right so that's the
error that captain was getting before
error that captain was getting before
with the assert free index less than
with the assert free index less than
equal experience rows right up there um
equal experience rows right up there um
is that the batch size wasn't big enough
is that the batch size wasn't big enough
for the number of ms
for the number of ms
because you also have to think about if
because you also have to think about if
you have each of these rows right each
you have each of these rows right each
row is an agent so if you like have
row is an agent so if you like have
let's say 40
let's say 40
m then you have to have batch
Right. And ideally you have it as a
Right. And ideally you have it as a
constant multiple because otherwise you
constant multiple because otherwise you
get um if you don't have it as a
get um if you don't have it as a
constant multiple then like you're not
constant multiple then like you're not
going to have anywhere to put your data
going to have anywhere to put your data
for a lot of your
for a lot of your
environments. So I
environments. So I
mean I kind of want to think about this
mean I kind of want to think about this
design for a little bit.
design for a little bit.
cuz I'm open to maybe like maybe the old
cuz I'm open to maybe like maybe the old
one was better and this is just annoying
one was better and this is just annoying
as
as
hell,
hell,
but it's kind of tricky to say
but it's kind of tricky to say
because the way that I did this before
because the way that I did this before
uh you also couldn't do GA anywhere near
uh you also couldn't do GA anywhere near
as fast. So when you have this uh this
as fast. So when you have this uh this
buffer up top and you sort it, you still
buffer up top and you sort it, you still
end up with one gigantic row, right? So
end up with one gigantic row, right? So
when you run generalized advantage
when you run generalized advantage
estimation on this or you run some
estimation on this or you run some
advantage formula, you have to start at
advantage formula, you have to start at
the back, right? And then you keep like
the back, right? And then you keep like
hopping backwards until you hit a
hopping backwards until you hit a
done. Like what is this D? Like a done.
done. Like what is this D? Like a done.
And then you go to the next one.
And then you go to the next one.
But you can't parallelize this at all.
But you can't parallelize this at all.
Like you just have to write a C loop and
Like you just have to write a C loop and
that's not actually fast enough. Even
that's not actually fast enough. Even
doing this like in optimized C is not
doing this like in optimized C is not
fast enough. So the cool thing with this
fast enough. So the cool thing with this
uh is now we have CUDA kernels, right?
uh is now we have CUDA kernels, right?
And the CUDA kernels are row parallel.
And the CUDA kernels are row parallel.
So they're really really really fast.
So they're really really really fast.
And that's actually important for what
And that's actually important for what
we're doing the new advantage filtering
we're doing the new advantage filtering
stuff. If you
stuff. If you
have like fully vectorized ends with
have like fully vectorized ends with
that return fixed length data
that return fixed length data
segments, I can make this work
segments, I can make this work
100%. Because you basically just go from
100%. Because you basically just go from
n1 to n2 to n1 to n2 to n1 to n2. You
n1 to n2 to n1 to n2 to n1 to n2. You
get like this nice buffer uh this nice
get like this nice buffer uh this nice
like buffer sampling and everything just
like buffer sampling and everything just
works perfectly.
works perfectly.
So I guess the question
So I guess the question
is how much do we
is how much do we
care about environments with like
care about environments with like
masking and like what is the reasonable
masking and like what is the reasonable
workaround for that? Because the top one
workaround for that? Because the top one
here, this natively just handles
here, this natively just handles
masking, variable agent length. It
masking, variable agent length. It
handles all that stuff just
handles all that stuff just
perfectly. That's why I did it this way
perfectly. That's why I did it this way
originally because it actually handles
originally because it actually handles
all like the jank ends perfectly.
The new one doesn't really handle that
The new one doesn't really handle that
at
all. Well, I mean, at the possible
all. Well, I mean, at the possible
expense of a little bit of
expense of a little bit of
uh CPU end of
uh CPU end of
time, which probably isn't even going to
time, which probably isn't even going to
be that bad to be
be that bad to be
fair. I can make it so that the
fair. I can make it so that the
vectorzation always samples contiguous
vectorzation always samples contiguous
segments of environments and like does
segments of environments and like does
this sort of ring buffer thing or this
this sort of ring buffer thing or this
like multiple buffered thing, right? I
like multiple buffered thing, right? I
can actually do that very
nicely. So even for single agent M and
nicely. So even for single agent M and
like stuff like Atari, I actually can
like stuff like Atari, I actually can
guarantee you that you fill up this
guarantee you that you fill up this
buffer perfectly.
Now you are going to end up with
Now you are going to end up with
um M's that go across segments I
um M's that go across segments I
guess like let's say that this is 128 or
guess like let's say that this is 128 or
this is something really long like you
this is something really long like you
could get a done signal right
could get a done signal right
here but it's not that bad cuz
here but it's not that bad cuz
like at least you're going to keep your
like at least you're going to keep your
LSTM state from the same you're going to
LSTM state from the same you're going to
keep your LSTM state from the same
keep your LSTM state from the same
agent. This is how the data was
agent. This is how the data was
collected during training. This is kind
collected during training. This is kind
of probably fine.
The other issue though is this kind of
The other issue though is this kind of
does force you
does force you
into really long BPT
into really long BPT
horizons. Um so you could get into a
horizons. Um so you could get into a
state where like you don't have enough
state where like you don't have enough
rows per batch to be compute efficient
rows per batch to be compute efficient
because you need to have at least a
because you need to have at least a
certain number of rows uh in your batch
certain number of rows uh in your batch
for the GPU to be efficient. And you
for the GPU to be efficient. And you
know, if you make this really
long, is that a problem? Yeah, because
long, is that a problem? Yeah, because
the mini batch size gets too
the mini batch size gets too
big. But technically,
big. But technically,
like I could always chunk this up,
like I could always chunk this up,
right? I could cut this in half this way
right? I could cut this in half this way
if like that became an issue. And that
if like that became an issue. And that
would actually be not that difficult to
would actually be not that difficult to
do. I
do. I
think I could do that. So, I think that
think I could do that. So, I think that
this design works for that.
Um, so I mean with some warnings on like
Um, so I mean with some warnings on like
acceptable scales and
acceptable scales and
stuff, this is mostly
stuff, this is mostly
fine. The only thing that where this
fine. The only thing that where this
actually really does screw up I think
actually really does screw up I think
it's and this actually might affect
it's and this actually might affect
captain here.
captain here.
Um, it's when you have like masked
Um, it's when you have like masked
multi- aent
multi- aent
Ms because like you can get a bunch of
Ms because like you can get a bunch of
data for one agent and then a few steps
data for one agent and then a few steps
for another
agent. Not
agent. Not
ideal. The old design was way better for
ideal. The old design was way better for
that specific case.
I keep getting the assertion in
popcu. I tried playing around with batch
popcu. I tried playing around with batch
size.
size.
260k batch 128mm 16
workers. I rebased your latest. You
workers. I rebased your latest. You
still get it with the latest
commit. The thread assertion should you
commit. The thread assertion should you
should wait. Are you getting the thread
should wait. Are you getting the thread
assertion or the indexing one? The free
index. You're getting the thread
index. You're getting the thread
error. Okay, that's weird.
What's your uh your end batch
size? So, you're only getting 32 M's
size? So, you're only getting 32 M's
each with like what? Four
agents. Sure.
agents. Sure.
Let's do that real
quick and then I will go back to
quick and then I will go back to
fixing experience buffer
fixing experience buffer
stuff. I the masking thing is going to
stuff. I the masking thing is going to
drive me nuts. I that's the main thing I
drive me nuts. I that's the main thing I
got to figure out. I think everything
got to figure out. I think everything
else is workable.
I am on the Discord. Captain, when
I am on the Discord. Captain, when
you're ready,
I wonder if ad companies have already
I wonder if ad companies have already
started like trying to data poison
LLMs. That's going to be
LLMs. That's going to be
annoying. Desktop isn't recognized in
annoying. Desktop isn't recognized in
headphones. Always fun.
headphones. Always fun.
I was actually looking at getting a
I was actually looking at getting a
higherend headset yesterday, but then I
higherend headset yesterday, but then I
thought that like I actually wanted to
thought that like I actually wanted to
keep this one because this has this
keep this one because this has this
retractable mic just in case I need a
retractable mic just in case I need a
backup because my really nice mic
backup because my really nice mic
doesn't get recognized by like some
doesn't get recognized by like some
stupid conferencing software.
So, I'm getting a very nice mic and a
So, I'm getting a very nice mic and a
very nice camera and all the things you
very nice camera and all the things you
need for those. And then the same like
need for those. And then the same like
shitty
headset. Shitty for audio quality. These
headset. Shitty for audio quality. These
are actually super
comfortable. They are SEO spam. Most LM
comfortable. They are SEO spam. Most LM
companies scrape without much
filtering.
Um, I guess I don't know if that is
Um, I guess I don't know if that is
going to matter as much as like fake
going to matter as much as like fake
review stuff. I think the LLM's will be
review stuff. I think the LLM's will be
like way more screwed up by like fake
like way more screwed up by like fake
reviews and
things. I mean like I went through a
things. I mean like I went through a
whole thing yesterday trying to figure
whole thing yesterday trying to figure
out a like high-end AV stuff with uh
out a like high-end AV stuff with uh
with
with
Brock and I mean it's like you really
Brock and I mean it's like you really
want confidence in the answers to simple
want confidence in the answers to simple
things like hey what do uh what
things like hey what do uh what
microphone do uh like most profession
microphone do uh like most profession
like what most like professional
like what most like professional
streamers use that actually have
streamers use that actually have
high-rade stuff like what mic and camera
high-rade stuff like what mic and camera
do they get? Like it's actually hard to
do they get? Like it's actually hard to
get like clear consistent answers to
get like clear consistent answers to
stuff like
that even though most of them are using
that even though most of them are using
like one of the same three cameras or
microphones. But yeah, long story short,
microphones. But yeah, long story short,
I got like a whole AV setup with like a
I got like a whole AV setup with like a
I mean a fullsize camera, tripod, like
I mean a fullsize camera, tripod, like
boom, proper boom arm. Um it's going to
boom, proper boom arm. Um it's going to
be really cool for the new facility.
be really cool for the new facility.
And it's going to let me do some cool
And it's going to let me do some cool
new content around all this dev.
And I will keep the like webcam whatever
And I will keep the like webcam whatever
set up
set up
for elsewhere because I'm not going to
for elsewhere because I'm not going to
be in that facility year round but good
be in that facility year round but good
chunk of the
year. Hey C.
year. Hey C.
Uh, okay. Can you hear me? Yep. Cool.
Uh, okay. Can you hear me? Yep. Cool.
All right. Uh, let me log in the buffer
All right. Uh, let me log in the buffer
box. Can I had to reboot and then I will
box. Can I had to reboot and then I will
uh share my Your box is good, right?
uh share my Your box is good, right?
Yeah. Oh, yeah. Sorry, I forgot to uh
Yeah. Oh, yeah. Sorry, I forgot to uh
What one are you on? Uh, box two. Okay.
What one are you on? Uh, box two. Okay.
Do make sure you have everything synced
Do make sure you have everything synced
today because um tomorrow all boxes that
today because um tomorrow all boxes that
have either broken CPUs or loose fans
have either broken CPUs or loose fans
are getting sent to Main Gear most
are getting sent to Main Gear most
likely. Uh, and then I'm going to like
likely. Uh, and then I'm going to like
rep prioritize compute so that people
rep prioritize compute so that people
actually still have stuff. I think I'm
actually still have stuff. I think I'm
taking both of my personal boxes with me
taking both of my personal boxes with me
and then I will give up my uh the two
and then I will give up my uh the two
that I'm using on the cluster for
that I'm using on the cluster for
temporarily so people can use stuff.
temporarily so people can use stuff.
Okay. Um, all right. Can you see my
Okay. Um, all right. Can you see my
screen?
screen?
See, and of course you know we're on
See, and of course you know we're on
stream, right? Yep.
stream, right? Yep.
Just checking.
So yeah, this is just me on box two
So yeah, this is just me on box two
right now. Um I have played around with
right now. Um I have played around with
a couple things. The original params
a couple things. The original params
were like I said uh the number of ends
were like I said uh the number of ends
were 128. The number of workers was 16.
were 128. The number of workers was 16.
I I changed some things obviously. I can
I I changed some things obviously. I can
change them back. But even right now,
change them back. But even right now,
how many agents per end?
how many agents per end?
Uh 64 agents in each end. Oh, uh I guess
Uh 64 agents in each end. Oh, uh I guess
it's two times that. So I guess it's
it's two times that. So I guess it's
128. No. Oh, sorry. Sorry. Per M. Two.
128. No. Oh, sorry. Sorry. Per M. Two.
Two. Two agents per M. Do you usually
Two. Two agents per M. Do you usually
train with that few MS?
train with that few MS?
No. Normally I do Well, some some sweeps
No. Normally I do Well, some some sweeps
have actually found Yeah. 64 M per
have actually found Yeah. 64 M per
worker. done. Well, but usually it's
worker. done. Well, but usually it's
either 64 or 128. Yeah, it's usually not
either 64 or 128. Yeah, it's usually not
64 128 when you use the x-axis is time.
64 128 when you use the x-axis is time.
Uh yeah, really?
Uh yeah, really?
Yeah, cuz I mean as hell. So, when I'm
Yeah, cuz I mean as hell. So, when I'm
doing protein, um the uh what is it?
doing protein, um the uh what is it?
Like the cost is standard like walk
Like the cost is standard like walk
clock time and then the score is just
clock time and then the score is just
well I originally had the score being my
well I originally had the score being my
custom like uh win rate but then I just
custom like uh win rate but then I just
changed it to score because I saw that
changed it to score because I saw that
puffer li kind of does like an eval
puffer li kind of does like an eval
metric. Yes. For you so score seemed
metric. Yes. For you so score seemed
accurate. I I have not seen like the
accurate. I I have not seen like the
very small M screw up things like the
very small M screw up things like the
three small M weird hypers since uh we
three small M weird hypers since uh we
switched to Muon and reram sweeps. We
switched to Muon and reram sweeps. We
were getting that on Adam before that
were getting that on Adam before that
like it had to have like a small number
like it had to have like a small number
of ends.
of ends.
Yeah, this is with Muan though. This is
Yeah, this is with Muan though. This is
on latest dev. I've been running a lot
on latest dev. I've been running a lot
of these sweeps. Okay, well we can try
of these sweeps. Okay, well we can try
it. Let's just see to get something
it. Let's just see to get something
working here because I this is not a bug
working here because I this is not a bug
in Puffer. This is like a lack of
in Puffer. This is like a lack of
warnings that I need to fix. And the
warnings that I need to fix. And the
only thing I haven't done, by the way,
only thing I haven't done, by the way,
and this is literally your is the only
and this is literally your is the only
one affected by this. Um, but I have to
one affected by this. Um, but I have to
figure out something to do about
figure out something to do about
masking. Masking is currently not in the
masking. Masking is currently not in the
dev. Um, and I need to figure out what
dev. Um, and I need to figure out what
to do about that. The new experience
to do about that. The new experience
buffer design basically works with
buffer design basically works with
everything that you could possibly
everything that you could possibly
imagine except masking. Um, yes, that's
imagine except masking. Um, yes, that's
Well, if it makes you feel any better,
Well, if it makes you feel any better,
masking for impulse wars is only
masking for impulse wars is only
important when you have more than two
important when you have more than two
agents. Um, which I'm not currently
agents. Um, which I'm not currently
training on. I'm just trying to get like
training on. I'm just trying to get like
a two agent baseline working. So, Oh,
a two agent baseline working. So, Oh,
and you do one. Yeah. Okay, that makes
and you do one. Yeah. Okay, that makes
sense. Yeah. So, let's actually perfect.
sense. Yeah. So, let's actually perfect.
Let's do your experiments with two
Let's do your experiments with two
agents for now and then that'll give me
agents for now and then that'll give me
some time to sort this [ __ ] out. Yeah.
some time to sort this [ __ ] out. Yeah.
Okay. Now, we are going to have to crank
Okay. Now, we are going to have to crank
up numbers of ends and stuff to start
up numbers of ends and stuff to start
with. So, set this to like 1024 M. 1024
with. So, set this to like 1024 M. 1024
M. Okay. I don't think that'll do well
M. Okay. I don't think that'll do well
performance. I'm going to find other
performance. I'm going to find other
pars for you in a second. Um, so you got
pars for you in a second. Um, so you got
no drones, no agents. What do you do?
no drones, no agents. What do you do?
buffer. Holy BPT Horizon Batman. What
buffer. Holy BPT Horizon Batman. What
the hell is that? Um, what? 256.
the hell is that? Um, what? 256.
That's That's big. That's chunky.
That's That's big. That's chunky.
So, I found that um the sweeps either
So, I found that um the sweeps either
like 256 or 128. Was it sensitive to it?
like 256 or 128. Was it sensitive to it?
Like, was there a big perk difference?
Like, was there a big perk difference?
Uh, decent. Uh, I can pull up I can send
Uh, decent. Uh, I can pull up I can send
you the sweeps or Sure. 2048
you the sweeps or Sure. 2048
M. Oh, wait. No, 1024x is too many
M. Oh, wait. No, 1024x is too many
because we have you have 16 process.
because we have you have 16 process.
Wait, wait, wait. Okay. No, dude. 64 is
Wait, wait, wait. Okay. No, dude. 64 is
totally fine because that gets
totally fine because that gets
multiplied by the number of workers.
multiplied by the number of workers.
Yes. Okay. I didn't realize you had 16.
Yes. Okay. I didn't realize you had 16.
So 64 * 16 is 1024 m. That's like
So 64 * 16 is 1024 m. That's like
roughly on par. I would still want to
roughly on par. I would still want to
see 128, but that's still fine. So then
see 128, but that's still fine. So then
if we do uh 128 16 128 is what the uh I
if we do uh 128 16 128 is what the uh I
was doing originally, but um that was
was doing originally, but um that was
giving me a different error. So I we'll
giving me a different error. So I we'll
fix both of those.
fix both of those.
So 2048
So 2048
uh times 250 was it 256 horizon has a
uh times 250 was it 256 horizon has a
minimum batch size requirement of 524.
minimum batch size requirement of 524.
Yep. So that's what you have. So you can
Yep. So that's what you have. So you can
just try try this to see what happens
just try try this to see what happens
first. And mini batch isn't affected by
first. And mini batch isn't affected by
this, right?
this, right?
uh mini back. The only thing is that's
uh mini back. The only thing is that's
going to be slow as hell of a learner
going to be slow as hell of a learner
because the thing you got to think about
because the thing you got to think about
16 384 divide by 256 horizon you only
16 384 divide by 256 horizon you only
have 64 rows parallel for your GPU.
have 64 rows parallel for your GPU.
Yeah, this is just what the sweep well
Yeah, this is just what the sweep well
actually the sweep suggested 264k batch
actually the sweep suggested 264k batch
and 8K mini batch. So, I just basically
and 8K mini batch. So, I just basically
doubled when I doubled the batch. I was
doubled when I doubled the batch. I was
a double the mini badge. But I can I'm
a double the mini badge. But I can I'm
planning on helping you with retuning
planning on helping you with retuning
once you have this anyways. Let's just
once you have this anyways. Let's just
see if this runs though. Yes. Like I'm
see if this runs though. Yes. Like I'm
going to help you with this anyways.
going to help you with this anyways.
Cool. Yeah.
Cool. Yeah.
Yep. This is what I'm getting. Is it Are
Yep. This is what I'm getting. Is it Are
the corners cut off in Puffer Live on
the corners cut off in Puffer Live on
your turn?
your turn?
Uh, I think because I'm double
t-moxing because I have T-Mox in the
t-moxing because I have T-Mox in the
puffer box and I have T-Mox on my host.
puffer box and I have T-Mox on my host.
So, it sometimes gets weird. I guess a
So, it sometimes gets weird. I guess a
stop nasty character or whatever.
stop nasty character or whatever.
Yeah, it doesn't show either, right? It
Yeah, it doesn't show either, right? It
doesn't show the puffer fish emote.
doesn't show the puffer fish emote.
Yeah. Okay. No. Yeah, it's probably
Yeah. Okay. No. Yeah, it's probably
okay. Maybe I'll find an character that
okay. Maybe I'll find an character that
I can use for that. Good to know.
I can use for that. Good to know.
That's so weird though that you have
That's so weird though that you have
colors. You have like full color but you
colors. You have like full color but you
don't have like UTF8.
So
So
weird. Um getting the last commit was
weird. Um getting the last commit was
not today. Did you push that?
not today. Did you push that?
I might have [ __ ] up. I
know. That's buffer maximum.
Okay, try that.
This is just rebuilding.
What? So this is something to do with
What? So this is something to do with
mini batch and what else?
Size of a
[Music]
tensor config
tensor config
loss. All right, go put a break point
loss. All right, go put a break point
there.
there.
All
All
right. So, it should be getting batched
right. So, it should be getting batched
correctly. It seems to me like one of
correctly. It seems to me like one of
those is flat and the other
isn't 476. I'm
happy. Actually, you should just be able
happy. Actually, you should just be able
to hop on this box, right? If you just
to hop on this box, right? If you just
wanted it, it' probably be faster if you
wanted it, it' probably be faster if you
did it. That is true because this is
did it. That is true because this is
just on box two. That is a perk of uh of
just on box two. That is a perk of uh of
the cluster. I forget
the cluster. I forget
about it. Papa box five. No, two
is in T-Max.
is in T-Max.
Yeah. So, just attach and I get your
Yeah. So, just attach and I get your
break point. Perfect.
break point. Perfect.
Man, this is like the more I play around
Man, this is like the more I play around
this, the more I like this cluster
this, the more I like this cluster
setup.
setup.
Yeah, it's
nice. I don't really need the stream up.
nice. I don't really need the stream up.
There's like a couple second
There's like a couple second
lag. No, no, no. The stream is for uh
lag. No, no, no. The stream is for uh
YouTube. We got five folks on YouTube.
Hello.
Uh well, that's screwy.
How did I mess that up?
How did I mess that up?
So, you know what happened? Well,
So, you know what happened? Well,
something's not getting reshaped
something's not getting reshaped
correctly. You're using puffer
correctly. You're using puffer
advantage.
advantage.
Uh, should be default. I I'm If it's a
Uh, should be default. I I'm If it's a
default, yes, I think
default, yes, I think
config puff advantage. I might need to
config puff advantage. I might need to
check the default. So I don't set it but
check the default. So I don't set it but
I remember I okay I am
I remember I okay I am
use puff advantage.
use puff advantage.
It's just V trace plus generalized
It's just V trace plus generalized
advantage estimation kernel.
That took me like that took me a solid
That took me like that took me a solid
two days of just banging my head on math
two days of just banging my head on math
to realize it was like a threeline
to realize it was like a threeline
change in CUDA.
change in CUDA.
Wait really? Yeah, it literally is
Wait really? Yeah, it literally is
because there were two papers and they
because there were two papers and they
wrote their math like completely
wrote their math like completely
differently, but like when you actually
differently, but like when you actually
see how they see what it is and you like
see how they see what it is and you like
change all the variables to match each
change all the variables to match each
other and like you rewrite all the
other and like you rewrite all the
formulas like it's literally the same
formulas like it's literally the same
algorithm with like one small change so
algorithm with like one small change so
you can just put them together trivially
you can just put them together trivially
but it took me two days of banging my
but it took me two days of banging my
head against the math to figure that
head against the math to figure that
out.
out.
Dang. Yeah, I guess there's a lot of
Dang. Yeah, I guess there's a lot of
different ways to express the same thing
different ways to express the same thing
in math and stuff. So, yeah, that makes
in math and stuff. So, yeah, that makes
sense.
sense.
But yeah, that's cool. Yeah, I would
But yeah, that's cool. Yeah, I would
have realized it. I should have realized
have realized it. I should have realized
it sooner. My math is it's improving.
it sooner. My math is it's improving.
You know, I've been doing more of it
You know, I've been doing more of it
lately. It's been improving. It's still
lately. It's been improving. It's still
kind of bad,
kind of bad,
but still leagues above mine, but I
but still leagues above mine, but I
don't I don't Well, mine was never that
don't I don't Well, mine was never that
good to begin with, and I also don't
good to begin with, and I also don't
really need to flex hardly flex what
really need to flex hardly flex what
little I have hardly ever.
little I have hardly ever.
Yeah, I this is why mine isn't good,
Yeah, I this is why mine isn't good,
right? It's because I don't like need it
right? It's because I don't like need it
that often, but now I'm starting like,
that often, but now I'm starting like,
okay, I got to pick some stuff up.
okay, I got to pick some stuff up.
Um, so the returns are batched and the
Um, so the returns are batched and the
value is not. I thought
value is not. I thought
this new value
got mean that's like
Do you have a custom LSTM wrapper or
Do you have a custom LSTM wrapper or
something?
something?
Uh, not I mean I wrap around it. I mean,
Uh, not I mean I wrap around it. I mean,
you can look at it. It's I mean, yeah,
you can look at it. It's I mean, yeah,
but it doesn't really do much. It
but it doesn't really do much. It
basically just wraps. I probably did
basically just wraps. I probably did
some like jank reshape stuff and you
some like jank reshape stuff and you
probably don't have it.
probably don't have it.
I mean, I might have done something. You
I mean, I might have done something. You
can take a look at it quick. Let me just
can take a look at it quick. Let me just
see.
Value. So, look. Do you see the three
Value. So, look. Do you see the three
shape here? You're missing this is my
shape here? You're missing this is my
guess.
guess.
So, where's your
So, where's your
policy? Um, it should, excuse me, should
policy? Um, it should, excuse me, should
be in puffer li torch at the
bottom. Uh, this subass is recurrent.
bottom. Uh, this subass is recurrent.
Yeah. And you you uh basically alias it
Yeah. And you you uh basically alias it
above to like the LCM wrap or whatever.
Um, you don't have forward train on
Um, you don't have forward train on
this. How's this running without forward
this. How's this running without forward
train? Maybe it's not
train? Maybe it's not
needed. Seems weird though. Oh, forward
needed. Seems weird though. Oh, forward
train is a new thing. Yeah, I don't know
train is a new thing. Yeah, I don't know
if you actually need it outside of the
if you actually need it outside of the
um the com though if you're not using
um the com though if you're not using
like nonrecurrent policy. And I do have
like nonrecurrent policy. And I do have
to go
to go
soon. But let me see if we can at least
soon. But let me see if we can at least
fix this for
you. Seems fine.
Well, there's your there's your problem.
Well, there's your there's your problem.
What? Oh, you're supposed to be
Mhm. Uh, this something. Did this get
Mhm. Uh, this something. Did this get
merged correctly?
merged correctly?
I don't know if you got
I don't know if you got
merge nicely or not.
merge nicely or not.
Also, if I look I have a it's not ready
Also, if I look I have a it's not ready
for merge quite yet, but I have a uh I
for merge quite yet, but I have a uh I
have a branch and when I do a uh like a
have a branch and when I do a uh like a
diff basically on puffer dev, I don't
diff basically on puffer dev, I don't
think anything in queen RL gets
think anything in queen RL gets
modified.
So, I'm suspicious of this thing right
here. Also, if you don't tag with
here. Also, if you don't tag with
Neptune, it'll load faster. If you don't
Neptune, it'll load faster. If you don't
run Neptune for test to load faster.
run Neptune for test to load faster.
Okay, good to
Okay, good to
know. Makes sense.
You have compile on. Yeah. Okay. That's
You have compile on. Yeah. Okay. That's
I didn't account for that. That's
I didn't account for that. That's
why.
why.
Uh where's the config ocean
Uh where's the config ocean
uh impulse? Yep.
Okay, so that's a me problem. My bad.
What was the issue? Uh, I just didn't
What was the issue? Uh, I just didn't
account for compile in uh my reshapes
account for compile in uh my reshapes
because I I check if it's an instance of
because I I check if it's an instance of
an LSPM and uh I forgot that compile
an LSPM and uh I forgot that compile
adds an extra wrapper.
adds an extra wrapper.
Gotcha. That makes sense. So I will fix
Gotcha. That makes sense. So I will fix
that. I will make a note to fix that.
Okay. Yeah, it seems to be working.
Okay. Yeah, it seems to be working.
No, it isn't cuz you're not getting a
No, it isn't cuz you're not getting a
steps.
steps.
What?
What?
Yes, that's the other bug with the
Yes, that's the other bug with the
experience buffer.
experience buffer.
Yeah, cuz I've never seen That's the
Yeah, cuz I've never seen That's the
other other bug.
So, what causes that?
So, what causes that?
Uh, that one's just demons.
Uh, that one's just demons.
Fair
enough.
Okay, let the SPS update
Okay, let the SPS update
160. You're probably higher than that
160. You're probably higher than that
before, right?
before, right?
Uh yeah. Well, so yesterday I saw up to
Uh yeah. Well, so yesterday I saw up to
like 200 with different hyperparameters
like 200 with different hyperparameters
or something, but we are losing all the
or something, but we are losing all the
time in the board pass right now. Yeah.
time in the board pass right now. Yeah.
And that's because of the multi-discreet
And that's because of the multi-discreet
actions, is it? So I actually I actually
actions, is it? So I actually I actually
would very much like to get the stats
would very much like to get the stats
and the reporting on that because I had
and the reporting on that because I had
uh I've got a client that is likewise
uh I've got a client that is likewise
unhappy with multi-discrete sampling
unhappy with multi-discrete sampling
perf. So, I will be very happy like
perf. So, I will be very happy like
let's get this integrated because this
let's get this integrated because this
will be a perfect test case for me to
will be a perfect test case for me to
figure out what the heck is wrong with
figure out what the heck is wrong with
that because I can't reproduce it
that because I can't reproduce it
anywhere else.
anywhere else.
Um there probably is some well I don't
Um there probably is some well I don't
know actually what to do to fix it but I
know actually what to do to fix it but I
know like I said if you just get um
know like I said if you just get um
torch compile working like correctly
torch compile working like correctly
with minimal graph breaks that basically
with minimal graph breaks that basically
eliminates it. And now well once that
eliminates it. And now well once that
was working before the bottleneck went
was working before the bottleneck went
from forward well once torch 2.6 hit um
from forward well once torch 2.6 hit um
the bottleneck went from forward to M
the bottleneck went from forward to M
like M I was being hard M bottleneck
like M I was being hard M bottleneck
which makes sense because impulse force
which makes sense because impulse force
is a slowish M with all the physics
is a slowish M with all the physics
stuff. So right now I'm being like
stuff. So right now I'm being like
almost totally as you can see um forward
almost totally as you can see um forward
pass bottlenecks but I'm confident we
pass bottlenecks but I'm confident we
will be able to at least double your PF
will be able to at least double your PF
though with this. We should be able to.
though with this. We should be able to.
Okay, cool. So 300k is like kind of a
Okay, cool. So 300k is like kind of a
minimum for being fast. Um, all right,
minimum for being fast. Um, all right,
cool. And this is this is also slower
cool. And this is this is also slower
because there is one agent per M. So two
because there is one agent per M. So two
well wait, yeah, actually my bad. One
well wait, yeah, actually my bad. One
agent perm. There's two drones but one
agent perm. There's two drones but one
is scripted. If you have two agents per
is scripted. If you have two agents per
M, then it like basically doubles, which
M, then it like basically doubles, which
is nice. But I haven't got selfplay
is nice. But I haven't got selfplay
working well yet. Something I'd like to
working well yet. Something I'd like to
do.
do.
Yeah, but definitely just like get this
Yeah, but definitely just like get this
PR finished and I will see if I can
PR finished and I will see if I can
merge it later today. Depends how long
merge it later today. Depends how long
cluster stuff takes after meeting and uh
cluster stuff takes after meeting and uh
Okay. Yeah, we will chat. But I'm
Okay. Yeah, we will chat. But I'm
looking forward to getting some
looking forward to getting some
baselines on this. It'll be fun. And I'm
baselines on this. It'll be fun. And I'm
going to make the experience buffer less
going to make the experience buffer less
of a pain to like get params right and
of a pain to like get params right and
stuff like that's on the that's the very
stuff like that's on the that's the very
next thing on the to-do list. So yeah.
next thing on the to-do list. So yeah.
So I know you So yeah, I know you have
So I know you So yeah, I know you have
to go but really quick. It's pretty much
to go but really quick. It's pretty much
ready to be merged. Obviously, I need to
ready to be merged. Obviously, I need to
touch up a few things, but uh I won't be
touch up a few things, but uh I won't be
able to do a test run until you fix
able to do a test run until you fix
these things. I'm guessing we should I
these things. I'm guessing we should I
should wait for that and then do a test
should wait for that and then do a test
run and then get emerged. Fix what
run and then get emerged. Fix what
things?
things?
Uh well, just until you make it so I can
Uh well, just until you make it so I can
actually train on it. Should I wait
actually train on it. Should I wait
until that like I can just do a test
until that like I can just do a test
run? Huh? All I changed was your config
run? Huh? All I changed was your config
here, didn't I? Oh, never mind. Yeah,
here, didn't I? Oh, never mind. Yeah,
you're right. Yeah. So, just put that as
you're right. Yeah. So, just put that as
a config for now and then we'll reweep
a config for now and then we'll reweep
it once that's unbroken. Yeah. Cool.
it once that's unbroken. Yeah. Cool.
Sounds good. Yeah. I'll just as long as
Sounds good. Yeah. I'll just as long as
it learns something, we're good, right,
it learns something, we're good, right,
with these hypers. Cool. All right. All
with these hypers. Cool. All right. All
right. Yeah. Thanks, man. See y.
right. Yeah. Thanks, man. See y.
Okay. Thanks, folks, for tuning in. I
Okay. Thanks, folks, for tuning in. I
will be back this afternoon uh after
will be back this afternoon uh after
meeting most
meeting most
likely. All my stuff is at
likely. All my stuff is at
puffer.ai. You can help us out for free
puffer.ai. You can help us out for free
by starring the repo on GitHub. really
by starring the repo on GitHub. really
helps a lot. And you can join Discord to
helps a lot. And you can join Discord to
get involved in dev. It's all open
get involved in dev. It's all open
source and most of the people who came
source and most of the people who came
in here came in with zero RL experience
in here came in with zero RL experience
and sometimes zero AI experience. Just
and sometimes zero AI experience. Just
need to be able to
need to be able to
program.
program.
Um I will be I'm usually live most days.
Um I will be I'm usually live most days.
I've got some stuff to do to move to the
I've got some stuff to do to move to the
new facility, but then there will be
new facility, but then there will be
full-time streaming setup there with all
full-time streaming setup there with all
the dev for the foreseeable future.
the dev for the foreseeable future.
yourself. Thanks and see you

Kind: captions
Language: en
We are
We are
live.
Hi. Let me tweet out the uh the stream
Hi. Let me tweet out the uh the stream
and then uh it's a short stream. Just
and then uh it's a short stream. Just
going to be here for an hour and then I
going to be here for an hour and then I
will talk a little bit about uh some of
will talk a little bit about uh some of
the plans for stuff I have going forward
the plans for stuff I have going forward
and uh where my head's at on all the
and uh where my head's at on all the
latest things in RL land.
Also, let me make sure I got the time
Also, let me make sure I got the time
zones right. I'm pretty darn sure that I
zones right. I'm pretty darn sure that I
did, but let me make sure I'm not
did, but let me make sure I'm not
missing this
missing this
other meeting that it is in fact in an
other meeting that it is in fact in an
hour. Pretty darn sure that it was PST.
hour. Pretty darn sure that it was PST.
Yes, it is. Perfect.
Okay, so here's what is going on right
Okay, so here's what is going on right
now. Um, I am flying out to the new
now. Um, I am flying out to the new
facility on Thursday.
facility on Thursday.
I just made a massive Amazon order of
I just made a massive Amazon order of
all of the things that I'm going to need
all of the things that I'm going to need
uh to set up the stream at the new
uh to set up the stream at the new
facility as well as hardware. We got to
facility as well as hardware. We got to
move servers. We got to do a whole bunch
move servers. We got to do a whole bunch
of stuff for that. So, my hope uh is
of stuff for that. So, my hope uh is
that I will get there on Thursday. I
that I will get there on Thursday. I
will maybe be able to stream Friday or
will maybe be able to stream Friday or
Saturday, but definitely the next week I
Saturday, but definitely the next week I
should be there with my whole setup
should be there with my whole setup
nicely working.
nicely working.
Um, and that's going to be really cool.
Um, and that's going to be really cool.
I'm going to have some cool things to
I'm going to have some cool things to
show off with that. We don't have all
show off with that. We don't have all
the new servers yet. We just have the
the new servers yet. We just have the
existing ones until the tariffs go down.
existing ones until the tariffs go down.
But that's what it's going to be. And
But that's what it's going to be. And
when we get the new servers, the
when we get the new servers, the
facility will have capacity for them.
facility will have capacity for them.
And that means that contributors get
And that means that contributors get
access to more compute, which is going
access to more compute, which is going
to be great. Um, but in the
to be great. Um, but in the
meantime, we have Puffer in a pretty
meantime, we have Puffer in a pretty
awesome spot. Uh, the new dev branch is
awesome spot. Uh, the new dev branch is
kind of dissolving everything I throw at
kind of dissolving everything I throw at
it. It seems to be a very nice step up
it. It seems to be a very nice step up
from the current 20 main branch and the
from the current 20 main branch and the
key advant the key things that I've
key advant the key things that I've
added on which will be in a long blog
added on which will be in a long blog
post later but you know you get to hear
post later but you know you get to hear
it here early muon really big cosign and
it here early muon really big cosign and
kneeling seems like it works I need to
kneeling seems like it works I need to
do a little bit more testing on it
do a little bit more testing on it
because it kind of does weird things to
because it kind of does weird things to
the learning rate uh the experience
the learning rate uh the experience
buffer is a big one as well so we have
buffer is a big one as well so we have
this new experience buffer it works on
this new experience buffer it works on
trajectory
trajectory
segments. It's really difficult to
segments. It's really difficult to
design these things well uh in a way
design these things well uh in a way
that's performance. Most of the
that's performance. Most of the
implementations out there that are used
implementations out there that are used
in most RL libraries just suck. So I've
in most RL libraries just suck. So I've
done my best with it. I think it's
done my best with it. I think it's
pretty decent and that's what we're
pretty decent and that's what we're
going to work on in a few minutes is
going to work on in a few minutes is
trying to shore up a couple things with
trying to shore up a couple things with
that. Uh but the thing with that
that. Uh but the thing with that
experience buffer is it lets us do
experience buffer is it lets us do
advantage filtering. Uh so we can
advantage filtering. Uh so we can
actually like look at all the samples
actually like look at all the samples
and we can do like prioritized replay on
and we can do like prioritized replay on
which samples are actually good and
which samples are actually good and
train on those. Uh and that actually
train on those. Uh and that actually
seems to help a fair bit. So we have
seems to help a fair bit. So we have
that. We also have our brand new puffer
that. We also have our brand new puffer
exclusive algorithm. I guess it's uh
exclusive algorithm. I guess it's uh
it's kind of a neat little thing. It
it's kind of a neat little thing. It
combines V trace and generalized
combines V trace and generalized
advantage estimation. And we have a food
advantage estimation. And we have a food
kernel for it. So it's really really
kernel for it. So it's really really
fast. Uh, so between experience buffer
fast. Uh, so between experience buffer
changes, advantage filtering, Kio
changes, advantage filtering, Kio
Replay, all that, that's kind of what is
Replay, all that, that's kind of what is
at the moment looking like it is going
at the moment looking like it is going
into the next update. There a few other
into the next update. There a few other
things I want to play around with more.
things I want to play around with more.
Uh, exploration algorithms, diversity is
Uh, exploration algorithms, diversity is
all you need. That algorithm had like
all you need. That algorithm had like
some promising results. It needs a lot
some promising results. It needs a lot
more experiments. I probably going to
more experiments. I probably going to
have to put together a whole custom
have to put together a whole custom
environment uh just to see if that
environment uh just to see if that
algorithm works the way it should.
algorithm works the way it should.
Protein. Yes, thank you, Captain.
Protein. Yes, thank you, Captain.
Protein is huge. We have that whole
Protein is huge. We have that whole
hyperparameter tuning algo. Really,
hyperparameter tuning algo. Really,
really solid. I'm happy with that. Uh,
really solid. I'm happy with that. Uh,
that's pretty much what is the core algo
that's pretty much what is the core algo
side of the new update. There are some
side of the new update. There are some
fiddly things with the experience
fiddly things with the experience
buffer. There are some perf things with
buffer. There are some perf things with
the experience buffer that we're going
the experience buffer that we're going
to look at for the next hour or so, but
to look at for the next hour or so, but
outside of that, it's pretty darn good.
outside of that, it's pretty darn good.
Um, yeah. So, that's mostly where my
Um, yeah. So, that's mostly where my
mind is at. And my goal at the moment
mind is at. And my goal at the moment
timelinewise is I want to have by
timelinewise is I want to have by
sometime next week I want to have this
sometime next week I want to have this
experience buffer ideally this week but
experience buffer ideally this week but
we'll see you know with all the stuff I
we'll see you know with all the stuff I
have to do to prep for the new facility.
have to do to prep for the new facility.
I want to have the experience buffer uh
I want to have the experience buffer uh
really nice and stable because that's
really nice and stable because that's
the last remaining like annoying thing
the last remaining like annoying thing
that can cause issues. Uh yeah. So I
that can cause issues. Uh yeah. So I
think that is mostly what I'm thinking
think that is mostly what I'm thinking
about. We've got a whole bunch of new
about. We've got a whole bunch of new
M's that I want to get added to Puffer.
M's that I want to get added to Puffer.
Waiting on Captain for the Impulse Force
Waiting on Captain for the Impulse Force
PR so I can run some experiments for
PR so I can run some experiments for
him. Uh the meta stuff is awesome. I
him. Uh the meta stuff is awesome. I
just got decent baselines on that. Uh we
just got decent baselines on that. Uh we
got kind of okay benchmarks on our new
got kind of okay benchmarks on our new
GPU drive port with Spencer yesterday.
GPU drive port with Spencer yesterday.
Uh we're going to see what his hyper
Uh we're going to see what his hyper
sweep does. I don't think he posted it.
sweep does. I don't think he posted it.
Did he? He posted Spencer. Oh, wait. He
Did he? He posted Spencer. Oh, wait. He
did. restarting sweep because time step
did. restarting sweep because time step
still went
beyond. Let's see what he gets. We'll
beyond. Let's see what he gets. We'll
take look at these real
quick. Not amazing. Yeah, not amazing.
quick. Not amazing. Yeah, not amazing.
Um, that actually seems kind of sketchy,
Um, that actually seems kind of sketchy,
doesn't it?
Where's the
score? So, this one is 64, which is
score? So, this one is 64, which is
lower than what I had
lower than what I had
before. And this is like by far the
before. And this is like by far the
cleanest. Now, something's weird if it's
cleanest. Now, something's weird if it's
hard plateaued right here, I think.
hard plateaued right here, I think.
Right.
That seems like something's wrong.
That seems like something's wrong.
Show. Let me send this dispenser.
All
right, there we
right, there we
go. So, Spencer's got this end. Got
go. So, Spencer's got this end. Got
Impulse Wars. Uh, Tower Climb is pretty
Impulse Wars. Uh, Tower Climb is pretty
awesome as well. He's That one's like
awesome as well. He's That one's like
stable and good. So, we have one, two,
stable and good. So, we have one, two,
is it three new big environments plus
is it three new big environments plus
blastar uh bet did blast and cartpole
blastar uh bet did blast and cartpole
simple m plus we got uh the CPR the
simple m plus we got uh the CPR the
common was it CRP was it common pool
common was it CRP was it common pool
resource or something we got that as
resource or something we got that as
well so I think that is a total of is
well so I think that is a total of is
that six new environments yeah six new
that six new environments yeah six new
environments three of which are pretty
environments three of which are pretty
darn big so that's nice and then
darn big so that's nice and then
Spencer's going to be working on more
Spencer's going to be working on more
end stuff but that seems good for update
Still around.
All
right, let's go to the experience
right, let's go to the experience
buffer because I think everything else
buffer because I think everything else
is in a good is in a good uh spot.
is in a good is in a good uh spot.
Um, I guess I will say a couple of the
Um, I guess I will say a couple of the
other things on my mind. So, I need to
other things on my mind. So, I need to
get the servers at some point. I have no
get the servers at some point. I have no
idea when because they're like plus 40%
idea when because they're like plus 40%
price tag right now with the tariffs.
price tag right now with the tariffs.
I hope that these don't stay in place
I hope that these don't stay in place
because it would be unbelievably stupid
because it would be unbelievably stupid
to like have an exemption on assembled
to like have an exemption on assembled
machines but not on components because
machines but not on components because
you're literally incentivizing
you're literally incentivizing
assembling machines in China and then
assembling machines in China and then
shipping them here instead of shipping
shipping them here instead of shipping
the parts and doing the assembly
the parts and doing the assembly
business in America. So that seems like
business in America. So that seems like
just an oversight to me. That doesn't
just an oversight to me. That doesn't
seem intentional. Um yeah, but I don't
seem intentional. Um yeah, but I don't
know. So,
uh, that's one thing. The other thing is
uh, that's one thing. The other thing is
that at some point I'm going to have to
that at some point I'm going to have to
do like a monthlong deep dive into hard
do like a monthlong deep dive into hard
RLT
RLT
area. Yeah. And the reason for that is
area. Yeah. And the reason for that is
that there's like a whole bunch of
that there's like a whole bunch of
algorithmic innovations from DeepMind
algorithmic innovations from DeepMind
that they've like gone this whole other
that they've like gone this whole other
direction with RL. And uh I want to see
direction with RL. And uh I want to see
what's in there and whether any of it's
what's in there and whether any of it's
applicable, but I want to get the new
applicable, but I want to get the new
update out before I do that because
update out before I do that because
otherwise it's like just lag too far
otherwise it's like just lag too far
behind. Okay, let's talk about
behind. Okay, let's talk about
experience buffer. We do real
experience buffer. We do real
engineering.
engineering.
Now, there are a few problems with this.
Now, there are a few problems with this.
Um one, it doesn't have a clean CUDA
Um one, it doesn't have a clean CUDA
fallback. It can run on CPU, but you
fallback. It can run on CPU, but you
still need to have CUDA even to run it
still need to have CUDA even to run it
on CPU the way it's set up. uh there's a
on CPU the way it's set up. uh there's a
threading issue where you can't use like
threading issue where you can't use like
you have to have a certain batch size or
you have to have a certain batch size or
it breaks and then the really big one is
it breaks and then the really big one is
that uh the way the experience buffer
that uh the way the experience buffer
collects data it kind of just breaks if
collects data it kind of just breaks if
you have the wrong number of
you have the wrong number of
environments and I haven't been able to
environments and I haven't been able to
figure out a good workaround for that.
figure out a good workaround for that.
So I think that those are the three
So I think that those are the three
things we're going to fiddle with at the
things we're going to fiddle with at the
moment. We can start with the easy ones
moment. We can start with the easy ones
here though for sure. So uh if I just
go open the CUDA
go open the CUDA
file. I was thinking about
file. I was thinking about
this and I think that
this and I think that
um yeah so this numbum steps
here I don't know if this is allowed to
here I don't know if this is allowed to
be a not a multiple of 128. I think this
be a not a multiple of 128. I think this
number has to
number has to
be but this number doesn't maybe.
be but this number doesn't maybe.
So, let's
do I think it's 256
max steps.
Is this going to compile out or is this
Is this going to compile out or is this
going to actually
going to actually
do? This shouldn't get compiled out,
do? This shouldn't get compiled out,
right?
We'll try
that. And then to test this, we'll just
that. And then to test this, we'll just
get like some simple
get like some simple
N breakout
maybe. We'll recompile kernel and then
maybe. We'll recompile kernel and then
we're going to mess around with the
we're going to mess around with the
number of amps and the batch size a
number of amps and the batch size a
bunch.
It's my 1 hour alarm. I'm good for a
bit. But so we can see here breakout
bit. But so we can see here breakout
trains and it should solve in under a
trains and it should solve in under a
minute. 1.8 8 million steps per second.
minute. 1.8 8 million steps per second.
Very, very fast.
Right. That doesn't look like 1.8
Right. That doesn't look like 1.8
million steps per second to me, though.
million steps per second to me, though.
It seems like it's a bit slower, doesn't
It seems like it's a bit slower, doesn't
it? It seems like it's more like 1
it? It seems like it's more like 1
million. So, the reporting might be
million. So, the reporting might be
wrong, which would make sense. I did
wrong, which would make sense. I did
mess with the profiling.
Looks more like a million. Yeah, we'll
Looks more like a million. Yeah, we'll
fix that though. We should be able to
fix that though. We should be able to
get close to 2
get close to 2
million. But there you go. That's
million. But there you go. That's
breakout
sol. Next, we
sol. Next, we
do let's just mess with this number of
do let's just mess with this number of
M's. Like, let's just do
M's. Like, let's just do
like let's make it some awkward number
like let's make it some awkward number
of like 14 m or
something. Ready to open attempt dirty
something. Ready to open attempt dirty
Impulse Wars PR? Awesome.
Impulse Wars PR? Awesome.
You did. You check that it trains in
You did. You check that it trains in
puffer like that though.
should definitely make sure we got
should definitely make sure we got
baselines like give me something to work
baselines like give me something to work
off of and we will uh merge that. I have
off of and we will uh merge that. I have
a meeting at 11:00 and then probably
a meeting at 11:00 and then probably
like breakfast and stuff or brunch and
like breakfast and stuff or brunch and
stuff and then afternoon I can do merge
stuff and then afternoon I can do merge
for you and uh we can start getting that
for you and uh we can start getting that
yeah into puffer before test
yeah into puffer before test
ends or benchmark
ends or benchmark
ends. Okay, so this does run. It's not
ends. Okay, so this does run. It's not
supposed to be fast. We just want to
supposed to be fast. We just want to
make sure that it doesn't break
make sure that it doesn't break
horribly. If I set the mini batch size
horribly. If I set the mini batch size
to
to
like Well, if I set this to like
512. Okay, it still works.
Lovely. And
then, oh, actually, there's one other
then, oh, actually, there's one other
thing I wanted to test first.
there's like some weirdness in uh with
there's like some weirdness in uh with
one end. I
believe seems like it works here just
believe seems like it works here just
fine.
Just per
batch. I'm trying to repro this bug that
batch. I'm trying to repro this bug that
people hit occasionally when they try to
people hit occasionally when they try to
like use one end or
like use one end or
whatever. Doesn't seem to repro
nicely. I think this was the one that
nicely. I think this was the one that
people were getting the
people were getting the
issue. Was this not a native end?
Maybe that's one
Yeah, let me go check on one other thing
Yeah, let me go check on one other thing
for Spencer as well. I was supposed to
for Spencer as well. I was supposed to
set
set
up We were supposed to set up a meeting.
floating point exception for
floating point exception for
dumped.
Well, that doesn't seem like a MI
Well, that doesn't seem like a MI
problem. I would doubt that that is a MI
problem. I would doubt that that is a MI
problem.
problem.
How about we do config Atari
breakout? Let's see how that
breakout? Let's see how that
does. Native vector effects. It's
does. Native vector effects. It's
multipprocessing. It's
fine. Okay. Okay. So, Atari
has
has
multiprocessing and then we do
multiprocessing and then we do
uh 128
uh 128
m 16 workers 32 and baptize.
This policy is
This policy is
probably a comm policy,
right? Yeah. pump policy, but it doesn't
right? Yeah. pump policy, but it doesn't
have Star Parks.
doesn't need output
doesn't need output
size or no this one doesn't need output
size or no this one doesn't need output
size
size
right this or it doesn't need num layers
anymore just making sure we still have
anymore just making sure we still have
our mms
running we definitely need to remove
running we definitely need to remove
like some of these
like some of these
is continuous blah blah blah.
3.9 mil per is actually not that far off
3.9 mil per is actually not that far off
from what we're training with neural MMO
from what we're training with neural MMO
at like millions uh was it 500k steps
at like millions uh was it 500k steps
per second. So we should actually be
per second. So we should actually be
able to make this fast maybe.
So, what's happened here?
Why is this
Why is this
a You know what this probably
a You know what this probably
is? Hang on. We know what this
is. We don't need this anymore.
is. We don't need this anymore.
That's just API shenanigans like growing
That's just API shenanigans like growing
pain cans puff are getting
pain cans puff are getting
bigger puff are getting simpler but
bigger puff are getting simpler but
um have to refactor things.
We technically probably should just have
We technically probably should just have
like um plugandplay
like um plugandplay
Atari benchmark that we can just
Atari benchmark that we can just
run. It's just so freaking compute
run. It's just so freaking compute
inefficient. It's like
We could do it like 30 minutes max per
We could do it like 30 minutes max per
end or something so that we run the
end or something so that we run the
whole suite in a
day. We probably still get some decent
perf. Okay, the SPS will update in a
perf. Okay, the SPS will update in a
second.
92% in
learn. That seems
sketchy. Doesn't that seem sketchy to
you? Well, let's make it sketchier first
you? Well, let's make it sketchier first
by doing
by doing
One,
One,
one,
one. Certex. Yep, that's the experience
one. Certex. Yep, that's the experience
buff for shenanigans I'm fixing right
buff for shenanigans I'm fixing right
now. Um, that means your batch size
now. Um, that means your batch size
isn't big
isn't big
enough or you don't have enough M's
enough or you don't have enough M's
rather for the I think it means you have
rather for the I think it means you have
too many M's for your batch
size. I'm going to add warnings and
size. I'm going to add warnings and
stuff so that like that's easier to not
stuff so that like that's easier to not
do.
just like either double the batch size
just like either double the batch size
or half the number of M's and see if it
works. Okay, this seems like it's
works. Okay, this seems like it's
broken,
broken,
right? Oh, no. It
right? Oh, no. It
works. It's just really freaking
works. It's just really freaking
slow.
slow.
Works. So, I think I might have fixed
Works. So, I think I might have fixed
that other weird bug like because I'm
that other weird bug like because I'm
not I'm not hitting it
anymore. Let's go back to this
one. Mini batch size of 1024. This is
one. Mini batch size of 1024. This is
probably too small,
right? Or too big or something. What the
right? Or too big or something. What the
hell is this?
Now I'm getting no steps
Now I'm getting no steps
percent. Uh so that's the thing I
percent. Uh so that's the thing I
literally just patched. But now it means
literally just patched. But now it means
that you have too few environments per
batch. Yeah. So this is the jank thing
batch. Yeah. So this is the jank thing
with the uh the new experience buffer
with the uh the new experience buffer
that I'm fixing. It works, but it
that I'm fixing. It works, but it
doesn't like certain param ranges right
doesn't like certain param ranges right
now. So I'm fixing
now. So I'm fixing
that. It's stuff like um Pudinum
that. It's stuff like um Pudinum
threads, right? Like that's like pudinum
threads, right? Like that's like pudinum
threads. I just fixed that. Um and then
threads. I just fixed that. Um and then
there like other related
there like other related
shenanigans
basically. That's good. Now, let's fix
basically. That's good. Now, let's fix
that bug next. Well, not even really a
bug. Let me just send this over to you.
Okay, take this. If I rebase it out, uh,
Okay, take this. If I rebase it out, uh,
well, it'll fix some of your issues and
well, it'll fix some of your issues and
probably cause several
others, I would imagine.
I mean, it's probably worth at least
I mean, it's probably worth at least
trying because then you'll have up to
trying because then you'll have up to
date with this and we'll see if we can
date with this and we'll see if we can
get it to train on
get it to train on
that like on the
latest. The one that is tricky though is
latest. The one that is tricky though is
the one we're going to try to fix right
the one we're going to try to fix right
now. I got to think about the experience
now. I got to think about the experience
buffer the way it
works. I can draw this actually.
If they start putting like product
If they start putting like product
placement stuff into LLMs, I'm just
placement stuff into LLMs, I'm just
ditching them
immediately. It's like half the use case
immediately. It's like half the use case
of what I use it for is like finding
of what I use it for is like finding
stuff. If it's like sponsored chat GPT
stuff. If it's like sponsored chat GPT
suggestion,
nope. All right.
nope. All right.
So, here's the thing with the experience
So, here's the thing with the experience
buffer. The way it used to work was the
buffer. The way it used to work was the
experience buffer looked like
experience buffer looked like
this. And you'd get a block of data in
this. And you'd get a block of data in
like
like
so. And the block of data has lots of uh
so. And the block of data has lots of uh
different agents worth of experience.
different agents worth of experience.
And you just do
this. And then the next block comes
in. You again just do this.
in. You again just do this.
And then when you need to train, when
And then when you need to train, when
you need to
train, what you have to do is you make
train, what you have to do is you make
like a big buffer like
this. Well, you have to like sort it
this. Well, you have to like sort it
first.
first.
So first this is like hang
on you have to like sort it first so
on you have to like sort it first so
that you know this where is it this one
that you know this where is it this one
here has to line up with this one here.
here has to line up with this one here.
So you get like this.
So there's like this complicated sort
So there's like this complicated sort
operation
operation
here where like you need the first step
here where like you need the first step
from each a or you need the step of all
from each a or you need the step of all
the agent one like all of agent one time
the agent one like all of agent one time
steps and then all of ages time steps
steps and then all of ages time steps
and whatever. So you do a sort and then
and whatever. So you do a sort and then
you would reshape it into like this
you would reshape it into like this
block goes here and this block goes here
block goes here and this block goes here
and then this block goes here. That's
and then this block goes here. That's
how it worked.
Um, this is cool, but and this like
Um, this is cool, but and this like
handles variable length data segments
handles variable length data segments
very very well. The issue with
very very well. The issue with
this is that because you have variable
this is that because you have variable
length data segments when you do this
length data segments when you do this
reshape, you don't actually get like
reshape, you don't actually get like
contiguous
contiguous
uh like you're going to get overflow and
uh like you're going to get overflow and
like segments are going to get cut
like segments are going to get cut
across in between these
So, how I currently have the new one
working your new experience buffer,
working your new experience buffer,
right? Um, this goes here, this goes
right? Um, this goes here, this goes
here, this goes here, this goes here,
here, this goes here, this goes here,
and it saves it pretty much in just the
and it saves it pretty much in just the
order that you want to use your data,
order that you want to use your data,
right? So, this is like all ready to go.
right? So, this is like all ready to go.
Um there are a few issues on this that
Um there are a few issues on this that
I've punted on like what happens if m
I've punted on like what happens if m
return very different amounts of data
return very different amounts of data
then like some of these are longer than
then like some of these are longer than
others and you get problems. I have some
others and you get problems. I have some
ways of handling this but it's not
ways of handling this but it's not
ideal. Uh but this does fix this
ideal. Uh but this does fix this
annoyance. So what you can do with this
annoyance. So what you can do with this
is uh you can just
is uh you can just
like filter out which of these segments
like filter out which of these segments
you want and which of these segments you
you want and which of these segments you
don't
want. That's what you can do with this
want. That's what you can do with this
design.
design.
Now the
Now the
issue
issue
is you actually have to start thinking
is you actually have to start thinking
about like the number of rows and stuff
about like the number of rows and stuff
that you
that you
have because this
have because this
is so this is your segment length. So
is so this is your segment length. So
this is like BP
this is like BP
TT
TT
horizon and then this
is this is
is this is
like
bash divided
by and this actually has to be a
by and this actually has to be a
reasonable number right so that's the
reasonable number right so that's the
error that captain was getting before
error that captain was getting before
with the assert free index less than
with the assert free index less than
equal experience rows right up there um
equal experience rows right up there um
is that the batch size wasn't big enough
is that the batch size wasn't big enough
for the number of ms
for the number of ms
because you also have to think about if
because you also have to think about if
you have each of these rows right each
you have each of these rows right each
row is an agent so if you like have
row is an agent so if you like have
let's say 40
let's say 40
m then you have to have batch
Right. And ideally you have it as a
Right. And ideally you have it as a
constant multiple because otherwise you
constant multiple because otherwise you
get um if you don't have it as a
get um if you don't have it as a
constant multiple then like you're not
constant multiple then like you're not
going to have anywhere to put your data
going to have anywhere to put your data
for a lot of your
for a lot of your
environments. So I
environments. So I
mean I kind of want to think about this
mean I kind of want to think about this
design for a little bit.
design for a little bit.
cuz I'm open to maybe like maybe the old
cuz I'm open to maybe like maybe the old
one was better and this is just annoying
one was better and this is just annoying
as
as
hell,
hell,
but it's kind of tricky to say
but it's kind of tricky to say
because the way that I did this before
because the way that I did this before
uh you also couldn't do GA anywhere near
uh you also couldn't do GA anywhere near
as fast. So when you have this uh this
as fast. So when you have this uh this
buffer up top and you sort it, you still
buffer up top and you sort it, you still
end up with one gigantic row, right? So
end up with one gigantic row, right? So
when you run generalized advantage
when you run generalized advantage
estimation on this or you run some
estimation on this or you run some
advantage formula, you have to start at
advantage formula, you have to start at
the back, right? And then you keep like
the back, right? And then you keep like
hopping backwards until you hit a
hopping backwards until you hit a
done. Like what is this D? Like a done.
done. Like what is this D? Like a done.
And then you go to the next one.
And then you go to the next one.
But you can't parallelize this at all.
But you can't parallelize this at all.
Like you just have to write a C loop and
Like you just have to write a C loop and
that's not actually fast enough. Even
that's not actually fast enough. Even
doing this like in optimized C is not
doing this like in optimized C is not
fast enough. So the cool thing with this
fast enough. So the cool thing with this
uh is now we have CUDA kernels, right?
uh is now we have CUDA kernels, right?
And the CUDA kernels are row parallel.
And the CUDA kernels are row parallel.
So they're really really really fast.
So they're really really really fast.
And that's actually important for what
And that's actually important for what
we're doing the new advantage filtering
we're doing the new advantage filtering
stuff. If you
stuff. If you
have like fully vectorized ends with
have like fully vectorized ends with
that return fixed length data
that return fixed length data
segments, I can make this work
segments, I can make this work
100%. Because you basically just go from
100%. Because you basically just go from
n1 to n2 to n1 to n2 to n1 to n2. You
n1 to n2 to n1 to n2 to n1 to n2. You
get like this nice buffer uh this nice
get like this nice buffer uh this nice
like buffer sampling and everything just
like buffer sampling and everything just
works perfectly.
works perfectly.
So I guess the question
So I guess the question
is how much do we
is how much do we
care about environments with like
care about environments with like
masking and like what is the reasonable
masking and like what is the reasonable
workaround for that? Because the top one
workaround for that? Because the top one
here, this natively just handles
here, this natively just handles
masking, variable agent length. It
masking, variable agent length. It
handles all that stuff just
handles all that stuff just
perfectly. That's why I did it this way
perfectly. That's why I did it this way
originally because it actually handles
originally because it actually handles
all like the jank ends perfectly.
The new one doesn't really handle that
The new one doesn't really handle that
at
all. Well, I mean, at the possible
all. Well, I mean, at the possible
expense of a little bit of
expense of a little bit of
uh CPU end of
uh CPU end of
time, which probably isn't even going to
time, which probably isn't even going to
be that bad to be
be that bad to be
fair. I can make it so that the
fair. I can make it so that the
vectorzation always samples contiguous
vectorzation always samples contiguous
segments of environments and like does
segments of environments and like does
this sort of ring buffer thing or this
this sort of ring buffer thing or this
like multiple buffered thing, right? I
like multiple buffered thing, right? I
can actually do that very
nicely. So even for single agent M and
nicely. So even for single agent M and
like stuff like Atari, I actually can
like stuff like Atari, I actually can
guarantee you that you fill up this
guarantee you that you fill up this
buffer perfectly.
Now you are going to end up with
Now you are going to end up with
um M's that go across segments I
um M's that go across segments I
guess like let's say that this is 128 or
guess like let's say that this is 128 or
this is something really long like you
this is something really long like you
could get a done signal right
could get a done signal right
here but it's not that bad cuz
here but it's not that bad cuz
like at least you're going to keep your
like at least you're going to keep your
LSTM state from the same you're going to
LSTM state from the same you're going to
keep your LSTM state from the same
keep your LSTM state from the same
agent. This is how the data was
agent. This is how the data was
collected during training. This is kind
collected during training. This is kind
of probably fine.
The other issue though is this kind of
The other issue though is this kind of
does force you
does force you
into really long BPT
into really long BPT
horizons. Um so you could get into a
horizons. Um so you could get into a
state where like you don't have enough
state where like you don't have enough
rows per batch to be compute efficient
rows per batch to be compute efficient
because you need to have at least a
because you need to have at least a
certain number of rows uh in your batch
certain number of rows uh in your batch
for the GPU to be efficient. And you
for the GPU to be efficient. And you
know, if you make this really
long, is that a problem? Yeah, because
long, is that a problem? Yeah, because
the mini batch size gets too
the mini batch size gets too
big. But technically,
big. But technically,
like I could always chunk this up,
like I could always chunk this up,
right? I could cut this in half this way
right? I could cut this in half this way
if like that became an issue. And that
if like that became an issue. And that
would actually be not that difficult to
would actually be not that difficult to
do. I
do. I
think I could do that. So, I think that
think I could do that. So, I think that
this design works for that.
Um, so I mean with some warnings on like
Um, so I mean with some warnings on like
acceptable scales and
acceptable scales and
stuff, this is mostly
stuff, this is mostly
fine. The only thing that where this
fine. The only thing that where this
actually really does screw up I think
actually really does screw up I think
it's and this actually might affect
it's and this actually might affect
captain here.
captain here.
Um, it's when you have like masked
Um, it's when you have like masked
multi- aent
multi- aent
Ms because like you can get a bunch of
Ms because like you can get a bunch of
data for one agent and then a few steps
data for one agent and then a few steps
for another
agent. Not
agent. Not
ideal. The old design was way better for
ideal. The old design was way better for
that specific case.
I keep getting the assertion in
popcu. I tried playing around with batch
popcu. I tried playing around with batch
size.
size.
260k batch 128mm 16
workers. I rebased your latest. You
workers. I rebased your latest. You
still get it with the latest
commit. The thread assertion should you
commit. The thread assertion should you
should wait. Are you getting the thread
should wait. Are you getting the thread
assertion or the indexing one? The free
index. You're getting the thread
index. You're getting the thread
error. Okay, that's weird.
What's your uh your end batch
size? So, you're only getting 32 M's
size? So, you're only getting 32 M's
each with like what? Four
agents. Sure.
agents. Sure.
Let's do that real
quick and then I will go back to
quick and then I will go back to
fixing experience buffer
fixing experience buffer
stuff. I the masking thing is going to
stuff. I the masking thing is going to
drive me nuts. I that's the main thing I
drive me nuts. I that's the main thing I
got to figure out. I think everything
got to figure out. I think everything
else is workable.
I am on the Discord. Captain, when
I am on the Discord. Captain, when
you're ready,
I wonder if ad companies have already
I wonder if ad companies have already
started like trying to data poison
LLMs. That's going to be
LLMs. That's going to be
annoying. Desktop isn't recognized in
annoying. Desktop isn't recognized in
headphones. Always fun.
headphones. Always fun.
I was actually looking at getting a
I was actually looking at getting a
higherend headset yesterday, but then I
higherend headset yesterday, but then I
thought that like I actually wanted to
thought that like I actually wanted to
keep this one because this has this
keep this one because this has this
retractable mic just in case I need a
retractable mic just in case I need a
backup because my really nice mic
backup because my really nice mic
doesn't get recognized by like some
doesn't get recognized by like some
stupid conferencing software.
So, I'm getting a very nice mic and a
So, I'm getting a very nice mic and a
very nice camera and all the things you
very nice camera and all the things you
need for those. And then the same like
need for those. And then the same like
shitty
headset. Shitty for audio quality. These
headset. Shitty for audio quality. These
are actually super
comfortable. They are SEO spam. Most LM
comfortable. They are SEO spam. Most LM
companies scrape without much
filtering.
Um, I guess I don't know if that is
Um, I guess I don't know if that is
going to matter as much as like fake
going to matter as much as like fake
review stuff. I think the LLM's will be
review stuff. I think the LLM's will be
like way more screwed up by like fake
like way more screwed up by like fake
reviews and
things. I mean like I went through a
things. I mean like I went through a
whole thing yesterday trying to figure
whole thing yesterday trying to figure
out a like high-end AV stuff with uh
out a like high-end AV stuff with uh
with
with
Brock and I mean it's like you really
Brock and I mean it's like you really
want confidence in the answers to simple
want confidence in the answers to simple
things like hey what do uh what
things like hey what do uh what
microphone do uh like most profession
microphone do uh like most profession
like what most like professional
like what most like professional
streamers use that actually have
streamers use that actually have
high-rade stuff like what mic and camera
high-rade stuff like what mic and camera
do they get? Like it's actually hard to
do they get? Like it's actually hard to
get like clear consistent answers to
get like clear consistent answers to
stuff like
that even though most of them are using
that even though most of them are using
like one of the same three cameras or
microphones. But yeah, long story short,
microphones. But yeah, long story short,
I got like a whole AV setup with like a
I got like a whole AV setup with like a
I mean a fullsize camera, tripod, like
I mean a fullsize camera, tripod, like
boom, proper boom arm. Um it's going to
boom, proper boom arm. Um it's going to
be really cool for the new facility.
be really cool for the new facility.
And it's going to let me do some cool
And it's going to let me do some cool
new content around all this dev.
And I will keep the like webcam whatever
And I will keep the like webcam whatever
set up
set up
for elsewhere because I'm not going to
for elsewhere because I'm not going to
be in that facility year round but good
be in that facility year round but good
chunk of the
year. Hey C.
year. Hey C.
Uh, okay. Can you hear me? Yep. Cool.
Uh, okay. Can you hear me? Yep. Cool.
All right. Uh, let me log in the buffer
All right. Uh, let me log in the buffer
box. Can I had to reboot and then I will
box. Can I had to reboot and then I will
uh share my Your box is good, right?
uh share my Your box is good, right?
Yeah. Oh, yeah. Sorry, I forgot to uh
Yeah. Oh, yeah. Sorry, I forgot to uh
What one are you on? Uh, box two. Okay.
What one are you on? Uh, box two. Okay.
Do make sure you have everything synced
Do make sure you have everything synced
today because um tomorrow all boxes that
today because um tomorrow all boxes that
have either broken CPUs or loose fans
have either broken CPUs or loose fans
are getting sent to Main Gear most
are getting sent to Main Gear most
likely. Uh, and then I'm going to like
likely. Uh, and then I'm going to like
rep prioritize compute so that people
rep prioritize compute so that people
actually still have stuff. I think I'm
actually still have stuff. I think I'm
taking both of my personal boxes with me
taking both of my personal boxes with me
and then I will give up my uh the two
and then I will give up my uh the two
that I'm using on the cluster for
that I'm using on the cluster for
temporarily so people can use stuff.
temporarily so people can use stuff.
Okay. Um, all right. Can you see my
Okay. Um, all right. Can you see my
screen?
screen?
See, and of course you know we're on
See, and of course you know we're on
stream, right? Yep.
stream, right? Yep.
Just checking.
So yeah, this is just me on box two
So yeah, this is just me on box two
right now. Um I have played around with
right now. Um I have played around with
a couple things. The original params
a couple things. The original params
were like I said uh the number of ends
were like I said uh the number of ends
were 128. The number of workers was 16.
were 128. The number of workers was 16.
I I changed some things obviously. I can
I I changed some things obviously. I can
change them back. But even right now,
change them back. But even right now,
how many agents per end?
how many agents per end?
Uh 64 agents in each end. Oh, uh I guess
Uh 64 agents in each end. Oh, uh I guess
it's two times that. So I guess it's
it's two times that. So I guess it's
128. No. Oh, sorry. Sorry. Per M. Two.
128. No. Oh, sorry. Sorry. Per M. Two.
Two. Two agents per M. Do you usually
Two. Two agents per M. Do you usually
train with that few MS?
train with that few MS?
No. Normally I do Well, some some sweeps
No. Normally I do Well, some some sweeps
have actually found Yeah. 64 M per
have actually found Yeah. 64 M per
worker. done. Well, but usually it's
worker. done. Well, but usually it's
either 64 or 128. Yeah, it's usually not
either 64 or 128. Yeah, it's usually not
64 128 when you use the x-axis is time.
64 128 when you use the x-axis is time.
Uh yeah, really?
Uh yeah, really?
Yeah, cuz I mean as hell. So, when I'm
Yeah, cuz I mean as hell. So, when I'm
doing protein, um the uh what is it?
doing protein, um the uh what is it?
Like the cost is standard like walk
Like the cost is standard like walk
clock time and then the score is just
clock time and then the score is just
well I originally had the score being my
well I originally had the score being my
custom like uh win rate but then I just
custom like uh win rate but then I just
changed it to score because I saw that
changed it to score because I saw that
puffer li kind of does like an eval
puffer li kind of does like an eval
metric. Yes. For you so score seemed
metric. Yes. For you so score seemed
accurate. I I have not seen like the
accurate. I I have not seen like the
very small M screw up things like the
very small M screw up things like the
three small M weird hypers since uh we
three small M weird hypers since uh we
switched to Muon and reram sweeps. We
switched to Muon and reram sweeps. We
were getting that on Adam before that
were getting that on Adam before that
like it had to have like a small number
like it had to have like a small number
of ends.
of ends.
Yeah, this is with Muan though. This is
Yeah, this is with Muan though. This is
on latest dev. I've been running a lot
on latest dev. I've been running a lot
of these sweeps. Okay, well we can try
of these sweeps. Okay, well we can try
it. Let's just see to get something
it. Let's just see to get something
working here because I this is not a bug
working here because I this is not a bug
in Puffer. This is like a lack of
in Puffer. This is like a lack of
warnings that I need to fix. And the
warnings that I need to fix. And the
only thing I haven't done, by the way,
only thing I haven't done, by the way,
and this is literally your is the only
and this is literally your is the only
one affected by this. Um, but I have to
one affected by this. Um, but I have to
figure out something to do about
figure out something to do about
masking. Masking is currently not in the
masking. Masking is currently not in the
dev. Um, and I need to figure out what
dev. Um, and I need to figure out what
to do about that. The new experience
to do about that. The new experience
buffer design basically works with
buffer design basically works with
everything that you could possibly
everything that you could possibly
imagine except masking. Um, yes, that's
imagine except masking. Um, yes, that's
Well, if it makes you feel any better,
Well, if it makes you feel any better,
masking for impulse wars is only
masking for impulse wars is only
important when you have more than two
important when you have more than two
agents. Um, which I'm not currently
agents. Um, which I'm not currently
training on. I'm just trying to get like
training on. I'm just trying to get like
a two agent baseline working. So, Oh,
a two agent baseline working. So, Oh,
and you do one. Yeah. Okay, that makes
and you do one. Yeah. Okay, that makes
sense. Yeah. So, let's actually perfect.
sense. Yeah. So, let's actually perfect.
Let's do your experiments with two
Let's do your experiments with two
agents for now and then that'll give me
agents for now and then that'll give me
some time to sort this [ __ ] out. Yeah.
some time to sort this [ __ ] out. Yeah.
Okay. Now, we are going to have to crank
Okay. Now, we are going to have to crank
up numbers of ends and stuff to start
up numbers of ends and stuff to start
with. So, set this to like 1024 M. 1024
with. So, set this to like 1024 M. 1024
M. Okay. I don't think that'll do well
M. Okay. I don't think that'll do well
performance. I'm going to find other
performance. I'm going to find other
pars for you in a second. Um, so you got
pars for you in a second. Um, so you got
no drones, no agents. What do you do?
no drones, no agents. What do you do?
buffer. Holy BPT Horizon Batman. What
buffer. Holy BPT Horizon Batman. What
the hell is that? Um, what? 256.
the hell is that? Um, what? 256.
That's That's big. That's chunky.
That's That's big. That's chunky.
So, I found that um the sweeps either
So, I found that um the sweeps either
like 256 or 128. Was it sensitive to it?
like 256 or 128. Was it sensitive to it?
Like, was there a big perk difference?
Like, was there a big perk difference?
Uh, decent. Uh, I can pull up I can send
Uh, decent. Uh, I can pull up I can send
you the sweeps or Sure. 2048
you the sweeps or Sure. 2048
M. Oh, wait. No, 1024x is too many
M. Oh, wait. No, 1024x is too many
because we have you have 16 process.
because we have you have 16 process.
Wait, wait, wait. Okay. No, dude. 64 is
Wait, wait, wait. Okay. No, dude. 64 is
totally fine because that gets
totally fine because that gets
multiplied by the number of workers.
multiplied by the number of workers.
Yes. Okay. I didn't realize you had 16.
Yes. Okay. I didn't realize you had 16.
So 64 * 16 is 1024 m. That's like
So 64 * 16 is 1024 m. That's like
roughly on par. I would still want to
roughly on par. I would still want to
see 128, but that's still fine. So then
see 128, but that's still fine. So then
if we do uh 128 16 128 is what the uh I
if we do uh 128 16 128 is what the uh I
was doing originally, but um that was
was doing originally, but um that was
giving me a different error. So I we'll
giving me a different error. So I we'll
fix both of those.
fix both of those.
So 2048
So 2048
uh times 250 was it 256 horizon has a
uh times 250 was it 256 horizon has a
minimum batch size requirement of 524.
minimum batch size requirement of 524.
Yep. So that's what you have. So you can
Yep. So that's what you have. So you can
just try try this to see what happens
just try try this to see what happens
first. And mini batch isn't affected by
first. And mini batch isn't affected by
this, right?
this, right?
uh mini back. The only thing is that's
uh mini back. The only thing is that's
going to be slow as hell of a learner
going to be slow as hell of a learner
because the thing you got to think about
because the thing you got to think about
16 384 divide by 256 horizon you only
16 384 divide by 256 horizon you only
have 64 rows parallel for your GPU.
have 64 rows parallel for your GPU.
Yeah, this is just what the sweep well
Yeah, this is just what the sweep well
actually the sweep suggested 264k batch
actually the sweep suggested 264k batch
and 8K mini batch. So, I just basically
and 8K mini batch. So, I just basically
doubled when I doubled the batch. I was
doubled when I doubled the batch. I was
a double the mini badge. But I can I'm
a double the mini badge. But I can I'm
planning on helping you with retuning
planning on helping you with retuning
once you have this anyways. Let's just
once you have this anyways. Let's just
see if this runs though. Yes. Like I'm
see if this runs though. Yes. Like I'm
going to help you with this anyways.
going to help you with this anyways.
Cool. Yeah.
Cool. Yeah.
Yep. This is what I'm getting. Is it Are
Yep. This is what I'm getting. Is it Are
the corners cut off in Puffer Live on
the corners cut off in Puffer Live on
your turn?
your turn?
Uh, I think because I'm double
t-moxing because I have T-Mox in the
t-moxing because I have T-Mox in the
puffer box and I have T-Mox on my host.
puffer box and I have T-Mox on my host.
So, it sometimes gets weird. I guess a
So, it sometimes gets weird. I guess a
stop nasty character or whatever.
stop nasty character or whatever.
Yeah, it doesn't show either, right? It
Yeah, it doesn't show either, right? It
doesn't show the puffer fish emote.
doesn't show the puffer fish emote.
Yeah. Okay. No. Yeah, it's probably
Yeah. Okay. No. Yeah, it's probably
okay. Maybe I'll find an character that
okay. Maybe I'll find an character that
I can use for that. Good to know.
I can use for that. Good to know.
That's so weird though that you have
That's so weird though that you have
colors. You have like full color but you
colors. You have like full color but you
don't have like UTF8.
So
So
weird. Um getting the last commit was
weird. Um getting the last commit was
not today. Did you push that?
not today. Did you push that?
I might have [ __ ] up. I
know. That's buffer maximum.
Okay, try that.
This is just rebuilding.
What? So this is something to do with
What? So this is something to do with
mini batch and what else?
Size of a
[Music]
tensor config
tensor config
loss. All right, go put a break point
loss. All right, go put a break point
there.
there.
All
All
right. So, it should be getting batched
right. So, it should be getting batched
correctly. It seems to me like one of
correctly. It seems to me like one of
those is flat and the other
isn't 476. I'm
happy. Actually, you should just be able
happy. Actually, you should just be able
to hop on this box, right? If you just
to hop on this box, right? If you just
wanted it, it' probably be faster if you
wanted it, it' probably be faster if you
did it. That is true because this is
did it. That is true because this is
just on box two. That is a perk of uh of
just on box two. That is a perk of uh of
the cluster. I forget
the cluster. I forget
about it. Papa box five. No, two
is in T-Max.
is in T-Max.
Yeah. So, just attach and I get your
Yeah. So, just attach and I get your
break point. Perfect.
break point. Perfect.
Man, this is like the more I play around
Man, this is like the more I play around
this, the more I like this cluster
this, the more I like this cluster
setup.
setup.
Yeah, it's
nice. I don't really need the stream up.
nice. I don't really need the stream up.
There's like a couple second
There's like a couple second
lag. No, no, no. The stream is for uh
lag. No, no, no. The stream is for uh
YouTube. We got five folks on YouTube.
Hello.
Uh well, that's screwy.
How did I mess that up?
How did I mess that up?
So, you know what happened? Well,
So, you know what happened? Well,
something's not getting reshaped
something's not getting reshaped
correctly. You're using puffer
correctly. You're using puffer
advantage.
advantage.
Uh, should be default. I I'm If it's a
Uh, should be default. I I'm If it's a
default, yes, I think
default, yes, I think
config puff advantage. I might need to
config puff advantage. I might need to
check the default. So I don't set it but
check the default. So I don't set it but
I remember I okay I am
I remember I okay I am
use puff advantage.
use puff advantage.
It's just V trace plus generalized
It's just V trace plus generalized
advantage estimation kernel.
That took me like that took me a solid
That took me like that took me a solid
two days of just banging my head on math
two days of just banging my head on math
to realize it was like a threeline
to realize it was like a threeline
change in CUDA.
change in CUDA.
Wait really? Yeah, it literally is
Wait really? Yeah, it literally is
because there were two papers and they
because there were two papers and they
wrote their math like completely
wrote their math like completely
differently, but like when you actually
differently, but like when you actually
see how they see what it is and you like
see how they see what it is and you like
change all the variables to match each
change all the variables to match each
other and like you rewrite all the
other and like you rewrite all the
formulas like it's literally the same
formulas like it's literally the same
algorithm with like one small change so
algorithm with like one small change so
you can just put them together trivially
you can just put them together trivially
but it took me two days of banging my
but it took me two days of banging my
head against the math to figure that
head against the math to figure that
out.
out.
Dang. Yeah, I guess there's a lot of
Dang. Yeah, I guess there's a lot of
different ways to express the same thing
different ways to express the same thing
in math and stuff. So, yeah, that makes
in math and stuff. So, yeah, that makes
sense.
sense.
But yeah, that's cool. Yeah, I would
But yeah, that's cool. Yeah, I would
have realized it. I should have realized
have realized it. I should have realized
it sooner. My math is it's improving.
it sooner. My math is it's improving.
You know, I've been doing more of it
You know, I've been doing more of it
lately. It's been improving. It's still
lately. It's been improving. It's still
kind of bad,
kind of bad,
but still leagues above mine, but I
but still leagues above mine, but I
don't I don't Well, mine was never that
don't I don't Well, mine was never that
good to begin with, and I also don't
good to begin with, and I also don't
really need to flex hardly flex what
really need to flex hardly flex what
little I have hardly ever.
little I have hardly ever.
Yeah, I this is why mine isn't good,
Yeah, I this is why mine isn't good,
right? It's because I don't like need it
right? It's because I don't like need it
that often, but now I'm starting like,
that often, but now I'm starting like,
okay, I got to pick some stuff up.
okay, I got to pick some stuff up.
Um, so the returns are batched and the
Um, so the returns are batched and the
value is not. I thought
value is not. I thought
this new value
got mean that's like
Do you have a custom LSTM wrapper or
Do you have a custom LSTM wrapper or
something?
something?
Uh, not I mean I wrap around it. I mean,
Uh, not I mean I wrap around it. I mean,
you can look at it. It's I mean, yeah,
you can look at it. It's I mean, yeah,
but it doesn't really do much. It
but it doesn't really do much. It
basically just wraps. I probably did
basically just wraps. I probably did
some like jank reshape stuff and you
some like jank reshape stuff and you
probably don't have it.
probably don't have it.
I mean, I might have done something. You
I mean, I might have done something. You
can take a look at it quick. Let me just
can take a look at it quick. Let me just
see.
Value. So, look. Do you see the three
Value. So, look. Do you see the three
shape here? You're missing this is my
shape here? You're missing this is my
guess.
guess.
So, where's your
So, where's your
policy? Um, it should, excuse me, should
policy? Um, it should, excuse me, should
be in puffer li torch at the
bottom. Uh, this subass is recurrent.
bottom. Uh, this subass is recurrent.
Yeah. And you you uh basically alias it
Yeah. And you you uh basically alias it
above to like the LCM wrap or whatever.
Um, you don't have forward train on
Um, you don't have forward train on
this. How's this running without forward
this. How's this running without forward
train? Maybe it's not
train? Maybe it's not
needed. Seems weird though. Oh, forward
needed. Seems weird though. Oh, forward
train is a new thing. Yeah, I don't know
train is a new thing. Yeah, I don't know
if you actually need it outside of the
if you actually need it outside of the
um the com though if you're not using
um the com though if you're not using
like nonrecurrent policy. And I do have
like nonrecurrent policy. And I do have
to go
to go
soon. But let me see if we can at least
soon. But let me see if we can at least
fix this for
you. Seems fine.
Well, there's your there's your problem.
Well, there's your there's your problem.
What? Oh, you're supposed to be
Mhm. Uh, this something. Did this get
Mhm. Uh, this something. Did this get
merged correctly?
merged correctly?
I don't know if you got
I don't know if you got
merge nicely or not.
merge nicely or not.
Also, if I look I have a it's not ready
Also, if I look I have a it's not ready
for merge quite yet, but I have a uh I
for merge quite yet, but I have a uh I
have a branch and when I do a uh like a
have a branch and when I do a uh like a
diff basically on puffer dev, I don't
diff basically on puffer dev, I don't
think anything in queen RL gets
think anything in queen RL gets
modified.
So, I'm suspicious of this thing right
here. Also, if you don't tag with
here. Also, if you don't tag with
Neptune, it'll load faster. If you don't
Neptune, it'll load faster. If you don't
run Neptune for test to load faster.
run Neptune for test to load faster.
Okay, good to
Okay, good to
know. Makes sense.
You have compile on. Yeah. Okay. That's
You have compile on. Yeah. Okay. That's
I didn't account for that. That's
I didn't account for that. That's
why.
why.
Uh where's the config ocean
Uh where's the config ocean
uh impulse? Yep.
Okay, so that's a me problem. My bad.
What was the issue? Uh, I just didn't
What was the issue? Uh, I just didn't
account for compile in uh my reshapes
account for compile in uh my reshapes
because I I check if it's an instance of
because I I check if it's an instance of
an LSPM and uh I forgot that compile
an LSPM and uh I forgot that compile
adds an extra wrapper.
adds an extra wrapper.
Gotcha. That makes sense. So I will fix
Gotcha. That makes sense. So I will fix
that. I will make a note to fix that.
Okay. Yeah, it seems to be working.
Okay. Yeah, it seems to be working.
No, it isn't cuz you're not getting a
No, it isn't cuz you're not getting a
steps.
steps.
What?
What?
Yes, that's the other bug with the
Yes, that's the other bug with the
experience buffer.
experience buffer.
Yeah, cuz I've never seen That's the
Yeah, cuz I've never seen That's the
other other bug.
So, what causes that?
So, what causes that?
Uh, that one's just demons.
Uh, that one's just demons.
Fair
enough.
Okay, let the SPS update
Okay, let the SPS update
160. You're probably higher than that
160. You're probably higher than that
before, right?
before, right?
Uh yeah. Well, so yesterday I saw up to
Uh yeah. Well, so yesterday I saw up to
like 200 with different hyperparameters
like 200 with different hyperparameters
or something, but we are losing all the
or something, but we are losing all the
time in the board pass right now. Yeah.
time in the board pass right now. Yeah.
And that's because of the multi-discreet
And that's because of the multi-discreet
actions, is it? So I actually I actually
actions, is it? So I actually I actually
would very much like to get the stats
would very much like to get the stats
and the reporting on that because I had
and the reporting on that because I had
uh I've got a client that is likewise
uh I've got a client that is likewise
unhappy with multi-discrete sampling
unhappy with multi-discrete sampling
perf. So, I will be very happy like
perf. So, I will be very happy like
let's get this integrated because this
let's get this integrated because this
will be a perfect test case for me to
will be a perfect test case for me to
figure out what the heck is wrong with
figure out what the heck is wrong with
that because I can't reproduce it
that because I can't reproduce it
anywhere else.
anywhere else.
Um there probably is some well I don't
Um there probably is some well I don't
know actually what to do to fix it but I
know actually what to do to fix it but I
know like I said if you just get um
know like I said if you just get um
torch compile working like correctly
torch compile working like correctly
with minimal graph breaks that basically
with minimal graph breaks that basically
eliminates it. And now well once that
eliminates it. And now well once that
was working before the bottleneck went
was working before the bottleneck went
from forward well once torch 2.6 hit um
from forward well once torch 2.6 hit um
the bottleneck went from forward to M
the bottleneck went from forward to M
like M I was being hard M bottleneck
like M I was being hard M bottleneck
which makes sense because impulse force
which makes sense because impulse force
is a slowish M with all the physics
is a slowish M with all the physics
stuff. So right now I'm being like
stuff. So right now I'm being like
almost totally as you can see um forward
almost totally as you can see um forward
pass bottlenecks but I'm confident we
pass bottlenecks but I'm confident we
will be able to at least double your PF
will be able to at least double your PF
though with this. We should be able to.
though with this. We should be able to.
Okay, cool. So 300k is like kind of a
Okay, cool. So 300k is like kind of a
minimum for being fast. Um, all right,
minimum for being fast. Um, all right,
cool. And this is this is also slower
cool. And this is this is also slower
because there is one agent per M. So two
because there is one agent per M. So two
well wait, yeah, actually my bad. One
well wait, yeah, actually my bad. One
agent perm. There's two drones but one
agent perm. There's two drones but one
is scripted. If you have two agents per
is scripted. If you have two agents per
M, then it like basically doubles, which
M, then it like basically doubles, which
is nice. But I haven't got selfplay
is nice. But I haven't got selfplay
working well yet. Something I'd like to
working well yet. Something I'd like to
do.
do.
Yeah, but definitely just like get this
Yeah, but definitely just like get this
PR finished and I will see if I can
PR finished and I will see if I can
merge it later today. Depends how long
merge it later today. Depends how long
cluster stuff takes after meeting and uh
cluster stuff takes after meeting and uh
Okay. Yeah, we will chat. But I'm
Okay. Yeah, we will chat. But I'm
looking forward to getting some
looking forward to getting some
baselines on this. It'll be fun. And I'm
baselines on this. It'll be fun. And I'm
going to make the experience buffer less
going to make the experience buffer less
of a pain to like get params right and
of a pain to like get params right and
stuff like that's on the that's the very
stuff like that's on the that's the very
next thing on the to-do list. So yeah.
next thing on the to-do list. So yeah.
So I know you So yeah, I know you have
So I know you So yeah, I know you have
to go but really quick. It's pretty much
to go but really quick. It's pretty much
ready to be merged. Obviously, I need to
ready to be merged. Obviously, I need to
touch up a few things, but uh I won't be
touch up a few things, but uh I won't be
able to do a test run until you fix
able to do a test run until you fix
these things. I'm guessing we should I
these things. I'm guessing we should I
should wait for that and then do a test
should wait for that and then do a test
run and then get emerged. Fix what
run and then get emerged. Fix what
things?
things?
Uh well, just until you make it so I can
Uh well, just until you make it so I can
actually train on it. Should I wait
actually train on it. Should I wait
until that like I can just do a test
until that like I can just do a test
run? Huh? All I changed was your config
run? Huh? All I changed was your config
here, didn't I? Oh, never mind. Yeah,
here, didn't I? Oh, never mind. Yeah,
you're right. Yeah. So, just put that as
you're right. Yeah. So, just put that as
a config for now and then we'll reweep
a config for now and then we'll reweep
it once that's unbroken. Yeah. Cool.
it once that's unbroken. Yeah. Cool.
Sounds good. Yeah. I'll just as long as
Sounds good. Yeah. I'll just as long as
it learns something, we're good, right,
it learns something, we're good, right,
with these hypers. Cool. All right. All
with these hypers. Cool. All right. All
right. Yeah. Thanks, man. See y.
right. Yeah. Thanks, man. See y.
Okay. Thanks, folks, for tuning in. I
Okay. Thanks, folks, for tuning in. I
will be back this afternoon uh after
will be back this afternoon uh after
meeting most
meeting most
likely. All my stuff is at
likely. All my stuff is at
puffer.ai. You can help us out for free
puffer.ai. You can help us out for free
by starring the repo on GitHub. really
by starring the repo on GitHub. really
helps a lot. And you can join Discord to
helps a lot. And you can join Discord to
get involved in dev. It's all open
get involved in dev. It's all open
source and most of the people who came
source and most of the people who came
in here came in with zero RL experience
in here came in with zero RL experience
and sometimes zero AI experience. Just
and sometimes zero AI experience. Just
need to be able to
need to be able to
program.
program.
Um I will be I'm usually live most days.
Um I will be I'm usually live most days.
I've got some stuff to do to move to the
I've got some stuff to do to move to the
new facility, but then there will be
new facility, but then there will be
full-time streaming setup there with all
full-time streaming setup there with all
the dev for the foreseeable future.
the dev for the foreseeable future.
yourself. Thanks and see you
