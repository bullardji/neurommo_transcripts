Kind: captions
Language: en
One second.
Cool.
Um, so this didn't quite
Um, so this didn't quite
work, but it's definitely worth
sweeping. Let me see if they've linked
sweeping. Let me see if they've linked
me anything else. I think Lucas sent me
me anything else. I think Lucas sent me
a
PR. Keep
PR. Keep
this. I can see myself live. Hey, how's
this. I can see myself live. Hey, how's
it going, man?
This did not
This did not
work. This is no
bueno. Lucas sent me a PR, I think. So,
bueno. Lucas sent me a PR, I think. So,
I should check that
I should check that
first. I don't know if he sent me a PR
first. I don't know if he sent me a PR
or he just like
You can
You can
boost LR as big as 1.8 and reduce it as
boost LR as big as 1.8 and reduce it as
little as
0.001. You can also have
scheduleuler for L. What is the pre?
scheduleuler for L. What is the pre?
What is LR
preconditioning? Do you you anneal both
preconditioning? Do you you anneal both
or no? Right now I have cosign analing
or no? Right now I have cosign analing
on just the learning
rate but it's not going to do anything
rate but it's not going to do anything
because this is
because this is
like this is literally like 1% into
training. So like scheduling this over
training. So like scheduling this over
training isn't going to do anything
training isn't going to do anything
because this is like 1% of
because this is like 1% of
training. You
see like you got to zoom out on this
thing. So this is the actual graph,
thing. So this is the actual graph,
right?
is
big. This axis is in tens of billions.
big. This axis is in tens of billions.
So this is 40 billion steps.
And um if you want to see like how far
And um if you want to see like how far
off this end is from
off this end is from
solved, this goes to 3.5. The max is I
solved, this goes to 3.5. The max is I
believe 20 or
25. Where's this thing Evan sent me?
You sent me
a GitHub is eding at your service.
a GitHub is eding at your service.
That's funny as hell.
one is the learning
one is the learning
rate of the neural net and the one is
rate of the neural net and the one is
the learning rate of the sub problem to
the learning rate of the sub problem to
learn the
preconditioner. Okay, so I can sweep
preconditioner. Okay, so I can sweep
those BRB.
those BRB.
Yeah, I can sweep
those. The sweeps take a good while, you
those. The sweeps take a good while, you
know.
move the
move the
Discord. Um, I can see it over here on
Discord. Um, I can see it over here on
the side.
Heavy
bulron. Uh, where is his optimizer that
bulron. Uh, where is his optimizer that
he
he
added? Oh, hey Evan. Evan's here.
Yeah, this is the same
Yeah, this is the same
Evan. I don't know if he's watching
Evan. I don't know if he's watching
stream or
not. Where is this
thing? So this is just
thing? So this is just
like loss go
like loss go
here.
Closure. Oh, these this is gross. I
Closure. Oh, these this is gross. I
freaking hate closures, but whatever. If
freaking hate closures, but whatever. If
it works, it works. Freaking jacks in my
it works, it works. Freaking jacks in my
pie
torch. I thought it was the was it 4H
torch. I thought it was the was it 4H
Newton PSGD. this
Newton PSGD. this
thing. They're blind without
thing. They're blind without
testing and give a list of
hypers to give it a
hypers to give it a
closure. Not enough values to unpack.
Not enough values to unpack. Expected
Not enough values to unpack. Expected
two got
zero. Loss equals
Heavy ball
Heavy ball
utils handle
closure. Uh, don't
closure. Uh, don't
work. What's this thing supposed to
work. What's this thing supposed to
return?
Send one thing real
whatever closure return should return
whatever closure return should return
from optimizer step ideally.
from optimizer step ideally.
So
So
loss
losses. But that's what you did, isn't
it?
Zero. Oh, but you can't do this.
Let's just send screenshots of code
Let's just send screenshots of code
because that's a smart thing to do.
It's breaking in step,
It's breaking in step,
right? A step
closure. It should be like this, right?
Oh, is that not the
Oh, is that not the
bug? Handle
bug? Handle
closure. Loss equals handle
closure. I don't understand how this
closure. I don't understand how this
thing freaking works.
I don't know why that doesn't
I don't know why that doesn't
uh I do something stupid. I
don't No, this gets
don't No, this gets
policy.parameters, right? For each cache
policy.parameters, right? For each cache
Newton PSG.
return
return
step. When you hit step, it should pass
step. When you hit step, it should pass
something
something
back. Wait, when you hit
back. Wait, when you hit
step, it should pass something how it's
step, it should pass something how it's
back.
Wait, is it? I've never actually used
Wait, is it? I've never actually used
eye torch that
way. Oh, wait. Hang on. I do something a
way. Oh, wait. Hang on. I do something a
thing.
thing.
loss equals
model. Wait, but then the closure
model. Wait, but then the closure
doesn't do anything,
doesn't do anything,
right? So why does it need a function
right? So why does it need a function
like that?
What? Oh, wait. Evan gave me this like
What? Oh, wait. Evan gave me this like
loss and batch losses
thing. So, he has return loss
But wait, what's that
But wait, what's that
mean?
mean?
Step
closure loss equals
closure. So you can just pass the damn
closure. So you can just pass the damn
loss in, right?
They just have enable
They just have enable
grad on this freaking
thing. Confused
then. Why do you have this entire thing
then. Why do you have this entire thing
in closure?
And then I'm confused because actually
And then I'm confused because actually
this just returns loss, not batch
this just returns loss, not batch
losses. So that's probably the
losses. So that's probably the
bug. Here's another way to do it. Let's
bug. Here's another way to do it. Let's
say you linked me
This is just how PyTorch works via
This is just how PyTorch works via
closure. But this one needs a closure,
closure. But this one needs a closure,
right? This one didn't work without
right? This one didn't work without
it.
it.
[Music]
[Music]
So
cron, this is for
Newton. You can't just do this, right?
I don't know if you're still watching
I don't know if you're still watching
stream.
loss. Create graph equal
true.
true.
Wait, the readme is not updated. Create
Wait, the readme is not updated. Create
graph. So, it doesn't need a
closure. This is default stuff.
So, do
So, do
I just return loss?
That doesn't work
either. Um
Pie torch works here. This whole This
Pie torch works here. This whole This
isn't a
closure. [ __ ]
I don't know why it would even need one
I don't know why it would even need one
anyways, right? Why does it need to
anyways, right? Why does it need to
compute the loss in that? Is there like
compute the loss in that? Is there like
some decorator it has to throw on
everything? You can do it multiple
everything? You can do it multiple
ways. Okay.
Oh, hang on. I got confused. Let's see.
Oh, hang on. I got confused. Let's see.
Heavy. So, yeah, it's the it's this
Heavy. So, yeah, it's the it's this
implementation. So, this is not even
implementation. So, this is not even
Okay, I see what you mean when it's a
Okay, I see what you mean when it's a
heavy ball thing. Okay,
heavy ball thing. Okay,
cool. HVs default way is via
closure. But I don't understand the
closure. But I don't understand the
signature that this thing
signature that this thing
has because
It has this thing. Where is this? Maybe
It has this thing. Where is this? Maybe
I just go find the
I just go find the
source. I'm just getting tired here.
source. I'm just getting tired here.
850. Is it 869 or
850. Is it 869 or
something? Long ass
thing. Yeah. So, it's closure is
thing. Yeah. So, it's closure is
none. Requires a closure.
none. Requires a closure.
loss equal
closure. Well, it's failing on this
closure. Well, it's failing on this
param groups thing. Is this not how you
param groups thing. Is this not how you
initialize this
initialize this
optimizer? You have to give this
optimizer? You have to give this
thing
thing
params
params
defaults just takes params, right?
Well, looks like the loss definitely has
Well, looks like the loss definitely has
to
to
be
closure. Oh, it just returns the loss.
closure. Oh, it just returns the loss.
And what the hell does it need the
And what the hell does it need the
closure for? What the [ __ ]
Modify closure of closure.
So all of this just to add one stupid
keyword. Why do optimizers have this
keyword. Why do optimizers have this
stupid ass
API? Second
API? Second
order LBFG. We'll use
order LBFG. We'll use
closure. Oh, because it runs the stupid
closure. Oh, because it runs the stupid
thing multiple times, doesn't
thing multiple times, doesn't
it? But this doesn't run that multiple
it? But this doesn't run that multiple
times, right? This doesn't run them loss
times, right? This doesn't run them loss
multiple times. I'm hoping because
multiple times. I'm hoping because
otherwise that's slow as hell and
otherwise that's slow as hell and
there's no point in
it. This doesn't run the loss multiple
it. This doesn't run the loss multiple
times, does it?
How is it not that slow if it has to run
How is it not that slow if it has to run
the It has to run the whole model for it
the It has to run the whole model for it
pass again, doesn't
it? Two back
props. So it needs a closure then for
sure. So why does it not work?
860. You're going to tell me this and
860. You're going to tell me this and
then it's going to be like five steps
then it's going to be like five steps
per second.
So for some there's something wrong with
So for some there's something wrong with
I think the optimizer because this is
I think the optimizer because this is
it's failing on this thing which is just
it's failing on this thing which is just
like getting params and grabs from self
like getting params and grabs from self
param
param
groups. So, I don't know how that
happens, All right.
I'm now realizing this is a
double split PNG in
double split PNG in
group self
group self
parames. Lucas had examples
parames. Lucas had examples
somewhere. No, I cannot find. Yeah, I
somewhere. No, I cannot find. Yeah, I
didn't see Newton in
here. Oh, the closure is supposed to do
here. Oh, the closure is supposed to do
the backward pass
here. It's not just supposed to compute
here. It's not just supposed to compute
the loss. supposed to do the backward
the loss. supposed to do the backward
pass. Hang
on. Yeah. So, this has been not correct.
on. Yeah. So, this has been not correct.
I think it
Well, this is just deleted now. There's
Well, this is just deleted now. There's
no
backward zero grad and
backward zero grad and
then where does he do this? Is
it
optimizerstep and then zero brad?
So I think this just has to have
So I think this just has to have
backward in
backward in
it. That's probably what it is.
Does this thing not works with freaking
um so now I don't get cudn
Then I say different.
Name space does not support item
Name space does not support item
assignment.
Uh, he made
this the hell. Oh,
Oh, the [ __ ] Yeah, he messed with this.
I'm curious as to how he even did
this. Shouldn't have had to touch my
this. Shouldn't have had to touch my
losses.
How the [ __ ] is this even broken? I'm
How the [ __ ] is this even broken? I'm
like
confused. Oh, wait. Hang
confused. Oh, wait. Hang
on. Compute
losses losses.
Oh, this might just be how freaking
Oh, this might just be how freaking
Python works. If you call this losses,
Python works. If you call this losses,
do I have to call this batch losses?
What the [ __ ] is this?
What the [ __ ] is this?
Like I got to see how I had this
Like I got to see how I had this
originally. This is just like a
originally. This is just like a
[Music]
[Music]
random
Oh. Break this
badly. Oh, yeah. You tried to get clever
badly. Oh, yeah. You tried to get clever
and you broke. Yeah, there's
and you broke. Yeah, there's
one. Lucas codes in a very particular
one. Lucas codes in a very particular
way. Yeah, it's clearly functional
way. Yeah, it's clearly functional
inspired. Though to be fair, the closure
inspired. Though to be fair, the closure
API is from
PyTorch. I have literally no time or
PyTorch. I have literally no time or
patience for people who are like either
patience for people who are like either
very dogmatically functional or very
very dogmatically functional or very
dogmatically object-oriented. They're
dogmatically object-oriented. They're
both just a [ __ ] waste of time.
both just a [ __ ] waste of time.
Though from Lucas's code, it wasn't that
Though from Lucas's code, it wasn't that
bad. This isn't Lucas's code. This is
bad. This isn't Lucas's code. This is
just a PR that didn't understand that
just a PR that didn't understand that
you can't [ __ ] do this because uh
you can't [ __ ] do this because uh
this is not a
this is not a
dictionary. It's
fine. We'll do this for now.
closure is used twice for two different
closure is used twice for two different
gradients. Okay, so this runs now.
If you would please tell me why this
If you would please tell me why this
runs 30,000 steps per second, that would
runs 30,000 steps per second, that would
be great.
Yeah, that's way too slow.
as if I'm doing anything dumb, but I
as if I'm doing anything dumb, but I
don't think
so. What should it be at? At least 10
so. What should it be at? At least 10
times this fast.
times this fast.
More like 400, 500,000.
So,
like what do I do with
this? It shouldn't be that much lower,
this? It shouldn't be that much lower,
right?
Oh, it's the stupid QDNN probably on the
Oh, it's the stupid QDNN probably on the
LSTM cuz this didn't work without QDN
LSTM cuz this didn't work without QDN
like
right. It's probably the LSTM being
right. It's probably the LSTM being
stupid slow.
I mean, not working with
I mean, not working with
LSTMs is like kind of [ __ ]
I can run it without the LSTM, but then
I can run it without the LSTM, but then
it's not going to do anything
it's not going to do anything
anyways. I can do that just to
anyways. I can do that just to
check it works in JS. Yeah, but then
check it works in JS. Yeah, but then
you're writing JS and then you get put
you're writing JS and then you get put
on suis. No, you get it put on
on suis. No, you get it put on
demonetized watch. Yeah, there you go.
demonetized watch. Yeah, there you go.
[ __ ] YouTube.
[ __ ] YouTube. I
swear and
Twitch. I actually don't know if you can
Twitch. I actually don't know if you can
get uh if it even you can even do that
get uh if it even you can even do that
on X. I think you still get deboosted.
on X. I think you still get deboosted.
Dell, are you on Twitch? Yeah, I'm on
Dell, are you on Twitch? Yeah, I'm on
here, Twitch, and YouTube.
Is it L? This is the Newton whatever you
Is it L? This is the Newton whatever you
linked
me. The [ __ ] is
this? Oh, maybe I just genuinely didn't
this? Oh, maybe I just genuinely didn't
test it with that. Huh?
I don't know what the [ __ ] is wrong with
I don't know what the [ __ ] is wrong with
this.
Why is it still doing
Why is it still doing
this? What?
Ah, yes. It's a stupid thing.
575k. I was at 575k per
still. I'm so confused.
Well, there's
Well, there's
no no LSTM, so I guess it does have this
no no LSTM, so I guess it does have this
many
many
params. Oh, it's going to be that big
params. Oh, it's going to be that big
ass projection now. Okay, but this is
ass projection now. Okay, but this is
smaller than before. And it's still so
smaller than before. And it's still so
with no LSTM, this thing is still like
with no LSTM, this thing is still like
at least 2x too
at least 2x too
slow, probably more.
and it just doesn't work with an LSTM.
Who the hell came up with the idea of a
Who the hell came up with the idea of a
closure anyways? It's like god awful
closure anyways? It's like god awful
programming. Freaking hurts my soul to
programming. Freaking hurts my soul to
have this even in the messy dev
branch. So,
branch. So,
um, presumably this means I just can't I
um, presumably this means I just can't I
can't use this, right? if it just
can't use this, right? if it just
doesn't work with
LSTMs. Just to check, this is cron
LSTMs. Just to check, this is cron
Newton.
Newton.
Yeah. Newton P
Yeah. Newton P
SGD. So, it actually did get faster. It
SGD. So, it actually did get faster. It
looks like it's is getting faster, but
looks like it's is getting faster, but
like it doesn't work with
uh doesn't work with LSTMs.
uh doesn't work with LSTMs.
because you have to disable
KUDNN. Yeah. So, it did get it got back
KUDNN. Yeah. So, it did get it got back
to like reasonable speed,
to like reasonable speed,
but I mean you have to disable KUDNN.
I mean, is there anything else I should
I mean, is there anything else I should
try? cuz I
like masking still bugged or did you fix
like masking still bugged or did you fix
that? Um, I don't know.
Spencer, we'll give you a finite
Spencer, we'll give you a finite
difference. So yeah, let's try that
difference. So yeah, let's try that
before I go and like revert this stuff.
before I go and like revert this stuff.
And I guess I'll it'll be up to me to
And I guess I'll it'll be up to me to
figure out a way to make this not
figure out a way to make this not
terribly awful. Like what the hell is
terribly awful. Like what the hell is
this?
It's one-sided. Whatever you think I
It's one-sided. Whatever you think I
should try on
this. I'm going to go back to the LSTM
this. I'm going to go back to the LSTM
version at least.
Build it into happy
ball. There's
your what?
Can you add
Mars? Well, this doesn't even run like
Mars? Well, this doesn't even run like
this runs at 30k steps per second,
right? Maybe it gets faster eventually.
Yeah, I know. There's um what on Newton
Yeah, I know. There's um what on Newton
or do you want me to run with like uh or
or do you want me to run with like uh or
do you want me to run just
cro where you want me to run?
Okay. Want me to just run this on mobile
Okay. Want me to just run this on mobile
since that'll be
quick?
Let's make sure this runs and I'll add
Let's make sure this runs and I'll add
Neptune. Once it recompiles the kernels
Neptune. Once it recompiles the kernels
and all
that big delay in chat, I can read it
that big delay in chat, I can read it
from the Discord just as
from the Discord just as
well. Yeah, X has a big
delay. I mean, if you want, you can jump
delay. I mean, if you want, you can jump
in the puffer discord voice as well.
in the puffer discord voice as well.
That also works.
[Music]
Twitch is
faster. I mean, you can just jump in the
faster. I mean, you can just jump in the
puff or voice unless you don't want to
puff or voice unless you don't want to
be in voice on stream.
It's a lot
easier. We pay for this freaking server
easier. We pay for this freaking server
for a reason. It's got like the max
for a reason. It's got like the max
quality audio and everything.
Hello. Hey, how's it going? It's
Hello. Hey, how's it going? It's
probably a hell of a lot easier, right?
probably a hell of a lot easier, right?
It's so much easier. This is kind of
It's so much easier. This is kind of
dumb. Yeah. Yeah. Okay. So,
dumb. Yeah. Yeah. Okay. So,
uh I'll run this on
uh I'll run this on
um Puffer Mobile real quick cuz neural
um Puffer Mobile real quick cuz neural
MMO 3 usually takes a little bit.
MMO 3 usually takes a little bit.
Yeah. So, I think Mars. Okay. So, a
Yeah. So, I think Mars. Okay. So, a
variance. Yeah. It's a variance
variance. Yeah. It's a variance
reduction technique.
reduction technique.
All that it does is it says I'm going to
All that it does is it says I'm going to
take my current gradients and subtract
take my current gradients and subtract
my previous gradient information from
my previous gradient information from
it. And so in some
it. And so in some
ways it um acts as like a semiann vector
ways it um acts as like a semiann vector
product, right? It'll do a finite
product, right? It'll do a finite
difference. I'm going to take my current
difference. I'm going to take my current
gradients
gradients
and I'm going to subtract my
and I'm going to subtract my
previous gradient from it.
previous gradient from it.
Gradient not gradient variance.
Gradient not gradient variance.
No, you're you're you're just just your
No, you're you're you're just just your
previous buffer. So this is this is
previous buffer. So this is this is
gradient scale invariance, right? Yes.
gradient scale invariance, right? Yes.
Not gradients. Not gradient variance
Not gradients. Not gradient variance
invariance.
invariance.
No. No. Okay. Is there anything that
No. No. Okay. Is there anything that
does that?
does that?
Gradient variance and variance. What
Gradient variance and variance. What
does that mean? Let me think about. So
does that mean? Let me think about. So
if you're training on if you're training
if you're training on if you're training
on a language model, you kind of have
on a language model, you kind of have
like the same problem everywhere.
like the same problem everywhere.
So you can tune your params and it'll
So you can tune your params and it'll
just work. Um, in RL, different
just work. Um, in RL, different
environments are going to give you
environments are going to give you
different gradients and some of them are
different gradients and some of them are
going to have very high variance and
going to have very high variance and
some of them aren't. Yeah. And that
some of them aren't. Yeah. And that
screws up your hypers. I see. I see. So,
screws up your hypers. I see. I see. So,
and you just have to adjust the learning
and you just have to adjust the learning
rate is all it is. You just have to
rate is all it is. You just have to
adjust the learning rate. But, uh, it
adjust the learning rate. But, uh, it
would be really cool if that could have
would be really cool if that could have
been done like if that could be done
been done like if that could be done
automatically.
So with PSGD okay so I I will I will not
So with PSGD okay so I I will I will not
only talk about the things I do but what
only talk about the things I do but what
other people do as well. So there's a
other people do as well. So there's a
work called the
work called the
adapt by Defasio I'll link you right
adapt by Defasio I'll link you right
now.
Um, and all
Um, and all
right. Yeah. So, there's this work the
right. Yeah. So, there's this work the
adaption and I've built this in to a
adaption and I've built this in to a
variant of PSG. Where' you link it? Uh,
variant of PSG. Where' you link it? Uh,
yeah. Uh, is it okay if I drop it in the
yeah. Uh, is it okay if I drop it in the
puffer chat? Whatever.
puffer chat? Whatever.
Um, so there is a newer version called
Um, so there is a newer version called
Prodigy.
Prodigy.
Um, but I have actually found deaption.
Um, but I have actually found deaption.
Okay, Prodigy is a little bit more
Okay, Prodigy is a little bit more
aggressive than D adaption on its
aggressive than D adaption on its
learning rate. Um, I prefer deaption to
learning rate. Um, I prefer deaption to
prody, but you could consider both. So,
prody, but you could consider both. So,
I can try this. I will say that I tried
I can try this. I will say that I tried
a couple of learning like the schedule
a couple of learning like the schedule
free or whatever uh
free or whatever uh
in heavy ball and they just didn't work.
in heavy ball and they just didn't work.
Yeah. Okay. So, schedule free is
Yeah. Okay. So, schedule free is
something different. Schedule free will
something different. Schedule free will
absolutely not work for reinforcement
absolutely not work for reinforcement
learning. Um, and it also will not work
learning. Um, and it also will not work
for a lot of smaller tasks. What
for a lot of smaller tasks. What
schedule free is doing is it's
schedule free is doing is it's
um, so it'll work great for language
um, so it'll work great for language
models, right? Because it has this
models, right? Because it has this
basically thing where your problem is
basically thing where your problem is
basically the same over your entire
basically the same over your entire
setting. And that's not true for RL at
setting. And that's not true for RL at
all. Yeah, we know language model people
all. Yeah, we know language model people
they they like to solve easy problems.
they they like to solve easy problems.
Yes. No. No. Exactly right. Um and
Yes. No. No. Exactly right. Um and
so with schedule free what it's doing is
so with schedule free what it's doing is
it's literally doing an interpolation on
it's literally doing an interpolation on
the weights through time as well, right?
the weights through time as well, right?
And so you're finding some like
And so you're finding some like
interpolation of a bunch of solutions,
interpolation of a bunch of solutions,
right? And this absolutely will not work
right? And this absolutely will not work
in many sense, right?
in many sense, right?
Um but d adaption all it does is it just
Um but d adaption all it does is it just
does some upper or lower bound on some
does some upper or lower bound on some
stuff to basically find a good
stuff to basically find a good
initialization for your learning rate.
initialization for your learning rate.
Um is this like a thing that you would
Um is this like a thing that you would
that you could because it says for atom
that you could because it says for atom
and stuff. Could you throw it on muan or
and stuff. Could you throw it on muan or
something? Yeah, you could on PSPD or
something? Yeah, you could on PSPD or
something. Yeah. Yeah. I I yeah I can if
something. Yeah. Yeah. I I yeah I can if
you really want I can um well I can wrap
you really want I can um well I can wrap
something with with uh muan I will
something with with uh muan I will
instead of what I really want look at it
instead of what I really want look at it
this way I have a lot of experiments
this way I have a lot of experiments
that I can run very very quickly on
that I can run very very quickly on
relatively interesting problems so that
relatively interesting problems so that
is useful for testing stuff cuz like I
is useful for testing stuff cuz like I
mean this is a way harder optim
mean this is a way harder optim
benchmark than something like an LLM
benchmark than something like an LLM
right no I agree so I like I can run
right no I agree so I like I can run
this here it's running on neural MMO.
this here it's running on neural MMO.
Now, we'll see if it does anything. And
Now, we'll see if it does anything. And
by the way, here's your Mars result just
by the way, here's your Mars result just
immediately. Um, yeah. So, this is very
immediately. Um, yeah. So, this is very
slightly
slightly
better potentially or comparable
better potentially or comparable
probably probably in the noise to be
probably probably in the noise to be
honest. But then the thing, the reason I
honest. But then the thing, the reason I
didn't use this is because I saw this
didn't use this is because I saw this
and uh it has a little bit of a does it
and uh it has a little bit of a does it
have a little bit of a perf penalty or
have a little bit of a perf penalty or
am I making that up?
am I making that up?
Does it have a what? Sorry. Uh, it
Does it have a what? Sorry. Uh, it
doesn't really have a perf penalty. It's
doesn't really have a perf penalty. It's
kind of the same. No, it doesn't. It's
kind of the same. No, it doesn't. It's
It's the memory penalty.
It's the memory penalty.
Oh, yeah. We don't care about that. We
Oh, yeah. We don't care about that. We
have infinite memory. Yeah, exactly. Um,
have infinite memory. Yeah, exactly. Um,
yeah, we have infinite memory until we
yeah, we have infinite memory until we
start have to start offloading stuff
start have to start offloading stuff
because we do have we keep batch size.
because we do have we keep batch size.
We will keep like batch size 500,000 in
We will keep like batch size 500,000 in
memory.
memory.
Okay, so here's something kind of
Okay, so here's something kind of
interesting with PSGD. You can more or
interesting with PSGD. You can more or
less set your batch size to one and it
less set your batch size to one and it
will just do fine. Um,
will just do fine. Um,
you can set your batch size to one and
you can set your batch size to one and
it will do fine.
it will do fine.
Yes.
Uh, that cannot possibly work because
Uh, that cannot possibly work because
LSTM.
Okay. On LSTM, I don't know.
Okay. On LSTM, I don't know.
Maybe one trajectory segment. Yeah,
Maybe one trajectory segment. Yeah,
maybe one trajectory segment. Yeah.
maybe one trajectory segment. Yeah.
Yeah.
Yeah.
On. Yeah. But for like images for
On. Yeah. But for like images for
example, you can you can set the you can
example, you can you can set the you can
set to one and it'll do fine. The other
set to one and it'll do fine. The other
thing you can do here is you can Okay.
thing you can do here is you can Okay.
So I I don't know what uh
So I I don't know what uh
hyperparameters you play around with,
hyperparameters you play around with,
but just to kind of explain like PSG is
but just to kind of explain like PSG is
something very different. it's an
something very different. it's an
optimization framework um in itself. So
optimization framework um in itself. So
it it there is the neural network side
it it there is the neural network side
of things which you're minimizing right
of things which you're minimizing right
your precondition
your precondition
gradient descent but then there's also
gradient descent but then there's also
um your what is it called?
um your what is it called?
There's also your
There's also your
uh premature fitting. So you're running
uh premature fitting. So you're running
a convex optimization problem to learn
a convex optimization problem to learn
your
your
preconditioner and then you're also
preconditioner and then you're also
using that preconditioner to to
using that preconditioner to to
precondition your gradients to minimize
precondition your gradients to minimize
the loss of your normal. So if your
the loss of your normal. So if your
preconditioner is too high um or too low
preconditioner is too high um or too low
uh the learning of that is too high or
uh the learning of that is too high or
too low it can also mess you up. Yeah.
too low it can also mess you up. Yeah.
So I can run a sweep on that. That's no
So I can run a sweep on that. That's no
big deal. Yeah. Yeah. which is which is
big deal. Yeah. Yeah. which is which is
why I thought I mean I rewarded we
why I thought I mean I rewarded we
yeah I can probably run I mean neural
yeah I can probably run I mean neural
MMO 3 will train like I don't know
MMO 3 will train like I don't know
billion and a half steps an hour at
billion and a half steps an hour at
least so I can just run like a good
least so I can just run like a good
chunk of uh of experiments even on
chunk of uh of experiments even on
that are the defaults generally not like
that are the defaults generally not like
the precondition defaults not good or
the precondition defaults not good or
no they're They're good. Um, and Evan
no they're They're good. Um, and Evan
has
has
seen Evan has seen them be pretty good.
seen Evan has seen them be pretty good.
This is um Mars, by the way. Nice. No,
This is um Mars, by the way. Nice. No,
but the thing is like this, you see what
but the thing is like this, you see what
happened with the last run is it did way
happened with the last run is it did way
better and then it just like didn't. It
better and then it just like didn't. It
got stuck completely. Yeah. So, you can
got stuck completely. Yeah. So, you can
I mean, what's the LR precon right now?
I mean, what's the LR precon right now?
Probably 0.1. uh whatever default is
Probably 0.1. uh whatever default is
let's just just quickly reduce it by an
let's just just quickly reduce it by an
order magnitude with Mars
order magnitude with Mars
uh yeah with without the other thing is
uh yeah with without the other thing is
yeah what's the
uh was it heavy ball specific yeah I
uh was it heavy ball specific yeah I
that's the thing I either use Zealand's
that's the thing I either use Zealand's
implementation which is also probably my
implementation which is also probably my
implementation We kind of wrote all that
implementation We kind of wrote all that
together. Oh, I'll test I'll test it.
together. Oh, I'll test I'll test it.
Sure. I tried Evan's implementation of
Sure. I tried Evan's implementation of
uh of PSGD and it and then I tried this
uh of PSGD and it and then I tried this
one and Evans got like 350 score on
one and Evans got like 350 score on
breakout and this one solved it with the
breakout and this one solved it with the
defaults.
defaults.
Yeah. So, you can look at Zealand's
Yeah. So, you can look at Zealand's
code. Um
code. Um
it's it has a lot more optimizers than
it's it has a lot more optimizers than
Evans code. Um, but it's also written in
Evans code. Um, but it's also written in
a way that might make you want to pull
a way that might make you want to pull
your hair out. Oh, lovely. Um, is which
your hair out. Oh, lovely. Um, is which
of these params should I change by the
of these params should I change by the
way? Yeah, let me see.
way? Yeah, let me see.
Uh, it should be like Okay. LR weight
Uh, it should be like Okay. LR weight
decay preconditioner update
decay preconditioner update
probability. No, not that.
Um, okay. Precon LR. The last one. So
Um, okay. Precon LR. The last one. So
there's also this precondinet scale. So
there's also this precondinet scale. So
just reduce this to 0. Oh, it's 0.1. No,
just reduce this to 0. Oh, it's 0.1. No,
no, no. That that Yeah. Yeah. Uh
no, no. That that Yeah. Yeah. Uh
yeah. Z.
There's also that scale
and that that I think it was just added
and that that I think it was just added
into
into
um I don't know if he's pushed it, but
um I don't know if he's pushed it, but
he's seeing like much much better
he's seeing like much much better
results with the way that Zealand
results with the way that Zealand
automatically sets it. Um well, we can
automatically sets it. Um well, we can
definitely try that. So, here's the cool
definitely try that. So, here's the cool
thing with Puffer, right? Is a as a
thing with Puffer, right? Is a as a
benchmark for this type of stuff. Um
benchmark for this type of stuff. Um
I and I'm sure you've seen this, but
I and I'm sure you've seen this, but
like we have all these apps and they're
like we have all these apps and they're
fast and we actually have more in dev,
fast and we actually have more in dev,
some of which are pretty cool as well.
some of which are pretty cool as well.
So like if it just works on all these,
So like if it just works on all these,
it's probably pretty good.
it's probably pretty good.
And like most of these will train in a
And like most of these will train in a
minute.
minute.
Yeah. Some of these, you know, maybe
Yeah. Some of these, you know, maybe
some it's like five minutes, but the
some it's like five minutes, but the
only ones that are like really hard and
only ones that are like really hard and
take quite a bit, it's like neural MMO 3
take quite a bit, it's like neural MMO 3
is pretty hard. Other than that, you can
is pretty hard. Other than that, you can
like
like
kind of just go fast.
kind of just go fast.
Um, what are your thoughts on all these
Um, what are your thoughts on all these
I mean new again I know we're worried
I mean new again I know we're worried
and pissed about me saying this not
and pissed about me saying this not
pissed but um what are your thoughts on
pissed but um what are your thoughts on
like Jerio and all that stuff do you
like Jerio and all that stuff do you
think of language model specific? Yeah,
think of language model specific? Yeah,
for sure. Right. But um do you think
for sure. Right. But um do you think
that you would ever consider putting
that you would ever consider putting
like a language model into Puffer and
like a language model into Puffer and
just play around with the different RL
just play around with the different RL
algorithms whether they're specific or
algorithms whether they're specific or
not to LMS?
not to LMS?
I mean, I don't see the point. It's like
I mean, I don't see the point. It's like
an easy problem. If they can't figure
an easy problem. If they can't figure
that out, I don't know what to say
that out, I don't know what to say
because it's like you have a tenth of
because it's like you have a tenth of
the number of problems. The only thing
the number of problems. The only thing
that's hard, like really the only thing
that's hard, like really the only thing
that's hard about language models is the
that's hard about language models is the
uh getting like very performant
uh getting like very performant
distributed training.
Like there's this whole extra stack that
Like there's this whole extra stack that
you have to deal with to make stuff
you have to deal with to make stuff
fast. You get like multi-million step
fast. You get like multi-million step
per second training. None of the stuff I
per second training. None of the stuff I
do in puffer matters if your model
do in puffer matters if your model
trains like 0.5 steps per second, right?
trains like 0.5 steps per second, right?
Yeah, it's fair. Like GRPO is a one-step
Yeah, it's fair. Like GRPO is a one-step
problem. like it's being used for
problem. like it's being used for
one-step problems. And now like you know
one-step problems. And now like you know
there are these papers coming out for a
there are these papers coming out for a
multi-turn learning. So they're like a
multi-turn learning. So they're like a
few steps in the future which is
few steps in the future which is
actually good. Like that's how you would
actually good. Like that's how you would
use RL on a language model and I think
use RL on a language model and I think
that there's actually a good chance that
that there's actually a good chance that
that does really great stuff cuz RL is
that does really great stuff cuz RL is
really powerful. Um but that's not
really powerful. Um but that's not
really my concern because people are
really my concern because people are
going to do that stuff with or without
going to do that stuff with or without
me, right? My concern is mainly like how
me, right? My concern is mainly like how
do we make sure that all the research
do we make sure that all the research
that really matters for RL and RL
that really matters for RL and RL
methods to be good that gets done
methods to be good that gets done
because nobody else is doing that.
because nobody else is doing that.
This is how I view stuff, right? Like
This is how I view stuff, right? Like
like multi you're saying multi-step on
like multi you're saying multi-step on
LMS. Is that what you're saying? There
LMS. Is that what you're saying? There
are a couple people doing that now. But
are a couple people doing that now. But
like I mean it's taken so freaking long.
like I mean it's taken so freaking long.
Like I don't even know how it takes you
Like I don't even know how it takes you
this long, right? It's been like three
this long, right? It's been like three
years or whatever. two years maybe. I
years or whatever. two years maybe. I
mean, still you have how many billion
mean, still you have how many billion
dollars in this and you and like
dollars in this and you and like
everybody goes nuts over one stop like
everybody goes nuts over one stop like
one step RHF followed by one step RL.
one step RHF followed by one step RL.
Like come on.
Like come on.
No, I I agree. So, I'm not pissed about
No, I I agree. So, I'm not pissed about
it at all. It's like, yeah, good. You
it at all. It's like, yeah, good. You
should be doing this stuff. If anything,
should be doing this stuff. If anything,
it's kind of embarrassing that it took
it's kind of embarrassing that it took
that long. But, you know, I get it. The
that long. But, you know, I get it. The
reason it takes long is because your
reason it takes long is because your
experiments are so bloody massive that
experiments are so bloody massive that
you can't run very many of them. So what
you can't run very many of them. So what
do I do instead of running like you know
do I do instead of running like you know
one experiment which is all I'd be able
one experiment which is all I'd be able
to do with the hardware I have right
to do with the hardware I have right
I've made puffer so now I can run I've
I've made puffer so now I can run I've
run 25,000 RL experiments uh since the
run 25,000 RL experiments uh since the
start of the year
start of the year
right so my concern is really more how
right so my concern is really more how
do we develop RL as a core tech right
do we develop RL as a core tech right
how do we make sure core RL makes
how do we make sure core RL makes
progress and I cover everything but
progress and I cover everything but
language models for the time being
language models for the time being
and there's So many like there are so
and there's So many like there are so
many problems in industry where language
many problems in industry where language
models just don't make any sense. So I
models just don't make any sense. So I
mean they're too big. The priors that
mean they're too big. The priors that
they give you are not useful because the
they give you are not useful because the
problems are really unintuitive. You
problems are really unintuitive. You
just need to train from scratch and it
just need to train from scratch and it
needs to be good and it needs to be
needs to be good and it needs to be
fast. That's what I'm looking for. Okay.
fast. That's what I'm looking for. Okay.
By the way, that matches the uh I don't
By the way, that matches the uh I don't
see any noticeable difference in the
see any noticeable difference in the
curve yet. We'll see. But it kind of
curve yet. We'll see. But it kind of
looks flat.
looks flat.
Interesting.
Uh I guess turn off Mars maybe and then
Uh I guess turn off Mars maybe and then
reduce it even more.
That's what
That's what
that is that if so if that
like if that doesn't wait was that cron
like if that doesn't wait was that cron
or was that cash? No, that must have
or was that cash? No, that must have
been cron, right? Craw. I didn't see any
been cron, right? Craw. I didn't see any
difference with cash.
difference with cash.
But it's not new, right?
What I ran this with before was Mars
What I ran this with before was Mars
true pre-rown 0.01. Okay. Okay. But you
true pre-rown 0.01. Okay. Okay. But you
you you comped it up the top line, I
you you comped it up the top line, I
guess. Yes.
Commented color scheme included with our
Commented color scheme included with our
Docker container.
But yeah, Neural MMO 3 is like a really
But yeah, Neural MMO 3 is like a really
hard and interesting app. Like you can
hard and interesting app. Like you can
play it right here, right? If you just
play it right here, right? If you just
hit control, you can take over and you
hit control, you can take over and you
can play
can play
it. So I'm taking over now for this guy.
it. So I'm taking over now for this guy.
And
And
like you see that you will see that it
like you see that you will see that it
takes quite a while to like figure out
takes quite a while to like figure out
how to even do anything in this. And
how to even do anything in this. And
like there's so much stuff that just
like there's so much stuff that just
like the delay is very long what you
like the delay is very long what you
have to do. So I have to go kill this
have to do. So I have to go kill this
enemy. I have to equip this tool. I have
enemy. I have to equip this tool. I have
to go find something that I can actually
to go find something that I can actually
harvest like this sword. I can go pick
harvest like this sword. I can go pick
up this
up this
sword. And then I have to like unequip
sword. And then I have to like unequip
this, equip this. And now I can go find
this, equip this. And now I can go find
something like very slightly higher
something like very slightly higher
level and maybe go kill that. Um, and
level and maybe go kill that. Um, and
there's like a market you can trade with
there's like a market you can trade with
other agents. There are there's like
other agents. There are there's like
armor and equipment and weapons and
armor and equipment and weapons and
consumables. And then uh if I let this
consumables. And then uh if I let this
agent just take back over the map looks
agent just take back over the map looks
like this is
like this is
big. There's a lot of stuff that can go
big. There's a lot of stuff that can go
on in here. Really nice benchmark for
on in here. Really nice benchmark for
this stuff. Um, way harder than anything
this stuff. Um, way harder than anything
else.
else.
That's an atom. What does the atom curve
That's an atom. What does the atom curve
look like? Uh, hang on. Let me get rid
look like? Uh, hang on. Let me get rid
of this bot. Go. This is
Muan. So, this is this is Muan. And
Muan. So, this is this is Muan. And
actually, this is not the best metric.
actually, this is not the best metric.
The best metric is this mincom prof.
The best metric is this mincom prof.
This is like how many levels you
This is like how many levels you
get. Um, so this looks really good,
get. Um, so this looks really good,
right? Um but now if I
right? Um but now if I
just do
this. So this is the learning curve.
this. So this is the learning curve.
Very smooth. And then if I go find the
Very smooth. And then if I go find the
wand curve for
wand curve for
you. Where's the womb
you. Where's the womb
curve? Man, I thought I had it. Oh, here
curve? Man, I thought I had it. Oh, here
it is. Nope. Yeah, this is the wombi
it is. Nope. Yeah, this is the wombi
curve. So it gets up to
curve. So it gets up to
about what was that 2.5. This is about
about what was that 2.5. This is about
1500 years worth of games played. 73
1500 years worth of games played. 73
billion steps.
billion steps.
Yeah. There's about 1500 years worth of
Yeah. There's about 1500 years worth of
games played.
Wait, is that like in game? Yeah. Yeah.
Wait, is that like in game? Yeah. Yeah.
If you were to like if you were to play
If you were to like if you were to play
the game in real time, this is 1500
the game in real time, this is 1500
years worth of game.
And then this one here, this
And then this one here, this
is this is about a thousand years worth
is this is about a thousand years worth
of game, little less.
of game, little less.
That's kind of absurd, isn't it? I
That's kind of absurd, isn't it? I
started this run on one GPU less than 24
started this run on one GPU less than 24
hours ago.
Kind of crazy. Yeah. So it's very fast
Kind of crazy. Yeah. So it's very fast
and like you know you can kind of learn
and like you know you can kind of learn
some pretty complex stuff because it's
some pretty complex stuff because it's
like oh what can you learn with a small
like oh what can you learn with a small
model on one GPU? It's like well when
model on one GPU? It's like well when
your x-axis is in tens of billions I
your x-axis is in tens of billions I
mean you can go look at the large scale
mean you can go look at the large scale
industry papers and very very few of
industry papers and very very few of
them have an x-axis in the tens of
them have an x-axis in the tens of
billions right? Yeah.
billions right? Yeah.
So you can kind of do a
So you can kind of do a
lot. Let's see where this graph is.
All right. So, your blue one here, this
All right. So, your blue one here, this
is your blue one.
is your blue one.
Okay, let's see how she does. So, it
Okay, let's see how she does. So, it
starts off way lower. Still better than
starts off way lower. Still better than
Muan, but then it'll see. We'll see how
Muan, but then it'll see. We'll see how
it converges. What's this thing that
it converges. What's this thing that
Evans said like, "Oh, learning fast is
Evans said like, "Oh, learning fast is
bad."
bad."
Yeah. So,
Yeah. So,
that seems like a screw up. No, no, no.
that seems like a screw up. No, no, no.
So there's there's a concept called
So there's there's a concept called
neuroplasticity. Yeah, I'm familiar. But
neuroplasticity. Yeah, I'm familiar. But
it's kind of stupid that that's a thing.
it's kind of stupid that that's a thing.
I'll say
Basically, I can explain what they did.
Basically, I can explain what they did.
They took um a
CNM and
CNM and
they for the first like sci-fi 10,
they for the first like sci-fi 10,
right? They train it for the first like
right? They train it for the first like
five epochs with blur. First 10 epochs
five epochs with blur. First 10 epochs
of blur, first 30 epox, first 50 epochs
of blur, first 30 epox, first 50 epochs
of blur, and then after. Yeah. No, I'm
of blur, and then after. Yeah. No, I'm
familiar with neuroplasticity, right?
familiar with neuroplasticity, right?
I'm familiar with that line of work. Um,
I'm familiar with that line of work. Um,
okay.
okay.
Oh, this doesn't seem like it helps.
Oh, this doesn't seem like it helps.
Like, this goes up and then goes back
Like, this goes up and then goes back
down immediately.
down immediately.
Yeah. I mean, let's see what happens.
Muan was pretty good here. I'm
Muan was pretty good here. I'm
surprised. Um, I mean I like the other
surprised. Um, I mean I like the other
thing is manually tuning hyperparameters
thing is manually tuning hyperparameters
is kind of dumb. Like I probably could
is kind of dumb. Like I probably could
just sweep this. Well, what do I sweep?
just sweep this. Well, what do I sweep?
Do I just sweep the the precondition
Do I just sweep the the precondition
learning rate and the learning rate or
learning rate and the learning rate or
do I sweep other stuff?
Let's see what
else. And we have a lot of fun stuff
else. And we have a lot of fun stuff
that we can play with. Yeah. Yeah,
that we can play with. Yeah. Yeah,
exactly.
exactly.
Unfortunately, these sweeps do take a
Unfortunately, these sweeps do take a
little time because, you know, big run,
little time because, you know, big run,
but yeah, all the other sweeps don't
but yeah, all the other sweeps don't
take much at all.
Okay. Can you also do like binary like
Okay. Can you also do like binary like
true false? Yeah. Right. Can sweep? I
true false? Yeah. Right. Can sweep? I
haven't, but we probably should. Yeah,
haven't, but we probably should. Yeah,
we could. Well, you just want Mars. I
we could. Well, you just want Mars. I
just run two sweeps.
Mind you, don't we don't have all our
Mind you, don't we don't have all our
nice hardware yet. Um 5090 test machine
nice hardware yet. Um 5090 test machine
should be in sometime this
should be in sometime this
week. We order more of
them. Sorry, I think my internet there
them. Sorry, I think my internet there
for a sec.
Wait, can you hear me?
you there? Yeah. Yeah. Sorry, I think my
you there? Yeah. Yeah. Sorry, I think my
internet cut off for a second. Yeah,
internet cut off for a second. Yeah,
you're good. Um I probably would also So
you're good. Um I probably would also So
there's this other setting which says
um Okay. So are you filled with caution?
um Okay. So are you filled with caution?
Not as it relates to optimizers.
Not as it relates to optimizers.
Okay. Yes. Okay. So caution is set to
Okay. Yes. Okay. So caution is set to
false. You can try toggling caution true
false. You can try toggling caution true
for both MUN and the SGDA. It basically
for both MUN and the SGDA. It basically
says if your gradient and your update,
says if your gradient and your update,
right, have sign changes, ignore them.
right, have sign changes, ignore them.
set that set that dimension to zero. Um,
set that set that dimension to zero. Um,
caution is interesting to use. There's
caution is interesting to use. There's
also if you do enable Mars, there's a
also if you do enable Mars, there's a
Mars gamma. There's the momentum.
Mars gamma. There's the momentum.
There's also this option that says
There's also this option that says
momentum into precon gradient. Okay, you
momentum into precon gradient. Okay, you
can set that to true and false. Oh my
can set that to true and false. Oh my
gosh, you guys have added so many
gosh, you guys have added so many
freaking hyperparameters.
freaking hyperparameters.
Yes, sir.
Yes, sir.
Um, so yeah. Is there a comprehensive
Um, so yeah. Is there a comprehensive
is there like a comprehensive thing on
is there like a comprehensive thing on
this versus um versus Muan anywhere?
this versus um versus Muan anywhere?
So okay. So if you use heavy ball, I
So okay. So if you use heavy ball, I
think every single optimizer has all
think every single optimizer has all
these type of parameters.
these type of parameters.
Yeah.
Yeah.
Uh so this is this is Lucas's idea is it
Uh so this is this is Lucas's idea is it
should all compile and it should all be
should all compile and it should all be
so functional that they can all just you
so functional that they can all just you
you could use any mix of optimizers
you could use any mix of optimizers
together.
together.
Um, you're asking for a comprehensive
Um, you're asking for a comprehensive
understanding of Muan versus I'm asking
understanding of Muan versus I'm asking
if they're experimental results, right?
If they're experimental results,
If they're experimental results,
experimental results. So, like the thing
experimental results. So, like the thing
that I went went for for Muan, right, is
that I went went for for Muan, right, is
uh people were trying pretty hard for a
uh people were trying pretty hard for a
bit to get the uh the GPT speedrun
bit to get the uh the GPT speedrun
to get that working, right? So I saw
to get that working, right? So I saw
that Muon was good on there and I said,
that Muon was good on there and I said,
"Oh, okay. Maybe that's useful." And
"Oh, okay. Maybe that's useful." And
then I went and I tried Muan on a bunch
then I went and I tried Muan on a bunch
of MS and it was, you know, it was doing
of MS and it was, you know, it was doing
uh and after I ran a sweep for hyperpan
uh and after I ran a sweep for hyperpan
was doing better than the other ones,
was doing better than the other ones,
right? Than Adam, it was doing uh
right? Than Adam, it was doing uh
solving faster. And then I tried this
solving faster. And then I tried this
thing. I tried
thing. I tried
PSGD cron and it matched muon just with
PSGD cron and it matched muon just with
the defaults on breakout out of the
the defaults on breakout out of the
box and it's did better on mobile
box and it's did better on mobile
versus not fully tuned muon params but
versus not fully tuned muon params but
like good
like good
default did better
default did better
there and uh the only thing it hasn't
there and uh the only thing it hasn't
done well so far is neural MMO it gets
done well so far is neural MMO it gets
stuck so I don't know if there's
stuck so I don't know if there's
something weird with the optimizer that
something weird with the optimizer that
causes that or what but uh Muan doesn't
causes that or what but uh Muan doesn't
get stuck stuck. Muan has a very very
get stuck stuck. Muan has a very very
smooth learning rate uh learning curve.
smooth learning rate uh learning curve.
Yeah. Um so we have like our own
Yeah. Um so we have like our own
internal results and then Stanford also
internal results and then Stanford also
has some results of PSG versus Mulan.
Um but this would definitely like the
Um but this would definitely like the
stuff you're doing is I mean I've been
stuff you're doing is I mean I've been
trying to get PSGD into Puffer for some
trying to get PSGD into Puffer for some
time now and I just haven't had you
time now and I just haven't had you
know. Yeah.
know. Yeah.
um with well here it is. We'll see if it
um with well here it is. We'll see if it
takes off at some point but it's just
takes off at some point but it's just
intersected with Muan.
intersected with Muan.
Yeah. So it we're really reducing the LR
Yeah. So it we're really reducing the LR
of the threeon 01. Um we can also try
of the threeon 01. Um we can also try
reducing the momentum for example. That
reducing the momentum for example. That
might help. Another thing that I have to
might help. Another thing that I have to
check if it's in heavy ball or not that
check if it's in heavy ball or not that
Zealand does differently than Evan.
Zealand does differently than Evan.
Zealand
Zealand
um trusts the gradients. Evan trusts the
um trusts the gradients. Evan trusts the
momentum,
momentum,
right? So that was if if if there was a
right? So that was if if if there was a
um if there was a like a contribution
um if there was a like a contribution
Evan has made it was that he he said hey
Evan has made it was that he he said hey
let's try to trust the momentum term uh
let's try to trust the momentum term uh
which I also had done but but he he kind
which I also had done but but he he kind
of did it he showed that it helps on LMS
of did it he showed that it helps on LMS
a lot um but maybe for RL it it hurts
a lot um but maybe for RL it it hurts
um so instead of passing the the
um so instead of passing the the
momentum term into the preconditioner
momentum term into the preconditioner
preconditioning that you can just
preconditioning that you can just
directly pass the gradients uh which is
directly pass the gradients uh which is
what Z1's code name does. Um
what Z1's code name does. Um
I have to look at that. The thing is
I have to look at that. The thing is
this isn't even an optimized muon,
this isn't even an optimized muon,
right? This is I um I optimized muon on
right? This is I um I optimized muon on
uh breakout and then I just use some of
uh breakout and then I just use some of
those params like the atom betas and
those params like the atom betas and
stuff. I use them here. So I don't even
stuff. I use them here. So I don't even
think this is like an optimized muon for
think this is like an optimized muon for
this. I'm It's a little weird that like
this. I'm It's a little weird that like
qualitatively the behavior is it gets
qualitatively the behavior is it gets
stuck that early.
stuck that early.
Yeah. I don't know why that would
Yeah. I don't know why that would
happen. Um I don't like I don't actually
happen. Um I don't like I don't actually
know the math behind this versus muon.
know the math behind this versus muon.
Something's very different here.
Yeah. So I've reduced the learning rate
Yeah. So I've reduced the learning rate
of the preconditioner by a lot. So it's
of the preconditioner by a lot. So it's
going to take a while too. I basic.
going to take a while too. I basic.
Okay. So now this is producing it by
Okay. So now this is producing it by
like two orders of magnitude. You can
like two orders of magnitude. You can
also increase it by order of magnitude.
also increase it by order of magnitude.
So
So
um but I would I would I might throw
um but I would I would I might throw
cards at it or at it. Okay. Well, I'll
cards at it or at it. Okay. Well, I'll
run I'll just run a sweep then. I'll
run I'll just run a sweep then. I'll
sweep some stuff and I'll see if this
sweep some stuff and I'll see if this
feels like this can do anything cuz I
feels like this can do anything cuz I
also had I did start a a muon
also had I did start a a muon
sweep on. It's a little annoying to get
sweep on. It's a little annoying to get
the thing to actually like consistently
the thing to actually like consistently
run experiments that are like this long,
run experiments that are like this long,
but
so let's zoom
in. Yeah. So like we
in. Yeah. So like we
have there like some results here. This
have there like some results here. This
one looks pretty
one looks pretty
good.
good.
02. This is all this is above um where
02. This is all this is above um where
PSPD got stuck I think
here. So there's definitely sensitivity.
here. So there's definitely sensitivity.
Yeah. Yeah, that's for sure. But it's
Yeah. Yeah, that's for sure. But it's
way less than before. Like the atom
way less than before. Like the atom
sensitivity is crazy by comparison.
All right. Well, I'll let this run. I
All right. Well, I'll let this run. I
got to go get dinner. Thanks for the
got to go get dinner. Thanks for the
help on this. Okay. And yeah, I think
help on this. Okay. And yeah, I think
just generally if you want to do optim
just generally if you want to do optim
stuff, right, we can just kind of use
stuff, right, we can just kind of use
puffer as a benchmark and then also use,
puffer as a benchmark and then also use,
you know, the algos for puffer. Um cuz
you know, the algos for puffer. Um cuz
this has been like a major thing. I mean
this has been like a major thing. I mean
this is I the cosign and healing was
this is I the cosign and healing was
also pretty major. But um I think
also pretty major. But um I think
overall this has been the largest thing
overall this has been the largest thing
in I I think this is like going to be
in I I think this is like going to be
the biggest RL advancement in the in
the biggest RL advancement in the in
this year so far. I mean, I know Evan's
this year so far. I mean, I know Evan's
been running this stuff, but I he hasn't
been running this stuff, but I he hasn't
been running it on all these different M
been running it on all these different M
it seems because like qualitatively I
it seems because like qualitatively I
can tell you this is a step change in
can tell you this is a step change in
capabilities in terms of just how janky
capabilities in terms of just how janky
everything is. I don't know if he was
everything is. I don't know if he was
trying to run like all the arcade games
trying to run like all the arcade games
with the same sets of parameters and
with the same sets of parameters and
stuff, but like if you try to run all
stuff, but like if you try to run all
the different puffs with the same set of
the different puffs with the same set of
atom params, they just don't work. Like
atom params, they just don't work. Like
they're very different tasks. It's not
they're very different tasks. It's not
like you just have a bunch of arcade
like you just have a bunch of arcade
games, right? Um, yeah. I mean, I trying
games, right? Um, yeah. I mean, I trying
to I've been trying to get PSGD into I
to I've been trying to get PSGD into I
mean it's if if you go look PSG 2015
mean it's if if you go look PSG 2015
puts puts this out, right? Like Mulan
puts puts this out, right? Like Mulan
was
was
made two months ago, three months
made two months ago, three months
ago after I I kind of was pushing for
ago after I I kind of was pushing for
whitening based optimizers for like a
whitening based optimizers for like a
year and a half. Is there a I mean I got
year and a half. Is there a I mean I got
to look at the math difference between
to look at the math difference between
then Muan and PSGD to see what they do
then Muan and PSGD to see what they do
differently. Um PSGD is a much wider
differently. Um PSGD is a much wider
framework. Right? So PSGD can do Mulan,
framework. Right? So PSGD can do Mulan,
it can do shampoo, it can do soap, it
it can do shampoo, it can do soap, it
can do kayak, it can do atom, it can do
can do kayak, it can do atom, it can do
you know all these different things. a
you know all these different things. a
bunch of different preconditioner
bunch of different preconditioner
shapes.
shapes.
Um,
Um,
and like way way more types of things
and like way way more types of things
than
than
than any other type of optimizer I think
than any other type of optimizer I think
in existence. Well, in that case, if
in existence. Well, in that case, if
it's, you know, if these things are
it's, you know, if these things are
parameterizations of PSPD, the question
parameterizations of PSPD, the question
is like, do you have one that's better
is like, do you have one that's better
than, you know, do you like you're going
than, you know, do you like you're going
to look for one that's
to look for one that's
You're going to look for a small set of
You're going to look for a small set of
hyper prams that are roughly stable
hyper prams that are roughly stable
across a wide range of problems ideally
across a wide range of problems ideally
or are relatively easy to tune.
or are relatively easy to tune.
Yeah. And I think also the
Yeah. And I think also the
preconditioner style, right? The shape
preconditioner style, right? The shape
of the preconditioner, right? This is a
of the preconditioner, right? This is a
chronicer factorized version of the
chronicer factorized version of the
preconditioner. But we also have low
preconditioner. But we also have low
rank approximations. We also have sketch
rank approximations. We also have sketch
approximations, right? We have all these
approximations, right? We have all these
different shapes. we can do like and the
different shapes. we can do like and the
point is let's find what's the best and
point is let's find what's the best and
I think buffer is really good for that
I think buffer is really good for that
because clearly you can run like 1500
because clearly you can run like 1500
15,000 years of game play yeah 24 hours
15,000 years of game play yeah 24 hours
right yeah it's pretty that's pretty fun
right yeah it's pretty that's pretty fun
that's out on one GPU we technically
that's out on one GPU we technically
have multiGPU support as well we've got
have multiGPU support as well we've got
one client who's tried it up to like 64
one client who's tried it up to like 64
GPUs and it does work
GPUs and it does work
Um, we don't have that hardware at the
Um, we don't have that hardware at the
moment. We're going to get some pretty
moment. We're going to get some pretty
soon, but in the meantime, like all the
soon, but in the meantime, like all the
simpler, they just run so fast. You just
simpler, they just run so fast. You just
you can do these in just a minute or
you can do these in just a minute or
whatever. Um,
whatever. Um,
how expensive is it to build a new app?
how expensive is it to build a new app?
What you mean, how expensive is it?
What you mean, how expensive is it?
Like let's say a client comes to you and
Like let's say a client comes to you and
says, "Hey, I want to get an agent that
says, "Hey, I want to get an agent that
works really really well on my setup.
works really really well on my setup.
You build me a simulator that would then
You build me a simulator that would then
transfer to the real world or
transfer to the real world or
something." Oh, I mean it it depends on
something." Oh, I mean it it depends on
what people want. It also depends on
what people want. It also depends on
like if the sims are going to be closed
like if the sims are going to be closed
source or not as well. Uh starting
source or not as well. Uh starting
though like puffer service packages if
though like puffer service packages if
you just want like priority service on
you just want like priority service on
features within puffer not new sims uh
features within puffer not new sims uh
and you just generally want assistance
and you just generally want assistance
with that that's 10k and then when you
with that that's 10k and then when you
start getting uh dedicated work on your
start getting uh dedicated work on your
stuff like if you want dedicated work on
stuff like if you want dedicated work on
a new sim or something then I get at
a new sim or something then I get at
least one contractor involved so that
least one contractor involved so that
starts at 20k a month and it goes up
starts at 20k a month and it goes up
depending on the amount of work that's
depending on the amount of work that's
needed cuz that's that's priced for like
needed cuz that's that's priced for like
quarter time work. So if you want people
quarter time work. So if you want people
working more hours on it, then it goes
working more hours on it, then it goes
up from there.
up from there.
20 grand a month.
20 grand a month.
Well, that's for me. That covers me plus
Well, that's for me. That covers me plus
a contractor.
a contractor.
Nice. And that's, you know, it's a slice
Nice. And that's, you know, it's a slice
of my time. Um, and then a contractor's
of my time. Um, and then a contractor's
quarter time.
quarter time.
Nice.
And how do you how do you find
And how do you how do you find
contractors to hire?
contractors to hire?
Um I mean one I had from a previous
Um I mean one I had from a previous
collaboration. Another one that we just
collaboration. Another one that we just
finished was a Twitter cold DM. And then
finished was a Twitter cold DM. And then
you know another one that I just started
you know another one that I just started
is from uh you know mutual acquaintance
is from uh you know mutual acquaintance
or mutual mutual colleague. Um and then
or mutual mutual colleague. Um and then
you know we've had a few others that
you know we've had a few others that
haven't like that haven't gone through
haven't like that haven't gone through
all the way that have been from either
all the way that have been from either
cold uh cold contact. Uh I mean we had
cold uh cold contact. Uh I mean we had
uh a few leads from GDC that almost
uh a few leads from GDC that almost
worked but I think you know Puffer has
worked but I think you know Puffer has
to get a little bit bigger to be landing
to get a little bit bigger to be landing
the larger companies um because it's
the larger companies um because it's
new. But yeah, stuff it's it's generally
new. But yeah, stuff it's it's generally
this is just there's service as a
this is just there's service as a
service and we're trying to make RL
service and we're trying to make RL
really really fast and we've got a bunch
really really fast and we've got a bunch
of good tools and they're all free and
of good tools and they're all free and
they're open source so you can just use
they're open source so you can just use
them and we actually encourage people to
them and we actually encourage people to
go ahead and use them first to make sure
go ahead and use them first to make sure
that they you know they see what we have
that they you know they see what we have
but you know if you use them and you're
but you know if you use them and you're
getting some good results well why
getting some good results well why
wouldn't you want to have us involved
wouldn't you want to have us involved
and on your side to make all your stuff
and on your side to make all your stuff
work way way way better right
work way way way better right
yeah makes sense I mean I have clients
yeah makes sense I mean I have clients
right where it's like ah I know that
right where it's like ah I know that
this thing that I was just developing on
this thing that I was just developing on
the algo side is going to be perfect for
the algo side is going to be perfect for
your end and I'm just like throw it on
your end and I'm just like throw it on
that immediately right otherwise you'd
that immediately right otherwise you'd
have to wait like you know you'd either
have to wait like you know you'd either
have to be watching the dev branch like
have to be watching the dev branch like
a hawk basically doing what I do anyways
a hawk basically doing what I do anyways
uh or you have to wait until everything
uh or you have to wait until everything
comes out and it's still not out of the
comes out and it's still not out of the
box it's RL it's hard right
but yeah I mean the goal longer term
but yeah I mean the goal longer term
right is that as we make RL more stable
right is that as we make RL more stable
and more consistent we spread throughout
and more consistent we spread throughout
a whole bunch of different industries
a whole bunch of different industries
that have sims that need optimizing and
that have sims that need optimizing and
uh you know we start taking on harder
uh you know we start taking on harder
and harder and more and more valuable
and harder and more and more valuable
problems with it and then the prices
problems with it and then the prices
will obviously go up from there, right?
will obviously go up from there, right?
Like we're not going to make somebody
Like we're not going to make somebody
$100 million for uh for 10 20k a month.
$100 million for uh for 10 20k a month.
Yeah. So you you actually did some work
Yeah. So you you actually did some work
with OpenAI, right? I I interned there
with OpenAI, right? I I interned there
in undergrad. I took Neural MMO there
in undergrad. I took Neural MMO there
and then I built the 10 version out from
there. It was like 2018, I think. It was
there. It was like 2018, I think. It was
cool. That was when they were doing
cool. That was when they were doing
Dota. It was really cool to see that.
Dota. It was really cool to see that.
Yes. I mean, what h happened with those
Yes. I mean, what h happened with those
like with that lead or those contacts,
like with that lead or those contacts,
right? like it it would make a lot of
right? like it it would make a lot of
sense, right? But whereas open not just
sense, right? But whereas open not just
they're not doing that anymore
they're not doing that anymore
unfortunately.
You know, they like the Dota stuff, if
You know, they like the Dota stuff, if
they kept doing it, RL wouldn't be in
they kept doing it, RL wouldn't be in
such a bad spot right now where I have
such a bad spot right now where I have
to be doing all this stuff cuz the Dota
to be doing all this stuff cuz the Dota
team, I think more than anybody, they
team, I think more than anybody, they
were crushing it in reinforcement
were crushing it in reinforcement
learning.
learning.
They had the hardest problem.
But they I mean they they like destroyed
But they I mean they they like destroyed
like the top teams, right? Yeah. But
like the top teams, right? Yeah. But
they like
they like
so they could have then tried to go and
so they could have then tried to go and
do it with 1% of the compute. They
do it with 1% of the compute. They
discussed doing that but they didn't.
discussed doing that but they didn't.
They kind of all moved on to language
They kind of all moved on to language
model stuff, right? They could have done
model stuff, right? They could have done
so so much work around that problem
so so much work around that problem
because they really they solved Dota
because they really they solved Dota
using very simple methods, right? There
using very simple methods, right? There
was so much room for ablations like what
was so much room for ablations like what
really matters to scale on these hard
really matters to scale on these hard
problems. There was so much room for
problems. There was so much room for
that and frankly a lot of the stuff in
that and frankly a lot of the stuff in
the Dota paper if you saw if you like
the Dota paper if you saw if you like
really go through the appendix is stuff
really go through the appendix is stuff
that people would keep rediscovering and
that people would keep rediscovering and
not really learning the lessons from for
not really learning the lessons from for
the next I mean even through till
the next I mean even through till
today. I don't know how many times I
today. I don't know how many times I
have to tell people like who are trying
have to tell people like who are trying
to throw a transformer on a super basic
to throw a transformer on a super basic
problem that a one layer LSTM solves
problem that a one layer LSTM solves
Dota.
Yeah. I mean people think it's not
Yeah. I mean people think it's not
possible to solve X problem without like
possible to solve X problem without like
fancier method fancier architecture.
fancier method fancier architecture.
It's like no one layer LSTM solves Dota
It's like no one layer LSTM solves Dota
say oh but they used a ton of compute.
say oh but they used a ton of compute.
Yeah they use a ton of compute for back
Yeah they use a ton of compute for back
then. It's not that much compute at all
then. It's not that much compute at all
by today's standards and it's a way
by today's standards and it's a way
harder problem than most people are
harder problem than most people are
dealing with.
dealing with.
Okay. So
then why don't you want to try to attack
then why don't you want to try to attack
language? Because everyone's doing
language? Because everyone's doing
language already. It'll get done with or
language already. It'll get done with or
without me.
without me.
Nobody's doing this, right? Nobody's
Nobody's doing this, right? Nobody's
doing I know, but clearly clearly like
doing I know, but clearly clearly like
the way that they're doing the language
the way that they're doing the language
is just by scaling up. Do you think I
is just by scaling up. Do you think I
guess I guess the right question is do
guess I guess the right question is do
you think that we could solve language
you think that we could solve language
in a way that is, you know, cheaper,
in a way that is, you know, cheaper,
faster, you
faster, you
know, smaller models. If you're just
know, smaller models. If you're just
using, you know, something better than
using, you know, something better than
next token prediction,
next token prediction,
next token prediction is pretty good.
next token prediction is pretty good.
The problem is, so that's like a good
The problem is, so that's like a good
base is the thing, but then you need to
base is the thing, but then you need to
be able to learn through interaction,
be able to learn through interaction,
which is what RL gives you. But like the
which is what RL gives you. But like the
thing is, labs are going to do that
thing is, labs are going to do that
anyways, and they're going to throw way
anyways, and they're going to throw way
more money at it and way more resources
more money at it and way more resources
at it than I can possibly muster, right?
at it than I can possibly muster, right?
The thing is like RL's been thrown to
The thing is like RL's been thrown to
the wayside and like a general purpose
the wayside and like a general purpose
RL has been thrown to the wayside and I
RL has been thrown to the wayside and I
can make so many so many core
can make so many so many core
contribution and advancements by doing
contribution and advancements by doing
stuff the way I have in uh in puffer
stuff the way I have in uh in puffer
right like I'm finding stuff that people
right like I'm finding stuff that people
haven't found in the last decade because
haven't found in the last decade because
I can just run experiments a thousand
I can just run experiments a thousand
times faster than them right like I keep
times faster than them right like I keep
finding the vast majority of published
finding the vast majority of published
RL methods are straight up wrong because
RL methods are straight up wrong because
they didn't bother tuning
they didn't bother tuning
hyperparameters because their M's were
hyperparameters because their M's were
too slow to do it I can actually do that
too slow to do it I can actually do that
right? I can run exhaustive experiments.
right? I can run exhaustive experiments.
I can run ablations and I can do them
I can run ablations and I can do them
very very quickly. Which is why you see
very very quickly. Which is why you see
a lot of the techniques in Puffer,
a lot of the techniques in Puffer,
right? They come from the just
right? They come from the just
ridiculously rapid speed of iteration.
Yeah,
Yeah,
that's true. I don't know. I just don't
that's true. I don't know. I just don't
really have any interest in like jumping
really have any interest in like jumping
on the general LLM bandwagon cuz like
on the general LLM bandwagon cuz like
what am I going to do? I'm going to try
what am I going to do? I'm going to try
to help them like leaprog the next other
to help them like leaprog the next other
LLM company out there by a couple
LLM company out there by a couple
months, right? Like they're not really
months, right? Like they're not really
all that interesting. But if I can just
all that interesting. But if I can just
put all of RL as a field forward, that's
put all of RL as a field forward, that's
going to help everything.
Yeah, that's
um I
um I
guess you said you have to Yeah, one
guess you said you have to Yeah, one
last thing. your your your clients. Who
last thing. your your your clients. Who
who are they mostly? Like are they
who are they mostly? Like are they
robotics companies? Like I only have I
robotics companies? Like I only have I
mean we only have a couple at a time
mean we only have a couple at a time
typically. Um because I I mostly focus
typically. Um because I I mostly focus
still on a lot of dev around Puffer. So
still on a lot of dev around Puffer. So
I try to save a fair bit of my time for
I try to save a fair bit of my time for
that. But uh so far it's been random.
that. But uh so far it's been random.
Like it's we did stuff in animation gen.
Like it's we did stuff in animation gen.
We do stuff with there's another
We do stuff with there's another
research lab. Um stuff in finance now.
research lab. Um stuff in finance now.
Like and I think that going forward
Like and I think that going forward
there's going to just be a ton of random
there's going to just be a ton of random
stuff in manufacturing as well. Just
stuff in manufacturing as well. Just
random industrial processes that are
random industrial processes that are
unintuitive and need to be optimized.
unintuitive and need to be optimized.
Like everybody's got an optimization
Like everybody's got an optimization
problem, right? And a ton of them
problem, right? And a ton of them
actually can build sims for them. Good
actually can build sims for them. Good
Sims. So RL's I think going to really
Sims. So RL's I think going to really
really improve just a ton of these
really improve just a ton of these
random processes where language models
random processes where language models
like the intuition you get from language
like the intuition you get from language
doesn't help you solve arbitrary
doesn't help you solve arbitrary
optimization problems, right? But RL
optimization problems, right? But RL
does.
does.
Yeah. I mean, could you do like you said
Yeah. I mean, could you do like you said
finance, would you like do like bio
finance, would you like do like bio
frequency trading or like Yeah. And like
frequency trading or like Yeah. And like
I can't The thing is a lot of the
I can't The thing is a lot of the
clients they have they have NDAs on like
clients they have they have NDAs on like
I you can't say exactly what they're
I you can't say exactly what they're
doing, but I mean I don't have the thing
doing, but I mean I don't have the thing
is I don't have I don't have interest in
is I don't have I don't have interest in
any specific area, right? I don't really
any specific area, right? I don't really
have a vested interest in any specific
have a vested interest in any specific
area. It's more so like RL in general.
area. It's more so like RL in general.
What can we solve with this? Let's throw
What can we solve with this? Let's throw
it on a ton of stuff. Let's just bring
it on a ton of stuff. Let's just bring
all of Puffer's tools, right? To try to
all of Puffer's tools, right? To try to
see if this can actually work on all
see if this can actually work on all
these different problems. And then
these different problems. And then
anything we learn comes back to Puffer,
anything we learn comes back to Puffer,
right? Anything that we learn in the
right? Anything that we learn in the
process about core methods comes back to
process about core methods comes back to
Puffer. So the more different types of
Puffer. So the more different types of
problems that we get to see and the more
problems that we get to see and the more
different types of problems we get to
different types of problems we get to
throw Puffer at, the better Puffer gets.
So it's basically like a work study
So it's basically like a work study
program in college.
Well, it's more so that it's more so
Well, it's more so that it's more so
that we do work for people, but we end
that we do work for people, but we end
up keeping a lot of the code open source
up keeping a lot of the code open source
because it's actually to their benefit
because it's actually to their benefit
to keep it open source because any as
to keep it open source because any as
soon as it's an open source and puffer,
soon as it's an open source and puffer,
right, we get to test all of our new
right, we get to test all of our new
stuff against it and our users get to
stuff against it and our users get to
test all their stuff against it. So like
test all their stuff against it. So like
you get so much free progress on your
you get so much free progress on your
thing by keeping it open source and
thing by keeping it open source and
puffer. that's it's really to your
benefit. Like who cares if you have a
benefit. Like who cares if you have a
simulator, right? If your simulator is
simulator, right? If your simulator is
open source, like you still have a
open source, like you still have a
business around it, right? And you're
business around it, right? And you're
still the first person to own that
still the first person to own that
business around it. Um there's so much
business around it. Um there's so much
other than just the sim or just that
other than just the sim or just that
code. So, and if you know, if you're
code. So, and if you know, if you're
able to move faster because the stuff is
able to move faster because the stuff is
open source and tougher, then why
open source and tougher, then why
wouldn't you? You don't want to deal
wouldn't you? You don't want to deal
with RL. It's really freaking hard,
with RL. It's really freaking hard,
right? Let us deal with it.
Yeah.
Yeah. It's true. The other nice thing
Yeah. It's true. The other nice thing
about it, right, is because Puffer is
about it, right, is because Puffer is
all open source, it's really easy to
all open source, it's really easy to
work with us cuz companies are worried
work with us cuz companies are worried
about like, you know, third-party SAS
about like, you know, third-party SAS
like outsourcing capabilities behind
like outsourcing capabilities behind
like a closed source wall or behind like
like a closed source wall or behind like
a service model, pay wall. You don't
a service model, pay wall. You don't
have any of that, right? You just use
have any of that, right? You just use
the code. The code's really simple. you
the code. The code's really simple. you
can in-house some of the RL capabilities
can in-house some of the RL capabilities
if you're interested in doing that and
if you're interested in doing that and
you get all the updates for free. You're
you get all the updates for free. You're
just paying for you're just paying for
just paying for you're just paying for
assistance on stuff. So
assistance on stuff. So
yeah, that makes sense. It's actually
yeah, that makes sense. It's actually
interesting model. Plus, you get like
interesting model. Plus, you get like
eyes on cluster, too. It's open. Well,
eyes on cluster, too. It's open. Well,
it's not I mean, it's not the best model
it's not I mean, it's not the best model
for pure profit, but that's not my
for pure profit, but that's not my
motivation, right? I want to make money,
motivation, right? I want to make money,
but I also really want to make RL work
but I also really want to make RL work
as a scientific field. Um like it and
as a scientific field. Um like it and
this is why also I haven't even gone for
this is why also I haven't even gone for
VC money of any sort, right? Like the
VC money of any sort, right? Like the
stuff I just did like just even the
stuff I just did like just even the
results of throwing Muan plus the other
results of throwing Muan plus the other
tweaks that I did at Puffer. No VC would
tweaks that I did at Puffer. No VC would
let me release that because like the
let me release that because like the
quality of those results, right? Like
quality of those results, right? Like
literally RL is a different field today
literally RL is a different field today
from 3 days ago because of MUN plus a
from 3 days ago because of MUN plus a
couple of these other optimizer tweaks,
couple of these other optimizer tweaks,
cosigning, a few other things, some bug
cosigning, a few other things, some bug
fixes.
fixes.
Um, I mean, you can't see it yet. You
Um, I mean, you can't see it yet. You
can't see it yet because you didn't see
can't see it yet because you didn't see
the uh the experiments, but like the
the uh the experiments, but like the
difference between you having to run 200
difference between you having to run 200
experiments to get a specific set of
experiments to get a specific set of
hyperparameters per end and the defaults
hyperparameters per end and the defaults
roughly working out of the box now with
roughly working out of the box now with
all these new changes is that's a
all these new changes is that's a
qualitatively new field
qualitatively new field
essentially, right?
essentially, right?
So I guess in my in my paper it's funny
So I guess in my in my paper it's funny
in my 20 it was 2021 2022
in my 20 it was 2021 2022
I did PSGD on RL and I saw results and I
I did PSGD on RL and I saw results and I
did cosign a nailing but I was just
did cosign a nailing but I was just
using like a really shitty reinforcement
using like a really shitty reinforcement
learning like package and if I had you
learning like package and if I had you
know put it into puffer in 2022
know put it into puffer in 2022
you know would have been um
you know would have been um
a lot better. Yeah, but that's the thing
a lot better. Yeah, but that's the thing
like you know the amount of effort it
like you know the amount of effort it
took in order to get to a state where we
took in order to get to a state where we
can do that. We now have 30,000 lines of
can do that. We now have 30,000 lines of
C environments.
C environments.
Yeah. Right.
I I think it was even before I did this
I I think it was even before I did this
in
20 I want to say 19. I did a I did um a
20 I want to say 19. I did a I did um a
Flappy Bird RL thing with um a very
Flappy Bird RL thing with um a very
early version of PSG. It was actually it
early version of PSG. It was actually it
was it was PSG product refactorized um
was it was PSG product refactorized um
and that worked quite well and there was
and that worked quite well and there was
like I I could pull up that report
like I I could pull up that report
actually.
actually.
Yeah, if you want I'm not claiming to
Yeah, if you want I'm not claiming to
I'm not claiming to be the first one to
I'm not claiming to be the first one to
uh you know to use Muon or use PSP or
uh you know to use Muon or use PSP or
anything. Right. The point is the point
anything. Right. The point is the point
is the the catalog, right? The catalog
is the the catalog, right? The catalog
of different problems and the ability to
of different problems and the ability to
like get these exhaustive results so
like get these exhaustive results so
quickly because like I can literally so
quickly because like I can literally so
if I wanted to like prove algorithm A
if I wanted to like prove algorithm A
generally better than algorithm B,
generally better than algorithm B,
right? I can give you it would take like
right? I can give you it would take like
maybe a week's worth of experiments, but
maybe a week's worth of experiments, but
I could give you a 200 parameter
I could give you a 200 parameter
hyperparameter sweep for every one of
hyperparameter sweep for every one of
these environments.
these environments.
No, exactly. a 200 experiment type of
No, exactly. a 200 experiment type of
parameters for every one of these. It's
parameters for every one of these. It's
increasingly difficult to convince
increasingly difficult to convince
people of anything. Mhm. But this makes
people of anything. Mhm. But this makes
it possible for at least for Yeah.
All right. I'm gonna go get dinner. Um,
All right. I'm gonna go get dinner. Um,
let me see. Do we have folks watching?
let me see. Do we have folks watching?
and we have a few folks watching. Um, so
and we have a few folks watching. Um, so
let me just say real quick for the folks
let me just say real quick for the folks
on Twitch, if you're interested in
on Twitch, if you're interested in
checking out Puffer,
checking out Puffer,
puffer.ai, go ahead and start the repo.
puffer.ai, go ahead and start the repo.
Really helps us out. And if you want to
Really helps us out. And if you want to
get involved with dev, build some cool
get involved with dev, build some cool
environments, push RL forward,
environments, push RL forward,
discord.gg/puffer, it's right here. and
discord.gg/puffer, it's right here. and
follow me on extra

Kind: captions
Language: en
One second.
Cool.
Um, so this didn't quite
Um, so this didn't quite
work, but it's definitely worth
sweeping. Let me see if they've linked
sweeping. Let me see if they've linked
me anything else. I think Lucas sent me
me anything else. I think Lucas sent me
a
PR. Keep
PR. Keep
this. I can see myself live. Hey, how's
this. I can see myself live. Hey, how's
it going, man?
This did not
This did not
work. This is no
bueno. Lucas sent me a PR, I think. So,
bueno. Lucas sent me a PR, I think. So,
I should check that
I should check that
first. I don't know if he sent me a PR
first. I don't know if he sent me a PR
or he just like
You can
You can
boost LR as big as 1.8 and reduce it as
boost LR as big as 1.8 and reduce it as
little as
0.001. You can also have
scheduleuler for L. What is the pre?
scheduleuler for L. What is the pre?
What is LR
preconditioning? Do you you anneal both
preconditioning? Do you you anneal both
or no? Right now I have cosign analing
or no? Right now I have cosign analing
on just the learning
rate but it's not going to do anything
rate but it's not going to do anything
because this is
because this is
like this is literally like 1% into
training. So like scheduling this over
training. So like scheduling this over
training isn't going to do anything
training isn't going to do anything
because this is like 1% of
because this is like 1% of
training. You
see like you got to zoom out on this
thing. So this is the actual graph,
thing. So this is the actual graph,
right?
is
big. This axis is in tens of billions.
big. This axis is in tens of billions.
So this is 40 billion steps.
And um if you want to see like how far
And um if you want to see like how far
off this end is from
off this end is from
solved, this goes to 3.5. The max is I
solved, this goes to 3.5. The max is I
believe 20 or
25. Where's this thing Evan sent me?
You sent me
a GitHub is eding at your service.
a GitHub is eding at your service.
That's funny as hell.
one is the learning
one is the learning
rate of the neural net and the one is
rate of the neural net and the one is
the learning rate of the sub problem to
the learning rate of the sub problem to
learn the
preconditioner. Okay, so I can sweep
preconditioner. Okay, so I can sweep
those BRB.
those BRB.
Yeah, I can sweep
those. The sweeps take a good while, you
those. The sweeps take a good while, you
know.
move the
move the
Discord. Um, I can see it over here on
Discord. Um, I can see it over here on
the side.
Heavy
bulron. Uh, where is his optimizer that
bulron. Uh, where is his optimizer that
he
he
added? Oh, hey Evan. Evan's here.
Yeah, this is the same
Yeah, this is the same
Evan. I don't know if he's watching
Evan. I don't know if he's watching
stream or
not. Where is this
thing? So this is just
thing? So this is just
like loss go
like loss go
here.
Closure. Oh, these this is gross. I
Closure. Oh, these this is gross. I
freaking hate closures, but whatever. If
freaking hate closures, but whatever. If
it works, it works. Freaking jacks in my
it works, it works. Freaking jacks in my
pie
torch. I thought it was the was it 4H
torch. I thought it was the was it 4H
Newton PSGD. this
Newton PSGD. this
thing. They're blind without
thing. They're blind without
testing and give a list of
hypers to give it a
hypers to give it a
closure. Not enough values to unpack.
Not enough values to unpack. Expected
Not enough values to unpack. Expected
two got
zero. Loss equals
Heavy ball
Heavy ball
utils handle
closure. Uh, don't
closure. Uh, don't
work. What's this thing supposed to
work. What's this thing supposed to
return?
Send one thing real
whatever closure return should return
whatever closure return should return
from optimizer step ideally.
from optimizer step ideally.
So
So
loss
losses. But that's what you did, isn't
it?
Zero. Oh, but you can't do this.
Let's just send screenshots of code
Let's just send screenshots of code
because that's a smart thing to do.
It's breaking in step,
It's breaking in step,
right? A step
closure. It should be like this, right?
Oh, is that not the
Oh, is that not the
bug? Handle
bug? Handle
closure. Loss equals handle
closure. I don't understand how this
closure. I don't understand how this
thing freaking works.
I don't know why that doesn't
I don't know why that doesn't
uh I do something stupid. I
don't No, this gets
don't No, this gets
policy.parameters, right? For each cache
policy.parameters, right? For each cache
Newton PSG.
return
return
step. When you hit step, it should pass
step. When you hit step, it should pass
something
something
back. Wait, when you hit
back. Wait, when you hit
step, it should pass something how it's
step, it should pass something how it's
back.
Wait, is it? I've never actually used
Wait, is it? I've never actually used
eye torch that
way. Oh, wait. Hang on. I do something a
way. Oh, wait. Hang on. I do something a
thing.
thing.
loss equals
model. Wait, but then the closure
model. Wait, but then the closure
doesn't do anything,
doesn't do anything,
right? So why does it need a function
right? So why does it need a function
like that?
What? Oh, wait. Evan gave me this like
What? Oh, wait. Evan gave me this like
loss and batch losses
thing. So, he has return loss
But wait, what's that
But wait, what's that
mean?
mean?
Step
closure loss equals
closure. So you can just pass the damn
closure. So you can just pass the damn
loss in, right?
They just have enable
They just have enable
grad on this freaking
thing. Confused
then. Why do you have this entire thing
then. Why do you have this entire thing
in closure?
And then I'm confused because actually
And then I'm confused because actually
this just returns loss, not batch
this just returns loss, not batch
losses. So that's probably the
losses. So that's probably the
bug. Here's another way to do it. Let's
bug. Here's another way to do it. Let's
say you linked me
This is just how PyTorch works via
This is just how PyTorch works via
closure. But this one needs a closure,
closure. But this one needs a closure,
right? This one didn't work without
right? This one didn't work without
it.
it.
[Music]
[Music]
So
cron, this is for
Newton. You can't just do this, right?
I don't know if you're still watching
I don't know if you're still watching
stream.
loss. Create graph equal
true.
true.
Wait, the readme is not updated. Create
Wait, the readme is not updated. Create
graph. So, it doesn't need a
closure. This is default stuff.
So, do
So, do
I just return loss?
That doesn't work
either. Um
Pie torch works here. This whole This
Pie torch works here. This whole This
isn't a
closure. [ __ ]
I don't know why it would even need one
I don't know why it would even need one
anyways, right? Why does it need to
anyways, right? Why does it need to
compute the loss in that? Is there like
compute the loss in that? Is there like
some decorator it has to throw on
everything? You can do it multiple
everything? You can do it multiple
ways. Okay.
Oh, hang on. I got confused. Let's see.
Oh, hang on. I got confused. Let's see.
Heavy. So, yeah, it's the it's this
Heavy. So, yeah, it's the it's this
implementation. So, this is not even
implementation. So, this is not even
Okay, I see what you mean when it's a
Okay, I see what you mean when it's a
heavy ball thing. Okay,
heavy ball thing. Okay,
cool. HVs default way is via
closure. But I don't understand the
closure. But I don't understand the
signature that this thing
signature that this thing
has because
It has this thing. Where is this? Maybe
It has this thing. Where is this? Maybe
I just go find the
I just go find the
source. I'm just getting tired here.
source. I'm just getting tired here.
850. Is it 869 or
850. Is it 869 or
something? Long ass
thing. Yeah. So, it's closure is
thing. Yeah. So, it's closure is
none. Requires a closure.
none. Requires a closure.
loss equal
closure. Well, it's failing on this
closure. Well, it's failing on this
param groups thing. Is this not how you
param groups thing. Is this not how you
initialize this
initialize this
optimizer? You have to give this
optimizer? You have to give this
thing
thing
params
params
defaults just takes params, right?
Well, looks like the loss definitely has
Well, looks like the loss definitely has
to
to
be
closure. Oh, it just returns the loss.
closure. Oh, it just returns the loss.
And what the hell does it need the
And what the hell does it need the
closure for? What the [ __ ]
Modify closure of closure.
So all of this just to add one stupid
keyword. Why do optimizers have this
keyword. Why do optimizers have this
stupid ass
API? Second
API? Second
order LBFG. We'll use
order LBFG. We'll use
closure. Oh, because it runs the stupid
closure. Oh, because it runs the stupid
thing multiple times, doesn't
thing multiple times, doesn't
it? But this doesn't run that multiple
it? But this doesn't run that multiple
times, right? This doesn't run them loss
times, right? This doesn't run them loss
multiple times. I'm hoping because
multiple times. I'm hoping because
otherwise that's slow as hell and
otherwise that's slow as hell and
there's no point in
it. This doesn't run the loss multiple
it. This doesn't run the loss multiple
times, does it?
How is it not that slow if it has to run
How is it not that slow if it has to run
the It has to run the whole model for it
the It has to run the whole model for it
pass again, doesn't
it? Two back
props. So it needs a closure then for
sure. So why does it not work?
860. You're going to tell me this and
860. You're going to tell me this and
then it's going to be like five steps
then it's going to be like five steps
per second.
So for some there's something wrong with
So for some there's something wrong with
I think the optimizer because this is
I think the optimizer because this is
it's failing on this thing which is just
it's failing on this thing which is just
like getting params and grabs from self
like getting params and grabs from self
param
param
groups. So, I don't know how that
happens, All right.
I'm now realizing this is a
double split PNG in
double split PNG in
group self
group self
parames. Lucas had examples
parames. Lucas had examples
somewhere. No, I cannot find. Yeah, I
somewhere. No, I cannot find. Yeah, I
didn't see Newton in
here. Oh, the closure is supposed to do
here. Oh, the closure is supposed to do
the backward pass
here. It's not just supposed to compute
here. It's not just supposed to compute
the loss. supposed to do the backward
the loss. supposed to do the backward
pass. Hang
on. Yeah. So, this has been not correct.
on. Yeah. So, this has been not correct.
I think it
Well, this is just deleted now. There's
Well, this is just deleted now. There's
no
backward zero grad and
backward zero grad and
then where does he do this? Is
it
optimizerstep and then zero brad?
So I think this just has to have
So I think this just has to have
backward in
backward in
it. That's probably what it is.
Does this thing not works with freaking
um so now I don't get cudn
Then I say different.
Name space does not support item
Name space does not support item
assignment.
Uh, he made
this the hell. Oh,
Oh, the [ __ ] Yeah, he messed with this.
I'm curious as to how he even did
this. Shouldn't have had to touch my
this. Shouldn't have had to touch my
losses.
How the [ __ ] is this even broken? I'm
How the [ __ ] is this even broken? I'm
like
confused. Oh, wait. Hang
confused. Oh, wait. Hang
on. Compute
losses losses.
Oh, this might just be how freaking
Oh, this might just be how freaking
Python works. If you call this losses,
Python works. If you call this losses,
do I have to call this batch losses?
What the [ __ ] is this?
What the [ __ ] is this?
Like I got to see how I had this
Like I got to see how I had this
originally. This is just like a
originally. This is just like a
[Music]
[Music]
random
Oh. Break this
badly. Oh, yeah. You tried to get clever
badly. Oh, yeah. You tried to get clever
and you broke. Yeah, there's
and you broke. Yeah, there's
one. Lucas codes in a very particular
one. Lucas codes in a very particular
way. Yeah, it's clearly functional
way. Yeah, it's clearly functional
inspired. Though to be fair, the closure
inspired. Though to be fair, the closure
API is from
PyTorch. I have literally no time or
PyTorch. I have literally no time or
patience for people who are like either
patience for people who are like either
very dogmatically functional or very
very dogmatically functional or very
dogmatically object-oriented. They're
dogmatically object-oriented. They're
both just a [ __ ] waste of time.
both just a [ __ ] waste of time.
Though from Lucas's code, it wasn't that
Though from Lucas's code, it wasn't that
bad. This isn't Lucas's code. This is
bad. This isn't Lucas's code. This is
just a PR that didn't understand that
just a PR that didn't understand that
you can't [ __ ] do this because uh
you can't [ __ ] do this because uh
this is not a
this is not a
dictionary. It's
fine. We'll do this for now.
closure is used twice for two different
closure is used twice for two different
gradients. Okay, so this runs now.
If you would please tell me why this
If you would please tell me why this
runs 30,000 steps per second, that would
runs 30,000 steps per second, that would
be great.
Yeah, that's way too slow.
as if I'm doing anything dumb, but I
as if I'm doing anything dumb, but I
don't think
so. What should it be at? At least 10
so. What should it be at? At least 10
times this fast.
times this fast.
More like 400, 500,000.
So,
like what do I do with
this? It shouldn't be that much lower,
this? It shouldn't be that much lower,
right?
Oh, it's the stupid QDNN probably on the
Oh, it's the stupid QDNN probably on the
LSTM cuz this didn't work without QDN
LSTM cuz this didn't work without QDN
like
right. It's probably the LSTM being
right. It's probably the LSTM being
stupid slow.
I mean, not working with
I mean, not working with
LSTMs is like kind of [ __ ]
I can run it without the LSTM, but then
I can run it without the LSTM, but then
it's not going to do anything
it's not going to do anything
anyways. I can do that just to
anyways. I can do that just to
check it works in JS. Yeah, but then
check it works in JS. Yeah, but then
you're writing JS and then you get put
you're writing JS and then you get put
on suis. No, you get it put on
on suis. No, you get it put on
demonetized watch. Yeah, there you go.
demonetized watch. Yeah, there you go.
[ __ ] YouTube.
[ __ ] YouTube. I
swear and
Twitch. I actually don't know if you can
Twitch. I actually don't know if you can
get uh if it even you can even do that
get uh if it even you can even do that
on X. I think you still get deboosted.
on X. I think you still get deboosted.
Dell, are you on Twitch? Yeah, I'm on
Dell, are you on Twitch? Yeah, I'm on
here, Twitch, and YouTube.
Is it L? This is the Newton whatever you
Is it L? This is the Newton whatever you
linked
me. The [ __ ] is
this? Oh, maybe I just genuinely didn't
this? Oh, maybe I just genuinely didn't
test it with that. Huh?
I don't know what the [ __ ] is wrong with
I don't know what the [ __ ] is wrong with
this.
Why is it still doing
Why is it still doing
this? What?
Ah, yes. It's a stupid thing.
575k. I was at 575k per
still. I'm so confused.
Well, there's
Well, there's
no no LSTM, so I guess it does have this
no no LSTM, so I guess it does have this
many
many
params. Oh, it's going to be that big
params. Oh, it's going to be that big
ass projection now. Okay, but this is
ass projection now. Okay, but this is
smaller than before. And it's still so
smaller than before. And it's still so
with no LSTM, this thing is still like
with no LSTM, this thing is still like
at least 2x too
at least 2x too
slow, probably more.
and it just doesn't work with an LSTM.
Who the hell came up with the idea of a
Who the hell came up with the idea of a
closure anyways? It's like god awful
closure anyways? It's like god awful
programming. Freaking hurts my soul to
programming. Freaking hurts my soul to
have this even in the messy dev
branch. So,
branch. So,
um, presumably this means I just can't I
um, presumably this means I just can't I
can't use this, right? if it just
can't use this, right? if it just
doesn't work with
LSTMs. Just to check, this is cron
LSTMs. Just to check, this is cron
Newton.
Newton.
Yeah. Newton P
Yeah. Newton P
SGD. So, it actually did get faster. It
SGD. So, it actually did get faster. It
looks like it's is getting faster, but
looks like it's is getting faster, but
like it doesn't work with
uh doesn't work with LSTMs.
uh doesn't work with LSTMs.
because you have to disable
KUDNN. Yeah. So, it did get it got back
KUDNN. Yeah. So, it did get it got back
to like reasonable speed,
to like reasonable speed,
but I mean you have to disable KUDNN.
I mean, is there anything else I should
I mean, is there anything else I should
try? cuz I
like masking still bugged or did you fix
like masking still bugged or did you fix
that? Um, I don't know.
Spencer, we'll give you a finite
Spencer, we'll give you a finite
difference. So yeah, let's try that
difference. So yeah, let's try that
before I go and like revert this stuff.
before I go and like revert this stuff.
And I guess I'll it'll be up to me to
And I guess I'll it'll be up to me to
figure out a way to make this not
figure out a way to make this not
terribly awful. Like what the hell is
terribly awful. Like what the hell is
this?
It's one-sided. Whatever you think I
It's one-sided. Whatever you think I
should try on
this. I'm going to go back to the LSTM
this. I'm going to go back to the LSTM
version at least.
Build it into happy
ball. There's
your what?
Can you add
Mars? Well, this doesn't even run like
Mars? Well, this doesn't even run like
this runs at 30k steps per second,
right? Maybe it gets faster eventually.
Yeah, I know. There's um what on Newton
Yeah, I know. There's um what on Newton
or do you want me to run with like uh or
or do you want me to run with like uh or
do you want me to run just
cro where you want me to run?
Okay. Want me to just run this on mobile
Okay. Want me to just run this on mobile
since that'll be
quick?
Let's make sure this runs and I'll add
Let's make sure this runs and I'll add
Neptune. Once it recompiles the kernels
Neptune. Once it recompiles the kernels
and all
that big delay in chat, I can read it
that big delay in chat, I can read it
from the Discord just as
from the Discord just as
well. Yeah, X has a big
delay. I mean, if you want, you can jump
delay. I mean, if you want, you can jump
in the puffer discord voice as well.
in the puffer discord voice as well.
That also works.
[Music]
Twitch is
faster. I mean, you can just jump in the
faster. I mean, you can just jump in the
puff or voice unless you don't want to
puff or voice unless you don't want to
be in voice on stream.
It's a lot
easier. We pay for this freaking server
easier. We pay for this freaking server
for a reason. It's got like the max
for a reason. It's got like the max
quality audio and everything.
Hello. Hey, how's it going? It's
Hello. Hey, how's it going? It's
probably a hell of a lot easier, right?
probably a hell of a lot easier, right?
It's so much easier. This is kind of
It's so much easier. This is kind of
dumb. Yeah. Yeah. Okay. So,
dumb. Yeah. Yeah. Okay. So,
uh I'll run this on
uh I'll run this on
um Puffer Mobile real quick cuz neural
um Puffer Mobile real quick cuz neural
MMO 3 usually takes a little bit.
MMO 3 usually takes a little bit.
Yeah. So, I think Mars. Okay. So, a
Yeah. So, I think Mars. Okay. So, a
variance. Yeah. It's a variance
variance. Yeah. It's a variance
reduction technique.
reduction technique.
All that it does is it says I'm going to
All that it does is it says I'm going to
take my current gradients and subtract
take my current gradients and subtract
my previous gradient information from
my previous gradient information from
it. And so in some
it. And so in some
ways it um acts as like a semiann vector
ways it um acts as like a semiann vector
product, right? It'll do a finite
product, right? It'll do a finite
difference. I'm going to take my current
difference. I'm going to take my current
gradients
gradients
and I'm going to subtract my
and I'm going to subtract my
previous gradient from it.
previous gradient from it.
Gradient not gradient variance.
Gradient not gradient variance.
No, you're you're you're just just your
No, you're you're you're just just your
previous buffer. So this is this is
previous buffer. So this is this is
gradient scale invariance, right? Yes.
gradient scale invariance, right? Yes.
Not gradients. Not gradient variance
Not gradients. Not gradient variance
invariance.
invariance.
No. No. Okay. Is there anything that
No. No. Okay. Is there anything that
does that?
does that?
Gradient variance and variance. What
Gradient variance and variance. What
does that mean? Let me think about. So
does that mean? Let me think about. So
if you're training on if you're training
if you're training on if you're training
on a language model, you kind of have
on a language model, you kind of have
like the same problem everywhere.
like the same problem everywhere.
So you can tune your params and it'll
So you can tune your params and it'll
just work. Um, in RL, different
just work. Um, in RL, different
environments are going to give you
environments are going to give you
different gradients and some of them are
different gradients and some of them are
going to have very high variance and
going to have very high variance and
some of them aren't. Yeah. And that
some of them aren't. Yeah. And that
screws up your hypers. I see. I see. So,
screws up your hypers. I see. I see. So,
and you just have to adjust the learning
and you just have to adjust the learning
rate is all it is. You just have to
rate is all it is. You just have to
adjust the learning rate. But, uh, it
adjust the learning rate. But, uh, it
would be really cool if that could have
would be really cool if that could have
been done like if that could be done
been done like if that could be done
automatically.
So with PSGD okay so I I will I will not
So with PSGD okay so I I will I will not
only talk about the things I do but what
only talk about the things I do but what
other people do as well. So there's a
other people do as well. So there's a
work called the
work called the
adapt by Defasio I'll link you right
adapt by Defasio I'll link you right
now.
Um, and all
Um, and all
right. Yeah. So, there's this work the
right. Yeah. So, there's this work the
adaption and I've built this in to a
adaption and I've built this in to a
variant of PSG. Where' you link it? Uh,
variant of PSG. Where' you link it? Uh,
yeah. Uh, is it okay if I drop it in the
yeah. Uh, is it okay if I drop it in the
puffer chat? Whatever.
puffer chat? Whatever.
Um, so there is a newer version called
Um, so there is a newer version called
Prodigy.
Prodigy.
Um, but I have actually found deaption.
Um, but I have actually found deaption.
Okay, Prodigy is a little bit more
Okay, Prodigy is a little bit more
aggressive than D adaption on its
aggressive than D adaption on its
learning rate. Um, I prefer deaption to
learning rate. Um, I prefer deaption to
prody, but you could consider both. So,
prody, but you could consider both. So,
I can try this. I will say that I tried
I can try this. I will say that I tried
a couple of learning like the schedule
a couple of learning like the schedule
free or whatever uh
free or whatever uh
in heavy ball and they just didn't work.
in heavy ball and they just didn't work.
Yeah. Okay. So, schedule free is
Yeah. Okay. So, schedule free is
something different. Schedule free will
something different. Schedule free will
absolutely not work for reinforcement
absolutely not work for reinforcement
learning. Um, and it also will not work
learning. Um, and it also will not work
for a lot of smaller tasks. What
for a lot of smaller tasks. What
schedule free is doing is it's
schedule free is doing is it's
um, so it'll work great for language
um, so it'll work great for language
models, right? Because it has this
models, right? Because it has this
basically thing where your problem is
basically thing where your problem is
basically the same over your entire
basically the same over your entire
setting. And that's not true for RL at
setting. And that's not true for RL at
all. Yeah, we know language model people
all. Yeah, we know language model people
they they like to solve easy problems.
they they like to solve easy problems.
Yes. No. No. Exactly right. Um and
Yes. No. No. Exactly right. Um and
so with schedule free what it's doing is
so with schedule free what it's doing is
it's literally doing an interpolation on
it's literally doing an interpolation on
the weights through time as well, right?
the weights through time as well, right?
And so you're finding some like
And so you're finding some like
interpolation of a bunch of solutions,
interpolation of a bunch of solutions,
right? And this absolutely will not work
right? And this absolutely will not work
in many sense, right?
in many sense, right?
Um but d adaption all it does is it just
Um but d adaption all it does is it just
does some upper or lower bound on some
does some upper or lower bound on some
stuff to basically find a good
stuff to basically find a good
initialization for your learning rate.
initialization for your learning rate.
Um is this like a thing that you would
Um is this like a thing that you would
that you could because it says for atom
that you could because it says for atom
and stuff. Could you throw it on muan or
and stuff. Could you throw it on muan or
something? Yeah, you could on PSPD or
something? Yeah, you could on PSPD or
something. Yeah. Yeah. I I yeah I can if
something. Yeah. Yeah. I I yeah I can if
you really want I can um well I can wrap
you really want I can um well I can wrap
something with with uh muan I will
something with with uh muan I will
instead of what I really want look at it
instead of what I really want look at it
this way I have a lot of experiments
this way I have a lot of experiments
that I can run very very quickly on
that I can run very very quickly on
relatively interesting problems so that
relatively interesting problems so that
is useful for testing stuff cuz like I
is useful for testing stuff cuz like I
mean this is a way harder optim
mean this is a way harder optim
benchmark than something like an LLM
benchmark than something like an LLM
right no I agree so I like I can run
right no I agree so I like I can run
this here it's running on neural MMO.
this here it's running on neural MMO.
Now, we'll see if it does anything. And
Now, we'll see if it does anything. And
by the way, here's your Mars result just
by the way, here's your Mars result just
immediately. Um, yeah. So, this is very
immediately. Um, yeah. So, this is very
slightly
slightly
better potentially or comparable
better potentially or comparable
probably probably in the noise to be
probably probably in the noise to be
honest. But then the thing, the reason I
honest. But then the thing, the reason I
didn't use this is because I saw this
didn't use this is because I saw this
and uh it has a little bit of a does it
and uh it has a little bit of a does it
have a little bit of a perf penalty or
have a little bit of a perf penalty or
am I making that up?
am I making that up?
Does it have a what? Sorry. Uh, it
Does it have a what? Sorry. Uh, it
doesn't really have a perf penalty. It's
doesn't really have a perf penalty. It's
kind of the same. No, it doesn't. It's
kind of the same. No, it doesn't. It's
It's the memory penalty.
It's the memory penalty.
Oh, yeah. We don't care about that. We
Oh, yeah. We don't care about that. We
have infinite memory. Yeah, exactly. Um,
have infinite memory. Yeah, exactly. Um,
yeah, we have infinite memory until we
yeah, we have infinite memory until we
start have to start offloading stuff
start have to start offloading stuff
because we do have we keep batch size.
because we do have we keep batch size.
We will keep like batch size 500,000 in
We will keep like batch size 500,000 in
memory.
memory.
Okay, so here's something kind of
Okay, so here's something kind of
interesting with PSGD. You can more or
interesting with PSGD. You can more or
less set your batch size to one and it
less set your batch size to one and it
will just do fine. Um,
will just do fine. Um,
you can set your batch size to one and
you can set your batch size to one and
it will do fine.
it will do fine.
Yes.
Uh, that cannot possibly work because
Uh, that cannot possibly work because
LSTM.
Okay. On LSTM, I don't know.
Okay. On LSTM, I don't know.
Maybe one trajectory segment. Yeah,
Maybe one trajectory segment. Yeah,
maybe one trajectory segment. Yeah.
maybe one trajectory segment. Yeah.
Yeah.
Yeah.
On. Yeah. But for like images for
On. Yeah. But for like images for
example, you can you can set the you can
example, you can you can set the you can
set to one and it'll do fine. The other
set to one and it'll do fine. The other
thing you can do here is you can Okay.
thing you can do here is you can Okay.
So I I don't know what uh
So I I don't know what uh
hyperparameters you play around with,
hyperparameters you play around with,
but just to kind of explain like PSG is
but just to kind of explain like PSG is
something very different. it's an
something very different. it's an
optimization framework um in itself. So
optimization framework um in itself. So
it it there is the neural network side
it it there is the neural network side
of things which you're minimizing right
of things which you're minimizing right
your precondition
your precondition
gradient descent but then there's also
gradient descent but then there's also
um your what is it called?
um your what is it called?
There's also your
There's also your
uh premature fitting. So you're running
uh premature fitting. So you're running
a convex optimization problem to learn
a convex optimization problem to learn
your
your
preconditioner and then you're also
preconditioner and then you're also
using that preconditioner to to
using that preconditioner to to
precondition your gradients to minimize
precondition your gradients to minimize
the loss of your normal. So if your
the loss of your normal. So if your
preconditioner is too high um or too low
preconditioner is too high um or too low
uh the learning of that is too high or
uh the learning of that is too high or
too low it can also mess you up. Yeah.
too low it can also mess you up. Yeah.
So I can run a sweep on that. That's no
So I can run a sweep on that. That's no
big deal. Yeah. Yeah. which is which is
big deal. Yeah. Yeah. which is which is
why I thought I mean I rewarded we
why I thought I mean I rewarded we
yeah I can probably run I mean neural
yeah I can probably run I mean neural
MMO 3 will train like I don't know
MMO 3 will train like I don't know
billion and a half steps an hour at
billion and a half steps an hour at
least so I can just run like a good
least so I can just run like a good
chunk of uh of experiments even on
chunk of uh of experiments even on
that are the defaults generally not like
that are the defaults generally not like
the precondition defaults not good or
the precondition defaults not good or
no they're They're good. Um, and Evan
no they're They're good. Um, and Evan
has
has
seen Evan has seen them be pretty good.
seen Evan has seen them be pretty good.
This is um Mars, by the way. Nice. No,
This is um Mars, by the way. Nice. No,
but the thing is like this, you see what
but the thing is like this, you see what
happened with the last run is it did way
happened with the last run is it did way
better and then it just like didn't. It
better and then it just like didn't. It
got stuck completely. Yeah. So, you can
got stuck completely. Yeah. So, you can
I mean, what's the LR precon right now?
I mean, what's the LR precon right now?
Probably 0.1. uh whatever default is
Probably 0.1. uh whatever default is
let's just just quickly reduce it by an
let's just just quickly reduce it by an
order magnitude with Mars
order magnitude with Mars
uh yeah with without the other thing is
uh yeah with without the other thing is
yeah what's the
uh was it heavy ball specific yeah I
uh was it heavy ball specific yeah I
that's the thing I either use Zealand's
that's the thing I either use Zealand's
implementation which is also probably my
implementation which is also probably my
implementation We kind of wrote all that
implementation We kind of wrote all that
together. Oh, I'll test I'll test it.
together. Oh, I'll test I'll test it.
Sure. I tried Evan's implementation of
Sure. I tried Evan's implementation of
uh of PSGD and it and then I tried this
uh of PSGD and it and then I tried this
one and Evans got like 350 score on
one and Evans got like 350 score on
breakout and this one solved it with the
breakout and this one solved it with the
defaults.
defaults.
Yeah. So, you can look at Zealand's
Yeah. So, you can look at Zealand's
code. Um
code. Um
it's it has a lot more optimizers than
it's it has a lot more optimizers than
Evans code. Um, but it's also written in
Evans code. Um, but it's also written in
a way that might make you want to pull
a way that might make you want to pull
your hair out. Oh, lovely. Um, is which
your hair out. Oh, lovely. Um, is which
of these params should I change by the
of these params should I change by the
way? Yeah, let me see.
way? Yeah, let me see.
Uh, it should be like Okay. LR weight
Uh, it should be like Okay. LR weight
decay preconditioner update
decay preconditioner update
probability. No, not that.
Um, okay. Precon LR. The last one. So
Um, okay. Precon LR. The last one. So
there's also this precondinet scale. So
there's also this precondinet scale. So
just reduce this to 0. Oh, it's 0.1. No,
just reduce this to 0. Oh, it's 0.1. No,
no, no. That that Yeah. Yeah. Uh
no, no. That that Yeah. Yeah. Uh
yeah. Z.
There's also that scale
and that that I think it was just added
and that that I think it was just added
into
into
um I don't know if he's pushed it, but
um I don't know if he's pushed it, but
he's seeing like much much better
he's seeing like much much better
results with the way that Zealand
results with the way that Zealand
automatically sets it. Um well, we can
automatically sets it. Um well, we can
definitely try that. So, here's the cool
definitely try that. So, here's the cool
thing with Puffer, right? Is a as a
thing with Puffer, right? Is a as a
benchmark for this type of stuff. Um
benchmark for this type of stuff. Um
I and I'm sure you've seen this, but
I and I'm sure you've seen this, but
like we have all these apps and they're
like we have all these apps and they're
fast and we actually have more in dev,
fast and we actually have more in dev,
some of which are pretty cool as well.
some of which are pretty cool as well.
So like if it just works on all these,
So like if it just works on all these,
it's probably pretty good.
it's probably pretty good.
And like most of these will train in a
And like most of these will train in a
minute.
minute.
Yeah. Some of these, you know, maybe
Yeah. Some of these, you know, maybe
some it's like five minutes, but the
some it's like five minutes, but the
only ones that are like really hard and
only ones that are like really hard and
take quite a bit, it's like neural MMO 3
take quite a bit, it's like neural MMO 3
is pretty hard. Other than that, you can
is pretty hard. Other than that, you can
like
like
kind of just go fast.
kind of just go fast.
Um, what are your thoughts on all these
Um, what are your thoughts on all these
I mean new again I know we're worried
I mean new again I know we're worried
and pissed about me saying this not
and pissed about me saying this not
pissed but um what are your thoughts on
pissed but um what are your thoughts on
like Jerio and all that stuff do you
like Jerio and all that stuff do you
think of language model specific? Yeah,
think of language model specific? Yeah,
for sure. Right. But um do you think
for sure. Right. But um do you think
that you would ever consider putting
that you would ever consider putting
like a language model into Puffer and
like a language model into Puffer and
just play around with the different RL
just play around with the different RL
algorithms whether they're specific or
algorithms whether they're specific or
not to LMS?
not to LMS?
I mean, I don't see the point. It's like
I mean, I don't see the point. It's like
an easy problem. If they can't figure
an easy problem. If they can't figure
that out, I don't know what to say
that out, I don't know what to say
because it's like you have a tenth of
because it's like you have a tenth of
the number of problems. The only thing
the number of problems. The only thing
that's hard, like really the only thing
that's hard, like really the only thing
that's hard about language models is the
that's hard about language models is the
uh getting like very performant
uh getting like very performant
distributed training.
Like there's this whole extra stack that
Like there's this whole extra stack that
you have to deal with to make stuff
you have to deal with to make stuff
fast. You get like multi-million step
fast. You get like multi-million step
per second training. None of the stuff I
per second training. None of the stuff I
do in puffer matters if your model
do in puffer matters if your model
trains like 0.5 steps per second, right?
trains like 0.5 steps per second, right?
Yeah, it's fair. Like GRPO is a one-step
Yeah, it's fair. Like GRPO is a one-step
problem. like it's being used for
problem. like it's being used for
one-step problems. And now like you know
one-step problems. And now like you know
there are these papers coming out for a
there are these papers coming out for a
multi-turn learning. So they're like a
multi-turn learning. So they're like a
few steps in the future which is
few steps in the future which is
actually good. Like that's how you would
actually good. Like that's how you would
use RL on a language model and I think
use RL on a language model and I think
that there's actually a good chance that
that there's actually a good chance that
that does really great stuff cuz RL is
that does really great stuff cuz RL is
really powerful. Um but that's not
really powerful. Um but that's not
really my concern because people are
really my concern because people are
going to do that stuff with or without
going to do that stuff with or without
me, right? My concern is mainly like how
me, right? My concern is mainly like how
do we make sure that all the research
do we make sure that all the research
that really matters for RL and RL
that really matters for RL and RL
methods to be good that gets done
methods to be good that gets done
because nobody else is doing that.
because nobody else is doing that.
This is how I view stuff, right? Like
This is how I view stuff, right? Like
like multi you're saying multi-step on
like multi you're saying multi-step on
LMS. Is that what you're saying? There
LMS. Is that what you're saying? There
are a couple people doing that now. But
are a couple people doing that now. But
like I mean it's taken so freaking long.
like I mean it's taken so freaking long.
Like I don't even know how it takes you
Like I don't even know how it takes you
this long, right? It's been like three
this long, right? It's been like three
years or whatever. two years maybe. I
years or whatever. two years maybe. I
mean, still you have how many billion
mean, still you have how many billion
dollars in this and you and like
dollars in this and you and like
everybody goes nuts over one stop like
everybody goes nuts over one stop like
one step RHF followed by one step RL.
one step RHF followed by one step RL.
Like come on.
Like come on.
No, I I agree. So, I'm not pissed about
No, I I agree. So, I'm not pissed about
it at all. It's like, yeah, good. You
it at all. It's like, yeah, good. You
should be doing this stuff. If anything,
should be doing this stuff. If anything,
it's kind of embarrassing that it took
it's kind of embarrassing that it took
that long. But, you know, I get it. The
that long. But, you know, I get it. The
reason it takes long is because your
reason it takes long is because your
experiments are so bloody massive that
experiments are so bloody massive that
you can't run very many of them. So what
you can't run very many of them. So what
do I do instead of running like you know
do I do instead of running like you know
one experiment which is all I'd be able
one experiment which is all I'd be able
to do with the hardware I have right
to do with the hardware I have right
I've made puffer so now I can run I've
I've made puffer so now I can run I've
run 25,000 RL experiments uh since the
run 25,000 RL experiments uh since the
start of the year
start of the year
right so my concern is really more how
right so my concern is really more how
do we develop RL as a core tech right
do we develop RL as a core tech right
how do we make sure core RL makes
how do we make sure core RL makes
progress and I cover everything but
progress and I cover everything but
language models for the time being
language models for the time being
and there's So many like there are so
and there's So many like there are so
many problems in industry where language
many problems in industry where language
models just don't make any sense. So I
models just don't make any sense. So I
mean they're too big. The priors that
mean they're too big. The priors that
they give you are not useful because the
they give you are not useful because the
problems are really unintuitive. You
problems are really unintuitive. You
just need to train from scratch and it
just need to train from scratch and it
needs to be good and it needs to be
needs to be good and it needs to be
fast. That's what I'm looking for. Okay.
fast. That's what I'm looking for. Okay.
By the way, that matches the uh I don't
By the way, that matches the uh I don't
see any noticeable difference in the
see any noticeable difference in the
curve yet. We'll see. But it kind of
curve yet. We'll see. But it kind of
looks flat.
looks flat.
Interesting.
Uh I guess turn off Mars maybe and then
Uh I guess turn off Mars maybe and then
reduce it even more.
That's what
That's what
that is that if so if that
like if that doesn't wait was that cron
like if that doesn't wait was that cron
or was that cash? No, that must have
or was that cash? No, that must have
been cron, right? Craw. I didn't see any
been cron, right? Craw. I didn't see any
difference with cash.
difference with cash.
But it's not new, right?
What I ran this with before was Mars
What I ran this with before was Mars
true pre-rown 0.01. Okay. Okay. But you
true pre-rown 0.01. Okay. Okay. But you
you you comped it up the top line, I
you you comped it up the top line, I
guess. Yes.
Commented color scheme included with our
Commented color scheme included with our
Docker container.
But yeah, Neural MMO 3 is like a really
But yeah, Neural MMO 3 is like a really
hard and interesting app. Like you can
hard and interesting app. Like you can
play it right here, right? If you just
play it right here, right? If you just
hit control, you can take over and you
hit control, you can take over and you
can play
can play
it. So I'm taking over now for this guy.
it. So I'm taking over now for this guy.
And
And
like you see that you will see that it
like you see that you will see that it
takes quite a while to like figure out
takes quite a while to like figure out
how to even do anything in this. And
how to even do anything in this. And
like there's so much stuff that just
like there's so much stuff that just
like the delay is very long what you
like the delay is very long what you
have to do. So I have to go kill this
have to do. So I have to go kill this
enemy. I have to equip this tool. I have
enemy. I have to equip this tool. I have
to go find something that I can actually
to go find something that I can actually
harvest like this sword. I can go pick
harvest like this sword. I can go pick
up this
up this
sword. And then I have to like unequip
sword. And then I have to like unequip
this, equip this. And now I can go find
this, equip this. And now I can go find
something like very slightly higher
something like very slightly higher
level and maybe go kill that. Um, and
level and maybe go kill that. Um, and
there's like a market you can trade with
there's like a market you can trade with
other agents. There are there's like
other agents. There are there's like
armor and equipment and weapons and
armor and equipment and weapons and
consumables. And then uh if I let this
consumables. And then uh if I let this
agent just take back over the map looks
agent just take back over the map looks
like this is
like this is
big. There's a lot of stuff that can go
big. There's a lot of stuff that can go
on in here. Really nice benchmark for
on in here. Really nice benchmark for
this stuff. Um, way harder than anything
this stuff. Um, way harder than anything
else.
else.
That's an atom. What does the atom curve
That's an atom. What does the atom curve
look like? Uh, hang on. Let me get rid
look like? Uh, hang on. Let me get rid
of this bot. Go. This is
Muan. So, this is this is Muan. And
Muan. So, this is this is Muan. And
actually, this is not the best metric.
actually, this is not the best metric.
The best metric is this mincom prof.
The best metric is this mincom prof.
This is like how many levels you
This is like how many levels you
get. Um, so this looks really good,
get. Um, so this looks really good,
right? Um but now if I
right? Um but now if I
just do
this. So this is the learning curve.
this. So this is the learning curve.
Very smooth. And then if I go find the
Very smooth. And then if I go find the
wand curve for
wand curve for
you. Where's the womb
you. Where's the womb
curve? Man, I thought I had it. Oh, here
curve? Man, I thought I had it. Oh, here
it is. Nope. Yeah, this is the wombi
it is. Nope. Yeah, this is the wombi
curve. So it gets up to
curve. So it gets up to
about what was that 2.5. This is about
about what was that 2.5. This is about
1500 years worth of games played. 73
1500 years worth of games played. 73
billion steps.
billion steps.
Yeah. There's about 1500 years worth of
Yeah. There's about 1500 years worth of
games played.
Wait, is that like in game? Yeah. Yeah.
Wait, is that like in game? Yeah. Yeah.
If you were to like if you were to play
If you were to like if you were to play
the game in real time, this is 1500
the game in real time, this is 1500
years worth of game.
And then this one here, this
And then this one here, this
is this is about a thousand years worth
is this is about a thousand years worth
of game, little less.
of game, little less.
That's kind of absurd, isn't it? I
That's kind of absurd, isn't it? I
started this run on one GPU less than 24
started this run on one GPU less than 24
hours ago.
Kind of crazy. Yeah. So it's very fast
Kind of crazy. Yeah. So it's very fast
and like you know you can kind of learn
and like you know you can kind of learn
some pretty complex stuff because it's
some pretty complex stuff because it's
like oh what can you learn with a small
like oh what can you learn with a small
model on one GPU? It's like well when
model on one GPU? It's like well when
your x-axis is in tens of billions I
your x-axis is in tens of billions I
mean you can go look at the large scale
mean you can go look at the large scale
industry papers and very very few of
industry papers and very very few of
them have an x-axis in the tens of
them have an x-axis in the tens of
billions right? Yeah.
billions right? Yeah.
So you can kind of do a
So you can kind of do a
lot. Let's see where this graph is.
All right. So, your blue one here, this
All right. So, your blue one here, this
is your blue one.
is your blue one.
Okay, let's see how she does. So, it
Okay, let's see how she does. So, it
starts off way lower. Still better than
starts off way lower. Still better than
Muan, but then it'll see. We'll see how
Muan, but then it'll see. We'll see how
it converges. What's this thing that
it converges. What's this thing that
Evans said like, "Oh, learning fast is
Evans said like, "Oh, learning fast is
bad."
bad."
Yeah. So,
Yeah. So,
that seems like a screw up. No, no, no.
that seems like a screw up. No, no, no.
So there's there's a concept called
So there's there's a concept called
neuroplasticity. Yeah, I'm familiar. But
neuroplasticity. Yeah, I'm familiar. But
it's kind of stupid that that's a thing.
it's kind of stupid that that's a thing.
I'll say
Basically, I can explain what they did.
Basically, I can explain what they did.
They took um a
CNM and
CNM and
they for the first like sci-fi 10,
they for the first like sci-fi 10,
right? They train it for the first like
right? They train it for the first like
five epochs with blur. First 10 epochs
five epochs with blur. First 10 epochs
of blur, first 30 epox, first 50 epochs
of blur, first 30 epox, first 50 epochs
of blur, and then after. Yeah. No, I'm
of blur, and then after. Yeah. No, I'm
familiar with neuroplasticity, right?
familiar with neuroplasticity, right?
I'm familiar with that line of work. Um,
I'm familiar with that line of work. Um,
okay.
okay.
Oh, this doesn't seem like it helps.
Oh, this doesn't seem like it helps.
Like, this goes up and then goes back
Like, this goes up and then goes back
down immediately.
down immediately.
Yeah. I mean, let's see what happens.
Muan was pretty good here. I'm
Muan was pretty good here. I'm
surprised. Um, I mean I like the other
surprised. Um, I mean I like the other
thing is manually tuning hyperparameters
thing is manually tuning hyperparameters
is kind of dumb. Like I probably could
is kind of dumb. Like I probably could
just sweep this. Well, what do I sweep?
just sweep this. Well, what do I sweep?
Do I just sweep the the precondition
Do I just sweep the the precondition
learning rate and the learning rate or
learning rate and the learning rate or
do I sweep other stuff?
Let's see what
else. And we have a lot of fun stuff
else. And we have a lot of fun stuff
that we can play with. Yeah. Yeah,
that we can play with. Yeah. Yeah,
exactly.
exactly.
Unfortunately, these sweeps do take a
Unfortunately, these sweeps do take a
little time because, you know, big run,
little time because, you know, big run,
but yeah, all the other sweeps don't
but yeah, all the other sweeps don't
take much at all.
Okay. Can you also do like binary like
Okay. Can you also do like binary like
true false? Yeah. Right. Can sweep? I
true false? Yeah. Right. Can sweep? I
haven't, but we probably should. Yeah,
haven't, but we probably should. Yeah,
we could. Well, you just want Mars. I
we could. Well, you just want Mars. I
just run two sweeps.
Mind you, don't we don't have all our
Mind you, don't we don't have all our
nice hardware yet. Um 5090 test machine
nice hardware yet. Um 5090 test machine
should be in sometime this
should be in sometime this
week. We order more of
them. Sorry, I think my internet there
them. Sorry, I think my internet there
for a sec.
Wait, can you hear me?
you there? Yeah. Yeah. Sorry, I think my
you there? Yeah. Yeah. Sorry, I think my
internet cut off for a second. Yeah,
internet cut off for a second. Yeah,
you're good. Um I probably would also So
you're good. Um I probably would also So
there's this other setting which says
um Okay. So are you filled with caution?
um Okay. So are you filled with caution?
Not as it relates to optimizers.
Not as it relates to optimizers.
Okay. Yes. Okay. So caution is set to
Okay. Yes. Okay. So caution is set to
false. You can try toggling caution true
false. You can try toggling caution true
for both MUN and the SGDA. It basically
for both MUN and the SGDA. It basically
says if your gradient and your update,
says if your gradient and your update,
right, have sign changes, ignore them.
right, have sign changes, ignore them.
set that set that dimension to zero. Um,
set that set that dimension to zero. Um,
caution is interesting to use. There's
caution is interesting to use. There's
also if you do enable Mars, there's a
also if you do enable Mars, there's a
Mars gamma. There's the momentum.
Mars gamma. There's the momentum.
There's also this option that says
There's also this option that says
momentum into precon gradient. Okay, you
momentum into precon gradient. Okay, you
can set that to true and false. Oh my
can set that to true and false. Oh my
gosh, you guys have added so many
gosh, you guys have added so many
freaking hyperparameters.
freaking hyperparameters.
Yes, sir.
Yes, sir.
Um, so yeah. Is there a comprehensive
Um, so yeah. Is there a comprehensive
is there like a comprehensive thing on
is there like a comprehensive thing on
this versus um versus Muan anywhere?
this versus um versus Muan anywhere?
So okay. So if you use heavy ball, I
So okay. So if you use heavy ball, I
think every single optimizer has all
think every single optimizer has all
these type of parameters.
these type of parameters.
Yeah.
Yeah.
Uh so this is this is Lucas's idea is it
Uh so this is this is Lucas's idea is it
should all compile and it should all be
should all compile and it should all be
so functional that they can all just you
so functional that they can all just you
you could use any mix of optimizers
you could use any mix of optimizers
together.
together.
Um, you're asking for a comprehensive
Um, you're asking for a comprehensive
understanding of Muan versus I'm asking
understanding of Muan versus I'm asking
if they're experimental results, right?
If they're experimental results,
If they're experimental results,
experimental results. So, like the thing
experimental results. So, like the thing
that I went went for for Muan, right, is
that I went went for for Muan, right, is
uh people were trying pretty hard for a
uh people were trying pretty hard for a
bit to get the uh the GPT speedrun
bit to get the uh the GPT speedrun
to get that working, right? So I saw
to get that working, right? So I saw
that Muon was good on there and I said,
that Muon was good on there and I said,
"Oh, okay. Maybe that's useful." And
"Oh, okay. Maybe that's useful." And
then I went and I tried Muan on a bunch
then I went and I tried Muan on a bunch
of MS and it was, you know, it was doing
of MS and it was, you know, it was doing
uh and after I ran a sweep for hyperpan
uh and after I ran a sweep for hyperpan
was doing better than the other ones,
was doing better than the other ones,
right? Than Adam, it was doing uh
right? Than Adam, it was doing uh
solving faster. And then I tried this
solving faster. And then I tried this
thing. I tried
thing. I tried
PSGD cron and it matched muon just with
PSGD cron and it matched muon just with
the defaults on breakout out of the
the defaults on breakout out of the
box and it's did better on mobile
box and it's did better on mobile
versus not fully tuned muon params but
versus not fully tuned muon params but
like good
like good
default did better
default did better
there and uh the only thing it hasn't
there and uh the only thing it hasn't
done well so far is neural MMO it gets
done well so far is neural MMO it gets
stuck so I don't know if there's
stuck so I don't know if there's
something weird with the optimizer that
something weird with the optimizer that
causes that or what but uh Muan doesn't
causes that or what but uh Muan doesn't
get stuck stuck. Muan has a very very
get stuck stuck. Muan has a very very
smooth learning rate uh learning curve.
smooth learning rate uh learning curve.
Yeah. Um so we have like our own
Yeah. Um so we have like our own
internal results and then Stanford also
internal results and then Stanford also
has some results of PSG versus Mulan.
Um but this would definitely like the
Um but this would definitely like the
stuff you're doing is I mean I've been
stuff you're doing is I mean I've been
trying to get PSGD into Puffer for some
trying to get PSGD into Puffer for some
time now and I just haven't had you
time now and I just haven't had you
know. Yeah.
know. Yeah.
um with well here it is. We'll see if it
um with well here it is. We'll see if it
takes off at some point but it's just
takes off at some point but it's just
intersected with Muan.
intersected with Muan.
Yeah. So it we're really reducing the LR
Yeah. So it we're really reducing the LR
of the threeon 01. Um we can also try
of the threeon 01. Um we can also try
reducing the momentum for example. That
reducing the momentum for example. That
might help. Another thing that I have to
might help. Another thing that I have to
check if it's in heavy ball or not that
check if it's in heavy ball or not that
Zealand does differently than Evan.
Zealand does differently than Evan.
Zealand
Zealand
um trusts the gradients. Evan trusts the
um trusts the gradients. Evan trusts the
momentum,
momentum,
right? So that was if if if there was a
right? So that was if if if there was a
um if there was a like a contribution
um if there was a like a contribution
Evan has made it was that he he said hey
Evan has made it was that he he said hey
let's try to trust the momentum term uh
let's try to trust the momentum term uh
which I also had done but but he he kind
which I also had done but but he he kind
of did it he showed that it helps on LMS
of did it he showed that it helps on LMS
a lot um but maybe for RL it it hurts
a lot um but maybe for RL it it hurts
um so instead of passing the the
um so instead of passing the the
momentum term into the preconditioner
momentum term into the preconditioner
preconditioning that you can just
preconditioning that you can just
directly pass the gradients uh which is
directly pass the gradients uh which is
what Z1's code name does. Um
what Z1's code name does. Um
I have to look at that. The thing is
I have to look at that. The thing is
this isn't even an optimized muon,
this isn't even an optimized muon,
right? This is I um I optimized muon on
right? This is I um I optimized muon on
uh breakout and then I just use some of
uh breakout and then I just use some of
those params like the atom betas and
those params like the atom betas and
stuff. I use them here. So I don't even
stuff. I use them here. So I don't even
think this is like an optimized muon for
think this is like an optimized muon for
this. I'm It's a little weird that like
this. I'm It's a little weird that like
qualitatively the behavior is it gets
qualitatively the behavior is it gets
stuck that early.
stuck that early.
Yeah. I don't know why that would
Yeah. I don't know why that would
happen. Um I don't like I don't actually
happen. Um I don't like I don't actually
know the math behind this versus muon.
know the math behind this versus muon.
Something's very different here.
Yeah. So I've reduced the learning rate
Yeah. So I've reduced the learning rate
of the preconditioner by a lot. So it's
of the preconditioner by a lot. So it's
going to take a while too. I basic.
going to take a while too. I basic.
Okay. So now this is producing it by
Okay. So now this is producing it by
like two orders of magnitude. You can
like two orders of magnitude. You can
also increase it by order of magnitude.
also increase it by order of magnitude.
So
So
um but I would I would I might throw
um but I would I would I might throw
cards at it or at it. Okay. Well, I'll
cards at it or at it. Okay. Well, I'll
run I'll just run a sweep then. I'll
run I'll just run a sweep then. I'll
sweep some stuff and I'll see if this
sweep some stuff and I'll see if this
feels like this can do anything cuz I
feels like this can do anything cuz I
also had I did start a a muon
also had I did start a a muon
sweep on. It's a little annoying to get
sweep on. It's a little annoying to get
the thing to actually like consistently
the thing to actually like consistently
run experiments that are like this long,
run experiments that are like this long,
but
so let's zoom
in. Yeah. So like we
in. Yeah. So like we
have there like some results here. This
have there like some results here. This
one looks pretty
one looks pretty
good.
good.
02. This is all this is above um where
02. This is all this is above um where
PSPD got stuck I think
here. So there's definitely sensitivity.
here. So there's definitely sensitivity.
Yeah. Yeah, that's for sure. But it's
Yeah. Yeah, that's for sure. But it's
way less than before. Like the atom
way less than before. Like the atom
sensitivity is crazy by comparison.
All right. Well, I'll let this run. I
All right. Well, I'll let this run. I
got to go get dinner. Thanks for the
got to go get dinner. Thanks for the
help on this. Okay. And yeah, I think
help on this. Okay. And yeah, I think
just generally if you want to do optim
just generally if you want to do optim
stuff, right, we can just kind of use
stuff, right, we can just kind of use
puffer as a benchmark and then also use,
puffer as a benchmark and then also use,
you know, the algos for puffer. Um cuz
you know, the algos for puffer. Um cuz
this has been like a major thing. I mean
this has been like a major thing. I mean
this is I the cosign and healing was
this is I the cosign and healing was
also pretty major. But um I think
also pretty major. But um I think
overall this has been the largest thing
overall this has been the largest thing
in I I think this is like going to be
in I I think this is like going to be
the biggest RL advancement in the in
the biggest RL advancement in the in
this year so far. I mean, I know Evan's
this year so far. I mean, I know Evan's
been running this stuff, but I he hasn't
been running this stuff, but I he hasn't
been running it on all these different M
been running it on all these different M
it seems because like qualitatively I
it seems because like qualitatively I
can tell you this is a step change in
can tell you this is a step change in
capabilities in terms of just how janky
capabilities in terms of just how janky
everything is. I don't know if he was
everything is. I don't know if he was
trying to run like all the arcade games
trying to run like all the arcade games
with the same sets of parameters and
with the same sets of parameters and
stuff, but like if you try to run all
stuff, but like if you try to run all
the different puffs with the same set of
the different puffs with the same set of
atom params, they just don't work. Like
atom params, they just don't work. Like
they're very different tasks. It's not
they're very different tasks. It's not
like you just have a bunch of arcade
like you just have a bunch of arcade
games, right? Um, yeah. I mean, I trying
games, right? Um, yeah. I mean, I trying
to I've been trying to get PSGD into I
to I've been trying to get PSGD into I
mean it's if if you go look PSG 2015
mean it's if if you go look PSG 2015
puts puts this out, right? Like Mulan
puts puts this out, right? Like Mulan
was
was
made two months ago, three months
made two months ago, three months
ago after I I kind of was pushing for
ago after I I kind of was pushing for
whitening based optimizers for like a
whitening based optimizers for like a
year and a half. Is there a I mean I got
year and a half. Is there a I mean I got
to look at the math difference between
to look at the math difference between
then Muan and PSGD to see what they do
then Muan and PSGD to see what they do
differently. Um PSGD is a much wider
differently. Um PSGD is a much wider
framework. Right? So PSGD can do Mulan,
framework. Right? So PSGD can do Mulan,
it can do shampoo, it can do soap, it
it can do shampoo, it can do soap, it
can do kayak, it can do atom, it can do
can do kayak, it can do atom, it can do
you know all these different things. a
you know all these different things. a
bunch of different preconditioner
bunch of different preconditioner
shapes.
shapes.
Um,
Um,
and like way way more types of things
and like way way more types of things
than
than
than any other type of optimizer I think
than any other type of optimizer I think
in existence. Well, in that case, if
in existence. Well, in that case, if
it's, you know, if these things are
it's, you know, if these things are
parameterizations of PSPD, the question
parameterizations of PSPD, the question
is like, do you have one that's better
is like, do you have one that's better
than, you know, do you like you're going
than, you know, do you like you're going
to look for one that's
to look for one that's
You're going to look for a small set of
You're going to look for a small set of
hyper prams that are roughly stable
hyper prams that are roughly stable
across a wide range of problems ideally
across a wide range of problems ideally
or are relatively easy to tune.
or are relatively easy to tune.
Yeah. And I think also the
Yeah. And I think also the
preconditioner style, right? The shape
preconditioner style, right? The shape
of the preconditioner, right? This is a
of the preconditioner, right? This is a
chronicer factorized version of the
chronicer factorized version of the
preconditioner. But we also have low
preconditioner. But we also have low
rank approximations. We also have sketch
rank approximations. We also have sketch
approximations, right? We have all these
approximations, right? We have all these
different shapes. we can do like and the
different shapes. we can do like and the
point is let's find what's the best and
point is let's find what's the best and
I think buffer is really good for that
I think buffer is really good for that
because clearly you can run like 1500
because clearly you can run like 1500
15,000 years of game play yeah 24 hours
15,000 years of game play yeah 24 hours
right yeah it's pretty that's pretty fun
right yeah it's pretty that's pretty fun
that's out on one GPU we technically
that's out on one GPU we technically
have multiGPU support as well we've got
have multiGPU support as well we've got
one client who's tried it up to like 64
one client who's tried it up to like 64
GPUs and it does work
GPUs and it does work
Um, we don't have that hardware at the
Um, we don't have that hardware at the
moment. We're going to get some pretty
moment. We're going to get some pretty
soon, but in the meantime, like all the
soon, but in the meantime, like all the
simpler, they just run so fast. You just
simpler, they just run so fast. You just
you can do these in just a minute or
you can do these in just a minute or
whatever. Um,
whatever. Um,
how expensive is it to build a new app?
how expensive is it to build a new app?
What you mean, how expensive is it?
What you mean, how expensive is it?
Like let's say a client comes to you and
Like let's say a client comes to you and
says, "Hey, I want to get an agent that
says, "Hey, I want to get an agent that
works really really well on my setup.
works really really well on my setup.
You build me a simulator that would then
You build me a simulator that would then
transfer to the real world or
transfer to the real world or
something." Oh, I mean it it depends on
something." Oh, I mean it it depends on
what people want. It also depends on
what people want. It also depends on
like if the sims are going to be closed
like if the sims are going to be closed
source or not as well. Uh starting
source or not as well. Uh starting
though like puffer service packages if
though like puffer service packages if
you just want like priority service on
you just want like priority service on
features within puffer not new sims uh
features within puffer not new sims uh
and you just generally want assistance
and you just generally want assistance
with that that's 10k and then when you
with that that's 10k and then when you
start getting uh dedicated work on your
start getting uh dedicated work on your
stuff like if you want dedicated work on
stuff like if you want dedicated work on
a new sim or something then I get at
a new sim or something then I get at
least one contractor involved so that
least one contractor involved so that
starts at 20k a month and it goes up
starts at 20k a month and it goes up
depending on the amount of work that's
depending on the amount of work that's
needed cuz that's that's priced for like
needed cuz that's that's priced for like
quarter time work. So if you want people
quarter time work. So if you want people
working more hours on it, then it goes
working more hours on it, then it goes
up from there.
up from there.
20 grand a month.
20 grand a month.
Well, that's for me. That covers me plus
Well, that's for me. That covers me plus
a contractor.
a contractor.
Nice. And that's, you know, it's a slice
Nice. And that's, you know, it's a slice
of my time. Um, and then a contractor's
of my time. Um, and then a contractor's
quarter time.
quarter time.
Nice.
And how do you how do you find
And how do you how do you find
contractors to hire?
contractors to hire?
Um I mean one I had from a previous
Um I mean one I had from a previous
collaboration. Another one that we just
collaboration. Another one that we just
finished was a Twitter cold DM. And then
finished was a Twitter cold DM. And then
you know another one that I just started
you know another one that I just started
is from uh you know mutual acquaintance
is from uh you know mutual acquaintance
or mutual mutual colleague. Um and then
or mutual mutual colleague. Um and then
you know we've had a few others that
you know we've had a few others that
haven't like that haven't gone through
haven't like that haven't gone through
all the way that have been from either
all the way that have been from either
cold uh cold contact. Uh I mean we had
cold uh cold contact. Uh I mean we had
uh a few leads from GDC that almost
uh a few leads from GDC that almost
worked but I think you know Puffer has
worked but I think you know Puffer has
to get a little bit bigger to be landing
to get a little bit bigger to be landing
the larger companies um because it's
the larger companies um because it's
new. But yeah, stuff it's it's generally
new. But yeah, stuff it's it's generally
this is just there's service as a
this is just there's service as a
service and we're trying to make RL
service and we're trying to make RL
really really fast and we've got a bunch
really really fast and we've got a bunch
of good tools and they're all free and
of good tools and they're all free and
they're open source so you can just use
they're open source so you can just use
them and we actually encourage people to
them and we actually encourage people to
go ahead and use them first to make sure
go ahead and use them first to make sure
that they you know they see what we have
that they you know they see what we have
but you know if you use them and you're
but you know if you use them and you're
getting some good results well why
getting some good results well why
wouldn't you want to have us involved
wouldn't you want to have us involved
and on your side to make all your stuff
and on your side to make all your stuff
work way way way better right
work way way way better right
yeah makes sense I mean I have clients
yeah makes sense I mean I have clients
right where it's like ah I know that
right where it's like ah I know that
this thing that I was just developing on
this thing that I was just developing on
the algo side is going to be perfect for
the algo side is going to be perfect for
your end and I'm just like throw it on
your end and I'm just like throw it on
that immediately right otherwise you'd
that immediately right otherwise you'd
have to wait like you know you'd either
have to wait like you know you'd either
have to be watching the dev branch like
have to be watching the dev branch like
a hawk basically doing what I do anyways
a hawk basically doing what I do anyways
uh or you have to wait until everything
uh or you have to wait until everything
comes out and it's still not out of the
comes out and it's still not out of the
box it's RL it's hard right
but yeah I mean the goal longer term
but yeah I mean the goal longer term
right is that as we make RL more stable
right is that as we make RL more stable
and more consistent we spread throughout
and more consistent we spread throughout
a whole bunch of different industries
a whole bunch of different industries
that have sims that need optimizing and
that have sims that need optimizing and
uh you know we start taking on harder
uh you know we start taking on harder
and harder and more and more valuable
and harder and more and more valuable
problems with it and then the prices
problems with it and then the prices
will obviously go up from there, right?
will obviously go up from there, right?
Like we're not going to make somebody
Like we're not going to make somebody
$100 million for uh for 10 20k a month.
$100 million for uh for 10 20k a month.
Yeah. So you you actually did some work
Yeah. So you you actually did some work
with OpenAI, right? I I interned there
with OpenAI, right? I I interned there
in undergrad. I took Neural MMO there
in undergrad. I took Neural MMO there
and then I built the 10 version out from
there. It was like 2018, I think. It was
there. It was like 2018, I think. It was
cool. That was when they were doing
cool. That was when they were doing
Dota. It was really cool to see that.
Dota. It was really cool to see that.
Yes. I mean, what h happened with those
Yes. I mean, what h happened with those
like with that lead or those contacts,
like with that lead or those contacts,
right? like it it would make a lot of
right? like it it would make a lot of
sense, right? But whereas open not just
sense, right? But whereas open not just
they're not doing that anymore
they're not doing that anymore
unfortunately.
You know, they like the Dota stuff, if
You know, they like the Dota stuff, if
they kept doing it, RL wouldn't be in
they kept doing it, RL wouldn't be in
such a bad spot right now where I have
such a bad spot right now where I have
to be doing all this stuff cuz the Dota
to be doing all this stuff cuz the Dota
team, I think more than anybody, they
team, I think more than anybody, they
were crushing it in reinforcement
were crushing it in reinforcement
learning.
learning.
They had the hardest problem.
But they I mean they they like destroyed
But they I mean they they like destroyed
like the top teams, right? Yeah. But
like the top teams, right? Yeah. But
they like
they like
so they could have then tried to go and
so they could have then tried to go and
do it with 1% of the compute. They
do it with 1% of the compute. They
discussed doing that but they didn't.
discussed doing that but they didn't.
They kind of all moved on to language
They kind of all moved on to language
model stuff, right? They could have done
model stuff, right? They could have done
so so much work around that problem
so so much work around that problem
because they really they solved Dota
because they really they solved Dota
using very simple methods, right? There
using very simple methods, right? There
was so much room for ablations like what
was so much room for ablations like what
really matters to scale on these hard
really matters to scale on these hard
problems. There was so much room for
problems. There was so much room for
that and frankly a lot of the stuff in
that and frankly a lot of the stuff in
the Dota paper if you saw if you like
the Dota paper if you saw if you like
really go through the appendix is stuff
really go through the appendix is stuff
that people would keep rediscovering and
that people would keep rediscovering and
not really learning the lessons from for
not really learning the lessons from for
the next I mean even through till
the next I mean even through till
today. I don't know how many times I
today. I don't know how many times I
have to tell people like who are trying
have to tell people like who are trying
to throw a transformer on a super basic
to throw a transformer on a super basic
problem that a one layer LSTM solves
problem that a one layer LSTM solves
Dota.
Yeah. I mean people think it's not
Yeah. I mean people think it's not
possible to solve X problem without like
possible to solve X problem without like
fancier method fancier architecture.
fancier method fancier architecture.
It's like no one layer LSTM solves Dota
It's like no one layer LSTM solves Dota
say oh but they used a ton of compute.
say oh but they used a ton of compute.
Yeah they use a ton of compute for back
Yeah they use a ton of compute for back
then. It's not that much compute at all
then. It's not that much compute at all
by today's standards and it's a way
by today's standards and it's a way
harder problem than most people are
harder problem than most people are
dealing with.
dealing with.
Okay. So
then why don't you want to try to attack
then why don't you want to try to attack
language? Because everyone's doing
language? Because everyone's doing
language already. It'll get done with or
language already. It'll get done with or
without me.
without me.
Nobody's doing this, right? Nobody's
Nobody's doing this, right? Nobody's
doing I know, but clearly clearly like
doing I know, but clearly clearly like
the way that they're doing the language
the way that they're doing the language
is just by scaling up. Do you think I
is just by scaling up. Do you think I
guess I guess the right question is do
guess I guess the right question is do
you think that we could solve language
you think that we could solve language
in a way that is, you know, cheaper,
in a way that is, you know, cheaper,
faster, you
faster, you
know, smaller models. If you're just
know, smaller models. If you're just
using, you know, something better than
using, you know, something better than
next token prediction,
next token prediction,
next token prediction is pretty good.
next token prediction is pretty good.
The problem is, so that's like a good
The problem is, so that's like a good
base is the thing, but then you need to
base is the thing, but then you need to
be able to learn through interaction,
be able to learn through interaction,
which is what RL gives you. But like the
which is what RL gives you. But like the
thing is, labs are going to do that
thing is, labs are going to do that
anyways, and they're going to throw way
anyways, and they're going to throw way
more money at it and way more resources
more money at it and way more resources
at it than I can possibly muster, right?
at it than I can possibly muster, right?
The thing is like RL's been thrown to
The thing is like RL's been thrown to
the wayside and like a general purpose
the wayside and like a general purpose
RL has been thrown to the wayside and I
RL has been thrown to the wayside and I
can make so many so many core
can make so many so many core
contribution and advancements by doing
contribution and advancements by doing
stuff the way I have in uh in puffer
stuff the way I have in uh in puffer
right like I'm finding stuff that people
right like I'm finding stuff that people
haven't found in the last decade because
haven't found in the last decade because
I can just run experiments a thousand
I can just run experiments a thousand
times faster than them right like I keep
times faster than them right like I keep
finding the vast majority of published
finding the vast majority of published
RL methods are straight up wrong because
RL methods are straight up wrong because
they didn't bother tuning
they didn't bother tuning
hyperparameters because their M's were
hyperparameters because their M's were
too slow to do it I can actually do that
too slow to do it I can actually do that
right? I can run exhaustive experiments.
right? I can run exhaustive experiments.
I can run ablations and I can do them
I can run ablations and I can do them
very very quickly. Which is why you see
very very quickly. Which is why you see
a lot of the techniques in Puffer,
a lot of the techniques in Puffer,
right? They come from the just
right? They come from the just
ridiculously rapid speed of iteration.
Yeah,
Yeah,
that's true. I don't know. I just don't
that's true. I don't know. I just don't
really have any interest in like jumping
really have any interest in like jumping
on the general LLM bandwagon cuz like
on the general LLM bandwagon cuz like
what am I going to do? I'm going to try
what am I going to do? I'm going to try
to help them like leaprog the next other
to help them like leaprog the next other
LLM company out there by a couple
LLM company out there by a couple
months, right? Like they're not really
months, right? Like they're not really
all that interesting. But if I can just
all that interesting. But if I can just
put all of RL as a field forward, that's
put all of RL as a field forward, that's
going to help everything.
Yeah, that's
um I
um I
guess you said you have to Yeah, one
guess you said you have to Yeah, one
last thing. your your your clients. Who
last thing. your your your clients. Who
who are they mostly? Like are they
who are they mostly? Like are they
robotics companies? Like I only have I
robotics companies? Like I only have I
mean we only have a couple at a time
mean we only have a couple at a time
typically. Um because I I mostly focus
typically. Um because I I mostly focus
still on a lot of dev around Puffer. So
still on a lot of dev around Puffer. So
I try to save a fair bit of my time for
I try to save a fair bit of my time for
that. But uh so far it's been random.
that. But uh so far it's been random.
Like it's we did stuff in animation gen.
Like it's we did stuff in animation gen.
We do stuff with there's another
We do stuff with there's another
research lab. Um stuff in finance now.
research lab. Um stuff in finance now.
Like and I think that going forward
Like and I think that going forward
there's going to just be a ton of random
there's going to just be a ton of random
stuff in manufacturing as well. Just
stuff in manufacturing as well. Just
random industrial processes that are
random industrial processes that are
unintuitive and need to be optimized.
unintuitive and need to be optimized.
Like everybody's got an optimization
Like everybody's got an optimization
problem, right? And a ton of them
problem, right? And a ton of them
actually can build sims for them. Good
actually can build sims for them. Good
Sims. So RL's I think going to really
Sims. So RL's I think going to really
really improve just a ton of these
really improve just a ton of these
random processes where language models
random processes where language models
like the intuition you get from language
like the intuition you get from language
doesn't help you solve arbitrary
doesn't help you solve arbitrary
optimization problems, right? But RL
optimization problems, right? But RL
does.
does.
Yeah. I mean, could you do like you said
Yeah. I mean, could you do like you said
finance, would you like do like bio
finance, would you like do like bio
frequency trading or like Yeah. And like
frequency trading or like Yeah. And like
I can't The thing is a lot of the
I can't The thing is a lot of the
clients they have they have NDAs on like
clients they have they have NDAs on like
I you can't say exactly what they're
I you can't say exactly what they're
doing, but I mean I don't have the thing
doing, but I mean I don't have the thing
is I don't have I don't have interest in
is I don't have I don't have interest in
any specific area, right? I don't really
any specific area, right? I don't really
have a vested interest in any specific
have a vested interest in any specific
area. It's more so like RL in general.
area. It's more so like RL in general.
What can we solve with this? Let's throw
What can we solve with this? Let's throw
it on a ton of stuff. Let's just bring
it on a ton of stuff. Let's just bring
all of Puffer's tools, right? To try to
all of Puffer's tools, right? To try to
see if this can actually work on all
see if this can actually work on all
these different problems. And then
these different problems. And then
anything we learn comes back to Puffer,
anything we learn comes back to Puffer,
right? Anything that we learn in the
right? Anything that we learn in the
process about core methods comes back to
process about core methods comes back to
Puffer. So the more different types of
Puffer. So the more different types of
problems that we get to see and the more
problems that we get to see and the more
different types of problems we get to
different types of problems we get to
throw Puffer at, the better Puffer gets.
So it's basically like a work study
So it's basically like a work study
program in college.
Well, it's more so that it's more so
Well, it's more so that it's more so
that we do work for people, but we end
that we do work for people, but we end
up keeping a lot of the code open source
up keeping a lot of the code open source
because it's actually to their benefit
because it's actually to their benefit
to keep it open source because any as
to keep it open source because any as
soon as it's an open source and puffer,
soon as it's an open source and puffer,
right, we get to test all of our new
right, we get to test all of our new
stuff against it and our users get to
stuff against it and our users get to
test all their stuff against it. So like
test all their stuff against it. So like
you get so much free progress on your
you get so much free progress on your
thing by keeping it open source and
thing by keeping it open source and
puffer. that's it's really to your
benefit. Like who cares if you have a
benefit. Like who cares if you have a
simulator, right? If your simulator is
simulator, right? If your simulator is
open source, like you still have a
open source, like you still have a
business around it, right? And you're
business around it, right? And you're
still the first person to own that
still the first person to own that
business around it. Um there's so much
business around it. Um there's so much
other than just the sim or just that
other than just the sim or just that
code. So, and if you know, if you're
code. So, and if you know, if you're
able to move faster because the stuff is
able to move faster because the stuff is
open source and tougher, then why
open source and tougher, then why
wouldn't you? You don't want to deal
wouldn't you? You don't want to deal
with RL. It's really freaking hard,
with RL. It's really freaking hard,
right? Let us deal with it.
Yeah.
Yeah. It's true. The other nice thing
Yeah. It's true. The other nice thing
about it, right, is because Puffer is
about it, right, is because Puffer is
all open source, it's really easy to
all open source, it's really easy to
work with us cuz companies are worried
work with us cuz companies are worried
about like, you know, third-party SAS
about like, you know, third-party SAS
like outsourcing capabilities behind
like outsourcing capabilities behind
like a closed source wall or behind like
like a closed source wall or behind like
a service model, pay wall. You don't
a service model, pay wall. You don't
have any of that, right? You just use
have any of that, right? You just use
the code. The code's really simple. you
the code. The code's really simple. you
can in-house some of the RL capabilities
can in-house some of the RL capabilities
if you're interested in doing that and
if you're interested in doing that and
you get all the updates for free. You're
you get all the updates for free. You're
just paying for you're just paying for
just paying for you're just paying for
assistance on stuff. So
assistance on stuff. So
yeah, that makes sense. It's actually
yeah, that makes sense. It's actually
interesting model. Plus, you get like
interesting model. Plus, you get like
eyes on cluster, too. It's open. Well,
eyes on cluster, too. It's open. Well,
it's not I mean, it's not the best model
it's not I mean, it's not the best model
for pure profit, but that's not my
for pure profit, but that's not my
motivation, right? I want to make money,
motivation, right? I want to make money,
but I also really want to make RL work
but I also really want to make RL work
as a scientific field. Um like it and
as a scientific field. Um like it and
this is why also I haven't even gone for
this is why also I haven't even gone for
VC money of any sort, right? Like the
VC money of any sort, right? Like the
stuff I just did like just even the
stuff I just did like just even the
results of throwing Muan plus the other
results of throwing Muan plus the other
tweaks that I did at Puffer. No VC would
tweaks that I did at Puffer. No VC would
let me release that because like the
let me release that because like the
quality of those results, right? Like
quality of those results, right? Like
literally RL is a different field today
literally RL is a different field today
from 3 days ago because of MUN plus a
from 3 days ago because of MUN plus a
couple of these other optimizer tweaks,
couple of these other optimizer tweaks,
cosigning, a few other things, some bug
cosigning, a few other things, some bug
fixes.
fixes.
Um, I mean, you can't see it yet. You
Um, I mean, you can't see it yet. You
can't see it yet because you didn't see
can't see it yet because you didn't see
the uh the experiments, but like the
the uh the experiments, but like the
difference between you having to run 200
difference between you having to run 200
experiments to get a specific set of
experiments to get a specific set of
hyperparameters per end and the defaults
hyperparameters per end and the defaults
roughly working out of the box now with
roughly working out of the box now with
all these new changes is that's a
all these new changes is that's a
qualitatively new field
qualitatively new field
essentially, right?
essentially, right?
So I guess in my in my paper it's funny
So I guess in my in my paper it's funny
in my 20 it was 2021 2022
in my 20 it was 2021 2022
I did PSGD on RL and I saw results and I
I did PSGD on RL and I saw results and I
did cosign a nailing but I was just
did cosign a nailing but I was just
using like a really shitty reinforcement
using like a really shitty reinforcement
learning like package and if I had you
learning like package and if I had you
know put it into puffer in 2022
know put it into puffer in 2022
you know would have been um
you know would have been um
a lot better. Yeah, but that's the thing
a lot better. Yeah, but that's the thing
like you know the amount of effort it
like you know the amount of effort it
took in order to get to a state where we
took in order to get to a state where we
can do that. We now have 30,000 lines of
can do that. We now have 30,000 lines of
C environments.
C environments.
Yeah. Right.
I I think it was even before I did this
I I think it was even before I did this
in
20 I want to say 19. I did a I did um a
20 I want to say 19. I did a I did um a
Flappy Bird RL thing with um a very
Flappy Bird RL thing with um a very
early version of PSG. It was actually it
early version of PSG. It was actually it
was it was PSG product refactorized um
was it was PSG product refactorized um
and that worked quite well and there was
and that worked quite well and there was
like I I could pull up that report
like I I could pull up that report
actually.
actually.
Yeah, if you want I'm not claiming to
Yeah, if you want I'm not claiming to
I'm not claiming to be the first one to
I'm not claiming to be the first one to
uh you know to use Muon or use PSP or
uh you know to use Muon or use PSP or
anything. Right. The point is the point
anything. Right. The point is the point
is the the catalog, right? The catalog
is the the catalog, right? The catalog
of different problems and the ability to
of different problems and the ability to
like get these exhaustive results so
like get these exhaustive results so
quickly because like I can literally so
quickly because like I can literally so
if I wanted to like prove algorithm A
if I wanted to like prove algorithm A
generally better than algorithm B,
generally better than algorithm B,
right? I can give you it would take like
right? I can give you it would take like
maybe a week's worth of experiments, but
maybe a week's worth of experiments, but
I could give you a 200 parameter
I could give you a 200 parameter
hyperparameter sweep for every one of
hyperparameter sweep for every one of
these environments.
these environments.
No, exactly. a 200 experiment type of
No, exactly. a 200 experiment type of
parameters for every one of these. It's
parameters for every one of these. It's
increasingly difficult to convince
increasingly difficult to convince
people of anything. Mhm. But this makes
people of anything. Mhm. But this makes
it possible for at least for Yeah.
All right. I'm gonna go get dinner. Um,
All right. I'm gonna go get dinner. Um,
let me see. Do we have folks watching?
let me see. Do we have folks watching?
and we have a few folks watching. Um, so
and we have a few folks watching. Um, so
let me just say real quick for the folks
let me just say real quick for the folks
on Twitch, if you're interested in
on Twitch, if you're interested in
checking out Puffer,
checking out Puffer,
puffer.ai, go ahead and start the repo.
puffer.ai, go ahead and start the repo.
Really helps us out. And if you want to
Really helps us out. And if you want to
get involved with dev, build some cool
get involved with dev, build some cool
environments, push RL forward,
environments, push RL forward,
discord.gg/puffer, it's right here. and
discord.gg/puffer, it's right here. and
follow me on extra
