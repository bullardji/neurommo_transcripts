Kind: captions
Language: en
we are
we are
back uh hold on there's my
window there we
window there we
go we're
go we're
back time for
science let me make sure I've missed
science let me make sure I've missed
anything on the
anything on the
Discord and also I'm not muted good
huh
okay
huh 1%
cool Captain's done some nice
cool Captain's done some nice
optimization up to
200k that's getting to a respectable
200k that's getting to a respectable
speed
my rest
stream okay so what we're going to do
stream okay so what we're going to do
now
now
is go back to the hyper parameter tuning
is go back to the hyper parameter tuning
stuff and it's going to take me a second
stuff and it's going to take me a second
to get back into this this is going to
to get back into this this is going to
be a little hard this is a combination
be a little hard this is a combination
of fixing messy code and hard
of fixing messy code and hard
math
so I think I was looking at the acis
so I think I was looking at the acis
position
function what is
function what is
YX the regular expected
Improvement okay okay so they're saying
that this one
that this one
is expected improvement over whichever
is expected improvement over whichever
parito value is sampled and this is
parito value is sampled and this is
expected improvement over the best
expected improvement over the best
parito
value and they want to balance they want
to they want to improve the best
ones empirically this this only samples
ones empirically this this only samples
the highest performing part of the predo
the highest performing part of the predo
front EF only samples only rarely yes
front EF only samples only rarely yes
our solution is to sample a threshold
our solution is to sample a threshold
cost from a log uniform distribution
cost from a log uniform distribution
okay that's sketchy
okay that's sketchy
because I mean I guess if cost is perf
because I mean I guess if cost is perf
in seconds then yeah that is log
distributed so you sample from log space
and then you evaluate the Paro
and then you evaluate the Paro
performance Paro front performance
there so this gives you a
there so this gives you a
comparison okay perfect so
we also
we also
note the version that always used the
note the version that always used the
maximum no parito does not
sample the version that always uses the
sample the version that always uses the
max no Paro
this is higher mean what the
it's got a bigger spread
but well this the thing is the parito
but well this the thing is the parito
thing that they're doing actually makes
thing that they're doing actually makes
sense so I assume it's the other choices
sense so I assume it's the other choices
that they've made that have screwed with
this figure five
oh they actually have a comparison to
oh they actually have a comparison to
hebo
perfect here's
perfect here's
hebo ah and there's the okay so here
hebo ah and there's the okay so here
this is fine this is what we're
this is fine this is what we're
interested in uh so hebo is over
here they don't have like random search
here they don't have like random search
and stuff on here which they should
and stuff on here which they should
have a is way better
what a synchronous success of
halfing wait this does better than them
let's see and for some reason they do
let's see and for some reason they do
way worse over
here and carbs is like
dude what the hell is wrong with this
they except on image classification they
they except on image classification they
underperform
underperform
Asha something
Asha something
screwy I mean this could literally be
screwy I mean this could literally be
like the way that search spaces are set
like the way that search spaces are set
up and stuff though
what is this we might have to implement
what is this we might have to implement
this if it's simple
allocate a small budget to each
configuration allocate a small budget to
configuration allocate a small budget to
each
each
configuration evaluate all
configuration evaluate all
configurations increase budget
configurations increase budget
configuration until maximum per
configuration until maximum per
configuration budget is reached
so you just give it a bunch so these are
so you just give it a bunch so these are
like you come up with random search
like you come up with random search
config or
whatever you train it for a bit
this is not
this is not
smart there's like a really obvious
smart there's like a really obvious
optimization here that they're missing
wait is this different successive paing
wait is this different successive paing
asynchronous success
pathing oh this is just that it's
pathing oh this is just that it's
embarrassingly parallel right
let me make sure I understand this if
let me make sure I understand this if
this is a strong Baseline against carbs
this is a strong Baseline against carbs
I want to understand this so resources
so you get the top
K draw
K draw
random okay so you just you keep
random okay so you just you keep
sampling basically you just you have a a
sampling basically you just you have a a
fixed number of
fixed number of
configurations and you just drop the
configurations and you just drop the
poor performers
okay so that's
interesting I mean this means it's
interesting I mean this means it's
probably
probably
worth you know continuing like maybe
worth you know continuing like maybe
carbs is not going to be the end all be
carbs is not going to be the end all be
all of this but I think that they have I
all of this but I think that they have I
mean they get pretty darn good results
mean they get pretty darn good results
even with all these screw-ups in
it that variant of random
search kind of does get better over time
search kind of does get better over time
come to think of it
because they keep they sample a bunch of
because they keep they sample a bunch of
parameters and they keep dropping the
parameters and they keep dropping the
bad ones and then they keep running for
longer there's definitely something to
longer there's definitely something to
that it would be a good base line
that it would be a good base line
it would be a good
Baseline and I could Implement that
Baseline and I could Implement that
pretty easily I think that I might since
pretty easily I think that I might since
this seems like the
this seems like the
simplest let me
see local start search
see local start search
start the set of specified lowc cost
parameters blend search
parameters blend search
does pretty
okay on everything
hebo this is a complicated method that
hebo this is a complicated method that
carbs is built
carbs is built
on carbs underperforms on language
on carbs underperforms on language
modeling outperforms on
modeling outperforms on
RL outperforms on image
RL outperforms on image
classification interesting
tpe is up there as well though this is
tpe is up there as well though this is
weird
this asynchronous halfing thing could be
this asynchronous halfing thing could be
pretty
decent cuz it is compute adaptive which
decent cuz it is compute adaptive which
I like and it's trivially
parallelizable I me I'm definitely going
parallelizable I me I'm definitely going
to need a better Benchmark than I
to need a better Benchmark than I
currently
currently
have man
it's crazy that this does so well on the
RL I guess it gets more runs the thing
RL I guess it gets more runs the thing
is the carbs is supposed to be able to
is the carbs is supposed to be able to
do this
automatically you know I kind of suspect
automatically you know I kind of suspect
that carbs just isn't tuned correctly
that carbs just isn't tuned correctly
because carbs should do this
because carbs should do this
adaptive compute better than
adaptive compute better than
this so we'll consider this as a
this so we'll consider this as a
baseline in a bit
baseline in a bit
um this would be a good
um this would be a good
Baseline but let's for now let's go back
Baseline but let's for now let's go back
to the math of
to the math of
carbs oh wait there was one thing I
carbs oh wait there was one thing I
wanted to look which is the ablations
so full carbs does the best on RL
so full carbs does the best on RL
no parito really hurts
no parito really hurts
it no clamping really hurts it and then
it no clamping really hurts it and then
no resampling
no resampling
okay hurts it a little bit um well some
okay hurts it a little bit um well some
a lot so let's see what this is
um your six
cost in hours okay
cost in hours okay
here
so L search does pretty
nicely doesn't give you this nice front
nicely doesn't give you this nice front
it's kind of like
it's kind of like
spotty Asha has a very nice one this is
spotty Asha has a very nice one this is
just like straight
just like straight
yeah this is
yeah this is
nice so hebo and carbs are both the gum
nice so hebo and carbs are both the gum
process based
ones you can see here that Asha just
ones you can see here that Asha just
does a ton of these really lowcost runs
does a ton of these really lowcost runs
I don't know how useful these
I don't know how useful these
are but Asha doesn't use a predictive
are but Asha doesn't use a predictive
model so actually for Asha these are
model so actually for Asha these are
useful because it's just going to keep
useful because it's just going to keep
the ones that are
the ones that are
good it's not trying to build a
good it's not trying to build a
predictive model okay that makes
sense what's this no clamping so no
sense what's this no clamping so no
clamping no Paro no resampling
this is probably the no parito
version which would make
version which would make
sense do you get this weird
distribution um
so let me see what they
so let me see what they
do sample a threshold
cost we then evaluate the Paro front
cost we then evaluate the Paro front
clost
there I'm confused
noisy okay let's take a look at these
noisy okay let's take a look at these
first output
first output
warping output variable have
warping output variable have
approximately Gan
approximately Gan
distribution with zero meain and unit
variance yeah
quantile
transform
transform
fine
fine
resampling so this doesn't matter in my
resampling so this doesn't matter in my
synthetic test because it's perfectly
synthetic test because it's perfectly
accurate but we will look at this noisy
accurate but we will look at this noisy
observations are particularly important
observations are particularly important
to handle correctly for carbs best
to handle correctly for carbs best
observations are used to define the
observations are used to define the
local search space that's a particularly
local search space that's a particularly
extreme noise Point might prevent the
extreme noise Point might prevent the
algorithm from finding a better Optimum
this is
this is
true one out of every n resample
true one out of every n resample
suggestions is a set of parameters that
suggestions is a set of parameters that
has already been
has already been
observed we count the samples for each
observed we count the samples for each
of the existing Paro front points and
of the existing Paro front points and
suggest parameters which have the
suggest parameters which have the
smallest number of samples breaking ties
smallest number of samples breaking ties
by one by choosing the one with lower
by one by choosing the one with lower
cost okay so they're resampling on the
cost okay so they're resampling on the
Paro front but the thing is the Paro
Paro front but the thing is the Paro
front can get to be pretty
big and also like okay let's say you get
big and also like okay let's say you get
a bad sample right that just
a bad sample right that just
means and now a different point becomes
means and now a different point becomes
parito
right more likely it just means you drop
right more likely it just means you drop
that from the burito set if it's
that from the burito set if it's
overestimated you're not going to have
overestimated you're not going to have
something with exactly the same cost so
something with exactly the same cost so
if it's overestimated you're just going
if it's overestimated you're just going
to end up dropping something from the
to end up dropping something from the
Pito
set this seems important
this is trying to robustify something
this is trying to robustify something
that might not be able to be robustified
that might not be able to be robustified
though some sometimes the MS are just
though some sometimes the MS are just
very noisy I guess that's why Asha would
very noisy I guess that's why Asha would
win because Asha doesn't care about this
win because Asha doesn't care about this
type of
type of
stuff all it cares about is um
stuff all it cares about is um
the final model basically it's not
the final model basically it's not
building it it's not trying to predict
building it it's not trying to predict
any of
this so I'm definitely going to do that
this so I'm definitely going to do that
as a
baseline Pito front
minimum we start the Paro
front because the cost itself is a noisy
front because the cost itself is a noisy
that's not true
this is true for a different reason this
this is true for a different reason this
is just because you have to train the
is just because you have to train the
models for a bit to start getting a
curve 20%
curve 20%
observations with the lowest
cost interesting
tuning parameters like model size can be
tuning parameters like model size can be
difficult if some parameters lead to
failure okay so they have a failure
failure okay so they have a failure
prediction model which you should be
prediction model which you should be
able to just know what's going to out of
able to just know what's going to out of
memory and not run that so we don't care
memory and not run that so we don't care
about this too
much clost ceiling this is fine
parallelism yeah
well let's see this still might this
well let's see this still might this
still might do better than uh than Asha
still might do better than uh than Asha
I don't want to give up on this this is
I don't want to give up on this this is
like this still seems in my mind like
like this still seems in my mind like
this has potential there's some things I
this has potential there's some things I
don't like about it but this still has a
don't like about it but this still has a
ton of potential let me just go turn the
ton of potential let me just go turn the
AC on real quick we'll get to this
by so let me figure out how this works
by so let me figure out how this works
because I need to understand this
because I need to understand this
sampling
so they take this
x x is sampled around a Paro value
I actually might like this hard trust
I actually might like this hard trust
region idea better
an issue
an issue
okay we're more interested in the best
okay we're more interested in the best
performing
parameters I don't really understand
this cuz you sampled this X
this cuz you sampled this X
from any Paro point
uniformly weights improvements across
uniformly weights improvements across
the
the
entire Paro
front okay I got to look at the code for
front okay I got to look at the code for
this because the math does not make
this because the math does not make
sense see to
sense see to
me it looks like you should be biasing
me it looks like you should be biasing
your X samples
your X samples
um towards higher cost
um towards higher cost
stuff what this looks like to me is that
stuff what this looks like to me is that
they are biasing their scoring function
they are biasing their scoring function
I don't
I don't
understand unless are they sampling from
this Aquis maybe they're sampling from
this Aquis maybe they're sampling from
this hold
this hold
on so wait if they
okay hold on I think that they are
okay hold on I think that they are
sampling from this so this is a soring
sampling from this so this is a soring
function this soring function works over
function this soring function works over
everything we got rid of pearch we're
everything we got rid of pearch we're
getting rid of this so all we have is
getting rid of this so all we have is
this expectation here um
I actually like the idea of the hard
I actually like the idea of the hard
trust region I'm going to try
trust region I'm going to try
that
that
um but this
thing this is going to give you a
distribution instead what they say
is they're going to
sample they're going to sample a
sample they're going to sample a
cost and then they
use this function to estimate the
use this function to estimate the
performance at that
cost and then they going to take a
cost and then they going to take a
Max between these
two but then they take a Max
two but then they take a Max
over I think they just take a Max over
over I think they just take a Max over
this thing Max over all candidates okay
this thing Max over all candidates okay
so
so
how how does this work
let me just
uh
e e
oh wait maybe I do get
it I think I actually do get it so this
it I think I actually do get it so this
is the expected
performance so if
performance so if
per
per
Point
random it's a chance that it will
random it's a chance that it will
higher score of
the lower
the lower
cost for to a point higher
cost for to a point higher
chance
chance
that random
s
POS one
POS one
that this means
bias points or
higher
not really like
so I mean this could just be paring back
so I mean this could just be paring back
what I said
there definitely seem to be some major
there definitely seem to be some major
limitations with this algorithm now that
limitations with this algorithm now that
I'm looking at
it quite some
limitations
for e
there quite some substantial limitations
there quite some substantial limitations
here though for
sure okay so I understand how they're
sure okay so I understand how they're
doing this let me make sure it's
doing this let me make sure it's
implemented
correctly so they take
correctly so they take
acquisition
function we don't need this we can just
function we don't need this we can just
get rid of
probabilities I don't actually see this
probabilities I don't actually see this
um Max cost
masking always
maxed this should be okay this is the
clamp let's make sure this is
clamp let's make sure this is
on this should be on right
what
it's using the same sample repeatedly
it's using the same sample repeatedly
here we'll fix that okay good this
here we'll fix that okay good this
is they do have this clamp
thing get rid of this
okay I'd like to figure out why all the
okay I'd like to figure out why all the
random samples are the same to start
random samples are the same to start
we'll have to look at
that let's see if I can find that in the
that let's see if I can find that in the
meantime while this
runs get random
sample search space of 32
oh you know what I bet it's just that
oh you know what I bet it's just that
we're using the time in
seconds
seconds
holy it's already got
200s well that's a ridiculously fast
Improvement it's pretty R noisy but I
Improvement it's pretty R noisy but I
guess that makes sense
yeah that's crazy fast
Improvement and we're not even getting
Improvement and we're not even getting
good random samples yet let's fix
that yeah I think h i I think the
that yeah I think h i I think the
science side effort of puffer lib for
science side effort of puffer lib for
the next little bit is going is going to
the next little bit is going is going to
be hyper pram that's the biggest ban for
be hyper pram that's the biggest ban for
our
buck it's so important and Nobody Does
buck it's so important and Nobody Does
it I mean really there's not any higher
it I mean really there's not any higher
return on
return on
investment um I'm going to be curious to
investment um I'm going to be curious to
see whether carbs is the algorithm that
see whether carbs is the algorithm that
we end up using it's a very complicated
we end up using it's a very complicated
algorithm it makes sense in principle
algorithm it makes sense in principle
but there are some rough edges and like
but there are some rough edges and like
with something that's complicated there
with something that's complicated there
just so many places to have stuff go
just so many places to have stuff go
wrong okay but this is like dramatically
wrong okay but this is like dramatically
dramatically better here like we're
dramatically better here like we're
getting these 200s and stuff we'll see
getting these 200s and stuff we'll see
how long it takes to max out but um you
how long it takes to max out but um you
know we're getting these
know we're getting these
samples way way way faster than before
samples way way way faster than before
this used to take like 200 or at least
this used to take like 200 or at least
100 experiments to get one of
100 experiments to get one of
these and now we're getting it quicker
there we go 250 okay so this is like
there we go 250 okay so this is like
twice as good as before by deleting
twice as good as before by deleting
their stupid uh sampling like their
their stupid uh sampling like their
stupid probability
stupid probability
waiting as expected that was just dumb I
waiting as expected that was just dumb I
don't know why they had
that we'll see whether hard trust or
that we'll see whether hard trust or
soft trust region is
soft trust region is
better hard trust is more conservative
and it should still be biasing to the
and it should still be biasing to the
top end of the range here doesn't mean
top end of the range here doesn't mean
it's going exclusively produce results
it's going exclusively produce results
there but it should be producing
there but it should be producing
um well actually you know it should
um well actually you know it should
actually go to the top end because the
actually go to the top end because the
cost is uniform
cost is uniform
so the parito
so the parito
points are at the top
points are at the top
end so this is kind of noisy to be fair
end so this is kind of noisy to be fair
this is kind of
this is kind of
noisy I mean it might just need more
noisy I mean it might just need more
samples to build up the predictive model
samples to build up the predictive model
it is getting better over
time there's a 67 in
there still way better than before
okay so next
thing is going to be
thing is going to be
this
this
time into time.
time random. seed
just take an
in it has to be converted to an
in it has to be converted to an
integer okay so what we want is
um I just multiply by a th
wow this is good
wow this is good
right I do
right I do
int yeah
that's
that's
good and that's same oh
good and that's same oh
yeah so much better than
yeah so much better than
before still gets a few bad samples
before still gets a few bad samples
but overall so much better than
before let's see if this fixes the
before let's see if this fixes the
random
problem seed must
be
okay what's a good way to get
okay what's a good way to get
uh to get this
seed way to get
ah perfect
ah perfect
lower 32
bits seed
equals that's good
perfect now you see we get different
seeds uh that should actually help the
seeds uh that should actually help the
algorithm out quite a bit because the
algorithm out quite a bit because the
initial data points are properly
separated so that's
separated so that's
fixed
now let me think about this from here
this is kind of the simplest possible
this is kind of the simplest possible
case for carbs because there should only
case for carbs because there should only
be one Pito
point I should double check that there
point I should double check that there
is only one parito
is only one parito
point for
it's pretty decent
okay so we'll just do like this
so
here oh there were just multiple parito
here oh there were just multiple parito
points that have the same output that's
points that have the same output that's
fine yeah that can happen with uh that
fine yeah that can happen with uh that
can that could happen that's
can that could happen that's
fine yeah they had the exact same output
fine yeah they had the exact same output
and then slightly different parameters
and then slightly different parameters
that's totally fine you say this the
that's totally fine you say this the
exact same
point these should have gotten merged
point these should have gotten merged
um but I guess it's a resample that's
um but I guess it's a resample that's
fine and now
why are these two
not oh I guess it doesn't distinguish
not oh I guess it doesn't distinguish
between um
between um
duplicated
duplicated
suggestions but this is like never going
suggestions but this is like never going
to happen in practice so this is fine
to happen in practice so this is fine
this is only because I have a perfectly
this is only because I have a perfectly
deterministic synthetic test
so it is sampling around it's sampling
so it is sampling around it's sampling
around a parito
point it's weird that some of these
point it's weird that some of these
suggestions are so
suggestions are so
bad you know because it should be
bad you know because it should be
scoring
this I guess the gaussian process takes
this I guess the gaussian process takes
more samples to be able to build a good
more samples to be able to build a good
predictive
model I mean this is still this is a
model I mean this is still this is a
massive Improvement
I kind of want to now compare this to
I kind of want to now compare this to
asynchronous
having just see how that does
I mean hold on that's basically that's
I mean hold on that's basically that's
random
search e
I also haven't added the scale parameter
I also haven't added the scale parameter
in
in
yet so this is making way too aggressive
yet so this is making way too aggressive
updates is
updates is
why so this is like almost random the
why so this is like almost random the
way it's
way it's
searching right so let's try
that
for e
try
this okay so this is now at 205 here
this okay so this is now at 205 here
with this many
with this many
parameters this is what we have here
and is there like a cumulative mean or
and is there like a cumulative mean or
something or and we can just have
something or and we can just have
this let's try this so what this should
this let's try this so what this should
do is massively reduce the
do is massively reduce the
per suggestion search
radius let's see if this gives us more
radius let's see if this gives us more
consistency
not so good so
far possibly set it too low
so it is making nice nice uh
so it is making nice nice uh
progress It's like pretty darn
progress It's like pretty darn
consistent now
was it going to be stuck at this rate of
was it going to be stuck at this rate of
improvement though
yeah the deviations are just very small
here it should have a good predictive
here it should have a good predictive
model so it should be improving more
model so it should be improving more
consistently than this
oh you know what it
oh you know what it
is
is
um carbs literally is only building the
um carbs literally is only building the
model based off of the one parito
point
point
okay so that's not
amazing there's a f 56
okay yeah so I'll let this run for a bit
okay yeah so I'll let this run for a bit
just to see what the shape of this curve
is I do think whatever I just did is too
is I do think whatever I just did is too
conservative
because it basically got one lucky
because it basically got one lucky
update and now it's still stuck in this
update and now it's still stuck in this
little
region I mean there are a couple
region I mean there are a couple
different Pito points looks at I
different Pito points looks at I
guess and there gets an update
it's still just so
slow okay let's try
um let's
um let's
try5 on
this 74 so it's doing stuff it's too
this 74 so it's doing stuff it's too
below
so the point of this
so the point of this
is the search range that you'd use for
is the search range that you'd use for
like random search or
like random search or
whatever uh that's like the goal of that
whatever uh that's like the goal of that
is to cover most of the space you're
is to cover most of the space you're
trying to do something that's like
trying to do something that's like
somewhat tight around the Paro values
somewhat tight around the Paro values
but not so tight that you you are
but not so tight that you you are
basically sampling the same
point we'll see how this
does I mean come to think of it I
does I mean come to think of it I
wouldn't be surprised the really
wouldn't be surprised the really
noisy version actually works because
noisy version actually works because
basically if you think about it if you
basically if you think about it if you
have a really high mean on this then
have a really high mean on this then
it's just random
it's just random
search it's like basically random search
search it's like basically random search
but with the mean variables the means of
but with the mean variables the means of
everything set to the best run and then
everything set to the best run and then
doing that on all the Paro
values that's actually kind of
good but it will produce like
good but it will produce like
noisy I me the downside is it will
noisy I me the downside is it will
produce noisy
results how how do they use the parito
results how how do they use the parito
fit hold on cuz this model's crap
fit hold on cuz this model's crap
whatever this Pito model is here is crap
whatever this Pito model is here is crap
so anything that comes out of this model
so anything that comes out of this model
is going to be bad let me
is going to be bad let me
see final model is used as a basine for
see final model is used as a basine for
the
expected okay so this
expected okay so this
Baseline is going to be crap for this
Baseline is going to be crap for this
because the cost is all the same
because the cost is all the same
I don't know why they only use the
I don't know why they only use the
parito points to fit that
model I guess it makes
model I guess it makes
sense I don't know I don't know if that
sense I don't know I don't know if that
should be a gaussian
process okay so this is better this is
process okay so this is better this is
giving
you up to 70s to 115
we'll tune this on the full problem
we'll tune this on the full problem
because right now we're not tuning on on
because right now we're not tuning on on
the full problem so let's
goer
Li
Li
CS so those are the optimals
CS so those are the optimals
um the full problem should be
there should be cost
there should be cost
assigned this doesn't change the
optimum but it should change how carbs
runs and then we can test we can test
runs and then we can test we can test
this stuff yeah
this is looking like a decent tradeoff
this is looking like a decent tradeoff
it takes a little longer but these are
it takes a little longer but these are
like very stable
updates yeah so now the the
updates yeah so now the the
uh acquisition function has been
uh acquisition function has been
learned it's just going to
solve that's pretty
decent the predictive model is
decent the predictive model is
not I'd expect more out of that model
though cuz they're supposed to have this
though cuz they're supposed to have this
calcium process is supposed to be
calcium process is supposed to be
predicting which of these parameters is
predicting which of these parameters is
going to do well and it should be
going to do well and it should be
something that's very easy to learn but
something that's very easy to learn but
it's just not doing
it's just not doing
it like this should basically
it like this should basically
consistently be going up and it's
not maybe we'll tune it a little more
not maybe we'll tune it a little more
we'll see
I want to get to asynchronous halfing
I want to get to asynchronous halfing
soon because I like I just don't have a
soon because I like I just don't have a
good Baseline for this and I'd like to
good Baseline for this and I'd like to
build out a test Suite cuz if I'm going
build out a test Suite cuz if I'm going
to spend you know a whole bunch of time
to spend you know a whole bunch of time
packaging up carbs nicely I want to know
packaging up carbs nicely I want to know
that it's actually outperforming you
that it's actually outperforming you
know strong
know strong
baselines uh or at least
baselines uh or at least
like relatively strong simple algorithms
okay so here's
cost we see that it
solves this
solves this
takes what is
takes what is
this here yeah 234 I think we'll count
this here yeah 234 I think we'll count
that as mostly
solved okay 79 update for this
solved okay 79 update for this
um call it like 85 to get the
um call it like 85 to get the
250ish so that's way oh yeah here there
250ish so that's way oh yeah here there
we go
so 83
so 83
updates compared
to our synthetic test
oh it's about the
oh it's about the
same about 83 either
way I carb sweep question when I try to
way I carb sweep question when I try to
bound I need to make the background on
bound I need to make the background on
this less transparent when I try to
this less transparent when I try to
bound on
when I try to bound
gamma what branch are you
want are you on my ablations
want are you on my ablations
Branch Oh no you're on
Branch Oh no you're on
2 huh
yeah I don't know man that's
yeah I don't know man that's
weird um there are some serious serious
weird um there are some serious serious
problems with the base carbs I will say
problems with the base carbs I will say
there's some very serious problems with
there's some very serious problems with
it we've made it like about it's about
it we've made it like about it's about
three times better already than it was
three times better already than it was
before and I'm not done yet
you can try
you can try
that it's going to be Jank so you if
that it's going to be Jank so you if
you're going to do
you're going to do
that that might be Jank you can probably
that that might be Jank you can probably
actually for your problem it'll be fine
actually for your problem it'll be fine
if you just do from 0.9 to 99 linear
if you just do from 0.9 to 99 linear
it'll work because yours isn't going to
it'll work because yours isn't going to
be super high
anyways okay so this works at least as
anyways okay so this works at least as
well as before if not a little bit
well as before if not a little bit
better this looks
nice now we got to do the full
nice now we got to do the full
problem now there's cost involved here
when when I finish it man this is very
when when I finish it man this is very
difficult because I'm waiting through a
difficult because I'm waiting through a
whole bunch of math and carbs and I'm
whole bunch of math and carbs and I'm
starting to see some questionable
starting to see some questionable
choices that they've made and then
choices that they've made and then
looking at their baselines it seems like
looking at their baselines it seems like
it's possible that I'm going to be able
it's possible that I'm going to be able
to implement something much simpler that
to implement something much simpler that
might outperform it anyways so I really
might outperform it anyways so I really
have to do some uh some proper tests on
this why is cost
1.0 oh CU I forgot to uncomment this
line okay good so now we have
line okay good so now we have
a dramatically varying
a dramatically varying
costs in a bigger Paro
front was this one with cost or without
cost cost up time
okay so this is the one without
CLS so let's see how carbs
CLS so let's see how carbs
does with cost
it's going to
do a little bit
differently I kind of want this one to
differently I kind of want this one to
be on Neptune though
really
so we're going to put this on to uh
so we're going to put this on to uh
Neptune as our
Neptune as our
Baseline there are still improvements
Baseline there are still improvements
that can be made here but uh I'm going
that can be made here but uh I'm going
to do I think we're going to
to do I think we're going to
implement Asha after this as a basine
implement Asha after this as a basine
and we're also going to do the random as
and we're also going to do the random as
a
a
baseline let's just run this here
baseline let's just run this here
dude what the I just installed Neptune
off p is screwed
up API token
than
for e
is there no carb suggest where's
is there no carb suggest where's
suggest oh yeah there it
is
for e
and let's see how our run is
going there we
go much
go much
nicer batch size going up mini batches
nicer batch size going up mini batches
interesting
stuff there may still be some uh some
stuff there may still be some uh some
search parameters to
tune is native carbs integration with
tune is native carbs integration with
wan to be improved nope they haven't
wan to be improved nope they haven't
done Jacks yet or have you just switched
done Jacks yet or have you just switched
to Neptune uh it's not really Neptune
to Neptune uh it's not really Neptune
doesn't do hyperparameter tuning at all
doesn't do hyperparameter tuning at all
so we're doing our own hyperparameter
so we're doing our own hyperparameter
tuning and managing it via tags so we'll
tuning and managing it via tags so we'll
support wanb and Neptune but we're doing
support wanb and Neptune but we're doing
it through tags instead of through their
it through tags instead of through their
sweeps API um currently what I'm doing
sweeps API um currently what I'm doing
is I found some uh some major
is I found some uh some major
limitations of carbs so I'm making like
limitations of carbs so I'm making like
neoc carbs new version of carbs and uh
neoc carbs new version of carbs and uh
I'm also going to Benchmark it against
I'm also going to Benchmark it against
some other Baseline algorithms to see if
some other Baseline algorithms to see if
you know if even my better version of
you know if even my better version of
carbs holds up um because it's possible
carbs holds up um because it's possible
that we're just going to have a simpler
that we're just going to have a simpler
thing that works better we will see that
thing that works better we will see that
said I have made carbs like 3x better
said I have made carbs like 3x better
than the original already
than the original already
so yeah this is a synthetic
so yeah this is a synthetic
Benchmark let me see if I have the
Benchmark let me see if I have the
original result here from before I mess
original result here from before I mess
with
it synthetic
carbs maybe this one yeah okay so this
carbs maybe this one yeah okay so this
is before I messed with it this is what
is before I messed with it this is what
carbs got us like 70 or what whatever
carbs got us like 70 or what whatever
exact same
exact same
Benchmark okay and
then I did some rescaling work and I got
then I did some rescaling work and I got
us to
us to
here
here
250 but this is without cost
250 but this is without cost
awareness um so now with cost
awareness we will see how well we do
awareness we will see how well we do
cost awareness should hurt it a little
cost awareness should hurt it a little
bit but uh what cost awareness does it
bit but uh what cost awareness does it
prevents you from running really long
prevents you from running really long
experiments too
early so the way to evaluate it is um
early so the way to evaluate it is um
you need to take into account the cost
you need to take into account the cost
of all of the experiments
I can probably do these at the same
I can probably do these at the same
time if you set search center out of
time if you set search center out of
bounds carbs yells at you yes that does
happen we're going to see so basically
happen we're going to see so basically
Spencer if um if carbs ends up being the
Spencer if um if carbs ends up being the
god algorithm after these tweaks then I
god algorithm after these tweaks then I
will invest the time that it takes to
will invest the time that it takes to
really clean up the code and like it'll
really clean up the code and like it'll
be like half the code or whatever it'll
be like half the code or whatever it'll
be a file in puffer lib um there a
be a file in puffer lib um there a
couple tricky things like yeah you have
couple tricky things like yeah you have
to load in gaussian processes and stuff
to load in gaussian processes and stuff
so it will depend on Torch but it'll be
so it will depend on Torch but it'll be
much much simpler uh if we beat carbs
much much simpler uh if we beat carbs
with something simpler then obviously
with something simpler then obviously
we're just going to use the simpler
we're just going to use the simpler
thing so we'll
thing so we'll
see I remember trying to do carbs with
see I remember trying to do carbs with
wbie sweeps well that's actually we do
wbie sweeps well that's actually we do
have WV sweeps working with carbs in
have WV sweeps working with carbs in
puffer 2 right now you can do it it's
puffer 2 right now you can do it it's
just that like there are some
just that like there are some
limitations of carbs um that could be
limitations of carbs um that could be
improved I don't I'll be somewhat
improved I don't I'll be somewhat
disappointed if this is just beaten by
disappointed if this is just beaten by
something way simpler but here's the
something way simpler but here's the
thing I'm looking at the paper and yeah
thing I'm looking at the paper and yeah
this is their version of carbs with like
this is their version of carbs with like
a whole bunch of things
a whole bunch of things
screwy
but so you know we're going to have a
but so you know we're going to have a
way stronger thing than their version of
way stronger thing than their version of
carbs but like look at
carbs but like look at
Asha it doesn't do so well on language
Asha it doesn't do so well on language
model and image classification but for
model and image classification but for
whatever reason on RL it does the best
whatever reason on RL it does the best
and Asha is really simple to
implement also Asha has the advantage of
implement also Asha has the advantage of
being trivially multi Noe like we
being trivially multi Noe like we
wouldn't even have to do any fancy stuff
wouldn't even have to do any fancy stuff
and Asha would just be multi- Noe
it's really weird how different the
it's really weird how different the
performances of these algorithms on
performances of these algorithms on
different tasks
different tasks
like I kind of don't believe that the
like I kind of don't believe that the
ranking isn't similar across tasks i'
ranking isn't similar across tasks i'
like I almost want to think that they
like I almost want to think that they
were just configured better in some like
were just configured better in some like
in some cases than others
okay so after 40 we're at 140 we're
okay so after 40 we're at 140 we're
starting to run the really long
starting to run the really long
runs it's missed a couple parameters so
runs it's missed a couple parameters so
I think that there are some
I think that there are some
configuration improvements to make here
configuration improvements to make here
cuz like it should definitely figure out
cuz like it should definitely figure out
uh mini batches 8 and stuff like
uh mini batches 8 and stuff like
that but I think it's it's also possible
that but I think it's it's also possible
just being conservative because this is
just being conservative because this is
cost now updox to four yeah mm's I don't
cost now updox to four yeah mm's I don't
think this
think this
matters maybe it
matters maybe it
does PT this is 16 case we got
that and
then
for environment up time so this should
for environment up time so this should
be or
be or
cost okay so this is doing the correct
cost okay so this is doing the correct
thing look it's getting more cost over
time it started out low
cost so this is the correct thing
cost so this is the correct thing
roughly
I can probably do two of these sweeps at
I can probably do two of these sweeps at
the same time like I don't think Neptune
the same time like I don't think Neptune
will yell at me I'm going to just do
will yell at me I'm going to just do
this and we're going to get a random
this and we're going to get a random
sweep working
okay so now we got to do
uh now we got to do
our our setup
here I know it's done well and puffer
here I know it's done well and puffer
because I copied for my own repo with
because I copied for my own repo with
some jack space Ms it was done at least
some jack space Ms it was done at least
originally was very finicky yeah it was
originally was very finicky yeah it was
very
very
much give puffer m a try they're faster
much give puffer m a try they're faster
than most Jack baced M and you don't
than most Jack baced M and you don't
have to
suffer though we are starting a collab
suffer though we are starting a collab
with Flare to provide um nice puffer
with Flare to provide um nice puffer
bindings for craftex and kinetics pretty
bindings for craftex and kinetics pretty
soon soon as I get to that those ones
soon soon as I get to that those ones
are pretty nice oh we got a fair few
are pretty nice oh we got a fair few
folks on YouTube at the moment welcome
folks on YouTube at the moment welcome
so we what we are currently doing we're
so we what we are currently doing we're
currently looking at the carbs
currently looking at the carbs
hyperparameter tuning algorithm which is
hyperparameter tuning algorithm which is
at least on paper a very good
at least on paper a very good
hyperparameter tuning algorithm from a
hyperparameter tuning algorithm from a
UI I found some pretty
UI I found some pretty
substantial limitations and quirks in
substantial limitations and quirks in
the implementation that I fixed so now
the implementation that I fixed so now
it's at least like two or three times
it's at least like two or three times
better than before and now what I'm
better than before and now what I'm
going to do is I'm going to Benchmark
going to do is I'm going to Benchmark
this on some synthetic uh on a synthetic
this on some synthetic uh on a synthetic
hyperparameter optimization task I made
hyperparameter optimization task I made
versus random search and then probably
versus random search and then probably
versus Asha once I Implement that and
versus Asha once I Implement that and
we're going to see whether carbs is
we're going to see whether carbs is
truly the king of hyper pram optim
truly the king of hyper pram optim
ization once you clean it up and if so I
ization once you clean it up and if so I
will uh package it up nicely simplify
will uh package it up nicely simplify
the implementation and put carbs into
the implementation and put carbs into
puffer or if one of the other baselines
puffer or if one of the other baselines
is easier and performs just as well or
is easier and performs just as well or
better and if so that's great news
better and if so that's great news
because as is way simpler to implement
because as is way simpler to implement
way simpler to understand and would uh
way simpler to understand and would uh
also parallelize trivially to
also parallelize trivially to
multinode craftex is going to be the
multinode craftex is going to be the
focus of one so would be very appealing
focus of one so would be very appealing
yeah craftex is a great M original craft
yeah craftex is a great M original craft
is a God awful M it's the worst python
is a God awful M it's the worst python
code I've ever seen um I don't even know
code I've ever seen um I don't even know
how you make an environment that slow
how you make an environment that slow
craftex is
craftex is
awesome I that said I had talked with
awesome I that said I had talked with
the authors of uh you know this Jack
the authors of uh you know this Jack
sson flairer and they're very talented
sson flairer and they're very talented
for being able to do this stuff at Jack
for being able to do this stuff at Jack
but you know they're kind of shooting
but you know they're kind of shooting
themselves in the foot a little bit it
themselves in the foot a little bit it
would literally be so much easier to
would literally be so much easier to
just Implement these things and see like
just Implement these things and see like
it would be so much easier it would be
it would be so much easier it would be
the same amount or less code and you
the same amount or less code and you
wouldn't have to think in a
wouldn't have to think in a
race I mean we literally have brand new
race I mean we literally have brand new
people to RL implementing the all the
people to RL implementing the all the
various puffer Ms and c and it's like so
various puffer Ms and c and it's like so
easy by
comparison I don't know if anybody wants
comparison I don't know if anybody wants
to go Port crafter to uh craft X to see
to go Port crafter to uh craft X to see
you're welcome to um I don't think I
you're welcome to um I don't think I
wasn't going to do it because like the
wasn't going to do it because like the
jackm already exists and it's already
jackm already exists and it's already
fast but I would not be surprised if
fast but I would not be surprised if
it's just faster and less code and
see possibly except for rendering I
see possibly except for rendering I
don't have an amazing way of doing
don't have an amazing way of doing
rendering yet but I mean their rendered
rendering yet but I mean their rendered
version is not that fast
version is not that fast
anyways okay let's take a quick look at
anyways okay let's take a quick look at
this now so all I got to do is update
this now so all I got to do is update
our random sampling code for the new
our random sampling code for the new
param spaces if I add a new reward to
param spaces if I add a new reward to
tune can I keep the previous bounds that
tune can I keep the previous bounds that
led to better
led to better
results or do I have to set everything
results or do I have to set everything
back you're probably decently safe
back you're probably decently safe
Spencer
Spencer
um it depends how tight the bounds are
um it depends how tight the bounds are
so what you would do is You' run you
so what you would do is You' run you
would run then Sweep with the tight
would run then Sweep with the tight
bounds and then if when you go look at
bounds and then if when you go look at
the graphs you know the best results are
the graphs you know the best results are
pushed up to one edge of the range on on
pushed up to one edge of the range on on
one of the parameters then you loosen
one of the parameters then you loosen
that bound right you do a little
that bound right you do a little
sensitivity analysis
sensitivity analysis
is why good graphs are
is why good graphs are
important oh and for YouTube folks
important oh and for YouTube folks
discord.gg puffer stop by come build
discord.gg puffer stop by come build
some RL environments with us it's
fun and St the repo as well that helps
fun and St the repo as well that helps
me a
me a
lot okay so here we got min max I'm
lot okay so here we got min max I'm
trying to think I want to do this
um there's also like a mean and a clip
um there's also like a mean and a clip
or did I get rid of Cl I think I got
or did I get rid of Cl I think I got
mean in
scale let's just do for now
scale let's just do for now
um let's keep these Min and Maxes
let's try this and then log uniform
let's try this and then log uniform
power
power
to oh actually do we even need
to oh actually do we even need
this no we don't need this cuz it's a
this no we don't need this cuz it's a
uniform and then we'll do log normal
be and do that
and with Jack it can be run on GPU
and with Jack it can be run on GPU
though are you sitting just writing and
though are you sitting just writing and
see and running on CPU would be better
see and running on CPU would be better
yep gpus are not designed to process
yep gpus are not designed to process
arbitrary logic and they're definitely
arbitrary logic and they're definitely
not designed to process you like writing
not designed to process you like writing
entire games in terms of array logic is
entire games in terms of array logic is
kind of an insane thing to
kind of an insane thing to
do like it's not CPUs are not slow
do like it's not CPUs are not slow
python is slow all of our environments
python is slow all of our environments
in C run in million steps a second and
in C run in million steps a second and
we have some of the most complex
we have some of the most complex
environments out there like here this
environments out there like here this
thing runs I think 1.9 million steps per
thing runs I think 1.9 million steps per
second on a single CPU core the
second on a single CPU core the
simulation of this does and we train
simulation of this does and we train
this at like 500k as well and like this
this at like 500k as well and like this
is here this
is here this
is a massive environment with like you
is a massive environment with like you
know tons of stuff going on uh there's
know tons of stuff going on uh there's
an economy system like like there's
an economy system like like there's
trade there's like enemies there's all
trade there's like enemies there's all
sorts of stuff going on in here and this
sorts of stuff going on in here and this
is just fast and all of our environments
is just fast and all of our environments
are fast like the faster ones are 10
are fast like the faster ones are 10
million we stop trying after a million
million we stop trying after a million
because it's fast enough but some of
because it's fast enough but some of
them are just even
them are just even
faster
faster
yeah there are like a few places where
yeah there are like a few places where
there are like things that are hard to
there are like things that are hard to
make fast but for the most part
yeah I don't know what you're on about
yeah I don't know what you're on about
Jason we're not
Jason we're not
doing we're not doing like train time
doing we're not doing like train time
rendering for the most
part the rendering is uh inference time
part the rendering is uh inference time
in
rib believe that just running the M1 CPU
rib believe that just running the M1 CPU
with the total training time yeah
with the total training time yeah
there's potentially transfer overhead so
there's potentially transfer overhead so
we do some bandwidth optimization I mean
we do some bandwidth optimization I mean
we have m's training in a million steps
we have m's training in a million steps
per second does anybody
per second does anybody
else maybe there a couple like Jack
else maybe there a couple like Jack
things on really simple environments
things on really simple environments
that do but for the most part not really
that do but for the most part not really
and like you're not implementing neural
and like you're not implementing neural
and Jack you're just not okay maybe if
and Jack you're just not okay maybe if
you're a total genius you can write like
you're a total genius you can write like
10,000 lines of jacks for an ear Mo 3 in
10,000 lines of jacks for an ear Mo 3 in
like a year but it's just or you can be
like a year but it's just or you can be
completely brain dead and write it and
completely brain dead and write it and
see and have it run 1.9 million steps
see and have it run 1.9 million steps
per second and it's just fine we train
per second and it's just fine we train
at 500k on that on one
GPU like our default training setup is
GPU like our default training setup is
at least a 100 times faster than the
at least a 100 times faster than the
majority of what is being done in RL
majority of what is being done in RL
right now like every time I get sent an
right now like every time I get sent an
academic Repository it's like 1 to
academic Repository it's like 1 to
10,000 steps per second and then most of
10,000 steps per second and then most of
our baselines it's like 400,000 to a
our baselines it's like 400,000 to a
million or 1.2 million steps per
second it's kind of
nutty this is how we get stuff done
nutty this is how we get stuff done
though
okay so this min max this is fine
okay so this min max this is fine
uniform power two is also
uniform power two is also
fine and then the log the logs are going
fine and then the log the logs are going
to be a little
different min max mean
different min max mean
scale what's neural MMOs SPS last I
scale what's neural MMOs SPS last I
checked about .9 million uh steps per
checked about .9 million uh steps per
second per CPU core and then the
second per CPU core and then the
training at the moment with the policy
training at the moment with the policy
that I'm using is around 500,000 on a
490 so that's like arguably one of the
490 so that's like arguably one of the
most complicated environments out there
most complicated environments out there
that's still ludicrously fast that's the
that's still ludicrously fast that's the
nice thing with with C is like I can
nice thing with with C is like I can
just make everything fast with Jack
just make everything fast with Jack
like when you get to complicated
like when you get to complicated
branching logic stuff's really hard to
branching logic stuff's really hard to
keep fast or like really hard to
keep fast or like really hard to
represent it's trivial and see you just
represent it's trivial and see you just
write the code in the most brain dead
write the code in the most brain dead
way possible and it's
fast e
see compiler yeah I mean but here like
see compiler yeah I mean but here like
the crazy thing about this is I'm pretty
the crazy thing about this is I'm pretty
sure the neural MMO 3 code is shorter
sure the neural MMO 3 code is shorter
than the crafter
code and that's like including all the
code and that's like including all the
really nice fancy rendering that we did
really nice fancy rendering that we did
like you literally just go here puffer
like you literally just go here puffer
lib ocean all the environments and then
lib ocean all the environments and then
like nurl mm mo3 is 1H
like nurl mm mo3 is 1H
file 20800 lines and like if you scroll
file 20800 lines and like if you scroll
through it the code is brain dead like
through it the code is brain dead like
this is about as complicated as it
this is about as complicated as it
gets it's it's it's like conditionals
gets it's it's it's like conditionals
and loops
like a first year CS student can read
like a first year CS student can read
most of this there may be a couple
most of this there may be a couple
things I do for perf in here but like a
things I do for perf in here but like a
first year CS student from their first
first year CS student from their first
systems force can read this because
systems force can read this because
there's nothing fancy at
all I don't think our first year CS
all I don't think our first year CS
student could have write this as cleanly
student could have write this as cleanly
I would hope I'd hope I do a little
I would hope I'd hope I do a little
better than that but read it
better than that but read it
yeah not true with the Jack's code
I'm going gamma and Lambda
G and Lambda not getting sample
I should switch my view back to
I should switch my view back to
this here we
go um let's see
go um let's see
this so gamma
how's this happen
05 right
oh hold on
um how do I do
this I think it's got to be like this
this I think it's got to be like this
cuz it's getting
cuz it's getting
clipped what's
happening I think it's one minus Max
happening I think it's one minus Max
man like
this still
this still
no that's
weird for
getting
getting
clipped it's
weird scale is5
05 I get from 05 to
02 get
O7 wait how the heck does the mean go
O7 wait how the heck does the mean go
from I don't understand this wait Min
from I don't understand this wait Min
Min Max
Min Max
oh oh something screwy here is this
oh oh something screwy here is this
parameter well
defined how's this the
mean okay so it's Max is
995 min this point mean should be
995 min this point mean should be
Point
Point
98 maybe these are the wrong order hold
on min max mean scale
on min max mean scale
right we do switch Max and Min yeah and
right we do switch Max and Min yeah and
then mean and
then mean and
scale okay so something screwy here
what is Max
what is Max
.98 min max mean
scale logic normal min max mean and
scale logic normal min max mean and
scale
right min max mean scale min
right min max mean scale min
oh
stupid
okay I probably don't need to reverse
okay I probably don't need to reverse
these now either do I let's
see okay that's better
yeah there we
go gamma
go gamma
Lambda Gamma
Lambda Gamma
Lambda these are
good BT Horizon
good BT Horizon
yep everything look good there
I think the big one is it's just not
I think the big one is it's just not
going to explore the
going to explore the
uh the highend of the total time steps
uh the highend of the total time steps
we will see
though I know nothing about AI is there
though I know nothing about AI is there
a tool that allows regular players to
a tool that allows regular players to
share their game matches with AI which
share their game matches with AI which
is intended to help them learn is that
is intended to help them learn is that
every currently and probably won't be
every currently and probably won't be
seeing in the near future just makes
seeing in the near future just makes
sense for it to
sense for it to
exist thanks uh I think people have done
exist thanks uh I think people have done
kind of shitty versions of this for
kind of shitty versions of this for
individual
games um
games um
there's not just like a plug into any
there's not just like a plug into any
game and have it work to help AI learn
game and have it work to help AI learn
oh that that's separate
oh that that's separate
um it's still going to be per game
um it's still going to be per game
there's not just a thing that's going to
there's not just a thing that's going to
take arbitrary data and solve it we're
take arbitrary data and solve it we're
still at the point where like you need
still at the point where like you need
to have there's a fair bit of per
to have there's a fair bit of per
problem work to be done I mean the
problem work to be done I mean the
closest thing that I've seen to what
closest thing that I've seen to what
you're describing there's this really
you're describing there's this really
funny uh League of Legends Tool that you
funny uh League of Legends Tool that you
can get where uh you can get like like
can get where uh you can get like like
it's it's like an AI that looks at your
it's it's like an AI that looks at your
screen it's probably a GPT rapper or
screen it's probably a GPT rapper or
something uh but then based on what's
something uh but then based on what's
going on you get Tyler one yelling at
going on you get Tyler one yelling at
you for being stupid which is pretty
you for being stupid which is pretty
funny so it's like this like fake AI
funny so it's like this like fake AI
coach
coach
thing that's the closest thing I've
thing that's the closest thing I've
seen I don't know if this is useful it's
seen I don't know if this is useful it's
but it's pretty funny
e
that's not
that's not
bad so just random
bad so just random
search I mean doesn't solve the
Tas the cost is too
low I mean it's not going to explore the
low I mean it's not going to explore the
full range unless you set
full range unless you set
it hang
it hang
on where's my uh my
on where's my uh my
window okay this is almost
window okay this is almost
done so we can compare to neoc
carps
carps
okay it doesn't quite solve the task
okay it doesn't quite solve the task
either here but this is with the cost
either here but this is with the cost
awareness
awareness
uh it's not doing higher cost over time
uh it's not doing higher cost over time
so something seems to have gone horribly
so something seems to have gone horribly
wrong
wrong
here I'm surprised it's
found I'm actually kind of surprised
found I'm actually kind of surprised
that it's found Solutions this good at
that it's found Solutions this good at
this low
this low
cost what did it
cost what did it
do any
do any
batches got Lambda spot on gamma spot on
batches got Lambda spot on gamma spot on
update
update
BPT Horizon off a little
BPT Horizon off a little
bit but I guess it increases cost and
bit but I guess it increases cost and
num man is spot on so I guess it did
num man is spot on so I guess it did
everything perfectly except that it
everything perfectly except that it
didn't want to increase the cost via
didn't want to increase the cost via
time step so it's a little too
time step so it's a little too
conservative here so that's why it's not
conservative here so that's why it's not
full solved interesting
enough now if I just do
enough now if I just do
do I'm pretty sure that if I just do
um if I just set this to one real
quick oh it's a little better but it
quick oh it's a little better but it
doesn't full solve
still it's going to it does explore the
still it's going to it does explore the
higher cost
higher cost
ones but uh it doesn't full solve still
ones but uh it doesn't full solve still
okay I mean it's random search you you
okay I mean it's random search you you
don't expect it to really do too
much so the next thing is going to be
um Asha
I really wish that there were random
I really wish that there were random
searches of base if like random search
searches of base if like random search
would just be a baseline here let me
would just be a baseline here let me
just make sure before I invest time in
just make sure before I invest time in
this that I think this is going to be
this that I think this is going to be
reasonable think I got a little jumbled
reasonable think I got a little jumbled
among my own words I saw some of your
among my own words I saw some of your
games on puffer what if the game was
games on puffer what if the game was
played by real people would it be
played by real people would it be
possible to improve these
possible to improve these
Bots yeah so it yes that is possible if
Bots yeah so it yes that is possible if
we set it up to collect data but the
we set it up to collect data but the
thing is you need quite a bit of data um
thing is you need quite a bit of data um
to give you an idea
to give you an idea
here these things like these games that
here these things like these games that
we have when we train these they are so
we have when we train these they are so
fast like you see this running in real
fast like you see this running in real
time but when we run this for training
time but when we run this for training
like this game runs at hours of gameplay
like this game runs at hours of gameplay
every second it runs at like 10,000
every second it runs at like 10,000
times real time or something ridiculous
times real time or something ridiculous
like that they're just very very highly
like that they're just very very highly
optimized so like this one for instance
optimized so like this one for instance
we literally trained a model to play
we literally trained a model to play
2,000 years worth of this game in a few
2,000 years worth of this game in a few
days on one GPU um so the amount of
days on one GPU um so the amount of
human data you need is quite substantial
human data you need is quite substantial
uh in order to get like really good
uh in order to get like really good
stuff out of
stuff out of
it it's also just not the area that I
it it's also just not the area that I
work in for the most part like most of
work in for the most part like most of
my work isn't on human data it's on like
my work isn't on human data it's on like
learning from interaction so I just
learning from interaction so I just
haven't really gone too far down that
haven't really gone too far down that
route um the methods in that area are
route um the methods in that area are
generally pretty easy and better
generally pretty easy and better
established whereas the area I'm working
established whereas the area I'm working
in here is like a way less developed
in here is like a way less developed
area of science there's just like a lot
area of science there's just like a lot
of potential for
improvement but yeah this this
improvement but yeah this this
environment here if each like time you
environment here if each like time you
see it move a tile that's one step this
see it move a tile that's one step this
thing runs at 1.9 million steps per
thing runs at 1.9 million steps per
second on one CPU
core without rendering when we uh when
core without rendering when we uh when
we run it for training so yeah these
we run it for training so yeah these
things are just like unfathomably
Fast they're all implemented in they're
Fast they're all implemented in they're
I mean they're all implemented in
I mean they're all implemented in
optimized C or relatively optimized C so
optimized C or relatively optimized C so
they're
fast okay so here's our random
fast okay so here's our random
Baseline know we average it a few times
Baseline know we average it a few times
it's probably 180 190 something like
it's probably 180 190 something like
that let's go look at
Asha back to review
mode so cars is a very complicated
mode so cars is a very complicated
method um it's quite possible is this
method um it's quite possible is this
it this is not it
it this is not it
carbs than you
AI let's do this so
carbs the reason that I'm a little
carbs the reason that I'm a little
suspicious right is they do all of this
suspicious right is they do all of this
math uh but they don't like smash the
math uh but they don't like smash the
Bas lines right it seems like they're
Bas lines right it seems like they're
really just the first ones to throw this
really just the first ones to throw this
on
on
RL and they lose to Asha by quite a
lot but the thing is Asha does worse on
lot but the thing is Asha does worse on
well actually no it wins at language
modeling it only does worse on image
classification it almost seems like a
classification it almost seems like a
bug because I have no idea why it would
bug because I have no idea why it would
just like randomly be way worse at
just like randomly be way worse at
that let me
that let me
see cuz like okay tpe and hebo are
see cuz like okay tpe and hebo are
pretty consistent right tpe is
pretty consistent right tpe is
consistently a little bit ahead then
consistently a little bit ahead then
carbs
is well it's not ahead here okay so
is well it's not ahead here okay so
there are a few shakeups I
guess so this result bothers me that o
guess so this result bothers me that o
was way down
here carbs performance is consistent
here carbs performance is consistent
across
across
tasks I
mean not really it sucks at language
modeling oh wait lowers better here why
modeling oh wait lowers better here why
would they do this what is wrong with
them what the hell's wrong with them
them what the hell's wrong with them
that they
that they
do episode reward accuracy then cross
do episode reward accuracy then cross
entropy okay hold on
so okay Asha is like second to carbs
so okay Asha is like second to carbs
here it's the best on
here it's the best on
RL and it's like the worst on image
RL and it's like the worst on image
classification
much lower variance distribution of
outputs that looks pretty high variance
outputs that looks pretty high variance
to
me they have bigger they have more
me they have bigger they have more
variance on ourl and Asha
does I have no idea why it would be
does I have no idea why it would be
specifically better for RL but let's see
let's go read this thing in a little
let's go read this thing in a little
more
more
detail it's
2020 I wonder if they're better
2020 I wonder if they're better
algorithms since then let's see
algorithms since then let's see
optuna what hyperparameter algorithms
optuna what hyperparameter algorithms
they
they
have state-of-the-art algorithms
have state-of-the-art algorithms
not
clickable where are your
algorithms red
algorithms red
search TP okay here's
search TP okay here's
tpe which we saw
right here is
tpe it
tpe it
does not so great on language
does not so great on language
modeling not so great on
modeling not so great on
RL and pretty well on image
RL and pretty well on image
classification they got C GP is just
classification they got C GP is just
kind of
kind of
generic non-dominated sorting genetic
generic non-dominated sorting genetic
algorithm
algorithm
2 in quasi
monflo this seems to not
be okay so it seems like really there
be okay so it seems like really there
aren't like updated libraries with all
aren't like updated libraries with all
the good
the good
algorithms this seems very neglected a
algorithms this seems very neglected a
very neglected area of science
let's look at
Asha scales linearly with number for
yep they offer this as a service this is
yep they offer this as a service this is
like a 100 lines of code that they offer
like a 100 lines of code that they offer
as a service that's fun
where's this
from CMU Google research and determined
from CMU Google research and determined
okay so some startup
okay so this is what they
okay so this is what they
use sequential
methods we don't care about
this Asha
algorithm successive
having the need to adopt it to a okay
having the need to adopt it to a okay
this is silly because it's trivially
this is silly because it's trivially
this is embarrassingly
this is embarrassingly
parallel allocate a small budget to each
parallel allocate a small budget to each
configuration evaluate all
configuration evaluate all
configurations keep the top
configurations keep the top
fraction increase the budget and repeat
fraction increase the budget and repeat
until maximum budget is
reached the resources can the resource
reached the resources can the resource
is going to be wall clocked
obviously
input number of configurations and
minimum resource R maximum resource
R reduction Factor
R reduction Factor
n minimum early stopping rate
s
okay s Max is log
okay T is get hyper parameter
okay T is get hyper parameter
configuration of n so you get and Hyper
configuration of n so you get and Hyper
parameter
configurations so you get what the hell
configurations so you get what the hell
is
is
this
ni r
ni r
i okay I don't understand what they're
i okay I don't understand what they're
doing with this but this seems like a
doing with this but this seems like a
very simple algorithm you're basically
very simple algorithm you're basically
just getting the top
K it doesn't seem like they're
adding more configurations back in
takes more resources to explore same
takes more resources to explore same
number configurate
number configurate
okay so this is literally just
okay so this is literally just
like run some configurations keep the
like run some configurations keep the
best ones double the time
best ones double the time
allocated um very simple idea unless I'm
allocated um very simple idea unless I'm
missing something I'll drop this into
missing something I'll drop this into
Claude in a second I use it just to
Claude in a second I use it just to
sanity check I'm not missing stupid
sanity check I'm not missing stupid
straightforward ways are not suited
straightforward ways are not suited
for parallel
for parallel
regime embarrassingly parallel approach
regime embarrassingly parallel approach
this is not
this is not
suited we would like results in little
suited we would like results in little
more than the time to
more than the time to
train one config what I don't understand
train one config what I don't understand
assume training time for configuration
assume training time for configuration
scales linearly with allocated
scales linearly with allocated
resource for given bracket the minimum
resource for given bracket the minimum
time to return a configuration
trained the time needed to return a
trained the time needed to return a
fully
fully
contrained which is three times time
contrained which is three times time
since there are three rungs each rung is
since there are three rungs each rung is
allocated our
resource the return answer in just our
resource the return answer in just our
time I don't understand that at all so
time I don't understand that at all so
let me
see maximum resource
R evaluate all configurations increase
R evaluate all configurations increase
budget until maximum per configuration
budget until maximum per configuration
budget
budget
of R is
reached oh I think that they're trying
reached oh I think that they're trying
to like reduce the number of
to like reduce the number of
configurations by at least a factor of
configurations by at least a factor of
two and then double the resources
two and then double the resources
allocated so they're running half the
allocated so they're running half the
number of jobs for twice the time or
number of jobs for twice the time or
something like
that this seems very
arbitrary and they say within a bracket
arbitrary and they say within a bracket
we each round promotions are
wrong okay sure whatever
another naive way of paralyzing CH
another naive way of paralyzing CH
distribute the
distribute the
training of the N overend of the K
training of the N overend of the K
surviving
surviving
configurations on each
configurations on each
rung and add brackets when there are no
rung and add brackets when there are no
jobs available in existing graphic
jobs available in existing graphic
synchronous Sha Sha synchronous nature
synchronous Sha Sha synchronous nature
is sensitive to stragglers and drop jobs
is sensitive to stragglers and drop jobs
as every configuration must
as every configuration must
complete before con proceeding to the
complete before con proceeding to the
next r
next r
estimate of the top one of our end
estimate of the top one of our end
configurations for early stopping does
configurations for early stopping does
not improve as more brackets are run
not improve as more brackets are run
since promotions are performed
since promotions are performed
independently for each
bracket okay let's see what they're
bracket okay let's see what they're
doing I think I think they're vastly
doing I think I think they're vastly
over complicating this this seems like a
over complicating this this seems like a
very simple algorithm that they're over
very simple algorithm that they're over
complicating unless they're doing
complicating unless they're doing
something very very clever right here
I mean the idea here is is literally
I mean the idea here is is literally
just like randomly sample parameters run
just like randomly sample parameters run
them all for a short amount of time see
them all for a short amount of time see
which ones do well keep those running
which ones do well keep those running
for longer repeat that's
for longer repeat that's
it we now introduce a sha effective
it we now introduce a sha effective
technique to paralyze
technique to paralyze
sha promotes configurations to the next
sha promotes configurations to the next
R whenever
R whenever
possible instead of waiting for a rum to
possible instead of waiting for a rum to
complete before proceeding to the next
complete before proceeding to the next
Rong if no promotions are
Rong if no promotions are
possible Asha simply adds a
possible Asha simply adds a
configuration to the base Rong the more
configuration to the base Rong the more
configurations can be promoted to Upper
configurations can be promoted to Upper
rungs so asynchronous nature does not
rungs so asynchronous nature does not
require the use of pree by number of
require the use of pree by number of
configuration to evaluate but it
configuration to evaluate but it
otherwise requires the same number the
otherwise requires the same number the
same inputs as
Shaw run then return Val loss is
Shaw run then return Val loss is
asynchronous the code execution
asynchronous the code execution
continues after the job is passed to the
continues after the job is passed to the
worker promotion scheme is laid out in
worker promotion scheme is laid out in
the get job sube okay so whatever
the get job sube okay so whatever
they're doing things um they're doing
they're doing things um they're doing
things
things
whatever the only place we would
whatever the only place we would
possibly do this m like
possibly do this m like
on we're probably just going to have one
on we're probably just going to have one
job of these per node unless we're using
job of these per node unless we're using
a really tiny networks in which case we
a really tiny networks in which case we
could just run this on CPU and see what
could just run this on CPU and see what
happens I don't think it's very I don't
happens I don't think it's very I don't
think it's much faster
think it's much faster
though our fastest uh training
though our fastest uh training
configurations are about 1 million 1.2
configurations are about 1 million 1.2
and then usually we get like 200k or
and then usually we get like 200k or
something on CPU so unless I'm wrong and
something on CPU so unless I'm wrong and
unless the CPU is way faster like 16 *
unless the CPU is way faster like 16 *
2 well 200 that's still 3 million maybe
2 well 200 that's still 3 million maybe
it's worth CPU we'll see I mean
it's worth CPU we'll see I mean
technically we could do both we could
technically we could do both we could
have GPU and CPU drops that'd be kind of
have GPU and CPU drops that'd be kind of
cool
Asha is well suited for the large scale
Asha is well suited for the large scale
regime wall clock time is constrained to
regime wall clock time is constrained to
a small multiple of the time needed to
a small multiple of the time needed to
train a single
model I mean that's kind of like
PBT population based
training population Based training is a
training population Based training is a
pain in the ass to parallelize though if
pain in the ass to parallelize though if
you really want to do it well I mean
you really want to do it well I mean
there's a naive method that's kind of
decent yeah no population Based training
decent yeah no population Based training
is really
is really
um yeah that's not really easy to
um yeah that's not really easy to
paralyze I think we're going to stay
paralyze I think we're going to stay
away from
that also this is not a concern for us
that also this is not a concern for us
uh we're not small multiple time to
uh we're not small multiple time to
transing this is what you do when you
transing this is what you do when you
have like hundreds of nodes
13 over 9 * time of
r nine machines are
r nine machines are
sufficient to
sufficient to
promote configurations to the next Rong
promote configurations to the next Rong
and the time it takes to train and
and the time it takes to train and
configure a single configuration in the
configure a single configuration in the
Rong and training time for a
Rong and training time for a
configuration in run zero is 1 nth of
configuration in run zero is 1 nth of
the time one run it's
the time one run it's
13 or rung two it's time of
R log n machines are needed to advance a
R log n machines are needed to advance a
configuration in the next run the same
configuration in the next run the same
time it takes to train a single
time it takes to train a single
configuration in the
configuration in the
Run okay I don't really understand what
Run okay I don't really understand what
they're doing we're going to look at the
they're doing we're going to look at the
pseudo code
hyper band simply runs multiple sha
hyper band simply runs multiple sha
brackets we can asynchronously paralyze
brackets we can asynchronously paralyze
hyper
hyper
band
band
interesting so let's take a look at
interesting so let's take a look at
this for each worker get
this for each worker get
job run then return Val loss
job run then return Val loss
complete a
complete a
job update configuration
job update configuration
Theta in rung K with l
l so what do they do they do 4K
is log
I think they're just running fewer jobs
I think they're just running fewer jobs
for a machine is all
oh do they have a comparison to PBT
here no PBT is pentry Bank in this case
here no PBT is pentry Bank in this case
okay
let's do uh
what is
sherpa oh okay hyper parameter
sherpa oh okay hyper parameter
algorithm oh cool they have uh
algorithm oh cool they have uh
they got
they got
PBT they got a think really
PBT they got a think really
cool very
nice we'll see what their implementation
looks and what why
where's
Asha
algorithms sh algorithm successive Happ
algorithms sh algorithm successive Happ
thing
there's
algorithms My
algorithms My
Guy
where where
this it's the wrong
freaking is this the wrong one oh yeah
freaking is this the wrong one oh yeah
wait this is the wrong
wait this is the wrong
one isn't it wait didn't I click this
one isn't it wait didn't I click this
from here
here okay there we go me Dum Shar
here okay there we go me Dum Shar
algorithms okay good it's
algorithms okay good it's
simple there we
go oh what a nice Library this is
actually this is nice and simple good
actually this is nice and simple good
job um
um yeah this is
um yeah this is
good they have they have foure dead
good they have they have foure dead
spaces study
search R
search they actually have some nice
search they actually have some nice
little algorithms I could just try all
little algorithms I could just try all
these on our synthetic test as a
these on our synthetic test as a
baseline the only thing is I don't know
baseline the only thing is I don't know
if they've handled the uh the space
if they've handled the uh the space
Transformations
Transformations
correctly though I can probably
correctly though I can probably
Benchmark their version to start and
Benchmark their version to start and
then see if I can do
better that might be a fun thing to do
yeah let's do
yeah let's do
that cuz this looks really simple I'm
that cuz this looks really simple I'm
not going to add this as a dependency to
not going to add this as a dependency to
puffer it's just not happening but
um we will use it for some tests see if
um we will use it for some tests see if
we can beat
we can beat
carbs just you know out the gate
I think this will have finished by now
I think this will have finished by now
yeah so here's neoc
carbs not bad I got to about there with
carbs not bad I got to about there with
the full thing this is with clost
the full thing this is with clost
awareness a couple little quirks here I
awareness a couple little quirks here I
think we could make this still a fair
think we could make this still a fair
bit better but let's just see if we like
bit better but let's just see if we like
instantly crush this or anything right
okay you do not need Caris
good and they do have
good and they do have
log they don't let you specify me
log they don't let you specify me
they just let you specify
bounds it's not terrible though
like this
what the hell
what the hell
dashboard
e e
I don't
I don't
know I'm kind of tempted to just do my
know I'm kind of tempted to just do my
own implementation of
this they've got a bunch of obnoxious
this they've got a bunch of obnoxious
stuff here some
stuff here some
dashboard their own spaces
yeah I think we're going to just do our
yeah I think we're going to just do our
own it's only like 100 lines I'd rather
own it's only like 100 lines I'd rather
do that than have to implement this like
do that than have to implement this like
to add bindings for this stupid
thing just popping in how's it coming
thing just popping in how's it coming
going well running new tight Sweep use
going well running new tight Sweep use
your log X Trek should be like make the
your log X Trek should be like make the
sweep like five billion
sweep like five billion
steps writing 3D comp graphs go up and
steps writing 3D comp graphs go up and
right yeah I mean they're going to go up
right yeah I mean they're going to go up
and right on log scale
and right on log scale
maybe but like should it take five
maybe but like should it take five
billion steps to solve a simple level of
billion steps to solve a simple level of
your puzzle probably
not let me
not let me
do this back to
do this back to
here for
gra
this it starts
this it starts
with with random
right moded files
oops
oops
[Music]
[Music]
rain
one e
okay so it's just get suggestion I
okay so it's just get suggestion I
assume is the their API and then it just
assume is the their API and then it just
calls
calls
everything so we do
you start this with
okay I see
so I don't know why they have a data
so I don't know why they have a data
frame
jeez this is like very short but very
jeez this is like very short but very
confusing the way it's implemented
I really don't like the way this is
I really don't like the way this is
implemented
let me
let me
see you get a
job get job
job get job
is check if there is a promotable
fake e
minimum resource each config will be
minimum resource each config will be
trained for maximum
resource elimination rate
resource elimination rate
minimum early
stop okay that's not
stop okay that's not
bad
bad
um yeah that's not bad
you get a
suggestion top and
I don't see how this is
I don't see how this is
working seems like you need to
working seems like you need to
run a bunch of Trials before you do
this there's something tricky here
because if you initialize this with
16 how many times do you want to double
16 how many times do you want to double
this thing like
probably have two different cut
offs
e e
and I think
all this is just for a
baseline okay hold
on we can do
I think we can essentially
I think we can essentially
do the maximum performance of
this we can essentially get the maximum
this we can essentially get the maximum
performance of
this without implementing
this without implementing
it because the thing is it's based on a
it because the thing is it's based on a
random search
I think that's
fair let me see
fair let me see
something I think
something I think
if we're being smart here we can get we
if we're being smart here we can get we
can get the performance of this
thing okay so all we have to do
is if I just do like args
trains sweep
what if I just do
this is this correct
this okay so here
okay
so this does not consistently solve this
so this does not consistently solve this
Benchmark even at Max
cost it gets close on some of them but
cost it gets close on some of them but
it's not consistently
it's not consistently
solving this
solving this
Benchmark uh let me think if there's
Benchmark uh let me think if there's
anything I'm missing here
there could
there could
be some parameter
adjustments I
adjustments I
think yeah there could be some parameter
think yeah there could be some parameter
adjustments hold
on so this uniform is these are just s
on so this uniform is these are just s
randomly which is
fine about
to no I think that these are pretty good
so if we do this
right for
so here we have
so here we have
263 which is
solved those are very good
hypers this is 200 experiments
though and not consistently solve in
200 so basically the premise
200 so basically the premise
here is that Asha is random
here is that Asha is random
search uh with basically compute
search uh with basically compute
adjustments on it but if you're going to
adjustments on it but if you're going to
start with 200
experiments if you're going to start
experiments if you're going to start
with 200 experiments
with 200 experiments
then as is not going to do better than
then as is not going to do better than
just running those 200 experiments for
just running those 200 experiments for
the full amount of time so this
the full amount of time so this
basically this gives us an upper bound
basically this gives us an upper bound
on asha's performance uh it's going to
on asha's performance uh it's going to
drop some of those it's actually going
drop some of those it's actually going
to be a little worse than this and this
to be a little worse than this and this
is not full solving this task and I
is not full solving this task and I
believe that you should be able to solve
believe that you should be able to solve
this task uh in less than 200
this task uh in less than 200
samples so this is not going to cut it
samples so this is not going to cut it
for
for
us which which makes sense because again
us which which makes sense because again
it's
it's
like it's just based on random search
like it's just based on random search
it's not adaptive in any way
it's not adaptive in any way
so I mean it can probably get pretty
so I mean it can probably get pretty
close to these numbers which is like
close to these numbers which is like
decent but my guess is that the reason
decent but my guess is that the reason
that it uh it looked good on the
that it uh it looked good on the
baselines versus carbs is carbs is just
baselines versus carbs is carbs is just
very undertuned in the original
very undertuned in the original
implementation
implementation
so basically
so basically
whatever we do is going to have to be
whatever we do is going to have to be
adaptive to some
adaptive to some
extent
extent
um I think carbs really does make the
um I think carbs really does make the
most sense they're just some small
most sense they're just some small
algorithmic adjustments and some code
algorithmic adjustments and some code
quirks that we're going to have to
quirks that we're going to have to
address so this does give me more
address so this does give me more
confidence investing more into
confidence investing more into
carbs
carbs
um so I'm going to go I'm going to use
um so I'm going to go I'm going to use
the rest real quick I'm going to get
the rest real quick I'm going to get
myself another cup of
myself another cup of
tea and then we're going to continue on
tea and then we're going to continue on
uh on carbs so having a stronger
uh on carbs so having a stronger
Baseline because this is basically upper
Baseline because this is basically upper
bound of random search is what we just
bound of random search is what we just
did so yeah be right
back
e
e
e
e
e
e
e
e
e
e
e e
you know it's occurred to
you know it's occurred to
me that we should look at some of the
me that we should look at some of the
other
other
baselines uh versus carbs on RL because
baselines uh versus carbs on RL because
Asha should not beat
Asha should not beat
carbs um carb should be an improvement
carbs um carb should be an improvement
on
on
hebo which should I mean this should
hebo which should I mean this should
very handily beat Asha Asha is a very
very handily beat Asha Asha is a very
bad algorithm uh now that I'm looking at
bad algorithm uh now that I'm looking at
it so
it so
so let me see why that's not
happening too many
tabs too many
tabs too many
tabs uh where' the paper
tabs uh where' the paper
go here yeah so this is a bad algorithm
go here yeah so this is a bad algorithm
um back to review mode
where is
the where is it okay reinforcement
the where is it okay reinforcement
learning so Asha is up
learning so Asha is up
here okay carbs his second
best so the fact that
best so the fact that
Asha wins here is
insane terminates lowest performing runs
insane terminates lowest performing runs
early it's able to sample many more
early it's able to sample many more
parameters than other
parameters than other
algorithms that doesn't make
algorithms that doesn't make
sense at
sense at
all because the thing
all because the thing
is whatever the length of time is it's
is whatever the length of time is it's
running those early samples for is about
running those early samples for is about
the length of time that carb should be
the length of time that carb should be
running its early
running its early
samples so basically what that
means is that
means is that
the carbs and Asha doesn't build a
the carbs and Asha doesn't build a
predictive model of performance either
predictive model of performance either
so what should happen is carbs is going
so what should happen is carbs is going
to run a bunch of quick experiments like
to run a bunch of quick experiments like
Asha does but unlike Asha it should be
Asha does but unlike Asha it should be
running uh a predictive model of
running uh a predictive model of
performance it should be building a
performance it should be building a
predictive model of performance which
predictive model of performance which
then allows it to more intelligently
then allows it to more intelligently
select better parameters for longer
runs I guess what Asha does it just says
runs I guess what Asha does it just says
that hey whatever parameters I happen to
that hey whatever parameters I happen to
randomly sample that turn out to be good
randomly sample that turn out to be good
just keep running those but that's not
just keep running those but that's not
quite what you want to
quite what you want to
do you really want to be able to look at
do you really want to be able to look at
the good runs and then search locally
the good runs and then search locally
around those because like and use a
around those because like and use a
predictive model of performance like
predictive model of performance like
most of the variables in the hyper
most of the variables in the hyper
parameter search are pretty damn easy to
parameter search are pretty damn easy to
fit they're like mostly independent and
fit they're like mostly independent and
they've got a stable region or like it
they've got a stable region or like it
just like more is better or less is
just like more is better or less is
better you know something like that
does this out form hebo on
everything yes it does
everything yes it does
okay hebo is a simpler Baseline that
okay hebo is a simpler Baseline that
carbs is built off of it's not hugely
carbs is built off of it's not hugely
better though but at the cost awareness
better though but at the cost awareness
is important because I don't think the
is important because I don't think the
these tell the full picture Okay um
these tell the full picture Okay um
things to modify next on
carbs we've got our current
performance right
performance right
here okay so this is cost
aware and then this one
so this gets up to about there and then
so this gets up to about there and then
this
this
one the non-cost to wear one Sol in like
one the non-cost to wear one Sol in like
80ish 8 samples and it actually gets
80ish 8 samples and it actually gets
even like a little last bit of perf at
even like a little last bit of perf at
the end here you can
the end here you can
see it gets pretty well up there and
see it gets pretty well up there and
then it does the total time steps
then it does the total time steps
towards the end okay so
towards the end okay so
maybe wait a second
something screwy
here let me go look at the uh the
here let me go look at the uh the
formula I made maybe there's something
formula I made maybe there's something
true with the
test log two of total time
steps
um how do they get
score up
here how do they get score up
here how do they get score up
here without the total time steps being
here without the total time steps being
up
there okay I think there is something
there okay I think there is something
potentially screwy here because total
potentially screwy here because total
time step should be
total time steps should have more of an
effect something is quite screwy
here let me just make sure
okay so
okay so
254 is supposed to be the
254 is supposed to be the
max it found a 262 somehow hold on what
max it found a 262 somehow hold on what
the something's Bizarro
um
oh wait this is
oh wait this is
one9 this should
one9 this should
be is it one10 is the max
okay yeah 282 is the
max but something screwy there
oh I
see
oops e
let me think how I do this
it's better
27 is
27 is
like too
small for most
small for most
problems even be sampling
there five
Square Ro of batch
size e
it's not
bad that's probably
better cost is going to be
milon and this num m is okay
milon and this num m is okay
good let me see if this does anything
good let me see if this does anything
different
I kind of don't
like well this is fine just
like well this is fine just
scaling little time
steps I think this is
fine I think what was happening before
fine I think what was happening before
is that um the time steps on the score
is that um the time steps on the score
just wasn't very
sensitive it should be the case that
like okay so this is zero
do+
one so that gives you going from 50
one so that gives you going from 50
million to a
billion yeah so that actually gives you
billion yeah so that actually gives you
a reasonable
a reasonable
sport that's
better let me see just off of this hey
better let me see just off of this hey
welcome I miss see just off of
this if there's a substantial difference
this if there's a substantial difference
and then then while this runs we'll look
and then then while this runs we'll look
at other outfit and
stuff
stuff
cars where is this
I don't want to wait for
freaking we don't need this
anymore I want to wait for Neptune for
anymore I want to wait for Neptune for
this I kind of should
so this should
my
token and then this is the full task
token and then this is the full task
right yeah the full cost to wear task
good so let's see if that changes
good so let's see if that changes
anything
anything
and we'll see which parameters if any
and we'll see which parameters if any
now are
screwy oh yeah they're
screwy oh yeah they're
uh what are we
uh what are we
printing
score cost
no this isn't good they're all the same
no this isn't good they're all the same
they're all the same
they're all the same
score okay let's figure out what the
score okay let's figure out what the
hell's wrong with
this oh
this oh
du so that's like max four
this will actually be a nice
test we should scale this so that the
test we should scale this so that the
max score is like 100
though me think had to do
this maybe at the
this maybe at the
end let's just run this let's just get
end let's just run this let's just get
this
running actually and let's make sure we
running actually and let's make sure we
[Music]
[Music]
remove we remove the
these mess up our
run there we
run there we
go so the max is now like 3,000 or
go so the max is now like 3,000 or
whatever
where am
where am
I oh this is not doing it globally
I oh this is not doing it globally
okay just make sure we get Neptune on
wait test
wait test
carbs yeah right
here okay so that'll
run
run
check couple quick
things e
so here's neoc
carbs we'll see what this
carbs we'll see what this
does in the
does in the
meantime we look at the paper again
and we look for things to
improve this doesn't work quite the way
improve this doesn't work quite the way
I want it
to it's mostly these two pages the math
to it's mostly these two pages the math
pages
they've got three gan processs
it occurs to me that you could get
it occurs to me that you could get
dramatically more training data for
dramatically more training data for
these
these
things by giving it every
things by giving it every
single like checkpointed output of the
model I don't know if I want to do that
model I don't know if I want to do that
though because it basically presumes
though because it basically presumes
that your your graphs are nice and
that your your graphs are nice and
stable
I could just make sure
I could just make sure
that that they
are it's a little tricky though cuz like
are it's a little tricky though cuz like
if you need to evaluate 10 million steps
if you need to evaluate 10 million steps
worth of data for something to be stable
worth of data for something to be stable
like that per those perf numbers are
like that per those perf numbers are
already
old so I think we'll stick
with I guess we stick with the the
with I guess we stick with the the
current model
I mean
I mean
it's how many points do you need to
it's how many points do you need to
train a gan process
10 times for basic training
ah GP scale poorly with
data use
need some full solt puff puffer
merch once we get there you don't have
merch once we get there you don't have
you don't start making merch until
you don't start making merch until
you've got a bunch of Revenue and we've
you've got a bunch of Revenue and we've
got only a little Revenue at the
got only a little Revenue at the
moment there will eventually be puffer
moment there will eventually be puffer
things
let's solve RL first
freaking gaussian processes
man all right well that's actually
man all right well that's actually
looking way
looking way
better look at that 39 points and total
better look at that 39 points and total
time steps it's properly
increasing it's a little
increasing it's a little
early I would like it to have
early I would like it to have
full correct
full correct
typers they're pretty close hold
typers they're pretty close hold
on wait a second this is really
close okay it's missed update
close okay it's missed update
epox that should be
four it's literally just missing one
four it's literally just missing one
parameter after only 42 trials
that's very
good let me look at what we have now so
good let me look at what we have now so
we've gotten rid of
pearch we're still you we actually don't
pearch we're still you we actually don't
have a trust region at the
moment this could occasionally get get
moment this could occasionally get get
us a bad
sample we should compare this to hard
sample we should compare this to hard
trust region that will be an experiment
trust region that will be an experiment
we're
trying yeah so we will compare this to
trying yeah so we will compare this to
hard trust
hard trust
region they did not run that
region they did not run that
ablation that seems like a reasonable
ablation that seems like a reasonable
thing to do
yeah and the normal centered on the mean
yeah and the normal centered on the mean
as well as
as well as
like well hang
on yeah no I think the uniform is
on yeah no I think the uniform is
probably
better I mean
is normal a better distribution actually
is normal a better distribution actually
if you think about
it because like if you run a normal
it because like if you run a normal
distribution
distribution
right most of your parameters should
be should be closer to the center and
be should be closer to the center and
then you end up with like a few
then you end up with like a few
parameters that are more
deviating maybe you can solve that just
deviating maybe you can solve that just
by generating more samples
by generating more samples
though it needs to be clipped either way
though it needs to be clipped either way
I
think because otherwise you get outside
think because otherwise you get outside
the support of your gaussian
process so I think that they're correct
process so I think that they're correct
in doing a trust
region but it should have been a hard
region but it should have been a hard
trust
region soft trust region is
region soft trust region is
to it has NE it has weird interactions
to it has NE it has weird interactions
with your other parameters so we're not
with your other parameters so we're not
going to do
that let's look at the acquisition
that let's look at the acquisition
function
clamping this is the really weird one to
me and they do have an ablation for it
so no clamping is a
so no clamping is a
huge fail
here no parito huge fail and the no
here no parito huge fail and the no
resampling actually is not going to hurt
resampling actually is not going to hurt
us at all in the current test but it
us at all in the current test but it
will hurt us in the real
ones no Pito presumably is just they do
ones no Pito presumably is just they do
the
the
Mac so they turn it from a Pito based
Mac so they turn it from a Pito based
search to like a essentially just turns
search to like a essentially just turns
into a simple genetic
algorithm no
clamp this is what they mean right
let me think about this
this kind of does naturally like push up
this kind of does naturally like push up
the
the
time
allocation cuz as you sample points that
allocation cuz as you sample points that
are longer then you push your samples
are longer then you push your samples
towards that end of the
distribution this still is not great
distribution this still is not great
though
this is possibly the weakest part right
this is possibly the weakest part right
now
how are we doing
how are we doing
here over here
what the hell happened did it just get
bored and start doing others parts of
bored and start doing others parts of
the pero
the pero
front where is the Pito
front cost
is there not already
is there not already
a a prito front in
a a prito front in
here I thought I had
here I thought I had
one I guess
not access
okay so this is
okay so this is
the Pito
front to be fair it is actually very
front to be fair it is actually very
nicely filling this
nicely filling this
in
um see 11415
doesn't seem to be biasing strongly
doesn't seem to be biasing strongly
enough
though to over here
yeah I think it's just trying to sample
yeah I think it's just trying to sample
lower cost
lower cost
regions you see the cost goes up and now
regions you see the cost goes up and now
it's like kind of all over
so what would cause
that I guess just um this acquisition
that I guess just um this acquisition
function Maybe
it's a very inelegant way to do an in to
it's a very inelegant way to do an in to
do
this they're just sampling on log scale
this they're just sampling on log scale
so
so
like if you have 10 seconds 100 seconds
like if you have 10 seconds 100 seconds
and a th000
and a th000
seconds a third of your runs are going
seconds a third of your runs are going
to be the really fast stupid
ones a third are going to be these
ones a third are going to be these
like 100ish mil and you're not really
like 100ish mil and you're not really
going to sample that much at the top end
going to sample that much at the top end
are
you but I mean the question's how
you but I mean the question's how
strongly you want to
buyas it seems like it ought to be a
buyas it seems like it ought to be a
function of the confidence of your
function of the confidence of your
prediction
doesn't this
um wait doesn't this give you a
um wait doesn't this give you a
variance the Gan
process it does doesn't
it so like when you stick this into
gpy it should give you
variance let me go find the code for
variance let me go find the code for
that I think that's going to be the the
that I think that's going to be the the
major thing I can do
this is going to be a to write a
this is going to be a to write a
blog post about with all this math I'm
blog post about with all this math I'm
going have to simplify this so much
okay
okay
so here you get your samples
right surrogate model.
observe Sur again
ah you see
ah you see
this it has right here it
this it has right here it
has the surrogate variance in it
okay and then there's this exploration
okay and then there's this exploration
bias so we have expected
Improvement EI value
see acquisition function
value okay so this acquisition function
value okay so this acquisition function
value is just the EI value
value is just the EI value
value you've got a thing for Success
value you've got a thing for Success
probability a thing for Max
probability a thing for Max
cost this is fine it's right
cost this is fine it's right
here expected
Improvement cars utils
oh Jesus okay what is
this well there's a
link so it's this function right here or
link so it's this function right here or
this function right
here and it should
here and it should
be where's mu minus best mu
be where's mu minus best mu
minus exploration bias
probability of improvement
upper tail prob
ability
ability
ah
obnoxious this
obnoxious this
probability much we can improve so
probability much we can improve so
expected this is it
select the point that minimizes the
select the point that minimizes the
distance to the objective evaluated at
distance to the objective evaluated at
the
maximum mockus proposed okay JB
maximum mockus proposed okay JB
mockus feel like I'm being mocked here
what the hell is
this you've
got let me see what the parameters are
got let me see what the parameters are
actually going into this so you
have you have the Gan processes
have you have the Gan processes
prediction of mean and variance
prediction of mean and variance
okay and then you
okay and then you
have moo best moo which you're using as
have moo best moo which you're using as
a Baseline and then some
a Baseline and then some
exploration bias so prior Sigma
okay Z is going to be moo minus best moo
okay Z is going to be moo minus best moo
minus over
minus over
Sigma divide by standard deviation
fine original form is this and then
fine original form is this and then
simplified form
simplified form
is what what the okay um original form
is what what the okay um original form
is Sigma
Times log prob of
Times log prob of
Z didn't we just multiply by
Sigma oh yeah but it's okay it's over
Sigma oh yeah but it's okay it's over
fine so Sigma time log Prop z
fine so Sigma time log Prop z
1+ Z I have no idea what this
is and then there's even more okay
great do I have to care what the hell
great do I have to care what the hell
this
this
is or is the change I want to make
is or is the change I want to make
independent of this
how does the variance even come into
how does the variance even come into
this
if it's the expected
Improvement let me see if I can get GPT
Improvement let me see if I can get GPT
that I
that I
freaking or
freaking or
claw break in my freaking head
here
e
e
e
e e
oh wait hold
on
for
e e
I know what I want to do I got to figure
I know what I want to do I got to figure
out the math behind it
expect the improvement over Baseline is
expect the improvement over Baseline is
also normally
distributed okay so breaking it down
distributed okay so breaking it down
this is standard
Improvement so you penalize the
Improvement so you penalize the
uncertainty you remove the
uncertainty you remove the
re and then wait by cost
so the Improvement is going to
so the Improvement is going to
be mo minus Sigma minus best
be mo minus Sigma minus best
moo
moo
okay so we make this
pessimistic and then what what does this
pessimistic and then what what does this
thing propose this is times
cost 1 plus cost variance over
cost one plus cost variance over cost
I'm trying to
think I don't like this CU this isn't an
think I don't like this CU this isn't an
estimate of improvement right
the idea is to wait
the idea is to wait
heavily we want to wait heavily towards
heavily we want to wait heavily towards
high cost but heavily penalized
high cost but heavily penalized
uncertainty of the estimate
how does c come in
here I'll just C him in here
okay so this is what they're doing at
okay so this is what they're doing at
the moment which is really
stupid this is just like sampling time
stupid this is just like sampling time
in log space or biasing by a log factor
in log space or biasing by a log factor
in
time they're just using this as a
time they're just using this as a
baseline now what we want is we want to
baseline now what we want is we want to
use we want the actual improvement over
use we want the actual improvement over
the Paro
the Paro
point right
this is a deterministic sample at the
moment but that's actually fine yeah cuz
moment but that's actually fine yeah cuz
it is the max
okay so you generate
samples we make the expected Improvement
samples we make the expected Improvement
conservative
we subtract the true Paro
Baseline and now how do we bias
this so we're going to get rid of this
this so we're going to get rid of this
clamping
is
I mean the cost uncertainty should be
I mean the cost uncertainty should be
pretty darn easy to predict
we actually do we want to
upweight we want to upate high
cost let's do this initially and then
cost let's do this initially and then
we'll
see
e e
let me see what else I want to do with
let me see what else I want to do with
this so I
this so I
think I'm just using this thing as like
think I'm just using this thing as like
a sanity check basically to make sure
a sanity check basically to make sure
I'm not doing something like obviously
stupid it will absolutely suggest insane
stupid it will absolutely suggest insane
things if you let it um it's not even a
things if you let it um it's not even a
consistent s you check it's just better
consistent s you check it's just better
than
nothing semi-intelligent rubber
duck
e
e e
directly scaling
cost predicted cost performance
relationship we do have that
If instead of scaling by cost why don't
If instead of scaling by cost why don't
we scale by absolute
performance
um so performance Improvement you do Mo
um so performance Improvement you do Mo
minus Sigma variance and Paro moo
predicted
benefit okay so this is the performance
benefit okay so this is the performance
of the cost
you
more e
let me see
this okay so we get relative performance
then we get absolute performance does
then we get absolute performance does
this
this
work is this insane or does this work
likely better than the current Pito
likely better than the current Pito
front from high relative
front from high relative
Improvement likely to achieve good
Improvement likely to achieve good
absolute
performance for
could
be weighted
some different powers
some different powers
[Music]
M let me see
I think we try
this e
let's
see let me see this
so do I need to make this conservative
so do I need to make this conservative
to start
to start
with these are kind of two separate
improvements but they are somewhat
linked for
Al so
where's the re
I don't see the
re expected
Improvement uh I don't see the
clamp it's supposed to be
NOP there's no
NOP there's no
Max so I don't see the
re a clamp
okay well let's just see if this
okay well let's just see if this
completely breaks everything
right so here's neoc
right so here's neoc
carps it does like interesting
l servative
okay just wait until it gets uh past
okay just wait until it gets uh past
random at least I'm go use the restroom
random at least I'm go use the restroom
and we'll see if we get uh any points
and we'll see if we get uh any points
here
making it
conservative I mean it seems like it
conservative I mean it seems like it
should help it might not I think I'm
should help it might not I think I'm
going to keep it as long as it doesn't
going to keep it as long as it doesn't
completely destroy
performance because it makes sense to do
the idea here is that the Gan process
the idea here is that the Gan process
one of the nice things about it is it
one of the nice things about it is it
gives
gives
you the variance of its
you the variance of its
predictions so you you just subtract the
predictions so you you just subtract the
standard deviation you get a
standard deviation you get a
conservative estimate of performance uh
conservative estimate of performance uh
which then naturally up weights The
which then naturally up weights The
Confident
Confident
predictions which should lead to like
predictions which should lead to like
smoother and cleaner graphs where it
smoother and cleaner graphs where it
it's not just like sampling points that
it's not just like sampling points that
randomly just get terrible performance
randomly just get terrible performance
and wasting an
experiment I don't know if this one
experiment I don't know if this one
change alone is going to be enough to
change alone is going to be enough to
see a major difference um but then what
see a major difference um but then what
we're going to do is this is definitely
we're going to do is this is definitely
a problem here let me
see so this pattern is definitely a
see so this pattern is definitely a
problem it's just spending way too much
problem it's just spending way too much
time on just bad data points uh and you
time on just bad data points uh and you
can see here it's just it's running too
can see here it's just it's running too
many short runs even at the end here
many short runs even at the end here
it's just running too many short runs it
it's just running too many short runs it
hasn't full solve this like it can get
hasn't full solve this like it can get
substantially better performance than
substantially better performance than
this um let me see what it's
missing it's actually pretty funny
missing it's actually pretty funny
because
because
the Paro front if you look at this most
the Paro front if you look at this most
of the points are on the Pito front so
of the points are on the Pito front so
it's not doing like bad experiments it
it's not doing like bad experiments it
just has too many experiments down here
just has too many experiments down here
and it hasn't done enough up
and it hasn't done enough up
here um it should be like having more of
here um it should be like having more of
them out here
because yeah it just it hasn't gotten
because yeah it just it hasn't gotten
all the way full solve
yet this down here is especially
yet this down here is especially
bad there's no reason for it to be doing
this though I guess technically it's
this though I guess technically it's
still possible for carbs to do this um
still possible for carbs to do this um
but only once it's like it's confident
but only once it's like it's confident
that there are no more improvements at
that there are no more improvements at
the top end and I think that I think I
the top end and I think that I think I
checked this and I think that the
checked this and I think that the
maximum was like 3200 3500
maximum was like 3200 3500
something so I think there is more perf
something so I think there is more perf
I will double check that but I think
I will double check that but I think
that there is more perf to be
gained okay that's nice and clean to
gained okay that's nice and clean to
start we'll see if it
continues let me use the restroom I'll
continues let me use the restroom I'll
be right back and then we will we'll let
be right back and then we will we'll let
this thing run for a bit and we'll work
this thing run for a bit and we'll work
on the uh the new acquisition function
on the uh the new acquisition function
one second let's mute
this
e
e
e
e
e
e
e
e
e
e
e
e e
ah forgot to unmute my mic oh are
ah forgot to unmute my mic oh are
supposed to tell me I've been talking
supposed to tell me I've been talking
this whole
this whole
time
um whatever this is good this is I mean
um whatever this is good this is I mean
the summary is that this looks better
the summary is that this looks better
than the other one by a little bit um
than the other one by a little bit um
right up to
right up to
1,600 starting to widen a little bit but
1,600 starting to widen a little bit but
I this is just from adding conservative
I this is just from adding conservative
bound so up to 1600 versus on
here hold on if I do if I do
this I guess these look kind of
similar I don't think the cost looks
similar I don't think the cost looks
similar
similar
though hold on if I look at the
though hold on if I look at the
costs in the same time period right it's
costs in the same time period right it's
like here
c h may be kind of
c h may be kind of
similar I mean I said that if this
similar I mean I said that if this
doesn't horribly screw stuff up we're
doesn't horribly screw stuff up we're
going to keep it right
wait
wait
2600 what was the the highest Flore on
2600 what was the the highest Flore on
the other
the other
one what's the number to
one what's the number to
be oh we're actually better
be oh we're actually better
already this only gets to like 2500 all
already this only gets to like 2500 all
the way over
here yeah we're all the way up to 20
here yeah we're all the way up to 20
okay this is
okay this is
cool whether this is statistically
cool whether this is statistically
significant I I have no
significant I I have no
idea but uh this is it looks
good this is a relatively small
good this is a relatively small
change making it
change making it
conservative um I don't expect this to
conservative um I don't expect this to
magically solve everything hopefully it
magically solve everything hopefully it
doesn't just magically crash everything
doesn't just magically crash everything
either I'm not going to let this run all
either I'm not going to let this run all
200 without doing anything though so
200 without doing anything though so
let's
go so we're going to do a few things
here we're going to do this first of all
here we're going to do this first of all
get rid of
get rid of
that um and
that um and
then acquisition
then acquisition
function it's going to be the
function it's going to be the
conservative expected Improvement and
conservative expected Improvement and
what did we say we have it in here
somewhere yeah so we do relative
somewhere yeah so we do relative
Improvement we fix this and we do
Improvement we fix this and we do
absolute Improvement is going to be moo
absolute Improvement is going to be moo
minus Sigma then we multiply
them
them
salute the I
equals absolute score equals
and let's see if it's uh
and let's see if it's uh
utils
utils
spected do square
root
square okay and then what we're going to
square okay and then what we're going to
do is
multiply
multiply
them so it's going to
them so it's going to
be times conservative
be times conservative
absolute so this will be the new
one La this here as
one La this here as
well now this is dramatically different
well now this is dramatically different
so this might take some tuning and
so this might take some tuning and
such um this is a this is a big
change okay so this looks to be about
change okay so this looks to be about
the same as before
the same as before
frankly which is fine it doesn't break
frankly which is fine it doesn't break
stuff
we've already gotten the good runs out
we've already gotten the good runs out
of
this see 91 items so about halfway
through oh actually this is
through oh actually this is
substantially better you see look at all
substantially better you see look at all
this garbage down here so this is
this garbage down here so this is
actually substantially more
actually substantially more
stable uh than the other one
so the conservative bound actually does
so the conservative bound actually does
do something that's
do something that's
nice I did a
nice I did a
math cool
um let's kill
um let's kill
this and now we're going to see if this
runs
runs
oops ah
MTI let's see if this does anything
we're going to have to monitor several
we're going to have to monitor several
different metrics
different metrics
here might be too conservative might be
here might be too conservative might be
too aggressive um this is a big change
too aggressive um this is a big change
to the acquisition function so we will
to the acquisition function so we will
see
big
change we still have not tuned scales
change we still have not tuned scales
fully uh and a few other
things but I think that this is the
things but I think that this is the
biggest
biggest
spot uh for improvement
spot uh for improvement
here let me think if what I've done
here let me think if what I've done
makes
makes
sense
sense
so initially
they had this
they had this
soft this soft trust region on top of a
soft this soft trust region on top of a
normal that was unintuitively
normal that was unintuitively
constraining your search range so that
constraining your search range so that
you could not find parameters far enough
you could not find parameters far enough
from your starting point it was really
from your starting point it was really
biasing you towards the samples that
biasing you towards the samples that
were basically identical to the runs
were basically identical to the runs
that you've already done we got of that
that you've already done we got of that
um so now we have more aggressive
exploration
exploration
before they were biasing
before they were biasing
towards higher cost
towards higher cost
explicitly by adding like a log
explicitly by adding like a log
weighting
weighting
term uh to the cost so or to the
term uh to the cost so or to the
acquisition
acquisition
function now instead we bias the
function now instead we bias the
acquisition function by a conservative
acquisition function by a conservative
estimate of your absolute performance
estimate of your absolute performance
which is a thing we care about not cost
which is a thing we care about not cost
right so if cost and absolute
right so if cost and absolute
performance are correlated this should
performance are correlated this should
be fairly similar but this should uh
be fairly similar but this should uh
this should prevent you from doing long
this should prevent you from doing long
runs that don't really do all that
well let's see we also made the uh
well let's see we also made the uh
expected Improvement itself
expected Improvement itself
conservative so this accounts for the
conservative so this accounts for the
variance in the model
variance in the model
so the goal of all of this is that we
so the goal of all of this is that we
should
should
get we should get much
get we should get much
cleaner uh much cleaner curves with
cleaner uh much cleaner curves with
fewer points way off the parito
fewer points way off the parito
front that is the goal of
this things to change potentially
this things to change potentially
still we're still using normal
still we're still using normal
sampling I would like to compare to
sampling I would like to compare to
uniform
uniform
sampling because Normal sampling doesn't
sampling because Normal sampling doesn't
um it can get you out of the support
um it can get you out of the support
range we can also compare normal
range we can also compare normal
sampling to or we can compare that to
sampling to or we can compare that to
clipped normal sampling as well we could
clipped normal sampling as well we could
just
clip
e e
I don't think that there's anything C to
I don't think that there's anything C to
think think about I think this is just
empirical
yeah I think this is correct as well
I think we'll try that
next
next
oh wa 37
oh wa 37
samples is this comparable to before is
samples is this comparable to before is
this better
it's a little
it's a little
better we'll see if it keeps it
up it is nice to
up it is nice to
make you know well motivated changes to
make you know well motivated changes to
an algorithm and actually see it do
an algorithm and actually see it do
something it's pretty
good let's see so it's got the batch
good let's see so it's got the batch
size correct already it's got many
size correct already it's got many
batches
correct I don't know why it does this
correct I don't know why it does this
it's pushed learning rate way too far
down and I actually I don't know
why gamma it's hovered around 99 that's
why gamma it's hovered around 99 that's
fine Lambda it's pushed too high as well
fine Lambda it's pushed too high as well
I don't know why it pushes Lambda this
I don't know why it pushes Lambda this
high
high
there really shouldn't be any reason for
there really shouldn't be any reason for
it to do
this I mean the model must just be
this I mean the model must just be
learning a false correlation at the
learning a false correlation at the
start we'll try um we'll try the uniform
start we'll try um we'll try the uniform
sampling next and see if that changes
sampling next and see if that changes
anything the update epox is very nice
anything the update epox is very nice
BPT Horizon is perfect I'm surprised it
BPT Horizon is perfect I'm surprised it
hasn't even sampled anything outside of
hasn't even sampled anything outside of
this now Ms is now correct as well and
this now Ms is now correct as well and
it keeps a nice low cost
it keeps a nice low cost
and here's our Paro
front this is very nice now the question
front this is very nice now the question
is going to be whether it
is going to be whether it
maintains like a sharp
maintains like a sharp
curve or if it
curve or if it
uh if it still meanders around too much
uh if it still meanders around too much
the shape of this curve is going to
the shape of this curve is going to
matter a
lot this is cool though like these are
lot this is cool though like these are
like good well motivated changes to
like good well motivated changes to
carbs
those are very good well motivated
those are very good well motivated
changes to
cars we're going to see what this does
cars we're going to see what this does
when uh when we throw it on a real
when uh when we throw it on a real
environment
h
full DM
oo okay so this is the
oo okay so this is the
2000 this happened
before actually yeah let's just do both
before actually yeah let's just do both
the curves why not um so this is what it
the curves why not um so this is what it
was
was
before and this is what it is now that
before and this is what it is now that
is definitely that is a
is definitely that is a
steeper that's a steeper curve with
steeper that's a steeper curve with
fewer points
now it hasn't gotten like this big jump
now it hasn't gotten like this big jump
yet um but this is this is very
good very good
indeed I'm pretty happy with this
indeed I'm pretty happy with this
overall
yeah that's
great man this is very nice to see you
great man this is very nice to see you
know this is like a very
know this is like a very
intentional uh you know theoretically
intentional uh you know theoretically
motivated change to
motivated change to
carbs and at least on this synthetic
carbs and at least on this synthetic
test it's doing exactly what I think it
test it's doing exactly what I think it
should be doing now I mean we'll see on
should be doing now I mean we'll see on
real environments when there's a whole
real environments when there's a whole
bunch of noise involved uh that's the
bunch of noise involved uh that's the
main difference I guess right the
main difference I guess right the
relationships between hyper parameters
relationships between hyper parameters
aren't perfect and there's a bunch of
aren't perfect and there's a bunch of
noise involved
noise involved
um so we'll see if this
um so we'll see if this
transfers
transfers
but I mean they really can't have done
but I mean they really can't have done
that many experiments on the original
that many experiments on the original
cars because proen is just stupidly
cars because proen is just stupidly
slow so even though they have a bunch of
slow so even though they have a bunch of
gpus like they're literally trading at
gpus like they're literally trading at
less than 1% of the speed that we are on
less than 1% of the speed that we are on
most of our environments you know so
most of our environments you know so
they're like they really can't do very
they're like they really can't do very
much with that and then large language
much with that and then large language
model experiments are going to be crazy
model experiments are going to be crazy
slow so
like I don't know I'd honestly believe
like I don't know I'd honestly believe
that I fiddled with this as much as they
that I fiddled with this as much as they
have they didn't mention any synthetic
have they didn't mention any synthetic
tests either so I don't even think that
tests either so I don't even think that
they did like the rapid development like
they did like the rapid development like
this I think that they I mean this isn't
this I think that they I mean this isn't
even that rapid to be fair
even that rapid to be fair
it's carbs is kind of slow but like
it's carbs is kind of slow but like
still
yeah this is solid holy the question is
yeah this is solid holy the question is
where is uh going to be the end behavior
where is uh going to be the end behavior
so total time steps increasing here is
so total time steps increasing here is
very
good uh batch size just the right part
good uh batch size just the right part
of the graph here batch size is set mini
of the graph here batch size is set mini
batches is
set I don't know what it's doing with
set I don't know what it's doing with
this with the learning rate where it
this with the learning rate where it
like it goes too low
I have an absolute value on this it
I have an absolute value on this it
really should figure this out we'll try
really should figure this out we'll try
the different sample bounds and see if
the different sample bounds and see if
that changes anything so gamma
here gamma here is pretty
here gamma here is pretty
good 99
good 99
okay and Lambda it's starting to figure
okay and Lambda it's starting to figure
out Lambda but like I don't know why it
out Lambda but like I don't know why it
goes up here in the first place
this seems very
this seems very
odd like this is just this is drift this
odd like this is just this is drift this
is like individual hyperparameter Drift
is like individual hyperparameter Drift
But the thing is these are fully
But the thing is these are fully
independent it should be trivial for
independent it should be trivial for
this to figure out the value of Lambda
this to figure out the value of Lambda
which is should be
0.95 update EPO is correct BPT Horizon
0.95 update EPO is correct BPT Horizon
is correct num Ms is correct and then
is correct num Ms is correct and then
cost is nicely scaling and then the Peri
cost is nicely scaling and then the Peri
well this per front is a mess because
well this per front is a mess because
this has two experiments worth the runs
this has two experiments worth the runs
on it
um okay it's starting to level off a
um okay it's starting to level off a
little
bit but
bit but
the yeah it needs total time steps in
the yeah it needs total time steps in
order to max out the score is the main
order to max out the score is the main
thing and with the current the current
thing and with the current the current
version of this uh it will not max out
version of this uh it will not max out
current time steps until it's confident
current time steps until it's confident
at least it
at least it
shouldn't that's actually good though
shouldn't that's actually good though
because if it takes you know this curve
because if it takes you know this curve
up here is actually not as good like
up here is actually not as good like
this is too many experiments that are
this is too many experiments that are
expensive that aren't really doing very
expensive that aren't really doing very
much we would rather have more
much we would rather have more
experiments early that are quick and
experiments early that are quick and
then consistently get the high
then consistently get the high
performance later on so we'll see if
performance later on so we'll see if
that happens this one looks
good oh wow look at that you see so it
good oh wow look at that you see so it
didn't up the cost and it's still
didn't up the cost and it's still
finding it's still finding better
finding it's still finding better
parameters at the current
parameters at the current
cost so this is good because now we're
cost so this is good because now we're
no longer the previous version was
no longer the previous version was
biasing towards high cost they literally
biasing towards high cost they literally
had a bias term to do this but now we
had a bias term to do this but now we
have a bias turn towards better
have a bias turn towards better
performance not higher cost and you only
performance not higher cost and you only
get uh better perform you only get
get uh better perform you only get
higher cost when that's the best way to
higher cost when that's the best way to
improve so this is like I mean this is
improve so this is like I mean this is
the cleanest hyper pram sweep I've ever
the cleanest hyper pram sweep I've ever
seen so far I'm sure that it's going to
seen so far I'm sure that it's going to
like pit her out a little bit at some
like pit her out a little bit at some
point but we're going to see if it gets
point but we're going to see if it gets
closer to task
solve main problem at the moment is the
solve main problem at the moment is the
drift I do not understand this
drift well Lambda is fixed
drift well Lambda is fixed
here
for
e
e e
total
performance predicted performance
performance predicted performance
landscape
it's kind of hard to
it's kind of hard to
check I'm going to try the different
check I'm going to try the different
sampling thing before I do this
because it could technically be from the
because it could technically be from the
normal
normal
distribution normal distribution will it
distribution normal distribution will it
tend to keep parameters pretty
tend to keep parameters pretty
close maybe the not just not enough data
close maybe the not just not enough data
diversity in the sampling I don't
know this is very consistent holy
know this is very consistent holy
hell this is very consistent holy hell
hell this is very consistent holy hell
look at
this only thing you could possibly ask
this only thing you could possibly ask
for here is that it's
for here is that it's
quicker you know 109 trials for uh for
quicker you know 109 trials for uh for
this
but look at the total time steps holy
but look at the total time steps holy
look at
look at
this it's actually
this it's actually
keeping it's getting this perf with this
keeping it's getting this perf with this
low times step so I'm actually I'm
low times step so I'm actually I'm
curious to see the end behavior of
curious to see the end behavior of
this let me
see okay so it's pushing it's pushing I
see okay so it's pushing it's pushing I
think it's pushing the learning back up
think it's pushing the learning back up
slowly gamma's now
slowly gamma's now
correct Lambda has corrected update EPO
correct Lambda has corrected update EPO
is perfect this is perfect this is
perfect so what does it have left to
perfect so what does it have left to
fix I think just learning rate is off
fix I think just learning rate is off
now
it does is it going back
up I don't have this wrong
right look at cost over
right look at cost over
time look at
that I mean this is so much better
I'm just going to while this finishes
I'm just going to while this finishes
I'm going to answer a DM over on the
side
e
e
e
e
e
e e
we
we
DM it's very nice
this is the cleanest freaking hyper
this is the cleanest freaking hyper
parameter sweep I've ever seeing
this is pretty nice
this is crazy
this is crazy
clean for a hyper pram sweep this is
clean for a hyper pram sweep this is
insanely
insanely
clean so a little concerned with the
clean so a little concerned with the
learning
learning
rate it's kind of stuck here
than
I mean the only way that this thing
I mean the only way that this thing
improves at this point is to up total
improves at this point is to up total
time steps right
it's a little scared to
like it's kind of a little
like it's kind of a little
scared to uh to increase total time
scared to uh to increase total time
steps it should have the model
steps it should have the model
correlated by
correlated by
now let's see what it
does for
okay you know it is kind of doing it
I still think this can be made
better this thing should be able to be
better this thing should be able to be
very aggressive right
now hang on maybe this is me being
now hang on maybe this is me being
stupid wait let me look at this
because Global search
okay I got a global surf scale of
0.5
0.5
and yeah total time steps
and yeah total time steps
of12
of12
0.25 so total and then well let me see
0.25 so total and then well let me see
so
like8 we're basically we got like 1.5
like8 we're basically we got like 1.5
is so8 to10 is probably going to be I'm
is so8 to10 is probably going to be I'm
at like netive .5 to start and then the
at like netive .5 to start and then the
max is one so you got like
max is one so you got like
1.5 and you're sampling with a scale of
1.5 and you're sampling with a scale of
.125 okay so like it's going to take
.125 okay so like it's going to take
you it's going to take you a while to
you it's going to take you a while to
get to the upper end of that range like
get to the upper end of that range like
it's going to take you at
it's going to take you at
least 15
least 15
runs and you're not necessarily going to
runs and you're not necessarily going to
sample something all the way at the edge
sample something all the way at the edge
of that scale
either so this probably just needs to
either so this probably just needs to
get increased
there might still be some other
there might still be some other
improvements we can make though hold on
improvements we can make though hold on
because once we increase
because once we increase
this we don't wanted to sampling really
this we don't wanted to sampling really
high time Steps From the get-go
yeah this is continuing to improve but
yeah this is continuing to improve but
it's
slow what
next conservative
let me think about
this we're waiting towards higher
this we're waiting towards higher
absolute
performance we're penalizing this by
performance we're penalizing this by
uncertainty
uncertainty
of the performance
of the performance
estimates but we're not incorporating
estimates but we're not incorporating
cost in any way
right hang on and what way is this thing
right hang on and what way is this thing
incorporating
cost I don't think it is
right so we have GPC
with the expected
cost we can use the Paro value y
PF I mean we're barely using that one
is this better than just using the
is this better than just using the
actual Pito point
wait you could literally just use the
wait you could literally just use the
parito point here couldn't
parito point here couldn't
you I mean I guess yeah technically you
you I mean I guess yeah technically you
know your suggestion can be higher
cost so maybe this is better
I kind of want to see how good this
I kind of want to see how good this
parito function
parito function
is you
know cuz if this thing sucks like you're
know cuz if this thing sucks like you're
going to be better off just using the
going to be better off just using the
parito
point okay that's like a relatively
point okay that's like a relatively
small
small
thing
thing
though the main thing is not taking into
though the main thing is not taking into
account
account
cost like this is uniform waiting across
cost like this is uniform waiting across
the Paro front
right this is biased to high
cost this really isn't cost to wear in
cost this really isn't cost to wear in
the way that you would think about
it
like okay so this is good but it's not
like okay so this is good but it's not
intelligently using
intelligently using
cost in order to
cost in order to
like maximize the results The Sweep
like maximize the results The Sweep
result over like the result over the
result over like the result over the
time of the whole sweep
time of the whole sweep
it's really
it's really
not I'm trying to think how you would do
that
e
e
e
e
e e
damnn
it this is
tricky e
Dynamic cost
penalty let's see
yeah this is very very
yeah this is very very
good it could be faster right but what
good it could be faster right but what
it's doing is very
it's doing is very
good is it figured out the learning rate
good is it figured out the learning rate
thing at
thing at
all that one's kind of
all that one's kind of
screwy n not really missed one parameter
let me see
let me see
so decreases as model
cost I don't want to
penalize I don't really want to penalize
penalize I don't really want to penalize
low cost or high cost runs even more
low cost or high cost runs even more
than they already are
we could have an explicit exploration
we could have an explicit exploration
budget that
budget that
favors using more cheap experiments
early I mean you should get explicitly
early I mean you should get explicitly
penalized based on the cost is the thing
penalized based on the cost is the thing
like that is the correct thing to do
information value per
information value per
compute the EI function already does
that
e
e e
this is difficult because it's clearly
this is difficult because it's clearly
clearly the case that you want to
clearly the case that you want to
penalize linearly for
cost what in the hell is this so we got
cost what in the hell is this so we got
base
acquisition what the is that
that's like the stupidest thing ever oh
that's like the stupidest thing ever oh
s
s
AGI
AGI
no literally gave me a formula with cost
no literally gave me a formula with cost
cancelling
out yeah so this is what we this is what
out yeah so this is what we this is what
it should
be should just be divided by
be should just be divided by
cost it should be divid by cost
Improvement times absolute
Improvement times absolute
performance over
cost let me think about when you're
cost let me think about when you're
actually going to get this
this doesn't capture the right thing
Improvement wait if we have an optimal
Improvement wait if we have an optimal
parito front what
parito front what
happens hey
happens hey
welcome we are currently
welcome we are currently
optimizing the heck out of carbs and
optimizing the heck out of carbs and
fixing hyperparameter tuning in
fixing hyperparameter tuning in
RL it's going quite well was breaking my
brain this is the cleanest hyper
brain this is the cleanest hyper
parameter speep I've ever
seen yeah it doesn't 200 experiments
seen yeah it doesn't 200 experiments
though and it doesn't upweight total
though and it doesn't upweight total
time steps enough
time steps enough
um let me just run while I since I'm
um let me just run while I since I'm
doing this let me
do I'm trying to
think wait no
okay
so sample
so sample
around origins in
around origins in
basic this one here
basic this one here
right search just distribution in basic.
Sample normal okay
so what if we do
so what if we do
this we do
uniform and then I
uniform and then I
think do we have to up with this
we'll try this initially to see just
we'll try this initially to see just
what happens with the same search scale
what happens with the same search scale
and everything
see if this
works okay
solid well we will compare this in a
moment there's going to be more work to
moment there's going to be more work to
do I'm going use a restro from real
do I'm going use a restro from real
quick be right
back
e e
we'll see how this does by comparison
we'll see how this does by comparison
with The Uniform sampling
I actually don't know if it's going to
I actually don't know if it's going to
be any better
be any better
because
because
um the implicit trust region that we had
um the implicit trust region that we had
is actually kind of pretty good
already like us uh we're already
already like us uh we're already
modulating by variant so this might just
modulating by variant so this might just
be worse
I actually don't know if you think about
I actually don't know if you think about
it right with hyperparameter
it right with hyperparameter
optimization do you think any webchain
optimization do you think any webchain
blockchain GPU stuff will be interest no
blockchain GPU stuff will be interest no
not at all blockchain maybe web 3 is a
scam um yeah not really at
all blockchain Tech is legit but I just
all blockchain Tech is legit but I just
there's not really
there's not really
like there really aren't that many uses
like there really aren't that many uses
for it if you think about
it okay
how do I get two
windows let me see
here I like how NYU schedules meetings
here I like how NYU schedules meetings
on Sunday it's
on Sunday it's
funny enable a
funny enable a
Marketplace like why what does
Marketplace like why what does
blockchain add to that right
like in what way does it add
anything that's the thing like it's like
anything that's the thing like it's like
oh what if we do block like it doesn't
oh what if we do block like it doesn't
like you can just do that without
like you can just do that without
blockchain right like it doesn't make
blockchain right like it doesn't make
sense I mean yeah you can add a crypto
sense I mean yeah you can add a crypto
payment option or whatever but that
payment option or whatever but that
doesn't mean that the like the whole
doesn't mean that the like the whole
rest of it has to be on blockchain
rest of it has to be on blockchain
that's just a payment option
so it really doesn't do very
much I don't even know if you can
much I don't even know if you can
legally do
a what is
this I don't really want to click
this I don't really want to click
that bit tenser
that bit tenser
okay I'll Google
it guide emissions I don't know what
it guide emissions I don't know what
that
is I I don't know what that
is is this way worse than
is is this way worse than
before I think this might be way worse
before I think this might be way worse
than before we'll
than before we'll
see that'd be interesting if it is way
see that'd be interesting if it is way
worse
I want to think about this
here let's say that you have an optimal
here let's say that you have an optimal
Paro
Paro
front different categories of sub
networks a lot of these things you don't
networks a lot of these things you don't
understand how they work because there's
understand how they work because there's
nothing to understand it just doesn't
nothing to understand it just doesn't
make sense don't let me yeah I mean the
make sense don't let me yeah I mean the
stuff we're doing at the moment is
stuff we're doing at the moment is
really freaking cool
really freaking cool
um I'm a little tired cuz it's the math
um I'm a little tired cuz it's the math
is breaking my freaking head but this is
is breaking my freaking head but this is
carbs oops this is carbs this is a very
carbs oops this is carbs this is a very
complicated basian optimization
complicated basian optimization
algorithm for hyperparameter
algorithm for hyperparameter
search um and I'm making it like
search um and I'm making it like
dramatically dramatically better so if
dramatically dramatically better so if
if these results if I can finish this
if these results if I can finish this
and these transfer well to from the
and these transfer well to from the
synthetic tasks to real tasks this will
synthetic tasks to real tasks this will
like overnight be a severalfold
like overnight be a severalfold
Improvement just across the board to all
Improvement just across the board to all
of
RL because this will let you I mean this
RL because this will let you I mean this
will just let you get ridiculously good
will just let you get ridiculously good
results out of your
results out of your
experiments and to be fair
experiments and to be fair
like this
like this
is so it's possible that I've overfit
is so it's possible that I've overfit
the task right it's possible that I've
the task right it's possible that I've
overfit the task but so
overfit the task but so
far um this is the before
no not this one this is after I've
no not this one this is after I've
already screwed with
it okay so this is the
it okay so this is the
before I think I rescaled the task since
before I think I rescaled the task since
then but this is like a third of the
then but this is like a third of the
overall possible
performance and then
now this is the cleanest hyperparameter
now this is the cleanest hyperparameter
sweep I've ever seen and this is like
sweep I've ever seen and this is like
90% performance
so we're making crazy progress
so we're making crazy progress
here okay this modification seems to not
here okay this modification seems to not
be particularly good uh it's still clean
be particularly good uh it's still clean
but it's way
slower maybe we'll give it a little
longer do you need to change the search
longer do you need to change the search
range parameter for normal versus
uniform 30% of data Falls outside
oh it gave me two different answers
LOL I think it is 61% right
68% Falls within one standard
deviation my probability
deviation my probability
sucks my linear algebra is pretty good
sucks my linear algebra is pretty good
but my stats and probability sucks
so 40% of data Falls so maybe you just
so 40% of data Falls so maybe you just
need to
upscale maybe you just need to upscale
upscale maybe you just need to upscale
the
the
parameter now I really don't know
parameter now I really don't know
intuitively though which of these should
intuitively though which of these should
be better
be better
so you're generating a th samples around
so you're generating a th samples around
a Paro optimal
a Paro optimal
point intuitively would you rather do
point intuitively would you rather do
uniform sampling around the point or
uniform sampling around the point or
normal sampling normal sampling is going
normal sampling normal sampling is going
to get you more points where several of
to get you more points where several of
the parameters are close to their
the parameters are close to their
existing values and like a few
existing values and like a few
parameters are different so intuitively
parameters are different so intuitively
I mean that would be better for like
I mean that would be better for like
tuning the last few stubborn parameters
probably whereas with uniform
probably whereas with uniform
sampling you get more diverse data
sampling you get more diverse data
overall but like probably a lot of the
overall but like probably a lot of the
parameters are going to be far from
parameters are going to be far from
their initial settings I think that
their initial settings I think that
normal normal might just be the smarter
normal normal might just be the smarter
Choice
here let's see how it's fitting these
here let's see how it's fitting these
parameters
so gamma's
so gamma's
good you know it is kind of fitting
good you know it is kind of fitting
these better
these better
though oh well this is stuck
okay so a lot of these parameters are
stuck maybe you have to
upweight so this is just too
slow I'm going to try I'm going to kill
slow I'm going to try I'm going to kill
this and I'm going to try to
double yeah I'm going try to double
this and if this doesn't work then I'm
this and if this doesn't work then I'm
going to go back to normal because I'm
going to go back to normal because I'm
I'm going to
assume I'm going to assume that
assume I'm going to assume that
basically the
basically the
uh the normal is better because I if I
uh the normal is better because I if I
double the standard deviation and it's
double the standard deviation and it's
still
still
screwy this should be way stable as well
screwy this should be way stable as well
we'll see it it's kind of nice cuz it's
we'll see it it's kind of nice cuz it's
like it's clipped for
you but I think I have I I think
you but I think I have I I think
implicitly I already have a good
implicitly I already have a good
mechanism of
clipping we're using a conservative
clipping we're using a conservative
estimate based on the variance of the
estimate based on the variance of the
gaussian
gaussian
process so that should prevent you from
process so that should prevent you from
sampling way out their hyper parameters
sampling way out their hyper parameters
that you don't know what they're going
that you don't know what they're going
to do should I haven't actually checked
to do should I haven't actually checked
how good the gaussian processes
are uh hold on is this immediately
better these are just random
better these are just random
samples so the random samples are better
samples so the random samples are better
we'll see if this translates to anything
we'll see if this translates to anything
I'm only going to give this like maybe
I'm only going to give this like maybe
50 or so samples to do
50 or so samples to do
something I'm getting hungry as
well okay this is actually now suddenly
that made a huge
difference to be fair I introduced that
difference to be fair I introduced that
scaling
scaling
term
term
um I introduced that scaling term before
um I introduced that scaling term before
I changed the acquisition function so
I changed the acquisition function so
now we have a way better acquisition
now we have a way better acquisition
function so maybe we get away
function so maybe we get away
with larger sample
with larger sample
ranges I'm GNA have to try this with
ranges I'm GNA have to try this with
gaussian as well
is there any reason I can't run these
is there any reason I can't run these
both at the same
time I think I should be able to
right
right
oh hold
oh hold
on cannot get random
sample that's interesting
that's very
interesting that was doing better as
interesting that was doing better as
well
no candidates
found sample search space
sample oh I see
two to
two to
the numb dims
the numb dims
with
bounce two to the
5ifth me figure this out
M24 so this is generating a ton of
M24 so this is generating a ton of
samples
get masth for invalid points in basic
huh I mean that's
interesting so I think I know what's
interesting so I think I know what's
happening here I
happening here I
think
think
um though it's pretty surprising
so if you have um too big of a search
range then it's very likely that at
range then it's very likely that at
least one of your parameters is going to
least one of your parameters is going to
be out of bounds
get mask for invalid points in
basic self. Min bounds in basic
okay can I just clip
this instead of
masking samples in
basic e
it these seem valid though
so these are all
valid it's a bit
valid it's a bit
weird cuz you're sampling are
round in
basic I mean this looks like this should
basic I mean this looks like this should
go out of
go out of
bounds to
me search distribution and
me search distribution and
basic there's no clipping on
this self.
surf wait uniform how the did this
surf wait uniform how the did this
happen
no
wait uh-uh
is just just have a weird print on it
okay yeah so this is correct just has a
okay yeah so this is correct just has a
weird print string on
it oh I'm stupid that's why so this
it oh I'm stupid that's why so this
is but wait a second this is
plus yeah cuz nothing starts at one okay
plus yeah cuz nothing starts at one okay
cool so this has got to be negative
cool so this has got to be negative
config
Global so maybe this uniform is not that
Global so maybe this uniform is not that
bad maybe I just screwed up you know
we're going to do this for a
we're going to do this for a
bit cuz I don't think you should be uh
bit cuz I don't think you should be uh
masking
these okay valid sample
these okay valid sample
mask okay so there's some
mask okay so there's some
false valid
size
size
shape um
valid okay look there are a lot of
valid okay look there are a lot of
invalid samples in there so I think that
invalid samples in there so I think that
what would be better is we
what would be better is we
do this in
do this in
basic.
basic.
clamp and this is not it it should
be in
men bounds in
men bounds in
basic men
and now we no longer have this
and hopefully this works
okay
cool we're we'll see if this is too
cool we're we'll see if this is too
ridiculous um
we'll do
this so I changed this I don't know why
this so I changed this I don't know why
they do this
they do this
um because pretty
um because pretty
much let's say that you sample a point
much let's say that you sample a point
that's like really close to a
that's like really close to a
boundary it's a pretty good chance to
boundary it's a pretty good chance to
just generate all invalid samples
just generate all invalid samples
uh but you should just clamp and then
uh but you should just clamp and then
you never generate invalid
samples at least that's what should
samples at least that's what should
happen I mean we're like we're very very
happen I mean we're like we're very very
quickly improving this thing
quickly improving this thing
here so let's see so this is the next
run we'll see how this compares to you
run we'll see how this compares to you
to uh to normal if it's too noisy I'll
to uh to normal if it's too noisy I'll
crank the standard deviation back down
crank the standard deviation back down
since I had it wrong
before dinner in about an
before dinner in about an
hour and
hour and
then how ever many hours I've got left
then how ever many hours I've got left
in me on this
in me on this
tonight this is exciting though this is
tonight this is exciting though this is
very very good
very very good
progress very good
progress I realistically I think we're
progress I realistically I think we're
only a few changes away from having this
only a few changes away from having this
algorithm solid uh there are a few
algorithm solid uh there are a few
things I'm not fully happy with
things I'm not fully happy with
yet
yet
so the algorithm right now need needs to
so the algorithm right now need needs to
be biased to Shorter runs but it needs
be biased to Shorter runs but it needs
to have a mechanism to quickly scale to
to have a mechanism to quickly scale to
longer runs right now it's not biased to
longer runs right now it's not biased to
Shorter runs but for some reason tends
Shorter runs but for some reason tends
to do them anyways and takes forever to
to do them anyways and takes forever to
do longer
runs this can be improved
maybe too noisy we'll
see intuitively should
see intuitively should
you if you switch distributions from
you if you switch distributions from
from normal to uniform for sampling do
from normal to uniform for sampling do
you need to change the
you need to change the
size if you're sampling with a standard
size if you're sampling with a standard
deviation of 0.5
with a normal
with a normal
distribution you just change that to a
distribution you just change that to a
uniform with
0.5 I
0.5 I
mean like 32% of your data from the
mean like 32% of your data from the
normal is going to fall
normal is going to fall
outside of those
outside of those
bounds but a lot of your points are
bounds but a lot of your points are
going to be closer to the center whereas
going to be closer to the center whereas
with the
with the
uniform you're going to end up with more
uniform you're going to end up with more
points not near the center but then
points not near the center but then
you're hard clip to
0.5 I don't
0.5 I don't
know I really don't
know I really don't
know we'll just have to do this
know we'll just have to do this
empirically which is slightly annoying
empirically which is slightly annoying
because
because
um I mean Neptune takes a couple seconds
um I mean Neptune takes a couple seconds
to load and then carbs takes a few
to load and then carbs takes a few
seconds per point which is annoying but
seconds per point which is annoying but
whatever
better than doing full scale
experiments let's load this up versus
experiments let's load this up versus
the other
the other
one so this is what we had
one so this is what we had
here I can't there're too many
points I'm pretty damn sure this is
points I'm pretty damn sure this is
worse
[Music]
maybe
maybe
not I think you have to give it like
not I think you have to give it like
what 40 points or something let's see
what 40 points or something let's see
let's just eyeball
let's just eyeball
this so around here is 50
this so around here is 50
points takes around 50 points to get to
points takes around 50 points to get to
2,000
2,000
nice and stable
here could be on
track hard to
say these samples are pretty darn noisy
though pretty darn noisy
see we would like these to be much more
consistent this could possibly be me
consistent this could possibly be me
cranking the standard deviation too high
cranking the standard deviation too high
though we'll give it 50 points and then
though we'll give it 50 points and then
I'll try again with uh half the standard
I'll try again with uh half the standard
deviation
unless this magically improves very soon
unless this magically improves very soon
I think this is just
worse I'll make the change preemptively
worse I'll make the change preemptively
and then we'll wait until uh we'll wait
and then we'll wait until uh we'll wait
until 50 to actually rerun
it so this
it so this
one was at 0.5
also this exploration bias we got look
also this exploration bias we got look
at as well
I mean this kind of
works it's noisier than before
though definitely
though definitely
noisier so
like here let's just look at
like here let's just look at
what what this gave us right so this is
what what this gave us right so this is
about half and this is about half a half
about half and this is about half a half
so we're going to just feel like
this this was a very
this this was a very
smooth up to 2,000 right very very
smooth yeah this is
smooth yeah this is
worse we can kill this and then we can
worse we can kill this and then we can
try this with
try this with
uh we can try this with 0.5
this should be the closer
this should be the closer
comparison if this isn't
comparison if this isn't
better then I think normal is just
better and we can tune the normal I
better and we can tune the normal I
guess normal intuitively
guess normal intuitively
like normal is worse if you don't have
like normal is worse if you don't have
some sort of trust region if you don't
some sort of trust region if you don't
have some sort of mechanism for
have some sort of mechanism for
rejecting
rejecting
crazy outliers but we have that so
crazy outliers but we have that so
normal should give you more points that
normal should give you more points that
like change just a few
variables it's kind of close to a local
search is possibly
better so I do
better so I do
wonder if we look at
this now see the learning rate is still
this now see the learning rate is still
screwy even with
screwy even with
this and Lambda is still drifted with
this and Lambda is still drifted with
this so the uniform does not solve this
this so the uniform does not solve this
weird drift
weird drift
issue um I thought it would but it does
not which is very weird
okay
so we'll see if this does better if not
so we'll see if this does better if not
I think we just stick to
normal it's very weird how much worse
normal it's very weird how much worse
this is It's like a lot
this is It's like a lot
worse this is just one synthetic problem
worse this is just one synthetic problem
problem but still
it's a
lot how is it this bad
it's kind of impressive like what a
it's kind of impressive like what a
difference that makes I didn't screw it
difference that makes I didn't screw it
up right
like
like
[Music]
[Music]
you no this is correct
makes a huge difference
holy there's probably a ton of value in
holy there's probably a ton of value in
just getting the scales right for
just getting the scales right for
everything as
well we'll definitely have to look at
that I wouldn't be surprised as well
that I wouldn't be surprised as well
that this synthetic tuning it on this
that this synthetic tuning it on this
is pretty representative of a lot of
is pretty representative of a lot of
problems cuz like the parameters are
problems cuz like the parameters are
pretty close and
all I don't know how it's this much
all I don't know how it's this much
worse it's kind of crazy
I mean it's not like it's it's failing
I mean it's not like it's it's failing
it's definitely working it's just
um what's this doing it should be going
um what's this doing it should be going
down
down
to1 okay gamma's
to1 okay gamma's
good this actually looks
better these haven't been sampled at all
possibly the search range is too small
interesting it's missing a lot of the
interesting it's missing a lot of the
scale
PRS I get stuck on all of
these the normal just kind of worked
okay so let me think now going
forward I'm going to think about this in
forward I'm going to think about this in
the
the
[Music]
[Music]
meantime we want to downweight by clost
meantime we want to downweight by clost
we want to penalize you for running high
we want to penalize you for running high
cost experiments for no
cost experiments for no
reason what happens if your parito front
reason what happens if your parito front
is
perfect so if you have a perfect Paro
perfect so if you have a perfect Paro
front
front
the only way to
the only way to
improve is
by the only way to improve is by running
by the only way to improve is by running
longer
longer
experiments oh wait their formula
sucked who wait this is wrong this is
sucked who wait this is wrong this is
just wrong look I think I found another
just wrong look I think I found another
thing damn it so look they use this
thing damn it so look they use this
whole extra gaan process for estimating
whole extra gaan process for estimating
cost okay and then whatever point you
cost okay and then whatever point you
sample you
sample you
estimate the cost of this point and then
estimate the cost of this point and then
you take the Paro value here okay and
you take the Paro value here okay and
you estimate the cost of this parito
point so what you're doing is you're
point so what you're doing is you're
estimating the difference between the
estimating the difference between the
point that you've selected and your
point that you've selected and your
estimate of the optimal point that costs
estimate of the optimal point that costs
that much and then you're taking re
that much and then you're taking re
which to be fair I didn't see re in the
which to be fair I didn't see re in the
code but then what happens here is that
code but then what happens here is that
if your Paro front is if you're if you
if your Paro front is if you're if you
have a perfect estimate of parameters in
have a perfect estimate of parameters in
parito front this is always
zero so you can't even
zero so you can't even
improve you're not even allowed to
improve you're not even allowed to
improve by running longer experiments
improve by running longer experiments
under
this you're not even allowed to improve
this you're not even allowed to improve
by running longer experiments because
by running longer experiments because
you're being compared to a hypothetical
you're being compared to a hypothetical
point that you haven't even obtained
yet that's terrible
okay let's say that we fixed that
right then if we have a perfect parito
right then if we have a perfect parito
front the only Improvement will come
front the only Improvement will come
from running a longer
experiment so this is going to be zero
experiment so this is going to be zero
for everything that is not a longer
for everything that is not a longer
experiment
and then absolute
and then absolute
performance is going to be like a high
performance is going to be like a high
positive number
positive number
right and then
right and then
cost is going to downweight
you what if there's a little bit of
you what if there's a little bit of
error right what if there's a little bit
error right what if there's a little bit
of room to improve somewhere else on the
of room to improve somewhere else on the
Paro
front your basic Al going to improve
front your basic Al going to improve
everywhere first aren't
you you're going to improve everywhere
you you're going to improve everywhere
on the parito
on the parito
front before you improve at the end of
front before you improve at the end of
it so that might bias you towards too
it so that might bias you towards too
many short
experiments I mean we're going to have
experiments I mean we're going to have
to run it there're just a lot of
to run it there're just a lot of
ablation run
here okay this is definitely bad so
here okay this is definitely bad so
don't do this do a normal
sample we'll run a few of these
just to make sure that we're good uh and
just to make sure that we're good uh and
then let's replace
then let's replace
the the
estimate with the Paro
Point Let's replace the estimate with
Point Let's replace the estimate with
the Paro point
this will this should at least improve
this will this should at least improve
our synthetic test um we'll see what
our synthetic test um we'll see what
happens
when we will see what happens when you
when we will see what happens when you
have noise in the
estimate how badly that hurts you
okay so this is your parito surrogate
right best moo is equal to Pito
right best moo is equal to Pito
surrogate which is not what we want what
surrogate which is not what we want what
we want is
num to
generate origins in basic is the Paro
generate origins in basic is the Paro
groups
so we going have to look at that let's
so we going have to look at that let's
make sure yeah this is this is going
make sure yeah this is this is going
well you see so just
immediately we get our curve back okay
immediately we get our curve back okay
yeah I didn't break anything we're good
yeah I didn't break anything we're good
here um
so but I need to figure this one
so but I need to figure this one
out replace
out replace
this ironically I don't think we're
this ironically I don't think we're
going to need the cost estimate model
going to need the cost estimate model
are
are
we no we are we're still going to need
we no we are we're still going to need
the cost
the cost
estimate but
separate generate
separate generate
candidate e
why is it never calling generate
why is it never calling generate
candidates
hello get to 15
these random
these random
points oh there we go it's weird it took
points oh there we go it's weird it took
a while I guess it's cuz the the logging
a while I guess it's cuz the the logging
is fun so we have samples in basic
is fun so we have samples in basic
shape
Jesus that's generates way too many
Jesus that's generates way too many
samples
how's this work wait origin
samples uh
sample around Origins basic
wait origin index
categorical
categorical
ah okay so that's going to be
you're going to have to give it origin
index okay
sample search
space doing this in order
to get the
means do you need
this I don't know if you need
this I don't know if you need
this you definitely
need okay and then what you do is
no
no
equal watch
X
okay origin
index Maxs
uh that's
interesting oh for some reason they're
interesting oh for some reason they're
the
the
same uh they're resamples the same
same uh they're resamples the same
parameter
parameter
Okay so
answer X plus
4 e
burrito
grps orito
groups I misspell it or is it just a
stupid all
right let's try
this for
wait
oh okay I think you just have to do a
oh okay I think you just have to do a
zero
let's try this
this
run okay
oh this is incredibly
stupid e
uh this is going to be
fine okay so this is good
expected condenser
device hold on Origins and
basic samples in
basic samples in
basic
for e
they're getting rid of this
garbage e
really
H that's
interesting does torch. clamp not
interesting does torch. clamp not
actually clamp correctly
I guess the transformation is uh a
I guess the transformation is uh a
little
little
off it's not good
though
e e
TR
that and actually let's make that bound
that and actually let's make that bound
a little
tighter okay so now this is running
let's
let's
see how we
do see how we do
and this is going to ask me for a pi key
again I honestly did not expect to be
again I honestly did not expect to be
changing CBS this much today it's kind
changing CBS this much today it's kind
of
insane this is like so much outward mcdb
insane this is like so much outward mcdb
for one day
this is a brand new
algorithm I'm leaning towards neoc
carbs for a name
oops
let's
let's
see how this
does there are two changes
does there are two changes
here there's the masking
here there's the masking
change and there's the Paro Baseline
change we you shall
see it's quite a fair bit of
work e
is this worse than
before let's
[Music]
[Music]
see maybe
are we
stuck we're going to have to test these
stuck we're going to have to test these
two things
two things
separately it could be that the clamping
separately it could be that the clamping
doesn't
doesn't
work clamping should work though
work clamping should work though
I'd be quite
surprised
good let me think if there's any way
good let me think if there's any way
that I could be wrong
that I could be wrong
here I'm pretty sure I checked the math
here I'm pretty sure I checked the math
and it doesn't make sense as
and it doesn't make sense as
is
like this is your expected Improvement
like this is your expected Improvement
versus the Pito front
right if you have a perfect estimate of
right if you have a perfect estimate of
the Paro
the Paro
front then this is zero
everywhere right
everywhere right
even if you crucially even if you don't
even if you crucially even if you don't
have the experiments run yet
right that seems
wrong so I mean if you
if you
if you
use the estimate of the current Point
use the estimate of the current Point
instead then you get a benefit for doing
instead then you get a benefit for doing
a longer
a longer
run or you can get a benefit for doing a
run or you can get a benefit for doing a
longer run because you have a weaker
longer run because you have a weaker
Baseline to improve
Baseline to improve
over so this should help bias you
over so this should help bias you
towards longer runs as well provided
towards longer runs as well provided
that you're confident uh in the
that you're confident uh in the
parameters oh hold on here we
go yeah okay it just it took a
go yeah okay it just it took a
second let's see how the shape
is
is
okay whatever slightly different shape
okay whatever slightly different shape
curve I think what this
curve I think what this
1635 what did we have before
this
one yeah so in 35
one yeah so in 35
is I mean that's
like okay it's actually it's a pretty
like okay it's actually it's a pretty
similar shape
similar shape
initially I just wasn't looking at
initially I just wasn't looking at
it we'll see how that goes this point is
it we'll see how that goes this point is
not good but we'll
see I think it's like 2, 50 or
something I just want this to not hurt
something I just want this to not hurt
you know if it doesn't
help okay there you go so this total
help okay there you go so this total
time steps is a little sketchy we get up
time steps is a little sketchy we get up
to 350 300 something mil
to 350 300 something mil
does that happen in the other
one oh yeah no there's a little bump at
one oh yeah no there's a little bump at
the start okay that's
fine this is such a clean curve
though okay yeah that's good I don't
though okay yeah that's good I don't
know what these points are
doing are these parito
doing are these parito
you have this brown one
here oh yeah so it was going for like a
here oh yeah so it was going for like a
lower cost
lower cost
thing
interesting I don't know if this looks
interesting I don't know if this looks
as
as
good like this Lambda is still drifting
learning rate's about right but we'll
learning rate's about right but we'll
see if that
see if that
drifts other PRS are looking
good I mean this is pretty close to the
good I mean this is pretty close to the
original with a
original with a
few a few outlier points though we're
few a few outlier points though we're
pretty
pretty
close pretty darn close to the
close pretty darn close to the
original I don't like these outlier
original I don't like these outlier
points and I don't know why they're here
points and I don't know why they're here
let me think if there's any good reason
let me think if there's any good reason
for them to be
here well actually hold on yeah they
here well actually hold on yeah they
could
could
be okay so you actually can get stuff
be okay so you actually can get stuff
like
like
this um let me look at where can I find
this um let me look at where can I find
this this is
this this is
AB 2151
okay that is a parito point you
okay that is a parito point you
see so
yeah this is probably it's just getting
yeah this is probably it's just getting
rewarded for
rewarded for
um improving
um improving
over uh a previous parito point that it
sampled so that is actually kind of a
sampled so that is actually kind of a
problem now that I think about it
because you can kind of get pre you can
because you can kind of get pre you can
kind of get infinitely rewarded for
kind of get infinitely rewarded for
running the same experiment can't you
running the same experiment can't you
which would be this
cluster let's see if that happens in
practice I mean this is pretty good it's
practice I mean this is pretty good it's
not as clean though as
before let me think about that
exploit so the issue here is you get
exploit so the issue here is you get
clusters like this
clusters like this
because you get rewarded for running
because you get rewarded for running
something that's like a little higher
something that's like a little higher
cost and does a bit
cost and does a bit
better but you get rewarded repeatedly I
better but you get rewarded repeatedly I
think for the same experiment
okay
e
e
e e
sampled for
son it not that
smart what if we
smart what if we
compared the predicted efficiency
yeah this doesn't make any
sense so
I mean this is still
good
good
but we'll see how this goes by the end I
but we'll see how this goes by the end I
think that you are you're getting
think that you are you're getting
rewarded too much for doing stupid
like if I look at this there's this
like if I look at this there's this
like dense little Cloud here does this
like dense little Cloud here does this
happen with the uh with the 200
happen with the uh with the 200
run I do this
yeah this is like a better spread parito
yeah this is like a better spread parito
front you don't have like a little tiny
front you don't have like a little tiny
cluster
cluster
anywhere this is very very clean and you
anywhere this is very very clean and you
can see there's so few points out of the
can see there's so few points out of the
front by
comparison yeah this front is already
comparison yeah this front is already
worse
I don't know how it gets this
point I mean I guess technically you can
point I mean I guess technically you can
literally just take any point right you
literally just take any point right you
take any parito
take any parito
point like over here even and you just
point like over here even and you just
crank it up and run it for
longer even then I think these are bad
longer even then I think these are bad
samples
it's weird that that matters so much I
it's weird that that matters so much I
mean this makes sense this is literally
mean this makes sense this is literally
just like each of these points is
parito well that could actually be it
parito well that could actually be it
though like just losing that many
though like just losing that many
samples could also just screw up your uh
samples could also just screw up your uh
your prediction functions your gum
processes I mean this is not bad but I
processes I mean this is not bad but I
think it was better before but
think it was better before but
theoretically let me think about this
so you want to know how much
better is how much better your thing is
better is how much better your thing is
than the current optimal on that front
than the current optimal on that front
if you can't improve the
if you can't improve the
front you don't get
anything I don't like that it's improved
anything I don't like that it's improved
the optimal it's rough
what's the original function so if you
what's the original function so if you
take
this it's just the acquisition function
this it's just the acquisition function
originally
right what if I did
plus plus the absolute
performance you have to get the waiting
performance you have to get the waiting
term right as the
term right as the
problem to do
that that adds a waiting hyperparameter
that that adds a waiting hyperparameter
that's very difficult to tune
if you do absolute performance over
cost absolute performance over cost is
cost absolute performance over cost is
going to favor shorter run still is the
problem this is hard
so
like this is what I was looking at
like this is what I was looking at
here the problem is
Improvement first of all this is a
Improvement first of all this is a
conservative bound on
conservative bound on
Improvement
um well hang on I'm no longer clamping
yeah I'm no longer clamping this thing
yeah I'm no longer clamping this thing
right so even if these are all
right so even if these are all
conservative bound
conservative bound
negative you're still going to run the
negative you're still going to run the
best experiment out of all of these
right I think that I need to add a bias
right I think that I need to add a bias
term to this so that this is never
term to this so that this is never
negative though I need to add a bias
negative though I need to add a bias
term so this is negative never
negative cuz otherwise it's going to get
negative cuz otherwise it's going to get
Scaled weird
okay so I think that solves
okay so I think that solves
that as long as I'm not clamping this by
that as long as I'm not clamping this by
mistake somewhere am I clamping this by
mistake somewhere am I clamping this by
mistake
somewhere so here's the function
somewhere so here's the function
so you have the samples this is fine
so you have the samples this is fine
this plamp is
this plamp is
fine and
fine and
then you get surrogate
then you get surrogate
model
output
cast this is mask Max masking is fine
cast this is mask Max masking is fine
expected Improvement okay I think the re
expected Improvement okay I think the re
is not
is not
here at least I don't see a anywhere so
here at least I don't see a anywhere so
I think I got rid of that
successfully I
hope we'll see if that can be
negative so now
let me go get the Pito surget
back and go get the Pito
surget
oops let's go get the Pito so get back
oops let's go get the Pito so get back
where is
it get the
pitoo yeah this one
for e
so we're going to use this burrito
surrogate um and I like this wasn't
terrible this was not terrible I don't
terrible this was not terrible I don't
think
right let me make sure that this is so
right let me make sure that this is so
135 at
135 at
2500 let's just make sure this is not
2500 let's just make sure this is not
somehow better
and know this is about the same we get
and know this is about the same we get
2500 and about 100 points here is that's
2500 and about 100 points here is that's
about the
about the
average and uh time steps go up like
that cost might be lower here hard to
that cost might be lower here hard to
say
Fredo front is Messier
though I think they're pretty similar
though I think they're pretty similar
but I like this is slightly worse I'd
but I like this is slightly worse I'd
say and like
theoretically we just need a bias term
theoretically we just need a bias term
is all for the uh the cost
is all for the uh the cost
hold
on okay I got to go for dinner
on okay I got to go for dinner
um I'd like to run something while I'm
um I'd like to run something while I'm
at dinner just so I get one more
experiment I don't think that this is
experiment I don't think that this is
going to work
going to work
here we're going to try this but I don't
here we're going to try this but I don't
think that this
works it's conservative
works it's conservative
[Music]
[Music]
absolute by
where's
cost we'll run this
I do not think that this
I do not think that this
works for
reference okay I'm going to call that
reference okay I'm going to call that
there I'm going to go to dinner I'll be
there I'm going to go to dinner I'll be
back in roughly an
back in roughly an
hourish and we will will continue
hourish and we will will continue
working on this um probably for another
working on this um probably for another
two hours hopefully we get this like
two hours hopefully we get this like
solid it's it's very close there just a
solid it's it's very close there just a
few things that need to be
few things that need to be
improved um all the info for all this is
improved um all the info for all this is
on puffer doai
on puffer doai
Discord uh X GitHub start help

Kind: captions
Language: en
we are
we are
back uh hold on there's my
window there we
window there we
go we're
go we're
back time for
science let me make sure I've missed
science let me make sure I've missed
anything on the
anything on the
Discord and also I'm not muted good
huh
okay
huh 1%
cool Captain's done some nice
cool Captain's done some nice
optimization up to
200k that's getting to a respectable
200k that's getting to a respectable
speed
my rest
stream okay so what we're going to do
stream okay so what we're going to do
now
now
is go back to the hyper parameter tuning
is go back to the hyper parameter tuning
stuff and it's going to take me a second
stuff and it's going to take me a second
to get back into this this is going to
to get back into this this is going to
be a little hard this is a combination
be a little hard this is a combination
of fixing messy code and hard
of fixing messy code and hard
math
so I think I was looking at the acis
so I think I was looking at the acis
position
function what is
function what is
YX the regular expected
Improvement okay okay so they're saying
that this one
that this one
is expected improvement over whichever
is expected improvement over whichever
parito value is sampled and this is
parito value is sampled and this is
expected improvement over the best
expected improvement over the best
parito
value and they want to balance they want
to they want to improve the best
ones empirically this this only samples
ones empirically this this only samples
the highest performing part of the predo
the highest performing part of the predo
front EF only samples only rarely yes
front EF only samples only rarely yes
our solution is to sample a threshold
our solution is to sample a threshold
cost from a log uniform distribution
cost from a log uniform distribution
okay that's sketchy
okay that's sketchy
because I mean I guess if cost is perf
because I mean I guess if cost is perf
in seconds then yeah that is log
distributed so you sample from log space
and then you evaluate the Paro
and then you evaluate the Paro
performance Paro front performance
there so this gives you a
there so this gives you a
comparison okay perfect so
we also
we also
note the version that always used the
note the version that always used the
maximum no parito does not
sample the version that always uses the
sample the version that always uses the
max no Paro
this is higher mean what the
it's got a bigger spread
but well this the thing is the parito
but well this the thing is the parito
thing that they're doing actually makes
thing that they're doing actually makes
sense so I assume it's the other choices
sense so I assume it's the other choices
that they've made that have screwed with
this figure five
oh they actually have a comparison to
oh they actually have a comparison to
hebo
perfect here's
perfect here's
hebo ah and there's the okay so here
hebo ah and there's the okay so here
this is fine this is what we're
this is fine this is what we're
interested in uh so hebo is over
here they don't have like random search
here they don't have like random search
and stuff on here which they should
and stuff on here which they should
have a is way better
what a synchronous success of
halfing wait this does better than them
let's see and for some reason they do
let's see and for some reason they do
way worse over
here and carbs is like
dude what the hell is wrong with this
they except on image classification they
they except on image classification they
underperform
underperform
Asha something
Asha something
screwy I mean this could literally be
screwy I mean this could literally be
like the way that search spaces are set
like the way that search spaces are set
up and stuff though
what is this we might have to implement
what is this we might have to implement
this if it's simple
allocate a small budget to each
configuration allocate a small budget to
configuration allocate a small budget to
each
each
configuration evaluate all
configuration evaluate all
configurations increase budget
configurations increase budget
configuration until maximum per
configuration until maximum per
configuration budget is reached
so you just give it a bunch so these are
so you just give it a bunch so these are
like you come up with random search
like you come up with random search
config or
whatever you train it for a bit
this is not
this is not
smart there's like a really obvious
smart there's like a really obvious
optimization here that they're missing
wait is this different successive paing
wait is this different successive paing
asynchronous success
pathing oh this is just that it's
pathing oh this is just that it's
embarrassingly parallel right
let me make sure I understand this if
let me make sure I understand this if
this is a strong Baseline against carbs
this is a strong Baseline against carbs
I want to understand this so resources
so you get the top
K draw
K draw
random okay so you just you keep
random okay so you just you keep
sampling basically you just you have a a
sampling basically you just you have a a
fixed number of
fixed number of
configurations and you just drop the
configurations and you just drop the
poor performers
okay so that's
interesting I mean this means it's
interesting I mean this means it's
probably
probably
worth you know continuing like maybe
worth you know continuing like maybe
carbs is not going to be the end all be
carbs is not going to be the end all be
all of this but I think that they have I
all of this but I think that they have I
mean they get pretty darn good results
mean they get pretty darn good results
even with all these screw-ups in
it that variant of random
search kind of does get better over time
search kind of does get better over time
come to think of it
because they keep they sample a bunch of
because they keep they sample a bunch of
parameters and they keep dropping the
parameters and they keep dropping the
bad ones and then they keep running for
longer there's definitely something to
longer there's definitely something to
that it would be a good base line
that it would be a good base line
it would be a good
Baseline and I could Implement that
Baseline and I could Implement that
pretty easily I think that I might since
pretty easily I think that I might since
this seems like the
this seems like the
simplest let me
see local start search
see local start search
start the set of specified lowc cost
parameters blend search
parameters blend search
does pretty
okay on everything
hebo this is a complicated method that
hebo this is a complicated method that
carbs is built
carbs is built
on carbs underperforms on language
on carbs underperforms on language
modeling outperforms on
modeling outperforms on
RL outperforms on image
RL outperforms on image
classification interesting
tpe is up there as well though this is
tpe is up there as well though this is
weird
this asynchronous halfing thing could be
this asynchronous halfing thing could be
pretty
decent cuz it is compute adaptive which
decent cuz it is compute adaptive which
I like and it's trivially
parallelizable I me I'm definitely going
parallelizable I me I'm definitely going
to need a better Benchmark than I
to need a better Benchmark than I
currently
currently
have man
it's crazy that this does so well on the
RL I guess it gets more runs the thing
RL I guess it gets more runs the thing
is the carbs is supposed to be able to
is the carbs is supposed to be able to
do this
automatically you know I kind of suspect
automatically you know I kind of suspect
that carbs just isn't tuned correctly
that carbs just isn't tuned correctly
because carbs should do this
because carbs should do this
adaptive compute better than
adaptive compute better than
this so we'll consider this as a
this so we'll consider this as a
baseline in a bit
baseline in a bit
um this would be a good
um this would be a good
Baseline but let's for now let's go back
Baseline but let's for now let's go back
to the math of
to the math of
carbs oh wait there was one thing I
carbs oh wait there was one thing I
wanted to look which is the ablations
so full carbs does the best on RL
so full carbs does the best on RL
no parito really hurts
no parito really hurts
it no clamping really hurts it and then
it no clamping really hurts it and then
no resampling
no resampling
okay hurts it a little bit um well some
okay hurts it a little bit um well some
a lot so let's see what this is
um your six
cost in hours okay
cost in hours okay
here
so L search does pretty
nicely doesn't give you this nice front
nicely doesn't give you this nice front
it's kind of like
it's kind of like
spotty Asha has a very nice one this is
spotty Asha has a very nice one this is
just like straight
just like straight
yeah this is
yeah this is
nice so hebo and carbs are both the gum
nice so hebo and carbs are both the gum
process based
ones you can see here that Asha just
ones you can see here that Asha just
does a ton of these really lowcost runs
does a ton of these really lowcost runs
I don't know how useful these
I don't know how useful these
are but Asha doesn't use a predictive
are but Asha doesn't use a predictive
model so actually for Asha these are
model so actually for Asha these are
useful because it's just going to keep
useful because it's just going to keep
the ones that are
the ones that are
good it's not trying to build a
good it's not trying to build a
predictive model okay that makes
sense what's this no clamping so no
sense what's this no clamping so no
clamping no Paro no resampling
this is probably the no parito
version which would make
version which would make
sense do you get this weird
distribution um
so let me see what they
so let me see what they
do sample a threshold
cost we then evaluate the Paro front
cost we then evaluate the Paro front
clost
there I'm confused
noisy okay let's take a look at these
noisy okay let's take a look at these
first output
first output
warping output variable have
warping output variable have
approximately Gan
approximately Gan
distribution with zero meain and unit
variance yeah
quantile
transform
transform
fine
fine
resampling so this doesn't matter in my
resampling so this doesn't matter in my
synthetic test because it's perfectly
synthetic test because it's perfectly
accurate but we will look at this noisy
accurate but we will look at this noisy
observations are particularly important
observations are particularly important
to handle correctly for carbs best
to handle correctly for carbs best
observations are used to define the
observations are used to define the
local search space that's a particularly
local search space that's a particularly
extreme noise Point might prevent the
extreme noise Point might prevent the
algorithm from finding a better Optimum
this is
this is
true one out of every n resample
true one out of every n resample
suggestions is a set of parameters that
suggestions is a set of parameters that
has already been
has already been
observed we count the samples for each
observed we count the samples for each
of the existing Paro front points and
of the existing Paro front points and
suggest parameters which have the
suggest parameters which have the
smallest number of samples breaking ties
smallest number of samples breaking ties
by one by choosing the one with lower
by one by choosing the one with lower
cost okay so they're resampling on the
cost okay so they're resampling on the
Paro front but the thing is the Paro
Paro front but the thing is the Paro
front can get to be pretty
big and also like okay let's say you get
big and also like okay let's say you get
a bad sample right that just
a bad sample right that just
means and now a different point becomes
means and now a different point becomes
parito
right more likely it just means you drop
right more likely it just means you drop
that from the burito set if it's
that from the burito set if it's
overestimated you're not going to have
overestimated you're not going to have
something with exactly the same cost so
something with exactly the same cost so
if it's overestimated you're just going
if it's overestimated you're just going
to end up dropping something from the
to end up dropping something from the
Pito
set this seems important
this is trying to robustify something
this is trying to robustify something
that might not be able to be robustified
that might not be able to be robustified
though some sometimes the MS are just
though some sometimes the MS are just
very noisy I guess that's why Asha would
very noisy I guess that's why Asha would
win because Asha doesn't care about this
win because Asha doesn't care about this
type of
type of
stuff all it cares about is um
stuff all it cares about is um
the final model basically it's not
the final model basically it's not
building it it's not trying to predict
building it it's not trying to predict
any of
this so I'm definitely going to do that
this so I'm definitely going to do that
as a
baseline Pito front
minimum we start the Paro
front because the cost itself is a noisy
front because the cost itself is a noisy
that's not true
this is true for a different reason this
this is true for a different reason this
is just because you have to train the
is just because you have to train the
models for a bit to start getting a
curve 20%
curve 20%
observations with the lowest
cost interesting
tuning parameters like model size can be
tuning parameters like model size can be
difficult if some parameters lead to
failure okay so they have a failure
failure okay so they have a failure
prediction model which you should be
prediction model which you should be
able to just know what's going to out of
able to just know what's going to out of
memory and not run that so we don't care
memory and not run that so we don't care
about this too
much clost ceiling this is fine
parallelism yeah
well let's see this still might this
well let's see this still might this
still might do better than uh than Asha
still might do better than uh than Asha
I don't want to give up on this this is
I don't want to give up on this this is
like this still seems in my mind like
like this still seems in my mind like
this has potential there's some things I
this has potential there's some things I
don't like about it but this still has a
don't like about it but this still has a
ton of potential let me just go turn the
ton of potential let me just go turn the
AC on real quick we'll get to this
by so let me figure out how this works
by so let me figure out how this works
because I need to understand this
because I need to understand this
sampling
so they take this
x x is sampled around a Paro value
I actually might like this hard trust
I actually might like this hard trust
region idea better
an issue
an issue
okay we're more interested in the best
okay we're more interested in the best
performing
parameters I don't really understand
this cuz you sampled this X
this cuz you sampled this X
from any Paro point
uniformly weights improvements across
uniformly weights improvements across
the
the
entire Paro
front okay I got to look at the code for
front okay I got to look at the code for
this because the math does not make
this because the math does not make
sense see to
sense see to
me it looks like you should be biasing
me it looks like you should be biasing
your X samples
your X samples
um towards higher cost
um towards higher cost
stuff what this looks like to me is that
stuff what this looks like to me is that
they are biasing their scoring function
they are biasing their scoring function
I don't
I don't
understand unless are they sampling from
this Aquis maybe they're sampling from
this Aquis maybe they're sampling from
this hold
this hold
on so wait if they
okay hold on I think that they are
okay hold on I think that they are
sampling from this so this is a soring
sampling from this so this is a soring
function this soring function works over
function this soring function works over
everything we got rid of pearch we're
everything we got rid of pearch we're
getting rid of this so all we have is
getting rid of this so all we have is
this expectation here um
I actually like the idea of the hard
I actually like the idea of the hard
trust region I'm going to try
trust region I'm going to try
that
that
um but this
thing this is going to give you a
distribution instead what they say
is they're going to
sample they're going to sample a
sample they're going to sample a
cost and then they
use this function to estimate the
use this function to estimate the
performance at that
cost and then they going to take a
cost and then they going to take a
Max between these
two but then they take a Max
two but then they take a Max
over I think they just take a Max over
over I think they just take a Max over
this thing Max over all candidates okay
this thing Max over all candidates okay
so
so
how how does this work
let me just
uh
e e
oh wait maybe I do get
it I think I actually do get it so this
it I think I actually do get it so this
is the expected
performance so if
performance so if
per
per
Point
random it's a chance that it will
random it's a chance that it will
higher score of
the lower
the lower
cost for to a point higher
cost for to a point higher
chance
chance
that random
s
POS one
POS one
that this means
bias points or
higher
not really like
so I mean this could just be paring back
so I mean this could just be paring back
what I said
there definitely seem to be some major
there definitely seem to be some major
limitations with this algorithm now that
limitations with this algorithm now that
I'm looking at
it quite some
limitations
for e
there quite some substantial limitations
there quite some substantial limitations
here though for
sure okay so I understand how they're
sure okay so I understand how they're
doing this let me make sure it's
doing this let me make sure it's
implemented
correctly so they take
correctly so they take
acquisition
function we don't need this we can just
function we don't need this we can just
get rid of
probabilities I don't actually see this
probabilities I don't actually see this
um Max cost
masking always
maxed this should be okay this is the
clamp let's make sure this is
clamp let's make sure this is
on this should be on right
what
it's using the same sample repeatedly
it's using the same sample repeatedly
here we'll fix that okay good this
here we'll fix that okay good this
is they do have this clamp
thing get rid of this
okay I'd like to figure out why all the
okay I'd like to figure out why all the
random samples are the same to start
random samples are the same to start
we'll have to look at
that let's see if I can find that in the
that let's see if I can find that in the
meantime while this
runs get random
sample search space of 32
oh you know what I bet it's just that
oh you know what I bet it's just that
we're using the time in
seconds
seconds
holy it's already got
200s well that's a ridiculously fast
Improvement it's pretty R noisy but I
Improvement it's pretty R noisy but I
guess that makes sense
yeah that's crazy fast
Improvement and we're not even getting
Improvement and we're not even getting
good random samples yet let's fix
that yeah I think h i I think the
that yeah I think h i I think the
science side effort of puffer lib for
science side effort of puffer lib for
the next little bit is going is going to
the next little bit is going is going to
be hyper pram that's the biggest ban for
be hyper pram that's the biggest ban for
our
buck it's so important and Nobody Does
buck it's so important and Nobody Does
it I mean really there's not any higher
it I mean really there's not any higher
return on
return on
investment um I'm going to be curious to
investment um I'm going to be curious to
see whether carbs is the algorithm that
see whether carbs is the algorithm that
we end up using it's a very complicated
we end up using it's a very complicated
algorithm it makes sense in principle
algorithm it makes sense in principle
but there are some rough edges and like
but there are some rough edges and like
with something that's complicated there
with something that's complicated there
just so many places to have stuff go
just so many places to have stuff go
wrong okay but this is like dramatically
wrong okay but this is like dramatically
dramatically better here like we're
dramatically better here like we're
getting these 200s and stuff we'll see
getting these 200s and stuff we'll see
how long it takes to max out but um you
how long it takes to max out but um you
know we're getting these
know we're getting these
samples way way way faster than before
samples way way way faster than before
this used to take like 200 or at least
this used to take like 200 or at least
100 experiments to get one of
100 experiments to get one of
these and now we're getting it quicker
there we go 250 okay so this is like
there we go 250 okay so this is like
twice as good as before by deleting
twice as good as before by deleting
their stupid uh sampling like their
their stupid uh sampling like their
stupid probability
stupid probability
waiting as expected that was just dumb I
waiting as expected that was just dumb I
don't know why they had
that we'll see whether hard trust or
that we'll see whether hard trust or
soft trust region is
soft trust region is
better hard trust is more conservative
and it should still be biasing to the
and it should still be biasing to the
top end of the range here doesn't mean
top end of the range here doesn't mean
it's going exclusively produce results
it's going exclusively produce results
there but it should be producing
there but it should be producing
um well actually you know it should
um well actually you know it should
actually go to the top end because the
actually go to the top end because the
cost is uniform
cost is uniform
so the parito
so the parito
points are at the top
points are at the top
end so this is kind of noisy to be fair
end so this is kind of noisy to be fair
this is kind of
this is kind of
noisy I mean it might just need more
noisy I mean it might just need more
samples to build up the predictive model
samples to build up the predictive model
it is getting better over
time there's a 67 in
there still way better than before
okay so next
thing is going to be
thing is going to be
this
this
time into time.
time random. seed
just take an
in it has to be converted to an
in it has to be converted to an
integer okay so what we want is
um I just multiply by a th
wow this is good
wow this is good
right I do
right I do
int yeah
that's
that's
good and that's same oh
good and that's same oh
yeah so much better than
yeah so much better than
before still gets a few bad samples
before still gets a few bad samples
but overall so much better than
before let's see if this fixes the
before let's see if this fixes the
random
problem seed must
be
okay what's a good way to get
okay what's a good way to get
uh to get this
seed way to get
ah perfect
ah perfect
lower 32
bits seed
equals that's good
perfect now you see we get different
seeds uh that should actually help the
seeds uh that should actually help the
algorithm out quite a bit because the
algorithm out quite a bit because the
initial data points are properly
separated so that's
separated so that's
fixed
now let me think about this from here
this is kind of the simplest possible
this is kind of the simplest possible
case for carbs because there should only
case for carbs because there should only
be one Pito
point I should double check that there
point I should double check that there
is only one parito
is only one parito
point for
it's pretty decent
okay so we'll just do like this
so
here oh there were just multiple parito
here oh there were just multiple parito
points that have the same output that's
points that have the same output that's
fine yeah that can happen with uh that
fine yeah that can happen with uh that
can that could happen that's
can that could happen that's
fine yeah they had the exact same output
fine yeah they had the exact same output
and then slightly different parameters
and then slightly different parameters
that's totally fine you say this the
that's totally fine you say this the
exact same
point these should have gotten merged
point these should have gotten merged
um but I guess it's a resample that's
um but I guess it's a resample that's
fine and now
why are these two
not oh I guess it doesn't distinguish
not oh I guess it doesn't distinguish
between um
between um
duplicated
duplicated
suggestions but this is like never going
suggestions but this is like never going
to happen in practice so this is fine
to happen in practice so this is fine
this is only because I have a perfectly
this is only because I have a perfectly
deterministic synthetic test
so it is sampling around it's sampling
so it is sampling around it's sampling
around a parito
point it's weird that some of these
point it's weird that some of these
suggestions are so
suggestions are so
bad you know because it should be
bad you know because it should be
scoring
this I guess the gaussian process takes
this I guess the gaussian process takes
more samples to be able to build a good
more samples to be able to build a good
predictive
model I mean this is still this is a
model I mean this is still this is a
massive Improvement
I kind of want to now compare this to
I kind of want to now compare this to
asynchronous
having just see how that does
I mean hold on that's basically that's
I mean hold on that's basically that's
random
search e
I also haven't added the scale parameter
I also haven't added the scale parameter
in
in
yet so this is making way too aggressive
yet so this is making way too aggressive
updates is
updates is
why so this is like almost random the
why so this is like almost random the
way it's
way it's
searching right so let's try
that
for e
try
this okay so this is now at 205 here
this okay so this is now at 205 here
with this many
with this many
parameters this is what we have here
and is there like a cumulative mean or
and is there like a cumulative mean or
something or and we can just have
something or and we can just have
this let's try this so what this should
this let's try this so what this should
do is massively reduce the
do is massively reduce the
per suggestion search
radius let's see if this gives us more
radius let's see if this gives us more
consistency
not so good so
far possibly set it too low
so it is making nice nice uh
so it is making nice nice uh
progress It's like pretty darn
progress It's like pretty darn
consistent now
was it going to be stuck at this rate of
was it going to be stuck at this rate of
improvement though
yeah the deviations are just very small
here it should have a good predictive
here it should have a good predictive
model so it should be improving more
model so it should be improving more
consistently than this
oh you know what it
oh you know what it
is
is
um carbs literally is only building the
um carbs literally is only building the
model based off of the one parito
point
point
okay so that's not
amazing there's a f 56
okay yeah so I'll let this run for a bit
okay yeah so I'll let this run for a bit
just to see what the shape of this curve
is I do think whatever I just did is too
is I do think whatever I just did is too
conservative
because it basically got one lucky
because it basically got one lucky
update and now it's still stuck in this
update and now it's still stuck in this
little
region I mean there are a couple
region I mean there are a couple
different Pito points looks at I
different Pito points looks at I
guess and there gets an update
it's still just so
slow okay let's try
um let's
um let's
try5 on
this 74 so it's doing stuff it's too
this 74 so it's doing stuff it's too
below
so the point of this
so the point of this
is the search range that you'd use for
is the search range that you'd use for
like random search or
like random search or
whatever uh that's like the goal of that
whatever uh that's like the goal of that
is to cover most of the space you're
is to cover most of the space you're
trying to do something that's like
trying to do something that's like
somewhat tight around the Paro values
somewhat tight around the Paro values
but not so tight that you you are
but not so tight that you you are
basically sampling the same
point we'll see how this
does I mean come to think of it I
does I mean come to think of it I
wouldn't be surprised the really
wouldn't be surprised the really
noisy version actually works because
noisy version actually works because
basically if you think about it if you
basically if you think about it if you
have a really high mean on this then
have a really high mean on this then
it's just random
it's just random
search it's like basically random search
search it's like basically random search
but with the mean variables the means of
but with the mean variables the means of
everything set to the best run and then
everything set to the best run and then
doing that on all the Paro
values that's actually kind of
good but it will produce like
good but it will produce like
noisy I me the downside is it will
noisy I me the downside is it will
produce noisy
results how how do they use the parito
results how how do they use the parito
fit hold on cuz this model's crap
fit hold on cuz this model's crap
whatever this Pito model is here is crap
whatever this Pito model is here is crap
so anything that comes out of this model
so anything that comes out of this model
is going to be bad let me
is going to be bad let me
see final model is used as a basine for
see final model is used as a basine for
the
expected okay so this
expected okay so this
Baseline is going to be crap for this
Baseline is going to be crap for this
because the cost is all the same
because the cost is all the same
I don't know why they only use the
I don't know why they only use the
parito points to fit that
model I guess it makes
model I guess it makes
sense I don't know I don't know if that
sense I don't know I don't know if that
should be a gaussian
process okay so this is better this is
process okay so this is better this is
giving
you up to 70s to 115
we'll tune this on the full problem
we'll tune this on the full problem
because right now we're not tuning on on
because right now we're not tuning on on
the full problem so let's
goer
Li
Li
CS so those are the optimals
CS so those are the optimals
um the full problem should be
there should be cost
there should be cost
assigned this doesn't change the
optimum but it should change how carbs
runs and then we can test we can test
runs and then we can test we can test
this stuff yeah
this is looking like a decent tradeoff
this is looking like a decent tradeoff
it takes a little longer but these are
it takes a little longer but these are
like very stable
updates yeah so now the the
updates yeah so now the the
uh acquisition function has been
uh acquisition function has been
learned it's just going to
solve that's pretty
decent the predictive model is
decent the predictive model is
not I'd expect more out of that model
though cuz they're supposed to have this
though cuz they're supposed to have this
calcium process is supposed to be
calcium process is supposed to be
predicting which of these parameters is
predicting which of these parameters is
going to do well and it should be
going to do well and it should be
something that's very easy to learn but
something that's very easy to learn but
it's just not doing
it's just not doing
it like this should basically
it like this should basically
consistently be going up and it's
not maybe we'll tune it a little more
not maybe we'll tune it a little more
we'll see
I want to get to asynchronous halfing
I want to get to asynchronous halfing
soon because I like I just don't have a
soon because I like I just don't have a
good Baseline for this and I'd like to
good Baseline for this and I'd like to
build out a test Suite cuz if I'm going
build out a test Suite cuz if I'm going
to spend you know a whole bunch of time
to spend you know a whole bunch of time
packaging up carbs nicely I want to know
packaging up carbs nicely I want to know
that it's actually outperforming you
that it's actually outperforming you
know strong
know strong
baselines uh or at least
baselines uh or at least
like relatively strong simple algorithms
okay so here's
cost we see that it
solves this
solves this
takes what is
takes what is
this here yeah 234 I think we'll count
this here yeah 234 I think we'll count
that as mostly
solved okay 79 update for this
solved okay 79 update for this
um call it like 85 to get the
um call it like 85 to get the
250ish so that's way oh yeah here there
250ish so that's way oh yeah here there
we go
so 83
so 83
updates compared
to our synthetic test
oh it's about the
oh it's about the
same about 83 either
way I carb sweep question when I try to
way I carb sweep question when I try to
bound I need to make the background on
bound I need to make the background on
this less transparent when I try to
this less transparent when I try to
bound on
when I try to bound
gamma what branch are you
want are you on my ablations
want are you on my ablations
Branch Oh no you're on
Branch Oh no you're on
2 huh
yeah I don't know man that's
yeah I don't know man that's
weird um there are some serious serious
weird um there are some serious serious
problems with the base carbs I will say
problems with the base carbs I will say
there's some very serious problems with
there's some very serious problems with
it we've made it like about it's about
it we've made it like about it's about
three times better already than it was
three times better already than it was
before and I'm not done yet
you can try
you can try
that it's going to be Jank so you if
that it's going to be Jank so you if
you're going to do
you're going to do
that that might be Jank you can probably
that that might be Jank you can probably
actually for your problem it'll be fine
actually for your problem it'll be fine
if you just do from 0.9 to 99 linear
if you just do from 0.9 to 99 linear
it'll work because yours isn't going to
it'll work because yours isn't going to
be super high
anyways okay so this works at least as
anyways okay so this works at least as
well as before if not a little bit
well as before if not a little bit
better this looks
nice now we got to do the full
nice now we got to do the full
problem now there's cost involved here
when when I finish it man this is very
when when I finish it man this is very
difficult because I'm waiting through a
difficult because I'm waiting through a
whole bunch of math and carbs and I'm
whole bunch of math and carbs and I'm
starting to see some questionable
starting to see some questionable
choices that they've made and then
choices that they've made and then
looking at their baselines it seems like
looking at their baselines it seems like
it's possible that I'm going to be able
it's possible that I'm going to be able
to implement something much simpler that
to implement something much simpler that
might outperform it anyways so I really
might outperform it anyways so I really
have to do some uh some proper tests on
this why is cost
1.0 oh CU I forgot to uncomment this
line okay good so now we have
line okay good so now we have
a dramatically varying
a dramatically varying
costs in a bigger Paro
front was this one with cost or without
cost cost up time
okay so this is the one without
CLS so let's see how carbs
CLS so let's see how carbs
does with cost
it's going to
do a little bit
differently I kind of want this one to
differently I kind of want this one to
be on Neptune though
really
so we're going to put this on to uh
so we're going to put this on to uh
Neptune as our
Neptune as our
Baseline there are still improvements
Baseline there are still improvements
that can be made here but uh I'm going
that can be made here but uh I'm going
to do I think we're going to
to do I think we're going to
implement Asha after this as a basine
implement Asha after this as a basine
and we're also going to do the random as
and we're also going to do the random as
a
a
baseline let's just run this here
baseline let's just run this here
dude what the I just installed Neptune
off p is screwed
up API token
than
for e
is there no carb suggest where's
is there no carb suggest where's
suggest oh yeah there it
is
for e
and let's see how our run is
going there we
go much
go much
nicer batch size going up mini batches
nicer batch size going up mini batches
interesting
stuff there may still be some uh some
stuff there may still be some uh some
search parameters to
tune is native carbs integration with
tune is native carbs integration with
wan to be improved nope they haven't
wan to be improved nope they haven't
done Jacks yet or have you just switched
done Jacks yet or have you just switched
to Neptune uh it's not really Neptune
to Neptune uh it's not really Neptune
doesn't do hyperparameter tuning at all
doesn't do hyperparameter tuning at all
so we're doing our own hyperparameter
so we're doing our own hyperparameter
tuning and managing it via tags so we'll
tuning and managing it via tags so we'll
support wanb and Neptune but we're doing
support wanb and Neptune but we're doing
it through tags instead of through their
it through tags instead of through their
sweeps API um currently what I'm doing
sweeps API um currently what I'm doing
is I found some uh some major
is I found some uh some major
limitations of carbs so I'm making like
limitations of carbs so I'm making like
neoc carbs new version of carbs and uh
neoc carbs new version of carbs and uh
I'm also going to Benchmark it against
I'm also going to Benchmark it against
some other Baseline algorithms to see if
some other Baseline algorithms to see if
you know if even my better version of
you know if even my better version of
carbs holds up um because it's possible
carbs holds up um because it's possible
that we're just going to have a simpler
that we're just going to have a simpler
thing that works better we will see that
thing that works better we will see that
said I have made carbs like 3x better
said I have made carbs like 3x better
than the original already
than the original already
so yeah this is a synthetic
so yeah this is a synthetic
Benchmark let me see if I have the
Benchmark let me see if I have the
original result here from before I mess
original result here from before I mess
with
it synthetic
carbs maybe this one yeah okay so this
carbs maybe this one yeah okay so this
is before I messed with it this is what
is before I messed with it this is what
carbs got us like 70 or what whatever
carbs got us like 70 or what whatever
exact same
exact same
Benchmark okay and
then I did some rescaling work and I got
then I did some rescaling work and I got
us to
us to
here
here
250 but this is without cost
250 but this is without cost
awareness um so now with cost
awareness we will see how well we do
awareness we will see how well we do
cost awareness should hurt it a little
cost awareness should hurt it a little
bit but uh what cost awareness does it
bit but uh what cost awareness does it
prevents you from running really long
prevents you from running really long
experiments too
early so the way to evaluate it is um
early so the way to evaluate it is um
you need to take into account the cost
you need to take into account the cost
of all of the experiments
I can probably do these at the same
I can probably do these at the same
time if you set search center out of
time if you set search center out of
bounds carbs yells at you yes that does
happen we're going to see so basically
happen we're going to see so basically
Spencer if um if carbs ends up being the
Spencer if um if carbs ends up being the
god algorithm after these tweaks then I
god algorithm after these tweaks then I
will invest the time that it takes to
will invest the time that it takes to
really clean up the code and like it'll
really clean up the code and like it'll
be like half the code or whatever it'll
be like half the code or whatever it'll
be a file in puffer lib um there a
be a file in puffer lib um there a
couple tricky things like yeah you have
couple tricky things like yeah you have
to load in gaussian processes and stuff
to load in gaussian processes and stuff
so it will depend on Torch but it'll be
so it will depend on Torch but it'll be
much much simpler uh if we beat carbs
much much simpler uh if we beat carbs
with something simpler then obviously
with something simpler then obviously
we're just going to use the simpler
we're just going to use the simpler
thing so we'll
thing so we'll
see I remember trying to do carbs with
see I remember trying to do carbs with
wbie sweeps well that's actually we do
wbie sweeps well that's actually we do
have WV sweeps working with carbs in
have WV sweeps working with carbs in
puffer 2 right now you can do it it's
puffer 2 right now you can do it it's
just that like there are some
just that like there are some
limitations of carbs um that could be
limitations of carbs um that could be
improved I don't I'll be somewhat
improved I don't I'll be somewhat
disappointed if this is just beaten by
disappointed if this is just beaten by
something way simpler but here's the
something way simpler but here's the
thing I'm looking at the paper and yeah
thing I'm looking at the paper and yeah
this is their version of carbs with like
this is their version of carbs with like
a whole bunch of things
a whole bunch of things
screwy
but so you know we're going to have a
but so you know we're going to have a
way stronger thing than their version of
way stronger thing than their version of
carbs but like look at
carbs but like look at
Asha it doesn't do so well on language
Asha it doesn't do so well on language
model and image classification but for
model and image classification but for
whatever reason on RL it does the best
whatever reason on RL it does the best
and Asha is really simple to
implement also Asha has the advantage of
implement also Asha has the advantage of
being trivially multi Noe like we
being trivially multi Noe like we
wouldn't even have to do any fancy stuff
wouldn't even have to do any fancy stuff
and Asha would just be multi- Noe
it's really weird how different the
it's really weird how different the
performances of these algorithms on
performances of these algorithms on
different tasks
different tasks
like I kind of don't believe that the
like I kind of don't believe that the
ranking isn't similar across tasks i'
ranking isn't similar across tasks i'
like I almost want to think that they
like I almost want to think that they
were just configured better in some like
were just configured better in some like
in some cases than others
okay so after 40 we're at 140 we're
okay so after 40 we're at 140 we're
starting to run the really long
starting to run the really long
runs it's missed a couple parameters so
runs it's missed a couple parameters so
I think that there are some
I think that there are some
configuration improvements to make here
configuration improvements to make here
cuz like it should definitely figure out
cuz like it should definitely figure out
uh mini batches 8 and stuff like
uh mini batches 8 and stuff like
that but I think it's it's also possible
that but I think it's it's also possible
just being conservative because this is
just being conservative because this is
cost now updox to four yeah mm's I don't
cost now updox to four yeah mm's I don't
think this
think this
matters maybe it
matters maybe it
does PT this is 16 case we got
that and
then
for environment up time so this should
for environment up time so this should
be or
be or
cost okay so this is doing the correct
cost okay so this is doing the correct
thing look it's getting more cost over
time it started out low
cost so this is the correct thing
cost so this is the correct thing
roughly
I can probably do two of these sweeps at
I can probably do two of these sweeps at
the same time like I don't think Neptune
the same time like I don't think Neptune
will yell at me I'm going to just do
will yell at me I'm going to just do
this and we're going to get a random
this and we're going to get a random
sweep working
okay so now we got to do
uh now we got to do
our our setup
here I know it's done well and puffer
here I know it's done well and puffer
because I copied for my own repo with
because I copied for my own repo with
some jack space Ms it was done at least
some jack space Ms it was done at least
originally was very finicky yeah it was
originally was very finicky yeah it was
very
very
much give puffer m a try they're faster
much give puffer m a try they're faster
than most Jack baced M and you don't
than most Jack baced M and you don't
have to
suffer though we are starting a collab
suffer though we are starting a collab
with Flare to provide um nice puffer
with Flare to provide um nice puffer
bindings for craftex and kinetics pretty
bindings for craftex and kinetics pretty
soon soon as I get to that those ones
soon soon as I get to that those ones
are pretty nice oh we got a fair few
are pretty nice oh we got a fair few
folks on YouTube at the moment welcome
folks on YouTube at the moment welcome
so we what we are currently doing we're
so we what we are currently doing we're
currently looking at the carbs
currently looking at the carbs
hyperparameter tuning algorithm which is
hyperparameter tuning algorithm which is
at least on paper a very good
at least on paper a very good
hyperparameter tuning algorithm from a
hyperparameter tuning algorithm from a
UI I found some pretty
UI I found some pretty
substantial limitations and quirks in
substantial limitations and quirks in
the implementation that I fixed so now
the implementation that I fixed so now
it's at least like two or three times
it's at least like two or three times
better than before and now what I'm
better than before and now what I'm
going to do is I'm going to Benchmark
going to do is I'm going to Benchmark
this on some synthetic uh on a synthetic
this on some synthetic uh on a synthetic
hyperparameter optimization task I made
hyperparameter optimization task I made
versus random search and then probably
versus random search and then probably
versus Asha once I Implement that and
versus Asha once I Implement that and
we're going to see whether carbs is
we're going to see whether carbs is
truly the king of hyper pram optim
truly the king of hyper pram optim
ization once you clean it up and if so I
ization once you clean it up and if so I
will uh package it up nicely simplify
will uh package it up nicely simplify
the implementation and put carbs into
the implementation and put carbs into
puffer or if one of the other baselines
puffer or if one of the other baselines
is easier and performs just as well or
is easier and performs just as well or
better and if so that's great news
better and if so that's great news
because as is way simpler to implement
because as is way simpler to implement
way simpler to understand and would uh
way simpler to understand and would uh
also parallelize trivially to
also parallelize trivially to
multinode craftex is going to be the
multinode craftex is going to be the
focus of one so would be very appealing
focus of one so would be very appealing
yeah craftex is a great M original craft
yeah craftex is a great M original craft
is a God awful M it's the worst python
is a God awful M it's the worst python
code I've ever seen um I don't even know
code I've ever seen um I don't even know
how you make an environment that slow
how you make an environment that slow
craftex is
craftex is
awesome I that said I had talked with
awesome I that said I had talked with
the authors of uh you know this Jack
the authors of uh you know this Jack
sson flairer and they're very talented
sson flairer and they're very talented
for being able to do this stuff at Jack
for being able to do this stuff at Jack
but you know they're kind of shooting
but you know they're kind of shooting
themselves in the foot a little bit it
themselves in the foot a little bit it
would literally be so much easier to
would literally be so much easier to
just Implement these things and see like
just Implement these things and see like
it would be so much easier it would be
it would be so much easier it would be
the same amount or less code and you
the same amount or less code and you
wouldn't have to think in a
wouldn't have to think in a
race I mean we literally have brand new
race I mean we literally have brand new
people to RL implementing the all the
people to RL implementing the all the
various puffer Ms and c and it's like so
various puffer Ms and c and it's like so
easy by
comparison I don't know if anybody wants
comparison I don't know if anybody wants
to go Port crafter to uh craft X to see
to go Port crafter to uh craft X to see
you're welcome to um I don't think I
you're welcome to um I don't think I
wasn't going to do it because like the
wasn't going to do it because like the
jackm already exists and it's already
jackm already exists and it's already
fast but I would not be surprised if
fast but I would not be surprised if
it's just faster and less code and
see possibly except for rendering I
see possibly except for rendering I
don't have an amazing way of doing
don't have an amazing way of doing
rendering yet but I mean their rendered
rendering yet but I mean their rendered
version is not that fast
version is not that fast
anyways okay let's take a quick look at
anyways okay let's take a quick look at
this now so all I got to do is update
this now so all I got to do is update
our random sampling code for the new
our random sampling code for the new
param spaces if I add a new reward to
param spaces if I add a new reward to
tune can I keep the previous bounds that
tune can I keep the previous bounds that
led to better
led to better
results or do I have to set everything
results or do I have to set everything
back you're probably decently safe
back you're probably decently safe
Spencer
Spencer
um it depends how tight the bounds are
um it depends how tight the bounds are
so what you would do is You' run you
so what you would do is You' run you
would run then Sweep with the tight
would run then Sweep with the tight
bounds and then if when you go look at
bounds and then if when you go look at
the graphs you know the best results are
the graphs you know the best results are
pushed up to one edge of the range on on
pushed up to one edge of the range on on
one of the parameters then you loosen
one of the parameters then you loosen
that bound right you do a little
that bound right you do a little
sensitivity analysis
sensitivity analysis
is why good graphs are
is why good graphs are
important oh and for YouTube folks
important oh and for YouTube folks
discord.gg puffer stop by come build
discord.gg puffer stop by come build
some RL environments with us it's
fun and St the repo as well that helps
fun and St the repo as well that helps
me a
me a
lot okay so here we got min max I'm
lot okay so here we got min max I'm
trying to think I want to do this
um there's also like a mean and a clip
um there's also like a mean and a clip
or did I get rid of Cl I think I got
or did I get rid of Cl I think I got
mean in
scale let's just do for now
scale let's just do for now
um let's keep these Min and Maxes
let's try this and then log uniform
let's try this and then log uniform
power
power
to oh actually do we even need
to oh actually do we even need
this no we don't need this cuz it's a
this no we don't need this cuz it's a
uniform and then we'll do log normal
be and do that
and with Jack it can be run on GPU
and with Jack it can be run on GPU
though are you sitting just writing and
though are you sitting just writing and
see and running on CPU would be better
see and running on CPU would be better
yep gpus are not designed to process
yep gpus are not designed to process
arbitrary logic and they're definitely
arbitrary logic and they're definitely
not designed to process you like writing
not designed to process you like writing
entire games in terms of array logic is
entire games in terms of array logic is
kind of an insane thing to
kind of an insane thing to
do like it's not CPUs are not slow
do like it's not CPUs are not slow
python is slow all of our environments
python is slow all of our environments
in C run in million steps a second and
in C run in million steps a second and
we have some of the most complex
we have some of the most complex
environments out there like here this
environments out there like here this
thing runs I think 1.9 million steps per
thing runs I think 1.9 million steps per
second on a single CPU core the
second on a single CPU core the
simulation of this does and we train
simulation of this does and we train
this at like 500k as well and like this
this at like 500k as well and like this
is here this
is here this
is a massive environment with like you
is a massive environment with like you
know tons of stuff going on uh there's
know tons of stuff going on uh there's
an economy system like like there's
an economy system like like there's
trade there's like enemies there's all
trade there's like enemies there's all
sorts of stuff going on in here and this
sorts of stuff going on in here and this
is just fast and all of our environments
is just fast and all of our environments
are fast like the faster ones are 10
are fast like the faster ones are 10
million we stop trying after a million
million we stop trying after a million
because it's fast enough but some of
because it's fast enough but some of
them are just even
them are just even
faster
faster
yeah there are like a few places where
yeah there are like a few places where
there are like things that are hard to
there are like things that are hard to
make fast but for the most part
yeah I don't know what you're on about
yeah I don't know what you're on about
Jason we're not
Jason we're not
doing we're not doing like train time
doing we're not doing like train time
rendering for the most
part the rendering is uh inference time
part the rendering is uh inference time
in
rib believe that just running the M1 CPU
rib believe that just running the M1 CPU
with the total training time yeah
with the total training time yeah
there's potentially transfer overhead so
there's potentially transfer overhead so
we do some bandwidth optimization I mean
we do some bandwidth optimization I mean
we have m's training in a million steps
we have m's training in a million steps
per second does anybody
per second does anybody
else maybe there a couple like Jack
else maybe there a couple like Jack
things on really simple environments
things on really simple environments
that do but for the most part not really
that do but for the most part not really
and like you're not implementing neural
and like you're not implementing neural
and Jack you're just not okay maybe if
and Jack you're just not okay maybe if
you're a total genius you can write like
you're a total genius you can write like
10,000 lines of jacks for an ear Mo 3 in
10,000 lines of jacks for an ear Mo 3 in
like a year but it's just or you can be
like a year but it's just or you can be
completely brain dead and write it and
completely brain dead and write it and
see and have it run 1.9 million steps
see and have it run 1.9 million steps
per second and it's just fine we train
per second and it's just fine we train
at 500k on that on one
GPU like our default training setup is
GPU like our default training setup is
at least a 100 times faster than the
at least a 100 times faster than the
majority of what is being done in RL
majority of what is being done in RL
right now like every time I get sent an
right now like every time I get sent an
academic Repository it's like 1 to
academic Repository it's like 1 to
10,000 steps per second and then most of
10,000 steps per second and then most of
our baselines it's like 400,000 to a
our baselines it's like 400,000 to a
million or 1.2 million steps per
second it's kind of
nutty this is how we get stuff done
nutty this is how we get stuff done
though
okay so this min max this is fine
okay so this min max this is fine
uniform power two is also
uniform power two is also
fine and then the log the logs are going
fine and then the log the logs are going
to be a little
different min max mean
different min max mean
scale what's neural MMOs SPS last I
scale what's neural MMOs SPS last I
checked about .9 million uh steps per
checked about .9 million uh steps per
second per CPU core and then the
second per CPU core and then the
training at the moment with the policy
training at the moment with the policy
that I'm using is around 500,000 on a
490 so that's like arguably one of the
490 so that's like arguably one of the
most complicated environments out there
most complicated environments out there
that's still ludicrously fast that's the
that's still ludicrously fast that's the
nice thing with with C is like I can
nice thing with with C is like I can
just make everything fast with Jack
just make everything fast with Jack
like when you get to complicated
like when you get to complicated
branching logic stuff's really hard to
branching logic stuff's really hard to
keep fast or like really hard to
keep fast or like really hard to
represent it's trivial and see you just
represent it's trivial and see you just
write the code in the most brain dead
write the code in the most brain dead
way possible and it's
fast e
see compiler yeah I mean but here like
see compiler yeah I mean but here like
the crazy thing about this is I'm pretty
the crazy thing about this is I'm pretty
sure the neural MMO 3 code is shorter
sure the neural MMO 3 code is shorter
than the crafter
code and that's like including all the
code and that's like including all the
really nice fancy rendering that we did
really nice fancy rendering that we did
like you literally just go here puffer
like you literally just go here puffer
lib ocean all the environments and then
lib ocean all the environments and then
like nurl mm mo3 is 1H
like nurl mm mo3 is 1H
file 20800 lines and like if you scroll
file 20800 lines and like if you scroll
through it the code is brain dead like
through it the code is brain dead like
this is about as complicated as it
this is about as complicated as it
gets it's it's it's like conditionals
gets it's it's it's like conditionals
and loops
like a first year CS student can read
like a first year CS student can read
most of this there may be a couple
most of this there may be a couple
things I do for perf in here but like a
things I do for perf in here but like a
first year CS student from their first
first year CS student from their first
systems force can read this because
systems force can read this because
there's nothing fancy at
all I don't think our first year CS
all I don't think our first year CS
student could have write this as cleanly
student could have write this as cleanly
I would hope I'd hope I do a little
I would hope I'd hope I do a little
better than that but read it
better than that but read it
yeah not true with the Jack's code
I'm going gamma and Lambda
G and Lambda not getting sample
I should switch my view back to
I should switch my view back to
this here we
go um let's see
go um let's see
this so gamma
how's this happen
05 right
oh hold on
um how do I do
this I think it's got to be like this
this I think it's got to be like this
cuz it's getting
cuz it's getting
clipped what's
happening I think it's one minus Max
happening I think it's one minus Max
man like
this still
this still
no that's
weird for
getting
getting
clipped it's
weird scale is5
05 I get from 05 to
02 get
O7 wait how the heck does the mean go
O7 wait how the heck does the mean go
from I don't understand this wait Min
from I don't understand this wait Min
Min Max
Min Max
oh oh something screwy here is this
oh oh something screwy here is this
parameter well
defined how's this the
mean okay so it's Max is
995 min this point mean should be
995 min this point mean should be
Point
Point
98 maybe these are the wrong order hold
on min max mean scale
on min max mean scale
right we do switch Max and Min yeah and
right we do switch Max and Min yeah and
then mean and
then mean and
scale okay so something screwy here
what is Max
what is Max
.98 min max mean
scale logic normal min max mean and
scale logic normal min max mean and
scale
right min max mean scale min
right min max mean scale min
oh
stupid
okay I probably don't need to reverse
okay I probably don't need to reverse
these now either do I let's
see okay that's better
yeah there we
go gamma
go gamma
Lambda Gamma
Lambda Gamma
Lambda these are
good BT Horizon
good BT Horizon
yep everything look good there
I think the big one is it's just not
I think the big one is it's just not
going to explore the
going to explore the
uh the highend of the total time steps
uh the highend of the total time steps
we will see
though I know nothing about AI is there
though I know nothing about AI is there
a tool that allows regular players to
a tool that allows regular players to
share their game matches with AI which
share their game matches with AI which
is intended to help them learn is that
is intended to help them learn is that
every currently and probably won't be
every currently and probably won't be
seeing in the near future just makes
seeing in the near future just makes
sense for it to
sense for it to
exist thanks uh I think people have done
exist thanks uh I think people have done
kind of shitty versions of this for
kind of shitty versions of this for
individual
games um
games um
there's not just like a plug into any
there's not just like a plug into any
game and have it work to help AI learn
game and have it work to help AI learn
oh that that's separate
oh that that's separate
um it's still going to be per game
um it's still going to be per game
there's not just a thing that's going to
there's not just a thing that's going to
take arbitrary data and solve it we're
take arbitrary data and solve it we're
still at the point where like you need
still at the point where like you need
to have there's a fair bit of per
to have there's a fair bit of per
problem work to be done I mean the
problem work to be done I mean the
closest thing that I've seen to what
closest thing that I've seen to what
you're describing there's this really
you're describing there's this really
funny uh League of Legends Tool that you
funny uh League of Legends Tool that you
can get where uh you can get like like
can get where uh you can get like like
it's it's like an AI that looks at your
it's it's like an AI that looks at your
screen it's probably a GPT rapper or
screen it's probably a GPT rapper or
something uh but then based on what's
something uh but then based on what's
going on you get Tyler one yelling at
going on you get Tyler one yelling at
you for being stupid which is pretty
you for being stupid which is pretty
funny so it's like this like fake AI
funny so it's like this like fake AI
coach
coach
thing that's the closest thing I've
thing that's the closest thing I've
seen I don't know if this is useful it's
seen I don't know if this is useful it's
but it's pretty funny
e
that's not
that's not
bad so just random
bad so just random
search I mean doesn't solve the
Tas the cost is too
low I mean it's not going to explore the
low I mean it's not going to explore the
full range unless you set
full range unless you set
it hang
it hang
on where's my uh my
on where's my uh my
window okay this is almost
window okay this is almost
done so we can compare to neoc
carps
carps
okay it doesn't quite solve the task
okay it doesn't quite solve the task
either here but this is with the cost
either here but this is with the cost
awareness
awareness
uh it's not doing higher cost over time
uh it's not doing higher cost over time
so something seems to have gone horribly
so something seems to have gone horribly
wrong
wrong
here I'm surprised it's
found I'm actually kind of surprised
found I'm actually kind of surprised
that it's found Solutions this good at
that it's found Solutions this good at
this low
this low
cost what did it
cost what did it
do any
do any
batches got Lambda spot on gamma spot on
batches got Lambda spot on gamma spot on
update
update
BPT Horizon off a little
BPT Horizon off a little
bit but I guess it increases cost and
bit but I guess it increases cost and
num man is spot on so I guess it did
num man is spot on so I guess it did
everything perfectly except that it
everything perfectly except that it
didn't want to increase the cost via
didn't want to increase the cost via
time step so it's a little too
time step so it's a little too
conservative here so that's why it's not
conservative here so that's why it's not
full solved interesting
enough now if I just do
enough now if I just do
do I'm pretty sure that if I just do
um if I just set this to one real
quick oh it's a little better but it
quick oh it's a little better but it
doesn't full solve
still it's going to it does explore the
still it's going to it does explore the
higher cost
higher cost
ones but uh it doesn't full solve still
ones but uh it doesn't full solve still
okay I mean it's random search you you
okay I mean it's random search you you
don't expect it to really do too
much so the next thing is going to be
um Asha
I really wish that there were random
I really wish that there were random
searches of base if like random search
searches of base if like random search
would just be a baseline here let me
would just be a baseline here let me
just make sure before I invest time in
just make sure before I invest time in
this that I think this is going to be
this that I think this is going to be
reasonable think I got a little jumbled
reasonable think I got a little jumbled
among my own words I saw some of your
among my own words I saw some of your
games on puffer what if the game was
games on puffer what if the game was
played by real people would it be
played by real people would it be
possible to improve these
possible to improve these
Bots yeah so it yes that is possible if
Bots yeah so it yes that is possible if
we set it up to collect data but the
we set it up to collect data but the
thing is you need quite a bit of data um
thing is you need quite a bit of data um
to give you an idea
to give you an idea
here these things like these games that
here these things like these games that
we have when we train these they are so
we have when we train these they are so
fast like you see this running in real
fast like you see this running in real
time but when we run this for training
time but when we run this for training
like this game runs at hours of gameplay
like this game runs at hours of gameplay
every second it runs at like 10,000
every second it runs at like 10,000
times real time or something ridiculous
times real time or something ridiculous
like that they're just very very highly
like that they're just very very highly
optimized so like this one for instance
optimized so like this one for instance
we literally trained a model to play
we literally trained a model to play
2,000 years worth of this game in a few
2,000 years worth of this game in a few
days on one GPU um so the amount of
days on one GPU um so the amount of
human data you need is quite substantial
human data you need is quite substantial
uh in order to get like really good
uh in order to get like really good
stuff out of
stuff out of
it it's also just not the area that I
it it's also just not the area that I
work in for the most part like most of
work in for the most part like most of
my work isn't on human data it's on like
my work isn't on human data it's on like
learning from interaction so I just
learning from interaction so I just
haven't really gone too far down that
haven't really gone too far down that
route um the methods in that area are
route um the methods in that area are
generally pretty easy and better
generally pretty easy and better
established whereas the area I'm working
established whereas the area I'm working
in here is like a way less developed
in here is like a way less developed
area of science there's just like a lot
area of science there's just like a lot
of potential for
improvement but yeah this this
improvement but yeah this this
environment here if each like time you
environment here if each like time you
see it move a tile that's one step this
see it move a tile that's one step this
thing runs at 1.9 million steps per
thing runs at 1.9 million steps per
second on one CPU
core without rendering when we uh when
core without rendering when we uh when
we run it for training so yeah these
we run it for training so yeah these
things are just like unfathomably
Fast they're all implemented in they're
Fast they're all implemented in they're
I mean they're all implemented in
I mean they're all implemented in
optimized C or relatively optimized C so
optimized C or relatively optimized C so
they're
fast okay so here's our random
fast okay so here's our random
Baseline know we average it a few times
Baseline know we average it a few times
it's probably 180 190 something like
it's probably 180 190 something like
that let's go look at
Asha back to review
mode so cars is a very complicated
mode so cars is a very complicated
method um it's quite possible is this
method um it's quite possible is this
it this is not it
it this is not it
carbs than you
AI let's do this so
carbs the reason that I'm a little
carbs the reason that I'm a little
suspicious right is they do all of this
suspicious right is they do all of this
math uh but they don't like smash the
math uh but they don't like smash the
Bas lines right it seems like they're
Bas lines right it seems like they're
really just the first ones to throw this
really just the first ones to throw this
on
on
RL and they lose to Asha by quite a
lot but the thing is Asha does worse on
lot but the thing is Asha does worse on
well actually no it wins at language
modeling it only does worse on image
classification it almost seems like a
classification it almost seems like a
bug because I have no idea why it would
bug because I have no idea why it would
just like randomly be way worse at
just like randomly be way worse at
that let me
that let me
see cuz like okay tpe and hebo are
see cuz like okay tpe and hebo are
pretty consistent right tpe is
pretty consistent right tpe is
consistently a little bit ahead then
consistently a little bit ahead then
carbs
is well it's not ahead here okay so
is well it's not ahead here okay so
there are a few shakeups I
guess so this result bothers me that o
guess so this result bothers me that o
was way down
here carbs performance is consistent
here carbs performance is consistent
across
across
tasks I
mean not really it sucks at language
modeling oh wait lowers better here why
modeling oh wait lowers better here why
would they do this what is wrong with
them what the hell's wrong with them
them what the hell's wrong with them
that they
that they
do episode reward accuracy then cross
do episode reward accuracy then cross
entropy okay hold on
so okay Asha is like second to carbs
so okay Asha is like second to carbs
here it's the best on
here it's the best on
RL and it's like the worst on image
RL and it's like the worst on image
classification
much lower variance distribution of
outputs that looks pretty high variance
outputs that looks pretty high variance
to
me they have bigger they have more
me they have bigger they have more
variance on ourl and Asha
does I have no idea why it would be
does I have no idea why it would be
specifically better for RL but let's see
let's go read this thing in a little
let's go read this thing in a little
more
more
detail it's
2020 I wonder if they're better
2020 I wonder if they're better
algorithms since then let's see
algorithms since then let's see
optuna what hyperparameter algorithms
optuna what hyperparameter algorithms
they
they
have state-of-the-art algorithms
have state-of-the-art algorithms
not
clickable where are your
algorithms red
algorithms red
search TP okay here's
search TP okay here's
tpe which we saw
right here is
tpe it
tpe it
does not so great on language
does not so great on language
modeling not so great on
modeling not so great on
RL and pretty well on image
RL and pretty well on image
classification they got C GP is just
classification they got C GP is just
kind of
kind of
generic non-dominated sorting genetic
generic non-dominated sorting genetic
algorithm
algorithm
2 in quasi
monflo this seems to not
be okay so it seems like really there
be okay so it seems like really there
aren't like updated libraries with all
aren't like updated libraries with all
the good
the good
algorithms this seems very neglected a
algorithms this seems very neglected a
very neglected area of science
let's look at
Asha scales linearly with number for
yep they offer this as a service this is
yep they offer this as a service this is
like a 100 lines of code that they offer
like a 100 lines of code that they offer
as a service that's fun
where's this
from CMU Google research and determined
from CMU Google research and determined
okay so some startup
okay so this is what they
okay so this is what they
use sequential
methods we don't care about
this Asha
algorithm successive
having the need to adopt it to a okay
having the need to adopt it to a okay
this is silly because it's trivially
this is silly because it's trivially
this is embarrassingly
this is embarrassingly
parallel allocate a small budget to each
parallel allocate a small budget to each
configuration evaluate all
configuration evaluate all
configurations keep the top
configurations keep the top
fraction increase the budget and repeat
fraction increase the budget and repeat
until maximum budget is
reached the resources can the resource
reached the resources can the resource
is going to be wall clocked
obviously
input number of configurations and
minimum resource R maximum resource
R reduction Factor
R reduction Factor
n minimum early stopping rate
s
okay s Max is log
okay T is get hyper parameter
okay T is get hyper parameter
configuration of n so you get and Hyper
configuration of n so you get and Hyper
parameter
configurations so you get what the hell
configurations so you get what the hell
is
is
this
ni r
ni r
i okay I don't understand what they're
i okay I don't understand what they're
doing with this but this seems like a
doing with this but this seems like a
very simple algorithm you're basically
very simple algorithm you're basically
just getting the top
K it doesn't seem like they're
adding more configurations back in
takes more resources to explore same
takes more resources to explore same
number configurate
number configurate
okay so this is literally just
okay so this is literally just
like run some configurations keep the
like run some configurations keep the
best ones double the time
best ones double the time
allocated um very simple idea unless I'm
allocated um very simple idea unless I'm
missing something I'll drop this into
missing something I'll drop this into
Claude in a second I use it just to
Claude in a second I use it just to
sanity check I'm not missing stupid
sanity check I'm not missing stupid
straightforward ways are not suited
straightforward ways are not suited
for parallel
for parallel
regime embarrassingly parallel approach
regime embarrassingly parallel approach
this is not
this is not
suited we would like results in little
suited we would like results in little
more than the time to
more than the time to
train one config what I don't understand
train one config what I don't understand
assume training time for configuration
assume training time for configuration
scales linearly with allocated
scales linearly with allocated
resource for given bracket the minimum
resource for given bracket the minimum
time to return a configuration
trained the time needed to return a
trained the time needed to return a
fully
fully
contrained which is three times time
contrained which is three times time
since there are three rungs each rung is
since there are three rungs each rung is
allocated our
resource the return answer in just our
resource the return answer in just our
time I don't understand that at all so
time I don't understand that at all so
let me
see maximum resource
R evaluate all configurations increase
R evaluate all configurations increase
budget until maximum per configuration
budget until maximum per configuration
budget
budget
of R is
reached oh I think that they're trying
reached oh I think that they're trying
to like reduce the number of
to like reduce the number of
configurations by at least a factor of
configurations by at least a factor of
two and then double the resources
two and then double the resources
allocated so they're running half the
allocated so they're running half the
number of jobs for twice the time or
number of jobs for twice the time or
something like
that this seems very
arbitrary and they say within a bracket
arbitrary and they say within a bracket
we each round promotions are
wrong okay sure whatever
another naive way of paralyzing CH
another naive way of paralyzing CH
distribute the
distribute the
training of the N overend of the K
training of the N overend of the K
surviving
surviving
configurations on each
configurations on each
rung and add brackets when there are no
rung and add brackets when there are no
jobs available in existing graphic
jobs available in existing graphic
synchronous Sha Sha synchronous nature
synchronous Sha Sha synchronous nature
is sensitive to stragglers and drop jobs
is sensitive to stragglers and drop jobs
as every configuration must
as every configuration must
complete before con proceeding to the
complete before con proceeding to the
next r
next r
estimate of the top one of our end
estimate of the top one of our end
configurations for early stopping does
configurations for early stopping does
not improve as more brackets are run
not improve as more brackets are run
since promotions are performed
since promotions are performed
independently for each
bracket okay let's see what they're
bracket okay let's see what they're
doing I think I think they're vastly
doing I think I think they're vastly
over complicating this this seems like a
over complicating this this seems like a
very simple algorithm that they're over
very simple algorithm that they're over
complicating unless they're doing
complicating unless they're doing
something very very clever right here
I mean the idea here is is literally
I mean the idea here is is literally
just like randomly sample parameters run
just like randomly sample parameters run
them all for a short amount of time see
them all for a short amount of time see
which ones do well keep those running
which ones do well keep those running
for longer repeat that's
for longer repeat that's
it we now introduce a sha effective
it we now introduce a sha effective
technique to paralyze
technique to paralyze
sha promotes configurations to the next
sha promotes configurations to the next
R whenever
R whenever
possible instead of waiting for a rum to
possible instead of waiting for a rum to
complete before proceeding to the next
complete before proceeding to the next
Rong if no promotions are
Rong if no promotions are
possible Asha simply adds a
possible Asha simply adds a
configuration to the base Rong the more
configuration to the base Rong the more
configurations can be promoted to Upper
configurations can be promoted to Upper
rungs so asynchronous nature does not
rungs so asynchronous nature does not
require the use of pree by number of
require the use of pree by number of
configuration to evaluate but it
configuration to evaluate but it
otherwise requires the same number the
otherwise requires the same number the
same inputs as
Shaw run then return Val loss is
Shaw run then return Val loss is
asynchronous the code execution
asynchronous the code execution
continues after the job is passed to the
continues after the job is passed to the
worker promotion scheme is laid out in
worker promotion scheme is laid out in
the get job sube okay so whatever
the get job sube okay so whatever
they're doing things um they're doing
they're doing things um they're doing
things
things
whatever the only place we would
whatever the only place we would
possibly do this m like
possibly do this m like
on we're probably just going to have one
on we're probably just going to have one
job of these per node unless we're using
job of these per node unless we're using
a really tiny networks in which case we
a really tiny networks in which case we
could just run this on CPU and see what
could just run this on CPU and see what
happens I don't think it's very I don't
happens I don't think it's very I don't
think it's much faster
think it's much faster
though our fastest uh training
though our fastest uh training
configurations are about 1 million 1.2
configurations are about 1 million 1.2
and then usually we get like 200k or
and then usually we get like 200k or
something on CPU so unless I'm wrong and
something on CPU so unless I'm wrong and
unless the CPU is way faster like 16 *
unless the CPU is way faster like 16 *
2 well 200 that's still 3 million maybe
2 well 200 that's still 3 million maybe
it's worth CPU we'll see I mean
it's worth CPU we'll see I mean
technically we could do both we could
technically we could do both we could
have GPU and CPU drops that'd be kind of
have GPU and CPU drops that'd be kind of
cool
Asha is well suited for the large scale
Asha is well suited for the large scale
regime wall clock time is constrained to
regime wall clock time is constrained to
a small multiple of the time needed to
a small multiple of the time needed to
train a single
model I mean that's kind of like
PBT population based
training population Based training is a
training population Based training is a
pain in the ass to parallelize though if
pain in the ass to parallelize though if
you really want to do it well I mean
you really want to do it well I mean
there's a naive method that's kind of
decent yeah no population Based training
decent yeah no population Based training
is really
is really
um yeah that's not really easy to
um yeah that's not really easy to
paralyze I think we're going to stay
paralyze I think we're going to stay
away from
that also this is not a concern for us
that also this is not a concern for us
uh we're not small multiple time to
uh we're not small multiple time to
transing this is what you do when you
transing this is what you do when you
have like hundreds of nodes
13 over 9 * time of
r nine machines are
r nine machines are
sufficient to
sufficient to
promote configurations to the next Rong
promote configurations to the next Rong
and the time it takes to train and
and the time it takes to train and
configure a single configuration in the
configure a single configuration in the
Rong and training time for a
Rong and training time for a
configuration in run zero is 1 nth of
configuration in run zero is 1 nth of
the time one run it's
the time one run it's
13 or rung two it's time of
R log n machines are needed to advance a
R log n machines are needed to advance a
configuration in the next run the same
configuration in the next run the same
time it takes to train a single
time it takes to train a single
configuration in the
configuration in the
Run okay I don't really understand what
Run okay I don't really understand what
they're doing we're going to look at the
they're doing we're going to look at the
pseudo code
hyper band simply runs multiple sha
hyper band simply runs multiple sha
brackets we can asynchronously paralyze
brackets we can asynchronously paralyze
hyper
hyper
band
band
interesting so let's take a look at
interesting so let's take a look at
this for each worker get
this for each worker get
job run then return Val loss
job run then return Val loss
complete a
complete a
job update configuration
job update configuration
Theta in rung K with l
l so what do they do they do 4K
is log
I think they're just running fewer jobs
I think they're just running fewer jobs
for a machine is all
oh do they have a comparison to PBT
here no PBT is pentry Bank in this case
here no PBT is pentry Bank in this case
okay
let's do uh
what is
sherpa oh okay hyper parameter
sherpa oh okay hyper parameter
algorithm oh cool they have uh
algorithm oh cool they have uh
they got
they got
PBT they got a think really
PBT they got a think really
cool very
nice we'll see what their implementation
looks and what why
where's
Asha
algorithms sh algorithm successive Happ
algorithms sh algorithm successive Happ
thing
there's
algorithms My
algorithms My
Guy
where where
this it's the wrong
freaking is this the wrong one oh yeah
freaking is this the wrong one oh yeah
wait this is the wrong
wait this is the wrong
one isn't it wait didn't I click this
one isn't it wait didn't I click this
from here
here okay there we go me Dum Shar
here okay there we go me Dum Shar
algorithms okay good it's
algorithms okay good it's
simple there we
go oh what a nice Library this is
actually this is nice and simple good
actually this is nice and simple good
job um
um yeah this is
um yeah this is
good they have they have foure dead
good they have they have foure dead
spaces study
search R
search they actually have some nice
search they actually have some nice
little algorithms I could just try all
little algorithms I could just try all
these on our synthetic test as a
these on our synthetic test as a
baseline the only thing is I don't know
baseline the only thing is I don't know
if they've handled the uh the space
if they've handled the uh the space
Transformations
Transformations
correctly though I can probably
correctly though I can probably
Benchmark their version to start and
Benchmark their version to start and
then see if I can do
better that might be a fun thing to do
yeah let's do
yeah let's do
that cuz this looks really simple I'm
that cuz this looks really simple I'm
not going to add this as a dependency to
not going to add this as a dependency to
puffer it's just not happening but
um we will use it for some tests see if
um we will use it for some tests see if
we can beat
we can beat
carbs just you know out the gate
I think this will have finished by now
I think this will have finished by now
yeah so here's neoc
carbs not bad I got to about there with
carbs not bad I got to about there with
the full thing this is with clost
the full thing this is with clost
awareness a couple little quirks here I
awareness a couple little quirks here I
think we could make this still a fair
think we could make this still a fair
bit better but let's just see if we like
bit better but let's just see if we like
instantly crush this or anything right
okay you do not need Caris
good and they do have
good and they do have
log they don't let you specify me
log they don't let you specify me
they just let you specify
bounds it's not terrible though
like this
what the hell
what the hell
dashboard
e e
I don't
I don't
know I'm kind of tempted to just do my
know I'm kind of tempted to just do my
own implementation of
this they've got a bunch of obnoxious
this they've got a bunch of obnoxious
stuff here some
stuff here some
dashboard their own spaces
yeah I think we're going to just do our
yeah I think we're going to just do our
own it's only like 100 lines I'd rather
own it's only like 100 lines I'd rather
do that than have to implement this like
do that than have to implement this like
to add bindings for this stupid
thing just popping in how's it coming
thing just popping in how's it coming
going well running new tight Sweep use
going well running new tight Sweep use
your log X Trek should be like make the
your log X Trek should be like make the
sweep like five billion
sweep like five billion
steps writing 3D comp graphs go up and
steps writing 3D comp graphs go up and
right yeah I mean they're going to go up
right yeah I mean they're going to go up
and right on log scale
and right on log scale
maybe but like should it take five
maybe but like should it take five
billion steps to solve a simple level of
billion steps to solve a simple level of
your puzzle probably
not let me
not let me
do this back to
do this back to
here for
gra
this it starts
this it starts
with with random
right moded files
oops
oops
[Music]
[Music]
rain
one e
okay so it's just get suggestion I
okay so it's just get suggestion I
assume is the their API and then it just
assume is the their API and then it just
calls
calls
everything so we do
you start this with
okay I see
so I don't know why they have a data
so I don't know why they have a data
frame
jeez this is like very short but very
jeez this is like very short but very
confusing the way it's implemented
I really don't like the way this is
I really don't like the way this is
implemented
let me
let me
see you get a
job get job
job get job
is check if there is a promotable
fake e
minimum resource each config will be
minimum resource each config will be
trained for maximum
resource elimination rate
resource elimination rate
minimum early
stop okay that's not
stop okay that's not
bad
bad
um yeah that's not bad
you get a
suggestion top and
I don't see how this is
I don't see how this is
working seems like you need to
working seems like you need to
run a bunch of Trials before you do
this there's something tricky here
because if you initialize this with
16 how many times do you want to double
16 how many times do you want to double
this thing like
probably have two different cut
offs
e e
and I think
all this is just for a
baseline okay hold
on we can do
I think we can essentially
I think we can essentially
do the maximum performance of
this we can essentially get the maximum
this we can essentially get the maximum
performance of
this without implementing
this without implementing
it because the thing is it's based on a
it because the thing is it's based on a
random search
I think that's
fair let me see
fair let me see
something I think
something I think
if we're being smart here we can get we
if we're being smart here we can get we
can get the performance of this
thing okay so all we have to do
is if I just do like args
trains sweep
what if I just do
this is this correct
this okay so here
okay
so this does not consistently solve this
so this does not consistently solve this
Benchmark even at Max
cost it gets close on some of them but
cost it gets close on some of them but
it's not consistently
it's not consistently
solving this
solving this
Benchmark uh let me think if there's
Benchmark uh let me think if there's
anything I'm missing here
there could
there could
be some parameter
adjustments I
adjustments I
think yeah there could be some parameter
think yeah there could be some parameter
adjustments hold
on so this uniform is these are just s
on so this uniform is these are just s
randomly which is
fine about
to no I think that these are pretty good
so if we do this
right for
so here we have
so here we have
263 which is
solved those are very good
hypers this is 200 experiments
though and not consistently solve in
200 so basically the premise
200 so basically the premise
here is that Asha is random
here is that Asha is random
search uh with basically compute
search uh with basically compute
adjustments on it but if you're going to
adjustments on it but if you're going to
start with 200
experiments if you're going to start
experiments if you're going to start
with 200 experiments
with 200 experiments
then as is not going to do better than
then as is not going to do better than
just running those 200 experiments for
just running those 200 experiments for
the full amount of time so this
the full amount of time so this
basically this gives us an upper bound
basically this gives us an upper bound
on asha's performance uh it's going to
on asha's performance uh it's going to
drop some of those it's actually going
drop some of those it's actually going
to be a little worse than this and this
to be a little worse than this and this
is not full solving this task and I
is not full solving this task and I
believe that you should be able to solve
believe that you should be able to solve
this task uh in less than 200
this task uh in less than 200
samples so this is not going to cut it
samples so this is not going to cut it
for
for
us which which makes sense because again
us which which makes sense because again
it's
it's
like it's just based on random search
like it's just based on random search
it's not adaptive in any way
it's not adaptive in any way
so I mean it can probably get pretty
so I mean it can probably get pretty
close to these numbers which is like
close to these numbers which is like
decent but my guess is that the reason
decent but my guess is that the reason
that it uh it looked good on the
that it uh it looked good on the
baselines versus carbs is carbs is just
baselines versus carbs is carbs is just
very undertuned in the original
very undertuned in the original
implementation
implementation
so basically
so basically
whatever we do is going to have to be
whatever we do is going to have to be
adaptive to some
adaptive to some
extent
extent
um I think carbs really does make the
um I think carbs really does make the
most sense they're just some small
most sense they're just some small
algorithmic adjustments and some code
algorithmic adjustments and some code
quirks that we're going to have to
quirks that we're going to have to
address so this does give me more
address so this does give me more
confidence investing more into
confidence investing more into
carbs
carbs
um so I'm going to go I'm going to use
um so I'm going to go I'm going to use
the rest real quick I'm going to get
the rest real quick I'm going to get
myself another cup of
myself another cup of
tea and then we're going to continue on
tea and then we're going to continue on
uh on carbs so having a stronger
uh on carbs so having a stronger
Baseline because this is basically upper
Baseline because this is basically upper
bound of random search is what we just
bound of random search is what we just
did so yeah be right
back
e
e
e
e
e
e
e
e
e
e
e e
you know it's occurred to
you know it's occurred to
me that we should look at some of the
me that we should look at some of the
other
other
baselines uh versus carbs on RL because
baselines uh versus carbs on RL because
Asha should not beat
Asha should not beat
carbs um carb should be an improvement
carbs um carb should be an improvement
on
on
hebo which should I mean this should
hebo which should I mean this should
very handily beat Asha Asha is a very
very handily beat Asha Asha is a very
bad algorithm uh now that I'm looking at
bad algorithm uh now that I'm looking at
it so
it so
so let me see why that's not
happening too many
tabs too many
tabs too many
tabs uh where' the paper
tabs uh where' the paper
go here yeah so this is a bad algorithm
go here yeah so this is a bad algorithm
um back to review mode
where is
the where is it okay reinforcement
the where is it okay reinforcement
learning so Asha is up
learning so Asha is up
here okay carbs his second
best so the fact that
best so the fact that
Asha wins here is
insane terminates lowest performing runs
insane terminates lowest performing runs
early it's able to sample many more
early it's able to sample many more
parameters than other
parameters than other
algorithms that doesn't make
algorithms that doesn't make
sense at
sense at
all because the thing
all because the thing
is whatever the length of time is it's
is whatever the length of time is it's
running those early samples for is about
running those early samples for is about
the length of time that carb should be
the length of time that carb should be
running its early
running its early
samples so basically what that
means is that
means is that
the carbs and Asha doesn't build a
the carbs and Asha doesn't build a
predictive model of performance either
predictive model of performance either
so what should happen is carbs is going
so what should happen is carbs is going
to run a bunch of quick experiments like
to run a bunch of quick experiments like
Asha does but unlike Asha it should be
Asha does but unlike Asha it should be
running uh a predictive model of
running uh a predictive model of
performance it should be building a
performance it should be building a
predictive model of performance which
predictive model of performance which
then allows it to more intelligently
then allows it to more intelligently
select better parameters for longer
runs I guess what Asha does it just says
runs I guess what Asha does it just says
that hey whatever parameters I happen to
that hey whatever parameters I happen to
randomly sample that turn out to be good
randomly sample that turn out to be good
just keep running those but that's not
just keep running those but that's not
quite what you want to
quite what you want to
do you really want to be able to look at
do you really want to be able to look at
the good runs and then search locally
the good runs and then search locally
around those because like and use a
around those because like and use a
predictive model of performance like
predictive model of performance like
most of the variables in the hyper
most of the variables in the hyper
parameter search are pretty damn easy to
parameter search are pretty damn easy to
fit they're like mostly independent and
fit they're like mostly independent and
they've got a stable region or like it
they've got a stable region or like it
just like more is better or less is
just like more is better or less is
better you know something like that
does this out form hebo on
everything yes it does
everything yes it does
okay hebo is a simpler Baseline that
okay hebo is a simpler Baseline that
carbs is built off of it's not hugely
carbs is built off of it's not hugely
better though but at the cost awareness
better though but at the cost awareness
is important because I don't think the
is important because I don't think the
these tell the full picture Okay um
these tell the full picture Okay um
things to modify next on
carbs we've got our current
performance right
performance right
here okay so this is cost
aware and then this one
so this gets up to about there and then
so this gets up to about there and then
this
this
one the non-cost to wear one Sol in like
one the non-cost to wear one Sol in like
80ish 8 samples and it actually gets
80ish 8 samples and it actually gets
even like a little last bit of perf at
even like a little last bit of perf at
the end here you can
the end here you can
see it gets pretty well up there and
see it gets pretty well up there and
then it does the total time steps
then it does the total time steps
towards the end okay so
towards the end okay so
maybe wait a second
something screwy
here let me go look at the uh the
here let me go look at the uh the
formula I made maybe there's something
formula I made maybe there's something
true with the
test log two of total time
steps
um how do they get
score up
here how do they get score up
here how do they get score up
here without the total time steps being
here without the total time steps being
up
there okay I think there is something
there okay I think there is something
potentially screwy here because total
potentially screwy here because total
time step should be
total time steps should have more of an
effect something is quite screwy
here let me just make sure
okay so
okay so
254 is supposed to be the
254 is supposed to be the
max it found a 262 somehow hold on what
max it found a 262 somehow hold on what
the something's Bizarro
um
oh wait this is
oh wait this is
one9 this should
one9 this should
be is it one10 is the max
okay yeah 282 is the
max but something screwy there
oh I
see
oops e
let me think how I do this
it's better
27 is
27 is
like too
small for most
small for most
problems even be sampling
there five
Square Ro of batch
size e
it's not
bad that's probably
better cost is going to be
milon and this num m is okay
milon and this num m is okay
good let me see if this does anything
good let me see if this does anything
different
I kind of don't
like well this is fine just
like well this is fine just
scaling little time
steps I think this is
fine I think what was happening before
fine I think what was happening before
is that um the time steps on the score
is that um the time steps on the score
just wasn't very
sensitive it should be the case that
like okay so this is zero
do+
one so that gives you going from 50
one so that gives you going from 50
million to a
billion yeah so that actually gives you
billion yeah so that actually gives you
a reasonable
a reasonable
sport that's
better let me see just off of this hey
better let me see just off of this hey
welcome I miss see just off of
this if there's a substantial difference
this if there's a substantial difference
and then then while this runs we'll look
and then then while this runs we'll look
at other outfit and
stuff
stuff
cars where is this
I don't want to wait for
freaking we don't need this
anymore I want to wait for Neptune for
anymore I want to wait for Neptune for
this I kind of should
so this should
my
token and then this is the full task
token and then this is the full task
right yeah the full cost to wear task
good so let's see if that changes
good so let's see if that changes
anything
anything
and we'll see which parameters if any
and we'll see which parameters if any
now are
screwy oh yeah they're
screwy oh yeah they're
uh what are we
uh what are we
printing
score cost
no this isn't good they're all the same
no this isn't good they're all the same
they're all the same
they're all the same
score okay let's figure out what the
score okay let's figure out what the
hell's wrong with
this oh
this oh
du so that's like max four
this will actually be a nice
test we should scale this so that the
test we should scale this so that the
max score is like 100
though me think had to do
this maybe at the
this maybe at the
end let's just run this let's just get
end let's just run this let's just get
this
running actually and let's make sure we
running actually and let's make sure we
[Music]
[Music]
remove we remove the
these mess up our
run there we
run there we
go so the max is now like 3,000 or
go so the max is now like 3,000 or
whatever
where am
where am
I oh this is not doing it globally
I oh this is not doing it globally
okay just make sure we get Neptune on
wait test
wait test
carbs yeah right
here okay so that'll
run
run
check couple quick
things e
so here's neoc
carbs we'll see what this
carbs we'll see what this
does in the
does in the
meantime we look at the paper again
and we look for things to
improve this doesn't work quite the way
improve this doesn't work quite the way
I want it
to it's mostly these two pages the math
to it's mostly these two pages the math
pages
they've got three gan processs
it occurs to me that you could get
it occurs to me that you could get
dramatically more training data for
dramatically more training data for
these
these
things by giving it every
things by giving it every
single like checkpointed output of the
model I don't know if I want to do that
model I don't know if I want to do that
though because it basically presumes
though because it basically presumes
that your your graphs are nice and
that your your graphs are nice and
stable
I could just make sure
I could just make sure
that that they
are it's a little tricky though cuz like
are it's a little tricky though cuz like
if you need to evaluate 10 million steps
if you need to evaluate 10 million steps
worth of data for something to be stable
worth of data for something to be stable
like that per those perf numbers are
like that per those perf numbers are
already
old so I think we'll stick
with I guess we stick with the the
with I guess we stick with the the
current model
I mean
I mean
it's how many points do you need to
it's how many points do you need to
train a gan process
10 times for basic training
ah GP scale poorly with
data use
need some full solt puff puffer
merch once we get there you don't have
merch once we get there you don't have
you don't start making merch until
you don't start making merch until
you've got a bunch of Revenue and we've
you've got a bunch of Revenue and we've
got only a little Revenue at the
got only a little Revenue at the
moment there will eventually be puffer
moment there will eventually be puffer
things
let's solve RL first
freaking gaussian processes
man all right well that's actually
man all right well that's actually
looking way
looking way
better look at that 39 points and total
better look at that 39 points and total
time steps it's properly
increasing it's a little
increasing it's a little
early I would like it to have
early I would like it to have
full correct
full correct
typers they're pretty close hold
typers they're pretty close hold
on wait a second this is really
close okay it's missed update
close okay it's missed update
epox that should be
four it's literally just missing one
four it's literally just missing one
parameter after only 42 trials
that's very
good let me look at what we have now so
good let me look at what we have now so
we've gotten rid of
pearch we're still you we actually don't
pearch we're still you we actually don't
have a trust region at the
moment this could occasionally get get
moment this could occasionally get get
us a bad
sample we should compare this to hard
sample we should compare this to hard
trust region that will be an experiment
trust region that will be an experiment
we're
trying yeah so we will compare this to
trying yeah so we will compare this to
hard trust
hard trust
region they did not run that
region they did not run that
ablation that seems like a reasonable
ablation that seems like a reasonable
thing to do
yeah and the normal centered on the mean
yeah and the normal centered on the mean
as well as
as well as
like well hang
on yeah no I think the uniform is
on yeah no I think the uniform is
probably
better I mean
is normal a better distribution actually
is normal a better distribution actually
if you think about
it because like if you run a normal
it because like if you run a normal
distribution
distribution
right most of your parameters should
be should be closer to the center and
be should be closer to the center and
then you end up with like a few
then you end up with like a few
parameters that are more
deviating maybe you can solve that just
deviating maybe you can solve that just
by generating more samples
by generating more samples
though it needs to be clipped either way
though it needs to be clipped either way
I
think because otherwise you get outside
think because otherwise you get outside
the support of your gaussian
process so I think that they're correct
process so I think that they're correct
in doing a trust
region but it should have been a hard
region but it should have been a hard
trust
region soft trust region is
region soft trust region is
to it has NE it has weird interactions
to it has NE it has weird interactions
with your other parameters so we're not
with your other parameters so we're not
going to do
that let's look at the acquisition
that let's look at the acquisition
function
clamping this is the really weird one to
me and they do have an ablation for it
so no clamping is a
so no clamping is a
huge fail
here no parito huge fail and the no
here no parito huge fail and the no
resampling actually is not going to hurt
resampling actually is not going to hurt
us at all in the current test but it
us at all in the current test but it
will hurt us in the real
ones no Pito presumably is just they do
ones no Pito presumably is just they do
the
the
Mac so they turn it from a Pito based
Mac so they turn it from a Pito based
search to like a essentially just turns
search to like a essentially just turns
into a simple genetic
algorithm no
clamp this is what they mean right
let me think about this
this kind of does naturally like push up
this kind of does naturally like push up
the
the
time
allocation cuz as you sample points that
allocation cuz as you sample points that
are longer then you push your samples
are longer then you push your samples
towards that end of the
distribution this still is not great
distribution this still is not great
though
this is possibly the weakest part right
this is possibly the weakest part right
now
how are we doing
how are we doing
here over here
what the hell happened did it just get
bored and start doing others parts of
bored and start doing others parts of
the pero
the pero
front where is the Pito
front cost
is there not already
is there not already
a a prito front in
a a prito front in
here I thought I had
here I thought I had
one I guess
not access
okay so this is
okay so this is
the Pito
front to be fair it is actually very
front to be fair it is actually very
nicely filling this
nicely filling this
in
um see 11415
doesn't seem to be biasing strongly
doesn't seem to be biasing strongly
enough
though to over here
yeah I think it's just trying to sample
yeah I think it's just trying to sample
lower cost
lower cost
regions you see the cost goes up and now
regions you see the cost goes up and now
it's like kind of all over
so what would cause
that I guess just um this acquisition
that I guess just um this acquisition
function Maybe
it's a very inelegant way to do an in to
it's a very inelegant way to do an in to
do
this they're just sampling on log scale
this they're just sampling on log scale
so
so
like if you have 10 seconds 100 seconds
like if you have 10 seconds 100 seconds
and a th000
and a th000
seconds a third of your runs are going
seconds a third of your runs are going
to be the really fast stupid
ones a third are going to be these
ones a third are going to be these
like 100ish mil and you're not really
like 100ish mil and you're not really
going to sample that much at the top end
going to sample that much at the top end
are
you but I mean the question's how
you but I mean the question's how
strongly you want to
buyas it seems like it ought to be a
buyas it seems like it ought to be a
function of the confidence of your
function of the confidence of your
prediction
doesn't this
um wait doesn't this give you a
um wait doesn't this give you a
variance the Gan
process it does doesn't
it so like when you stick this into
gpy it should give you
variance let me go find the code for
variance let me go find the code for
that I think that's going to be the the
that I think that's going to be the the
major thing I can do
this is going to be a to write a
this is going to be a to write a
blog post about with all this math I'm
blog post about with all this math I'm
going have to simplify this so much
okay
okay
so here you get your samples
right surrogate model.
observe Sur again
ah you see
ah you see
this it has right here it
this it has right here it
has the surrogate variance in it
okay and then there's this exploration
okay and then there's this exploration
bias so we have expected
Improvement EI value
see acquisition function
value okay so this acquisition function
value okay so this acquisition function
value is just the EI value
value is just the EI value
value you've got a thing for Success
value you've got a thing for Success
probability a thing for Max
probability a thing for Max
cost this is fine it's right
cost this is fine it's right
here expected
Improvement cars utils
oh Jesus okay what is
this well there's a
link so it's this function right here or
link so it's this function right here or
this function right
here and it should
here and it should
be where's mu minus best mu
be where's mu minus best mu
minus exploration bias
probability of improvement
upper tail prob
ability
ability
ah
obnoxious this
obnoxious this
probability much we can improve so
probability much we can improve so
expected this is it
select the point that minimizes the
select the point that minimizes the
distance to the objective evaluated at
distance to the objective evaluated at
the
maximum mockus proposed okay JB
maximum mockus proposed okay JB
mockus feel like I'm being mocked here
what the hell is
this you've
got let me see what the parameters are
got let me see what the parameters are
actually going into this so you
have you have the Gan processes
have you have the Gan processes
prediction of mean and variance
prediction of mean and variance
okay and then you
okay and then you
have moo best moo which you're using as
have moo best moo which you're using as
a Baseline and then some
a Baseline and then some
exploration bias so prior Sigma
okay Z is going to be moo minus best moo
okay Z is going to be moo minus best moo
minus over
minus over
Sigma divide by standard deviation
fine original form is this and then
fine original form is this and then
simplified form
simplified form
is what what the okay um original form
is what what the okay um original form
is Sigma
Times log prob of
Times log prob of
Z didn't we just multiply by
Sigma oh yeah but it's okay it's over
Sigma oh yeah but it's okay it's over
fine so Sigma time log Prop z
fine so Sigma time log Prop z
1+ Z I have no idea what this
is and then there's even more okay
great do I have to care what the hell
great do I have to care what the hell
this
this
is or is the change I want to make
is or is the change I want to make
independent of this
how does the variance even come into
how does the variance even come into
this
if it's the expected
Improvement let me see if I can get GPT
Improvement let me see if I can get GPT
that I
that I
freaking or
freaking or
claw break in my freaking head
here
e
e
e
e e
oh wait hold
on
for
e e
I know what I want to do I got to figure
I know what I want to do I got to figure
out the math behind it
expect the improvement over Baseline is
expect the improvement over Baseline is
also normally
distributed okay so breaking it down
distributed okay so breaking it down
this is standard
Improvement so you penalize the
Improvement so you penalize the
uncertainty you remove the
uncertainty you remove the
re and then wait by cost
so the Improvement is going to
so the Improvement is going to
be mo minus Sigma minus best
be mo minus Sigma minus best
moo
moo
okay so we make this
pessimistic and then what what does this
pessimistic and then what what does this
thing propose this is times
cost 1 plus cost variance over
cost one plus cost variance over cost
I'm trying to
think I don't like this CU this isn't an
think I don't like this CU this isn't an
estimate of improvement right
the idea is to wait
the idea is to wait
heavily we want to wait heavily towards
heavily we want to wait heavily towards
high cost but heavily penalized
high cost but heavily penalized
uncertainty of the estimate
how does c come in
here I'll just C him in here
okay so this is what they're doing at
okay so this is what they're doing at
the moment which is really
stupid this is just like sampling time
stupid this is just like sampling time
in log space or biasing by a log factor
in log space or biasing by a log factor
in
time they're just using this as a
time they're just using this as a
baseline now what we want is we want to
baseline now what we want is we want to
use we want the actual improvement over
use we want the actual improvement over
the Paro
the Paro
point right
this is a deterministic sample at the
moment but that's actually fine yeah cuz
moment but that's actually fine yeah cuz
it is the max
okay so you generate
samples we make the expected Improvement
samples we make the expected Improvement
conservative
we subtract the true Paro
Baseline and now how do we bias
this so we're going to get rid of this
this so we're going to get rid of this
clamping
is
I mean the cost uncertainty should be
I mean the cost uncertainty should be
pretty darn easy to predict
we actually do we want to
upweight we want to upate high
cost let's do this initially and then
cost let's do this initially and then
we'll
see
e e
let me see what else I want to do with
let me see what else I want to do with
this so I
this so I
think I'm just using this thing as like
think I'm just using this thing as like
a sanity check basically to make sure
a sanity check basically to make sure
I'm not doing something like obviously
stupid it will absolutely suggest insane
stupid it will absolutely suggest insane
things if you let it um it's not even a
things if you let it um it's not even a
consistent s you check it's just better
consistent s you check it's just better
than
nothing semi-intelligent rubber
duck
e
e e
directly scaling
cost predicted cost performance
relationship we do have that
If instead of scaling by cost why don't
If instead of scaling by cost why don't
we scale by absolute
performance
um so performance Improvement you do Mo
um so performance Improvement you do Mo
minus Sigma variance and Paro moo
predicted
benefit okay so this is the performance
benefit okay so this is the performance
of the cost
you
more e
let me see
this okay so we get relative performance
then we get absolute performance does
then we get absolute performance does
this
this
work is this insane or does this work
likely better than the current Pito
likely better than the current Pito
front from high relative
front from high relative
Improvement likely to achieve good
Improvement likely to achieve good
absolute
performance for
could
be weighted
some different powers
some different powers
[Music]
M let me see
I think we try
this e
let's
see let me see this
so do I need to make this conservative
so do I need to make this conservative
to start
to start
with these are kind of two separate
improvements but they are somewhat
linked for
Al so
where's the re
I don't see the
re expected
Improvement uh I don't see the
clamp it's supposed to be
NOP there's no
NOP there's no
Max so I don't see the
re a clamp
okay well let's just see if this
okay well let's just see if this
completely breaks everything
right so here's neoc
right so here's neoc
carps it does like interesting
l servative
okay just wait until it gets uh past
okay just wait until it gets uh past
random at least I'm go use the restroom
random at least I'm go use the restroom
and we'll see if we get uh any points
and we'll see if we get uh any points
here
making it
conservative I mean it seems like it
conservative I mean it seems like it
should help it might not I think I'm
should help it might not I think I'm
going to keep it as long as it doesn't
going to keep it as long as it doesn't
completely destroy
performance because it makes sense to do
the idea here is that the Gan process
the idea here is that the Gan process
one of the nice things about it is it
one of the nice things about it is it
gives
gives
you the variance of its
you the variance of its
predictions so you you just subtract the
predictions so you you just subtract the
standard deviation you get a
standard deviation you get a
conservative estimate of performance uh
conservative estimate of performance uh
which then naturally up weights The
which then naturally up weights The
Confident
Confident
predictions which should lead to like
predictions which should lead to like
smoother and cleaner graphs where it
smoother and cleaner graphs where it
it's not just like sampling points that
it's not just like sampling points that
randomly just get terrible performance
randomly just get terrible performance
and wasting an
experiment I don't know if this one
experiment I don't know if this one
change alone is going to be enough to
change alone is going to be enough to
see a major difference um but then what
see a major difference um but then what
we're going to do is this is definitely
we're going to do is this is definitely
a problem here let me
see so this pattern is definitely a
see so this pattern is definitely a
problem it's just spending way too much
problem it's just spending way too much
time on just bad data points uh and you
time on just bad data points uh and you
can see here it's just it's running too
can see here it's just it's running too
many short runs even at the end here
many short runs even at the end here
it's just running too many short runs it
it's just running too many short runs it
hasn't full solve this like it can get
hasn't full solve this like it can get
substantially better performance than
substantially better performance than
this um let me see what it's
missing it's actually pretty funny
missing it's actually pretty funny
because
because
the Paro front if you look at this most
the Paro front if you look at this most
of the points are on the Pito front so
of the points are on the Pito front so
it's not doing like bad experiments it
it's not doing like bad experiments it
just has too many experiments down here
just has too many experiments down here
and it hasn't done enough up
and it hasn't done enough up
here um it should be like having more of
here um it should be like having more of
them out here
because yeah it just it hasn't gotten
because yeah it just it hasn't gotten
all the way full solve
yet this down here is especially
yet this down here is especially
bad there's no reason for it to be doing
this though I guess technically it's
this though I guess technically it's
still possible for carbs to do this um
still possible for carbs to do this um
but only once it's like it's confident
but only once it's like it's confident
that there are no more improvements at
that there are no more improvements at
the top end and I think that I think I
the top end and I think that I think I
checked this and I think that the
checked this and I think that the
maximum was like 3200 3500
maximum was like 3200 3500
something so I think there is more perf
something so I think there is more perf
I will double check that but I think
I will double check that but I think
that there is more perf to be
gained okay that's nice and clean to
gained okay that's nice and clean to
start we'll see if it
continues let me use the restroom I'll
continues let me use the restroom I'll
be right back and then we will we'll let
be right back and then we will we'll let
this thing run for a bit and we'll work
this thing run for a bit and we'll work
on the uh the new acquisition function
on the uh the new acquisition function
one second let's mute
this
e
e
e
e
e
e
e
e
e
e
e
e e
ah forgot to unmute my mic oh are
ah forgot to unmute my mic oh are
supposed to tell me I've been talking
supposed to tell me I've been talking
this whole
this whole
time
um whatever this is good this is I mean
um whatever this is good this is I mean
the summary is that this looks better
the summary is that this looks better
than the other one by a little bit um
than the other one by a little bit um
right up to
right up to
1,600 starting to widen a little bit but
1,600 starting to widen a little bit but
I this is just from adding conservative
I this is just from adding conservative
bound so up to 1600 versus on
here hold on if I do if I do
this I guess these look kind of
similar I don't think the cost looks
similar I don't think the cost looks
similar
similar
though hold on if I look at the
though hold on if I look at the
costs in the same time period right it's
costs in the same time period right it's
like here
c h may be kind of
c h may be kind of
similar I mean I said that if this
similar I mean I said that if this
doesn't horribly screw stuff up we're
doesn't horribly screw stuff up we're
going to keep it right
wait
wait
2600 what was the the highest Flore on
2600 what was the the highest Flore on
the other
the other
one what's the number to
one what's the number to
be oh we're actually better
be oh we're actually better
already this only gets to like 2500 all
already this only gets to like 2500 all
the way over
here yeah we're all the way up to 20
here yeah we're all the way up to 20
okay this is
okay this is
cool whether this is statistically
cool whether this is statistically
significant I I have no
significant I I have no
idea but uh this is it looks
good this is a relatively small
good this is a relatively small
change making it
change making it
conservative um I don't expect this to
conservative um I don't expect this to
magically solve everything hopefully it
magically solve everything hopefully it
doesn't just magically crash everything
doesn't just magically crash everything
either I'm not going to let this run all
either I'm not going to let this run all
200 without doing anything though so
200 without doing anything though so
let's
go so we're going to do a few things
here we're going to do this first of all
here we're going to do this first of all
get rid of
get rid of
that um and
that um and
then acquisition
then acquisition
function it's going to be the
function it's going to be the
conservative expected Improvement and
conservative expected Improvement and
what did we say we have it in here
somewhere yeah so we do relative
somewhere yeah so we do relative
Improvement we fix this and we do
Improvement we fix this and we do
absolute Improvement is going to be moo
absolute Improvement is going to be moo
minus Sigma then we multiply
them
them
salute the I
equals absolute score equals
and let's see if it's uh
and let's see if it's uh
utils
utils
spected do square
root
square okay and then what we're going to
square okay and then what we're going to
do is
multiply
multiply
them so it's going to
them so it's going to
be times conservative
be times conservative
absolute so this will be the new
one La this here as
one La this here as
well now this is dramatically different
well now this is dramatically different
so this might take some tuning and
so this might take some tuning and
such um this is a this is a big
change okay so this looks to be about
change okay so this looks to be about
the same as before
the same as before
frankly which is fine it doesn't break
frankly which is fine it doesn't break
stuff
we've already gotten the good runs out
we've already gotten the good runs out
of
this see 91 items so about halfway
through oh actually this is
through oh actually this is
substantially better you see look at all
substantially better you see look at all
this garbage down here so this is
this garbage down here so this is
actually substantially more
actually substantially more
stable uh than the other one
so the conservative bound actually does
so the conservative bound actually does
do something that's
do something that's
nice I did a
nice I did a
math cool
um let's kill
um let's kill
this and now we're going to see if this
runs
runs
oops ah
MTI let's see if this does anything
we're going to have to monitor several
we're going to have to monitor several
different metrics
different metrics
here might be too conservative might be
here might be too conservative might be
too aggressive um this is a big change
too aggressive um this is a big change
to the acquisition function so we will
to the acquisition function so we will
see
big
change we still have not tuned scales
change we still have not tuned scales
fully uh and a few other
things but I think that this is the
things but I think that this is the
biggest
biggest
spot uh for improvement
spot uh for improvement
here let me think if what I've done
here let me think if what I've done
makes
makes
sense
sense
so initially
they had this
they had this
soft this soft trust region on top of a
soft this soft trust region on top of a
normal that was unintuitively
normal that was unintuitively
constraining your search range so that
constraining your search range so that
you could not find parameters far enough
you could not find parameters far enough
from your starting point it was really
from your starting point it was really
biasing you towards the samples that
biasing you towards the samples that
were basically identical to the runs
were basically identical to the runs
that you've already done we got of that
that you've already done we got of that
um so now we have more aggressive
exploration
exploration
before they were biasing
before they were biasing
towards higher cost
towards higher cost
explicitly by adding like a log
explicitly by adding like a log
weighting
weighting
term uh to the cost so or to the
term uh to the cost so or to the
acquisition
acquisition
function now instead we bias the
function now instead we bias the
acquisition function by a conservative
acquisition function by a conservative
estimate of your absolute performance
estimate of your absolute performance
which is a thing we care about not cost
which is a thing we care about not cost
right so if cost and absolute
right so if cost and absolute
performance are correlated this should
performance are correlated this should
be fairly similar but this should uh
be fairly similar but this should uh
this should prevent you from doing long
this should prevent you from doing long
runs that don't really do all that
well let's see we also made the uh
well let's see we also made the uh
expected Improvement itself
expected Improvement itself
conservative so this accounts for the
conservative so this accounts for the
variance in the model
variance in the model
so the goal of all of this is that we
so the goal of all of this is that we
should
should
get we should get much
get we should get much
cleaner uh much cleaner curves with
cleaner uh much cleaner curves with
fewer points way off the parito
fewer points way off the parito
front that is the goal of
this things to change potentially
this things to change potentially
still we're still using normal
still we're still using normal
sampling I would like to compare to
sampling I would like to compare to
uniform
uniform
sampling because Normal sampling doesn't
sampling because Normal sampling doesn't
um it can get you out of the support
um it can get you out of the support
range we can also compare normal
range we can also compare normal
sampling to or we can compare that to
sampling to or we can compare that to
clipped normal sampling as well we could
clipped normal sampling as well we could
just
clip
e e
I don't think that there's anything C to
I don't think that there's anything C to
think think about I think this is just
empirical
yeah I think this is correct as well
I think we'll try that
next
next
oh wa 37
oh wa 37
samples is this comparable to before is
samples is this comparable to before is
this better
it's a little
it's a little
better we'll see if it keeps it
up it is nice to
up it is nice to
make you know well motivated changes to
make you know well motivated changes to
an algorithm and actually see it do
an algorithm and actually see it do
something it's pretty
good let's see so it's got the batch
good let's see so it's got the batch
size correct already it's got many
size correct already it's got many
batches
correct I don't know why it does this
correct I don't know why it does this
it's pushed learning rate way too far
down and I actually I don't know
why gamma it's hovered around 99 that's
why gamma it's hovered around 99 that's
fine Lambda it's pushed too high as well
fine Lambda it's pushed too high as well
I don't know why it pushes Lambda this
I don't know why it pushes Lambda this
high
high
there really shouldn't be any reason for
there really shouldn't be any reason for
it to do
this I mean the model must just be
this I mean the model must just be
learning a false correlation at the
learning a false correlation at the
start we'll try um we'll try the uniform
start we'll try um we'll try the uniform
sampling next and see if that changes
sampling next and see if that changes
anything the update epox is very nice
anything the update epox is very nice
BPT Horizon is perfect I'm surprised it
BPT Horizon is perfect I'm surprised it
hasn't even sampled anything outside of
hasn't even sampled anything outside of
this now Ms is now correct as well and
this now Ms is now correct as well and
it keeps a nice low cost
it keeps a nice low cost
and here's our Paro
front this is very nice now the question
front this is very nice now the question
is going to be whether it
is going to be whether it
maintains like a sharp
maintains like a sharp
curve or if it
curve or if it
uh if it still meanders around too much
uh if it still meanders around too much
the shape of this curve is going to
the shape of this curve is going to
matter a
lot this is cool though like these are
lot this is cool though like these are
like good well motivated changes to
like good well motivated changes to
carbs
those are very good well motivated
those are very good well motivated
changes to
cars we're going to see what this does
cars we're going to see what this does
when uh when we throw it on a real
when uh when we throw it on a real
environment
h
full DM
oo okay so this is the
oo okay so this is the
2000 this happened
before actually yeah let's just do both
before actually yeah let's just do both
the curves why not um so this is what it
the curves why not um so this is what it
was
was
before and this is what it is now that
before and this is what it is now that
is definitely that is a
is definitely that is a
steeper that's a steeper curve with
steeper that's a steeper curve with
fewer points
now it hasn't gotten like this big jump
now it hasn't gotten like this big jump
yet um but this is this is very
good very good
indeed I'm pretty happy with this
indeed I'm pretty happy with this
overall
yeah that's
great man this is very nice to see you
great man this is very nice to see you
know this is like a very
know this is like a very
intentional uh you know theoretically
intentional uh you know theoretically
motivated change to
motivated change to
carbs and at least on this synthetic
carbs and at least on this synthetic
test it's doing exactly what I think it
test it's doing exactly what I think it
should be doing now I mean we'll see on
should be doing now I mean we'll see on
real environments when there's a whole
real environments when there's a whole
bunch of noise involved uh that's the
bunch of noise involved uh that's the
main difference I guess right the
main difference I guess right the
relationships between hyper parameters
relationships between hyper parameters
aren't perfect and there's a bunch of
aren't perfect and there's a bunch of
noise involved
noise involved
um so we'll see if this
um so we'll see if this
transfers
transfers
but I mean they really can't have done
but I mean they really can't have done
that many experiments on the original
that many experiments on the original
cars because proen is just stupidly
cars because proen is just stupidly
slow so even though they have a bunch of
slow so even though they have a bunch of
gpus like they're literally trading at
gpus like they're literally trading at
less than 1% of the speed that we are on
less than 1% of the speed that we are on
most of our environments you know so
most of our environments you know so
they're like they really can't do very
they're like they really can't do very
much with that and then large language
much with that and then large language
model experiments are going to be crazy
model experiments are going to be crazy
slow so
like I don't know I'd honestly believe
like I don't know I'd honestly believe
that I fiddled with this as much as they
that I fiddled with this as much as they
have they didn't mention any synthetic
have they didn't mention any synthetic
tests either so I don't even think that
tests either so I don't even think that
they did like the rapid development like
they did like the rapid development like
this I think that they I mean this isn't
this I think that they I mean this isn't
even that rapid to be fair
even that rapid to be fair
it's carbs is kind of slow but like
it's carbs is kind of slow but like
still
yeah this is solid holy the question is
yeah this is solid holy the question is
where is uh going to be the end behavior
where is uh going to be the end behavior
so total time steps increasing here is
so total time steps increasing here is
very
good uh batch size just the right part
good uh batch size just the right part
of the graph here batch size is set mini
of the graph here batch size is set mini
batches is
set I don't know what it's doing with
set I don't know what it's doing with
this with the learning rate where it
this with the learning rate where it
like it goes too low
I have an absolute value on this it
I have an absolute value on this it
really should figure this out we'll try
really should figure this out we'll try
the different sample bounds and see if
the different sample bounds and see if
that changes anything so gamma
here gamma here is pretty
here gamma here is pretty
good 99
good 99
okay and Lambda it's starting to figure
okay and Lambda it's starting to figure
out Lambda but like I don't know why it
out Lambda but like I don't know why it
goes up here in the first place
this seems very
this seems very
odd like this is just this is drift this
odd like this is just this is drift this
is like individual hyperparameter Drift
is like individual hyperparameter Drift
But the thing is these are fully
But the thing is these are fully
independent it should be trivial for
independent it should be trivial for
this to figure out the value of Lambda
this to figure out the value of Lambda
which is should be
0.95 update EPO is correct BPT Horizon
0.95 update EPO is correct BPT Horizon
is correct num Ms is correct and then
is correct num Ms is correct and then
cost is nicely scaling and then the Peri
cost is nicely scaling and then the Peri
well this per front is a mess because
well this per front is a mess because
this has two experiments worth the runs
this has two experiments worth the runs
on it
um okay it's starting to level off a
um okay it's starting to level off a
little
bit but
bit but
the yeah it needs total time steps in
the yeah it needs total time steps in
order to max out the score is the main
order to max out the score is the main
thing and with the current the current
thing and with the current the current
version of this uh it will not max out
version of this uh it will not max out
current time steps until it's confident
current time steps until it's confident
at least it
at least it
shouldn't that's actually good though
shouldn't that's actually good though
because if it takes you know this curve
because if it takes you know this curve
up here is actually not as good like
up here is actually not as good like
this is too many experiments that are
this is too many experiments that are
expensive that aren't really doing very
expensive that aren't really doing very
much we would rather have more
much we would rather have more
experiments early that are quick and
experiments early that are quick and
then consistently get the high
then consistently get the high
performance later on so we'll see if
performance later on so we'll see if
that happens this one looks
good oh wow look at that you see so it
good oh wow look at that you see so it
didn't up the cost and it's still
didn't up the cost and it's still
finding it's still finding better
finding it's still finding better
parameters at the current
parameters at the current
cost so this is good because now we're
cost so this is good because now we're
no longer the previous version was
no longer the previous version was
biasing towards high cost they literally
biasing towards high cost they literally
had a bias term to do this but now we
had a bias term to do this but now we
have a bias turn towards better
have a bias turn towards better
performance not higher cost and you only
performance not higher cost and you only
get uh better perform you only get
get uh better perform you only get
higher cost when that's the best way to
higher cost when that's the best way to
improve so this is like I mean this is
improve so this is like I mean this is
the cleanest hyper pram sweep I've ever
the cleanest hyper pram sweep I've ever
seen so far I'm sure that it's going to
seen so far I'm sure that it's going to
like pit her out a little bit at some
like pit her out a little bit at some
point but we're going to see if it gets
point but we're going to see if it gets
closer to task
solve main problem at the moment is the
solve main problem at the moment is the
drift I do not understand this
drift well Lambda is fixed
drift well Lambda is fixed
here
for
e
e e
total
performance predicted performance
performance predicted performance
landscape
it's kind of hard to
it's kind of hard to
check I'm going to try the different
check I'm going to try the different
sampling thing before I do this
because it could technically be from the
because it could technically be from the
normal
normal
distribution normal distribution will it
distribution normal distribution will it
tend to keep parameters pretty
tend to keep parameters pretty
close maybe the not just not enough data
close maybe the not just not enough data
diversity in the sampling I don't
know this is very consistent holy
know this is very consistent holy
hell this is very consistent holy hell
hell this is very consistent holy hell
look at
this only thing you could possibly ask
this only thing you could possibly ask
for here is that it's
for here is that it's
quicker you know 109 trials for uh for
quicker you know 109 trials for uh for
this
but look at the total time steps holy
but look at the total time steps holy
look at
look at
this it's actually
this it's actually
keeping it's getting this perf with this
keeping it's getting this perf with this
low times step so I'm actually I'm
low times step so I'm actually I'm
curious to see the end behavior of
curious to see the end behavior of
this let me
see okay so it's pushing it's pushing I
see okay so it's pushing it's pushing I
think it's pushing the learning back up
think it's pushing the learning back up
slowly gamma's now
slowly gamma's now
correct Lambda has corrected update EPO
correct Lambda has corrected update EPO
is perfect this is perfect this is
perfect so what does it have left to
perfect so what does it have left to
fix I think just learning rate is off
fix I think just learning rate is off
now
it does is it going back
up I don't have this wrong
right look at cost over
right look at cost over
time look at
that I mean this is so much better
I'm just going to while this finishes
I'm just going to while this finishes
I'm going to answer a DM over on the
side
e
e
e
e
e
e e
we
we
DM it's very nice
this is the cleanest freaking hyper
this is the cleanest freaking hyper
parameter sweep I've ever seeing
this is pretty nice
this is crazy
this is crazy
clean for a hyper pram sweep this is
clean for a hyper pram sweep this is
insanely
insanely
clean so a little concerned with the
clean so a little concerned with the
learning
learning
rate it's kind of stuck here
than
I mean the only way that this thing
I mean the only way that this thing
improves at this point is to up total
improves at this point is to up total
time steps right
it's a little scared to
like it's kind of a little
like it's kind of a little
scared to uh to increase total time
scared to uh to increase total time
steps it should have the model
steps it should have the model
correlated by
correlated by
now let's see what it
does for
okay you know it is kind of doing it
I still think this can be made
better this thing should be able to be
better this thing should be able to be
very aggressive right
now hang on maybe this is me being
now hang on maybe this is me being
stupid wait let me look at this
because Global search
okay I got a global surf scale of
0.5
0.5
and yeah total time steps
and yeah total time steps
of12
of12
0.25 so total and then well let me see
0.25 so total and then well let me see
so
like8 we're basically we got like 1.5
like8 we're basically we got like 1.5
is so8 to10 is probably going to be I'm
is so8 to10 is probably going to be I'm
at like netive .5 to start and then the
at like netive .5 to start and then the
max is one so you got like
max is one so you got like
1.5 and you're sampling with a scale of
1.5 and you're sampling with a scale of
.125 okay so like it's going to take
.125 okay so like it's going to take
you it's going to take you a while to
you it's going to take you a while to
get to the upper end of that range like
get to the upper end of that range like
it's going to take you at
it's going to take you at
least 15
least 15
runs and you're not necessarily going to
runs and you're not necessarily going to
sample something all the way at the edge
sample something all the way at the edge
of that scale
either so this probably just needs to
either so this probably just needs to
get increased
there might still be some other
there might still be some other
improvements we can make though hold on
improvements we can make though hold on
because once we increase
because once we increase
this we don't wanted to sampling really
this we don't wanted to sampling really
high time Steps From the get-go
yeah this is continuing to improve but
yeah this is continuing to improve but
it's
slow what
next conservative
let me think about
this we're waiting towards higher
this we're waiting towards higher
absolute
performance we're penalizing this by
performance we're penalizing this by
uncertainty
uncertainty
of the performance
of the performance
estimates but we're not incorporating
estimates but we're not incorporating
cost in any way
right hang on and what way is this thing
right hang on and what way is this thing
incorporating
cost I don't think it is
right so we have GPC
with the expected
cost we can use the Paro value y
PF I mean we're barely using that one
is this better than just using the
is this better than just using the
actual Pito point
wait you could literally just use the
wait you could literally just use the
parito point here couldn't
parito point here couldn't
you I mean I guess yeah technically you
you I mean I guess yeah technically you
know your suggestion can be higher
cost so maybe this is better
I kind of want to see how good this
I kind of want to see how good this
parito function
parito function
is you
know cuz if this thing sucks like you're
know cuz if this thing sucks like you're
going to be better off just using the
going to be better off just using the
parito
point okay that's like a relatively
point okay that's like a relatively
small
small
thing
thing
though the main thing is not taking into
though the main thing is not taking into
account
account
cost like this is uniform waiting across
cost like this is uniform waiting across
the Paro front
right this is biased to high
cost this really isn't cost to wear in
cost this really isn't cost to wear in
the way that you would think about
it
like okay so this is good but it's not
like okay so this is good but it's not
intelligently using
intelligently using
cost in order to
cost in order to
like maximize the results The Sweep
like maximize the results The Sweep
result over like the result over the
result over like the result over the
time of the whole sweep
time of the whole sweep
it's really
it's really
not I'm trying to think how you would do
that
e
e
e
e
e e
damnn
it this is
tricky e
Dynamic cost
penalty let's see
yeah this is very very
yeah this is very very
good it could be faster right but what
good it could be faster right but what
it's doing is very
it's doing is very
good is it figured out the learning rate
good is it figured out the learning rate
thing at
thing at
all that one's kind of
all that one's kind of
screwy n not really missed one parameter
let me see
let me see
so decreases as model
cost I don't want to
penalize I don't really want to penalize
penalize I don't really want to penalize
low cost or high cost runs even more
low cost or high cost runs even more
than they already are
we could have an explicit exploration
we could have an explicit exploration
budget that
budget that
favors using more cheap experiments
early I mean you should get explicitly
early I mean you should get explicitly
penalized based on the cost is the thing
penalized based on the cost is the thing
like that is the correct thing to do
information value per
information value per
compute the EI function already does
that
e
e e
this is difficult because it's clearly
this is difficult because it's clearly
clearly the case that you want to
clearly the case that you want to
penalize linearly for
cost what in the hell is this so we got
cost what in the hell is this so we got
base
acquisition what the is that
that's like the stupidest thing ever oh
that's like the stupidest thing ever oh
s
s
AGI
AGI
no literally gave me a formula with cost
no literally gave me a formula with cost
cancelling
out yeah so this is what we this is what
out yeah so this is what we this is what
it should
be should just be divided by
be should just be divided by
cost it should be divid by cost
Improvement times absolute
Improvement times absolute
performance over
cost let me think about when you're
cost let me think about when you're
actually going to get this
this doesn't capture the right thing
Improvement wait if we have an optimal
Improvement wait if we have an optimal
parito front what
parito front what
happens hey
happens hey
welcome we are currently
welcome we are currently
optimizing the heck out of carbs and
optimizing the heck out of carbs and
fixing hyperparameter tuning in
fixing hyperparameter tuning in
RL it's going quite well was breaking my
brain this is the cleanest hyper
brain this is the cleanest hyper
parameter speep I've ever
seen yeah it doesn't 200 experiments
seen yeah it doesn't 200 experiments
though and it doesn't upweight total
though and it doesn't upweight total
time steps enough
time steps enough
um let me just run while I since I'm
um let me just run while I since I'm
doing this let me
do I'm trying to
think wait no
okay
so sample
so sample
around origins in
around origins in
basic this one here
basic this one here
right search just distribution in basic.
Sample normal okay
so what if we do
so what if we do
this we do
uniform and then I
uniform and then I
think do we have to up with this
we'll try this initially to see just
we'll try this initially to see just
what happens with the same search scale
what happens with the same search scale
and everything
see if this
works okay
solid well we will compare this in a
moment there's going to be more work to
moment there's going to be more work to
do I'm going use a restro from real
do I'm going use a restro from real
quick be right
back
e e
we'll see how this does by comparison
we'll see how this does by comparison
with The Uniform sampling
I actually don't know if it's going to
I actually don't know if it's going to
be any better
be any better
because
because
um the implicit trust region that we had
um the implicit trust region that we had
is actually kind of pretty good
already like us uh we're already
already like us uh we're already
modulating by variant so this might just
modulating by variant so this might just
be worse
I actually don't know if you think about
I actually don't know if you think about
it right with hyperparameter
it right with hyperparameter
optimization do you think any webchain
optimization do you think any webchain
blockchain GPU stuff will be interest no
blockchain GPU stuff will be interest no
not at all blockchain maybe web 3 is a
scam um yeah not really at
all blockchain Tech is legit but I just
all blockchain Tech is legit but I just
there's not really
there's not really
like there really aren't that many uses
like there really aren't that many uses
for it if you think about
it okay
how do I get two
windows let me see
here I like how NYU schedules meetings
here I like how NYU schedules meetings
on Sunday it's
on Sunday it's
funny enable a
funny enable a
Marketplace like why what does
Marketplace like why what does
blockchain add to that right
like in what way does it add
anything that's the thing like it's like
anything that's the thing like it's like
oh what if we do block like it doesn't
oh what if we do block like it doesn't
like you can just do that without
like you can just do that without
blockchain right like it doesn't make
blockchain right like it doesn't make
sense I mean yeah you can add a crypto
sense I mean yeah you can add a crypto
payment option or whatever but that
payment option or whatever but that
doesn't mean that the like the whole
doesn't mean that the like the whole
rest of it has to be on blockchain
rest of it has to be on blockchain
that's just a payment option
so it really doesn't do very
much I don't even know if you can
much I don't even know if you can
legally do
a what is
this I don't really want to click
this I don't really want to click
that bit tenser
that bit tenser
okay I'll Google
it guide emissions I don't know what
it guide emissions I don't know what
that
is I I don't know what that
is is this way worse than
is is this way worse than
before I think this might be way worse
before I think this might be way worse
than before we'll
than before we'll
see that'd be interesting if it is way
see that'd be interesting if it is way
worse
I want to think about this
here let's say that you have an optimal
here let's say that you have an optimal
Paro
Paro
front different categories of sub
networks a lot of these things you don't
networks a lot of these things you don't
understand how they work because there's
understand how they work because there's
nothing to understand it just doesn't
nothing to understand it just doesn't
make sense don't let me yeah I mean the
make sense don't let me yeah I mean the
stuff we're doing at the moment is
stuff we're doing at the moment is
really freaking cool
really freaking cool
um I'm a little tired cuz it's the math
um I'm a little tired cuz it's the math
is breaking my freaking head but this is
is breaking my freaking head but this is
carbs oops this is carbs this is a very
carbs oops this is carbs this is a very
complicated basian optimization
complicated basian optimization
algorithm for hyperparameter
algorithm for hyperparameter
search um and I'm making it like
search um and I'm making it like
dramatically dramatically better so if
dramatically dramatically better so if
if these results if I can finish this
if these results if I can finish this
and these transfer well to from the
and these transfer well to from the
synthetic tasks to real tasks this will
synthetic tasks to real tasks this will
like overnight be a severalfold
like overnight be a severalfold
Improvement just across the board to all
Improvement just across the board to all
of
RL because this will let you I mean this
RL because this will let you I mean this
will just let you get ridiculously good
will just let you get ridiculously good
results out of your
results out of your
experiments and to be fair
experiments and to be fair
like this
like this
is so it's possible that I've overfit
is so it's possible that I've overfit
the task right it's possible that I've
the task right it's possible that I've
overfit the task but so
overfit the task but so
far um this is the before
no not this one this is after I've
no not this one this is after I've
already screwed with
it okay so this is the
it okay so this is the
before I think I rescaled the task since
before I think I rescaled the task since
then but this is like a third of the
then but this is like a third of the
overall possible
performance and then
now this is the cleanest hyperparameter
now this is the cleanest hyperparameter
sweep I've ever seen and this is like
sweep I've ever seen and this is like
90% performance
so we're making crazy progress
so we're making crazy progress
here okay this modification seems to not
here okay this modification seems to not
be particularly good uh it's still clean
be particularly good uh it's still clean
but it's way
slower maybe we'll give it a little
longer do you need to change the search
longer do you need to change the search
range parameter for normal versus
uniform 30% of data Falls outside
oh it gave me two different answers
LOL I think it is 61% right
68% Falls within one standard
deviation my probability
deviation my probability
sucks my linear algebra is pretty good
sucks my linear algebra is pretty good
but my stats and probability sucks
so 40% of data Falls so maybe you just
so 40% of data Falls so maybe you just
need to
upscale maybe you just need to upscale
upscale maybe you just need to upscale
the
the
parameter now I really don't know
parameter now I really don't know
intuitively though which of these should
intuitively though which of these should
be better
be better
so you're generating a th samples around
so you're generating a th samples around
a Paro optimal
a Paro optimal
point intuitively would you rather do
point intuitively would you rather do
uniform sampling around the point or
uniform sampling around the point or
normal sampling normal sampling is going
normal sampling normal sampling is going
to get you more points where several of
to get you more points where several of
the parameters are close to their
the parameters are close to their
existing values and like a few
existing values and like a few
parameters are different so intuitively
parameters are different so intuitively
I mean that would be better for like
I mean that would be better for like
tuning the last few stubborn parameters
probably whereas with uniform
probably whereas with uniform
sampling you get more diverse data
sampling you get more diverse data
overall but like probably a lot of the
overall but like probably a lot of the
parameters are going to be far from
parameters are going to be far from
their initial settings I think that
their initial settings I think that
normal normal might just be the smarter
normal normal might just be the smarter
Choice
here let's see how it's fitting these
here let's see how it's fitting these
parameters
so gamma's
so gamma's
good you know it is kind of fitting
good you know it is kind of fitting
these better
these better
though oh well this is stuck
okay so a lot of these parameters are
stuck maybe you have to
upweight so this is just too
slow I'm going to try I'm going to kill
slow I'm going to try I'm going to kill
this and I'm going to try to
double yeah I'm going try to double
this and if this doesn't work then I'm
this and if this doesn't work then I'm
going to go back to normal because I'm
going to go back to normal because I'm
I'm going to
assume I'm going to assume that
assume I'm going to assume that
basically the
basically the
uh the normal is better because I if I
uh the normal is better because I if I
double the standard deviation and it's
double the standard deviation and it's
still
still
screwy this should be way stable as well
screwy this should be way stable as well
we'll see it it's kind of nice cuz it's
we'll see it it's kind of nice cuz it's
like it's clipped for
you but I think I have I I think
you but I think I have I I think
implicitly I already have a good
implicitly I already have a good
mechanism of
clipping we're using a conservative
clipping we're using a conservative
estimate based on the variance of the
estimate based on the variance of the
gaussian
gaussian
process so that should prevent you from
process so that should prevent you from
sampling way out their hyper parameters
sampling way out their hyper parameters
that you don't know what they're going
that you don't know what they're going
to do should I haven't actually checked
to do should I haven't actually checked
how good the gaussian processes
are uh hold on is this immediately
better these are just random
better these are just random
samples so the random samples are better
samples so the random samples are better
we'll see if this translates to anything
we'll see if this translates to anything
I'm only going to give this like maybe
I'm only going to give this like maybe
50 or so samples to do
50 or so samples to do
something I'm getting hungry as
well okay this is actually now suddenly
that made a huge
difference to be fair I introduced that
difference to be fair I introduced that
scaling
scaling
term
term
um I introduced that scaling term before
um I introduced that scaling term before
I changed the acquisition function so
I changed the acquisition function so
now we have a way better acquisition
now we have a way better acquisition
function so maybe we get away
function so maybe we get away
with larger sample
with larger sample
ranges I'm GNA have to try this with
ranges I'm GNA have to try this with
gaussian as well
is there any reason I can't run these
is there any reason I can't run these
both at the same
time I think I should be able to
right
right
oh hold
oh hold
on cannot get random
sample that's interesting
that's very
interesting that was doing better as
interesting that was doing better as
well
no candidates
found sample search space
sample oh I see
two to
two to
the numb dims
the numb dims
with
bounce two to the
5ifth me figure this out
M24 so this is generating a ton of
M24 so this is generating a ton of
samples
get masth for invalid points in basic
huh I mean that's
interesting so I think I know what's
interesting so I think I know what's
happening here I
happening here I
think
think
um though it's pretty surprising
so if you have um too big of a search
range then it's very likely that at
range then it's very likely that at
least one of your parameters is going to
least one of your parameters is going to
be out of bounds
get mask for invalid points in
basic self. Min bounds in basic
okay can I just clip
this instead of
masking samples in
basic e
it these seem valid though
so these are all
valid it's a bit
valid it's a bit
weird cuz you're sampling are
round in
basic I mean this looks like this should
basic I mean this looks like this should
go out of
go out of
bounds to
me search distribution and
me search distribution and
basic there's no clipping on
this self.
surf wait uniform how the did this
surf wait uniform how the did this
happen
no
wait uh-uh
is just just have a weird print on it
okay yeah so this is correct just has a
okay yeah so this is correct just has a
weird print string on
it oh I'm stupid that's why so this
it oh I'm stupid that's why so this
is but wait a second this is
plus yeah cuz nothing starts at one okay
plus yeah cuz nothing starts at one okay
cool so this has got to be negative
cool so this has got to be negative
config
Global so maybe this uniform is not that
Global so maybe this uniform is not that
bad maybe I just screwed up you know
we're going to do this for a
we're going to do this for a
bit cuz I don't think you should be uh
bit cuz I don't think you should be uh
masking
these okay valid sample
these okay valid sample
mask okay so there's some
mask okay so there's some
false valid
size
size
shape um
valid okay look there are a lot of
valid okay look there are a lot of
invalid samples in there so I think that
invalid samples in there so I think that
what would be better is we
what would be better is we
do this in
do this in
basic.
basic.
clamp and this is not it it should
be in
men bounds in
men bounds in
basic men
and now we no longer have this
and hopefully this works
okay
cool we're we'll see if this is too
cool we're we'll see if this is too
ridiculous um
we'll do
this so I changed this I don't know why
this so I changed this I don't know why
they do this
they do this
um because pretty
um because pretty
much let's say that you sample a point
much let's say that you sample a point
that's like really close to a
that's like really close to a
boundary it's a pretty good chance to
boundary it's a pretty good chance to
just generate all invalid samples
just generate all invalid samples
uh but you should just clamp and then
uh but you should just clamp and then
you never generate invalid
samples at least that's what should
samples at least that's what should
happen I mean we're like we're very very
happen I mean we're like we're very very
quickly improving this thing
quickly improving this thing
here so let's see so this is the next
run we'll see how this compares to you
run we'll see how this compares to you
to uh to normal if it's too noisy I'll
to uh to normal if it's too noisy I'll
crank the standard deviation back down
crank the standard deviation back down
since I had it wrong
before dinner in about an
before dinner in about an
hour and
hour and
then how ever many hours I've got left
then how ever many hours I've got left
in me on this
in me on this
tonight this is exciting though this is
tonight this is exciting though this is
very very good
very very good
progress very good
progress I realistically I think we're
progress I realistically I think we're
only a few changes away from having this
only a few changes away from having this
algorithm solid uh there are a few
algorithm solid uh there are a few
things I'm not fully happy with
things I'm not fully happy with
yet
yet
so the algorithm right now need needs to
so the algorithm right now need needs to
be biased to Shorter runs but it needs
be biased to Shorter runs but it needs
to have a mechanism to quickly scale to
to have a mechanism to quickly scale to
longer runs right now it's not biased to
longer runs right now it's not biased to
Shorter runs but for some reason tends
Shorter runs but for some reason tends
to do them anyways and takes forever to
to do them anyways and takes forever to
do longer
runs this can be improved
maybe too noisy we'll
see intuitively should
see intuitively should
you if you switch distributions from
you if you switch distributions from
from normal to uniform for sampling do
from normal to uniform for sampling do
you need to change the
you need to change the
size if you're sampling with a standard
size if you're sampling with a standard
deviation of 0.5
with a normal
with a normal
distribution you just change that to a
distribution you just change that to a
uniform with
0.5 I
0.5 I
mean like 32% of your data from the
mean like 32% of your data from the
normal is going to fall
normal is going to fall
outside of those
outside of those
bounds but a lot of your points are
bounds but a lot of your points are
going to be closer to the center whereas
going to be closer to the center whereas
with the
with the
uniform you're going to end up with more
uniform you're going to end up with more
points not near the center but then
points not near the center but then
you're hard clip to
0.5 I don't
0.5 I don't
know I really don't
know I really don't
know we'll just have to do this
know we'll just have to do this
empirically which is slightly annoying
empirically which is slightly annoying
because
because
um I mean Neptune takes a couple seconds
um I mean Neptune takes a couple seconds
to load and then carbs takes a few
to load and then carbs takes a few
seconds per point which is annoying but
seconds per point which is annoying but
whatever
better than doing full scale
experiments let's load this up versus
experiments let's load this up versus
the other
the other
one so this is what we had
one so this is what we had
here I can't there're too many
points I'm pretty damn sure this is
points I'm pretty damn sure this is
worse
[Music]
maybe
maybe
not I think you have to give it like
not I think you have to give it like
what 40 points or something let's see
what 40 points or something let's see
let's just eyeball
let's just eyeball
this so around here is 50
this so around here is 50
points takes around 50 points to get to
points takes around 50 points to get to
2,000
2,000
nice and stable
here could be on
track hard to
say these samples are pretty darn noisy
though pretty darn noisy
see we would like these to be much more
consistent this could possibly be me
consistent this could possibly be me
cranking the standard deviation too high
cranking the standard deviation too high
though we'll give it 50 points and then
though we'll give it 50 points and then
I'll try again with uh half the standard
I'll try again with uh half the standard
deviation
unless this magically improves very soon
unless this magically improves very soon
I think this is just
worse I'll make the change preemptively
worse I'll make the change preemptively
and then we'll wait until uh we'll wait
and then we'll wait until uh we'll wait
until 50 to actually rerun
it so this
it so this
one was at 0.5
also this exploration bias we got look
also this exploration bias we got look
at as well
I mean this kind of
works it's noisier than before
though definitely
though definitely
noisier so
like here let's just look at
like here let's just look at
what what this gave us right so this is
what what this gave us right so this is
about half and this is about half a half
about half and this is about half a half
so we're going to just feel like
this this was a very
this this was a very
smooth up to 2,000 right very very
smooth yeah this is
smooth yeah this is
worse we can kill this and then we can
worse we can kill this and then we can
try this with
try this with
uh we can try this with 0.5
this should be the closer
this should be the closer
comparison if this isn't
comparison if this isn't
better then I think normal is just
better and we can tune the normal I
better and we can tune the normal I
guess normal intuitively
guess normal intuitively
like normal is worse if you don't have
like normal is worse if you don't have
some sort of trust region if you don't
some sort of trust region if you don't
have some sort of mechanism for
have some sort of mechanism for
rejecting
rejecting
crazy outliers but we have that so
crazy outliers but we have that so
normal should give you more points that
normal should give you more points that
like change just a few
variables it's kind of close to a local
search is possibly
better so I do
better so I do
wonder if we look at
this now see the learning rate is still
this now see the learning rate is still
screwy even with
screwy even with
this and Lambda is still drifted with
this and Lambda is still drifted with
this so the uniform does not solve this
this so the uniform does not solve this
weird drift
weird drift
issue um I thought it would but it does
not which is very weird
okay
so we'll see if this does better if not
so we'll see if this does better if not
I think we just stick to
normal it's very weird how much worse
normal it's very weird how much worse
this is It's like a lot
this is It's like a lot
worse this is just one synthetic problem
worse this is just one synthetic problem
problem but still
it's a
lot how is it this bad
it's kind of impressive like what a
it's kind of impressive like what a
difference that makes I didn't screw it
difference that makes I didn't screw it
up right
like
like
[Music]
[Music]
you no this is correct
makes a huge difference
holy there's probably a ton of value in
holy there's probably a ton of value in
just getting the scales right for
just getting the scales right for
everything as
well we'll definitely have to look at
that I wouldn't be surprised as well
that I wouldn't be surprised as well
that this synthetic tuning it on this
that this synthetic tuning it on this
is pretty representative of a lot of
is pretty representative of a lot of
problems cuz like the parameters are
problems cuz like the parameters are
pretty close and
all I don't know how it's this much
all I don't know how it's this much
worse it's kind of crazy
I mean it's not like it's it's failing
I mean it's not like it's it's failing
it's definitely working it's just
um what's this doing it should be going
um what's this doing it should be going
down
down
to1 okay gamma's
to1 okay gamma's
good this actually looks
better these haven't been sampled at all
possibly the search range is too small
interesting it's missing a lot of the
interesting it's missing a lot of the
scale
PRS I get stuck on all of
these the normal just kind of worked
okay so let me think now going
forward I'm going to think about this in
forward I'm going to think about this in
the
the
[Music]
[Music]
meantime we want to downweight by clost
meantime we want to downweight by clost
we want to penalize you for running high
we want to penalize you for running high
cost experiments for no
cost experiments for no
reason what happens if your parito front
reason what happens if your parito front
is
perfect so if you have a perfect Paro
perfect so if you have a perfect Paro
front
front
the only way to
the only way to
improve is
by the only way to improve is by running
by the only way to improve is by running
longer
longer
experiments oh wait their formula
sucked who wait this is wrong this is
sucked who wait this is wrong this is
just wrong look I think I found another
just wrong look I think I found another
thing damn it so look they use this
thing damn it so look they use this
whole extra gaan process for estimating
whole extra gaan process for estimating
cost okay and then whatever point you
cost okay and then whatever point you
sample you
sample you
estimate the cost of this point and then
estimate the cost of this point and then
you take the Paro value here okay and
you take the Paro value here okay and
you estimate the cost of this parito
point so what you're doing is you're
point so what you're doing is you're
estimating the difference between the
estimating the difference between the
point that you've selected and your
point that you've selected and your
estimate of the optimal point that costs
estimate of the optimal point that costs
that much and then you're taking re
that much and then you're taking re
which to be fair I didn't see re in the
which to be fair I didn't see re in the
code but then what happens here is that
code but then what happens here is that
if your Paro front is if you're if you
if your Paro front is if you're if you
have a perfect estimate of parameters in
have a perfect estimate of parameters in
parito front this is always
zero so you can't even
zero so you can't even
improve you're not even allowed to
improve you're not even allowed to
improve by running longer experiments
improve by running longer experiments
under
this you're not even allowed to improve
this you're not even allowed to improve
by running longer experiments because
by running longer experiments because
you're being compared to a hypothetical
you're being compared to a hypothetical
point that you haven't even obtained
yet that's terrible
okay let's say that we fixed that
right then if we have a perfect parito
right then if we have a perfect parito
front the only Improvement will come
front the only Improvement will come
from running a longer
experiment so this is going to be zero
experiment so this is going to be zero
for everything that is not a longer
for everything that is not a longer
experiment
and then absolute
and then absolute
performance is going to be like a high
performance is going to be like a high
positive number
positive number
right and then
right and then
cost is going to downweight
you what if there's a little bit of
you what if there's a little bit of
error right what if there's a little bit
error right what if there's a little bit
of room to improve somewhere else on the
of room to improve somewhere else on the
Paro
front your basic Al going to improve
front your basic Al going to improve
everywhere first aren't
you you're going to improve everywhere
you you're going to improve everywhere
on the parito
on the parito
front before you improve at the end of
front before you improve at the end of
it so that might bias you towards too
it so that might bias you towards too
many short
experiments I mean we're going to have
experiments I mean we're going to have
to run it there're just a lot of
to run it there're just a lot of
ablation run
here okay this is definitely bad so
here okay this is definitely bad so
don't do this do a normal
sample we'll run a few of these
just to make sure that we're good uh and
just to make sure that we're good uh and
then let's replace
then let's replace
the the
estimate with the Paro
Point Let's replace the estimate with
Point Let's replace the estimate with
the Paro point
this will this should at least improve
this will this should at least improve
our synthetic test um we'll see what
our synthetic test um we'll see what
happens
when we will see what happens when you
when we will see what happens when you
have noise in the
estimate how badly that hurts you
okay so this is your parito surrogate
right best moo is equal to Pito
right best moo is equal to Pito
surrogate which is not what we want what
surrogate which is not what we want what
we want is
num to
generate origins in basic is the Paro
generate origins in basic is the Paro
groups
so we going have to look at that let's
so we going have to look at that let's
make sure yeah this is this is going
make sure yeah this is this is going
well you see so just
immediately we get our curve back okay
immediately we get our curve back okay
yeah I didn't break anything we're good
yeah I didn't break anything we're good
here um
so but I need to figure this one
so but I need to figure this one
out replace
out replace
this ironically I don't think we're
this ironically I don't think we're
going to need the cost estimate model
going to need the cost estimate model
are
are
we no we are we're still going to need
we no we are we're still going to need
the cost
the cost
estimate but
separate generate
separate generate
candidate e
why is it never calling generate
why is it never calling generate
candidates
hello get to 15
these random
these random
points oh there we go it's weird it took
points oh there we go it's weird it took
a while I guess it's cuz the the logging
a while I guess it's cuz the the logging
is fun so we have samples in basic
is fun so we have samples in basic
shape
Jesus that's generates way too many
Jesus that's generates way too many
samples
how's this work wait origin
samples uh
sample around Origins basic
wait origin index
categorical
categorical
ah okay so that's going to be
you're going to have to give it origin
index okay
sample search
space doing this in order
to get the
means do you need
this I don't know if you need
this I don't know if you need
this you definitely
need okay and then what you do is
no
no
equal watch
X
okay origin
index Maxs
uh that's
interesting oh for some reason they're
interesting oh for some reason they're
the
the
same uh they're resamples the same
same uh they're resamples the same
parameter
parameter
Okay so
answer X plus
4 e
burrito
grps orito
groups I misspell it or is it just a
stupid all
right let's try
this for
wait
oh okay I think you just have to do a
oh okay I think you just have to do a
zero
let's try this
this
run okay
oh this is incredibly
stupid e
uh this is going to be
fine okay so this is good
expected condenser
device hold on Origins and
basic samples in
basic samples in
basic
for e
they're getting rid of this
garbage e
really
H that's
interesting does torch. clamp not
interesting does torch. clamp not
actually clamp correctly
I guess the transformation is uh a
I guess the transformation is uh a
little
little
off it's not good
though
e e
TR
that and actually let's make that bound
that and actually let's make that bound
a little
tighter okay so now this is running
let's
let's
see how we
do see how we do
and this is going to ask me for a pi key
again I honestly did not expect to be
again I honestly did not expect to be
changing CBS this much today it's kind
changing CBS this much today it's kind
of
insane this is like so much outward mcdb
insane this is like so much outward mcdb
for one day
this is a brand new
algorithm I'm leaning towards neoc
carbs for a name
oops
let's
let's
see how this
does there are two changes
does there are two changes
here there's the masking
here there's the masking
change and there's the Paro Baseline
change we you shall
see it's quite a fair bit of
work e
is this worse than
before let's
[Music]
[Music]
see maybe
are we
stuck we're going to have to test these
stuck we're going to have to test these
two things
two things
separately it could be that the clamping
separately it could be that the clamping
doesn't
doesn't
work clamping should work though
work clamping should work though
I'd be quite
surprised
good let me think if there's any way
good let me think if there's any way
that I could be wrong
that I could be wrong
here I'm pretty sure I checked the math
here I'm pretty sure I checked the math
and it doesn't make sense as
and it doesn't make sense as
is
like this is your expected Improvement
like this is your expected Improvement
versus the Pito front
right if you have a perfect estimate of
right if you have a perfect estimate of
the Paro
the Paro
front then this is zero
everywhere right
everywhere right
even if you crucially even if you don't
even if you crucially even if you don't
have the experiments run yet
right that seems
wrong so I mean if you
if you
if you
use the estimate of the current Point
use the estimate of the current Point
instead then you get a benefit for doing
instead then you get a benefit for doing
a longer
a longer
run or you can get a benefit for doing a
run or you can get a benefit for doing a
longer run because you have a weaker
longer run because you have a weaker
Baseline to improve
Baseline to improve
over so this should help bias you
over so this should help bias you
towards longer runs as well provided
towards longer runs as well provided
that you're confident uh in the
that you're confident uh in the
parameters oh hold on here we
go yeah okay it just it took a
go yeah okay it just it took a
second let's see how the shape
is
is
okay whatever slightly different shape
okay whatever slightly different shape
curve I think what this
curve I think what this
1635 what did we have before
this
one yeah so in 35
one yeah so in 35
is I mean that's
like okay it's actually it's a pretty
like okay it's actually it's a pretty
similar shape
similar shape
initially I just wasn't looking at
initially I just wasn't looking at
it we'll see how that goes this point is
it we'll see how that goes this point is
not good but we'll
see I think it's like 2, 50 or
something I just want this to not hurt
something I just want this to not hurt
you know if it doesn't
help okay there you go so this total
help okay there you go so this total
time steps is a little sketchy we get up
time steps is a little sketchy we get up
to 350 300 something mil
to 350 300 something mil
does that happen in the other
one oh yeah no there's a little bump at
one oh yeah no there's a little bump at
the start okay that's
fine this is such a clean curve
though okay yeah that's good I don't
though okay yeah that's good I don't
know what these points are
doing are these parito
doing are these parito
you have this brown one
here oh yeah so it was going for like a
here oh yeah so it was going for like a
lower cost
lower cost
thing
interesting I don't know if this looks
interesting I don't know if this looks
as
as
good like this Lambda is still drifting
learning rate's about right but we'll
learning rate's about right but we'll
see if that
see if that
drifts other PRS are looking
good I mean this is pretty close to the
good I mean this is pretty close to the
original with a
original with a
few a few outlier points though we're
few a few outlier points though we're
pretty
pretty
close pretty darn close to the
close pretty darn close to the
original I don't like these outlier
original I don't like these outlier
points and I don't know why they're here
points and I don't know why they're here
let me think if there's any good reason
let me think if there's any good reason
for them to be
here well actually hold on yeah they
here well actually hold on yeah they
could
could
be okay so you actually can get stuff
be okay so you actually can get stuff
like
like
this um let me look at where can I find
this um let me look at where can I find
this this is
this this is
AB 2151
okay that is a parito point you
okay that is a parito point you
see so
yeah this is probably it's just getting
yeah this is probably it's just getting
rewarded for
rewarded for
um improving
um improving
over uh a previous parito point that it
sampled so that is actually kind of a
sampled so that is actually kind of a
problem now that I think about it
because you can kind of get pre you can
because you can kind of get pre you can
kind of get infinitely rewarded for
kind of get infinitely rewarded for
running the same experiment can't you
running the same experiment can't you
which would be this
cluster let's see if that happens in
practice I mean this is pretty good it's
practice I mean this is pretty good it's
not as clean though as
before let me think about that
exploit so the issue here is you get
exploit so the issue here is you get
clusters like this
clusters like this
because you get rewarded for running
because you get rewarded for running
something that's like a little higher
something that's like a little higher
cost and does a bit
cost and does a bit
better but you get rewarded repeatedly I
better but you get rewarded repeatedly I
think for the same experiment
okay
e
e
e e
sampled for
son it not that
smart what if we
smart what if we
compared the predicted efficiency
yeah this doesn't make any
sense so
I mean this is still
good
good
but we'll see how this goes by the end I
but we'll see how this goes by the end I
think that you are you're getting
think that you are you're getting
rewarded too much for doing stupid
like if I look at this there's this
like if I look at this there's this
like dense little Cloud here does this
like dense little Cloud here does this
happen with the uh with the 200
happen with the uh with the 200
run I do this
yeah this is like a better spread parito
yeah this is like a better spread parito
front you don't have like a little tiny
front you don't have like a little tiny
cluster
cluster
anywhere this is very very clean and you
anywhere this is very very clean and you
can see there's so few points out of the
can see there's so few points out of the
front by
comparison yeah this front is already
comparison yeah this front is already
worse
I don't know how it gets this
point I mean I guess technically you can
point I mean I guess technically you can
literally just take any point right you
literally just take any point right you
take any parito
take any parito
point like over here even and you just
point like over here even and you just
crank it up and run it for
longer even then I think these are bad
longer even then I think these are bad
samples
it's weird that that matters so much I
it's weird that that matters so much I
mean this makes sense this is literally
mean this makes sense this is literally
just like each of these points is
parito well that could actually be it
parito well that could actually be it
though like just losing that many
though like just losing that many
samples could also just screw up your uh
samples could also just screw up your uh
your prediction functions your gum
processes I mean this is not bad but I
processes I mean this is not bad but I
think it was better before but
think it was better before but
theoretically let me think about this
so you want to know how much
better is how much better your thing is
better is how much better your thing is
than the current optimal on that front
than the current optimal on that front
if you can't improve the
if you can't improve the
front you don't get
anything I don't like that it's improved
anything I don't like that it's improved
the optimal it's rough
what's the original function so if you
what's the original function so if you
take
this it's just the acquisition function
this it's just the acquisition function
originally
right what if I did
plus plus the absolute
performance you have to get the waiting
performance you have to get the waiting
term right as the
term right as the
problem to do
that that adds a waiting hyperparameter
that that adds a waiting hyperparameter
that's very difficult to tune
if you do absolute performance over
cost absolute performance over cost is
cost absolute performance over cost is
going to favor shorter run still is the
problem this is hard
so
like this is what I was looking at
like this is what I was looking at
here the problem is
Improvement first of all this is a
Improvement first of all this is a
conservative bound on
conservative bound on
Improvement
um well hang on I'm no longer clamping
yeah I'm no longer clamping this thing
yeah I'm no longer clamping this thing
right so even if these are all
right so even if these are all
conservative bound
conservative bound
negative you're still going to run the
negative you're still going to run the
best experiment out of all of these
right I think that I need to add a bias
right I think that I need to add a bias
term to this so that this is never
term to this so that this is never
negative though I need to add a bias
negative though I need to add a bias
term so this is negative never
negative cuz otherwise it's going to get
negative cuz otherwise it's going to get
Scaled weird
okay so I think that solves
okay so I think that solves
that as long as I'm not clamping this by
that as long as I'm not clamping this by
mistake somewhere am I clamping this by
mistake somewhere am I clamping this by
mistake
somewhere so here's the function
somewhere so here's the function
so you have the samples this is fine
so you have the samples this is fine
this plamp is
this plamp is
fine and
fine and
then you get surrogate
then you get surrogate
model
output
cast this is mask Max masking is fine
cast this is mask Max masking is fine
expected Improvement okay I think the re
expected Improvement okay I think the re
is not
is not
here at least I don't see a anywhere so
here at least I don't see a anywhere so
I think I got rid of that
successfully I
hope we'll see if that can be
negative so now
let me go get the Pito surget
back and go get the Pito
surget
oops let's go get the Pito so get back
oops let's go get the Pito so get back
where is
it get the
pitoo yeah this one
for e
so we're going to use this burrito
surrogate um and I like this wasn't
terrible this was not terrible I don't
terrible this was not terrible I don't
think
right let me make sure that this is so
right let me make sure that this is so
135 at
135 at
2500 let's just make sure this is not
2500 let's just make sure this is not
somehow better
and know this is about the same we get
and know this is about the same we get
2500 and about 100 points here is that's
2500 and about 100 points here is that's
about the
about the
average and uh time steps go up like
that cost might be lower here hard to
that cost might be lower here hard to
say
Fredo front is Messier
though I think they're pretty similar
though I think they're pretty similar
but I like this is slightly worse I'd
but I like this is slightly worse I'd
say and like
theoretically we just need a bias term
theoretically we just need a bias term
is all for the uh the cost
is all for the uh the cost
hold
on okay I got to go for dinner
on okay I got to go for dinner
um I'd like to run something while I'm
um I'd like to run something while I'm
at dinner just so I get one more
experiment I don't think that this is
experiment I don't think that this is
going to work
going to work
here we're going to try this but I don't
here we're going to try this but I don't
think that this
works it's conservative
works it's conservative
[Music]
[Music]
absolute by
where's
cost we'll run this
I do not think that this
I do not think that this
works for
reference okay I'm going to call that
reference okay I'm going to call that
there I'm going to go to dinner I'll be
there I'm going to go to dinner I'll be
back in roughly an
back in roughly an
hourish and we will will continue
hourish and we will will continue
working on this um probably for another
working on this um probably for another
two hours hopefully we get this like
two hours hopefully we get this like
solid it's it's very close there just a
solid it's it's very close there just a
few things that need to be
few things that need to be
improved um all the info for all this is
improved um all the info for all this is
on puffer doai
on puffer doai
Discord uh X GitHub start help
