Kind: captions
Language: en
Hello folks,
Hello folks,
we are back live.
Few things to do today.
Few things to do today.
Few different things.
Let me pull that up and I will switch
Let me pull that up and I will switch
over to the stream view.
All right.
All right.
So, uh, first and foremost here, new
So, uh, first and foremost here, new
article
right here.
If you're interested in uh the latest in
If you're interested in uh the latest in
RL, I would suggest reading this. It's a
RL, I would suggest reading this. It's a
pretty quick read, probably like a 5 10
pretty quick read, probably like a 5 10
minute read. I wrote this this morning.
minute read. I wrote this this morning.
pop the chat out real quick in case uh
pop the chat out real quick in case uh
we'll come on to discuss it because the
we'll come on to discuss it because the
bottom of it says come yell at me if you
bottom of it says come yell at me if you
don't like anything in here.
Okay,
Okay,
so uh yeah, this is pretty much just
so uh yeah, this is pretty much just
like a quick summary of the neural MMO
like a quick summary of the neural MMO
training run that we did. I did the
training run that we did. I did the
math. It's an absolutely ludicrous
math. It's an absolutely ludicrous
amount of data that it turns out a 6.2 2
amount of data that it turns out a 6.2 2
pedlops. It's something like comparable
pedlops. It's something like comparable
to the total amount of training data
to the total amount of training data
used in all language models ever. So
used in all language models ever. So
yeah, it's ridiculous. Hey Melor, that's
yeah, it's ridiculous. Hey Melor, that's
to right. Welcome, man.
to right. Welcome, man.
We've released all sorts of stuff in the
We've released all sorts of stuff in the
last little bit.
So new articles here on that.
So new articles here on that.
And then aside from that, we started
And then aside from that, we started
doing a little robotics
doing a little robotics
got back from RSS. How'd that go?
got back from RSS. How'd that go?
I will be at uh RLC
I will be at uh RLC
in a few weeks. Actually, I got to buy
in a few weeks. Actually, I got to buy
my tickets.
my tickets.
Important thing for me to remember to
Important thing for me to remember to
do.
Good.
Good.
Conferences can be hit or miss. So glad
Conferences can be hit or miss. So glad
that went well. Last one I went to
that went well. Last one I went to
nearly killed me.
That's cuz we Americans aren't as tough
That's cuz we Americans aren't as tough
though.
though.
You're dealing with like freaking
You're dealing with like freaking
malaria and Like, oh no, my
malaria and Like, oh no, my
pneumonia. Like,
pneumonia. Like,
[Music]
[Music]
fix the driver problem. Need to install
fix the driver problem. Need to install
nightly. Ah, I we tried to make it a
nightly. Ah, I we tried to make it a
little more stable in the latest as
little more stable in the latest as
well. We added like some stuff,
well. We added like some stuff,
but yeah, there's tons of cool RL around
but yeah, there's tons of cool RL around
here. And all actually works now, which
here. And all actually works now, which
is cool.
We're starting with robotics today.
Let me think how I want to do this. 15
Let me think how I want to do this. 15
over six like 4 mil.
over six like 4 mil.
Five mil
Five mil
like
see what this does.
We have robotics. Uh, I added a binding
We have robotics. Uh, I added a binding
to Manny skill which is a hell of a lot
to Manny skill which is a hell of a lot
easier to use than a lot of the other
easier to use than a lot of the other
ones. Also, a stone friend of mine works
ones. Also, a stone friend of mine works
on this. I've been working on we have
on this. I've been working on we have
some like basic task working, but we
some like basic task working, but we
haven't really given them the full
haven't really given them the full
puffer treatment yet where we've made
puffer treatment yet where we've made
them super super fast or anything.
them super super fast or anything.
Apparently what we're doing here.
Apparently what we're doing here.
Jeez. What did I have to do to make this
Jeez. What did I have to do to make this
work?
All right. I forgot about this thing.
That is welcome, Phoenix.
I'll show what this task is in a second
I'll show what this task is in a second
here.
here.
Oops. Come on, Chrome. Stop being weird.
Oh, saw this. It's cool.
Oh, saw this. It's cool.
So this is a Manny skill.
They have all sorts of robotics tasks.
Late today I had to write this article.
Late today I had to write this article.
Go read it.
It took me a few hours to like write
It took me a few hours to like write
everything.
everything.
I saw this at RSS.
I saw this at RSS.
You probably You might have even met
You probably You might have even met
Stone then. Stone's really cool in
Stone then. Stone's really cool in
robotic space. It was probably there.
We're starting with some of the simpler
We're starting with some of the simpler
tasks
like these two-finger gripper tasks.
Honestly, it's my intuition that doing
Honestly, it's my intuition that doing
stuff like this just really really well
stuff like this just really really well
could be like just as useful as some of
could be like just as useful as some of
the other stuff in robotics as a lot of
the other stuff in robotics as a lot of
the industry bots kind of look like
the industry bots kind of look like
this.
They follow any specific temporal logic
They follow any specific temporal logic
specifications
specifications
or non-marovian.
or non-marovian.
So one of the best things that you can
So one of the best things that you can
do in reinforcement learning for your
do in reinforcement learning for your
own intuition and understanding of
own intuition and understanding of
things is just completely ignore like
things is just completely ignore like
the marov property in everything. It's
the marov property in everything. It's
not relevant.
not relevant.
I sent a PR on the docs. What PR did you
I sent a PR on the docs. What PR did you
send on docs?
I thought I merged this.
I thought I merged this.
Oh, for work in progress.
Thank you.
Let's see what uh
Let's see what uh
what we're getting in this train run.
Okay, so this is not solving the task
Okay, so this is not solving the task
annoyingly.
Let's see why it's not solving the task.
Difficulty using importable APIs. How
Difficulty using importable APIs. How
so?
Eval's not working.
Eval's not working.
You got to give me more information than
You got to give me more information than
that, right?
Welcome YouTube folks. Welcome Twitch.
Welcome YouTube folks. Welcome Twitch.
Let me fix this run so I get this
Let me fix this run so I get this
working real quick and I'll pull the
working real quick and I'll pull the
article back up because I do recommend
article back up because I do recommend
folks read it. Um, it's kind of like a
folks read it. Um, it's kind of like a
nice showcase of some of what we can do
nice showcase of some of what we can do
with RL nowadays.
with RL nowadays.
I was thinking about it and I actually
I was thinking about it and I actually
think it's better than some of like the
think it's better than some of like the
2019 era results. Not like Dota or
2019 era results. Not like Dota or
anything like that, but I think this is
anything like that, but I think this is
a better result than emergent tool use
a better result than emergent tool use
for instance.
Okay, so that's annoying. Um
just
and we'll try to replicate it more
and we'll try to replicate it more
closely. if this works.
Yeah, here's the article.
There are very few reinforcement
There are very few reinforcement
learning projects uh that have been
learning projects uh that have been
trained on this amount of data and all
trained on this amount of data and all
of the other ones I'm aware of are at
of the other ones I'm aware of are at
large companies with lots of resources.
Welcome Jarish.
And the main result of this is
And the main result of this is
essentially that we actually can scale
essentially that we actually can scale
RL not to a huge amount of hardware
RL not to a huge amount of hardware
necessarily, but we can train we can
necessarily, but we can train we can
scale to a massive amount of data
and we can do this in a way that you can
and we can do this in a way that you can
actually replicate on like a relatively
actually replicate on like a relatively
accessible hardware format.
I believe that this would be
Based on rental prices that I saw, I
Based on rental prices that I saw, I
believe that you could replicate this
believe that you could replicate this
experiment for less than $100 if you
experiment for less than $100 if you
were getting like good vast pricing on a
were getting like good vast pricing on a
4090 spec machine.
4090 spec machine.
Not bad.
Not bad.
Dramatically more with H00's, but it's
Dramatically more with H00's, but it's
just like a like three days of training
just like a like three days of training
uh on a single one of these tiny boxes
uh on a single one of these tiny boxes
here.
Okay. Why are we still segting?
Okay. Why are we still segting?
Oh, I'm not allowed to. Believe me, I
Oh, I'm not allowed to. Believe me, I
should have checked that.
should have checked that.
Okay, we're just going to make sure I
Okay, we're just going to make sure I
didn't break the uh the baseline. We're
didn't break the uh the baseline. We're
going to make sure that we can actually
going to make sure that we can actually
still train this thing to like pick up
still train this thing to like pick up
the cube or whatever. And then we're
the cube or whatever. And then we're
going to try to like optimize this for
going to try to like optimize this for
multiGPU because this is one of the rare
multiGPU because this is one of the rare
cases where like the training is so
cases where like the training is so
obnoxiously slow and fiddly that like I
obnoxiously slow and fiddly that like I
can't even really do all that much on
can't even really do all that much on
one GPU with this thing at the moment.
one GPU with this thing at the moment.
So my goal is going to be to try to
So my goal is going to be to try to
create like a multiGPU setting that's
create like a multiGPU setting that's
pretty much the single GPU setting.
pretty much the single GPU setting.
Exact same training run just faster.
We can go from there.
We can go from there.
Yeah, this thing's pretty silly.
Yeah, this thing's pretty silly.
80k steps per second.
Let me look into the domain
Let me look into the domain
randomization docs because I think
randomization docs because I think
that's the next thing.
training speed scale linearly to number
training speed scale linearly to number
of GPUs. uh for this it does
in terms of steps per second it does in
in terms of steps per second it does in
this with the really really fast
this with the really really fast
environments it's hard to keep them
environments it's hard to keep them
linear but we are able to scale to like
linear but we are able to scale to like
millions of steps per second training
millions of steps per second training
from stuff that's like I think neural
from stuff that's like I think neural
MMO 3 goes from 400 something thousand
MMO 3 goes from 400 something thousand
to 2.2 million on six GPUs which is
to 2.2 million on six GPUs which is
pretty decent.
pretty decent.
We haven't super optimized it either.
What needs to get done for Manny skill
What needs to get done for Manny skill
exactly with Puffer? We kind of just
exactly with Puffer? We kind of just
need to see if we can like make their
need to see if we can like make their
stuff run faster in Puffer or if we can
stuff run faster in Puffer or if we can
get it to train faster, right? Like
get it to train faster, right? Like
Puffer is at the moment it's pretty
Puffer is at the moment it's pretty
highly optimized to Ms that run really
highly optimized to Ms that run really
fast. So, uh the fact that these are not
fast. So, uh the fact that these are not
super fast isn't great for us because
super fast isn't great for us because
things we like to do like running full
things we like to do like running full
hyperparameter sweeps and whatnot aren't
hyperparameter sweeps and whatnot aren't
super easy to do. So, uh, it's a bunch
super easy to do. So, uh, it's a bunch
of just like fiddling with it and seeing
of just like fiddling with it and seeing
if we can tune knobs for puffer that
if we can tune knobs for puffer that
they haven't thought of before.
they haven't thought of before.
Like you can run it out of the box,
Like you can run it out of the box,
right? And like try to match settings,
right? And like try to match settings,
but the thing is we don't really want to
but the thing is we don't really want to
do that because their settings are
do that because their settings are
really slow. We kind of want to like see
really slow. We kind of want to like see
if we can get it into the puffer regime,
if we can get it into the puffer regime,
which is like the really high data fast
which is like the really high data fast
sim regime, and then like get the Sims
sim regime, and then like get the Sims
to catch up with it.
Okay.
Yeah. So, I must have broken something
Yeah. So, I must have broken something
because we had this task succeeding
because we had this task succeeding
before.
Yeah. This task was like very clearly
Yeah. This task was like very clearly
succeeding before.
succeeding before.
Unless it just does all of it at the
Unless it just does all of it at the
end, but I don't think that's how it
end, but I don't think that's how it
does.
does.
Oh, control freak is way too high.
Oh, control freak is way too high.
Hang on.
So, this is the original training
So, this is the original training
setting.
setting.
going to be slower.
We kind of just would like need somebody
We kind of just would like need somebody
who's done a little bit of robotics and
who's done a little bit of robotics and
doesn't mind doing absolutely horrible
doesn't mind doing absolutely horrible
things to the sim in order to make it
things to the sim in order to make it
run faster. Like from my perspective, we
run faster. Like from my perspective, we
kind of don't care if the sim is super
kind of don't care if the sim is super
accurate. We really just want it to be
accurate. We really just want it to be
like we want it to be very fast and like
like we want it to be very fast and like
usable and then we want to randomize the
usable and then we want to randomize the
heck out of
I guess as hell.
a 46k SPS.
a 46k SPS.
Why marovian property is not relevant? I
Why marovian property is not relevant? I
think it helps when formulating a state
think it helps when formulating a state
for a new Enver problem.
for a new Enver problem.
How does it help you do that?
How does it help you do that?
And basically all you're saying is like
And basically all you're saying is like
there's some function that maps state to
there's some function that maps state to
next state. Um, and then you're hoping
next state. Um, and then you're hoping
it's not dependent on time.
it's not dependent on time.
Like that's all you're saying.
Like that's all you're saying.
How does that help you?
Like a lot of the environments we train
Like a lot of the environments we train
on don't respect that property. It goes
on don't respect that property. It goes
out of the window immediately with
out of the window immediately with
multi-agent for instance.
multi-agent for instance.
um it goes out of the window when you
um it goes out of the window when you
have like observations for instance
have like observations for instance
don't track things that depend on time
don't track things that depend on time
even if the state does
like the theoretical guarantees in RL
like the theoretical guarantees in RL
are basically worthless
it's way more easy to think of the
it's way more easy to think of the
agents as um learning a mapping which
agents as um learning a mapping which
can depend on time of observations to
can depend on time of observations to
action And that's about it.
action And that's about it.
Solves puffer eval issue. Why don't you
Solves puffer eval issue. Why don't you
use any IDE?
use any IDE?
Why would I? I've used I have used IDEs
Why would I? I've used I have used IDEs
before. They just kind of suck.
Kind of like just editing stuff in Neoim
Kind of like just editing stuff in Neoim
way more.
Okay, so this works again the baseline
Okay, so this works again the baseline
that we were trying to reproduce.
that we were trying to reproduce.
And now we'll see provided that this
And now we'll see provided that this
actually trains, we will see whether we
actually trains, we will see whether we
can make this learn um 6x faster.
I think we should be able to get it to
I think we should be able to get it to
train like 3x faster at least, not 6x,
train like 3x faster at least, not 6x,
but we'll see.
but we'll see.
And the reason for this is because
And the reason for this is because
robotic sims really really like it when
robotic sims really really like it when
you simulate more steps uh on one GPU in
you simulate more steps uh on one GPU in
a big batch. They like to do that. So,
a big batch. They like to do that. So,
you can't really just like split it up
you can't really just like split it up
into six shards and have it be 6x
into six shards and have it be 6x
faster. It doesn't work that way. You
faster. It doesn't work that way. You
can actually make it six times faster by
can actually make it six times faster by
multiplying the batch size by six, but
multiplying the batch size by six, but
the thing is then the problem actually
the thing is then the problem actually
has to be pretty hard for that to be
has to be pretty hard for that to be
worth anything, which is what we'll get
worth anything, which is what we'll get
to once we start domain randomizing.
to once we start domain randomizing.
Speaking of which,
they have a domain randomization thing
they have a domain randomization thing
on here, don't they?
Yeah, domain randomization.
Yeah, domain randomization.
So camera randomization's not relevant
So camera randomization's not relevant
yet
yet
right now. They're just randomizing the
right now. They're just randomizing the
position of the cube
pose. I don't actually know if pose is
pose. I don't actually know if pose is
randomized.
So this is going to be our next step, I
So this is going to be our next step, I
think, once we have it training faster
think, once we have it training faster
will be to do something like this.
Okay, so this works perfectly. We see
Okay, so this works perfectly. We see
test once 12.8 million 997.
test once 12.8 million 997.
Wait for this.
Uh, do I want to go over or under? I
Uh, do I want to go over or under? I
think I want to go over. We'll do 1024.
think I want to go over. We'll do 1024.
And then what we'll do is we'll assume
And then what we'll do is we'll assume
that I get
that I get
five mil.
five mil.
Do like 4 mil.
Do like 4 mil.
Let's see if this does anything.
Domain randomization refers to physics
Domain randomization refers to physics
param randomization. contact prem
param randomization. contact prem
density. Uh that's another part of it
density. Uh that's another part of it
and we will do that as well. I kind of
and we will do that as well. I kind of
want to like my vision for this right is
want to like my vision for this right is
we make the problem like really fuzzy by
we make the problem like really fuzzy by
randomizing literally everything. Okay.
randomizing literally everything. Okay.
And then we scale this thing to like
And then we scale this thing to like
20,000 robots or whatever.
20,000 robots or whatever.
And I forgot to edit the but luckily I
And I forgot to edit the but luckily I
won't have to do this again.
won't have to do this again.
have to do like this.
So, we're going to scale this thing to I
So, we're going to scale this thing to I
mean neural MMO it goes up to like 50k.
mean neural MMO it goes up to like 50k.
So, we could theoretically even go to
So, we could theoretically even go to
that and that would be faster.
that and that would be faster.
It's just like we have we can't start
It's just like we have we can't start
there because you don't actually benefit
there because you don't actually benefit
from having more parallel agents until
from having more parallel agents until
the problem gets harder. And I actually
the problem gets harder. And I actually
don't even know if domain randomization
don't even know if domain randomization
is going to make training harder or
is going to make training harder or
easier. It actually can go both ways.
I had control problems with not enough
I had control problems with not enough
to observe variables and had to think
to observe variables and had to think
about what to include. Yeah. So, you
about what to include. Yeah. So, you
don't need to think about MDP for that
don't need to think about MDP for that
though. You don't need any of the
though. You don't need any of the
mathematical formulism. That's literally
mathematical formulism. That's literally
just from the perspective of the agent.
just from the perspective of the agent.
What information do you need to solve
What information do you need to solve
the problem? That's it. There's no
the problem? That's it. There's no
mathematical formulism involved with
mathematical formulism involved with
that. Like, there's not even any RL
that. Like, there's not even any RL
involved with that. It's literally just
involved with that. It's literally just
what information do you need to solve
what information do you need to solve
the problem.
We use LSTM as a default choice. It's
We use LSTM as a default choice. It's
not to mitigate the non-marovian prop
not to mitigate the non-marovian prop
like the way we think about this, right?
like the way we think about this, right?
We use LSTMs by default so that the
We use LSTMs by default so that the
agents have memory. That's it.
agents have memory. That's it.
You can attempt to interpret that
You can attempt to interpret that
through mar like through like the thing
through mar like through like the thing
that you said, but it's going to be
that you said, but it's going to be
dramatically dramatically easier and
dramatically dramatically easier and
it's going to give you the exact same
it's going to give you the exact same
result if you just say that we add LSTM
result if you just say that we add LSTM
so that the agents can have memory.
so that the agents can have memory.
That's it.
That's it.
And then there are some problems that
And then there are some problems that
you don't need memory for. And there are
you don't need memory for. And there are
some problems that you do need memory
some problems that you do need memory
for.
And it's pretty immediately obvious for
And it's pretty immediately obvious for
most of them by looking at them which is
most of them by looking at them which is
which?
which?
You see what I mean?
You see what I mean?
We go to the mathematical formulas in
We go to the mathematical formulas in
reinforcement learning when it's
reinforcement learning when it's
actually the best way to describe what's
actually the best way to describe what's
happening.
But that is not all the time.
Okay. So something is very weird. Uh
Okay. So something is very weird. Uh
something very weird looks like it's
something very weird looks like it's
going on here. Uh because
we are not solving the task.
Bye man
things.
I have to reduce mini batch as well. Oh,
I have to reduce mini batch as well. Oh,
I probably messed that up, right?
Let's do uh 8192.
I guess this is not really a one one.
I guess this is not really a one one.
So, we'll see whether this does the same
So, we'll see whether this does the same
thing or not.
It should be pretty close though.
It should be pretty close though.
I'll be surprised if this doesn't at
I'll be surprised if this doesn't at
least give us um something, you know,
least give us um something, you know,
like maybe it takes a little longer to
like maybe it takes a little longer to
train, but it should uh in steps, but it
train, but it should uh in steps, but it
should actually give us something that's
should actually give us something that's
at least faster than uh wall clock. But
at least faster than uh wall clock. But
hopefully
it's like it's not that I hate math,
it's like it's not that I hate math,
Right. It's that I hate when we use math
Right. It's that I hate when we use math
to like when we use very heavy math to
to like when we use very heavy math to
describe very simple things where the
describe very simple things where the
math adds exactly zero additional
math adds exactly zero additional
precision and clarity. And I see it all
precision and clarity. And I see it all
the time in RL.
No.
Okay. I mean, this is like slightly
Okay. I mean, this is like slightly
better of a result.
better of a result.
This should maybe learn something. We'll
This should maybe learn something. We'll
see.
problem is linear and interpretable.
problem is linear and interpretable.
Yeah, but the thing is if the problem is
Yeah, but the thing is if the problem is
linear and interpretable like all this
linear and interpretable like all this
stuff is useless. Like why are you
stuff is useless. Like why are you
wasting your time studying linear
wasting your time studying linear
problems?
So this is like one of my pet peeves,
So this is like one of my pet peeves,
right, with how a lot of these things
right, with how a lot of these things
are taught. They'll like spend time
are taught. They'll like spend time
making you do like proofs or making you
making you do like proofs or making you
like solve equations on like problems
like solve equations on like problems
that just don't matter
like nobody is working on how do we
like nobody is working on how do we
design better RL for linear problems.
Okay. So, this takes a bit longer, but
yeah, we are solving.
yeah, we are solving.
Cool. So, and it is a little faster in
Cool. So, and it is a little faster in
wall clock even though we have not
wall clock even though we have not
reuned parameters or anything. Okay. So,
reuned parameters or anything. Okay. So,
this is like acceptable sub three
this is like acceptable sub three
minutes per experiment.
minutes per experiment.
We can at least like make progress with
We can at least like make progress with
things at this rate.
things at this rate.
When things get to like five plus minute
When things get to like five plus minute
experiments, I get annoyed because it's
experiments, I get annoyed because it's
like
like
what am I going to do? I just guess I
what am I going to do? I just guess I
hang out and I yap for five minutes
hang out and I yap for five minutes
between runs.
Okay, so this works
and
and
I think I ran at 30 mil.
I think I ran at 30 mil.
Current reward function justified in
Current reward function justified in
Manny skill.
Manny skill.
Uh, their rewards were not. I changed
Uh, their rewards were not. I changed
them and now they are.
them and now they are.
Sometimes they take 15 minutes. You need
Sometimes they take 15 minutes. You need
to get real hardware, my friend. CPUs
to get real hardware, my friend. CPUs
are not real hardware for RL.
And this robotic stuff is super slow. So
And this robotic stuff is super slow. So
like I have a server for it, but like
like I have a server for it, but like
all the other stuff in Puffer, you can
all the other stuff in Puffer, you can
just run it on a desktop. We have people
just run it on a desktop. We have people
running with like basically whatever
running with like basically whatever
GPU. It's can't you can't be doing stuff
GPU. It's can't you can't be doing stuff
on CPU. It'll be like 20 times faster
on CPU. It'll be like 20 times faster
than if you do it on CPU and another RL
than if you do it on CPU and another RL
library. But we can only work so much
library. But we can only work so much
magic.
magic.
Only do so much.
So this looks fine, right?
Here's our multiGPU baseline.
Here's our multiGPU baseline.
If I do real time,
if I do relative time, you can see it is
if I do relative time, you can see it is
better in relative time than our uh
better in relative time than our uh
previous experiment. So good. A. Okay.
Next we do domain randomization.
Okay. So, we'll have to figure out how
Okay. So, we'll have to figure out how
we do this.
we do this.
Think we can do RL on cyersc. I've
Think we can do RL on cyersc. I've
talked to a few people about that
talked to a few people about that
probably.
You can always toggle with how position
You can always toggle with how position
rewardsation
rewardsation
combined. Uh there was a much more
combined. Uh there was a much more
fundamental problem, Sean, that I had to
fundamental problem, Sean, that I had to
fix.
fix.
uh they were doing rewards in a way that
uh they were doing rewards in a way that
just do not make sense for standard RL
just do not make sense for standard RL
outside of robotics. And like basically
outside of robotics. And like basically
they had two errors that canceled each
they had two errors that canceled each
other out. And like all of robotics
other out. And like all of robotics
people do this apparently, but I just
people do this apparently, but I just
like I did it the same way where you
like I did it the same way where you
don't need to go make two errors to
don't need to go make two errors to
cancel each other out. And it's just
cancel each other out. And it's just
correct.
Sometimes they provide intuition to a
Sometimes they provide intuition to a
bigger empirical problem
bigger empirical problem
as far as I can.
as far as I can.
I hear what you're saying, Kovac, but
I hear what you're saying, Kovac, but
I've seen the way the RL is taught and
I've seen the way the RL is taught and
it's a mess.
Like, if you just look at like our quick
Like, if you just look at like our quick
start guide and like the resources I
start guide and like the resources I
link in the quick start guide and how
link in the quick start guide and how
much faster you will actually be able to
much faster you will actually be able to
solve stuff and understand things that
solve stuff and understand things that
matter, it's a world of difference. It
matter, it's a world of difference. It
advance. So, basically, and this is this
advance. So, basically, and this is this
is no shame to the manny skill guys,
is no shame to the manny skill guys,
okay? Because everybody in robotics does
okay? Because everybody in robotics does
this stuff and they all do it wrong.
this stuff and they all do it wrong.
What they do is they do these
What they do is they do these
state-based rewards where they say like,
state-based rewards where they say like,
okay, your reward goes up as you get
okay, your reward goes up as you get
closer to the target. All right?
closer to the target. All right?
And the reason that this is wrong is
And the reason that this is wrong is
because when you're right next to the
because when you're right next to the
target, you have maximal reward.
target, you have maximal reward.
So, you're incentivized to just sit
So, you're incentivized to just sit
there forever and do nothing because you
there forever and do nothing because you
farm more reward. That is what is wrong
farm more reward. That is what is wrong
with the way that roboticists specify
with the way that roboticists specify
their rewards.
their rewards.
To get around this, they do this
To get around this, they do this
infinite horizon bootstrap thing, which
infinite horizon bootstrap thing, which
basically says that you pretend that
basically says that you pretend that
you're going to keep going forever and
you're going to keep going forever and
that's the reward that you'll actually
that's the reward that you'll actually
get. And they change the way advantage
get. And they change the way advantage
estimation works in order to do that.
estimation works in order to do that.
But that's going to break learning for a
But that's going to break learning for a
ton of other environments because it
ton of other environments because it
basically says you get reward beyond the
basically says you get reward beyond the
current episode. Um, you don't get like
current episode. Um, you don't get like
a penalty for terminating.
a penalty for terminating.
So, it's a much more specialized
So, it's a much more specialized
implementation. And the thing is, you
implementation. And the thing is, you
don't need to do any of this. Literally,
don't need to do any of this. Literally,
all they had to do is just take their
all they had to do is just take their
current reward, right? And take the
current reward, right? And take the
delta from the current reward to the
delta from the current reward to the
reward at the previous time step. That's
reward at the previous time step. That's
your new reward function. And now all
your new reward function. And now all
those problems go away. It was like
those problems go away. It was like
three lines of code.
Why is command line interface slow
Why is command line interface slow
compared to importable?
compared to importable?
What do you mean why is it slow?
What do you mean why is it slow?
you get faster training speed or is it
you get faster training speed or is it
like is it just that it takes a second
like is it just that it takes a second
to show up? Because if it's that it
to show up? Because if it's that it
takes a second to show up, that's
takes a second to show up, that's
because Torch like PyTorch takes like a
because Torch like PyTorch takes like a
second to import.
It's very silly that we have packages in
It's very silly that we have packages in
Python that take like a second or two to
Python that take like a second or two to
import, but that is the state of
import, but that is the state of
software.
How do I do this? Me get rid of that
How do I do this? Me get rid of that
bot.
I import it's closer to 600K. Then
I import it's closer to 600K. Then
something's got to be different, right?
something's got to be different, right?
like settings or something got to be
like settings or something got to be
different.
The command line is literally just a
The command line is literally just a
wrapper around that.
wrapper around that.
Could be default args. I don't know. It
Could be default args. I don't know. It
depends how you're loading configs.
Is our RL stars fair? Yeah.
Is our RL stars fair? Yeah.
I mean, you see the thing, the way that
I mean, you see the thing, the way that
we build stuff around here, it's like
we build stuff around here, it's like
a lot of stuff in RL is just it's
a lot of stuff in RL is just it's
mindboggling how badly done a lot of the
mindboggling how badly done a lot of the
things are, right? Like look, I
things are, right? Like look, I
shouldn't be able to make RL a thousand
shouldn't be able to make RL a thousand
times faster. Okay? When a field is done
times faster. Okay? When a field is done
competently, it's like a big deal if you
competently, it's like a big deal if you
make something two times faster or two
make something two times faster or two
times better, right? That's like a huge
times better, right? That's like a huge
deal, right? When stuff is like I make
deal, right? When stuff is like I make
it a thousand times faster, that just
it a thousand times faster, that just
means the original was just a total
means the original was just a total
mess, right?
Let's see if we can figure out this pose
Let's see if we can figure out this pose
logic stuff. And since we do have a fair
logic stuff. And since we do have a fair
few folks on YouTube, a few quick
few folks on YouTube, a few quick
things. One is there's a a new article
things. One is there's a a new article
today. It's on X. You can get it right
today. It's on X. You can get it right
here, Jars 531. I post all the stuff
here, Jars 531. I post all the stuff
here. So, if you want to see how you can
here. So, if you want to see how you can
do reinforcement learning on a pabyte of
do reinforcement learning on a pabyte of
data with a relatively uh mediocre
data with a relatively uh mediocre
hardware setup, not a $300,000 server
hardware setup, not a $300,000 server
right here. And then generally, I stream
right here. And then generally, I stream
all this stuff live. It's all open
all this stuff live. It's all open
source at puffer.ai.
source at puffer.ai.
The GitHub to help us out. And you can
The GitHub to help us out. And you can
get involved with the development on the
get involved with the development on the
Discord right here. Well, back to Dev.
This is slightly irritating.
There's not really a way around this,
There's not really a way around this,
but they they kind of want you to
but they they kind of want you to
subclass their stuff.
Okay, fine. We'll do that.
We'll make like um
make like a puffer task or whatever.
How do we do this for um
How do we do this for um
for the panda robot
agent robot and controller randomization
Little tricky to figure out how we're
Little tricky to figure out how we're
supposed to do this.
Let me just message Stone to see if he's
Let me just message Stone to see if he's
on.
on.
Maybe he can help us get us started on
Maybe he can help us get us started on
this quicker. If not, I'll just figure
this quicker. If not, I'll just figure
it out.
Load
agent load scene
a custom environment. When you load the
a custom environment. When you load the
scene,
we definitely need to just get a base
we definitely need to just get a base
scene loaded. Let's do that first.
What is epoch? epoch is the number of uh
What is epoch? epoch is the number of uh
training steps that have run like
training steps that have run like
collect a bunch of data, you train on
collect a bunch of data, you train on
the data, that's one epoch.
cube.py
at register M cube.
Many skill M's tasks tabletop
pitch cube
Take them.
Okay. So now
We're going to just temporarily force
We're going to just temporarily force
this to be
this to be
upper
upper
buffer pick
buffer pick
cube.
And I guess we can pass all this stuff
And I guess we can pass all this stuff
directly maybe.
Not.
Hopefully we can get a good domain
Hopefully we can get a good domain
randomized config quite quickly here.
randomized config quite quickly here.
We'll see.
I think I know how we should go about
I think I know how we should go about
it. If we just open up the uh the
it. If we just open up the uh the
source,
I believe they randomized the uh the
I believe they randomized the uh the
goal position.
Oh, maybe they don't randomize the goal
Oh, maybe they don't randomize the goal
position at all.
actors.build sphere
pose.
Um,
this is 000.
this is 000.
Kind of confusing honestly. I don't know
Kind of confusing honestly. I don't know
where they're setting positions and
where they're setting positions and
things in here.
things in here.
Okay, so this works apparently, but um
Okay, so this works apparently, but um
I'm not seeing any logs. So something is
I'm not seeing any logs. So something is
screwy.
Maybe we should just do like this. Yeah.
Go through the register.
Episode and epochs are the same. Um, no.
Episode and epochs are the same. Um, no.
So, an episode Well, where do you see
So, an episode Well, where do you see
episodes? I don't think we record that
episodes? I don't think we record that
as a stat. You see episode return and
as a stat. You see episode return and
episode length that is for like one
episode length that is for like one
agent's interactions with one
agent's interactions with one
environment. How long is that? So how
environment. How long is that? So how
many steps and then what is the summed
many steps and then what is the summed
summed reward?
summed reward?
The epochs is like a bunch of episodes.
The epochs is like a bunch of episodes.
A whole bunch of uh episodes are in each
A whole bunch of uh episodes are in each
epoch.
Does anybody understand how they have
Does anybody understand how they have
this set up?
Cube spawn center
Cube spawn center
register M does not define. Okay.
Go XYZ.
Go XYZ.
Okay. Uh pose.create from PQ
positioning quaternian. Okay.
positioning quaternian. Okay.
So this is randomizing I believe just
So this is randomizing I believe just
the initial cube site
the initial cube site
and not the is it the goal?
and not the is it the goal?
Oh no it is randomizing the
Oh no it is randomizing the
some aspect of it.
Okay. So this runs now. We'll make sure
Okay. So this runs now. We'll make sure
that this frames.
We actually do have our ability though
We actually do have our ability though
to to mess with it now.
Oh, I understand. So, this is just
Oh, I understand. So, this is just
loading all the stuff and then on
loading all the stuff and then on
initialize episode
initialize episode
they uh they move the thing. So, really
they uh they move the thing. So, really
all we need to do
change the initialize episode
change the initialize episode
potentially,
at least for now.
And we should also probably get this on
And we should also probably get this on
our local
so that we can render stuff.
Yeah, so this is the default setup.
Yeah, so this is the default setup.
It does randomize the cubes
It does randomize the cubes
position
and the goal.
It does drop it occasionally as well,
It does drop it occasionally as well,
which is interesting.
as this works. We didn't break the
as this works. We didn't break the
environment.
environment.
Now we can attempt to break the
Now we can attempt to break the
environment.
This
This
you need this
start with
uh do we let me See, does this always
uh do we let me See, does this always
start the robot at the same spot?
Looks like the robot's pretty much
Looks like the robot's pretty much
always in the same spot, right?
table scene
table scene
robot.
tell you how to move the uh the robot
tell you how to move the uh the robot
agent robot and controller
agent robot and controller
randomizations.
Done.
Ripper frictions render material.
Yeah, this can be done during
Yeah, this can be done during
initialized episodes.
Environment episode current is the
Environment episode current is the
parameter that tells us the agent is
parameter that tells us the agent is
learn. It depends on the environment,
learn. It depends on the environment,
but yes,
but yes,
some environments have a score. Most of
some environments have a score. Most of
them should have a score in puffer lib.
them should have a score in puffer lib.
That's the actual like metric you care
That's the actual like metric you care
about. Episode return is the summed
about. Episode return is the summed
reward, the metric the agencies that
reward, the metric the agencies that
it's optimizing.
it's optimizing.
slightly different, but they'll both
slightly different, but they'll both
tell you things.
When it's flat, it means the agent's not
When it's flat, it means the agent's not
learning anything. If the score is flat
learning anything. If the score is flat
as well, then yes.
How do I make the robot start at a
How do I make the robot start at a
different spot?
different spot?
like
what is pros and cons of numbum M's?
Uh it depends which num ms param you
Uh it depends which num ms param you
mean.
mean.
If you set like a lot of the ends uh
If you set like a lot of the ends uh
like vec num ms param to zero like to
like vec num ms param to zero like to
one it's not going to learn if you set
one it's not going to learn if you set
num ms to one it's not going to learn
num ms to one it's not going to learn
anything.
anything.
If you set those to 20 then you have
If you set those to 20 then you have
you're going to have a ludicrous number
you're going to have a ludicrous number
of agents.
of agents.
Like if you change it if you change the
Like if you change it if you change the
default to 20 you're going to have like
default to 20 you're going to have like
20,000 agents in parallel. You don't
20,000 agents in parallel. You don't
have hardware for that and then only
have hardware for that and then only
some of the environments will benefit
some of the environments will benefit
from that.
What the heck?
Helicopter just went over flying super
Helicopter just went over flying super
low.
What's the ideal for CPU?
The settings that I have as defaults are
The settings that I have as defaults are
probably going to still be roughly ideal
probably going to still be roughly ideal
because like
because like
you'd have to rerun a whole sweep just
you'd have to rerun a whole sweep just
for CPU if you want to see what the
for CPU if you want to see what the
fastest solve time is. At least the
fastest solve time is. At least the
settings that I provided I know work,
settings that I provided I know work,
right?
Robot init pause noise.
Robot init pause noise.
The robot init pause noise. Can I crank
The robot init pause noise. Can I crank
this up?
Can I just like crank this way up?
What happens if I do this?
Then the next thing
the heck pose is not defined.
Oh,
this is everything in here.
So, this should make uh the robot start
So, this should make uh the robot start
at all sorts of different spots.
That works.
We can change the
We can change the
change the robot like joint stuff.
change the robot like joint stuff.
That's probably important.
I wanted to change like size of cube and
I wanted to change like size of cube and
stuff.
Well, stones replying.
Starting to learn
Ah,
Ah,
but here
has failed it seems.
Starting to learn
Starting to learn
if we just can run it for longer.
A lot of things you can change in theory
A lot of things you can change in theory
soon.
For everything must be rough. No, I Why
For everything must be rough. No, I Why
would I do it this way if it were
would I do it this way if it were
difficult? I do it this way because it's
difficult? I do it this way because it's
easy.
easy.
The one annoying thing is I don't have
The one annoying thing is I don't have
my Neovim on the tiny box. So like my
my Neovim on the tiny box. So like my
autocomplete and stuff isn't working
autocomplete and stuff isn't working
nicely.
I have it a little bit nicer locally.
I've used IDs before. They're just slow
I've used IDs before. They're just slow
and shitty.
This is what it looks like better.
Okay,
Okay,
this is doing something now.
Cool.
See if it's stable.
So, let's see. There's collision meshes
So, let's see. There's collision meshes
visual messages
visual messages
point PD
point PD
mass inertia
mass inertia
initial mean joint position
initial mean joint position
roller choice.
Most of these are not exposed directly.
We have to be a little careful not to do
We have to be a little careful not to do
stuff that just makes the task
stuff that just makes the task
impossible.
I don't think that's what's happening
I don't think that's what's happening
here, though.
here, though.
This seems like unstable learning or
This seems like unstable learning or
something.
Set it to 0.1 for now and see what
Set it to 0.1 for now and see what
happens.
I don't want it to like spawn through
I don't want it to like spawn through
the table or something, right?
Mass inertia
Mass inertia
point PD
Pose equals
random quitterians.
How do you fund this project? Do you
How do you fund this project? Do you
work somewhere else? Puffer is a
work somewhere else? Puffer is a
company. So, we have a couple companies
company. So, we have a couple companies
that we work with. Generally, you can
that we work with. Generally, you can
hire Puffer to help with your
hire Puffer to help with your
reinforcement learning efforts. We have
reinforcement learning efforts. We have
all sorts of tools. Uh, reinforcement
all sorts of tools. Uh, reinforcement
learning is hard. We make it easier.
Generally, a lot of our stuff just
Generally, a lot of our stuff just
solves a lot of problems out of the box.
solves a lot of problems out of the box.
Um, the ones that it doesn't we help
Um, the ones that it doesn't we help
with.
Okay. Okay, so this initial pose at
Okay. Okay, so this initial pose at
least gives us some reasonable success
least gives us some reasonable success
right here.
I don't know if this is going to be
I don't know if this is going to be
stable.
stable.
This seems like it's unstable a little
This seems like it's unstable a little
bit.
at
at
still don't even know what the
still don't even know what the
observations are, right?
observations are, right?
What are the observations?
Okay. So they have added the
Okay. So they have added the
obvious
grasp.
grasp.
Full pause.
Modify the task.
Yeah, this is stable.
pose raw pose.
pose raw pose.
I need to understand what this data is
I need to understand what this data is
next.
Close fully.
You pause noise. Okay.
Not super stable.
PCP pose
PCP pose
need to understand whether this is
need to understand whether this is
reasonable. Next.
just stick some break points in here and
just stick some break points in here and
start looking at data.
off pose.
Okay, so this is the pose.
to OB
pose.
Uh, okay. This is interesting.
Uh, okay. This is interesting.
The cube also has a 7D pose.
The cube also has a 7D pose.
Position and orientation of the robot.
Position and orientation of the robot.
Uh, what is environment?
Uh, what is environment?
This is Manny skill. This is a robot
This is Manny skill. This is a robot
picking up a cube and moving it to a
picking up a cube and moving it to a
goal.
So, is this position XYZ and then like a
So, is this position XYZ and then like a
quitterian?
This what it is.p
P
P
and Q.
and Q.
Yeah. Environment slashn on your ocean
Yeah. Environment slashn on your ocean
n. Oh, n is the number of data points
n. Oh, n is the number of data points
that were collected in displaying that
that were collected in displaying that
summary.
summary.
Last four straight up like quturnians.
Last four straight up like quturnians.
Yeah.
Yeah.
So that's weird though.
It should have more than this, right?
A robot is described by a root pose and
A robot is described by a root pose and
point angles.
hit cube only. It doesn't have anything
hit cube only. It doesn't have anything
to do with the cube though. It has to do
to do with the cube though. It has to do
with the robot, right? The robot is a
with the robot, right? The robot is a
like six degree of freedom or whatever
like six degree of freedom or whatever
arm
always given to the
You always provide Q pause and velocity
You always provide Q pause and velocity
extra obs specific data.
extra obs specific data.
Okay, I see.
Yeah. Okay. So, apparently there's a
Yeah. Okay. So, apparently there's a
base there's a base observation for all
base there's a base observation for all
the robots.
M and the Sapion M.
Get OBS.
Get OBS agent. Get OBS extra.
Get OBS agent. Get OBS extra.
Get proprioception.
grasping. Addict.
Looks like a pain. Yeah, it is a pain in
Looks like a pain. Yeah, it is a pain in
the ass, but it's dramatically less of a
the ass, but it's dramatically less of a
pain in the ass than the other robotics
pain in the ass than the other robotics
libraries.
This is like less, believe it or not,
This is like less, believe it or not,
this is like less overly abstracted and
this is like less overly abstracted and
ridiculous by a mile compared to like
ridiculous by a mile compared to like
Isaac
Isaac
in base agent.
Oh, agent. There's a base agent.
And then this calls get state
And then this calls get state
and get get pause get velocity.
Hey,
do we have a at.
So they actually are including then the
So they actually are including then the
information that seems
information that seems
mostly reasonable.
I think that this is mostly done
I think that this is mostly done
correctly.
I guess it's difficult to know because
I guess it's difficult to know because
of the coordinates schemes, right?
Okay, this is one that actually has like
Okay, this is one that actually has like
a pretty simple reward. That's probably
a pretty simple reward. That's probably
correct.
Then what makes this like remotely
Then what makes this like remotely
difficult or interesting, I guess, is my
difficult or interesting, I guess, is my
question,
question,
right?
right?
What makes this remotely difficult?
probably just comes down to
probably just comes down to
randomization. Now,
I do it here.
I do it here.
Builds all objects and all parallel M
Builds all objects and all parallel M
and share same physical properties.
and share same physical properties.
Boards are dense. So it has to work
Boards are dense. So it has to work
unless problem works. But I'm trying to
unless problem works. But I'm trying to
figure out what makes the problem like
figure out what makes the problem like
remotely difficult to learn.
I mean to be fair it does learn in like
I mean to be fair it does learn in like
a ridiculously short number of time
a ridiculously short number of time
steps uh by like our standards of our
steps uh by like our standards of our
other environments just the end is so
other environments just the end is so
unbelievably Hello.
That's the main limitation more than
That's the main limitation more than
anything is just it's order of magnitude
anything is just it's order of magnitude
minimum too slow.
minimum too slow.
Like minimum
Like minimum
full
pull center point.
pull center point.
Okay, so this is competently done.
I think what else I can mess
Okay.
Okay.
On
On
Here's
a thing I can try.
Can I do 4096?
Can I do this?
This is 280k. Okay.
We should practice
go explore style thing.
I could I think I think I see now with
I could I think I think I see now with
like robotics how you could do this from
like robotics how you could do this from
just a couple demos.
I don't think what we have to do that
I don't think what we have to do that
they're just loading some states.
being the issue, right? is that it
being the issue, right? is that it
doesn't
doesn't
doesn't learn anywhere near as Well,
Getting started with RL. Welcome.
Getting started with RL. Welcome.
This is the place for it.
I think we're just going to have to run
I think we're just going to have to run
a bunch of random experiments with how
a bunch of random experiments with how
slow this thing is is and hope we find
slow this thing is is and hope we find
something start with.
Like it's not like it doesn't work, it's
Like it's not like it doesn't work, it's
just not optimal.
this
this
first thing to try. That's kind of
first thing to try. That's kind of
obvious.
It's already a massive learning rate,
It's already a massive learning rate,
but no, screw it. Why not?
It's just like it's annoying that this
It's just like it's annoying that this
is basically they've kind of like
is basically they've kind of like
managed to bring RL back to the dark
managed to bring RL back to the dark
ages with the the slow sims here.
ages with the the slow sims here.
Realistically,
Realistically,
I'd have to add
I'd have to add
I'd have to add multiGPU
I'd have to add multiGPU
sweeps, I think.
Try
increase action limits.
A RMP PD
Why is it not learning
Why is it not learning
with
you're using 32,000 environments? That's
you're using 32,000 environments? That's
why you have a huge number of
why you have a huge number of
environments for a relatively simple
environments for a relatively simple
problem. It's not going to benefit from
problem. It's not going to benefit from
that amount of parallelism.
U because the config for that end
U because the config for that end
probably doesn't have end numm set to
probably doesn't have end numm set to
eight. Got this from your example. Which
eight. Got this from your example. Which
example has that?
portable. Oh yeah, that's just that's
portable. Oh yeah, that's just that's
just showing you like a random that's
just showing you like a random that's
not like meant to be a good config for
not like meant to be a good config for
whatever env, right? They're showing you
whatever env, right? They're showing you
the API
visual will be barely paralyzed at 1024
visual will be barely paralyzed at 1024
m
m
per GPU.
trying this.
Okay, this does something, but I don't
Okay, this does something, but I don't
think it learns like fast, right?
think it learns like fast, right?
So, this it learns, but it takes forever
So, this it learns, but it takes forever
here.
here.
That doesn't do it.
go mess with the panda.
go mess with the panda.
Where?
Okay. So that's
Okay. So that's
make the robot faster. There.
Okay, this looks better, right?
Are you running multi-en
Are you running multi-en
You're running multi-en 4096? How come
You're running multi-en 4096? How come
it's slow? This thing here, uh, this is
it's slow? This thing here, uh, this is
slow because this is a robotic sim,
slow because this is a robotic sim,
which is just slow. It's not a puffer
which is just slow. It's not a puffer
end. Our puffer ms are fast.
Okay. So, this learns like
Okay. So, this learns like
maybe a little bit faster.
Okay. So, increasing the actions kind of
Okay. So, increasing the actions kind of
did something.
I'm going to go like just 5x the force
I'm going to go like just 5x the force
limits and stuff as well.
You say it slows comparing apples and
You say it slows comparing apples and
bananas. Whole engines do tons of
bananas. Whole engines do tons of
calculations.
[Music]
[Music]
Honestly, it's like six degrees of
Honestly, it's like six degrees of
freedom in one cube. I think it's kind
freedom in one cube. I think it's kind
of just slow.
Like particularly when I look at the
Like particularly when I look at the
code and how the code is written, it's
code and how the code is written, it's
like
I bet it's just slow.
I bet it's just slow.
If I want to write an NVC, what do I
If I want to write an NVC, what do I
have to do? We have a tutorial for that
have to do? We have a tutorial for that
on the docs page.
I'm thinking about it and it's like
I'm thinking about it and it's like
there's no way you wouldn't be able to
there's no way you wouldn't be able to
run a fixed degree of freedom arm at
run a fixed degree of freedom arm at
like millions. Totally should be able
like millions. Totally should be able
to. Really not doing anything that
to. Really not doing anything that
crazy.
Ah, lovely. Okay. So, it crashes.
Okay.
Now, you find sweet spot for training
Now, you find sweet spot for training
config. Not the way I'm doing it here.
config. Not the way I'm doing it here.
Automated uh automatic sweeps.
Don't do what I'm doing here.
Don't do what I'm doing here.
Crazy. Contact sim with what? There's
Crazy. Contact sim with what? There's
nothing for it to contact.
uh puffer sweep. It should also be in
uh puffer sweep. It should also be in
the docs. We have a sweep. We have like
the docs. We have a sweep. We have like
a really nice hyper pram sweep algo
a really nice hyper pram sweep algo
built in.
See if this does anything.
Let's just I'm just kind of yoloing a
Let's just I'm just kind of yoloing a
few things here because like
the way that it's set up just can't be.
the way that it's set up just can't be.
It like just can't be the best way of
It like just can't be the best way of
doing it.
It's just no way you need to have it be
It's just no way you need to have it be
so high fidelity that it's low.
to help me find the best config. Yes.
All right. We'll see if this does
All right. We'll see if this does
anything.
This should allow it to just like flail
This should allow it to just like flail
about very very quickly if it wants to.
Not also getting limited by maxport.
Ignore it. Okay.
If this learns anything
realistic collision checks for complex
realistic collision checks for complex
meshes even though they are
meshes even though they are
rectangularistically
doing a lot of realism. Well, then they
doing a lot of realism. Well, then they
shouldn't be doing that, right?
The idea of like I want to train a good
The idea of like I want to train a good
robot, therefore I will simulate this
robot, therefore I will simulate this
thing as realistically as I possibly
thing as realistically as I possibly
can, train it on that and then deploy
can, train it on that and then deploy
it. That's just wrong. That's not how it
it. That's just wrong. That's not how it
works
because if you just have no data, right?
because if you just have no data, right?
If you have basically no data and it's
If you have basically no data and it's
very high fidelity, doesn't matter.
I'd rather be able to train something at
I'd rather be able to train something at
relatively lower fidelity with a ton of
relatively lower fidelity with a ton of
randomization such that I get a really
randomization such that I get a really
really robust policy out of it.
really robust policy out of it.
It doesn't matter if I can simulate
It doesn't matter if I can simulate
physics if I have something that can
physics if I have something that can
like work correctly on basically any
like work correctly on basically any
sort of janky physics setup that you
sort of janky physics setup that you
give it. Right?
give it. Right?
That's the idea.
how you deploy it because if it works on
how you deploy it because if it works on
any randomized set of physics,
any randomized set of physics,
collisions, etc. If it works on like all
collisions, etc. If it works on like all
randomized versions, then the real
randomized versions, then the real
version is in there somewhere, right?
version is in there somewhere, right?
So, think about it, right? one option
So, think about it, right? one option
one you make it as realistic as
one you make it as realistic as
possible. Okay. And then you deploy you
possible. Okay. And then you deploy you
train it on that it's really really slow
train it on that it's really really slow
and you deploy it and then your SIM is
and you deploy it and then your SIM is
not exactly like the real world. So
not exactly like the real world. So
there's still a big annoying gap, right?
there's still a big annoying gap, right?
Option two is that you randomize the
Option two is that you randomize the
heck out of everything such that you're
heck out of everything such that you're
not remotely training on something that
not remotely training on something that
looks like the real world, but the real
looks like the real world, but the real
world is in there somewhere in all the
world is in there somewhere in all the
configs that you have.
is determined by the sim to real gap
is determined by the sim to real gap
too.
too.
Um kind of but it's easier because like
Um kind of but it's easier because like
domain randomization gives you a big
domain randomization gives you a big
wide distribution over the real like
wide distribution over the real like
over
over
it gives you a big wide distribution
it gives you a big wide distribution
that hopefully the real one's in there
that hopefully the real one's in there
somewhere.
Okay. Yeah. So, this is taking longer to
Okay. Yeah. So, this is taking longer to
learn with random action. So my guess is
learn with random action. So my guess is
that it's just like
something very janky going on here.
I don't agree with you.
I mean, something's got to give because
I mean, something's got to give because
the Sims are just abysmally horribly
the Sims are just abysmally horribly
slow.
Like, you're literally in the dark ages
Like, you're literally in the dark ages
where like nothing works because you
where like nothing works because you
have no data.
This right here is running at less than
This right here is running at less than
5% of the speed of um you know my more
5% of the speed of um you know my more
complex M training runs
and like a hundth of the speed of the
and like a hundth of the speed of the
faster ones
for like a really easy task. like a task
for like a really easy task. like a task
that when you look at the task and you
that when you look at the task and you
assess the difficulty of the task like
assess the difficulty of the task like
it's trivial.
it's trivial.
You have this being this slow for a
You have this being this slow for a
problem that is trivial.
And this is the thing that we found with
And this is the thing that we found with
reinforcement learning as well. Stuff
reinforcement learning as well. Stuff
feels really difficult because it's
feels really difficult because it's
slow. That's it.
slow. That's it.
It's like this is why you had like
It's like this is why you had like
generations of RL students like working
generations of RL students like working
on Pong and stuff, right? Like, well,
on Pong and stuff, right? Like, well,
was ponging hard? No, it was just slow.
You can't optimize like all the all the
You can't optimize like all the all the
experiment settings, all the
experiment settings, all the
hyperparameters suck, okay? All the
hyperparameters suck, okay? All the
environment settings suck, okay? Like
environment settings suck, okay? Like
literally nothing is optimized correctly
literally nothing is optimized correctly
at all because you can't run enough
at all because you can't run enough
experiments because it's slow.
Exact same thing happened in the rest of
Exact same thing happened in the rest of
RL.
like a one one same playbook.
How to best understand config from
How to best understand config from
command. Yeah. So, Neptune will give you
command. Yeah. So, Neptune will give you
logs and you can see which experiments
logs and you can see which experiments
are the best and then you can kind of
are the best and then you can kind of
just pick one and grab the config
just pick one and grab the config
parameters from there.
I mean
annoying, man.
It's like I can't even really like the
It's like I can't even really like the
thing is I can't even really get into
thing is I can't even really get into
this type of stuff I'd want to do with
this type of stuff I'd want to do with
the way this is set up. It's like not
the way this is set up. It's like not
intended for the type of stuff I want to
intended for the type of stuff I want to
do.
We have any war sims? Uh yeah, the one I
We have any war sims? Uh yeah, the one I
was working on the other Okay.
At least do this.
didn't sacrifice the realism.
didn't sacrifice the realism.
Let me see how I can convince you with
Let me see how I can convince you with
this
quite
quite
like literally this is all I'm asking
like literally this is all I'm asking
for. Okay, take literally every single
for. Okay, take literally every single
physics parameter in here, right?
physics parameter in here, right?
like every single like you know force
like every single like you know force
parameter, gravity, inertia like just
parameter, gravity, inertia like just
take all the parameters
take all the parameters
okay
okay
and then define some like percentage
and then define some like percentage
based interval over them.
We should be able to tune the magnitude
We should be able to tune the magnitude
of that interval.
of that interval.
And then all the other parameters that's
And then all the other parameters that's
like solver fidelity and updates and all
like solver fidelity and updates and all
that stuff should also go into the tuner
and then we should figure it out
and then we should figure it out
automatically and algorithmically.
Roboticists are not doing that at the
Roboticists are not doing that at the
moment.
moment.
They're kind of just assuming that they
They're kind of just assuming that they
just make the thing as slow as human
just make the thing as slow as human
like un like as slow as they can
like un like as slow as they can
possibly bear
possibly bear
and then they just run their really slow
and then they just run their really slow
experiments and hope it works.
Horrible contact sim
possibly.
Is that a hardware thing though? Like
Is that a hardware thing though? Like
can't you get around that with other
can't you get around that with other
types of grippers?
can't parameterize
the dynamics are straight. I don't see
the dynamics are straight. I don't see
any fluid dynamics or cloth draping
any fluid dynamics or cloth draping
though is the thing. Like look, I don't
though is the thing. Like look, I don't
see fluid dynamics. I don't see cloth.
see fluid dynamics. I don't see cloth.
I see ultra basic rigid body physics,
I see ultra basic rigid body physics,
right? With like a literally trivial
right? With like a literally trivial
task.
task.
Hang on, let me show you the one that
Hang on, let me show you the one that
I'm looking at.
I'm looking at.
It's like pig cube.
Okay, if you want to think about like in
Okay, if you want to think about like in
terms of how complex like to learn this
terms of how complex like to learn this
behavior from first principles
behavior from first principles
in my head, this is like pong.
This is like nothing.
We kind of just get sensor data.
We kind of just get sensor data.
Okay. Okay. Easier than pong. I mean,
Okay. Okay. Easier than pong. I mean,
like
seriously, isn't it?
You're picking up a thing.
What is the decision making involved
What is the decision making involved
with picking up a thing?
with picking up a thing?
It's like a few joint angles.
It's like a few joint angles.
Can't you spawn more than 4K? Yes, you
Can't you spawn more than 4K? Yes, you
can. But here's the problem. The thing
can. But here's the problem. The thing
is really simple problems don't lend
is really simple problems don't lend
themselves to actually learning faster
themselves to actually learning faster
with more environments. Serial
with more environments. Serial
as the environment gets harder, you can
as the environment gets harder, you can
benefit from more parallel Ms. The thing
benefit from more parallel Ms. The thing
is this is just a very bad combination.
is this is just a very bad combination.
It's a simple problem that's really
It's a simple problem that's really
slow.
slow.
Simple and very slow.
Do you know how many Do you know how
Do you know how many Do you know how
long it takes us to learn pong in Puffer
long it takes us to learn pong in Puffer
Lib?
Lib?
3 seconds. It takes us three seconds to
3 seconds. It takes us three seconds to
learn a uh like a 20 score pong policy.
It has to learn pretty fast. Yeah. The
It has to learn pretty fast. Yeah. The
thing is it just doesn't learn fast.
thing is it just doesn't learn fast.
This number is 20 times too slow.
This number is 20 times too slow.
Actually, more than 20 times too slow.
Actually, more than 20 times too slow.
Pong trains 20 times faster than this on
Pong trains 20 times faster than this on
a sixth of the hardware. So, it's over
a sixth of the hardware. So, it's over
100x too slow. And I see the rewards
100x too slow. And I see the rewards
again. The rewards are very sly defined.
again. The rewards are very sly defined.
I will show you.
I will show you.
So, this is how they compute a dense
So, this is how they compute a dense
reward.
reward.
Okay,
Okay,
they have a reward for reaching the goal
they have a reward for reaching the goal
and then they have a is a grasping term.
and then they have a is a grasping term.
You only get a reward for placing the
You only get a reward for placing the
object if you're actually grasping it.
object if you're actually grasping it.
And then I have the static reward. These
And then I have the static reward. These
are state based rewards. So completely
are state based rewards. So completely
wrong. Uh but what we do is we take the
wrong. Uh but what we do is we take the
delta between this at this step and at
delta between this at this step and at
the previous step. So delta of reward at
the previous step. So delta of reward at
previous uh this step to previous step
previous uh this step to previous step
tells you progress towards goal and
tells you progress towards goal and
that's what we use as reward.
I don't know if we're getting hit by a
I don't know if we're getting hit by a
hurricane or what, but everything is
hurricane or what, but everything is
rattling and shaking.
rattling and shaking.
Those
are some pretty high speed winds.
Hopefully this doesn't take out the
Hopefully this doesn't take out the
stream. We'll see.
takes a lot longer to solve here.
Why do you do that? And what's wrong
Why do you do that? And what's wrong
with this reward?
with this reward?
The state based rewards
The state based rewards
when you get close to the goal, you'll
when you get close to the goal, you'll
just hover around it forever and you'll
just hover around it forever and you'll
not actually solve the task.
not actually solve the task.
This is a wrong way to define rewards.
This is a wrong way to define rewards.
And then what the roboticists do is they
And then what the roboticists do is they
modify generalized advantage estimation
modify generalized advantage estimation
to bootstrap on terminals.
to bootstrap on terminals.
We do instead is we just don't do that
We do instead is we just don't do that
and we use delta rewards and it's the
and we use delta rewards and it's the
same thing actually better because you
same thing actually better because you
actually get more fidelity out of your
actually get more fidelity out of your
reward signal.
It seems pretty easy. Well, we do solve
It seems pretty easy. Well, we do solve
it. It's just like it's proven annoying
it. It's just like it's proven annoying
to get it to solve faster.
Right.
I can kind of try to just guess a couple
I can kind of try to just guess a couple
things.
What if I just Yes.
Yeah, that's basically
Yeah, that's basically
the problem. The thing is it's not that
the problem. The thing is it's not that
my general like it's not that my
my general like it's not that my
advantage estimate is wrong, right? My
advantage estimate is wrong, right? My
advantage estimate is right. In general,
advantage estimate is right. In general,
the robotics version of it is wrong.
the robotics version of it is wrong.
At the very least, they have a
At the very least, they have a
specialized J that doesn't work with
specialized J that doesn't work with
other types of tasks. Mine will work
other types of tasks. Mine will work
with both provided you don't define the
with both provided you don't define the
reward the rewards insanely like Yes.
very weird to me how
very weird to me how
volatile the rewards are, you know.
I do wonder if this is a scaling issue.
This is way worse.
Makes sense.
Makes sense.
I see.
lower learning rate.
What if you continuously increase the
What if you continuously increase the
reward?
So because you normally you clip it off
So because you normally you clip it off
at the last state, it's going to rather
at the last state, it's going to rather
it would rather stay right next to the
it would rather stay right next to the
goal, right? Then get reset to the start
goal, right? Then get reset to the start
where it's going to get very low go like
where it's going to get very low go like
a very low reward.
Yeah. So, what's it's going to do is
Yeah. So, what's it's going to do is
it's going to like go right next to the
it's going to like go right next to the
goal and then just sit there. And then
goal and then just sit there. And then
if it's a smart, eventually it'll figure
if it's a smart, eventually it'll figure
out to touch the goal at the very end.
out to touch the goal at the very end.
But probably not.
Add an action to the termination. The
Add an action to the termination. The
way easier solution, right, is to just
way easier solution, right, is to just
not define your rewards like this
not define your rewards like this
is not this is not a good way to define
is not this is not a good way to define
rewards at all anyways because like you
rewards at all anyways because like you
compress the entire signal as well.
compress the entire signal as well.
The delta between one state and another
The delta between one state and another
state might be very small in terms of
state might be very small in terms of
difference in reward.
interestingly doing very badly.
Comment gamma
a ridiculous gamma though. I guess it's
a ridiculous gamma though. I guess it's
because it's a dense reward.
Never trust no RL algorithm to work on a
Never trust no RL algorithm to work on a
new endeavor.
new endeavor.
The thing is ours actually does like we
The thing is ours actually does like we
got it working on this pretty quickly.
got it working on this pretty quickly.
As soon as we fix the rewards, we have
As soon as we fix the rewards, we have
it working on this new end in a new
it working on this new end in a new
domain that we don't do like out of the
domain that we don't do like out of the
box.
box.
Uh it's just that the env is very slow
Uh it's just that the env is very slow
and we would like it to be faster. So
and we would like it to be faster. So
I'm looking for settings where it's
I'm looking for settings where it's
faster and we can still learn.
But yeah, actually try try the Puffer 3
But yeah, actually try try the Puffer 3
trainer. actually works out of the box
trainer. actually works out of the box
on most stuff. It's kind of crazy.
I wonder if I ought to just like sync
I wonder if I ought to just like sync
this up to the environment resets, you
this up to the environment resets, you
know?
I think you can.
Okay. So, it suddenly is at least like
Okay. So, it suddenly is at least like
somewhat better.
Yeah. But it looks like the hyper prams
Yeah. But it looks like the hyper prams
we swept from before are like overall
we swept from before are like overall
reasonable.
reasonable.
not terminating the end in case at all.
Then you'll stop learning because all
Then you'll stop learning because all
the agents will get to the goal position
the agents will get to the goal position
and you just don't reset. You're getting
and you just don't reset. You're getting
worthless data.
Okay, let's hope my roof is really good.
Okay, let's hope my roof is really good.
cuz uh this is a gale outside. My gosh.
cuz uh this is a gale outside. My gosh.
Holy.
My microphone should mostly be filtering
My microphone should mostly be filtering
it, but it's uh rain on a metal roof.
it, but it's uh rain on a metal roof.
So,
very loud. Ah, look at this.
very loud. Ah, look at this.
Okay.
So this is at least relatively cleaner
and nice and stable.
Now my question is with these more
Now my question is with these more
general hyperparams
general hyperparams
where we haven't like maxed the learning
where we haven't like maxed the learning
rate
rate
can we go up to
can we go up to
4096NS
and 32
and 32
32 384
this work now all of a sudden.
And to be safe, I'm getting a very high
And to be safe, I'm getting a very high
quality conversation on RL, which I
quality conversation on RL, which I
can't get at all in the day.
can't get at all in the day.
Yeah, I mean, this is the place. Look, I
Yeah, I mean, this is the place. Look, I
don't even really do robotics. We're
don't even really do robotics. We're
kind of just doing this as like, yeah,
kind of just doing this as like, yeah,
robotics is another RLN. And it's not
robotics is another RLN. And it's not
like we're failing, right? Like the
like we're failing, right? Like the
stuff works. It's just that I would like
stuff works. It's just that I would like
I'd like to find some settings to
I'd like to find some settings to
massively improve the speed here. But
massively improve the speed here. But
the thing is like my intuition here is
the thing is like my intuition here is
just that like this thing has been I've
just that like this thing has been I've
seen this all the time in the rest of
seen this all the time in the rest of
RL, right? You have like an environment
RL, right? You have like an environment
that's been very heavily tuned to work
that's been very heavily tuned to work
in like the defaults. It doesn't mean
in like the defaults. It doesn't mean
that there aren't other ways to set it
that there aren't other ways to set it
up. It's just very annoying and like a
up. It's just very annoying and like a
lot of work. My guess is that like if
lot of work. My guess is that like if
you actually have a very heavily
you actually have a very heavily
randomized setting with way way lower
randomized setting with way way lower
fidelity uh solver params,
fidelity uh solver params,
my estimate here is that it actually
my estimate here is that it actually
will do just as well, if not better.
will do just as well, if not better.
So that's what I'd like to do, but it's
So that's what I'd like to do, but it's
kind of becoming clear that it's pretty
kind of becoming clear that it's pretty
tough to set that type of stuff up.
tough to set that type of stuff up.
I at least want to see if we can like
I at least want to see if we can like
get something that's meaningful though,
get something that's meaningful though,
like some sort of meaningful improvement
like some sort of meaningful improvement
to this thing.
We will see.
This is like the reward is just not
This is like the reward is just not
quite doing it for us.
Okay, let me think about this.
I could keep the mini batch small,
I could keep the mini batch small,
right?
How would you define the reward for this
How would you define the reward for this
problem? Can you elaborate on this?
problem? Can you elaborate on this?
Because I'm bumping this into a spot.
Because I'm bumping this into a spot.
Well, I I actually have it. So, the
Well, I I actually have it. So, the
reward I showed you is not what I'm
reward I showed you is not what I'm
training on. That's their original.
training on. That's their original.
Okay. The reward that I have is
Okay. The reward that I have is
literally just that reward that I showed
literally just that reward that I showed
you computed at the current time step
you computed at the current time step
minus that reward at the previous time
minus that reward at the previous time
step.
step.
So, that tells you progress towards
So, that tells you progress towards
goal. It's unfarmmable. Because if you
goal. It's unfarmmable. Because if you
stay at a point, you get zero. Can't go
stay at a point, you get zero. Can't go
back and forth because that just
back and forth because that just
averages out to be zero. You get a
averages out to be zero. You get a
positive when you go towards the the
positive when you go towards the the
goal. This will work for like any of the
goal. This will work for like any of the
dense reward signals that the
dense reward signals that the
roboticists have set up. So like
roboticists have set up. So like
literally they were a threeline change
literally they were a threeline change
away from having it here.
Little bit off. No.
This doesn't work. Let's hypothesize.
Hypothesize a little bit.
Hypothesize a little bit.
Start with
96.
is the original reward good at all?
is the original reward good at all?
Uh, it's it contains good information,
Uh, it's it contains good information,
but it's not presented correctly. Let's
but it's not presented correctly. Let's
put it that way. So, literally, if you
put it that way. So, literally, if you
just take their reward signal and make
just take their reward signal and make
it delta instead of absolute, it works.
You don't need to do IRL like inverse
You don't need to do IRL like inverse
RL.
RL.
The thing I've given works, right? This
The thing I've given works, right? This
this does work.
We take some more steps than they do to
We take some more steps than they do to
solve it, but like we're literally
solve it, but like we're literally
training one update epoch as well. I
training one update epoch as well. I
haven't messed with that a whole bunch
haven't messed with that a whole bunch
yet.
So,
you have a totally new end. How would
you have a totally new end. How would
you design one?
you design one?
Uh, I can't really comment on robotics
Uh, I can't really comment on robotics
because I don't do like a ton of
because I don't do like a ton of
robotics. their rewards are like they're
robotics. their rewards are like they're
very manual and kind of a pain, but like
very manual and kind of a pain, but like
I see why they do the things. They're
I see why they do the things. They're
just posed wrong. So when I look when I
just posed wrong. So when I look when I
design new environments and I have
design new environments and I have
rewards, I usually do the simplest thing
rewards, I usually do the simplest thing
possible, right? Like I start with just
possible, right? Like I start with just
success and then if it's like obvious
success and then if it's like obvious
that that's way way way too hard to run
that that's way way way too hard to run
into, which is like basically if you
into, which is like basically if you
take random actions and you're never
take random actions and you're never
going to get the success condition, you
going to get the success condition, you
can't just train on that. So then I try
can't just train on that. So then I try
to do like the most lighthanded
to do like the most lighthanded
intermediate reward I possibly can,
intermediate reward I possibly can,
usually some sparse thing, right? And
usually some sparse thing, right? And
then if I end up with more complex that
then if I end up with more complex that
have like three or four different reward
have like three or four different reward
coefficients, uh then I just like I
coefficients, uh then I just like I
think okay, here are three or four
think okay, here are three or four
important things in the environment. I
important things in the environment. I
don't know what their relative waiting
don't know what their relative waiting
is and I put them into a hyperparameter
is and I put them into a hyperparameter
sweep.
Welcome.
Welcome.
Thank you. Yeah, I actually was just
Thank you. Yeah, I actually was just
going through all the YouTube comments
going through all the YouTube comments
uh this morning. I was like, "Hey,
uh this morning. I was like, "Hey,
actually, surprisingly, YouTube is way,
actually, surprisingly, YouTube is way,
way, way happier about all this stuff
way, way happier about all this stuff
than X's." X, they're always mad about
than X's." X, they're always mad about
everything. People here are pretty
everything. People here are pretty
chill.
Okay. Yay. This actually kind of works.
Okay. Yay. This actually kind of works.
Is it better?
It's like kind of good.
This is like kind of good.
Okay.
Oh, actually this is quite good, isn't
Oh, actually this is quite good, isn't
it? Because if I do this,
look at that.
look at that.
And that's our fastest solve yet. Two
And that's our fastest solve yet. Two
minutes.
Let me think if I can make sense of what
Let me think if I can make sense of what
I even just did. I kind of just guessed.
So I reduce the total batch size by a
So I reduce the total batch size by a
factor of four.
I can try eight.
I can try eight.
This is where things start to get
This is where things start to get
sketchy.
sketchy.
I can try eight. If this works better,
I can try eight. If this works better,
it just means the problem's trivial, by
it just means the problem's trivial, by
the way,
the way,
which is kind of expected. Like they use
which is kind of expected. Like they use
four in uh the original. Like this
four in uh the original. Like this
parameter they use like four steps
parameter they use like four steps
which is crazy like absolutely crazy
why don't you use different algorithm
why don't you use different algorithm
what do you mean
we have um
we have um
at least as far as online learning goes
at least as far as online learning goes
like we kind of have the best thing
like we kind of have the best thing
already in puffer
already in puffer
we don't like hop between a bunch of
we don't like hop between a bunch of
different algor algorithms. We just have
different algor algorithms. We just have
the best algorithm.
this uh re this is also not exactly PO
this uh re this is also not exactly PO
like the changes that we have in our
like the changes that we have in our
implementation are substantial enough to
implementation are substantial enough to
consider it a new algorithm and like in
consider it a new algorithm and like in
performance it's a generational
performance it's a generational
advancement over PO and I mean that for
advancement over PO and I mean that for
real this time not like all the papers
real this time not like all the papers
that claim to be better and like
that claim to be better and like
actually aren't this one's like for real
actually aren't this one's like for real
for real better.
Sounds like a paper material.
Sounds like a paper material.
I'm so sick of papers.
I'm so sick of papers.
We have some blogs on it on X if you
We have some blogs on it on X if you
want all the details. It's all open
want all the details. It's all open
source.
The idea that every time you come up
The idea that every time you come up
with something useful, you have it has
with something useful, you have it has
to be exactly eight dense pages of
to be exactly eight dense pages of
really boring content is like it takes a
really boring content is like it takes a
year to review is like kind of
year to review is like kind of
ludicrous.
Get to a place to make a pitch. Yeah, I
Get to a place to make a pitch. Yeah, I
do that. I mean, like with I have like a
do that. I mean, like with I have like a
good chunk of the RL audience on X as
good chunk of the RL audience on X as
like already. If you look at like the
like already. If you look at like the
biggest pure RL accounts and then you
biggest pure RL accounts and then you
look at the size of mine, it's they're
look at the size of mine, it's they're
pretty close to comparable.
pretty close to comparable.
Particularly because I really don't have
Particularly because I really don't have
followers from like other stuff, right?
followers from like other stuff, right?
They're like professors and things that
They're like professors and things that
have like they post politics and other
have like they post politics and other
stuff and they have larger accounts, but
stuff and they have larger accounts, but
like as for people that are just there
like as for people that are just there
because they post pure RL, it's pretty
because they post pure RL, it's pretty
good.
good.
You think that those people on X are
You think that those people on X are
real?
real?
I mean, yeah. I get messages from
I mean, yeah. I get messages from
researchers all the time.
Okay, so I was kind of expecting this to
Okay, so I was kind of expecting this to
work. The fact that it doesn't is
work. The fact that it doesn't is
actually kind of a good thing.
actually kind of a good thing.
What's What license does Puffer use? MIT
What's What license does Puffer use? MIT
free.
The only thing that we sell is we sell
The only thing that we sell is we sell
uh we sell our services, right? You can
uh we sell our services, right? You can
use all our stuff for free, but if you
use all our stuff for free, but if you
you have an RL problem and you're having
you have an RL problem and you're having
a hard time with it in industry, you can
a hard time with it in industry, you can
hire us to help. That includes like
hire us to help. That includes like
support and features around Puffer Lib
support and features around Puffer Lib
that includes direct work on your
that includes direct work on your
problem, custom simulation work, all
problem, custom simulation work, all
that type of stuff.
sement filtering
sement filtering
topics off.
Okay, so we're at 16. Horizon was the
Okay, so we're at 16. Horizon was the
good one.
I guess we just see if we can mess with
I guess we just see if we can mess with
fidelity now, right?
fidelity now, right?
Does this still learn?
We should in theory be a little bit more
We should in theory be a little bit more
robust to fidelity now because we have
robust to fidelity now because we have
massive batches.
Hell stone.
We have a a good number of folks on
We have a a good number of folks on
YouTube today. Hello, welcome.
YouTube today. Hello, welcome.
If you're new around here, this is just
If you're new around here, this is just
all sorts of reinforcement learning
all sorts of reinforcement learning
research done live. It's all open
research done live. It's all open
source. If you want to see a bunch of
source. If you want to see a bunch of
demos that are not me just staring at uh
demos that are not me just staring at uh
training graphs because we're doing
training graphs because we're doing
fiddly robotics today. We have all sorts
fiddly robotics today. We have all sorts
of stuff at puffer.ai,
of stuff at puffer.ai,
click ocean. You can see all these
click ocean. You can see all these
different demo ming
demo to games
demo to games
like this this type of a thing. And the
like this this type of a thing. And the
agents all run in your browser. And many
agents all run in your browser. And many
of these were contributed by uh brand
of these were contributed by uh brand
new people with like zero RL experience
new people with like zero RL experience
coming in.
coming in.
So, if you're interested in getting
So, if you're interested in getting
involved in building crazy stuff Oh,
involved in building crazy stuff Oh,
yeah. I like this one. I built this one.
yeah. I like this one. I built this one.
Each puffer is a different agent.
Each puffer is a different agent.
If you want to get involved with this,
If you want to get involved with this,
you can join the Discord, which is just
you can join the Discord, which is just
discord.gg/puffer.
discord.gg/puffer.
You can also help us out for free by
You can also help us out for free by
starring the GitHub. Also, follow me on
starring the GitHub. Also, follow me on
X for more.
X for more.
If I have stalled long enough, maybe the
If I have stalled long enough, maybe the
policy will have trained.
kind of
kind of
is this stable is my question.
If that's a stable 400k
does take longer
does take longer
steps.
Where's RL used in industry side aside
Where's RL used in industry side aside
from releasing things R1 or is it just
from releasing things R1 or is it just
catching on? It's a lot of random
catching on? It's a lot of random
problems.
problems.
Like RL is basically any anytime you
Like RL is basically any anytime you
have a sim and like a fiddly unintuitive
have a sim and like a fiddly unintuitive
optimization problem.
We've done all sorts of like weird
We've done all sorts of like weird
random things. The thing is like RL has
random things. The thing is like RL has
just kind of been cast to the wayside
just kind of been cast to the wayside
because nobody could get it to work. But
because nobody could get it to work. But
like nobody could get it to work because
like nobody could get it to work because
everybody did all the engineering wrong.
everybody did all the engineering wrong.
So just in the last year we've made so
So just in the last year we've made so
many things a thousand times faster.
many things a thousand times faster.
We've been able to like make some
We've been able to like make some
fundamental algorithm breakthroughs. Uh
fundamental algorithm breakthroughs. Uh
we've been able to like really really
we've been able to like really really
hone in and optimize a bunch of things.
hone in and optimize a bunch of things.
And now it it feels like a different
And now it it feels like a different
field at the very least. If nothing
field at the very least. If nothing
else, it feels like a totally different
else, it feels like a totally different
field from
Okay, this is actually pretty solid.
Okay, this is actually pretty solid.
We're at like two minute solve. Super
We're at like two minute solve. Super
low fidelity.
Can we crank up control?
Can we crank up control?
Demos on the website are fun to play
Demos on the website are fun to play
with. Thank you.
with. Thank you.
We had to do quite a bit of stuff to get
We had to do quite a bit of stuff to get
those to work.
those to work.
So those all of those agents those are
So those all of those agents those are
running like pure C versions of the
running like pure C versions of the
neural nets that we trained in PyTorch.
neural nets that we trained in PyTorch.
So we take the PyTorch weights, we load
So we take the PyTorch weights, we load
them into like this single file C
them into like this single file C
library that we made and then compile
library that we made and then compile
all that to web assembly including the
all that to web assembly including the
environments.
environments.
What do you think about VALA's?
What do you think about VALA's?
LA
LA
VMs or VAS.
VMs or VAS.
What's a VA?
What's a VA?
Very large action models. I don't know.
action models. Eh,
action models. Eh,
if you get enough data for a specific
if you get enough data for a specific
enough problem, sure. I kind of really
enough problem, sure. I kind of really
don't care what's happening in the world
don't care what's happening in the world
of like let's train massive like
of like let's train massive like
ridiculously sized models that are super
ridiculously sized models that are super
slow
slow
because like
because like
it's kind of funny that we are we're
it's kind of funny that we are we're
able to like solve like individual
able to like solve like individual
problems in seconds with these like
problems in seconds with these like
absolutely tiny models that you can just
absolutely tiny models that you can just
deploy anywhere.
deploy anywhere.
And like obviously this is a lot newer
And like obviously this is a lot newer
in the sense that it hasn't had billions
in the sense that it hasn't had billions
of dollars of investment poured into it,
of dollars of investment poured into it,
but like you can kind of see just from
but like you can kind of see just from
the progress we've made in the last year
the progress we've made in the last year
where some of this is going.
Okay. I mean, so this now works.
So this isn't a thing that they thought
So this isn't a thing that they thought
I would be able to get to work already.
I would be able to get to work already.
That's exciting.
That's exciting.
Can I do
let me think about this a little bit
let me think about this a little bit
carefully
BLM with imitation.
BLM with imitation.
Any wisdom on metalarning
Any wisdom on metalarning
hyperparameters?
I've been going off the heristic that if
I've been going off the heristic that if
puffer doesn't do it then it can't be
puffer doesn't do it then it can't be
great. Yes, except there's one small
great. Yes, except there's one small
caveat. Finn, there was one quirk with
caveat. Finn, there was one quirk with
our hyperparam sweep algorithm that I
our hyperparam sweep algorithm that I
could not fix uh in time for release. If
could not fix uh in time for release. If
you see that your problem is forcing all
you see that your problem is forcing all
the runs to be the minimum number of
the runs to be the minimum number of
time steps
time steps
that you specify in the sweep, you might
that you specify in the sweep, you might
want to try uh with one of like the down
want to try uh with one of like the down
sample equals zero configs.
sample equals zero configs.
And what that does is that basically it
And what that does is that basically it
tells the hyperparam sweep algorithm to
tells the hyperparam sweep algorithm to
not try to learn using the entire
not try to learn using the entire
training curve to just use the end
training curve to just use the end
point. And that's kind of needed for
point. And that's kind of needed for
some of the environments.
That's the only stumbling block.
Let me think about this a little bit
Let me think about this a little bit
more principled if I can. Um,
if I just hit 40 here,
this do anything.
Let's actually see what this does. I'm
Let's actually see what this does. I'm
curious.
I think this just gives you like fake
I think this just gives you like fake
additional fake frames.
gives you a lot of fake frames though
like approaching a respectable training
like approaching a respectable training
speed
hyper pram free RL ever be possible.
I will say this, the defaults that we
I will say this, the defaults that we
have in Puffer Lib work on like almost
have in Puffer Lib work on like almost
all the environments.
Like they work out of the box on a huge
Like they work out of the box on a huge
number of problems, but we're getting
number of problems, but we're getting
closer to that.
closer to that.
I think we could get to a point where
I think we could get to a point where
like we have defaults that work pretty
like we have defaults that work pretty
darn well out of the box on almost
darn well out of the box on almost
everything. The main thing, the main
everything. The main thing, the main
thing that is not solved yet is gamma.
thing that is not solved yet is gamma.
So generalized advantage estimation
So generalized advantage estimation
imposes a horizon on your tasks and it's
imposes a horizon on your tasks and it's
really screwy. So we would have to like
really screwy. So we would have to like
we'd basically have to come up with a
we'd basically have to come up with a
replacement for generalized advantage
replacement for generalized advantage
estimation.
That was through a search. Yeah. But the
That was through a search. Yeah. But the
thing is we did the search once. All
thing is we did the search once. All
right. We did the search on like
right. We did the search on like
breakout and those same parameters work
breakout and those same parameters work
on like at least six or eight of the
on like at least six or eight of the
environments. It just works.
environments. It just works.
I think more like 12 of them actually.
I think more like 12 of them actually.
Really the only ones it doesn't work
Really the only ones it doesn't work
like the bigger models need different
like the bigger models need different
learning rates and you can show that you
learning rates and you can show that you
need to decrease the learning rate and
need to decrease the learning rate and
then there's some environments that need
then there's some environments that need
different gamas.
model based dynamics based agent can
model based dynamics based agent can
solve the hyper prim. No, they can't.
solve the hyper prim. No, they can't.
How do you figure?
How do you figure?
What part about having a model solves
What part about having a model solves
hyperparameter tuning?
hyperparameter tuning?
If you quote the dreamer paper, I'm
If you quote the dreamer paper, I'm
going to submit a different academic
going to submit a different academic
paper back at you. Fair warning.
Mr. Q. I've been meaning to look at
Mr. Q. I've been meaning to look at
that. It's like big paper. Been meaning
that. It's like big paper. Been meaning
to look at that
model. Well,
model. Well,
there's something to modelbased RL, I
there's something to modelbased RL, I
think. But the thing is like
think. But the thing is like
it's like the rest of RL where all the
it's like the rest of RL where all the
science is done absolutely horribly. So,
science is done absolutely horribly. So,
it's kind of hard to tell.
So, this thing just doesn't do very
So, this thing just doesn't do very
well. Um,
okay. Can I Can I fix this?
You go find the thing that he told me to
You go find the thing that he told me to
adjust.
Arm PD joint delta pause.
arm
arm
point delta pause.
point delta pause.
Okay, so this is what he told me
and this is what he told me to mess
and this is what he told me to mess
with. So let me see if this does
with. So let me see if this does
anything.
efficient zero v2.
I actually think some of them use zero
I actually think some of them use zero
line of work makes more sense.
Like that makes more sense to me. Some
Like that makes more sense to me. Some
of it does.
I think a lot of the a lot of the
I think a lot of the a lot of the
algorithm work I want to do will look at
algorithm work I want to do will look at
off uh off policy as well as model based
off uh off policy as well as model based
but like we kind of have to just throw
but like we kind of have to just throw
puffer liib on a bunch of different
puffer liib on a bunch of different
applications first, right?
applications first, right?
Gain some more industry traction.
So, if this doesn't work, then there's
So, if this doesn't work, then there's
something that's not behaving as is uh
something that's not behaving as is uh
as anticipated
as anticipated
because technically
because technically
if we double the SIMs, like if we double
if we double the SIMs, like if we double
the control frequency and we double the
the control frequency and we double the
actions, it should work just the same.
I'm going to run this first to make sure
I'm going to run this first to make sure
it doesn't actually work.
[Music]
There is a difference,
but it's not like massive.
reward really jumps all over.
Yeah. So this is very similar to the
Yeah. So this is very similar to the
ABS.
How can you run multiple sweeps in
How can you run multiple sweeps in
parallel
parallel
like for different environments
like for different environments
and just launch different processes on
and just launch different processes on
different GPUs?
different GPUs?
Uh we don't have the ability to use
Uh we don't have the ability to use
multiple GPUs in the same sweep yet.
multiple GPUs in the same sweep yet.
That is something that we should add. We
That is something that we should add. We
do have the ability to do multiGPU
do have the ability to do multiGPU
training.
training.
Uh, we don't have the ability to do use
Uh, we don't have the ability to do use
like multiple GPUs where each GPU
like multiple GPUs where each GPU
contributes to the same sweep.
contributes to the same sweep.
That's one of the things I've been
That's one of the things I've been
meaning to add.
different process.
different process.
New man
out of here. See you. Thank you.
out of here. See you. Thank you.
I do this pretty much every day.
See, this is obnoxious because if we
See, this is obnoxious because if we
could get this to work
like this is the theoretical max SPS
like this is the theoretical max SPS
that I've obtained with this, right?
like pretty decent.
YouTube comments pretty slow.
YouTube comments pretty slow.
Yeah, the latency is a little better on
Yeah, the latency is a little better on
Twitch. Not much I can really do about
Twitch. Not much I can really do about
YouTube being YouTube.
Let me just try one like one weird thing
Let me just try one like one weird thing
just in case.
just in case.
like just in case
which is that technically you have to
which is that technically you have to
increase gamma
the way it's defined here
the way it's defined here
because you create a longer horizon
because you create a longer horizon
problem.
turn kind of just gets stuck.
Irritating
Yeah. Dang.
Okay.
Go back to the original 20.
Go back to the original 20.
I do like this.
Let's see if this is still stable.
See
if we can send Stone a cool policy.
Okay.
You managed to train 2048 and put it on
You managed to train 2048 and put it on
the website. I have not done that yet.
the website. I have not done that yet.
Yanick,
Yanick,
I need to do that though because that is
I need to do that though because that is
a cool environment. I've been
a cool environment. I've been
sidetracked by robotics and other
sidetracked by robotics and other
things.
things.
do have a cool new article up though
if you want to figure out uh if you want
if you want to figure out uh if you want
to hear how we did RL on a pabyte of
to hear how we did RL on a pabyte of
data one server
data one server
here's a nice write up here on X
wrote that this morning
that's up Here
I did
I did
if this is stable.
You have fully integrated multiGPU. Yes,
You have fully integrated multiGPU. Yes,
we do.
You're looking at it here.
you ve the no that's not how you do it
you ve the no that's not how you do it
and that's not how you'd want to do it.
and that's not how you'd want to do it.
If you did it that way then you would
If you did it that way then you would
like you'd multiply the overhead of any
like you'd multiply the overhead of any
CPU operation by the number of GPUs. So
CPU operation by the number of GPUs. So
we use torch run which launches
we use torch run which launches
independent processes per trainer and
independent processes per trainer and
then DDP just averages the gradients.
Imagine
this should right.
Not bad.
Does it scale two GPUs double as fast?
Does it scale two GPUs double as fast?
Depends on your environment speed.
Depends on your environment speed.
If you from the article, right, 2.2
If you from the article, right, 2.2
million training steps per second for a
million training steps per second for a
neural MMO 3 compared to like 400
neural MMO 3 compared to like 400
between 400 and 500k on one GPU.
Pretty good scaling.
To be fair, that's not even completely
To be fair, that's not even completely
fair because uh there's like some CPU
fair because uh there's like some CPU
conflicts as well doing neural MMO like
conflicts as well doing neural MMO like
that
that
overhead there as well.
overhead there as well.
Depends on your environment can be
Depends on your environment can be
linear.
Okay. So,
2 minutes 30 seconds.
Amusingly, we've managed to take more
Amusingly, we've managed to take more
steps to solve in about the same amount
steps to solve in about the same amount
of time.
of time.
But the uh the hope here is that this
But the uh the hope here is that this
will now allow us to
will now allow us to
do better.
We wanted to preserve the number of
We wanted to preserve the number of
gradient updates,
gradient updates,
make learning more stable.
make learning more stable.
What happens if I do like this?
What happens if I do like this?
Let's do 16384
update
update
2.
2.
That should be the same number of
That should be the same number of
updates to the policy
but they but more stable.
Welcome to all the uh stream's been
Welcome to all the uh stream's been
doing well the last couple of days.
Welcome to all the new folks here.
We'll of course have to actually look at
We'll of course have to actually look at
the policy. I don't want to run too many
the policy. I don't want to run too many
more of these because I got to run for
more of these because I got to run for
dinner uh a little bit before 6.
dinner uh a little bit before 6.
Like to be able to look at it.
Good work. Thank you.
I'd like some of these articles on X to
I'd like some of these articles on X to
do better. I mean, they they take quite
do better. I mean, they they take quite
a bit of time to write.
Amusingly, like the literally like AI
Amusingly, like the literally like AI
clipped shorts out of the stream have
clipped shorts out of the stream have
done like surprisingly well on YouTube.
done like surprisingly well on YouTube.
They get like over a thousand views. I
They get like over a thousand views. I
think one of them got like 2,000
think one of them got like 2,000
something. The other got a thousand. are
something. The other got a thousand. are
just like literally clips from this
just like literally clips from this
random stuff I do. I don't know what's
random stuff I do. I don't know what's
up with the shorts algorithm, but I
up with the shorts algorithm, but I
don't know if I if I do anything like
don't know if I if I do anything like
interesting like an explanation of
interesting like an explanation of
something or other, I might just start
something or other, I might just start
adding those.
So, are we not stable here?
We're comparing this one
We're comparing this one
to this one.
We have something.
We have something.
We have this metric.
We have this metric.
Huh.
Yeah. See, this is one of those weird
Yeah. See, this is one of those weird
things I can't even explain.
things I can't even explain.
This should pretty much always be
This should pretty much always be
strictly better.
strictly better.
Oh, no. It does track. Hang on. It's
Oh, no. It does track. Hang on. It's
actually starting to track this.
Okay, so it's pretty close to
Okay, so it's pretty close to
comparable.
comparable.
Uh, which means that we're not really
Uh, which means that we're not really
benefiting from larger mini batches.
benefiting from larger mini batches.
Played with a couple config brims. Got a
Played with a couple config brims. Got a
good run in 15 minutes training
good run in 15 minutes training
multiGPU.
multiGPU.
Solid.
Solid.
Yeah, we really need to do multiGPU
Yeah, we really need to do multiGPU
sweeps. I think Spencer, I just haven't
sweeps. I think Spencer, I just haven't
had the chance to add it. It kind of
had the chance to add it. It kind of
takes a little bit of work.
Do this instead. So, this is doubling
Do this instead. So, this is doubling
the number of updates. It's a very
the number of updates. It's a very
different thing. This will be the last
different thing. This will be the last
run we'll do. After this, we'll like
run we'll do. After this, we'll like
well the last new run we'll do. After
well the last new run we'll do. After
this, we'll train our baseline. Uh we'll
this, we'll train our baseline. Uh we'll
grab the policy. We'll like output some
grab the policy. We'll like output some
rendering and we'll call it
Alternative title for stream will be
Alternative title for stream will be
watch as I horribly break break all of
watch as I horribly break break all of
your Sims roboticists
your Sims roboticists
in order to make them actually run quick
in order to make them actually run quick
as they should.
18 minutes. Got six people, not five.
18 minutes. Got six people, not five.
Yeah.
What do you mean you forgot six, not
What do you mean you forgot six, not
five? How's that affect the time?
Change the access to real time and you
Change the access to real time and you
call it a day.
Okay, this is getting lift.
Okay, this is getting lift.
Yeah, now that we have all the stuff
Yeah, now that we have all the stuff
like total chaos or update epox does
like total chaos or update epox does
something.
It's kind of hard to see be, but this is
It's kind of hard to see be, but this is
the the green curve down here. This was
the the green curve down here. This was
our baseline before,
our baseline before,
and now it looks like we're getting lift
and now it looks like we're getting lift
here.
Hey, there we go.
That how far away you do from doing
That how far away you do from doing
something like OpenAI 5 if you can
something like OpenAI 5 if you can
already train for such a long time. Two
already train for such a long time. Two
problems with Open AI5. One, no matter
problems with Open AI5. One, no matter
what you do, you will need 50,000 CPU
what you do, you will need 50,000 CPU
cores to run Dota. All right.
cores to run Dota. All right.
uh and two
uh and two
the observation structure is really
the observation structure is really
obnoxious to make fast.
So I think that the result like I like I
So I think that the result like I like I
wrote in the paper I think that the
wrote in the paper I think that the
neural MMO result is a better result
neural MMO result is a better result
than emergent tool.
than emergent tool.
Dota is still like Dota is still pretty
Dota is still like Dota is still pretty
hard.
This is a very good result.
We're happy with this.
I go to four
99.5 to 99.6 Six
99.9999
99.9999
required Spencer
make the thing be perfect Everything.
Literally need to be able to just run
Literally need to be able to just run
the policy on an actual car and have it
the policy on an actual car and have it
somehow work.
There's a car that's physically in the
There's a car that's physically in the
way and there's another one behind you.
way and there's another one behind you.
There's no way to escape. Quantum tunnel
There's no way to escape. Quantum tunnel
through the car. Figure it out, man.
The uh the environment Spencer is
The uh the environment Spencer is
working on is a collab with NYU.
You can actually fiddle with pretty darn
You can actually fiddle with pretty darn
recent version of it on the website of
recent version of it on the website of
all the cars.
You can actually hold like space and go
You can actually hold like space and go
into first person mode
into first person mode
and it simulates these like short little
and it simulates these like short little
driving scenes. We're adding much longer
driving scenes. We're adding much longer
scenes to this as well.
scenes to this as well.
Yeah, it's pretty cool.
Uh New York University. Yeah.
There are a couple pretty cool RL labs
There are a couple pretty cool RL labs
over there.
Sadly, not a ton going on at MIT at the
Sadly, not a ton going on at MIT at the
moment as far as I'm aware. Otherwise,
moment as far as I'm aware. Otherwise,
we'd be, you know, we'd support them.
we'd be, you know, we'd support them.
Little annoying.
Okay,
slightly worse in terms of time
slightly worse in terms of time
than the uh the previous one.
than the uh the previous one.
We'll go back to two update epochs.
Hopefully this saves a policy for us. I
Hopefully this saves a policy for us. I
don't know if I'll get to actually check
don't know if I'll get to actually check
it before I got to run for dinner in
it before I got to run for dinner in
case I do have to run because I'll have
case I do have to run because I'll have
to just go all of a sudden. Um,
to just go all of a sudden. Um,
for folks watching,
puffer.ai for all the things, the GitHub
puffer.ai for all the things, the GitHub
to help us out. It's free. really helps
to help us out. It's free. really helps
us out. It's all open source and uh
us out. It's all open source and uh
yeah, join the Discord if you want to
yeah, join the Discord if you want to
actually get involved with building some
actually get involved with building some
of this cool stuff.
of this cool stuff.
We've had several people come in with
We've had several people come in with
like either no AI background or even
like either no AI background or even
some with no programming background been
some with no programming background been
able to get up and running pretty
able to get up and running pretty
quickly. Like we try to make it pretty
quickly. Like we try to make it pretty
easy. There's also a b there a whole
easy. There's also a b there a whole
bunch of resources on here to help you
bunch of resources on here to help you
as well. The blog, we have more articles
as well. The blog, we have more articles
on X, lots of things.
Okay,
go get ston a couple of shots from Yes.
We're dropping by.
It's a pretty good result here.
I honestly don't even know if it needs a
I honestly don't even know if it needs a
bigger network at all.
I think that's a new peak for the
I think that's a new peak for the
stream. That's uh I can never tell how
stream. That's uh I can never tell how
many on X, but that's at least 14
many on X, but that's at least 14
concurrent. So, cool.
Thanks for uh dropping in, folks. I got
Thanks for uh dropping in, folks. I got
to run for dinner. I will be back either
to run for dinner. I will be back either
after dinner or tomorrow. See how I'm
after dinner or tomorrow. See how I'm
feeling. Uh and I'll keep working on
feeling. Uh and I'll keep working on
robotics
robotics
large scale tactical battle end and all
large scale tactical battle end and all
that. So, thank you and uh have a nice
that. So, thank you and uh have a nice
evening.

Kind: captions
Language: en
Hello folks,
Hello folks,
we are back live.
Few things to do today.
Few things to do today.
Few different things.
Let me pull that up and I will switch
Let me pull that up and I will switch
over to the stream view.
All right.
All right.
So, uh, first and foremost here, new
So, uh, first and foremost here, new
article
right here.
If you're interested in uh the latest in
If you're interested in uh the latest in
RL, I would suggest reading this. It's a
RL, I would suggest reading this. It's a
pretty quick read, probably like a 5 10
pretty quick read, probably like a 5 10
minute read. I wrote this this morning.
minute read. I wrote this this morning.
pop the chat out real quick in case uh
pop the chat out real quick in case uh
we'll come on to discuss it because the
we'll come on to discuss it because the
bottom of it says come yell at me if you
bottom of it says come yell at me if you
don't like anything in here.
Okay,
Okay,
so uh yeah, this is pretty much just
so uh yeah, this is pretty much just
like a quick summary of the neural MMO
like a quick summary of the neural MMO
training run that we did. I did the
training run that we did. I did the
math. It's an absolutely ludicrous
math. It's an absolutely ludicrous
amount of data that it turns out a 6.2 2
amount of data that it turns out a 6.2 2
pedlops. It's something like comparable
pedlops. It's something like comparable
to the total amount of training data
to the total amount of training data
used in all language models ever. So
used in all language models ever. So
yeah, it's ridiculous. Hey Melor, that's
yeah, it's ridiculous. Hey Melor, that's
to right. Welcome, man.
to right. Welcome, man.
We've released all sorts of stuff in the
We've released all sorts of stuff in the
last little bit.
So new articles here on that.
So new articles here on that.
And then aside from that, we started
And then aside from that, we started
doing a little robotics
doing a little robotics
got back from RSS. How'd that go?
got back from RSS. How'd that go?
I will be at uh RLC
I will be at uh RLC
in a few weeks. Actually, I got to buy
in a few weeks. Actually, I got to buy
my tickets.
my tickets.
Important thing for me to remember to
Important thing for me to remember to
do.
Good.
Good.
Conferences can be hit or miss. So glad
Conferences can be hit or miss. So glad
that went well. Last one I went to
that went well. Last one I went to
nearly killed me.
That's cuz we Americans aren't as tough
That's cuz we Americans aren't as tough
though.
though.
You're dealing with like freaking
You're dealing with like freaking
malaria and Like, oh no, my
malaria and Like, oh no, my
pneumonia. Like,
pneumonia. Like,
[Music]
[Music]
fix the driver problem. Need to install
fix the driver problem. Need to install
nightly. Ah, I we tried to make it a
nightly. Ah, I we tried to make it a
little more stable in the latest as
little more stable in the latest as
well. We added like some stuff,
well. We added like some stuff,
but yeah, there's tons of cool RL around
but yeah, there's tons of cool RL around
here. And all actually works now, which
here. And all actually works now, which
is cool.
We're starting with robotics today.
Let me think how I want to do this. 15
Let me think how I want to do this. 15
over six like 4 mil.
over six like 4 mil.
Five mil
Five mil
like
see what this does.
We have robotics. Uh, I added a binding
We have robotics. Uh, I added a binding
to Manny skill which is a hell of a lot
to Manny skill which is a hell of a lot
easier to use than a lot of the other
easier to use than a lot of the other
ones. Also, a stone friend of mine works
ones. Also, a stone friend of mine works
on this. I've been working on we have
on this. I've been working on we have
some like basic task working, but we
some like basic task working, but we
haven't really given them the full
haven't really given them the full
puffer treatment yet where we've made
puffer treatment yet where we've made
them super super fast or anything.
them super super fast or anything.
Apparently what we're doing here.
Apparently what we're doing here.
Jeez. What did I have to do to make this
Jeez. What did I have to do to make this
work?
All right. I forgot about this thing.
That is welcome, Phoenix.
I'll show what this task is in a second
I'll show what this task is in a second
here.
here.
Oops. Come on, Chrome. Stop being weird.
Oh, saw this. It's cool.
Oh, saw this. It's cool.
So this is a Manny skill.
They have all sorts of robotics tasks.
Late today I had to write this article.
Late today I had to write this article.
Go read it.
It took me a few hours to like write
It took me a few hours to like write
everything.
everything.
I saw this at RSS.
I saw this at RSS.
You probably You might have even met
You probably You might have even met
Stone then. Stone's really cool in
Stone then. Stone's really cool in
robotic space. It was probably there.
We're starting with some of the simpler
We're starting with some of the simpler
tasks
like these two-finger gripper tasks.
Honestly, it's my intuition that doing
Honestly, it's my intuition that doing
stuff like this just really really well
stuff like this just really really well
could be like just as useful as some of
could be like just as useful as some of
the other stuff in robotics as a lot of
the other stuff in robotics as a lot of
the industry bots kind of look like
the industry bots kind of look like
this.
They follow any specific temporal logic
They follow any specific temporal logic
specifications
specifications
or non-marovian.
or non-marovian.
So one of the best things that you can
So one of the best things that you can
do in reinforcement learning for your
do in reinforcement learning for your
own intuition and understanding of
own intuition and understanding of
things is just completely ignore like
things is just completely ignore like
the marov property in everything. It's
the marov property in everything. It's
not relevant.
not relevant.
I sent a PR on the docs. What PR did you
I sent a PR on the docs. What PR did you
send on docs?
I thought I merged this.
I thought I merged this.
Oh, for work in progress.
Thank you.
Let's see what uh
Let's see what uh
what we're getting in this train run.
Okay, so this is not solving the task
Okay, so this is not solving the task
annoyingly.
Let's see why it's not solving the task.
Difficulty using importable APIs. How
Difficulty using importable APIs. How
so?
Eval's not working.
Eval's not working.
You got to give me more information than
You got to give me more information than
that, right?
Welcome YouTube folks. Welcome Twitch.
Welcome YouTube folks. Welcome Twitch.
Let me fix this run so I get this
Let me fix this run so I get this
working real quick and I'll pull the
working real quick and I'll pull the
article back up because I do recommend
article back up because I do recommend
folks read it. Um, it's kind of like a
folks read it. Um, it's kind of like a
nice showcase of some of what we can do
nice showcase of some of what we can do
with RL nowadays.
with RL nowadays.
I was thinking about it and I actually
I was thinking about it and I actually
think it's better than some of like the
think it's better than some of like the
2019 era results. Not like Dota or
2019 era results. Not like Dota or
anything like that, but I think this is
anything like that, but I think this is
a better result than emergent tool use
a better result than emergent tool use
for instance.
Okay, so that's annoying. Um
just
and we'll try to replicate it more
and we'll try to replicate it more
closely. if this works.
Yeah, here's the article.
There are very few reinforcement
There are very few reinforcement
learning projects uh that have been
learning projects uh that have been
trained on this amount of data and all
trained on this amount of data and all
of the other ones I'm aware of are at
of the other ones I'm aware of are at
large companies with lots of resources.
Welcome Jarish.
And the main result of this is
And the main result of this is
essentially that we actually can scale
essentially that we actually can scale
RL not to a huge amount of hardware
RL not to a huge amount of hardware
necessarily, but we can train we can
necessarily, but we can train we can
scale to a massive amount of data
and we can do this in a way that you can
and we can do this in a way that you can
actually replicate on like a relatively
actually replicate on like a relatively
accessible hardware format.
I believe that this would be
Based on rental prices that I saw, I
Based on rental prices that I saw, I
believe that you could replicate this
believe that you could replicate this
experiment for less than $100 if you
experiment for less than $100 if you
were getting like good vast pricing on a
were getting like good vast pricing on a
4090 spec machine.
4090 spec machine.
Not bad.
Not bad.
Dramatically more with H00's, but it's
Dramatically more with H00's, but it's
just like a like three days of training
just like a like three days of training
uh on a single one of these tiny boxes
uh on a single one of these tiny boxes
here.
Okay. Why are we still segting?
Okay. Why are we still segting?
Oh, I'm not allowed to. Believe me, I
Oh, I'm not allowed to. Believe me, I
should have checked that.
should have checked that.
Okay, we're just going to make sure I
Okay, we're just going to make sure I
didn't break the uh the baseline. We're
didn't break the uh the baseline. We're
going to make sure that we can actually
going to make sure that we can actually
still train this thing to like pick up
still train this thing to like pick up
the cube or whatever. And then we're
the cube or whatever. And then we're
going to try to like optimize this for
going to try to like optimize this for
multiGPU because this is one of the rare
multiGPU because this is one of the rare
cases where like the training is so
cases where like the training is so
obnoxiously slow and fiddly that like I
obnoxiously slow and fiddly that like I
can't even really do all that much on
can't even really do all that much on
one GPU with this thing at the moment.
one GPU with this thing at the moment.
So my goal is going to be to try to
So my goal is going to be to try to
create like a multiGPU setting that's
create like a multiGPU setting that's
pretty much the single GPU setting.
pretty much the single GPU setting.
Exact same training run just faster.
We can go from there.
We can go from there.
Yeah, this thing's pretty silly.
Yeah, this thing's pretty silly.
80k steps per second.
Let me look into the domain
Let me look into the domain
randomization docs because I think
randomization docs because I think
that's the next thing.
training speed scale linearly to number
training speed scale linearly to number
of GPUs. uh for this it does
in terms of steps per second it does in
in terms of steps per second it does in
this with the really really fast
this with the really really fast
environments it's hard to keep them
environments it's hard to keep them
linear but we are able to scale to like
linear but we are able to scale to like
millions of steps per second training
millions of steps per second training
from stuff that's like I think neural
from stuff that's like I think neural
MMO 3 goes from 400 something thousand
MMO 3 goes from 400 something thousand
to 2.2 million on six GPUs which is
to 2.2 million on six GPUs which is
pretty decent.
pretty decent.
We haven't super optimized it either.
What needs to get done for Manny skill
What needs to get done for Manny skill
exactly with Puffer? We kind of just
exactly with Puffer? We kind of just
need to see if we can like make their
need to see if we can like make their
stuff run faster in Puffer or if we can
stuff run faster in Puffer or if we can
get it to train faster, right? Like
get it to train faster, right? Like
Puffer is at the moment it's pretty
Puffer is at the moment it's pretty
highly optimized to Ms that run really
highly optimized to Ms that run really
fast. So, uh the fact that these are not
fast. So, uh the fact that these are not
super fast isn't great for us because
super fast isn't great for us because
things we like to do like running full
things we like to do like running full
hyperparameter sweeps and whatnot aren't
hyperparameter sweeps and whatnot aren't
super easy to do. So, uh, it's a bunch
super easy to do. So, uh, it's a bunch
of just like fiddling with it and seeing
of just like fiddling with it and seeing
if we can tune knobs for puffer that
if we can tune knobs for puffer that
they haven't thought of before.
they haven't thought of before.
Like you can run it out of the box,
Like you can run it out of the box,
right? And like try to match settings,
right? And like try to match settings,
but the thing is we don't really want to
but the thing is we don't really want to
do that because their settings are
do that because their settings are
really slow. We kind of want to like see
really slow. We kind of want to like see
if we can get it into the puffer regime,
if we can get it into the puffer regime,
which is like the really high data fast
which is like the really high data fast
sim regime, and then like get the Sims
sim regime, and then like get the Sims
to catch up with it.
Okay.
Yeah. So, I must have broken something
Yeah. So, I must have broken something
because we had this task succeeding
because we had this task succeeding
before.
Yeah. This task was like very clearly
Yeah. This task was like very clearly
succeeding before.
succeeding before.
Unless it just does all of it at the
Unless it just does all of it at the
end, but I don't think that's how it
end, but I don't think that's how it
does.
does.
Oh, control freak is way too high.
Oh, control freak is way too high.
Hang on.
So, this is the original training
So, this is the original training
setting.
setting.
going to be slower.
We kind of just would like need somebody
We kind of just would like need somebody
who's done a little bit of robotics and
who's done a little bit of robotics and
doesn't mind doing absolutely horrible
doesn't mind doing absolutely horrible
things to the sim in order to make it
things to the sim in order to make it
run faster. Like from my perspective, we
run faster. Like from my perspective, we
kind of don't care if the sim is super
kind of don't care if the sim is super
accurate. We really just want it to be
accurate. We really just want it to be
like we want it to be very fast and like
like we want it to be very fast and like
usable and then we want to randomize the
usable and then we want to randomize the
heck out of
I guess as hell.
a 46k SPS.
a 46k SPS.
Why marovian property is not relevant? I
Why marovian property is not relevant? I
think it helps when formulating a state
think it helps when formulating a state
for a new Enver problem.
for a new Enver problem.
How does it help you do that?
How does it help you do that?
And basically all you're saying is like
And basically all you're saying is like
there's some function that maps state to
there's some function that maps state to
next state. Um, and then you're hoping
next state. Um, and then you're hoping
it's not dependent on time.
it's not dependent on time.
Like that's all you're saying.
Like that's all you're saying.
How does that help you?
Like a lot of the environments we train
Like a lot of the environments we train
on don't respect that property. It goes
on don't respect that property. It goes
out of the window immediately with
out of the window immediately with
multi-agent for instance.
multi-agent for instance.
um it goes out of the window when you
um it goes out of the window when you
have like observations for instance
have like observations for instance
don't track things that depend on time
don't track things that depend on time
even if the state does
like the theoretical guarantees in RL
like the theoretical guarantees in RL
are basically worthless
it's way more easy to think of the
it's way more easy to think of the
agents as um learning a mapping which
agents as um learning a mapping which
can depend on time of observations to
can depend on time of observations to
action And that's about it.
action And that's about it.
Solves puffer eval issue. Why don't you
Solves puffer eval issue. Why don't you
use any IDE?
use any IDE?
Why would I? I've used I have used IDEs
Why would I? I've used I have used IDEs
before. They just kind of suck.
Kind of like just editing stuff in Neoim
Kind of like just editing stuff in Neoim
way more.
Okay, so this works again the baseline
Okay, so this works again the baseline
that we were trying to reproduce.
that we were trying to reproduce.
And now we'll see provided that this
And now we'll see provided that this
actually trains, we will see whether we
actually trains, we will see whether we
can make this learn um 6x faster.
I think we should be able to get it to
I think we should be able to get it to
train like 3x faster at least, not 6x,
train like 3x faster at least, not 6x,
but we'll see.
but we'll see.
And the reason for this is because
And the reason for this is because
robotic sims really really like it when
robotic sims really really like it when
you simulate more steps uh on one GPU in
you simulate more steps uh on one GPU in
a big batch. They like to do that. So,
a big batch. They like to do that. So,
you can't really just like split it up
you can't really just like split it up
into six shards and have it be 6x
into six shards and have it be 6x
faster. It doesn't work that way. You
faster. It doesn't work that way. You
can actually make it six times faster by
can actually make it six times faster by
multiplying the batch size by six, but
multiplying the batch size by six, but
the thing is then the problem actually
the thing is then the problem actually
has to be pretty hard for that to be
has to be pretty hard for that to be
worth anything, which is what we'll get
worth anything, which is what we'll get
to once we start domain randomizing.
to once we start domain randomizing.
Speaking of which,
they have a domain randomization thing
they have a domain randomization thing
on here, don't they?
Yeah, domain randomization.
Yeah, domain randomization.
So camera randomization's not relevant
So camera randomization's not relevant
yet
yet
right now. They're just randomizing the
right now. They're just randomizing the
position of the cube
pose. I don't actually know if pose is
pose. I don't actually know if pose is
randomized.
So this is going to be our next step, I
So this is going to be our next step, I
think, once we have it training faster
think, once we have it training faster
will be to do something like this.
Okay, so this works perfectly. We see
Okay, so this works perfectly. We see
test once 12.8 million 997.
test once 12.8 million 997.
Wait for this.
Uh, do I want to go over or under? I
Uh, do I want to go over or under? I
think I want to go over. We'll do 1024.
think I want to go over. We'll do 1024.
And then what we'll do is we'll assume
And then what we'll do is we'll assume
that I get
that I get
five mil.
five mil.
Do like 4 mil.
Do like 4 mil.
Let's see if this does anything.
Domain randomization refers to physics
Domain randomization refers to physics
param randomization. contact prem
param randomization. contact prem
density. Uh that's another part of it
density. Uh that's another part of it
and we will do that as well. I kind of
and we will do that as well. I kind of
want to like my vision for this right is
want to like my vision for this right is
we make the problem like really fuzzy by
we make the problem like really fuzzy by
randomizing literally everything. Okay.
randomizing literally everything. Okay.
And then we scale this thing to like
And then we scale this thing to like
20,000 robots or whatever.
20,000 robots or whatever.
And I forgot to edit the but luckily I
And I forgot to edit the but luckily I
won't have to do this again.
won't have to do this again.
have to do like this.
So, we're going to scale this thing to I
So, we're going to scale this thing to I
mean neural MMO it goes up to like 50k.
mean neural MMO it goes up to like 50k.
So, we could theoretically even go to
So, we could theoretically even go to
that and that would be faster.
that and that would be faster.
It's just like we have we can't start
It's just like we have we can't start
there because you don't actually benefit
there because you don't actually benefit
from having more parallel agents until
from having more parallel agents until
the problem gets harder. And I actually
the problem gets harder. And I actually
don't even know if domain randomization
don't even know if domain randomization
is going to make training harder or
is going to make training harder or
easier. It actually can go both ways.
I had control problems with not enough
I had control problems with not enough
to observe variables and had to think
to observe variables and had to think
about what to include. Yeah. So, you
about what to include. Yeah. So, you
don't need to think about MDP for that
don't need to think about MDP for that
though. You don't need any of the
though. You don't need any of the
mathematical formulism. That's literally
mathematical formulism. That's literally
just from the perspective of the agent.
just from the perspective of the agent.
What information do you need to solve
What information do you need to solve
the problem? That's it. There's no
the problem? That's it. There's no
mathematical formulism involved with
mathematical formulism involved with
that. Like, there's not even any RL
that. Like, there's not even any RL
involved with that. It's literally just
involved with that. It's literally just
what information do you need to solve
what information do you need to solve
the problem.
We use LSTM as a default choice. It's
We use LSTM as a default choice. It's
not to mitigate the non-marovian prop
not to mitigate the non-marovian prop
like the way we think about this, right?
like the way we think about this, right?
We use LSTMs by default so that the
We use LSTMs by default so that the
agents have memory. That's it.
agents have memory. That's it.
You can attempt to interpret that
You can attempt to interpret that
through mar like through like the thing
through mar like through like the thing
that you said, but it's going to be
that you said, but it's going to be
dramatically dramatically easier and
dramatically dramatically easier and
it's going to give you the exact same
it's going to give you the exact same
result if you just say that we add LSTM
result if you just say that we add LSTM
so that the agents can have memory.
so that the agents can have memory.
That's it.
That's it.
And then there are some problems that
And then there are some problems that
you don't need memory for. And there are
you don't need memory for. And there are
some problems that you do need memory
some problems that you do need memory
for.
And it's pretty immediately obvious for
And it's pretty immediately obvious for
most of them by looking at them which is
most of them by looking at them which is
which?
which?
You see what I mean?
You see what I mean?
We go to the mathematical formulas in
We go to the mathematical formulas in
reinforcement learning when it's
reinforcement learning when it's
actually the best way to describe what's
actually the best way to describe what's
happening.
But that is not all the time.
Okay. So something is very weird. Uh
Okay. So something is very weird. Uh
something very weird looks like it's
something very weird looks like it's
going on here. Uh because
we are not solving the task.
Bye man
things.
I have to reduce mini batch as well. Oh,
I have to reduce mini batch as well. Oh,
I probably messed that up, right?
Let's do uh 8192.
I guess this is not really a one one.
I guess this is not really a one one.
So, we'll see whether this does the same
So, we'll see whether this does the same
thing or not.
It should be pretty close though.
It should be pretty close though.
I'll be surprised if this doesn't at
I'll be surprised if this doesn't at
least give us um something, you know,
least give us um something, you know,
like maybe it takes a little longer to
like maybe it takes a little longer to
train, but it should uh in steps, but it
train, but it should uh in steps, but it
should actually give us something that's
should actually give us something that's
at least faster than uh wall clock. But
at least faster than uh wall clock. But
hopefully
it's like it's not that I hate math,
it's like it's not that I hate math,
Right. It's that I hate when we use math
Right. It's that I hate when we use math
to like when we use very heavy math to
to like when we use very heavy math to
describe very simple things where the
describe very simple things where the
math adds exactly zero additional
math adds exactly zero additional
precision and clarity. And I see it all
precision and clarity. And I see it all
the time in RL.
No.
Okay. I mean, this is like slightly
Okay. I mean, this is like slightly
better of a result.
better of a result.
This should maybe learn something. We'll
This should maybe learn something. We'll
see.
problem is linear and interpretable.
problem is linear and interpretable.
Yeah, but the thing is if the problem is
Yeah, but the thing is if the problem is
linear and interpretable like all this
linear and interpretable like all this
stuff is useless. Like why are you
stuff is useless. Like why are you
wasting your time studying linear
wasting your time studying linear
problems?
So this is like one of my pet peeves,
So this is like one of my pet peeves,
right, with how a lot of these things
right, with how a lot of these things
are taught. They'll like spend time
are taught. They'll like spend time
making you do like proofs or making you
making you do like proofs or making you
like solve equations on like problems
like solve equations on like problems
that just don't matter
like nobody is working on how do we
like nobody is working on how do we
design better RL for linear problems.
Okay. So, this takes a bit longer, but
yeah, we are solving.
yeah, we are solving.
Cool. So, and it is a little faster in
Cool. So, and it is a little faster in
wall clock even though we have not
wall clock even though we have not
reuned parameters or anything. Okay. So,
reuned parameters or anything. Okay. So,
this is like acceptable sub three
this is like acceptable sub three
minutes per experiment.
minutes per experiment.
We can at least like make progress with
We can at least like make progress with
things at this rate.
things at this rate.
When things get to like five plus minute
When things get to like five plus minute
experiments, I get annoyed because it's
experiments, I get annoyed because it's
like
like
what am I going to do? I just guess I
what am I going to do? I just guess I
hang out and I yap for five minutes
hang out and I yap for five minutes
between runs.
Okay, so this works
and
and
I think I ran at 30 mil.
I think I ran at 30 mil.
Current reward function justified in
Current reward function justified in
Manny skill.
Manny skill.
Uh, their rewards were not. I changed
Uh, their rewards were not. I changed
them and now they are.
them and now they are.
Sometimes they take 15 minutes. You need
Sometimes they take 15 minutes. You need
to get real hardware, my friend. CPUs
to get real hardware, my friend. CPUs
are not real hardware for RL.
And this robotic stuff is super slow. So
And this robotic stuff is super slow. So
like I have a server for it, but like
like I have a server for it, but like
all the other stuff in Puffer, you can
all the other stuff in Puffer, you can
just run it on a desktop. We have people
just run it on a desktop. We have people
running with like basically whatever
running with like basically whatever
GPU. It's can't you can't be doing stuff
GPU. It's can't you can't be doing stuff
on CPU. It'll be like 20 times faster
on CPU. It'll be like 20 times faster
than if you do it on CPU and another RL
than if you do it on CPU and another RL
library. But we can only work so much
library. But we can only work so much
magic.
magic.
Only do so much.
So this looks fine, right?
Here's our multiGPU baseline.
Here's our multiGPU baseline.
If I do real time,
if I do relative time, you can see it is
if I do relative time, you can see it is
better in relative time than our uh
better in relative time than our uh
previous experiment. So good. A. Okay.
Next we do domain randomization.
Okay. So, we'll have to figure out how
Okay. So, we'll have to figure out how
we do this.
we do this.
Think we can do RL on cyersc. I've
Think we can do RL on cyersc. I've
talked to a few people about that
talked to a few people about that
probably.
You can always toggle with how position
You can always toggle with how position
rewardsation
rewardsation
combined. Uh there was a much more
combined. Uh there was a much more
fundamental problem, Sean, that I had to
fundamental problem, Sean, that I had to
fix.
fix.
uh they were doing rewards in a way that
uh they were doing rewards in a way that
just do not make sense for standard RL
just do not make sense for standard RL
outside of robotics. And like basically
outside of robotics. And like basically
they had two errors that canceled each
they had two errors that canceled each
other out. And like all of robotics
other out. And like all of robotics
people do this apparently, but I just
people do this apparently, but I just
like I did it the same way where you
like I did it the same way where you
don't need to go make two errors to
don't need to go make two errors to
cancel each other out. And it's just
cancel each other out. And it's just
correct.
Sometimes they provide intuition to a
Sometimes they provide intuition to a
bigger empirical problem
bigger empirical problem
as far as I can.
as far as I can.
I hear what you're saying, Kovac, but
I hear what you're saying, Kovac, but
I've seen the way the RL is taught and
I've seen the way the RL is taught and
it's a mess.
Like, if you just look at like our quick
Like, if you just look at like our quick
start guide and like the resources I
start guide and like the resources I
link in the quick start guide and how
link in the quick start guide and how
much faster you will actually be able to
much faster you will actually be able to
solve stuff and understand things that
solve stuff and understand things that
matter, it's a world of difference. It
matter, it's a world of difference. It
advance. So, basically, and this is this
advance. So, basically, and this is this
is no shame to the manny skill guys,
is no shame to the manny skill guys,
okay? Because everybody in robotics does
okay? Because everybody in robotics does
this stuff and they all do it wrong.
this stuff and they all do it wrong.
What they do is they do these
What they do is they do these
state-based rewards where they say like,
state-based rewards where they say like,
okay, your reward goes up as you get
okay, your reward goes up as you get
closer to the target. All right?
closer to the target. All right?
And the reason that this is wrong is
And the reason that this is wrong is
because when you're right next to the
because when you're right next to the
target, you have maximal reward.
target, you have maximal reward.
So, you're incentivized to just sit
So, you're incentivized to just sit
there forever and do nothing because you
there forever and do nothing because you
farm more reward. That is what is wrong
farm more reward. That is what is wrong
with the way that roboticists specify
with the way that roboticists specify
their rewards.
their rewards.
To get around this, they do this
To get around this, they do this
infinite horizon bootstrap thing, which
infinite horizon bootstrap thing, which
basically says that you pretend that
basically says that you pretend that
you're going to keep going forever and
you're going to keep going forever and
that's the reward that you'll actually
that's the reward that you'll actually
get. And they change the way advantage
get. And they change the way advantage
estimation works in order to do that.
estimation works in order to do that.
But that's going to break learning for a
But that's going to break learning for a
ton of other environments because it
ton of other environments because it
basically says you get reward beyond the
basically says you get reward beyond the
current episode. Um, you don't get like
current episode. Um, you don't get like
a penalty for terminating.
a penalty for terminating.
So, it's a much more specialized
So, it's a much more specialized
implementation. And the thing is, you
implementation. And the thing is, you
don't need to do any of this. Literally,
don't need to do any of this. Literally,
all they had to do is just take their
all they had to do is just take their
current reward, right? And take the
current reward, right? And take the
delta from the current reward to the
delta from the current reward to the
reward at the previous time step. That's
reward at the previous time step. That's
your new reward function. And now all
your new reward function. And now all
those problems go away. It was like
those problems go away. It was like
three lines of code.
Why is command line interface slow
Why is command line interface slow
compared to importable?
compared to importable?
What do you mean why is it slow?
What do you mean why is it slow?
you get faster training speed or is it
you get faster training speed or is it
like is it just that it takes a second
like is it just that it takes a second
to show up? Because if it's that it
to show up? Because if it's that it
takes a second to show up, that's
takes a second to show up, that's
because Torch like PyTorch takes like a
because Torch like PyTorch takes like a
second to import.
It's very silly that we have packages in
It's very silly that we have packages in
Python that take like a second or two to
Python that take like a second or two to
import, but that is the state of
import, but that is the state of
software.
How do I do this? Me get rid of that
How do I do this? Me get rid of that
bot.
I import it's closer to 600K. Then
I import it's closer to 600K. Then
something's got to be different, right?
something's got to be different, right?
like settings or something got to be
like settings or something got to be
different.
The command line is literally just a
The command line is literally just a
wrapper around that.
wrapper around that.
Could be default args. I don't know. It
Could be default args. I don't know. It
depends how you're loading configs.
Is our RL stars fair? Yeah.
Is our RL stars fair? Yeah.
I mean, you see the thing, the way that
I mean, you see the thing, the way that
we build stuff around here, it's like
we build stuff around here, it's like
a lot of stuff in RL is just it's
a lot of stuff in RL is just it's
mindboggling how badly done a lot of the
mindboggling how badly done a lot of the
things are, right? Like look, I
things are, right? Like look, I
shouldn't be able to make RL a thousand
shouldn't be able to make RL a thousand
times faster. Okay? When a field is done
times faster. Okay? When a field is done
competently, it's like a big deal if you
competently, it's like a big deal if you
make something two times faster or two
make something two times faster or two
times better, right? That's like a huge
times better, right? That's like a huge
deal, right? When stuff is like I make
deal, right? When stuff is like I make
it a thousand times faster, that just
it a thousand times faster, that just
means the original was just a total
means the original was just a total
mess, right?
Let's see if we can figure out this pose
Let's see if we can figure out this pose
logic stuff. And since we do have a fair
logic stuff. And since we do have a fair
few folks on YouTube, a few quick
few folks on YouTube, a few quick
things. One is there's a a new article
things. One is there's a a new article
today. It's on X. You can get it right
today. It's on X. You can get it right
here, Jars 531. I post all the stuff
here, Jars 531. I post all the stuff
here. So, if you want to see how you can
here. So, if you want to see how you can
do reinforcement learning on a pabyte of
do reinforcement learning on a pabyte of
data with a relatively uh mediocre
data with a relatively uh mediocre
hardware setup, not a $300,000 server
hardware setup, not a $300,000 server
right here. And then generally, I stream
right here. And then generally, I stream
all this stuff live. It's all open
all this stuff live. It's all open
source at puffer.ai.
source at puffer.ai.
The GitHub to help us out. And you can
The GitHub to help us out. And you can
get involved with the development on the
get involved with the development on the
Discord right here. Well, back to Dev.
This is slightly irritating.
There's not really a way around this,
There's not really a way around this,
but they they kind of want you to
but they they kind of want you to
subclass their stuff.
Okay, fine. We'll do that.
We'll make like um
make like a puffer task or whatever.
How do we do this for um
How do we do this for um
for the panda robot
agent robot and controller randomization
Little tricky to figure out how we're
Little tricky to figure out how we're
supposed to do this.
Let me just message Stone to see if he's
Let me just message Stone to see if he's
on.
on.
Maybe he can help us get us started on
Maybe he can help us get us started on
this quicker. If not, I'll just figure
this quicker. If not, I'll just figure
it out.
Load
agent load scene
a custom environment. When you load the
a custom environment. When you load the
scene,
we definitely need to just get a base
we definitely need to just get a base
scene loaded. Let's do that first.
What is epoch? epoch is the number of uh
What is epoch? epoch is the number of uh
training steps that have run like
training steps that have run like
collect a bunch of data, you train on
collect a bunch of data, you train on
the data, that's one epoch.
cube.py
at register M cube.
Many skill M's tasks tabletop
pitch cube
Take them.
Okay. So now
We're going to just temporarily force
We're going to just temporarily force
this to be
this to be
upper
upper
buffer pick
buffer pick
cube.
And I guess we can pass all this stuff
And I guess we can pass all this stuff
directly maybe.
Not.
Hopefully we can get a good domain
Hopefully we can get a good domain
randomized config quite quickly here.
randomized config quite quickly here.
We'll see.
I think I know how we should go about
I think I know how we should go about
it. If we just open up the uh the
it. If we just open up the uh the
source,
I believe they randomized the uh the
I believe they randomized the uh the
goal position.
Oh, maybe they don't randomize the goal
Oh, maybe they don't randomize the goal
position at all.
actors.build sphere
pose.
Um,
this is 000.
this is 000.
Kind of confusing honestly. I don't know
Kind of confusing honestly. I don't know
where they're setting positions and
where they're setting positions and
things in here.
things in here.
Okay, so this works apparently, but um
Okay, so this works apparently, but um
I'm not seeing any logs. So something is
I'm not seeing any logs. So something is
screwy.
Maybe we should just do like this. Yeah.
Go through the register.
Episode and epochs are the same. Um, no.
Episode and epochs are the same. Um, no.
So, an episode Well, where do you see
So, an episode Well, where do you see
episodes? I don't think we record that
episodes? I don't think we record that
as a stat. You see episode return and
as a stat. You see episode return and
episode length that is for like one
episode length that is for like one
agent's interactions with one
agent's interactions with one
environment. How long is that? So how
environment. How long is that? So how
many steps and then what is the summed
many steps and then what is the summed
summed reward?
summed reward?
The epochs is like a bunch of episodes.
The epochs is like a bunch of episodes.
A whole bunch of uh episodes are in each
A whole bunch of uh episodes are in each
epoch.
Does anybody understand how they have
Does anybody understand how they have
this set up?
Cube spawn center
Cube spawn center
register M does not define. Okay.
Go XYZ.
Go XYZ.
Okay. Uh pose.create from PQ
positioning quaternian. Okay.
positioning quaternian. Okay.
So this is randomizing I believe just
So this is randomizing I believe just
the initial cube site
the initial cube site
and not the is it the goal?
and not the is it the goal?
Oh no it is randomizing the
Oh no it is randomizing the
some aspect of it.
Okay. So this runs now. We'll make sure
Okay. So this runs now. We'll make sure
that this frames.
We actually do have our ability though
We actually do have our ability though
to to mess with it now.
Oh, I understand. So, this is just
Oh, I understand. So, this is just
loading all the stuff and then on
loading all the stuff and then on
initialize episode
initialize episode
they uh they move the thing. So, really
they uh they move the thing. So, really
all we need to do
change the initialize episode
change the initialize episode
potentially,
at least for now.
And we should also probably get this on
And we should also probably get this on
our local
so that we can render stuff.
Yeah, so this is the default setup.
Yeah, so this is the default setup.
It does randomize the cubes
It does randomize the cubes
position
and the goal.
It does drop it occasionally as well,
It does drop it occasionally as well,
which is interesting.
as this works. We didn't break the
as this works. We didn't break the
environment.
environment.
Now we can attempt to break the
Now we can attempt to break the
environment.
This
This
you need this
start with
uh do we let me See, does this always
uh do we let me See, does this always
start the robot at the same spot?
Looks like the robot's pretty much
Looks like the robot's pretty much
always in the same spot, right?
table scene
table scene
robot.
tell you how to move the uh the robot
tell you how to move the uh the robot
agent robot and controller
agent robot and controller
randomizations.
Done.
Ripper frictions render material.
Yeah, this can be done during
Yeah, this can be done during
initialized episodes.
Environment episode current is the
Environment episode current is the
parameter that tells us the agent is
parameter that tells us the agent is
learn. It depends on the environment,
learn. It depends on the environment,
but yes,
but yes,
some environments have a score. Most of
some environments have a score. Most of
them should have a score in puffer lib.
them should have a score in puffer lib.
That's the actual like metric you care
That's the actual like metric you care
about. Episode return is the summed
about. Episode return is the summed
reward, the metric the agencies that
reward, the metric the agencies that
it's optimizing.
it's optimizing.
slightly different, but they'll both
slightly different, but they'll both
tell you things.
When it's flat, it means the agent's not
When it's flat, it means the agent's not
learning anything. If the score is flat
learning anything. If the score is flat
as well, then yes.
How do I make the robot start at a
How do I make the robot start at a
different spot?
different spot?
like
what is pros and cons of numbum M's?
Uh it depends which num ms param you
Uh it depends which num ms param you
mean.
mean.
If you set like a lot of the ends uh
If you set like a lot of the ends uh
like vec num ms param to zero like to
like vec num ms param to zero like to
one it's not going to learn if you set
one it's not going to learn if you set
num ms to one it's not going to learn
num ms to one it's not going to learn
anything.
anything.
If you set those to 20 then you have
If you set those to 20 then you have
you're going to have a ludicrous number
you're going to have a ludicrous number
of agents.
of agents.
Like if you change it if you change the
Like if you change it if you change the
default to 20 you're going to have like
default to 20 you're going to have like
20,000 agents in parallel. You don't
20,000 agents in parallel. You don't
have hardware for that and then only
have hardware for that and then only
some of the environments will benefit
some of the environments will benefit
from that.
What the heck?
Helicopter just went over flying super
Helicopter just went over flying super
low.
What's the ideal for CPU?
The settings that I have as defaults are
The settings that I have as defaults are
probably going to still be roughly ideal
probably going to still be roughly ideal
because like
because like
you'd have to rerun a whole sweep just
you'd have to rerun a whole sweep just
for CPU if you want to see what the
for CPU if you want to see what the
fastest solve time is. At least the
fastest solve time is. At least the
settings that I provided I know work,
settings that I provided I know work,
right?
Robot init pause noise.
Robot init pause noise.
The robot init pause noise. Can I crank
The robot init pause noise. Can I crank
this up?
Can I just like crank this way up?
What happens if I do this?
Then the next thing
the heck pose is not defined.
Oh,
this is everything in here.
So, this should make uh the robot start
So, this should make uh the robot start
at all sorts of different spots.
That works.
We can change the
We can change the
change the robot like joint stuff.
change the robot like joint stuff.
That's probably important.
I wanted to change like size of cube and
I wanted to change like size of cube and
stuff.
Well, stones replying.
Starting to learn
Ah,
Ah,
but here
has failed it seems.
Starting to learn
Starting to learn
if we just can run it for longer.
A lot of things you can change in theory
A lot of things you can change in theory
soon.
For everything must be rough. No, I Why
For everything must be rough. No, I Why
would I do it this way if it were
would I do it this way if it were
difficult? I do it this way because it's
difficult? I do it this way because it's
easy.
easy.
The one annoying thing is I don't have
The one annoying thing is I don't have
my Neovim on the tiny box. So like my
my Neovim on the tiny box. So like my
autocomplete and stuff isn't working
autocomplete and stuff isn't working
nicely.
I have it a little bit nicer locally.
I've used IDs before. They're just slow
I've used IDs before. They're just slow
and shitty.
This is what it looks like better.
Okay,
Okay,
this is doing something now.
Cool.
See if it's stable.
So, let's see. There's collision meshes
So, let's see. There's collision meshes
visual messages
visual messages
point PD
point PD
mass inertia
mass inertia
initial mean joint position
initial mean joint position
roller choice.
Most of these are not exposed directly.
We have to be a little careful not to do
We have to be a little careful not to do
stuff that just makes the task
stuff that just makes the task
impossible.
I don't think that's what's happening
I don't think that's what's happening
here, though.
here, though.
This seems like unstable learning or
This seems like unstable learning or
something.
Set it to 0.1 for now and see what
Set it to 0.1 for now and see what
happens.
I don't want it to like spawn through
I don't want it to like spawn through
the table or something, right?
Mass inertia
Mass inertia
point PD
Pose equals
random quitterians.
How do you fund this project? Do you
How do you fund this project? Do you
work somewhere else? Puffer is a
work somewhere else? Puffer is a
company. So, we have a couple companies
company. So, we have a couple companies
that we work with. Generally, you can
that we work with. Generally, you can
hire Puffer to help with your
hire Puffer to help with your
reinforcement learning efforts. We have
reinforcement learning efforts. We have
all sorts of tools. Uh, reinforcement
all sorts of tools. Uh, reinforcement
learning is hard. We make it easier.
Generally, a lot of our stuff just
Generally, a lot of our stuff just
solves a lot of problems out of the box.
solves a lot of problems out of the box.
Um, the ones that it doesn't we help
Um, the ones that it doesn't we help
with.
Okay. Okay, so this initial pose at
Okay. Okay, so this initial pose at
least gives us some reasonable success
least gives us some reasonable success
right here.
I don't know if this is going to be
I don't know if this is going to be
stable.
stable.
This seems like it's unstable a little
This seems like it's unstable a little
bit.
at
at
still don't even know what the
still don't even know what the
observations are, right?
observations are, right?
What are the observations?
Okay. So they have added the
Okay. So they have added the
obvious
grasp.
grasp.
Full pause.
Modify the task.
Yeah, this is stable.
pose raw pose.
pose raw pose.
I need to understand what this data is
I need to understand what this data is
next.
Close fully.
You pause noise. Okay.
Not super stable.
PCP pose
PCP pose
need to understand whether this is
need to understand whether this is
reasonable. Next.
just stick some break points in here and
just stick some break points in here and
start looking at data.
off pose.
Okay, so this is the pose.
to OB
pose.
Uh, okay. This is interesting.
Uh, okay. This is interesting.
The cube also has a 7D pose.
The cube also has a 7D pose.
Position and orientation of the robot.
Position and orientation of the robot.
Uh, what is environment?
Uh, what is environment?
This is Manny skill. This is a robot
This is Manny skill. This is a robot
picking up a cube and moving it to a
picking up a cube and moving it to a
goal.
So, is this position XYZ and then like a
So, is this position XYZ and then like a
quitterian?
This what it is.p
P
P
and Q.
and Q.
Yeah. Environment slashn on your ocean
Yeah. Environment slashn on your ocean
n. Oh, n is the number of data points
n. Oh, n is the number of data points
that were collected in displaying that
that were collected in displaying that
summary.
summary.
Last four straight up like quturnians.
Last four straight up like quturnians.
Yeah.
Yeah.
So that's weird though.
It should have more than this, right?
A robot is described by a root pose and
A robot is described by a root pose and
point angles.
hit cube only. It doesn't have anything
hit cube only. It doesn't have anything
to do with the cube though. It has to do
to do with the cube though. It has to do
with the robot, right? The robot is a
with the robot, right? The robot is a
like six degree of freedom or whatever
like six degree of freedom or whatever
arm
always given to the
You always provide Q pause and velocity
You always provide Q pause and velocity
extra obs specific data.
extra obs specific data.
Okay, I see.
Yeah. Okay. So, apparently there's a
Yeah. Okay. So, apparently there's a
base there's a base observation for all
base there's a base observation for all
the robots.
M and the Sapion M.
Get OBS.
Get OBS agent. Get OBS extra.
Get OBS agent. Get OBS extra.
Get proprioception.
grasping. Addict.
Looks like a pain. Yeah, it is a pain in
Looks like a pain. Yeah, it is a pain in
the ass, but it's dramatically less of a
the ass, but it's dramatically less of a
pain in the ass than the other robotics
pain in the ass than the other robotics
libraries.
This is like less, believe it or not,
This is like less, believe it or not,
this is like less overly abstracted and
this is like less overly abstracted and
ridiculous by a mile compared to like
ridiculous by a mile compared to like
Isaac
Isaac
in base agent.
Oh, agent. There's a base agent.
And then this calls get state
And then this calls get state
and get get pause get velocity.
Hey,
do we have a at.
So they actually are including then the
So they actually are including then the
information that seems
information that seems
mostly reasonable.
I think that this is mostly done
I think that this is mostly done
correctly.
I guess it's difficult to know because
I guess it's difficult to know because
of the coordinates schemes, right?
Okay, this is one that actually has like
Okay, this is one that actually has like
a pretty simple reward. That's probably
a pretty simple reward. That's probably
correct.
Then what makes this like remotely
Then what makes this like remotely
difficult or interesting, I guess, is my
difficult or interesting, I guess, is my
question,
question,
right?
right?
What makes this remotely difficult?
probably just comes down to
probably just comes down to
randomization. Now,
I do it here.
I do it here.
Builds all objects and all parallel M
Builds all objects and all parallel M
and share same physical properties.
and share same physical properties.
Boards are dense. So it has to work
Boards are dense. So it has to work
unless problem works. But I'm trying to
unless problem works. But I'm trying to
figure out what makes the problem like
figure out what makes the problem like
remotely difficult to learn.
I mean to be fair it does learn in like
I mean to be fair it does learn in like
a ridiculously short number of time
a ridiculously short number of time
steps uh by like our standards of our
steps uh by like our standards of our
other environments just the end is so
other environments just the end is so
unbelievably Hello.
That's the main limitation more than
That's the main limitation more than
anything is just it's order of magnitude
anything is just it's order of magnitude
minimum too slow.
minimum too slow.
Like minimum
Like minimum
full
pull center point.
pull center point.
Okay, so this is competently done.
I think what else I can mess
Okay.
Okay.
On
On
Here's
a thing I can try.
Can I do 4096?
Can I do this?
This is 280k. Okay.
We should practice
go explore style thing.
I could I think I think I see now with
I could I think I think I see now with
like robotics how you could do this from
like robotics how you could do this from
just a couple demos.
I don't think what we have to do that
I don't think what we have to do that
they're just loading some states.
being the issue, right? is that it
being the issue, right? is that it
doesn't
doesn't
doesn't learn anywhere near as Well,
Getting started with RL. Welcome.
Getting started with RL. Welcome.
This is the place for it.
I think we're just going to have to run
I think we're just going to have to run
a bunch of random experiments with how
a bunch of random experiments with how
slow this thing is is and hope we find
slow this thing is is and hope we find
something start with.
Like it's not like it doesn't work, it's
Like it's not like it doesn't work, it's
just not optimal.
this
this
first thing to try. That's kind of
first thing to try. That's kind of
obvious.
It's already a massive learning rate,
It's already a massive learning rate,
but no, screw it. Why not?
It's just like it's annoying that this
It's just like it's annoying that this
is basically they've kind of like
is basically they've kind of like
managed to bring RL back to the dark
managed to bring RL back to the dark
ages with the the slow sims here.
ages with the the slow sims here.
Realistically,
Realistically,
I'd have to add
I'd have to add
I'd have to add multiGPU
I'd have to add multiGPU
sweeps, I think.
Try
increase action limits.
A RMP PD
Why is it not learning
Why is it not learning
with
you're using 32,000 environments? That's
you're using 32,000 environments? That's
why you have a huge number of
why you have a huge number of
environments for a relatively simple
environments for a relatively simple
problem. It's not going to benefit from
problem. It's not going to benefit from
that amount of parallelism.
U because the config for that end
U because the config for that end
probably doesn't have end numm set to
probably doesn't have end numm set to
eight. Got this from your example. Which
eight. Got this from your example. Which
example has that?
portable. Oh yeah, that's just that's
portable. Oh yeah, that's just that's
just showing you like a random that's
just showing you like a random that's
not like meant to be a good config for
not like meant to be a good config for
whatever env, right? They're showing you
whatever env, right? They're showing you
the API
visual will be barely paralyzed at 1024
visual will be barely paralyzed at 1024
m
m
per GPU.
trying this.
Okay, this does something, but I don't
Okay, this does something, but I don't
think it learns like fast, right?
think it learns like fast, right?
So, this it learns, but it takes forever
So, this it learns, but it takes forever
here.
here.
That doesn't do it.
go mess with the panda.
go mess with the panda.
Where?
Okay. So that's
Okay. So that's
make the robot faster. There.
Okay, this looks better, right?
Are you running multi-en
Are you running multi-en
You're running multi-en 4096? How come
You're running multi-en 4096? How come
it's slow? This thing here, uh, this is
it's slow? This thing here, uh, this is
slow because this is a robotic sim,
slow because this is a robotic sim,
which is just slow. It's not a puffer
which is just slow. It's not a puffer
end. Our puffer ms are fast.
Okay. So, this learns like
Okay. So, this learns like
maybe a little bit faster.
Okay. So, increasing the actions kind of
Okay. So, increasing the actions kind of
did something.
I'm going to go like just 5x the force
I'm going to go like just 5x the force
limits and stuff as well.
You say it slows comparing apples and
You say it slows comparing apples and
bananas. Whole engines do tons of
bananas. Whole engines do tons of
calculations.
[Music]
[Music]
Honestly, it's like six degrees of
Honestly, it's like six degrees of
freedom in one cube. I think it's kind
freedom in one cube. I think it's kind
of just slow.
Like particularly when I look at the
Like particularly when I look at the
code and how the code is written, it's
code and how the code is written, it's
like
I bet it's just slow.
I bet it's just slow.
If I want to write an NVC, what do I
If I want to write an NVC, what do I
have to do? We have a tutorial for that
have to do? We have a tutorial for that
on the docs page.
I'm thinking about it and it's like
I'm thinking about it and it's like
there's no way you wouldn't be able to
there's no way you wouldn't be able to
run a fixed degree of freedom arm at
run a fixed degree of freedom arm at
like millions. Totally should be able
like millions. Totally should be able
to. Really not doing anything that
to. Really not doing anything that
crazy.
Ah, lovely. Okay. So, it crashes.
Okay.
Now, you find sweet spot for training
Now, you find sweet spot for training
config. Not the way I'm doing it here.
config. Not the way I'm doing it here.
Automated uh automatic sweeps.
Don't do what I'm doing here.
Don't do what I'm doing here.
Crazy. Contact sim with what? There's
Crazy. Contact sim with what? There's
nothing for it to contact.
uh puffer sweep. It should also be in
uh puffer sweep. It should also be in
the docs. We have a sweep. We have like
the docs. We have a sweep. We have like
a really nice hyper pram sweep algo
a really nice hyper pram sweep algo
built in.
See if this does anything.
Let's just I'm just kind of yoloing a
Let's just I'm just kind of yoloing a
few things here because like
the way that it's set up just can't be.
the way that it's set up just can't be.
It like just can't be the best way of
It like just can't be the best way of
doing it.
It's just no way you need to have it be
It's just no way you need to have it be
so high fidelity that it's low.
to help me find the best config. Yes.
All right. We'll see if this does
All right. We'll see if this does
anything.
This should allow it to just like flail
This should allow it to just like flail
about very very quickly if it wants to.
Not also getting limited by maxport.
Ignore it. Okay.
If this learns anything
realistic collision checks for complex
realistic collision checks for complex
meshes even though they are
meshes even though they are
rectangularistically
doing a lot of realism. Well, then they
doing a lot of realism. Well, then they
shouldn't be doing that, right?
The idea of like I want to train a good
The idea of like I want to train a good
robot, therefore I will simulate this
robot, therefore I will simulate this
thing as realistically as I possibly
thing as realistically as I possibly
can, train it on that and then deploy
can, train it on that and then deploy
it. That's just wrong. That's not how it
it. That's just wrong. That's not how it
works
because if you just have no data, right?
because if you just have no data, right?
If you have basically no data and it's
If you have basically no data and it's
very high fidelity, doesn't matter.
I'd rather be able to train something at
I'd rather be able to train something at
relatively lower fidelity with a ton of
relatively lower fidelity with a ton of
randomization such that I get a really
randomization such that I get a really
really robust policy out of it.
really robust policy out of it.
It doesn't matter if I can simulate
It doesn't matter if I can simulate
physics if I have something that can
physics if I have something that can
like work correctly on basically any
like work correctly on basically any
sort of janky physics setup that you
sort of janky physics setup that you
give it. Right?
give it. Right?
That's the idea.
how you deploy it because if it works on
how you deploy it because if it works on
any randomized set of physics,
any randomized set of physics,
collisions, etc. If it works on like all
collisions, etc. If it works on like all
randomized versions, then the real
randomized versions, then the real
version is in there somewhere, right?
version is in there somewhere, right?
So, think about it, right? one option
So, think about it, right? one option
one you make it as realistic as
one you make it as realistic as
possible. Okay. And then you deploy you
possible. Okay. And then you deploy you
train it on that it's really really slow
train it on that it's really really slow
and you deploy it and then your SIM is
and you deploy it and then your SIM is
not exactly like the real world. So
not exactly like the real world. So
there's still a big annoying gap, right?
there's still a big annoying gap, right?
Option two is that you randomize the
Option two is that you randomize the
heck out of everything such that you're
heck out of everything such that you're
not remotely training on something that
not remotely training on something that
looks like the real world, but the real
looks like the real world, but the real
world is in there somewhere in all the
world is in there somewhere in all the
configs that you have.
is determined by the sim to real gap
is determined by the sim to real gap
too.
too.
Um kind of but it's easier because like
Um kind of but it's easier because like
domain randomization gives you a big
domain randomization gives you a big
wide distribution over the real like
wide distribution over the real like
over
over
it gives you a big wide distribution
it gives you a big wide distribution
that hopefully the real one's in there
that hopefully the real one's in there
somewhere.
Okay. Yeah. So, this is taking longer to
Okay. Yeah. So, this is taking longer to
learn with random action. So my guess is
learn with random action. So my guess is
that it's just like
something very janky going on here.
I don't agree with you.
I mean, something's got to give because
I mean, something's got to give because
the Sims are just abysmally horribly
the Sims are just abysmally horribly
slow.
Like, you're literally in the dark ages
Like, you're literally in the dark ages
where like nothing works because you
where like nothing works because you
have no data.
This right here is running at less than
This right here is running at less than
5% of the speed of um you know my more
5% of the speed of um you know my more
complex M training runs
and like a hundth of the speed of the
and like a hundth of the speed of the
faster ones
for like a really easy task. like a task
for like a really easy task. like a task
that when you look at the task and you
that when you look at the task and you
assess the difficulty of the task like
assess the difficulty of the task like
it's trivial.
it's trivial.
You have this being this slow for a
You have this being this slow for a
problem that is trivial.
And this is the thing that we found with
And this is the thing that we found with
reinforcement learning as well. Stuff
reinforcement learning as well. Stuff
feels really difficult because it's
feels really difficult because it's
slow. That's it.
slow. That's it.
It's like this is why you had like
It's like this is why you had like
generations of RL students like working
generations of RL students like working
on Pong and stuff, right? Like, well,
on Pong and stuff, right? Like, well,
was ponging hard? No, it was just slow.
You can't optimize like all the all the
You can't optimize like all the all the
experiment settings, all the
experiment settings, all the
hyperparameters suck, okay? All the
hyperparameters suck, okay? All the
environment settings suck, okay? Like
environment settings suck, okay? Like
literally nothing is optimized correctly
literally nothing is optimized correctly
at all because you can't run enough
at all because you can't run enough
experiments because it's slow.
Exact same thing happened in the rest of
Exact same thing happened in the rest of
RL.
like a one one same playbook.
How to best understand config from
How to best understand config from
command. Yeah. So, Neptune will give you
command. Yeah. So, Neptune will give you
logs and you can see which experiments
logs and you can see which experiments
are the best and then you can kind of
are the best and then you can kind of
just pick one and grab the config
just pick one and grab the config
parameters from there.
I mean
annoying, man.
It's like I can't even really like the
It's like I can't even really like the
thing is I can't even really get into
thing is I can't even really get into
this type of stuff I'd want to do with
this type of stuff I'd want to do with
the way this is set up. It's like not
the way this is set up. It's like not
intended for the type of stuff I want to
intended for the type of stuff I want to
do.
We have any war sims? Uh yeah, the one I
We have any war sims? Uh yeah, the one I
was working on the other Okay.
At least do this.
didn't sacrifice the realism.
didn't sacrifice the realism.
Let me see how I can convince you with
Let me see how I can convince you with
this
quite
quite
like literally this is all I'm asking
like literally this is all I'm asking
for. Okay, take literally every single
for. Okay, take literally every single
physics parameter in here, right?
physics parameter in here, right?
like every single like you know force
like every single like you know force
parameter, gravity, inertia like just
parameter, gravity, inertia like just
take all the parameters
take all the parameters
okay
okay
and then define some like percentage
and then define some like percentage
based interval over them.
We should be able to tune the magnitude
We should be able to tune the magnitude
of that interval.
of that interval.
And then all the other parameters that's
And then all the other parameters that's
like solver fidelity and updates and all
like solver fidelity and updates and all
that stuff should also go into the tuner
and then we should figure it out
and then we should figure it out
automatically and algorithmically.
Roboticists are not doing that at the
Roboticists are not doing that at the
moment.
moment.
They're kind of just assuming that they
They're kind of just assuming that they
just make the thing as slow as human
just make the thing as slow as human
like un like as slow as they can
like un like as slow as they can
possibly bear
possibly bear
and then they just run their really slow
and then they just run their really slow
experiments and hope it works.
Horrible contact sim
possibly.
Is that a hardware thing though? Like
Is that a hardware thing though? Like
can't you get around that with other
can't you get around that with other
types of grippers?
can't parameterize
the dynamics are straight. I don't see
the dynamics are straight. I don't see
any fluid dynamics or cloth draping
any fluid dynamics or cloth draping
though is the thing. Like look, I don't
though is the thing. Like look, I don't
see fluid dynamics. I don't see cloth.
see fluid dynamics. I don't see cloth.
I see ultra basic rigid body physics,
I see ultra basic rigid body physics,
right? With like a literally trivial
right? With like a literally trivial
task.
task.
Hang on, let me show you the one that
Hang on, let me show you the one that
I'm looking at.
I'm looking at.
It's like pig cube.
Okay, if you want to think about like in
Okay, if you want to think about like in
terms of how complex like to learn this
terms of how complex like to learn this
behavior from first principles
behavior from first principles
in my head, this is like pong.
This is like nothing.
We kind of just get sensor data.
We kind of just get sensor data.
Okay. Okay. Easier than pong. I mean,
Okay. Okay. Easier than pong. I mean,
like
seriously, isn't it?
You're picking up a thing.
What is the decision making involved
What is the decision making involved
with picking up a thing?
with picking up a thing?
It's like a few joint angles.
It's like a few joint angles.
Can't you spawn more than 4K? Yes, you
Can't you spawn more than 4K? Yes, you
can. But here's the problem. The thing
can. But here's the problem. The thing
is really simple problems don't lend
is really simple problems don't lend
themselves to actually learning faster
themselves to actually learning faster
with more environments. Serial
with more environments. Serial
as the environment gets harder, you can
as the environment gets harder, you can
benefit from more parallel Ms. The thing
benefit from more parallel Ms. The thing
is this is just a very bad combination.
is this is just a very bad combination.
It's a simple problem that's really
It's a simple problem that's really
slow.
slow.
Simple and very slow.
Do you know how many Do you know how
Do you know how many Do you know how
long it takes us to learn pong in Puffer
long it takes us to learn pong in Puffer
Lib?
Lib?
3 seconds. It takes us three seconds to
3 seconds. It takes us three seconds to
learn a uh like a 20 score pong policy.
It has to learn pretty fast. Yeah. The
It has to learn pretty fast. Yeah. The
thing is it just doesn't learn fast.
thing is it just doesn't learn fast.
This number is 20 times too slow.
This number is 20 times too slow.
Actually, more than 20 times too slow.
Actually, more than 20 times too slow.
Pong trains 20 times faster than this on
Pong trains 20 times faster than this on
a sixth of the hardware. So, it's over
a sixth of the hardware. So, it's over
100x too slow. And I see the rewards
100x too slow. And I see the rewards
again. The rewards are very sly defined.
again. The rewards are very sly defined.
I will show you.
I will show you.
So, this is how they compute a dense
So, this is how they compute a dense
reward.
reward.
Okay,
Okay,
they have a reward for reaching the goal
they have a reward for reaching the goal
and then they have a is a grasping term.
and then they have a is a grasping term.
You only get a reward for placing the
You only get a reward for placing the
object if you're actually grasping it.
object if you're actually grasping it.
And then I have the static reward. These
And then I have the static reward. These
are state based rewards. So completely
are state based rewards. So completely
wrong. Uh but what we do is we take the
wrong. Uh but what we do is we take the
delta between this at this step and at
delta between this at this step and at
the previous step. So delta of reward at
the previous step. So delta of reward at
previous uh this step to previous step
previous uh this step to previous step
tells you progress towards goal and
tells you progress towards goal and
that's what we use as reward.
I don't know if we're getting hit by a
I don't know if we're getting hit by a
hurricane or what, but everything is
hurricane or what, but everything is
rattling and shaking.
rattling and shaking.
Those
are some pretty high speed winds.
Hopefully this doesn't take out the
Hopefully this doesn't take out the
stream. We'll see.
takes a lot longer to solve here.
Why do you do that? And what's wrong
Why do you do that? And what's wrong
with this reward?
with this reward?
The state based rewards
The state based rewards
when you get close to the goal, you'll
when you get close to the goal, you'll
just hover around it forever and you'll
just hover around it forever and you'll
not actually solve the task.
not actually solve the task.
This is a wrong way to define rewards.
This is a wrong way to define rewards.
And then what the roboticists do is they
And then what the roboticists do is they
modify generalized advantage estimation
modify generalized advantage estimation
to bootstrap on terminals.
to bootstrap on terminals.
We do instead is we just don't do that
We do instead is we just don't do that
and we use delta rewards and it's the
and we use delta rewards and it's the
same thing actually better because you
same thing actually better because you
actually get more fidelity out of your
actually get more fidelity out of your
reward signal.
It seems pretty easy. Well, we do solve
It seems pretty easy. Well, we do solve
it. It's just like it's proven annoying
it. It's just like it's proven annoying
to get it to solve faster.
Right.
I can kind of try to just guess a couple
I can kind of try to just guess a couple
things.
What if I just Yes.
Yeah, that's basically
Yeah, that's basically
the problem. The thing is it's not that
the problem. The thing is it's not that
my general like it's not that my
my general like it's not that my
advantage estimate is wrong, right? My
advantage estimate is wrong, right? My
advantage estimate is right. In general,
advantage estimate is right. In general,
the robotics version of it is wrong.
the robotics version of it is wrong.
At the very least, they have a
At the very least, they have a
specialized J that doesn't work with
specialized J that doesn't work with
other types of tasks. Mine will work
other types of tasks. Mine will work
with both provided you don't define the
with both provided you don't define the
reward the rewards insanely like Yes.
very weird to me how
very weird to me how
volatile the rewards are, you know.
I do wonder if this is a scaling issue.
This is way worse.
Makes sense.
Makes sense.
I see.
lower learning rate.
What if you continuously increase the
What if you continuously increase the
reward?
So because you normally you clip it off
So because you normally you clip it off
at the last state, it's going to rather
at the last state, it's going to rather
it would rather stay right next to the
it would rather stay right next to the
goal, right? Then get reset to the start
goal, right? Then get reset to the start
where it's going to get very low go like
where it's going to get very low go like
a very low reward.
Yeah. So, what's it's going to do is
Yeah. So, what's it's going to do is
it's going to like go right next to the
it's going to like go right next to the
goal and then just sit there. And then
goal and then just sit there. And then
if it's a smart, eventually it'll figure
if it's a smart, eventually it'll figure
out to touch the goal at the very end.
out to touch the goal at the very end.
But probably not.
Add an action to the termination. The
Add an action to the termination. The
way easier solution, right, is to just
way easier solution, right, is to just
not define your rewards like this
not define your rewards like this
is not this is not a good way to define
is not this is not a good way to define
rewards at all anyways because like you
rewards at all anyways because like you
compress the entire signal as well.
compress the entire signal as well.
The delta between one state and another
The delta between one state and another
state might be very small in terms of
state might be very small in terms of
difference in reward.
interestingly doing very badly.
Comment gamma
a ridiculous gamma though. I guess it's
a ridiculous gamma though. I guess it's
because it's a dense reward.
Never trust no RL algorithm to work on a
Never trust no RL algorithm to work on a
new endeavor.
new endeavor.
The thing is ours actually does like we
The thing is ours actually does like we
got it working on this pretty quickly.
got it working on this pretty quickly.
As soon as we fix the rewards, we have
As soon as we fix the rewards, we have
it working on this new end in a new
it working on this new end in a new
domain that we don't do like out of the
domain that we don't do like out of the
box.
box.
Uh it's just that the env is very slow
Uh it's just that the env is very slow
and we would like it to be faster. So
and we would like it to be faster. So
I'm looking for settings where it's
I'm looking for settings where it's
faster and we can still learn.
But yeah, actually try try the Puffer 3
But yeah, actually try try the Puffer 3
trainer. actually works out of the box
trainer. actually works out of the box
on most stuff. It's kind of crazy.
I wonder if I ought to just like sync
I wonder if I ought to just like sync
this up to the environment resets, you
this up to the environment resets, you
know?
I think you can.
Okay. So, it suddenly is at least like
Okay. So, it suddenly is at least like
somewhat better.
Yeah. But it looks like the hyper prams
Yeah. But it looks like the hyper prams
we swept from before are like overall
we swept from before are like overall
reasonable.
reasonable.
not terminating the end in case at all.
Then you'll stop learning because all
Then you'll stop learning because all
the agents will get to the goal position
the agents will get to the goal position
and you just don't reset. You're getting
and you just don't reset. You're getting
worthless data.
Okay, let's hope my roof is really good.
Okay, let's hope my roof is really good.
cuz uh this is a gale outside. My gosh.
cuz uh this is a gale outside. My gosh.
Holy.
My microphone should mostly be filtering
My microphone should mostly be filtering
it, but it's uh rain on a metal roof.
it, but it's uh rain on a metal roof.
So,
very loud. Ah, look at this.
very loud. Ah, look at this.
Okay.
So this is at least relatively cleaner
and nice and stable.
Now my question is with these more
Now my question is with these more
general hyperparams
general hyperparams
where we haven't like maxed the learning
where we haven't like maxed the learning
rate
rate
can we go up to
can we go up to
4096NS
and 32
and 32
32 384
this work now all of a sudden.
And to be safe, I'm getting a very high
And to be safe, I'm getting a very high
quality conversation on RL, which I
quality conversation on RL, which I
can't get at all in the day.
can't get at all in the day.
Yeah, I mean, this is the place. Look, I
Yeah, I mean, this is the place. Look, I
don't even really do robotics. We're
don't even really do robotics. We're
kind of just doing this as like, yeah,
kind of just doing this as like, yeah,
robotics is another RLN. And it's not
robotics is another RLN. And it's not
like we're failing, right? Like the
like we're failing, right? Like the
stuff works. It's just that I would like
stuff works. It's just that I would like
I'd like to find some settings to
I'd like to find some settings to
massively improve the speed here. But
massively improve the speed here. But
the thing is like my intuition here is
the thing is like my intuition here is
just that like this thing has been I've
just that like this thing has been I've
seen this all the time in the rest of
seen this all the time in the rest of
RL, right? You have like an environment
RL, right? You have like an environment
that's been very heavily tuned to work
that's been very heavily tuned to work
in like the defaults. It doesn't mean
in like the defaults. It doesn't mean
that there aren't other ways to set it
that there aren't other ways to set it
up. It's just very annoying and like a
up. It's just very annoying and like a
lot of work. My guess is that like if
lot of work. My guess is that like if
you actually have a very heavily
you actually have a very heavily
randomized setting with way way lower
randomized setting with way way lower
fidelity uh solver params,
fidelity uh solver params,
my estimate here is that it actually
my estimate here is that it actually
will do just as well, if not better.
will do just as well, if not better.
So that's what I'd like to do, but it's
So that's what I'd like to do, but it's
kind of becoming clear that it's pretty
kind of becoming clear that it's pretty
tough to set that type of stuff up.
tough to set that type of stuff up.
I at least want to see if we can like
I at least want to see if we can like
get something that's meaningful though,
get something that's meaningful though,
like some sort of meaningful improvement
like some sort of meaningful improvement
to this thing.
We will see.
This is like the reward is just not
This is like the reward is just not
quite doing it for us.
Okay, let me think about this.
I could keep the mini batch small,
I could keep the mini batch small,
right?
How would you define the reward for this
How would you define the reward for this
problem? Can you elaborate on this?
problem? Can you elaborate on this?
Because I'm bumping this into a spot.
Because I'm bumping this into a spot.
Well, I I actually have it. So, the
Well, I I actually have it. So, the
reward I showed you is not what I'm
reward I showed you is not what I'm
training on. That's their original.
training on. That's their original.
Okay. The reward that I have is
Okay. The reward that I have is
literally just that reward that I showed
literally just that reward that I showed
you computed at the current time step
you computed at the current time step
minus that reward at the previous time
minus that reward at the previous time
step.
step.
So, that tells you progress towards
So, that tells you progress towards
goal. It's unfarmmable. Because if you
goal. It's unfarmmable. Because if you
stay at a point, you get zero. Can't go
stay at a point, you get zero. Can't go
back and forth because that just
back and forth because that just
averages out to be zero. You get a
averages out to be zero. You get a
positive when you go towards the the
positive when you go towards the the
goal. This will work for like any of the
goal. This will work for like any of the
dense reward signals that the
dense reward signals that the
roboticists have set up. So like
roboticists have set up. So like
literally they were a threeline change
literally they were a threeline change
away from having it here.
Little bit off. No.
This doesn't work. Let's hypothesize.
Hypothesize a little bit.
Hypothesize a little bit.
Start with
96.
is the original reward good at all?
is the original reward good at all?
Uh, it's it contains good information,
Uh, it's it contains good information,
but it's not presented correctly. Let's
but it's not presented correctly. Let's
put it that way. So, literally, if you
put it that way. So, literally, if you
just take their reward signal and make
just take their reward signal and make
it delta instead of absolute, it works.
You don't need to do IRL like inverse
You don't need to do IRL like inverse
RL.
RL.
The thing I've given works, right? This
The thing I've given works, right? This
this does work.
We take some more steps than they do to
We take some more steps than they do to
solve it, but like we're literally
solve it, but like we're literally
training one update epoch as well. I
training one update epoch as well. I
haven't messed with that a whole bunch
haven't messed with that a whole bunch
yet.
So,
you have a totally new end. How would
you have a totally new end. How would
you design one?
you design one?
Uh, I can't really comment on robotics
Uh, I can't really comment on robotics
because I don't do like a ton of
because I don't do like a ton of
robotics. their rewards are like they're
robotics. their rewards are like they're
very manual and kind of a pain, but like
very manual and kind of a pain, but like
I see why they do the things. They're
I see why they do the things. They're
just posed wrong. So when I look when I
just posed wrong. So when I look when I
design new environments and I have
design new environments and I have
rewards, I usually do the simplest thing
rewards, I usually do the simplest thing
possible, right? Like I start with just
possible, right? Like I start with just
success and then if it's like obvious
success and then if it's like obvious
that that's way way way too hard to run
that that's way way way too hard to run
into, which is like basically if you
into, which is like basically if you
take random actions and you're never
take random actions and you're never
going to get the success condition, you
going to get the success condition, you
can't just train on that. So then I try
can't just train on that. So then I try
to do like the most lighthanded
to do like the most lighthanded
intermediate reward I possibly can,
intermediate reward I possibly can,
usually some sparse thing, right? And
usually some sparse thing, right? And
then if I end up with more complex that
then if I end up with more complex that
have like three or four different reward
have like three or four different reward
coefficients, uh then I just like I
coefficients, uh then I just like I
think okay, here are three or four
think okay, here are three or four
important things in the environment. I
important things in the environment. I
don't know what their relative waiting
don't know what their relative waiting
is and I put them into a hyperparameter
is and I put them into a hyperparameter
sweep.
Welcome.
Welcome.
Thank you. Yeah, I actually was just
Thank you. Yeah, I actually was just
going through all the YouTube comments
going through all the YouTube comments
uh this morning. I was like, "Hey,
uh this morning. I was like, "Hey,
actually, surprisingly, YouTube is way,
actually, surprisingly, YouTube is way,
way, way happier about all this stuff
way, way happier about all this stuff
than X's." X, they're always mad about
than X's." X, they're always mad about
everything. People here are pretty
everything. People here are pretty
chill.
Okay. Yay. This actually kind of works.
Okay. Yay. This actually kind of works.
Is it better?
It's like kind of good.
This is like kind of good.
Okay.
Oh, actually this is quite good, isn't
Oh, actually this is quite good, isn't
it? Because if I do this,
look at that.
look at that.
And that's our fastest solve yet. Two
And that's our fastest solve yet. Two
minutes.
Let me think if I can make sense of what
Let me think if I can make sense of what
I even just did. I kind of just guessed.
So I reduce the total batch size by a
So I reduce the total batch size by a
factor of four.
I can try eight.
I can try eight.
This is where things start to get
This is where things start to get
sketchy.
sketchy.
I can try eight. If this works better,
I can try eight. If this works better,
it just means the problem's trivial, by
it just means the problem's trivial, by
the way,
the way,
which is kind of expected. Like they use
which is kind of expected. Like they use
four in uh the original. Like this
four in uh the original. Like this
parameter they use like four steps
parameter they use like four steps
which is crazy like absolutely crazy
why don't you use different algorithm
why don't you use different algorithm
what do you mean
we have um
we have um
at least as far as online learning goes
at least as far as online learning goes
like we kind of have the best thing
like we kind of have the best thing
already in puffer
already in puffer
we don't like hop between a bunch of
we don't like hop between a bunch of
different algor algorithms. We just have
different algor algorithms. We just have
the best algorithm.
this uh re this is also not exactly PO
this uh re this is also not exactly PO
like the changes that we have in our
like the changes that we have in our
implementation are substantial enough to
implementation are substantial enough to
consider it a new algorithm and like in
consider it a new algorithm and like in
performance it's a generational
performance it's a generational
advancement over PO and I mean that for
advancement over PO and I mean that for
real this time not like all the papers
real this time not like all the papers
that claim to be better and like
that claim to be better and like
actually aren't this one's like for real
actually aren't this one's like for real
for real better.
Sounds like a paper material.
Sounds like a paper material.
I'm so sick of papers.
I'm so sick of papers.
We have some blogs on it on X if you
We have some blogs on it on X if you
want all the details. It's all open
want all the details. It's all open
source.
The idea that every time you come up
The idea that every time you come up
with something useful, you have it has
with something useful, you have it has
to be exactly eight dense pages of
to be exactly eight dense pages of
really boring content is like it takes a
really boring content is like it takes a
year to review is like kind of
year to review is like kind of
ludicrous.
Get to a place to make a pitch. Yeah, I
Get to a place to make a pitch. Yeah, I
do that. I mean, like with I have like a
do that. I mean, like with I have like a
good chunk of the RL audience on X as
good chunk of the RL audience on X as
like already. If you look at like the
like already. If you look at like the
biggest pure RL accounts and then you
biggest pure RL accounts and then you
look at the size of mine, it's they're
look at the size of mine, it's they're
pretty close to comparable.
pretty close to comparable.
Particularly because I really don't have
Particularly because I really don't have
followers from like other stuff, right?
followers from like other stuff, right?
They're like professors and things that
They're like professors and things that
have like they post politics and other
have like they post politics and other
stuff and they have larger accounts, but
stuff and they have larger accounts, but
like as for people that are just there
like as for people that are just there
because they post pure RL, it's pretty
because they post pure RL, it's pretty
good.
good.
You think that those people on X are
You think that those people on X are
real?
real?
I mean, yeah. I get messages from
I mean, yeah. I get messages from
researchers all the time.
Okay, so I was kind of expecting this to
Okay, so I was kind of expecting this to
work. The fact that it doesn't is
work. The fact that it doesn't is
actually kind of a good thing.
actually kind of a good thing.
What's What license does Puffer use? MIT
What's What license does Puffer use? MIT
free.
The only thing that we sell is we sell
The only thing that we sell is we sell
uh we sell our services, right? You can
uh we sell our services, right? You can
use all our stuff for free, but if you
use all our stuff for free, but if you
you have an RL problem and you're having
you have an RL problem and you're having
a hard time with it in industry, you can
a hard time with it in industry, you can
hire us to help. That includes like
hire us to help. That includes like
support and features around Puffer Lib
support and features around Puffer Lib
that includes direct work on your
that includes direct work on your
problem, custom simulation work, all
problem, custom simulation work, all
that type of stuff.
sement filtering
sement filtering
topics off.
Okay, so we're at 16. Horizon was the
Okay, so we're at 16. Horizon was the
good one.
I guess we just see if we can mess with
I guess we just see if we can mess with
fidelity now, right?
fidelity now, right?
Does this still learn?
We should in theory be a little bit more
We should in theory be a little bit more
robust to fidelity now because we have
robust to fidelity now because we have
massive batches.
Hell stone.
We have a a good number of folks on
We have a a good number of folks on
YouTube today. Hello, welcome.
YouTube today. Hello, welcome.
If you're new around here, this is just
If you're new around here, this is just
all sorts of reinforcement learning
all sorts of reinforcement learning
research done live. It's all open
research done live. It's all open
source. If you want to see a bunch of
source. If you want to see a bunch of
demos that are not me just staring at uh
demos that are not me just staring at uh
training graphs because we're doing
training graphs because we're doing
fiddly robotics today. We have all sorts
fiddly robotics today. We have all sorts
of stuff at puffer.ai,
of stuff at puffer.ai,
click ocean. You can see all these
click ocean. You can see all these
different demo ming
demo to games
demo to games
like this this type of a thing. And the
like this this type of a thing. And the
agents all run in your browser. And many
agents all run in your browser. And many
of these were contributed by uh brand
of these were contributed by uh brand
new people with like zero RL experience
new people with like zero RL experience
coming in.
coming in.
So, if you're interested in getting
So, if you're interested in getting
involved in building crazy stuff Oh,
involved in building crazy stuff Oh,
yeah. I like this one. I built this one.
yeah. I like this one. I built this one.
Each puffer is a different agent.
Each puffer is a different agent.
If you want to get involved with this,
If you want to get involved with this,
you can join the Discord, which is just
you can join the Discord, which is just
discord.gg/puffer.
discord.gg/puffer.
You can also help us out for free by
You can also help us out for free by
starring the GitHub. Also, follow me on
starring the GitHub. Also, follow me on
X for more.
X for more.
If I have stalled long enough, maybe the
If I have stalled long enough, maybe the
policy will have trained.
kind of
kind of
is this stable is my question.
If that's a stable 400k
does take longer
does take longer
steps.
Where's RL used in industry side aside
Where's RL used in industry side aside
from releasing things R1 or is it just
from releasing things R1 or is it just
catching on? It's a lot of random
catching on? It's a lot of random
problems.
problems.
Like RL is basically any anytime you
Like RL is basically any anytime you
have a sim and like a fiddly unintuitive
have a sim and like a fiddly unintuitive
optimization problem.
We've done all sorts of like weird
We've done all sorts of like weird
random things. The thing is like RL has
random things. The thing is like RL has
just kind of been cast to the wayside
just kind of been cast to the wayside
because nobody could get it to work. But
because nobody could get it to work. But
like nobody could get it to work because
like nobody could get it to work because
everybody did all the engineering wrong.
everybody did all the engineering wrong.
So just in the last year we've made so
So just in the last year we've made so
many things a thousand times faster.
many things a thousand times faster.
We've been able to like make some
We've been able to like make some
fundamental algorithm breakthroughs. Uh
fundamental algorithm breakthroughs. Uh
we've been able to like really really
we've been able to like really really
hone in and optimize a bunch of things.
hone in and optimize a bunch of things.
And now it it feels like a different
And now it it feels like a different
field at the very least. If nothing
field at the very least. If nothing
else, it feels like a totally different
else, it feels like a totally different
field from
Okay, this is actually pretty solid.
Okay, this is actually pretty solid.
We're at like two minute solve. Super
We're at like two minute solve. Super
low fidelity.
Can we crank up control?
Can we crank up control?
Demos on the website are fun to play
Demos on the website are fun to play
with. Thank you.
with. Thank you.
We had to do quite a bit of stuff to get
We had to do quite a bit of stuff to get
those to work.
those to work.
So those all of those agents those are
So those all of those agents those are
running like pure C versions of the
running like pure C versions of the
neural nets that we trained in PyTorch.
neural nets that we trained in PyTorch.
So we take the PyTorch weights, we load
So we take the PyTorch weights, we load
them into like this single file C
them into like this single file C
library that we made and then compile
library that we made and then compile
all that to web assembly including the
all that to web assembly including the
environments.
environments.
What do you think about VALA's?
What do you think about VALA's?
LA
LA
VMs or VAS.
VMs or VAS.
What's a VA?
What's a VA?
Very large action models. I don't know.
action models. Eh,
action models. Eh,
if you get enough data for a specific
if you get enough data for a specific
enough problem, sure. I kind of really
enough problem, sure. I kind of really
don't care what's happening in the world
don't care what's happening in the world
of like let's train massive like
of like let's train massive like
ridiculously sized models that are super
ridiculously sized models that are super
slow
slow
because like
because like
it's kind of funny that we are we're
it's kind of funny that we are we're
able to like solve like individual
able to like solve like individual
problems in seconds with these like
problems in seconds with these like
absolutely tiny models that you can just
absolutely tiny models that you can just
deploy anywhere.
deploy anywhere.
And like obviously this is a lot newer
And like obviously this is a lot newer
in the sense that it hasn't had billions
in the sense that it hasn't had billions
of dollars of investment poured into it,
of dollars of investment poured into it,
but like you can kind of see just from
but like you can kind of see just from
the progress we've made in the last year
the progress we've made in the last year
where some of this is going.
Okay. I mean, so this now works.
So this isn't a thing that they thought
So this isn't a thing that they thought
I would be able to get to work already.
I would be able to get to work already.
That's exciting.
That's exciting.
Can I do
let me think about this a little bit
let me think about this a little bit
carefully
BLM with imitation.
BLM with imitation.
Any wisdom on metalarning
Any wisdom on metalarning
hyperparameters?
I've been going off the heristic that if
I've been going off the heristic that if
puffer doesn't do it then it can't be
puffer doesn't do it then it can't be
great. Yes, except there's one small
great. Yes, except there's one small
caveat. Finn, there was one quirk with
caveat. Finn, there was one quirk with
our hyperparam sweep algorithm that I
our hyperparam sweep algorithm that I
could not fix uh in time for release. If
could not fix uh in time for release. If
you see that your problem is forcing all
you see that your problem is forcing all
the runs to be the minimum number of
the runs to be the minimum number of
time steps
time steps
that you specify in the sweep, you might
that you specify in the sweep, you might
want to try uh with one of like the down
want to try uh with one of like the down
sample equals zero configs.
sample equals zero configs.
And what that does is that basically it
And what that does is that basically it
tells the hyperparam sweep algorithm to
tells the hyperparam sweep algorithm to
not try to learn using the entire
not try to learn using the entire
training curve to just use the end
training curve to just use the end
point. And that's kind of needed for
point. And that's kind of needed for
some of the environments.
That's the only stumbling block.
Let me think about this a little bit
Let me think about this a little bit
more principled if I can. Um,
if I just hit 40 here,
this do anything.
Let's actually see what this does. I'm
Let's actually see what this does. I'm
curious.
I think this just gives you like fake
I think this just gives you like fake
additional fake frames.
gives you a lot of fake frames though
like approaching a respectable training
like approaching a respectable training
speed
hyper pram free RL ever be possible.
I will say this, the defaults that we
I will say this, the defaults that we
have in Puffer Lib work on like almost
have in Puffer Lib work on like almost
all the environments.
Like they work out of the box on a huge
Like they work out of the box on a huge
number of problems, but we're getting
number of problems, but we're getting
closer to that.
closer to that.
I think we could get to a point where
I think we could get to a point where
like we have defaults that work pretty
like we have defaults that work pretty
darn well out of the box on almost
darn well out of the box on almost
everything. The main thing, the main
everything. The main thing, the main
thing that is not solved yet is gamma.
thing that is not solved yet is gamma.
So generalized advantage estimation
So generalized advantage estimation
imposes a horizon on your tasks and it's
imposes a horizon on your tasks and it's
really screwy. So we would have to like
really screwy. So we would have to like
we'd basically have to come up with a
we'd basically have to come up with a
replacement for generalized advantage
replacement for generalized advantage
estimation.
That was through a search. Yeah. But the
That was through a search. Yeah. But the
thing is we did the search once. All
thing is we did the search once. All
right. We did the search on like
right. We did the search on like
breakout and those same parameters work
breakout and those same parameters work
on like at least six or eight of the
on like at least six or eight of the
environments. It just works.
environments. It just works.
I think more like 12 of them actually.
I think more like 12 of them actually.
Really the only ones it doesn't work
Really the only ones it doesn't work
like the bigger models need different
like the bigger models need different
learning rates and you can show that you
learning rates and you can show that you
need to decrease the learning rate and
need to decrease the learning rate and
then there's some environments that need
then there's some environments that need
different gamas.
model based dynamics based agent can
model based dynamics based agent can
solve the hyper prim. No, they can't.
solve the hyper prim. No, they can't.
How do you figure?
How do you figure?
What part about having a model solves
What part about having a model solves
hyperparameter tuning?
hyperparameter tuning?
If you quote the dreamer paper, I'm
If you quote the dreamer paper, I'm
going to submit a different academic
going to submit a different academic
paper back at you. Fair warning.
Mr. Q. I've been meaning to look at
Mr. Q. I've been meaning to look at
that. It's like big paper. Been meaning
that. It's like big paper. Been meaning
to look at that
model. Well,
model. Well,
there's something to modelbased RL, I
there's something to modelbased RL, I
think. But the thing is like
think. But the thing is like
it's like the rest of RL where all the
it's like the rest of RL where all the
science is done absolutely horribly. So,
science is done absolutely horribly. So,
it's kind of hard to tell.
So, this thing just doesn't do very
So, this thing just doesn't do very
well. Um,
okay. Can I Can I fix this?
You go find the thing that he told me to
You go find the thing that he told me to
adjust.
Arm PD joint delta pause.
arm
arm
point delta pause.
point delta pause.
Okay, so this is what he told me
and this is what he told me to mess
and this is what he told me to mess
with. So let me see if this does
with. So let me see if this does
anything.
efficient zero v2.
I actually think some of them use zero
I actually think some of them use zero
line of work makes more sense.
Like that makes more sense to me. Some
Like that makes more sense to me. Some
of it does.
I think a lot of the a lot of the
I think a lot of the a lot of the
algorithm work I want to do will look at
algorithm work I want to do will look at
off uh off policy as well as model based
off uh off policy as well as model based
but like we kind of have to just throw
but like we kind of have to just throw
puffer liib on a bunch of different
puffer liib on a bunch of different
applications first, right?
applications first, right?
Gain some more industry traction.
So, if this doesn't work, then there's
So, if this doesn't work, then there's
something that's not behaving as is uh
something that's not behaving as is uh
as anticipated
as anticipated
because technically
because technically
if we double the SIMs, like if we double
if we double the SIMs, like if we double
the control frequency and we double the
the control frequency and we double the
actions, it should work just the same.
I'm going to run this first to make sure
I'm going to run this first to make sure
it doesn't actually work.
[Music]
There is a difference,
but it's not like massive.
reward really jumps all over.
Yeah. So this is very similar to the
Yeah. So this is very similar to the
ABS.
How can you run multiple sweeps in
How can you run multiple sweeps in
parallel
parallel
like for different environments
like for different environments
and just launch different processes on
and just launch different processes on
different GPUs?
different GPUs?
Uh we don't have the ability to use
Uh we don't have the ability to use
multiple GPUs in the same sweep yet.
multiple GPUs in the same sweep yet.
That is something that we should add. We
That is something that we should add. We
do have the ability to do multiGPU
do have the ability to do multiGPU
training.
training.
Uh, we don't have the ability to do use
Uh, we don't have the ability to do use
like multiple GPUs where each GPU
like multiple GPUs where each GPU
contributes to the same sweep.
contributes to the same sweep.
That's one of the things I've been
That's one of the things I've been
meaning to add.
different process.
different process.
New man
out of here. See you. Thank you.
out of here. See you. Thank you.
I do this pretty much every day.
See, this is obnoxious because if we
See, this is obnoxious because if we
could get this to work
like this is the theoretical max SPS
like this is the theoretical max SPS
that I've obtained with this, right?
like pretty decent.
YouTube comments pretty slow.
YouTube comments pretty slow.
Yeah, the latency is a little better on
Yeah, the latency is a little better on
Twitch. Not much I can really do about
Twitch. Not much I can really do about
YouTube being YouTube.
Let me just try one like one weird thing
Let me just try one like one weird thing
just in case.
just in case.
like just in case
which is that technically you have to
which is that technically you have to
increase gamma
the way it's defined here
the way it's defined here
because you create a longer horizon
because you create a longer horizon
problem.
turn kind of just gets stuck.
Irritating
Yeah. Dang.
Okay.
Go back to the original 20.
Go back to the original 20.
I do like this.
Let's see if this is still stable.
See
if we can send Stone a cool policy.
Okay.
You managed to train 2048 and put it on
You managed to train 2048 and put it on
the website. I have not done that yet.
the website. I have not done that yet.
Yanick,
Yanick,
I need to do that though because that is
I need to do that though because that is
a cool environment. I've been
a cool environment. I've been
sidetracked by robotics and other
sidetracked by robotics and other
things.
things.
do have a cool new article up though
if you want to figure out uh if you want
if you want to figure out uh if you want
to hear how we did RL on a pabyte of
to hear how we did RL on a pabyte of
data one server
data one server
here's a nice write up here on X
wrote that this morning
that's up Here
I did
I did
if this is stable.
You have fully integrated multiGPU. Yes,
You have fully integrated multiGPU. Yes,
we do.
You're looking at it here.
you ve the no that's not how you do it
you ve the no that's not how you do it
and that's not how you'd want to do it.
and that's not how you'd want to do it.
If you did it that way then you would
If you did it that way then you would
like you'd multiply the overhead of any
like you'd multiply the overhead of any
CPU operation by the number of GPUs. So
CPU operation by the number of GPUs. So
we use torch run which launches
we use torch run which launches
independent processes per trainer and
independent processes per trainer and
then DDP just averages the gradients.
Imagine
this should right.
Not bad.
Does it scale two GPUs double as fast?
Does it scale two GPUs double as fast?
Depends on your environment speed.
Depends on your environment speed.
If you from the article, right, 2.2
If you from the article, right, 2.2
million training steps per second for a
million training steps per second for a
neural MMO 3 compared to like 400
neural MMO 3 compared to like 400
between 400 and 500k on one GPU.
Pretty good scaling.
To be fair, that's not even completely
To be fair, that's not even completely
fair because uh there's like some CPU
fair because uh there's like some CPU
conflicts as well doing neural MMO like
conflicts as well doing neural MMO like
that
that
overhead there as well.
overhead there as well.
Depends on your environment can be
Depends on your environment can be
linear.
Okay. So,
2 minutes 30 seconds.
Amusingly, we've managed to take more
Amusingly, we've managed to take more
steps to solve in about the same amount
steps to solve in about the same amount
of time.
of time.
But the uh the hope here is that this
But the uh the hope here is that this
will now allow us to
will now allow us to
do better.
We wanted to preserve the number of
We wanted to preserve the number of
gradient updates,
gradient updates,
make learning more stable.
make learning more stable.
What happens if I do like this?
What happens if I do like this?
Let's do 16384
update
update
2.
2.
That should be the same number of
That should be the same number of
updates to the policy
but they but more stable.
Welcome to all the uh stream's been
Welcome to all the uh stream's been
doing well the last couple of days.
Welcome to all the new folks here.
We'll of course have to actually look at
We'll of course have to actually look at
the policy. I don't want to run too many
the policy. I don't want to run too many
more of these because I got to run for
more of these because I got to run for
dinner uh a little bit before 6.
dinner uh a little bit before 6.
Like to be able to look at it.
Good work. Thank you.
I'd like some of these articles on X to
I'd like some of these articles on X to
do better. I mean, they they take quite
do better. I mean, they they take quite
a bit of time to write.
Amusingly, like the literally like AI
Amusingly, like the literally like AI
clipped shorts out of the stream have
clipped shorts out of the stream have
done like surprisingly well on YouTube.
done like surprisingly well on YouTube.
They get like over a thousand views. I
They get like over a thousand views. I
think one of them got like 2,000
think one of them got like 2,000
something. The other got a thousand. are
something. The other got a thousand. are
just like literally clips from this
just like literally clips from this
random stuff I do. I don't know what's
random stuff I do. I don't know what's
up with the shorts algorithm, but I
up with the shorts algorithm, but I
don't know if I if I do anything like
don't know if I if I do anything like
interesting like an explanation of
interesting like an explanation of
something or other, I might just start
something or other, I might just start
adding those.
So, are we not stable here?
We're comparing this one
We're comparing this one
to this one.
We have something.
We have something.
We have this metric.
We have this metric.
Huh.
Yeah. See, this is one of those weird
Yeah. See, this is one of those weird
things I can't even explain.
things I can't even explain.
This should pretty much always be
This should pretty much always be
strictly better.
strictly better.
Oh, no. It does track. Hang on. It's
Oh, no. It does track. Hang on. It's
actually starting to track this.
Okay, so it's pretty close to
Okay, so it's pretty close to
comparable.
comparable.
Uh, which means that we're not really
Uh, which means that we're not really
benefiting from larger mini batches.
benefiting from larger mini batches.
Played with a couple config brims. Got a
Played with a couple config brims. Got a
good run in 15 minutes training
good run in 15 minutes training
multiGPU.
multiGPU.
Solid.
Solid.
Yeah, we really need to do multiGPU
Yeah, we really need to do multiGPU
sweeps. I think Spencer, I just haven't
sweeps. I think Spencer, I just haven't
had the chance to add it. It kind of
had the chance to add it. It kind of
takes a little bit of work.
Do this instead. So, this is doubling
Do this instead. So, this is doubling
the number of updates. It's a very
the number of updates. It's a very
different thing. This will be the last
different thing. This will be the last
run we'll do. After this, we'll like
run we'll do. After this, we'll like
well the last new run we'll do. After
well the last new run we'll do. After
this, we'll train our baseline. Uh we'll
this, we'll train our baseline. Uh we'll
grab the policy. We'll like output some
grab the policy. We'll like output some
rendering and we'll call it
Alternative title for stream will be
Alternative title for stream will be
watch as I horribly break break all of
watch as I horribly break break all of
your Sims roboticists
your Sims roboticists
in order to make them actually run quick
in order to make them actually run quick
as they should.
18 minutes. Got six people, not five.
18 minutes. Got six people, not five.
Yeah.
What do you mean you forgot six, not
What do you mean you forgot six, not
five? How's that affect the time?
Change the access to real time and you
Change the access to real time and you
call it a day.
Okay, this is getting lift.
Okay, this is getting lift.
Yeah, now that we have all the stuff
Yeah, now that we have all the stuff
like total chaos or update epox does
like total chaos or update epox does
something.
It's kind of hard to see be, but this is
It's kind of hard to see be, but this is
the the green curve down here. This was
the the green curve down here. This was
our baseline before,
our baseline before,
and now it looks like we're getting lift
and now it looks like we're getting lift
here.
Hey, there we go.
That how far away you do from doing
That how far away you do from doing
something like OpenAI 5 if you can
something like OpenAI 5 if you can
already train for such a long time. Two
already train for such a long time. Two
problems with Open AI5. One, no matter
problems with Open AI5. One, no matter
what you do, you will need 50,000 CPU
what you do, you will need 50,000 CPU
cores to run Dota. All right.
cores to run Dota. All right.
uh and two
uh and two
the observation structure is really
the observation structure is really
obnoxious to make fast.
So I think that the result like I like I
So I think that the result like I like I
wrote in the paper I think that the
wrote in the paper I think that the
neural MMO result is a better result
neural MMO result is a better result
than emergent tool.
than emergent tool.
Dota is still like Dota is still pretty
Dota is still like Dota is still pretty
hard.
This is a very good result.
We're happy with this.
I go to four
99.5 to 99.6 Six
99.9999
99.9999
required Spencer
make the thing be perfect Everything.
Literally need to be able to just run
Literally need to be able to just run
the policy on an actual car and have it
the policy on an actual car and have it
somehow work.
There's a car that's physically in the
There's a car that's physically in the
way and there's another one behind you.
way and there's another one behind you.
There's no way to escape. Quantum tunnel
There's no way to escape. Quantum tunnel
through the car. Figure it out, man.
The uh the environment Spencer is
The uh the environment Spencer is
working on is a collab with NYU.
You can actually fiddle with pretty darn
You can actually fiddle with pretty darn
recent version of it on the website of
recent version of it on the website of
all the cars.
You can actually hold like space and go
You can actually hold like space and go
into first person mode
into first person mode
and it simulates these like short little
and it simulates these like short little
driving scenes. We're adding much longer
driving scenes. We're adding much longer
scenes to this as well.
scenes to this as well.
Yeah, it's pretty cool.
Uh New York University. Yeah.
There are a couple pretty cool RL labs
There are a couple pretty cool RL labs
over there.
Sadly, not a ton going on at MIT at the
Sadly, not a ton going on at MIT at the
moment as far as I'm aware. Otherwise,
moment as far as I'm aware. Otherwise,
we'd be, you know, we'd support them.
we'd be, you know, we'd support them.
Little annoying.
Okay,
slightly worse in terms of time
slightly worse in terms of time
than the uh the previous one.
than the uh the previous one.
We'll go back to two update epochs.
Hopefully this saves a policy for us. I
Hopefully this saves a policy for us. I
don't know if I'll get to actually check
don't know if I'll get to actually check
it before I got to run for dinner in
it before I got to run for dinner in
case I do have to run because I'll have
case I do have to run because I'll have
to just go all of a sudden. Um,
to just go all of a sudden. Um,
for folks watching,
puffer.ai for all the things, the GitHub
puffer.ai for all the things, the GitHub
to help us out. It's free. really helps
to help us out. It's free. really helps
us out. It's all open source and uh
us out. It's all open source and uh
yeah, join the Discord if you want to
yeah, join the Discord if you want to
actually get involved with building some
actually get involved with building some
of this cool stuff.
of this cool stuff.
We've had several people come in with
We've had several people come in with
like either no AI background or even
like either no AI background or even
some with no programming background been
some with no programming background been
able to get up and running pretty
able to get up and running pretty
quickly. Like we try to make it pretty
quickly. Like we try to make it pretty
easy. There's also a b there a whole
easy. There's also a b there a whole
bunch of resources on here to help you
bunch of resources on here to help you
as well. The blog, we have more articles
as well. The blog, we have more articles
on X, lots of things.
Okay,
go get ston a couple of shots from Yes.
We're dropping by.
It's a pretty good result here.
I honestly don't even know if it needs a
I honestly don't even know if it needs a
bigger network at all.
I think that's a new peak for the
I think that's a new peak for the
stream. That's uh I can never tell how
stream. That's uh I can never tell how
many on X, but that's at least 14
many on X, but that's at least 14
concurrent. So, cool.
Thanks for uh dropping in, folks. I got
Thanks for uh dropping in, folks. I got
to run for dinner. I will be back either
to run for dinner. I will be back either
after dinner or tomorrow. See how I'm
after dinner or tomorrow. See how I'm
feeling. Uh and I'll keep working on
feeling. Uh and I'll keep working on
robotics
robotics
large scale tactical battle end and all
large scale tactical battle end and all
that. So, thank you and uh have a nice
that. So, thank you and uh have a nice
evening.
