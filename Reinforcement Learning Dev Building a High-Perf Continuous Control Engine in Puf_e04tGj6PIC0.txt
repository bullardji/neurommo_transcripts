Kind: captions
Language: en
sat down this morning did a little bit
sat down this morning did a little bit
of little bit of work fix some of the
of little bit of work fix some of the
puffer box that were down
puffer box that were down
um and then I realized
um and then I realized
that uh I wasn't really feeling like
that uh I wasn't really feeling like
doing some of the low-level nitty-gritty
doing some of the low-level nitty-gritty
stuff uh that I was going to have to do
stuff uh that I was going to have to do
so I figured I'd just flip things around
so I figured I'd just flip things around
and work a little bit more on this now I
and work a little bit more on this now I
thought of a way to get around the uh
thought of a way to get around the uh
annoying model loading thing that I was
annoying model loading thing that I was
looking at yesterday will require some
looking at yesterday will require some
kind of substantial changes to puffer
kind of substantial changes to puffer
lib so it's going to take a few days
lib so it's going to take a few days
before all this gets merged
before all this gets merged
into I can probably merge it into Dev
into I can probably merge it into Dev
but before it gets merged into main
but before it gets merged into main
will'll be a little
will'll be a little
bit um but yeah let's just start from
bit um but yeah let's just start from
there uh I found a
okay Chris you can fight me on Twitter
okay Chris you can fight me on Twitter
as much as you want but this is just
as much as you want but this is just
this is just wrong this is because the
this is just wrong this is because the
baselines are horribly
baselines are horribly
tuned
um you do good work though I will give
um you do good work though I will give
you
that where's the thing that I posted cuz
that where's the thing that I posted cuz
I found a nice way of putting it this is
I found a nice way of putting it this is
what I want to answer with this project
what I want to answer with this project
object is continuous control
object is continuous control
fundamentally harder to learn than a
fundamentally harder to learn than a
discret action
discret action
space and I think that the way to answer
space and I think that the way to answer
this problem is to sort of separate
this problem is to sort of separate
continuous control from what people
continuous control from what people
think of as you know as continuous
think of as you know as continuous
control being
control being
robotics so just fundamentally if you
robotics so just fundamentally if you
make the action space continuous does
make the action space continuous does
that make the problem harder because
that make the problem harder because
like are robots hard to are robots hard
like are robots hard to are robots hard
to deal with is a different problem from
to deal with is a different problem from
uh is continuous control
uh is continuous control
hard so this is kind of like the the
hard so this is kind of like the the
essence of the problem and if these M's
essence of the problem and if these M's
can be made hyperf enough then we're
can be made hyperf enough then we're
going to just be able to exhaustively
going to just be able to exhaustively
run everything uh run everything against
run everything uh run everything against
this and it will work give quick a Chris
this and it will work give quick a Chris
reply on this and then start
parallel for
whatever let's go back to
whatever let's go back to
this so the thing I realized is that uh
this so the thing I realized is that uh
we need a way to support multiple
we need a way to support multiple
different policies within each um each
different policies within each um each
environment or Suite because like ocean
environment or Suite because like ocean
for instance puffer environments we want
for instance puffer environments we want
to have different policies for the
to have different policies for the
different environments in there and
different environments in there and
conceivably you could want to have
conceivably you could want to have
different architectures for um different
different architectures for um different
environments in other environment Suites
environments in other environment Suites
as well so what we're going to
do go into
do go into
puffer and we're going to add
puffer and we're going to add
a uh this policy here uh argument we
a uh this policy here uh argument we
need to add like a policy name or
need to add like a policy name or
whatever policy
whatever policy
name and this is just going to be uh
policy and we can
policy and we can
do
do
uh this can be RNN
name
name
oops none
oops none
or actually it's just this and then
or actually it's just this and then
we're going to have to adjust some
we're going to have to adjust some
annoying configurations in order to make
annoying configurations in order to make
all of this work but the idea is going
all of this work but the idea is going
to be that we're going to have uh
to be that we're going to have uh
multiple different policy names
multiple different policy names
specified
specified
here so if I go into where is it o
here so if I go into where is it o
ocean these ones will have the
ocean these ones will have the
default um but then grid continuous will
default um but then grid continuous will
have
our that name is recurrent we'll have
our that name is recurrent we'll have
policy name is going to be
grid and then instead of having these
grid and then instead of having these
all named
all named
policy where is it this one will be
policy where is it this one will be
policy the default
policy the default
model okay and then this one will be
model okay and then this one will be
uh this one's
snake and this one is
grid and in order to fix this hold on
grid and in order to fix this hold on
let me check the stream real
let me check the stream real
quick cool yeah this works so in order
quick cool yeah this works so in order
to fix
to fix
this uh we're going to have to just
this uh we're going to have to just
modify the way that the config loader
modify the way that the config loader
Works a little
Works a little
bit which is I hate having to edit this
bit which is I hate having to edit this
thing I really do um I wish there were a
thing I really do um I wish there were a
simpler way to do this
simpler way to do this
but this is like a multi-stage config
but this is like a multi-stage config
loader and this is as simple as I could
loader and this is as simple as I could
get it and it's still a pain to edit um
get it and it's still a pain to edit um
so we're going to do policy
name and where did they get RNN arcs
yeah so policy name
yeah so policy name
equals not big de yet policy
name policy name
none well no it has to have this so it's
none well no it has to have this so it's
just going to be
n how do we how did we do the uh the
n how do we how did we do the uh the
override thing before
end of
policy
policy
uh so obnoxious I'm trying to do like
uh so obnoxious I'm trying to do like
this cascading inherited thing I I don't
this cascading inherited thing I I don't
want to go rewrite the whole config now
want to go rewrite the whole config now
I've been tempted to for a while but
I've been tempted to for a while but
then that's going to be like a 5H hour
then that's going to be like a 5H hour
Rabbit Hole uh and I don't want to do
Rabbit Hole uh and I don't want to do
that
that
so for now
so for now
we will
do isn't there one that we did this
already yeah right like this this type
already yeah right like this this type
of a thing so we're just going to add
of a thing so we're just going to add
this as a hack for now and um we'll go
this as a hack for now and um we'll go
from here so this is going to be policy
name policy name
okay so now we have the policy
name and then we have uh policy what is
name and then we have uh policy what is
it policy class class
it policy class class
equals get Adder
policy
class
name we're actually going to be able to
name we're actually going to be able to
get rid of the use RNN thing so that's
get rid of the use RNN thing so that's
kind of
kind of
nice RNN name RNN class RN
args we'll get rid of just use
RNN oh wait shoot we have to
RNN oh wait shoot we have to
add do we have to add stuff
add do we have to add stuff
here we use
RNN yes we
do all right
hopefully that's
good let's just start running stuff and
good let's just start running stuff and
debugging that way
okay has no attribute
okay has no attribute
grid uh get Adder end module policy
name do
torch okay invalid argument CNN channels
torch okay invalid argument CNN channels
to
default that is
default that is
reasonable do we actually have that in
reasonable do we actually have that in
there that shouldn't be
there that shouldn't be
there oh wait no because we
there oh wait no because we
have what's policy
name what is policy
name it's grid
name it's grid
unrelated but is loss of network
unrelated but is loss of network
plasticity something that has cropped up
plasticity something that has cropped up
as an
as an
issue it's hard to
issue it's hard to
say and it depends what you mean yes
say and it depends what you mean yes
there's been some evidence of that in
there's been some evidence of that in
reinforcement learning in general
reinforcement learning in general
there's this thing called rerun that
there's this thing called rerun that
I've been wanting to try out for a while
I've been wanting to try out for a while
now that's like that some of the I have
now that's like that some of the I have
this plan that's like make everything
this plan that's like make everything
fast make a bunch of good environments
fast make a bunch of good environments
and then be able to run lots of fast
and then be able to run lots of fast
experiments to determine stuff like that
experiments to determine stuff like that
conclusively rather than the thing what
conclusively rather than the thing what
that is happening now where like you run
that is happening now where like you run
a very small number of experiments and
a very small number of experiments and
get very weak evidence for something um
get very weak evidence for something um
but there are things like rerun that is
but there are things like rerun that is
something that is potentially cropped up
something that is potentially cropped up
and uh it definitely I can get I can
and uh it definitely I can get I can
tell you one place for sure that it is
tell you one place for sure that it is
an issue let's say that you have an
an issue let's say that you have an
environment where you have a big action
environment where you have a big action
space and you don't need half of it
space and you don't need half of it
until later on in the game like maybe
until later on in the game like maybe
you can use a bow right there's an
you can use a bow right there's an
action space portion that's for a bow
action space portion that's for a bow
but you don't get bow until halfway
but you don't get bow until halfway
through the game well by the time you
through the game well by the time you
get the bow the model is going to have
get the bow the model is going to have
learned to never take those actions so
learned to never take those actions so
that type of a thing is absolutely a
that type of a thing is absolutely a
problem um more generally we'll have to
problem um more generally we'll have to
wait for the experiments check out the
wait for the experiments check out the
rerun
paper was definitely a good idea to set
paper was definitely a good idea to set
the multi streaming on this to uh to
the multi streaming on this to uh to
YouTube as well since it seems like
YouTube as well since it seems like
folks do actually see it from there as
folks do actually see it from there as
well so that's
cool goal for today is to fix up a bunch
cool goal for today is to fix up a bunch
of the mess that I made with these
of the mess that I made with these
multiple policies and then start on
multiple policies and then start on
getting like actual training done with
getting like actual training done with
uh some of these grid environments I
uh some of these grid environments I
have a spare puffer box reserved
have a spare puffer box reserved
specifically for this purpose today I
specifically for this purpose today I
have one of them running a 10 billion
have one of them running a 10 billion
experiment on a fun environment that I'm
experiment on a fun environment that I'm
working on and one of them reserved for
working on and one of them reserved for
Dev
today so I think it's just
um oh that is you linky I thought that
um oh that is you linky I thought that
was from
was from
yesterday fair
yesterday fair
enough I think the video quality is
enough I think the video quality is
actually better on YouTube than on X as
actually better on YouTube than on X as
well because I'm streaming at 1080p but
well because I'm streaming at 1080p but
X goes uh only up to
720 but most of my audience is on is on
720 but most of my audience is on is on
X
X
right though I actually I guess with the
right though I actually I guess with the
uh the thesis video I did maybe not so
uh the thesis video I did maybe not so
much um I got to stop starting all these
much um I got to stop starting all these
new projects because I have promised a
new projects because I have promised a
high quality video on puffer 10 um I
high quality video on puffer 10 um I
have a really cool idea for a video and
have a really cool idea for a video and
it's it's just going to take me like
it's it's just going to take me like
several days to do
several days to do
it yeah I know you were here yesterday
it yeah I know you were here yesterday
well I mean this will be good for you to
well I mean this will be good for you to
have on in the background because you'll
have on in the background because you'll
get uh you know if you're looking for
get uh you know if you're looking for
Dev stuff this will absolutely teach you
Dev stuff this will absolutely teach you
a few
a few
things uh link is one of our
things uh link is one of our
contributors on uh the Pokemon Red
contributors on uh the Pokemon Red
project he's done awesome over there
I hate this code right here so
much but if the code does not bring you
much but if the code does not bring you
pain you never go make it
pain you never go make it
better um where's this make policy
function wait why am I returning
function wait why am I returning
make why am I returning make policy that
make why am I returning make policy that
makes absolutely no
makes absolutely no
sense I'm sure that this was like
sense I'm sure that this was like
leftover from something else right this
leftover from something else right this
should be
should be
um what
um what
policy no I don't need I because I have
policy no I don't need I because I have
it in config now
it in config now
right yeah so let's not touch this for
right yeah so let's not touch this for
now even though it's stupid just so I
now even though it's stupid just so I
don't break stuff uh and then this is
don't break stuff uh and then this is
going to
be this is args of I think this is like
be this is args of I think this is like
args of policy class
args of policy class
right yeah and then this
right yeah and then this
is
is
args here so if args
args here so if args
of RNN class is not none yeah there you
of RNN class is not none yeah there you
go thanks super
Maven invalid
arguments one
102 ah right here this
102 ah right here this
is validate args is going to
is validate args is going to
be end
class or no policy class what is it
class or no policy class what is it
policy class yeah
did I not add this to the
did I not add this to the
ARs what do I call this
ARs what do I call this
policy policy name
policy policy name
ooh so I do actually need to return this
ooh so I do actually need to return this
that's
that's
obnoxious okay
we'll do
we'll do
policy policy class and RNN class we'll
policy policy class and RNN class we'll
just say this is going to be EnV en
just say this is going to be EnV en
module policy class RN class and then
module policy class RN class and then
this will be
this will be
if policy class super Maven will
if policy class super Maven will
probably get the
probably get the
idea
idea
yeah wait
yeah wait
what no it did not get this whatsoever
what no it did not get this whatsoever
did
did
it that just was uh
bad so we do policy is
bad so we do policy is
equaly isal to policy
equaly isal to policy
class
class
this if RNN class is not none then we
this if RNN class is not none then we
make it an
RNN
RNN
else yeah there we go this is what we
else yeah there we go this is what we
want uh and we do not need end module
want uh and we do not need end module
here anymore
here anymore
right don't need end
right don't need end
module so we do let's go get the
module so we do let's go get the
signature for load
config policy class RN
config policy class RN
class and
then I guess all this junk has to go
then I guess all this junk has to go
into train
into train
right make
right make
n policy class
policy class and RN
class I really hate this demo file
class I really hate this demo file
everybody hates this demo file to be
everybody hates this demo file to be
fair but so far nobody has given me a
fair but so far nobody has given me a
better like a better way to do this file
better like a better way to do this file
I've tried very hard to make this file
I've tried very hard to make this file
reasonable just nobody's come up with a
reasonable just nobody's come up with a
good way of doing it
of policy class RNN class args
policy
class
so you're going to be very dis appointed
so you're going to be very dis appointed
by my Vim
config
config
um that's such a bait question oh my
um that's such a bait question oh my
gosh I it is on here I think
right yeah so I think that I just forgot
right yeah so I think that I just forgot
to
to
change oh no yeah this is updated
change oh no yeah this is updated
um I used Vim with no plugin nothing for
um I used Vim with no plugin nothing for
like six years now I have two plugins I
like six years now I have two plugins I
have super Maven and I have SEI for some
have super Maven and I have SEI for some
extra syntax highlights I don't really
extra syntax highlights I don't really
have any extra commands or anything on
have any extra commands or anything on
here and then all of this is just my uh
here and then all of this is just my uh
the color scheme that I made for
the color scheme that I made for
this that's literally it and as it turns
this that's literally it and as it turns
out we actually fixed training while I
out we actually fixed training while I
was talking that talking about that but
was talking that talking about that but
yeah puffer tank comes with my uh my Vim
yeah puffer tank comes with my uh my Vim
config and neim set up in it just
config and neim set up in it just
cuz
convenient okay so this is nice that
convenient okay so this is nice that
this actually
this actually
trains let's make sure um eval works
does this
does this
work make
policy no because it needs
the policy class and RNN class right
so ar. M policy
class I've always thought you're kind of
class I've always thought you're kind of
missing the point if you spend a whole
missing the point if you spend a whole
bunch of
bunch of
time like hyper fixating on
time like hyper fixating on
your on like making a very complicated
your on like making a very complicated
editor cont fig or like system setup or
something on the flip side you know
something on the flip side you know
buying a Mac and just using it with a
buying a Mac and just using it with a
default is also probably just not giving
default is also probably just not giving
enough fcks
see class class yeah there we
see class class yeah there we
go I want to make sure that eval and
go I want to make sure that eval and
train
work how's it missing two keyword ARS or
work how's it missing two keyword ARS or
positional ARS I just added
them oh cuz I added it to this stupid
them oh cuz I added it to this stupid
Pokemon function that shouldn't even be
Pokemon function that shouldn't even be
here
do we still not do we still need this in
do we still not do we still need this in
here linky is there still like a custom
here linky is there still like a custom
roll out thing or is there some way that
roll out thing or is there some way that
I can get this out of the main
file sick of having like custom
file sick of having like custom
environment stuff in the demo
file I'm really tempted to just
file I'm really tempted to just
like move it okay I'm really tempted to
like move it okay I'm really tempted to
just give you guys um cuz like you guys
just give you guys um cuz like you guys
hate the demo file too I know like why
hate the demo file too I know like why
don't we just give you guys like a
don't we just give you guys like a
simple demo script like one of these
simple demo script like one of these
this one's designed to be edited you
this one's designed to be edited you
know the real demo script isn't designed
know the real demo script isn't designed
to be edited as often this one is like
to be edited as often this one is like
you hardcode whatever single environment
you hardcode whatever single environment
you want into it it's like 100 line
you want into it it's like 100 line
shorter it doesn't use it doesn't use
shorter it doesn't use it doesn't use
the configs something like
that oh well
that would make
sense love deleting code that is not
required and
module make
module make
policy got unexpected
M policy class R and N class arcs
M policy class R and N class arcs
right well this isn't supposed to be
right well this isn't supposed to be
star star ARS this is just supposed to
star star ARS this is just supposed to
be
be
args isn't
it like
this dick object has no attribute
policy what
does actually make any sense
whatsoever stop policy
and module
ARS you just need this let actually get
simpler very nice
it is a good day when your code
it is a good day when your code
works and chocolate chip cookies are a
works and chocolate chip cookies are a
legitimate part of your
diet for
I think we can Commit This to Dev
I think we can Commit This to Dev
right let me make sure I didn't break
right let me make sure I didn't break
snake cuz there are actually people
snake cuz there are actually people
using uh I know there are people who
using uh I know there are people who
actually want to use the snake
en yeah let's just fix this nen
en yeah let's just fix this nen
everything else can be broken in Dev for
everything else can be broken in Dev for
now I don't care but the snake en needs
now I don't care but the snake en needs
to be working
well that
well that
works here's your snake
works here's your snake
end you can play it it's very
end you can play it it's very
nice it's actually kind of
fun make sure train
fun make sure train
Works joined a little late what are you
Works joined a little late what are you
up to now what did you decide to do for
up to now what did you decide to do for
the continuous control engine
the continuous control engine
discretization yes uh I'm literally just
discretization yes uh I'm literally just
about to do that stuff so it ended up
about to do that stuff so it ended up
being way simpler than I thought um
being way simpler than I thought um
literally all you have to do is
literally all you have to do is
discretize uh the the state into a grid
discretize uh the the state into a grid
for the purpose of of computing
for the purpose of of computing
observations that's all you have to
observations that's all you have to
do um and then you have an identical
do um and then you have an identical
environment for discreet and continuous
environment for discreet and continuous
action
action
spaces we're actually about to talk
spaces we're actually about to talk
about that let me commit all this stuff
about that let me commit all this stuff
up and then we're going to go through
up and then we're going to go through
some stuff
pu RL config
demo
environments is ocean meant to be overly
environments is ocean meant to be overly
simplified environments that can be all
simplified environments that can be all
trained side by side wondering if it can
trained side by side wondering if it can
be used for generalized training
be used for generalized training
approach as an entrance to curricul
approach as an entrance to curricul
learning or with neural MMO be better um
learning or with neural MMO be better um
ocean is just that's what it started as
ocean is just that's what it started as
but ocean is now just the blanket repo
but ocean is now just the blanket repo
for or the B the blanket term for puffer
for or the B the blanket term for puffer
Liv's first party environments so
Liv's first party environments so
anything that I have made or I have you
anything that I have made or I have you
know personally done for puffer
know personally done for puffer
lib
lib
um the classic ocean tasks which we're
um the classic ocean tasks which we're
going to have to like find some sub
going to have to like find some sub
package name for or whatever are probe
package name for or whatever are probe
tasks that are designed to test very
tasks that are designed to test very
specific deficiencies in uh
specific deficiencies in uh
implementations they're meant as quick
implementations they're meant as quick
sanity checks they train in 10 seconds
sanity checks they train in 10 seconds
the snake environment and neural MMO and
the snake environment and neural MMO and
like other things that are going to be
like other things that are going to be
in
in
Ocean are uh well they're pretty much
Ocean are uh well they're pretty much
anything that I've done for it um the
anything that I've done for it um the
general themes are ultra high
general themes are ultra high
performance simulation variety of
performance simulation variety of
different complexities often massively
different complexities often massively
multi-agent cuz that's my main Jam but
multi-agent cuz that's my main Jam but
not necessarily
not necessarily
yeah it doesn't really derail me much
like actually kind of keeps me on track
like actually kind of keeps me on track
when
when
I go interact with the
I go interact with the
chat
chat
um let me commit all this stuff up
welcome red
ey I've always done my my commits
ey I've always done my my commits
manually like this just because I want
manually like this just because I want
to know exactly what I am uh what I am
to know exactly what I am uh what I am
messing
with like in here I can see that there
is
is
D.P
D.P
IX PNG
and that's all I
need I'm just committing up some files
need I'm just committing up some files
you're going to see I actually I have a
you're going to see I actually I have a
cool segment coming up just in a second
cool segment coming up just in a second
here let me just push
here let me just push
this latest Dev
this latest Dev
Massi break some Ms snake
works okay so now what we're going to do
works okay so now what we're going to do
is I'm going to go back to The
is I'm going to go back to The
Continuous control engine I'm going to
Continuous control engine I'm going to
talk about what I want to do with it
talk about what I want to do with it
we're going to decide on some tasks to
we're going to decide on some tasks to
implement hopefully that's going to be
implement hopefully that's going to be
easier than yesterday since I'm fresh
relatively
so
so
yeah the guy doing everything in Vim is
yeah the guy doing everything in Vim is
not using get desktop I'm stubborn
so we had this original environment
so we had this original environment
that's discreet uh a discret environment
that's discreet uh a discret environment
that supports multi-agent particles like
that supports multi-agent particles like
every environment is uh every agent is a
every environment is uh every agent is a
particle it can move around you can
particle it can move around you can
instruct them to do different things via
instruct them to do different things via
the reward signal um there is a little
the reward signal um there is a little
bit of infrastructure behind doing this
bit of infrastructure behind doing this
incredibly quickly so it simulates
incredibly quickly so it simulates
multiple millions of steps per second
multiple millions of steps per second
the general idea is that you can either
the general idea is that you can either
just simulate a 3D grid and then every
just simulate a 3D grid and then every
property you need of an environment can
property you need of an environment can
live on a 3D Grid or if you want to be a
live on a 3D Grid or if you want to be a
little fancier and a little bit more
little fancier and a little bit more
efficient you just store a grid of
efficient you just store a grid of
player IDs where each ID is an integer
player IDs where each ID is an integer
that tells you what agent it controls
that tells you what agent it controls
and then you index use this to index
and then you index use this to index
into a a struct essentially of different
into a a struct essentially of different
players so you know element zero is
players so you know element zero is
player one and it has various different
player one and it has various different
properties etc
properties etc
etc fairly simple you don't really need
etc fairly simple you don't really need
this cuz this environment is so simple
this cuz this environment is so simple
but whatever I wanted to then make this
but whatever I wanted to then make this
environment
environment
continuous so I thought about different
continuous so I thought about different
ways to do this and I came to the
ways to do this and I came to the
conclusion that it's actually really
conclusion that it's actually really
easy and all you have to do is you allow
easy and all you have to do is you allow
the agents to move a uh continuous non-
the agents to move a uh continuous non-
integer value so they can move however
integer value so they can move however
much they want you store their position
much they want you store their position
as a float but then when you write to
as a float but then when you write to
the observations you discretize to
the observations you discretize to
whatever the resolution is of the image
whatever the resolution is of the image
so you have continuous actions
so you have continuous actions
continuous movement you could even do uh
continuous movement you could even do uh
acceleration and stuff like that if you
acceleration and stuff like that if you
want to though I didn't add that yet but
want to though I didn't add that yet but
uh you get the same exact observation
uh you get the same exact observation
space so this produces tasks that are
space so this produces tasks that are
one to one the same between the
one to one the same between the
continuous and the discret case which
continuous and the discret case which
means that if we have several of these
means that if we have several of these
tasks that are kind of reasonable and we
tasks that are kind of reasonable and we
can train discret agents we can train
can train discret agents we can train
continuous agents we can see what
continuous agents we can see what
whether our continuous learning setup
whether our continuous learning setup
works as well as our discret learning
works as well as our discret learning
setup which will be a huge Boon to
setup which will be a huge Boon to
continuous control people that have
continuous control people that have
problems with janky
algorithms as well as being a very nice
algorithms as well as being a very nice
test case a way to show that puffer is
test case a way to show that puffer is
provably working on these types of
provably working on these types of
environments as well uh I started this
environments as well uh I started this
doc where we started looking at
doc where we started looking at
different
tasks I think some of these are good
tasks I think some of these are good
some of these are not so good
some of these are not so good
good so these two are classic problems
good so these two are classic problems
that have been used for a long time in
that have been used for a long time in
multi-agent sometimes they're posed in
multi-agent sometimes they're posed in
weird ways but I think some variation of
weird ways but I think some variation of
foraging where you have to spread out
foraging where you have to spread out
and eat food and some version of
and eat food and some version of
Predator prey where you want some agents
Predator prey where you want some agents
to chase others these are
to chase others these are
good this one was a little bit more
good this one was a little bit more
questionable I'll think about it
questionable I'll think about it
um grouping up so so you have several
um grouping up so so you have several
different types of agents that have to
different types of agents that have to
group up with their own groups and stay
group up with their own groups and stay
away from other groups uh that is
away from other groups uh that is
slightly different from the Predator
slightly different from the Predator
prey one this one is purely Cooperative
prey one this one is purely Cooperative
it's kind of interesting in that
it's kind of interesting in that
sense and then I felt like doing a promo
sense and then I felt like doing a promo
task in which we just the agents have to
task in which we just the agents have to
assemble to spell out the words puff or
assemble to spell out the words puff or
Li I thought that would be fun and uh
Li I thought that would be fun and uh
it's a decent little uh test case as
it's a decent little uh test case as
well so that's four
well so that's four
tasks I don't know
tasks I don't know
let me think if I want to do this
let me think if I want to do this
optimized task how I would do
optimized task how I would do
it it might not be that
it it might not be that
hard if I make the food continuous
hard if I make the food continuous
rather than discreet the food is just a
rather than discreet the food is just a
continuous amount that would
continuous amount that would
work jungle basketball from Eric Jang
[Music]
[Music]
nope
uh robotics guy oh this one oh oh yeah
uh robotics guy oh this one oh oh yeah
yeah the Olympics I saw this thing
yeah the Olympics I saw this thing
yesterday this thing here
right is this this guy's work or is this
right is this this guy's work or is this
just something he's reposting though
just something he's reposting though
because I saw this online
yeah it's cool we could potentially add
yeah it's cool we could potentially add
a binding for it do they have a do they
a binding for it do they have a do they
tell you how fast it
is man people really got to say how fast
is man people really got to say how fast
stuff is initial code release code
stuff is initial code release code
trainable repo under construction so
trainable repo under construction so
we'll give this a little bit of time to
we'll give this a little bit of time to
cook uh if it's basically I'm going to
cook uh if it's basically I'm going to
add bindings for anything that is
add bindings for anything that is
reasonable enough to work with and is
reasonable enough to work with and is
fast
let's see
this is not true
okay whatever
Chris let's go look at this
welcome I can't quite see the
welcome I can't quite see the
name yanero if I'm pronouncing that
name yanero if I'm pronouncing that
welcome let's get into some actual Dev
welcome let's get into some actual Dev
instead of just freaking scrolling
instead of just freaking scrolling
Twitter
Twitter
endlessly the thing with Chris is
endlessly the thing with Chris is
somewhat annoying because like Chris is
somewhat annoying because like Chris is
somebody who I actually I respect a lot
somebody who I actually I respect a lot
and does very good work um
and does very good work um
I disagree with him on Jax but I think
I disagree with him on Jax but I think
he's mostly just mad that the article
he's mostly just mad that the article
was written in a sort of inflammatory
was written in a sort of inflammatory
way um I actually I held back a lot on
way um I actually I held back a lot on
the way that I would post stuff during
the way that I would post stuff during
my PhD because I didn't want it to
my PhD because I didn't want it to
reflect badly on my lab or on MIT or
reflect badly on my lab or on MIT or
stuff but the state of RL is just
stuff but the state of RL is just
pathetic we need to fix it it needs to
pathetic we need to fix it it needs to
be fixed now it's not that hard just do
be fixed now it's not that hard just do
it uh what we're working on today is a
it uh what we're working on today is a
continuous control engine for a bunch of
continuous control engine for a bunch of
simple tasks that can be done obviously
simple tasks that can be done obviously
continuously but also discreet this is
continuously but also discreet this is
going to be a very nice way of seeing
going to be a very nice way of seeing
whether or not uh continuous
whether or not uh continuous
optimization losses are as good as
optimization losses are as good as
discreet ones or whether they're like
discreet ones or whether they're like
fundamentally different and one is
fundamentally different and one is
harder to tune than the other this will
harder to tune than the other this will
be very useful for continuous control
be very useful for continuous control
people and it will be very useful as a
people and it will be very useful as a
test case in puffer as well as for other
test case in puffer as well as for other
people who want to use puffer on this
people who want to use puffer on this
stuff because these environment should
stuff because these environment should
be trainable within a few minutes at
be trainable within a few minutes at
most and we'll run at millions of steps
most and we'll run at millions of steps
per second that is what we are currently
per second that is what we are currently
building
building
so let me think where I'm going to get
so let me think where I'm going to get
started here I think we can start
started here I think we can start
locally we can start with this list of
locally we can start with this list of
tasks um we're going to have to
tasks um we're going to have to
add the only two features that I have to
add the only two features that I have to
add to this environment to make this
add to this environment to make this
work
work
are there needs to be a layer for
are there needs to be a layer for
food ability for them to see
food ability for them to see
food
food
and there has to be um the ability for
and there has to be um the ability for
the individual agents to have different
the individual agents to have different
colors associated with them so let's
colors associated with them so let's
look at that that's going to be slightly
look at that that's going to be slightly
awkward because one of those values is
awkward because one of those values is
potentially continuous and the other is
potentially continuous and the other is
discret
[Music]
[Music]
right maybe
not starting from more RL people that
not starting from more RL people that
are major problems with the seal why do
are major problems with the seal why do
you starting from more RL
you starting from more RL
people that there are major problems in
people that there are major problems in
the field why do you also say it's
the field why do you also say it's
pathetic I say it's pathetic because if
pathetic I say it's pathetic because if
the field were if this were like yeah
the field were if this were like yeah
the problems are just really hard and
the problems are just really hard and
you know we've just and we've done a
you know we've just and we've done a
good job then I would not be able to
good job then I would not be able to
like sit down write a few hundred lines
like sit down write a few hundred lines
of code and solve something that's been
of code and solve something that's been
unsolved for 10 years which has been
unsolved for 10 years which has been
happening like regularly over the last
happening like regularly over the last
couple of months um
couple of months um
Academia heavily
Academia heavily
disincentivizes core infrastructure work
disincentivizes core infrastructure work
that is required for the field to
that is required for the field to
actually function heavily heavily do
actually function heavily heavily do
incentivizes core infrastructure work so
incentivizes core infrastructure work so
just nobody's done it so everybody works
just nobody's done it so everybody works
with really slow janky environments
with really slow janky environments
doesn't have like proper ways to tune
doesn't have like proper ways to tune
baselines it's work that I'm not going
baselines it's work that I'm not going
to say it's necessarily simple but it is
to say it's necessarily simple but it is
absolutely something that can be done at
absolutely something that can be done at
the scale of a single individual and has
the scale of a single individual and has
never been done like this continuous
never been done like this continuous
control environment I mean this seems
control environment I mean this seems
like a really basic question to ask
like a really basic question to ask
right hey if we swap the action space
right hey if we swap the action space
from discreet to continuous does the
from discreet to continuous does the
problem fundamentally change I've never
problem fundamentally change I've never
seen anybody ask that and I've certainly
seen anybody ask that and I've certainly
never seen anybody like do a
never seen anybody like do a
comprehensive evaluation of that like
comprehensive evaluation of that like
we're doing right here this is like
we're doing right here this is like
something that is going to be a I mean
something that is going to be a I mean
I'd be shocked if this takes me more
I'd be shocked if this takes me more
than a few days to complete
than a few days to complete
fully if it does take me more than a few
fully if it does take me more than a few
days to complete fully it will be
days to complete fully it will be
because Contin control is Jank that's
because Contin control is Jank that's
another way of putting it
so right now I have the grid as a u
8 if I want to add continuous amounts of
8 if I want to add continuous amounts of
food I'm not going to be able to have it
food I'm not going to be able to have it
be a u and 8 I'd have to make the
be a u and 8 I'd have to make the
observations
observations
continuous or the food layer at least
continuous or the food layer at least
continuous this would be
continuous this would be
bad yeah the academic environments
bad yeah the academic environments
are I've done more in the last two
are I've done more in the last two
months than I've done in the last two
months than I've done in the last two
years um and that's with a good lab like
years um and that's with a good lab like
it's just the publishing cycle is a mess
um
um
yeah
so I can't do if I just add the so this
so I can't do if I just add the so this
would be basically the same as the snake
would be basically the same as the snake
environment if I just add a few
environment if I just add a few
different agent colors to this that
different agent colors to this that
would support this group up task I could
would support this group up task I could
support the puffer lib task I could
support the puffer lib task I could
support Predator can I do Predator prey
support Predator can I do Predator prey
with that yeah I can do Predator prey
with that yeah I can do Predator prey
with that uh I could do foraging with
with that uh I could do foraging with
that as well I just have to add food as
that as well I just have to add food as
a channel the only one I wouldn't be
a channel the only one I wouldn't be
able to do is this optimize
able to do is this optimize
task
task
which you know we really don't need
which you know we really don't need
that I think I'm good with there being
that I think I'm good with there being
four tasks because they have to be
four tasks because they have to be
optimized continuous and discreet so
optimized continuous and discreet so
having eight different environments
having eight different environments
essentially eight different things to to
essentially eight different things to to
test that's a nice
test that's a nice
start and these
start and these
are yeah these are pretty
are yeah these are pretty
nice so let's just add
nice so let's just add
that so we're going to
do wall will be
do wall will be
here wall will be one agent will
be uh we need to choose some uh some
so what did I do for snake I did red
so what did I do for snake I did red
white and blue for snake didn't I I
white and blue for snake didn't I I
think I didn't just copy
think I didn't just copy
that that'd be funny we just make all
that that'd be funny we just make all
the the environments America
the the environments America
themed since I did the snake one on 4th
themed since I did the snake one on 4th
of July and already have the color
codes yeah there we go
codes yeah there we go
it's Reds whites and blues
it's Reds whites and blues
okay and did I have the I think I just
okay and did I have the I think I just
called them like agent One agent two
called them like agent One agent two
agent three
agent three
right
right
one
one
here agent
here agent
two three and
two three and
four we also need
food USA
all right that's seven different
all right that's seven different
things playing around with
things playing around with
generalization experiments in Sp sp3 had
generalization experiments in Sp sp3 had
to expose a lot of core sp3 code to fit
to expose a lot of core sp3 code to fit
my use cases not to bash on it but it
my use cases not to bash on it but it
does feel junky yeah and SP sp3 is one
does feel junky yeah and SP sp3 is one
of the better ones as well but the
of the better ones as well but the
design philosophy is just fundamentally
design philosophy is just fundamentally
different so the way I approach stuff
different so the way I approach stuff
with puffer is I assume everything is
with puffer is I assume everything is
white box
white box
software um
software um
which means that you're going to have to
which means that you're going to have to
edit every part of the software at some
edit every part of the software at some
point because you're going to be doing
point because you're going to be doing
some crazy thing that I can't anticipate
some crazy thing that I can't anticipate
like reinforcement learning is not a
like reinforcement learning is not a
mature space right this is all research
mature space right this is all research
so what I do is instead of trying to
so what I do is instead of trying to
like box up all these components so that
like box up all these components so that
they snap together like Legos I just
they snap together like Legos I just
make the code as simple and as short as
make the code as simple and as short as
possible so that it's easy for you to
possible so that it's easy for you to
onboard and edit that's how I write
onboard and edit that's how I write
stuff here if it doesn't have rk4 it's
stuff here if it doesn't have rk4 it's
not continuous control
the action space is
continuous if the implementation works
continuous if the implementation works
well with continuous action spaces then
well with continuous action spaces then
the problem of whether this specific
the problem of whether this specific
robot and is hard or not is going to be
robot and is hard or not is going to be
separate from whether or not continuous
separate from whether or not continuous
control is
hard that's the
hard that's the
idea what time it 12:49 I think I have a
idea what time it 12:49 I think I have a
400 p.m. call so I'm good for 3 hours
400 p.m. call so I'm good for 3 hours
I have three hours in
me okay so we need to add food
me okay so we need to add food
fundamentally we need to add food uh to
fundamentally we need to add food uh to
this
environment how are we going
environment how are we going
to well the agents aren't going to die
to well the agents aren't going to die
if they don't get food right it's just
if they don't get food right it's just
going to be a reward for food so I can
going to be a reward for food so I can
just add like food reward
right so reward function introverts and
right so reward function introverts and
I can just add a food reward as well as
I can just add a food reward as well as
an extra thing right food reward
an extra thing right food reward
equals uh
equals uh
0.1
okay oh and then there's this other
okay oh and then there's this other
trick that I like to do
where I think I did this with snake
where I think I did this with snake
right didn't I do this thing with snake
where no I think as fine as this never
where no I think as fine as this never
mind I thought something was applicable
mind I thought something was applicable
and it isn't
okay so we add self. food
reward oh yeah this is where I was going
reward oh yeah this is where I was going
to do it
to do it
so the trick here is instead of doing
so the trick here is instead of doing
like equal agent one or equal agent 2 or
like equal agent one or equal agent 2 or
equal agent
equal agent
3 we just do greater than equal to agent
3 we just do greater than equal to agent
one that was the
one that was the
trick because you define the indices
trick because you define the indices
like that
self. food
self. food
reward okay and now we go
into I'm going to actually just go over
into I'm going to actually just go over
to here it's going to be easier
uh float no not
add food reward
add food reward
here up size agent
here up size agent
speed load food
speed load food
reward
okay now what we do is we add
okay now what we do is we add
unfortunately we do have to dupc at the
unfortunately we do have to dupc at the
enums here which is slightly
annoying so we're going to put these
annoying so we're going to put these
here
okay and now uh it's very easy I think
okay and now uh it's very easy I think
for us to
just yeah it should
just yeah it should
be very easy for us here
I'm actually tempted to move this
I'm actually tempted to move this
into python
into python
but it's
but it's
okay where's the move
function okay so if this is this is
function okay so if this is this is
supposed to be empty not food got to be
supposed to be empty not food got to be
careful with the hardcoded indices so if
careful with the hardcoded indices so if
it's food then what we do is
we we set it to
empty for a second and then we do self.
empty for a second and then we do self.
rewards it's going to be food reward
rewards it's going to be food reward
does this thing have rewards no it
does this thing have rewards no it
doesn't have rewards so we need to uh we
doesn't have rewards so we need to uh we
need to add uh the reward function into
need to add uh the reward function into
here which is somewhat annoying but not
here which is somewhat annoying but not
too bad
too bad
agent positions observations and we do
rewards where's the
rewards where's the
observations Lo
observations Lo
rewards load
rewards load
array
array
and rewards is equal to rewards
good pass this after observations
where is
it self UP up. Rewards right
it self UP up. Rewards right
here so what the reason for this is that
here so what the reason for this is that
if we pass this array it's the same
if we pass this array it's the same
trick we're doing with observations if
trick we're doing with observations if
we pass this as a buffer into the C
we pass this as a buffer into the C
environment uh then we have access to
environment uh then we have access to
mem shared data from both Python and
mem shared data from both Python and
from C so we have very very fast ability
from C so we have very very fast ability
to update um rewards and to read rewards
to update um rewards and to read rewards
from Python and from
from Python and from
C so this is now done
and now I have to change the way that
and now I have to change the way that
rewards are um rewards are applied for
rewards are um rewards are applied for
sure
so the way to do that is
so the way to do that is
uh you just have to add the reward
uh you just have to add the reward
function to the buffer so these come
function to the buffer so these come
from C these come from python
oh wait can you even do this like that
oh wait can you even do this like that
no wait you can't there's no point in
no wait you can't there's no point in
doing the fill operation from scon
doing the fill operation from scon
because it's a numpy operation anyways
because it's a numpy operation anyways
so it's just going to call back to
so it's just going to call back to
python to do that anyways so let's just
python to do that anyways so let's just
do uh right here we'll just do self
dot fill zero good
uh okay so this is now
uh okay so this is now
reasonable where's the
reasonable where's the
renderer yes this is the renderer
here I kind of want to one file this
here I kind of want to one file this
whole
thing
thing
there was this where it was wait am I
there was this where it was wait am I
yeah it's
yeah it's
it's let's just take all this
code 100
code 100
lines we're just going to put it in this
file okay now we got 300 lines total for
file okay now we got 300 lines total for
the environment with
the environment with
no additional stuff floating around
now we can just have this is just make
now we can just have this is just make
render and we'll apply some of the
render and we'll apply some of the
cleanups from the snake en in a bit
cleanups from the snake en in a bit
here
here
but as for the asset
map we kind of want this snake one here
map we kind of want this snake one here
don't
we didn't we actually make like a way
we didn't we actually make like a way
better renderer here as well
I think we want the snake grid
renderer yeah because this is the entire
renderer yeah because this is the entire
snake renderer right
snake renderer right
here so we don't need this grid render
here so we don't need this grid render
thing at all it's just a
thing at all it's just a
waste
waste
um make render so this make renderer
um make render so this make renderer
thing is a total waste as
thing is a total waste as
well
well
um so
let's just update
let's just update
that oops where is
it so if render mode is human self.
it so if render mode is human self.
client be equal to
client be equal to
this uh asset map with
this uh asset map with
height I think we just do this
yeah cuz we're not going to use this
yeah cuz we're not going to use this
mode for a bit
mode for a bit
anyways so self. render
anyways so self. render
mode RGB array then we can do frame
mode RGB array then we can do frame
equal
colors
grid actually we can cut the oh this is
grid actually we can cut the oh this is
well
let's do Vision where is it
Vision self. Vision
Vision self. Vision
range GD
of self
of self
grid
grid
no grid equals self.
grid yeah and we'll just leave like this
grid yeah and we'll just leave like this
for now and and now we just go get
for now and and now we just go get
colors from
snake grab all
these I could make the uh the asy render
these I could make the uh the asy render
for this as well you know this is the
for this as well you know this is the
snake render is kind of
snake render is kind of
good I might just use uh something
good I might just use uh something
similar to that and then I might end up
similar to that and then I might end up
making like a just a uniform render for
making like a just a uniform render for
you know all these different little
you know all these different little
projects of mine but for now it makes
projects of mine but for now it makes
more sense to just one file stuff and
more sense to just one file stuff and
just see if that's actually the same
just see if that's actually the same
code or
not yeah so this is fine turn
frame for
what did I do
here 11 positional arcs
did I like forget to pass
something wait
ah se grid
ah se grid
continuous what's the signature here
continuous what's the signature here
grid AG agent position spawn position
grid AG agent position spawn position
cans observations rewards width height
cans observations rewards width height
num agents Horizon
num agents Horizon
Vision speed discretize food reward this
Vision speed discretize food reward this
is correct oh did I just not I didn't
is correct oh did I just not I didn't
compile it right
that's
fine and uh
agent going do agent one for now like
this and we will edit this in a little
this and we will edit this in a little
bit to support different
colors for
right we have to edit the uh we have to
right we have to edit the uh we have to
edit the torch model just a little
bit because this now takes uh a few
bit because this now takes uh a few
additional
channels believe seven additional
channels believe seven additional
channels
channels
and is this still three you know this is
and is this still three you know this is
still three
still three
cool I think we're set here um we'll see
cool I think we're set here um we'll see
if there are any little additional
errors expected to have three channels
errors expected to have three channels
oh it right the observation space we
oh it right the observation space we
also have to update so that it correctly
also have to update so that it correctly
reflects
reflects
the no wait this is
the no wait this is
fine um
base yeah no this is totally fine
base yeah no this is totally fine
um what is wrong
um what is wrong
here given weights of size
three didn't I just edit this did I edit
three didn't I just edit this did I edit
the wrong one or
something CNN
something CNN
channels oh this was three right what
channels oh this was three right what
was
was
this yeah this this was five up
this yeah this this was five up
here this is the kernel size this is the
here this is the kernel size this is the
one that had to be
seven puffer grid has no attribute
rewards this is self. buff. Rewards
Okay cool so this now runs and we have
Okay cool so this now runs and we have
our nice puffer en uh now we can make
our nice puffer en uh now we can make
this have different agent colors or
this have different agent colors or
whatever and we can add food and we can
whatever and we can add food and we can
do all sorts of nice
things I think we're going to start by
we're going to have to add a uh an
argument we're going to have to add like
argument we're going to have to add like
agent colors or something right to the
agent colors or something right to the
the arcs of the in
it yeah that would be the easiest right
we just add
like
like
H let's do it that way for now I have
H let's do it that way for now I have
some ideas on how we can make this a
some ideas on how we can make this a
like a slightly more General and useful
like a slightly more General and useful
thing in a bit but for now let's just do
thing in a bit but for now let's just do
like
uh self. agent
uh self. agent
colors p. random.
colors p. random.
randant and it's going to be
randant and it's going to be
from 3 to six so is that 0 to 4 + three
from 3 to six so is that 0 to 4 + three
or
or
whatever but we can just do
whatever but we can just do
three
three
agents and we will pass this
to the Sea environment
maybe does that make sense to pass this
maybe does that make sense to pass this
to the Sea
to the Sea
environment I don't know I can't really
environment I don't know I can't really
think all that well today let's do uh
think all that well today let's do uh
let's just do it the dumb way for now
let's just do it the dumb way for now
and then we'll fix it if it needs to be
fixed spawn position
fixed spawn position
cans agent
cans agent
colors agent
colors agent
colors C grid
continuous agent
continuous agent
colors and we'll do
colors and we'll do
uh float agent no in agent colors right
uh float agent no in agent colors right
in agent colors
did I give this thing a d type
did I give this thing a d type
before no it's probably an
INT and we'll do self. agent
colors assign
colors assign
that and now when we spawn the
that and now when we spawn the
agents and when we spawn the
agents this is the agent spawn code
agents this is the agent spawn code
right here
instead of being agent one this is going
instead of being agent one this is going
to be self. agent colors of agent
idx now this is also going to
idx now this is also going to
be s. agent colors of agent
be s. agent colors of agent
idx
idx
okay let's see if that does oops well we
okay let's see if that does oops well we
forgot to compile it so that's not
help and if I've done this correctly we
help and if I've done this correctly we
should
get various different colored
get various different colored
agents expected in got long okay that I
agents expected in got long okay that I
was somewhat suspicious of
that D
that D
type and 32
hey very
hey very
nice so we've
nice so we've
got red and white agents
got red and white agents
here and we will go make
here and we will go make
some blue food to add to
some blue food to add to
this
this
right then this will be
set so let's do
it's food indexed uh
two let's actually make the wall two and
two let's actually make the wall two and
food
food
one uh one second and you'll see
one uh one second and you'll see
why it's just going to make it slightly
easier it's going to be slightly easier
easier it's going to be slightly easier
to do
numpy random. randint
02 and that would give us food
not seeing the
food should be
blue oh it's because of this grid
fill
fill
okay we'll just do like
okay we'll just do like
this we'll just remake
this we'll just remake
the oh no we can't do that you have to
the oh no we can't do that you have to
fill
fill
it so uh here self.
it so uh here self.
grid so we make it zeros and then we
grid so we make it zeros and then we
fill it
fill it
here so self. grid uh star
here so self. grid uh star
star you can copy stuff into the
star you can copy stuff into the
grid that's
grid that's
acceptable kind of slow um we might find
acceptable kind of slow um we might find
some other way of doing
some other way of doing
it soft. height soft. width
okay there we
go so now are they eating the
go so now are they eating the
food yeah right like whenever they hit
food yeah right like whenever they hit
they walk on the food it disappears
they walk on the food it disappears
yeah awesome
so this is uh our basic environment that
so this is uh our basic environment that
is going to be required to implement
is going to be required to implement
whatever we
want we
want we
need a few things from
need a few things from
here we need to implement the tasks and
here we need to implement the tasks and
we would like to have a clean way of
we would like to have a clean way of
implementing tasks on top of this grid
implementing tasks on top of this grid
type environment
type environment
is what we would like to
have I think that in order to do
that it's actually a fair number of
that it's actually a fair number of
things you need in order to do
things you need in order to do
that
that
um you need a reward function I guess a
um you need a reward function I guess a
task consists of an environment
task consists of an environment
initialization and a reward function
initialization and a reward function
right yeah
environment initialization and a reward
function let me think how I want to do
that really don't want to make a bunch
that really don't want to make a bunch
of sub classes of this environment
I could give it the functions to
I could give it the functions to
call um I could give it a nit function
call um I could give it a nit function
to call and I could give it a reward
to call and I could give it a reward
function that's kind of how we have it
function that's kind of how we have it
at the moment
that wouldn't be
that wouldn't be
bad
right man it's
right man it's
like I'm so used to just subclassing
like I'm so used to just subclassing
stuff and making a mess of things in the
stuff and making a mess of things in the
long run that I have to like get my head
long run that I have to like get my head
out of that that space we're just going
out of that that space we're just going
to do it how we're going to do it for
now so we're going to take a uh we're
now so we're going to take a uh we're
going to just Define some reward
going to just Define some reward
functions so we're going to move these
functions so we're going to move these
outside up top Maybe
takes
observations think this is just n
right you full axess to the
end okay and uh we need to make some
end okay and uh we need to make some
init functions
init functions
right so this is going to take reward
right so this is going to take reward
function init function reward function
right
and instead of doing string we can just
and instead of doing string we can just
make this the reward introverts function
make this the reward introverts function
right init
and it
and it
introverts like
so and then it's just going to be self
so and then it's just going to be self
dot nit function reward
dot nit function reward
function that's actually kind of
function that's actually kind of
clean that's really not that
bad so we make uh an init function for
bad so we make uh an init function for
introverts now the NP
function should take care
function should take care
of uh
of uh
spawning and the placement
spawning and the placement
of
of
any obstacles or food or whatever
right so these spawn position cans
we'll leave them like this for now but
we'll leave them like this for now but
they're probably going to have to
change I don't think we have to do
change I don't think we have to do
anything for this specific
end we call this inside of reset
self. AIT
function and we call this
function and we call this
here and yeah so this would be inside of
here and yeah so this would be inside of
like the reward food one or whatever in
like the reward food one or whatever in
it forging
yeah let's see how this
yeah let's see how this
does still
work yeah
no self. reward function
yeah one little
change okay so now we have this particle
change okay so now we have this particle
n we can Define tasks in it we're going
n we can Define tasks in it we're going
to have to come up with some slightly
to have to come up with some slightly
cleaner ways of doing that but I can
cleaner ways of doing that but I can
sort of think about that over time uh
sort of think about that over time uh
let's get all of the tasks that we are
let's get all of the tasks that we are
interested in implemented at the very
interested in implemented at the very
least let's get all of the tasks
least let's get all of the tasks
implemented
so we would like to do the forging
task so we have init forging which is um
task so we have init forging which is um
you make some
food
food
and this is kind of too much food isn't
and this is kind of too much food isn't
it
I think what you do is you do like RNG
I think what you do is you do like RNG
equal numpy random.
equal numpy random.
Rand of uh m.
Rand of uh m.
height height m. width and then you do
height height m. width and then you do
m. grid of RNG less
than
prob food cr1
we prob is
we prob is
food and then otherwise it's
empty reward
empty reward
forging uh what do you have to
forging uh what do you have to
return you don't really have to return
return you don't really have to return
anything
anything
right so this is inefficient but I'm
right so this is inefficient but I'm
going to return just zeros for
now okay so we have the forging spec
now okay so we have the forging spec
here
here
um we actually kind of want to do the
um we actually kind of want to do the
agent colors in here as well don't
we you do want to put the agent colors
we you do want to put the agent colors
in
in
here but I can do it inside of um
here but I can do it inside of um
because it's memory shared I can do it
because it's memory shared I can do it
inside
inside
here so I can do um it's fine for this
here so I can do um it's fine for this
one for them to all be just random
right
right
Predator
right uh I think we're not going to mix
right uh I think we're not going to mix
it we're not going to make like the
it we're not going to make like the
Predators ALS the prey also have to get
Predators ALS the prey also have to get
food we're just going to do a pure uh
food we're just going to do a pure uh
stay away from other agents task
so how do I have the color set
up this is from snake right so I have
up this is from snake right so I have
red white red so
yeah um
yeah um
agents so let's do m.
agents so let's do m.
agent colors so
so over
4 n is going to be agent
4 n is going to be agent
one and it's going to this is going to
one and it's going to this is going to
be agent three which is going to be
Predators
um
and well
and well
I just use the first two
I just use the first two
colors maybe I should just use the first
colors maybe I should just use the first
two
two
colors first two colors are bright red
colors first two colors are bright red
and bright white yeah that's fine we'll
and bright white yeah that's fine we'll
just use the first two
colors so this is going to be agent one
colors so this is going to be agent one
and then this is Agent
two and what else do we have to do to
two and what else do we have to do to
initialize these
we're probably going to have to like
we're probably going to have to like
move them apart or something but we'll
move them apart or something but we'll
handle the spawn position differences in
handle the spawn position differences in
a little
a little
bit I think that's all we have to do for
bit I think that's all we have to do for
a nit is just make sure that they are
a nit is just make sure that they are
defined this
defined this
way reward Predator
prey
prey
W is going to be
W is going to be
zeros and then um
reward introverts here
right so n is going to be num agents
right so n is going to be num agents
over two and then we do wordss
of so the prey reward is going to be
it stay away from the Predator
it stay away from the Predator
specifically
so so this is the Predator agent
here we're going to still clip the
here we're going to still clip the
rewards I think just like
rewards I think just like
before and uh we need the
before and uh we need the
the the Predator reward
the the Predator reward
right which is going to
be this doesn't need to be one minus
be this doesn't need to be one minus
anymore
anymore
because uh the current agent is not
because uh the current agent is not
included so this is just going to be
included so this is just going to be
positive
right yes so
so Predator wants
so Predator wants
to uh keep prey in
to uh keep prey in
sight
sight
prey wants to keep Predator out of
prey wants to keep Predator out of
sight and that's the reward for predator
prey optimize is not happening for
prey optimize is not happening for
now um group up is going to be simpler I
think in it
group we're going to
group we're going to
do I think the default just like
do I think the default just like
randomly
randomly
assigning agent colors is actually good
assigning agent colors is actually good
here
here
right yeah
right yeah
so uh the reward actually should be very
so uh the reward actually should be very
easy did you Define insight if is it
easy did you Define insight if is it
just a check of surrounding area yeah so
just a check of surrounding area yeah so
the observations here this is a uh a 11
the observations here this is a uh a 11
by1 window that is centered on the agent
by1 window that is centered on the agent
so by checking just the local window of
so by checking just the local window of
observations right you can sum over that
observations right you can sum over that
window so this is comparing the window
window so this is comparing the window
all of the stuff in that window to the
all of the stuff in that window to the
specific agent type and then we can see
specific agent type and then we can see
how many agents of that type are in the
how many agents of that type are in the
window the init group one is going to be
window the init group one is going to be
very
very
similar um it's going to be even a
similar um it's going to be even a
little easier so it's going to be
little easier so it's going to be
rewards
equal no not zeros it's going to
be it's well it's going to be like same
equal
to what this thing
here m. buff.
observations equal
to we need the agent types don't
to we need the agent types don't
we self. agent
colors is that going to
colors is that going to
be well we might have to
be well we might have to
broadcast
broadcast
colors m. agent colors so you get
colors m. agent colors so you get
rewarded based on the number that you
rewarded based on the number that you
can see of your
own this you have to subtract one
own this you have to subtract one
because you can always see
because you can always see
yourself and diff reward is going to
yourself and diff reward is going to
be
be
one is going to be negative of this so
one is going to be negative of this so
you get rewarded one for each AG that
you get rewarded one for each AG that
you can see of your same
you can see of your same
population you get penalized for each
population you get penalized for each
agent uh that is in a different
agent uh that is in a different
population then you
return so rewards is going to be equal
return so rewards is going to be equal
to same plus
to same plus
diff and actually we can get rid of this
diff and actually we can get rid of this
negative and we can just put minus here
negative and we can just put minus here
and we can just clip
and we can just clip
it this is not supposed to be the NIT
it this is not supposed to be the NIT
this should be reward group
this should be reward group
and then in it
and then in it
group is actually just going to be a
group is actually just going to be a
pass don't need to do
anything and then the puffer Li task is
anything and then the puffer Li task is
going to be a little harder because I'm
going to be a little harder because I'm
going to have to go and uh figure out
going to have to go and uh figure out
how to get a bit map of puffer lib maybe
how to get a bit map of puffer lib maybe
even the puffer logo we'll see we're
even the puffer logo we'll see we're
going to have to get a bit map of that
going to have to get a bit map of that
and see if they will uh match
it for
so we maybe we can even start trying
so we maybe we can even start trying
these out already
um easiest one of these is going to be
um easiest one of these is going to be
forging this m is parti is this uh envis
forging this m is parti is this uh envis
partially observed how will agents
partially observed how will agents
complete puffer task if the logo is
complete puffer task if the logo is
bigger than their window so yes the m is
bigger than their window so yes the m is
partially observed um the starting
partially observed um the starting
positions are going to be appropriate
positions are going to be appropriate
for the task so for the puffer lib task
for the task so for the puffer lib task
I'm going to spread out uh lean key this
I'm going to spread out uh lean key this
is right before I started this I
is right before I started this I
committed the sum version of this to Dev
committed the sum version of this to Dev
you can look at it
you can look at it
um yeah I'm I'm trying to make it easier
um yeah I'm I'm trying to make it easier
to add tasks at the moment
to add tasks at the moment
um JBL
um JBL
uh yeah so for the puffer task for
uh yeah so for the puffer task for
instance I'm just going to start all the
instance I'm just going to start all the
agents at the top of the map in like a
agents at the top of the map in like a
blob type thing that covers the top of
blob type thing that covers the top of
the map and the hope is that they're
the map and the hope is that they're
going to learn to just Cascade downward
going to learn to just Cascade downward
and then they'll just sort of fill in
and then they'll just sort of fill in
pixels based on where they're getting
pixels based on where they're getting
reward um you know the task has to make
reward um you know the task has to make
sense if it's partially observed uh and
sense if it's partially observed uh and
there's no way to get there then they're
there's no way to get there then they're
not going to be able to do it um but
not going to be able to do it um but
like the Predator prey ones make sense
like the Predator prey ones make sense
part partially observed um the forging
part partially observed um the forging
ones make sense partially observed
ones make sense partially observed
though you're going to have to learn to
though you're going to have to learn to
explore a little bit right everything
explore a little bit right everything
else sort of make sense the reward for
else sort of make sense the reward for
groups maybe that you won't get one
groups maybe that you won't get one
group for each agent type right maybe
group for each agent type right maybe
they'll split into multiple groups so
they'll split into multiple groups so
they won't get like as much reward as
they won't get like as much reward as
they could but it should still be able
they could but it should still be able
to learn something very reasonable that
to learn something very reasonable that
we should be able to evaluate and uh
we should be able to evaluate and uh
it's very important with these types of
it's very important with these types of
M to get the reward on a scale that
M to get the reward on a scale that
makes sense so hopefully we're going to
makes sense so hopefully we're going to
scale stuff so that like the maximum
scale stuff so that like the maximum
reward or whatever is going to be like
reward or whatever is going to be like
one and the minimum will be zero or
one and the minimum will be zero or
something like that or negative 1
something like that or negative 1
whatever across all the different
whatever across all the different
environments so we're going to have very
environments so we're going to have very
nice easy to evaluate metrics on all of
nice easy to evaluate metrics on all of
this
stuff let's
stuff let's
do foraging in it
foraging first does the eval make sense
EV makes
EV makes
sense is this
sense is this
10% seems like a lot of
10% seems like a lot of
food whatever should be very easy to
food whatever should be very easy to
learn
learn
right uh so let's do mode
right uh so let's do mode
train dash dash ulti
pross and let me make sure I have a
pross and let me make sure I have a
reasonable whoops
reasonable whoops
reasonable
config
grid yeah so I do I have a reasonable
grid yeah so I do I have a reasonable
configuration
here let's just see if this does
here let's just see if this does
anything remotely
anything remotely
reasonable I think it was being a little
reasonable I think it was being a little
bit obnoxious yesterday
it is a little bit annoying that this
it is a little bit annoying that this
does not give you rewards back
consistently I'm going to go ahead and
consistently I'm going to go ahead and
fix
that oh interesting that I made it
that oh interesting that I made it
episode episode Rewards
H
H
tick
percent o
we'll do like
we'll do like
this we get any rewards
here yeah so this is obnoxious that this
here yeah so this is obnoxious that this
doesn't work the way I'd like it to so
doesn't work the way I'd like it to so
let's just recast uh redo this
let's just recast uh redo this
experiment and uh this should give
experiment and uh this should give
us something a little bit more
us something a little bit more
reasonable
pretty much same network as
snake still not seeing stuff showing up
snake still not seeing stuff showing up
in uh user stats which is
in uh user stats which is
weird I did Commit This I mean I did
weird I did Commit This I mean I did
yeah episode
yeah episode
return did I just messed this up uh if
return did I just messed this up uh if
tick percent 32
tick percent 32
oh am I not increasing self.
oh am I not increasing self.
tick no I
tick no I
am
am
[Music]
[Music]
so maybe it shows up in
here very
weird there a little bit weird how this
weird there a little bit weird how this
thing logs
um episode
um episode
return yeah it's a little weird how this
return yeah it's a little weird how this
thing
thing
logs I'll have to mess with
logs I'll have to mess with
that how did I do it in snake that it
that how did I do it in snake that it
worked I was just running it at much
worked I was just running it at much
larger
scale this is zero
scale this is zero
regardless cuz that's not
helpful yeah so here there's a special
helpful yeah so here there's a special
case um to see if Episode return is in
case um to see if Episode return is in
here so it prioritizes logging data that
here so it prioritizes logging data that
has episode return in
has episode return in
it but I didn't do that for snake and it
it but I didn't do that for snake and it
still
worked I do
it so for snake I
did I did every 128 steps which is not
did I did every 128 steps which is not
very often and it still worked
so is
something is something off here they
something is something off here they
didn't do episode return
here oh right here this
else so what we're going to do is we're
else so what we're going to do is we're
going to just put this down
here yeah that'll do
here yeah that'll do
it now we'll be able to see
[Music]
[Music]
cool so let's see if this is actually
cool so let's see if this is actually
zero or if there's um a slight positive
zero or if there's um a slight positive
reward
reward
here
here
uhoh I'm going to have to delete some uh
uhoh I'm going to have to delete some uh
some runs
apparently should be okay for a little
apparently should be okay for a little
bit
okay the reward is actually
okay the reward is actually
zero which leads me to believe that
zero which leads me to believe that
we're just like not updating it
correctly so where's the reward for
correctly so where's the reward for
food okay so this is zero reward for
food okay so this is zero reward for
food but you should be getting a reward
food but you should be getting a reward
based on food eaten from the environment
based on food eaten from the environment
itself provided that we've passed the
itself provided that we've passed the
rewards
rewards
correctly which where did we do
that yes so we pass buff rewards
that yes so we pass buff rewards
here
and we fill it with zeros here but then
and we fill it with zeros here but then
we step it right
continuous food reward
this looks
this looks
reasonable food is one wall is two
right there's one walls two did I just
right there's one walls two did I just
forget to recompile it like since I last
forget to recompile it like since I last
changed
that looks like it's building something
that looks like it's building something
new so something
changed we should hopefully get
changed we should hopefully get
something that is not
zero
really welcome back Samurai
we are currently fixing this environment
we are currently fixing this environment
up so that it will actually train some
up so that it will actually train some
cool
tasks hey curious have you faced any
tasks hey curious have you faced any
limitations with scyon if you've
limitations with scyon if you've
answered this apologies scyon is
answered this apologies scyon is
absolutely amazing like it is so
absolutely amazing like it is so
freaking good scyon has single-handedly
freaking good scyon has single-handedly
fixed like most of the problems that I
fixed like most of the problems that I
have had uh limitations that I've had in
have had uh limitations that I've had in
the field because you can give it to
the field because you can give it to
researchers it looks like python it it
researchers it looks like python it it
is python with a couple types added um
is python with a couple types added um
it's super fast to write and you can get
it's super fast to write and you can get
C native speeds and you can get shared
C native speeds and you can get shared
data access between Python and scyon
data access between Python and scyon
read and write shared access very very
read and write shared access very very
easily um so so many good things I've
easily um so so many good things I've
had like a couple very small nitpicks
had like a couple very small nitpicks
with it but so far it's like use syon
with it but so far it's like use syon
it's amazing
love me some
scon the few nitpicks would be they kept
scon the few nitpicks would be they kept
the C behavior of passing uh structs by
the C behavior of passing uh structs by
value so you have to use pointers where
value so you have to use pointers where
if they just passed by reference you
if they just passed by reference you
wouldn't I wish they would have kept
wouldn't I wish they would have kept
that from python um you cannot Define
that from python um you cannot Define
variables inside of conditionals and
variables inside of conditionals and
Loops you have to Define them at the top
Loops you have to Define them at the top
that's slightly annoying and um the
that's slightly annoying and um the
compile times could be better those are
compile times could be better those are
my main
my main
[Music]
annoyances okay so seriously what is
annoyances okay so seriously what is
wrong with this food thing I think we
wrong with this food thing I think we
need to just go put a print somewhere in
need to just go put a print somewhere in
here oh yeah and you can't run pdb
here oh yeah and you can't run pdb
inside of scon I really wish could run
inside of scon I really wish could run
pdb inside of cython and like debug at
pdb inside of cython and like debug at
the python level I know that would be
the python level I know that would be
slow but maybe you would be able to do
slow but maybe you would be able to do
it
food yeah cython is like keys to the
food yeah cython is like keys to the
kingdom for high performance simulation
kingdom for high performance simulation
engineering without spending months
engineering without spending months
having to do
having to do
it it's like I can build stuff so fast
it it's like I can build stuff so fast
now and actually have it be high
now and actually have it be high
perf in fact it can actually be faster
perf in fact it can actually be faster
to build in scon because I can write the
to build in scon because I can write the
code really stupid you know I don't have
code really stupid you know I don't have
to spend time uh doing I don't have to
to spend time uh doing I don't have to
spend time doing like optimization crap
spend time doing like optimization crap
that I would in Python like I just do it
that I would in Python like I just do it
in scon and it's fast it's so
in scon and it's fast it's so
nice love me
scon okay so right here
scon okay so right here
this should
this should
be
something that was a lot of
foodprints that was like a lot of
foodprints that was like a lot of
foodprints but okay so
says that we're getting rewards but I'm
says that we're getting rewards but I'm
not seeing
not seeing
any which is very
any which is very
weird um my guess would be that we
weird um my guess would be that we
accidentally assigned rewards somehow
accidentally assigned rewards somehow
did we overwrite reward
somewhere I have a fill here the fill
somewhere I have a fill here the fill
should be fine I don't know why this uh
should be fine I don't know why this uh
rewards is not
rewards is not
synced self. rewards equals
synced self. rewards equals
rewards self rewards of agent
rewards self rewards of agent
idx this looks fine to
me is it the
fill I don't think that the fill would
fill I don't think that the fill would
do it
do it
right oops
WS did I do like m. rewards equals or
something now this returns
zeros I do like
this maybe the plus equals is not in
this maybe the plus equals is not in
place is I think it
is
really uh it's occurring to me that I'm
really uh it's occurring to me that I'm
not seeing actually getting food
here it should be getting some
food
so what is up with
so what is up with
that knit
function oh is it deleting all the
function oh is it deleting all the
agents let me see
yeah yeah yeah
um so it needs to be
like
times
times
n. grid
grid
uh I think I was overriding the
agents
right thoughts on Jack
right thoughts on Jack
is this a
is this a
troll I literally wrote like a super
troll I literally wrote like a super
ranty post about
ranty post about
this I mean this Artic I literally wrote
this I mean this Artic I literally wrote
like this stupid rant article that blew
like this stupid rant article that blew
up here this thing has 140,000 views on
up here this thing has 140,000 views on
it on my relatively small
it on my relatively small
Twitter um not a jack
fan main reason for it is the problems
fan main reason for it is the problems
that it Sol solving are way more easy to
that it Sol solving are way more easy to
solve directly
solve directly
with relatively basic infrastructure and
with relatively basic infrastructure and
the price of jacks is introducing a
the price of jacks is introducing a
domain specific language to everything
domain specific language to everything
so especially for environment
so especially for environment
simulations I can write dead simple code
simulations I can write dead simple code
and have stuff run at millions of steps
and have stuff run at millions of steps
per second or I can be forced to
per second or I can be forced to
implement stuff in a domain specific
implement stuff in a domain specific
language and now be heavily constrained
language and now be heavily constrained
in the types of environments I can
in the types of environments I can
Implement in the first place
a lot of people do like Jack but wait do
a lot of people do like Jack but wait do
you like it for training or do you like
you like it for training or do you like
it for
it for
environments these are two different
environments these are two different
things I don't really like it for either
things I don't really like it for either
I'm slightly more tolerant of it for
I'm slightly more tolerant of it for
training than for environments in most
training than for environments in most
cases though there are some environments
cases though there are some environments
some like Niche cases where it makes a
some like Niche cases where it makes a
lot of sense for
environments but I think it's definitely
environments but I think it's definitely
being overused right now heavily heavily
being overused right now heavily heavily
overused and it's causing RL a lot of
overused and it's causing RL a lot of
problems
mostly for OD yeah that's totally fine
mostly for OD yeah that's totally fine
if you're using it for like scientific
if you're using it for like scientific
Computing stuff and you just want like
Computing stuff and you just want like
an easy way to do array Ops on GPU
an easy way to do array Ops on GPU
that's totally
that's totally
fine um the problem is people are trying
fine um the problem is people are trying
to implement full Stacks where the
to implement full Stacks where the
environment which is a complex simulator
environment which is a complex simulator
is end to end Jacks the reinforcement
is end to end Jacks the reinforcement
learning libraries end to end Jacks with
learning libraries end to end Jacks with
tons of third-party poorly developed
tons of third-party poorly developed
libraries like flax which are like god-
libraries like flax which are like god-
awful hybrid of overly functional and
awful hybrid of overly functional and
overly object-oriented programming and
overly object-oriented programming and
the whole stack essentially grinds to a
the whole stack essentially grinds to a
halt because they've introduce all this
halt because they've introduce all this
complexity in the name of performance
complexity in the name of performance
instead of just like not writing stuff
instead of just like not writing stuff
in in playing python like domain
in in playing python like domain
specific language inside of python
specific language inside of python
versus add a couple types in just syon
versus add a couple types in just syon
like I don't understand how this is a
like I don't understand how this is a
comparison right totally fine for other
comparison right totally fine for other
applications the post was for Riel
applications the post was for Riel
infrastructure
specifically can I do and personand
here
here
really what about Plus
what's the Boolean mask op in
what's the Boolean mask op in
um isn't there Boolean mask op in numpy
um isn't there Boolean mask op in numpy
that just works or do I just do I really
that just works or do I just do I really
have to do like this
have to do like this
times like
times like
bu what ites
bu what ites
type
type
w truth value of an array with
w truth value of an array with
am I doing this
wrong
what
huh oh is it the
huh oh is it the
parenthesis it's the parenthesis right
parenthesis it's the parenthesis right
that's so obnoxious
oops I would have thought that the
oops I would have thought that the
uh I guess it makes sense why that
uh I guess it makes sense why that
happens right if you think about
it yeah of course it's going to bind it
it yeah of course it's going to bind it
binds those operators last
okay I'm something is really Jank here
okay I'm something is really Jank here
and I don't know what it is
um can I just like evil this
yes yes I've spoken uh with Alexi quite
yes yes I've spoken uh with Alexi quite
a bit um so sample Factory gives you
a bit um so sample Factory gives you
very good
very good
performance the code is much more
performance the code is much more
complicated than something like puffer
complicated than something like puffer
so I am trying to get as close to that
so I am trying to get as close to that
level of performance as possible
level of performance as possible
actually in some cases I think I have
actually in some cases I think I have
better um but he has a lot of
better um but he has a lot of
optimizations that I don't but I'm not
optimizations that I don't but I'm not
willing to complicate the code bases I
willing to complicate the code bases I
want reinforcement learning to be like
want reinforcement learning to be like
dead simple and performant by default
dead simple and performant by default
that's the whole thing that I'm going
for like clean ourl Simplicity sample
for like clean ourl Simplicity sample
Factory performance if you want to look
Factory performance if you want to look
at it that way that is the that is the
objective cleaner Simplicity sample
objective cleaner Simplicity sample
Factory
performance and
performance and
actually sample Factory performance only
actually sample Factory performance only
takes into account the um the learning
takes into account the um the learning
library uh I'm also re-engineering the
library uh I'm also re-engineering the
environments themselves to be a thousand
environments themselves to be a thousand
times faster so like together you can
times faster so like together you can
get stuff that is way way way faster
get stuff that is way way way faster
than sample Factory is now because
than sample Factory is now because
you're training on like equally complex
you're training on like equally complex
environments to what you'd be using
environments to what you'd be using
anyways but the environments just run
anyways but the environments just run
way
faster like the snake environment runs
faster like the snake environment runs
at 14 million steps per second on One
at 14 million steps per second on One
Core
Core
and trains it a
million linky it makes the RL more
million linky it makes the RL more
accessible to everybody like PhD
accessible to everybody like PhD
researchers don't understand what the
researchers don't understand what the
hell is going on in this space either
hell is going on in this space either
it's just all a mess it's not like oh
it's just all a mess it's not like oh
you need to take the time to learn it
you need to take the time to learn it
it's just everything is freaking hard
it's just everything is freaking hard
and a mess
sample factor is good though sample
sample factor is good though sample
factor is very very
good I mean the main objective of puffer
good I mean the main objective of puffer
is not to make RL easier for beginners
is not to make RL easier for beginners
the main objective of puffer is to push
the main objective of puffer is to push
the boundaries of what RL can do um but
the boundaries of what RL can do um but
the thing is that I think that the best
the thing is that I think that the best
way to do that is also to make it simple
way to do that is also to make it simple
which happens to have this nice side
which happens to have this nice side
effect of making like a lot more people
effect of making like a lot more people
able to get into RL
easily but yeah like a good thing to do
easily but yeah like a good thing to do
is like with sample Factory it's just
is like with sample Factory it's just
like open up the source code for sample
like open up the source code for sample
Factory try to figure out where all the
Factory try to figure out where all the
components that you would expect are
components that you would expect are
open up the code for puffer see where
open up the code for puffer see where
all the components are and see which one
all the components are and see which one
is easier to figure
is easier to figure
out right puffer doesn't have all the
out right puffer doesn't have all the
stuff that sample Factory
though the link key you're rapidly
though the link key you're rapidly
running out of Noob credits you know
running out of Noob credits you know
after it's been a year you're going to
after it's been a year you're going to
just be expected to know things that's
just be expected to know things that's
how it works
[Music]
right what is going on here this is so
right what is going on here this is so
weird is it this thing
something is
wonk okay so the rewards are just not
wonk okay so the rewards are just not
coming through and I don't get why
coming through and I don't get why
not self. buff.
not self. buff.
rewards it's
rewards it's
here gets
here gets
assigned oh let's put this like this
assigned oh let's put this like this
maybe
interesting so when you you do that
fill does that like screw something
up wait did I mess
up where is
food reward did I put it in the wrong
food reward did I put it in the wrong
spot oh man I put this
spot oh man I put this
on I put this on the
on I put this on the
spawn
o that was stupid that was very very
o that was stupid that was very very
stupid supposed to be
stupid supposed to be
there my bad
one of the side effects of starting your
one of the side effects of starting your
day with two to three hours of Hard
day with two to three hours of Hard
Exercise um is that occasionally your
Exercise um is that occasionally your
brain will just decide no I'm not going
brain will just decide no I'm not going
to work today
are these eating
food they're moving around the food
food they're moving around the food
aren't
they wait
they wait
if disk this is d y it needs to be here
if disk this is d y it needs to be here
right disk
right disk
dest d x yeah yeah yeah I just I have
dest d x yeah yeah yeah I just I have
the I have the names wrong here
cool yeah I thought that was a pretty
cool yeah I thought that was a pretty
good line I'm sure that's going to piss
good line I'm sure that's going to piss
off a lot of a lot of
people I swear like heavy functional
people I swear like heavy functional
programming is just like computer
programming is just like computer
science's
science's
world of like intellectual Naval gazing
world of like intellectual Naval gazing
it's just what the
hell heavily functional heavily oop are
hell heavily functional heavily oop are
just both insane and like trying to like
just both insane and like trying to like
oh yeah well we're going to make a
oh yeah well we're going to make a
functional library and the crazy thing
functional library and the crazy thing
about it is like you know I'll post like
about it is like you know I'll post like
some absolute garbage snippet of code
some absolute garbage snippet of code
that's like makes no sense and then I'll
that's like makes no sense and then I'll
have like three or four people online
have like three or four people online
telling me well it avoids side effects
telling me well it avoids side effects
and you avoid bugs because it's
and you avoid bugs because it's
declarative or whatever and it's just
declarative or whatever and it's just
like look at the code it's garbage like
like look at the code it's garbage like
I don't literally it's just doing random
I don't literally it's just doing random
stuff that's not even written there what
stuff that's not even written there what
on
Earth okay that's
Earth okay that's
good this is work this is uh working
now e
there we go these things get reward they
there we go these things get reward they
eat food I think that there was
eat food I think that there was
something weird happening in reset so
something weird happening in reset so
we're just going to watch this for a
we're just going to watch this for a
second until it resets and does the
second until it resets and does the
weird thing
yep
yep
okay we
okay we
do self. grid.
fill and hopefully this is no
good you know the sad thing about that
good you know the sad thing about that
Jack's rant is like I get mad because I
Jack's rant is like I get mad because I
waste an hour and a half on stupid
waste an hour and a half on stupid
Jack's things and I'm like this is not
Jack's things and I'm like this is not
the way forward so I write this really
the way forward so I write this really
low effort post
low effort post
right and then the next day I write an
right and then the next day I write an
actually good
post and everybody gets mad as well
post and everybody gets mad as well
everybody's mad about this stupid post
everybody's mad about this stupid post
and then the next day I write a good
and then the next day I write a good
post which is
post which is
like actual infr tear down of the puffer
like actual infr tear down of the puffer
cluster that has a lot of really cool
cluster that has a lot of really cool
information in it and it gets less than
information in it and it gets less than
a tenth of the well actually no this is
a tenth of the well actually no this is
a hund this is like a h hundredth of the
a hund this is like a h hundredth of the
exposure
so I don't know
interesting we actually got some cool
interesting we actually got some cool
replies on this
so search
space I don't buy
space I don't buy
this I don't buy this take
this
this
is sometimes
true small just
yeah maybe
yeah maybe
yeah is this yeah okay this is the Sony
yeah is this yeah okay this is the Sony
AI
guy cool
oo very nice this is old but
uh
what oh that's nothing all right let's
what oh that's nothing all right let's
see yeah this is now correct there's no
see yeah this is now correct there's no
like old artifacting
here let's uh let's train
leave a trainer on it for a couple
leave a trainer on it for a couple
seconds let me read chat uh been super
seconds let me read chat uh been super
bullish on Jacks realizing how much
bullish on Jacks realizing how much
effort I put into hackly massaging
effort I put into hackly massaging
things just to get them jetable or yes
things just to get them jetable or yes
so that is the problem with Jacks and if
so that is the problem with Jacks and if
you don't know that you need GPU and you
you don't know that you need GPU and you
just need things to be not
just need things to be not
python try cython it's awesome what if
python try cython it's awesome what if
you could just write loops and
you could just write loops and
conditionals and like arbitrary code and
conditionals and like arbitrary code and
just have it be
just have it be
c-speed but not actually have to do c
c-speed but not actually have to do c
things and like you know not like get
things and like you know not like get
actual reasonable error messages when
actual reasonable error messages when
you go to compile stuff or when runtime
you go to compile stuff or when runtime
stuff happens like it's really really
stuff happens like it's really really
nice for
nice for
that it also has very nice integration
that it also has very nice integration
with numpy so if you're doing um
with numpy so if you're doing um
scientific Computing stuff and you need
scientific Computing stuff and you need
to do like slice operations over arrays
to do like slice operations over arrays
that can be done very efficiently
but yeah the thing that you described
but yeah the thing that you described
that is the precise problem with
ja now mind you it's a little different
ja now mind you it's a little different
in my case right because we actually are
in my case right because we actually are
doing something with the GPU like we'd
doing something with the GPU like we'd
like to use the GPU
like to use the GPU
for simulating neural networks not for
for simulating neural networks not for
simulating the environment
simulating the environment
so with puffer I have the core spinning
so with puffer I have the core spinning
at 100 doing the environment and then
at 100 doing the environment and then
the GPU spinning at you know 100 doing
the GPU spinning at you know 100 doing
uh the neural net so there's a
uh the neural net so there's a
efficiency there and uh this is actually
efficiency there and uh this is actually
already training this is already
already training this is already
optimizing into
peers doesn't look like it's necessarily
peers doesn't look like it's necessarily
performing great but it's better than
performing great but it's better than
better than nothing
better than nothing
maybe is this continuous mode or
maybe is this continuous mode or
discreet
mode I forgot what I left it
mode I forgot what I left it
on uh this is discret mode
okay not
amazing do we get episode return
amazing do we get episode return
though o episode return went up and then
though o episode return went up and then
it goes back
it goes back
down that could just be hyper parameters
down that could just be hyper parameters
though let's get the uh the other tasks
though let's get the uh the other tasks
to appear to be working and uh do a few
to appear to be working and uh do a few
more things from there
yeah so this crashed
other
environments I don't know why I'm so
environments I don't know why I'm so
tired today holy
I've got two evening meetings so that's
I've got two evening meetings so that's
chill at
least uh
reward where did the dock
reward where did the dock
go you have the dock
go you have the dock
somewhere
somewhere
yeah forging Predator pre optimize group
yeah forging Predator pre optimize group
okay
so that
so that
was
was
foring we
go Predator
go Predator
prey we'll see how this does if this
prey we'll see how this does if this
um makes any sense whatsoever
oops slice IND Dees must be
integers probably should have before I
integers probably should have before I
tried to track the run I should have
tried to track the run I should have
just done um train like this
this one as well
oh did it not post this
oh did it not post this
stream maybe it didn't post the stream
stream maybe it didn't post the stream
on X I'm trying to figure out why like I
on X I'm trying to figure out why like I
was doing the same stuff yesterday and
was doing the same stuff yesterday and
it went and it was like 10 times more
it went and it was like 10 times more
views did it post it to my
views did it post it to my
timeline no it did it did that's fine I
timeline no it did it did that's fine I
guess just day two of the same project
guess just day two of the same project
as
well okay so uh reward here
that's
that's
interesting you're not necessarily going
interesting you're not necessarily going
to know what the uh thing looks like
to know what the uh thing looks like
until we have ELO of some type because
until we have ELO of some type because
Predator pray is an adversarial
task but it does train so that's
nice let's see if it looks reasonable
yes it
yes it
does uh the red ones are on the
outside okay that's interesting I didn't
outside okay that's interesting I didn't
expect that
so as is probably
fine and so Predator prey and then was
fine and so Predator prey and then was
group up is the third
one
grp which I would hope would be one of
grp which I would hope would be one of
the easier tasks
quick reminder for folks if you haven't
quick reminder for folks if you haven't
starred a puffer lib please go ahead and
starred a puffer lib please go ahead and
do that it helps me out a whole
ton helps me get all this work out there
that's really
that's really
weird
weird
observations.
observations.
shape colors.
shape colors.
shape
buff
same
group
group
some diff reward
can I just do like knot like
this
this
not I don't know if the the
not I don't know if the the
unary you got to do like this right
yeah and we get all the different pops
cool and let's just open uh
cool and let's just open uh
config Rd
config Rd
continuous we'll do this like
continuous we'll do this like
20 mil steps or
whatever and we'll see if it actually
trains let's see if this trains anything
flat negative one means I'm Computing
flat negative one means I'm Computing
the reward wrong guaranteed
the reward wrong guaranteed
let's go figure that out
oops guaranteed means that I'm Computing
oops guaranteed means that I'm Computing
it
wrong what on Earth
okay oh the rewards are being clipped
stupid
right yeah this
right yeah this
is we can clip them between
is we can clip them between
-1 and one
yeah that's
fine
um this still might not work but we'll
see this will be better at least
so it's stuck at negative
-1 because it can
see it can see um other agents around it
see it can see um other agents around it
a lot of other agents around it that are
pop let me see if it's actually flat
pop let me see if it's actually flat
negative one I think it probably
negative one I think it probably
is yeah it's flat negative one okay so
is yeah it's flat negative one okay so
we need to
do can I do like divide by 50 or
do can I do like divide by 50 or
something or do I do divide by 10 and do
something or do I do divide by 10 and do
like Nega five to
like Nega five to
five for now
now it's just stuck at neg five
all
right
H well it shouldn't be just stationary
H well it shouldn't be just stationary
right if I do buy if I divide by 50
gets worse over
time
well weird
well weird
least it's not stuck
stationary let's do over
stationary let's do over
100 and then let's just clip it from
100 and then let's just clip it from
negative 1 to
negative 1 to
one it's probably the best
thing well whether this actually helps
thing well whether this actually helps
we'll we'll see
we'll we'll see
I think it's probably getting two large
rewards oh well doesn't have to train
rewards oh well doesn't have to train
immediately we can always think about
immediately we can always think about
why it's not
why it's not
training uh we do have the the tasks
training uh we do have the the tasks
though which is
though which is
nice only one I'm missing is the puffer
nice only one I'm missing is the puffer
lib
task um I'm going to have to
task um I'm going to have to
decide a few things before I can do
that yeah I'm going to have to decide on
that yeah I'm going to have to decide on
a few
things
so thousand by th000
map let's let's change some defaults I
map let's let's change some defaults I
think we want 4096 agents not
think we want 4096 agents not
1,24 um if I do some Division if I
1,24 um if I do some Division if I
do like 1
E6 for
E6 for
4096 200 Tils per agent that's plenty
and what we do is we say Map size is
and what we do is we say Map size is
going to
going to
be 1080
oops
uh I thought I didn't have render size
uh I thought I didn't have render size
anymore do I
wait what
wait what
I'm hold
on
on
wait width height n agent
Horizon
Horizon
oh it's just going to be
with hey was talking to some dude about
with hey was talking to some dude about
your and whether or not your platform
your and whether or not your platform
could be used for agent-based
could be used for agent-based
computational economics
computational economics
um yeah I've had a lot of people ask me
um yeah I've had a lot of people ask me
that
that
um you can kind of do
um you can kind of do
it it you can get some interesting stuff
it it you can get some interesting stuff
it's very hard to train agents to
it's very hard to train agents to
actually engage with a market system uh
actually engage with a market system uh
I do have some upcoming work in this
I do have some upcoming work in this
area though um the the inspiration for
area though um the the inspiration for
is not necessarily economic I guess it
is not necessarily economic I guess it
will be somewhat interesting for that
will be somewhat interesting for that
purpose
purpose
um yeah definitely with puffer you could
um yeah definitely with puffer you could
very very easily write like an RL
very very easily write like an RL
environment that has those types of
environment that has those types of
action spaces that would be fast enough
action spaces that would be fast enough
to train to get some interesting
to train to get some interesting
behaviors for sure
should the Horizon be for this
task it's 100 ticks per
minute
so well a good way to decide that would
so well a good way to decide that would
be it takes
be it takes
512 from the
512 from the
center it's going to take like a th
center it's going to take like a th
steps to get to the
edges Y at Ker's machine learning
Discord
um you can get some I there were some
um you can get some I there were some
things in neural MMO in the
things in neural MMO in the
competition and with some of my other
competition and with some of my other
results where we actually got kind of
results where we actually got kind of
like reasonable looking supply and
like reasonable looking supply and
demand stuff to
demand stuff to
emerge where like items would actually
emerge where like items would actually
just be priced reasonably by the
just be priced reasonably by the
agents I think some of it was because
agents I think some of it was because
people were scripting it in but some of
people were scripting it in but some of
it was definitely emergent and
it was definitely emergent and
um the problems that we have with a lot
um the problems that we have with a lot
of this are less to do with the
of this are less to do with the
difficulty of that problem specifically
difficulty of that problem specifically
and more to do with the fact that like
and more to do with the fact that like
the market is one small part of a much
the market is one small part of a much
larger environment and getting agents to
larger environment and getting agents to
use a market when it's a lot one small
use a market when it's a lot one small
part of a larger environment and it's
part of a larger environment and it's
not immediately clear how it's important
not immediately clear how it's important
is difficult if you just wanted to have
is difficult if you just wanted to have
like agents to optimize on a market to
like agents to optimize on a market to
like observe interesting trends that
like observe interesting trends that
would be substantially substantially
would be substantially substantially
easier
okay so this is a nice little frame this
okay so this is a nice little frame this
has this 4096 agents in it now does that
has this 4096 agents in it now does that
look like 4096
agents guess so right
and some of them are stuck like they
and some of them are stuck like they
cannot
cannot
move because they're blocked
in and they're slowly diffusing over
in and they're slowly diffusing over
time you can
see okay so this is perfectly fine for
now one aspect we discussed was the
now one aspect we discussed was the
dictator game Nash equilibrium whether
dictator game Nash equilibrium whether
not agents engaging in sanctions
not agents engaging in sanctions
policies would lead to more peaceful
policies would lead to more peaceful
behaviors
uh so the really obnoxious thing about
uh so the really obnoxious thing about
Game Theory and RL is
Game Theory and RL is
like we solve a lot of these problems in
like we solve a lot of these problems in
the real world like we break Nash
the real world like we break Nash
equilibri in the real world by cheating
equilibri in the real world by cheating
outside of the game right like we Sol
outside of the game right like we Sol
one of the things that uh that's common
one of the things that uh that's common
like iterated prisoners dilemma for
like iterated prisoners dilemma for
inance
inance
we don't necessarily solve that problem
we don't necessarily solve that problem
in the real world we solve it by like
in the real world we solve it by like
you know making promises or threats
you know making promises or threats
outside of that game environment right
outside of that game environment right
it's not like oh tip for Tad in prison
it's not like oh tip for Tad in prison
it's hey uh if you you know if you wrap
it's hey uh if you you know if you wrap
me out you're going to get it when I get
me out you're going to get it when I get
out right
out right
um so what I would say for that type of
um so what I would say for that type of
a thing
a thing
is is the behavior you're looking for
is is the behavior you're looking for
something that is reasonably learnable
something that is reasonably learnable
in the first place cuz you don't break
in the first place cuz you don't break
Nash equilibrio without having external
Nash equilibrio without having external
factors that allow you to do so
generally there's some work with like
generally there's some work with like
Alpha star and exploiter agents um
Alpha star and exploiter agents um
that's probably relevant
here but it still relies on the game
here but it still relies on the game
having like some sort of way around it
did I get this right hold on I think I
did I get this right hold on I think I
got to update the
got to update the
config yeah cuz we're going to replace
config yeah cuz we're going to replace
uh
uh
eight 512 agent environments with 1
eight 512 agent environments with 1
14096 agent environment so this is going
14096 agent environment so this is going
to
to
be 2
M's one M fat
size this should be substantially better
size this should be substantially better
I still don't know why they're not
I still don't know why they're not
learning uh we should probably
learning uh we should probably
do the diffusion task
right do we have the diffusion task in
right do we have the diffusion task in
here
here
still we should put like scatter back in
here cuz that was kind of an interesting
here cuz that was kind of an interesting
task oops
is it reward
introverts I know I like calling it
introverts I know I like calling it
introverts it's kind of
funny
introverts yeah let's add introverts
introverts yeah let's add introverts
back to the task pool
see if it
runs okay so this does run does it
train okay so this does
train okay so this does
train now what we're going to do is
train now what we're going to do is
we're going to commit all this stuff up
we're going to commit all this stuff up
and we're train it on the big machion
and we're train it on the big machion
and we're going to figure out why this
and we're going to figure out why this
thing is not working the way we want it
thing is not working the way we want it
to
okay this is the I9 14900 uh 13900 K
okay this is the I9 14900 uh 13900 K
Machine slightly worse than the other
Machine slightly worse than the other
boxes I have and the 4090 box
so skibbidy fortnite
I don't know why that's funny to me but
I don't know why that's funny to me but
that is funny
[Music]
we run this 120 I think 125 mil was the
we run this 120 I think 125 mil was the
original setting
right let's
do Rd continuous
if this does not hit a million uh steps
if this does not hit a million uh steps
per second then we have some
per second then we have some
optimization that's
wrong okay it's close but we probably
wrong okay it's close but we probably
have some a little a little bit more
have some a little a little bit more
optimization to do because it should
optimization to do because it should
hit a million and I can see that the
hit a million and I can see that the
environment is not at
environment is not at
0% though it is it is decreasing so
0% though it is it is decreasing so
maybe
maybe ah the reward is actually maybe
maybe ah the reward is actually maybe
improving let's
see puff
grid little bit of patience
right the reward is going to end up
right the reward is going to end up
being cyclic I think
being cyclic I think
because
because
um at the start of the environment it's
um at the start of the environment it's
bad and at the end of the environment
bad and at the end of the environment
it's good so at the start of the episode
it's good so at the start of the episode
um all the agents are stuck together and
um all the agents are stuck together and
then they spread out so this is going to
then they spread out so this is going to
like be pretty Jagged but
like be pretty Jagged but
um hopefully
reasonable is this sp3 based
reasonable is this sp3 based
definitely not sp3 based this is puffer
VEC in fact this whole environment would
VEC in fact this whole environment would
be like 20 times slower just because of
be like 20 times slower just because of
the uh the petting zoo API if I were
the uh the petting zoo API if I were
using that this is the advanced puffer
using that this is the advanced puffer
version nope the model is not compiled
version nope the model is not compiled
uh I can make it faster than this
uh I can make it faster than this
anyways I'm sure even without doing that
anyways I'm sure even without doing that
cuz I have 1 million plus on snake I'm
cuz I have 1 million plus on snake I'm
sure it just needs a little bit of
sure it just needs a little bit of
tuning in fact let me just look what's
tuning in fact let me just look what's
what do I do different with snake
what do I do different with snake
here snake
has uh it looks like snake just has a
has uh it looks like snake just has a
bigger mini batch
bigger mini batch
size is that
size is that
all
so num Ms num
so num Ms num
workers yeah bigger Mini bch which
workers yeah bigger Mini bch which
actually means I could probably make it
actually means I could probably make it
even
even
faster substantially over a
million what did I do
million what did I do
here yeah so 4096 would be the limit I'm
here yeah so 4096 would be the limit I'm
assuming
assuming
um 42% forward pass that's why
maybe this is not
learning yeah this reward function
learning yeah this reward function
doesn't look too
good so the question is what did I
good so the question is what did I
break we definitely had this learning
break we definitely had this learning
before and this should be a very easy
before and this should be a very easy
task
well we'll go look at that right
first of
all let's just get the speed of this to
all let's just get the speed of this to
be something you know a little bit more
be something you know a little bit more
reasonable and let's just borrow the
reasonable and let's just borrow the
snake uh the snake settings because I'm
snake uh the snake settings because I'm
assuming they're probably going to be
assuming they're probably going to be
decent
right so for
right so for
snake we do a batch size
snake we do a batch size
of seems dated I'm pretty sure it was
of seems dated I'm pretty sure it was
larger than this but 131 we'll use 131k
larger than this but 131 we'll use 131k
for now this should be simpler than
for now this should be simpler than
snake anyways we'll do
snake anyways we'll do
32k batch
32k batch
here gamma's fine this is fine entropy
here gamma's fine this is fine entropy
is really high
is really high
here do
here do
o 01 or O2
o 01 or O2
maybe do
O2 learning rate of 0.003
it's actually
it's actually
fine so let's start with
this okay there you go there's your 1
this okay there you go there's your 1
million steps per
second it's funny we can actually make
second it's funny we can actually make
this way way faster if we just use more
this way way faster if we just use more
agents
I'm kind of
tempted
um 32k
um 32k
reasonable 32,000
reasonable 32,000
agents I don't know what would be a
agents I don't know what would be a
good or optimal batch size for this
just for
just for
fun yeah let's not do this tracking
fun yeah let's not do this tracking
stuff
right
sh let's say that we're going to make
sh let's say that we're going to make
this 32,000 agents right
this 32,000 agents right
7 uh 8 768 like this
how fast do this
train only
1.2 it's kind of weird isn't
1.2 it's kind of weird isn't
it Forward pass is still slow
very
slow I wonder if that's the data
slow I wonder if that's the data
transfer
transfer
right but I thought I'd optimized it
right but I thought I'd optimized it
pretty well
well yeah it's TR we're going to fix the
well yeah it's TR we're going to fix the
training
separately I want to make sure that I
separately I want to make sure that I
have the Sim running at a good speed
have the Sim running at a good speed
first and then I have it op like
first and then I have it op like
Hardware
optimal uh 32k agents right
why would that be
why would that be
slow
continuous I think I defined this as un8
continuous I think I defined this as un8
didn't
didn't
I the OB
slices yeah Vision range is
slices yeah Vision range is
five so these observations are very
tiny I mean I can't say it's very tiny
tiny I mean I can't say it's very tiny
cuz it's
cuz it's
32,000 t
32,000 t
time what's that
time what's that
100ish by
100ish by
11 so 32 KB 3
MB where's the Ford pass time
I no wait this is in MK right so the two
I no wait this is in MK right so the two
device time is actually just in
MK
um Place update should be faster
is
is
14% which is still
high why is
it why is it that the Ford passes this
slow it was was made substantially
slow it was was made substantially
faster right before training was made
faster right before training was made
substantially
faster well
definitely some room for
optimizations this was faster than
optimizations this was faster than
before or
no I don't actually want to mess with
no I don't actually want to mess with
the we're going to take the uh the one
the we're going to take the uh the one
mil version for
mil version for
now because this this number of Agents
now because this this number of Agents
can actually start to mess stuff up like
can actually start to mess stuff up like
this this can actually start to mess up
this this can actually start to mess up
a lot of uh tricky to track down
a lot of uh tricky to track down
things just the way the memory works
out
continuous mini batch is
fine uh Eight's probably fine
frankly
frankly
grid
grid
entropy yeah so all these things are fun
fine
fine
so next debug
check orops next debug check is going to
be the
Horizon so we're going to set a nice
Horizon so we're going to set a nice
very short Horizon
let's do Dash Dash Track as well and to
let's do Dash Dash Track as well and to
set a nice short
Horizon and that should make it easier
Horizon and that should make it easier
to learn a little bit
we will see whether this is the case uh
we will see whether this is the case uh
as a matter of
fact the reward does seem to be
fact the reward does seem to be
improving quite consistently
right yes
is it getting worse
again or is it just
stable all just
say so regardless though it is learning
something entropy curve is
something entropy curve is
fine clip Frack is a little high but
fine clip Frack is a little high but
probably
fine uh this is L
crashing odd for an environment like
crashing odd for an environment like
this to be
this to be
unstable very
odd environment like should this should
odd environment like should this should
be very
be very
stable does seem to be learning back a
stable does seem to be learning back a
little
bit let's see the curves
it does learn back a little bit so it's
it does learn back a little bit so it's
very weird how it's not fully stable
very weird how it's not fully stable
like
like
this um you can see the entropy crash
this um you can see the entropy crash
down pretty
down pretty
bad we'll have to do a sweep on
this and this is on the easy
version easy scatter environment
Let's uh let's see what this policy
Let's uh let's see what this policy
actually does
right this
model e
okay that's actually very
okay that's actually very
reasonable wait hold on that's very
reasonable wait hold on that's very
reasonable
reasonable
holy what the
[Laughter]
[Laughter]
heck that's
heck that's
sweet what the heck
y
y
uh okay and then like they screw up
uh okay and then like they screw up
because they all accumulate on the edge
because they all accumulate on the edge
but
uh that's kind of
cool it's so funny that they just chill
cool it's so funny that they just chill
down there as well they don't they
down there as well they don't they
didn't learn anything else other than
didn't learn anything else other than
that
okay let's let's actually get a
okay let's let's actually get a
recording of
recording of
that yeah but did you see the way it
that yeah but did you see the way it
separated it was kind of
separated it was kind of
cool it like they kind of like did this
cool it like they kind of like did this
diagonal separation so they had like
diagonal separation so they had like
they were moving across each other it
they were moving across each other it
was kind of cool
but I will steal that for
but I will steal that for
uh I will steal that as a
quote so that is going to be what 50
frames
frames
at 15
at 15
FPS we'll do
FPS we'll do
grid grid
grid grid
that
GI just want to watch this thing it's so
cool
like I mean they that actually really
like I mean they that actually really
looks like
sand you're right that's that's kind of
cool that's like a really convincing
cool that's like a really convincing
sand
sand
simulation isn't it it's like if you
simulation isn't it it's like if you
just hooked a pile of sand isn't
it
for e
this actually yeah okay cool
that looks really
sweet I mean it's like a nothing but it
sweet I mean it's like a nothing but it
it's still
funny people got to stop liking the
funny people got to stop liking the
stupid
post
post
bad okay uh
I really didn't expect that I like I
I really didn't expect that I like I
really didn't expect that to happen
really didn't expect that to happen
that's kind of weird that that worked
that's kind of weird that that worked
that
that
way
way
yeah
um what do we do next
um what do we do next
here well
here well
because I'll show you why so before I
because I'll show you why so before I
made this I I messed with this
made this I I messed with this
environment right I had like a similar
environment right I had like a similar
version of the same thing
well first of all the reward the reward
well first of all the reward the reward
curves didn't look very good
curves didn't look very good
right
right
and where is
it my darn
posts freaking thing
is there a command like I don't think
is there a command like I don't think
there's a Twitter command on YouTube is
there it's j Wars
there it's j Wars
5341 okay so this was
5341 okay so this was
the the file you this grid environment
the the file you this grid environment
so this is what happened when I trained
so this is what happened when I trained
it
it
before you see like they end up in these
before you see like they end up in these
weird
clusters so I don't know what happened
clusters so I don't know what happened
like what I changed that makes them not
like what I changed that makes them not
end up in these
clusters but this is what happened like
clusters but this is what happened like
after I tuned it a little bit um with
after I tuned it a little bit um with
the original version of the
project there are a few different things
project there are a few different things
like this this is uh a slightly higher
like this this is uh a slightly higher
per version of the above one so you can
per version of the above one so you can
see they spread out in two directions
see they spread out in two directions
but they still end up in these clusters
but they still end up in these clusters
these clumps and then if you use a
these clumps and then if you use a
really expensive to compute reward
really expensive to compute reward
function then you get something that
function then you get something that
looks a little bit more like this but I
looks a little bit more like this but I
didn't want to do this one cuz this
didn't want to do this one cuz this
one's really really
slow yeah and centralized ukan
slow yeah and centralized ukan
distance you get like this
thing and you can sometimes get him to
thing and you can sometimes get him to
like move in this amorphous mass as well
like move in this amorphous mass as well
with weird Rewards
um I guess we can try training one of
um I guess we can try training one of
the other
the other
ones just to see like how far off base
ones just to see like how far off base
we
we
are I also want to think about the time
are I also want to think about the time
Horizon a little
bit I think it's kind of fine as
bit I think it's kind of fine as
is it would be more stable if I cut the
is it would be more stable if I cut the
timer Horizon to 512
timer Horizon to 512
though I'm going to cut it to 512 just
though I'm going to cut it to 512 just
while I'm
deving it'll just make it a little bit
deving it'll just make it a little bit
easier because like they won't they're
easier because like they won't they're
wasting half of their training data
wasting half of their training data
now oh wow that's funny so this was the
now oh wow that's funny so this was the
one that I trained with 64 Horizon and I
one that I trained with 64 Horizon and I
guess it just
guess it just
generalized huh that's
generalized huh that's
cool we'll leave it on this for now and
cool we'll leave it on this for now and
we'll do
we'll do
[Music]
um
um
ocean uh great
ocean uh great
continuous
slash
slash
continuous and we're going to change the
continuous and we're going to change the
task
task
to let's do the foraging task
all
all
right we'll see how this one
goes okay reward goes
up reward does go
up so this will be done this is 125
up so this will be done this is 125
million steps training to put this into
million steps training to put this into
perspective and it finishes in 2 minutes
perspective and it finishes in 2 minutes
um when I was starting in reinforcement
um when I was starting in reinforcement
learning we would train stuff on
learning we would train stuff on
Atari people still do this but we would
Atari people still do this but we would
train stuff on Atari that was like the
train stuff on Atari that was like the
most common test suite and it would
most common test suite and it would
train
train
at like 1,000th maybe a couple
at like 1,000th maybe a couple
thousandths of this speed so You' do a
thousandths of this speed so You' do a
10 million step run and it would take
10 million step run and it would take
like multiple hours to do a 10 million
like multiple hours to do a 10 million
step Run 100 million step run in 2
step Run 100 million step run in 2
minutes not Apples to Apples but very
minutes not Apples to Apples but very
very nice uh this one actually appears
very nice uh this one actually appears
to be very stable
to be very stable
at least with the very short Horizon
at least with the very short Horizon
that I've given
that I've given
it so I'm hoping that this gives us some
it so I'm hoping that this gives us some
sort of interesting
sort of interesting
policy this should be a very easy task
policy this should be a very easy task
to be fair no reason for this task to be
to be fair no reason for this task to be
expected to be hard but um yeah we
expected to be hard but um yeah we
should just get sort of get an idea of
should just get sort of get an idea of
uh get an idea out of
uh get an idea out of
this and then we'll run some sweeps and
this and then we'll run some sweeps and
we'll debug the environments and stuff
we'll debug the environments and stuff
over the next couple of days and uh
over the next couple of days and uh
we'll figure out
we'll figure out
all the questions that we wanted to
all the questions that we wanted to
answer with it about continuous versus
answer with it about continuous versus
discreet was a discreet version just
discreet was a discreet version just
because I know it's going to be easier
because I know it's going to be easier
in
testing but we'll be able to do a lot
testing but we'll be able to do a lot
from
here and that is
here and that is
completed let's just make sure it didn't
completed let's just make sure it didn't
um
crash no it didn't crash actually got a
crash no it didn't crash actually got a
little better towards the end even
little better towards the end even
so pretty nice
so pretty nice
policy learns in 2
minutes and we have a model that is
minutes and we have a model that is
ready for
ready for
us this is public by the way so you can
us this is public by the way so you can
just grab my models if you
want uh this one's going to to be forg
PT I got to change the task to
PT I got to change the task to
forge haven't added a hook for this yet
forge haven't added a hook for this yet
so we'll just do it in the code
I think just G continuous
holy well that's
cool
huh for
so weird how it does this
so weird how it does this
like yeah I don't know it's
fun let's do
fun let's do
um let's do one other one a herting
um let's do one other one a herting
task uh your wish is my command let me
task uh your wish is my command let me
try that one cuz we actually already
try that one cuz we actually already
have
have
it it's a predator prey which is
it it's a predator prey which is
basically that
now the initial conditions for this are
now the initial conditions for this are
a little
screwy cuz the enemies start like around
screwy cuz the enemies start like around
the prey so I don't know if it's going
the prey so I don't know if it's going
to do anything cool or not like I might
to do anything cool or not like I might
have just screwed up this environment
reward goes
up ah so reward going like this is not
up ah so reward going like this is not
necessarily bad
necessarily bad
because it's an adversarial task
because it's an adversarial task
so if the prey does better the Predator
so if the prey does better the Predator
does worse and if the Predator does
does worse and if the Predator does
better then the prey does worse and
better then the prey does worse and
actually the rewards are like one is the
actually the rewards are like one is the
negative of the other so in order for
negative of the other so in order for
one of them to do better the other has
one of them to do better the other has
to do worse
to do worse
it's I don't know if it's quite Zero Sum
it's I don't know if it's quite Zero Sum
I think it might be Zero
Sum anyways I have no idea what this is
Sum anyways I have no idea what this is
going to do
going to do
so let's see
one of my favorite things about this
one of my favorite things about this
type of work is you can be really stupid
type of work is you can be really stupid
about
about
it like I don't have to know what the
it like I don't have to know what the
hell's going to happen um I just have to
hell's going to happen um I just have to
make a fast Sim make sure the Sim is
make a fast Sim make sure the Sim is
right and then I mean this is just me
right and then I mean this is just me
playing around with it because it's fun
playing around with it because it's fun
um since I just built it and kind of
um since I just built it and kind of
want to get a feel for it but I'm going
want to get a feel for it but I'm going
to go run a bunch of hyper parameter
to go run a bunch of hyper parameter
sweeps on this stuff and we're going to
sweeps on this stuff and we're going to
figure out
figure out
exactly how we should be running the
exactly how we should be running the
bigger tasks like what what settings and
bigger tasks like what what settings and
stuff we're going to get way better
stuff we're going to get way better
policies out of it and then we're going
policies out of it and then we're going
to do some actual science based off of
to do some actual science based off of
that so that's going to be real
cool million steps per second training
all I guess I should emphasize just like
all I guess I should emphasize just like
how unheard of it would have
how unheard of it would have
been up to very very recently for me to
been up to very very recently for me to
be doing like hundred some odd million
be doing like hundred some odd million
step experiments live because they take
step experiments live because they take
two minutes
technically I could load like the forge
technically I could load like the forge
model with the Predator pre task or
model with the Predator pre task or
whatever but I don't think it would do
whatever but I don't think it would do
anything cool
[Laughter]
[Laughter]
did you see
did you see
that so I guess I did it backwards so I
that so I guess I did it backwards so I
guess the prey is the
guess the prey is the
red
red
um which is kind of better so the prey
um which is kind of better so the prey
they just like they run off this
they just like they run off this
way and then the Predators go wait
way and then the Predators go wait
they're escaping
oops
yeah yeah well it's an undertrained
yeah yeah well it's an undertrained
model right like undertrained models
model right like undertrained models
develop all these weird directional
biases um the one I would like to do
biases um the one I would like to do
actually there is one that would maybe
actually there is one that would maybe
be a little bit more interesting because
be a little bit more interesting because
we didn't do there's one other
we didn't do there's one other
one yeah you're going to get way better
one yeah you're going to get way better
results with longer training with better
results with longer training with better
hyper parameters just all sorts of stuff
hyper parameters just all sorts of stuff
that we can this is just like we're
that we can this is just like we're
having fun here this is just like
having fun here this is just like
getting a sense of whether I made you
getting a sense of whether I made you
know remotely reasonable environments in
know remotely reasonable environments in
the first place that kind of do
the first place that kind of do
something that looks decent before I go
something that looks decent before I go
burn some compute on sweeps it's not
burn some compute on sweeps it's not
even burning that much compute on this
even burning that much compute on this
like what 10 minutes for 500 mil
like what 10 minutes for 500 mil
something like that so I can
something like that so I can
do uh 12 experiments per
do uh 12 experiments per
hour so I mean I could do you know 100x
hour so I mean I could do you know 100x
sweep uh over these in a few hours
sweep uh over these in a few hours
whatever
whatever
it's yeah we're going to be able to do
it's yeah we're going to be able to do
lots of cool
lots of cool
experiments like I said on Twitter we're
experiments like I said on Twitter we're
going to run like a 100 billion sweep or
going to run like a 100 billion sweep or
something on this and that's going to be
something on this and that's going to be
very easily doable a million at a
very easily doable a million at a
million steps per second
train uh where is it reward group
yeah
group so this one they shouldn't run
group so this one they shouldn't run
away from each other but I wouldn't be
away from each other but I wouldn't be
surprised if they do run
surprised if they do run
away
away
um I don't know we'll
um I don't know we'll
see I don't have to know I can just run
see I don't have to know I can just run
the experiment and find out
right what are you building I am
right what are you building I am
building a an engine for continuous
building a an engine for continuous
control with uh reinforcement learning
control with uh reinforcement learning
it is you well I have some examples on
it is you well I have some examples on
Twitter here that I just posted cool to
Twitter here that I just posted cool to
see the first twitch M uh twitch chat
see the first twitch M uh twitch chat
message as well I just added that as a
message as well I just added that as a
streaming platform so I've got these
streaming platform so I've got these
tasks that look like this where each of
tasks that look like this where each of
these pixels is an agent this is
these pixels is an agent this is
predator prey with the gray ones trying
predator prey with the gray ones trying
to chase the red ones this one they're
to chase the red ones this one they're
trying to eat all the blue food this one
trying to eat all the blue food this one
they're just trying to get away from
they're just trying to get away from
each other and uh really brand new
each other and uh really brand new
project just started this in the last
project just started this in the last
couple of days but the goal is ultra
couple of days but the goal is ultra
high performance reinforcement learning
high performance reinforcement learning
and uh there's some additional
and uh there's some additional
scientific stuff here like I want to do
scientific stuff here like I want to do
continuous and discreet uh with the same
continuous and discreet uh with the same
environments this will let us uh gain
environments this will let us uh gain
some insights as to whether training
some insights as to whether training
robots is fundamentally harder than like
robots is fundamentally harder than like
training RL to play games stuff like
training RL to play games stuff like
that um but we're going to get to that
that um but we're going to get to that
over the next couple of days once this
over the next couple of days once this
gets a little bit more wellb built
out it's fun fun
stuff probably is the first follow on uh
stuff probably is the first follow on uh
on Twitch I think you can see the chat
on Twitch I think you can see the chat
overlay this is multi streamed to Twitch
overlay this is multi streamed to Twitch
YouTube and uh
YouTube and uh
Twitter so the twitch is uh entirely
Twitter so the twitch is uh entirely
new so thank you for the
interest I don't know how the twitch
interest I don't know how the twitch
page looks I just set it up real quick
page looks I just set it up real quick
the other day
and this things they take uh about 2
and this things they take uh about 2
minutes to train 125 million simulation
minutes to train 125 million simulation
steps with a little neural network and
steps with a little neural network and
little
little
lstm this one's almost
lstm this one's almost
done let's actually open up the logs on
done let's actually open up the logs on
this to
see did this train anything
reasonable yeah the reward went up
reasonable yeah the reward went up
did something it looks
did something it looks
like so actually this not so bad for
like so actually this not so bad for
first try haven't optimized anything um
first try haven't optimized anything um
the models trained for like four
the models trained for like four
different tasks that I set up I kind of
different tasks that I set up I kind of
guessed on everything and it still works
guessed on everything and it still works
at least somewhat
Works uh group. PT
think Wan has like Cloud flare or
think Wan has like Cloud flare or
something because it takes way too long
something because it takes way too long
to curl
stuff it's 12 second
timeout okay so we're going to do
R and do they do anything
what
what
uh what how do you decide a agent
uh what how do you decide a agent
architecture I will show you the agent
architecture I will show you the agent
architecture it's mainly common sense
architecture it's mainly common sense
um but it's highly optimized for Speed
um but it's highly optimized for Speed
this is this this is almost The
this is this this is almost The
Identical architecture that I use for
Identical architecture that I use for
multi snake actually we've got the uh
multi snake actually we've got the uh
the fellow still on Twitch
the fellow still on Twitch
I'll give you a reason to check out this
I'll give you a reason to check out this
content uh the snake project that I
content uh the snake project that I
built this is what something I've done
built this is what something I've done
that this took like an actual week worth
that this took like an actual week worth
of work if I can find my snake
of work if I can find my snake
project on I posted it just a few days
project on I posted it just a few days
ago
ago
a yeah Okay so we've got snake where you
a yeah Okay so we've got snake where you
have reinforcement learning with snake
have reinforcement learning with snake
but lots of snakes
and this here is is reinforcement
and this here is is reinforcement
learning with 4096 snakes lots of snakes
learning with 4096 snakes lots of snakes
do I have the local one it would look
do I have the local one it would look
way cooler if I just open the
way cooler if I just open the
local this do it this is one of the
local this do it this is one of the
gifts
gifts
nope yeah there we
go snake with 496 snakes trained with
go snake with 496 snakes trained with
reinforcement
learning so the agent architecture I
learning so the agent architecture I
came up with for that was reused for
came up with for that was reused for
this project that I that uh I'm using
this project that I that uh I'm using
now which is to answer the
question it's right here yes
question it's right here yes
architecture choice is very important
architecture choice is very important
within some some bounds of acceptable
within some some bounds of acceptable
stuff so this is two
stuff so this is two
comps comp with size five filters comp
comps comp with size five filters comp
with size three
with size three
filters um and then there is a linear
filters um and then there is a linear
layer they're in
layer they're in
between the actor and the value function
between the actor and the value function
are both uh individual just linear
are both uh individual just linear
layers very simple and the nice part
layers very simple and the nice part
about this is this is recurrent it does
about this is this is recurrent it does
have an lstm on here I believe it is a
have an lstm on here I believe it is a
128 dimensional lstm so very very small
128 dimensional lstm so very very small
policy 150,000 or so parameters um but
policy 150,000 or so parameters um but
the nice thing with it is I can train it
the nice thing with it is I can train it
at a million steps per second so I can
at a million steps per second so I can
just this thing just inhales data I
just this thing just inhales data I
trained snake on 10 billion steps of
trained snake on 10 billion steps of
course it was a pretty good policy after
course it was a pretty good policy after
like 5 minutes of training but you know
like 5 minutes of training but you know
for the sake of it I just kept training
for the sake of it I just kept training
it for 10 billion to see if it would
it for 10 billion to see if it would
learn anything else cool and it kind of
learn anything else cool and it kind of
did and um we're using basically the
did and um we're using basically the
same architecture for this right now as
well so what the heck just happened with
this I did not expect
this I did not expect
this um the task at at least I thought
this um the task at at least I thought
the task was for the ones that are the
the task was for the ones that are the
same color to group together and the
same color to group together and the
ones that are different color to spread
ones that are different color to spread
apart
apart
and instead we have created a water
and instead we have created a water
spout
thing I and the reward is going up as
thing I and the reward is going up as
well
so is it just me or they like locally
so is it just me or they like locally
assembling themselves to get
close to the same
H well it's probably the case that
H well it's probably the case that
you're that I'm really crunching
you're that I'm really crunching
learning here by
learning here by
um making them spawn right next to each
um making them spawn right next to each
other
other
huh why use lstm versus RNN versus
huh why use lstm versus RNN versus
Transformer you never use a vanilla
Transformer you never use a vanilla
recurrent neural network um it very
recurrent neural network um it very
Vanishing gradients problem very bad it
Vanishing gradients problem very bad it
it essentially has no ability to train
it essentially has no ability to train
over reasonable contexts Transformer is
over reasonable contexts Transformer is
heavier and uh very hard to implement in
heavier and uh very hard to implement in
an RL context because of the way that
an RL context because of the way that
you collect data so some people have had
you collect data so some people have had
a little bit of success with them for uh
a little bit of success with them for uh
like small scale online RL but generally
like small scale online RL but generally
the standard is uh lstm
the standard is uh lstm
here yeah they can't occupy the same
here yeah they can't occupy the same
space Lan key so the same color ones are
space Lan key so the same color ones are
blocking I'm going to make a little
tweaks yeah I'm going to make a couple
tweaks yeah I'm going to make a couple
tweaks we're going to
do times it 324 still
good did I forget lunch
good did I forget lunch
again I'm going
again I'm going
to get this experiment set up to train
to get this experiment set up to train
and then I'm going to go make my myself
and then I'm going to go make my myself
a shake for like 2 minutes keep
a shake for like 2 minutes keep
forgetting to eat damn
it man's got to
eat but I'm going to get this thing to
eat but I'm going to get this thing to
train so that it'll take two minutes to
train so that it'll take two minutes to
train and then I'll come back and I'll
train and then I'll come back and I'll
have um you know I'll have it done and
have um you know I'll have it done and
then we'll finish up the
then we'll finish up the
stream if I don't do it now I'm not
stream if I don't do it now I'm not
going to have time before my
meeting where was the gen was it spawn
positions so this is going to be every
positions so this is going to be every
fourth
position okay perfect be right back
position okay perfect be right back
going to go make my shake one
sec
e
e
e
e
e e
okay missing meals normally is bad
okay missing meals normally is bad
missing meals when you start your day
missing meals when you start your day
with two to three hours of
with two to three hours of
exercise is absolutely atrocious
let's go grab that
policy
policy
o well this is a little bit
o well this is a little bit
odd do we have a crash we had an entropy
odd do we have a crash we had an entropy
crash this might not be particularly
crash this might not be particularly
good because it looks like it was doing
good because it looks like it was doing
well and then we got the entropy crash
well and then we got the entropy crash
but I'll I'll open up the
but I'll I'll open up the
model uh anyways and then what I'm going
model uh anyways and then what I'm going
to do is while I'm opening the model I'm
to do is while I'm opening the model I'm
going to just train this for
longer so we're going to
longer so we're going to
do uh 300
do uh 300
mil while I'm doing the analysis on this
mil while I'm doing the analysis on this
one yeah I should have time for that
one yeah I should have time for that
that's
fine let's do
oh it's the same yeah
um
yeah the multi stream thing is cool
let's see if this does
anything um
what is it just me or did they oh wait
what is it just me or did they oh wait
they didn't spawn correctly because I
they didn't spawn correctly because I
didn't pull that change down
didn't pull that change down
right what you passed for the Target
right what you passed for the Target
features on unseen new data when the
features on unseen new data when the
target is
target is
unknown what do you mean the target
feature um I I don't know what you mean
feature um I I don't know what you mean
by the Target feature
uh I did what was it star star uh col
uh I did what was it star star uh col
col 4
col 4
right yeah that'll work
oh wow that's
oh wow that's
weird uh
weird uh
okay I guess that's the starting
pattern did not expect that but okay
let's see if we have a better
let's see if we have a better
model this one has been training
model this one has been training
right does the crash get
reproduced yes the crash
reproduced yes the crash
reproduces it's interesting that it does
then it starts going back
up 9
up 9
38 yeah so it's about where it is now
38 yeah so it's about where it is now
doesn't really seem to be improving much
doesn't really seem to be improving much
from here
from here
though maybe 9
though maybe 9
28 yeah it was improving back
okay oh yeah there
okay oh yeah there
goes it's a little
goes it's a little
jumpy I'm wondering if I specified this
jumpy I'm wondering if I specified this
task
correctly this was the group task
correctly this was the group task
right
right
so you get rewarded
for seeing agents of the same
color and you get Negative ly rewarded
color and you get Negative ly rewarded
for seeing agents of a different
color but the conditioning on this
color but the conditioning on this
reward is very
reward is very
weird so the probably this the probably
weird so the probably this the probably
the reason that it's so unstable is that
the reason that it's so unstable is that
the reward magnitude is too
the reward magnitude is too
big um I had to divide by 100 and it's
big um I had to divide by 100 and it's
still clipping to negative 1 and one
still clipping to negative 1 and one
very
very
often like this negative .9 is way too
often like this negative .9 is way too
big for a reward though it does seem
big for a reward though it does seem
like it has gone back up somewhat and if
like it has gone back up somewhat and if
it doesn't crash right at the end
it doesn't crash right at the end
here yeah this should
be 8 n
be 8 n
oops yeah yeah yeah this should be
oops yeah yeah yeah this should be
good to
good to
there perfect if I refresh
this oh yeah look at that perfect so
let's see what I have no idea what this
let's see what I have no idea what this
model does let's see if it learned
model does let's see if it learned
anything remotely
anything remotely
reasonable um and then I'll think about
reasonable um and then I'll think about
what I'm going to do with this next and
what I'm going to do with this next and
uh we'll go from
there quick
continuous
files copy for
okay so again I didn't mean to have them
okay so again I didn't mean to have them
start like this
start like this
but that's weird that they do
that well not exactly what I was hoping
that well not exactly what I was hoping
for
um they clearly haven't learned this one
um they clearly haven't learned this one
so
so
the other tasks may be performed
the other tasks may be performed
somewhat reasonably this one does not uh
somewhat reasonably this one does not uh
the forging one in particular leads me
the forging one in particular leads me
to believe that they are you know kind
to believe that they are you know kind
of
of
reasonable
um
yeah I think what I'm going to have to
yeah I think what I'm going to have to
do with this is I'm going to have to
do with this is I'm going to have to
fiddle with it a little bit I'm going to
fiddle with it a little bit I'm going to
have to fiddle with the different
have to fiddle with the different
Horizons I'm going to have to do a
Horizons I'm going to have to do a
couple more of these like experiments
couple more of these like experiments
type things offline and um then I'm
type things offline and um then I'm
going to have to run some hyper
going to have to run some hyper
parameter
parameter
sweeps get some reasonable
sweeps get some reasonable
defaults and then
defaults and then
hopefully that will be enough to get
hopefully that will be enough to get
some
some
visually compelling results and we can
visually compelling results and we can
do the continuous
do the continuous
versions you know just for the sake of
versions you know just for the sake of
it let's just see if anything happens to
it let's just see if anything happens to
work immediately with the uh The
work immediately with the uh The
Continuous forging one
Continuous forging one
right so I have a little bit of time
right so I have a little bit of time
that'll still give me a few minutes
that'll still give me a few minutes
before my next
meeting let's
do
do
foraging so we're going to do the
foraging so we're going to do the
foraging task CU I think that's going to
foraging task CU I think that's going to
be the most consistent one
and we're going to set the
and we're going to set the
continuous we're going to set continuous
continuous we're going to set continuous
to true so discretise be
false action
false action
spaces oh right I didn't add it to this
spaces oh right I didn't add it to this
model okay so that's going to be hard
I thought I was going to be able to do
I thought I was going to be able to do
that easily I have to change a couple
that easily I have to change a couple
things to support it in the new model
things to support it in the new model
that's fine
that's fine
um okay in that case then let's do it
um okay in that case then let's do it
something slightly different let's do
something slightly different let's do
Horizon 512 300 mil forging and let's
Horizon 512 300 mil forging and let's
see if it learns anything and I want to
see if it learns anything and I want to
fix
the I'd like to also fix the uh the the
the I'd like to also fix the uh the the
initial condition
how the heck did it happen to be
this colon colon
4 yeah this thing how this happen
H
H
-12 I think if I put it here it'll do
-12 I think if I put it here it'll do
it yeah this this should do what I want
it yeah this this should do what I want
if I put it here I
think
think
yeah let's try this
and then we'll commit everything
up for new folks here if you'd like to
up for new folks here if you'd like to
support my work all I ask is that you
support my work all I ask is that you
star the puffer repo on GitHub helps me
star the puffer repo on GitHub helps me
out a whole bunch I work on
out a whole bunch I work on
reinforcement learning stuff making
reinforcement learning stuff making
everything a lot easier and more
everything a lot easier and more
consistent and faster fulltime that
consistent and faster fulltime that
helps me quite a
bit and I also post a bunch of this
bit and I also post a bunch of this
stuff on
stuff on
Twitter under JS 5341 same
handle let's see if we've got any cool
graphs okay so the reward goes up and
graphs okay so the reward goes up and
then I think it might crash back
then I think it might crash back
down is
that
that
maybe we'll just have to wait on it for
maybe we'll just have to wait on it for
a second
I made this longer Horizon and I made
I made this longer Horizon and I made
better initial conditions uh when you
better initial conditions uh when you
spawn all the agents right next to each
spawn all the agents right next to each
other and they can't move that makes it
other and they can't move that makes it
really
hard I think I might want to change this
hard I think I might want to change this
environment up quite a bit as well
um the best the best thing you can do
um the best the best thing you can do
for learning stability with these like
for learning stability with these like
big multi-agent Sims is to make it so
big multi-agent Sims is to make it so
that the uh the agents respawn
that the uh the agents respawn
periodically so that you get uh
periodically so that you get uh
constantly diverse data like when you
constantly diverse data like when you
have a big chunk of data that's like all
have a big chunk of data that's like all
the agents at the start of the n and
the agents at the start of the n and
then all the agents in the middle all
then all the agents in the middle all
the agents at the end that destabilizes
the agents at the end that destabilizes
stuff if you can make like a nice smooth
stuff if you can make like a nice smooth
thing where you have different agents at
thing where you have different agents at
different parts all the time that makes
different parts all the time that makes
it a lot easier to
learn so like every time it resets here
learn so like every time it resets here
it screws it up if I were to just like
it screws it up if I were to just like
respawn food or whatever and just
respawn food or whatever and just
randomly spawn agents it would be a lot
randomly spawn agents it would be a lot
easier
though it is cool to see them spread out
though it is cool to see them spread out
like this
sand I'm seeing o1 reward constantly
sand I'm seeing o1 reward constantly
here how is it that this is so far
here how is it that this is so far
behind maybe it's rounding
X features
X features
equals hold on let me see I just saw
equals hold on let me see I just saw
your thing X train equals
features wait X train equals
features wait X train equals
features X test
features X test
equals features to predict has same
equals features to predict has same
shape prediction
equals okay
equals okay
so the syntax that you're
so the syntax that you're
using
using
um reminds me of like ml courses and
um reminds me of like ml courses and
stuff so I'm not sure if this is
stuff so I'm not sure if this is
intended specifically for re enforcement
intended specifically for re enforcement
or for learning or for machine learning
or for learning or for machine learning
in
in
general um but the train and test thing
general um but the train and test thing
is a little bit different here so you're
is a little bit different here so you're
interacting with an
interacting with an
environment all of your data comes from
environment all of your data comes from
this simulator this game it has a
this simulator this game it has a
specific shape the shape of the data is
specific shape the shape of the data is
in this case uh a crop from like the
in this case uh a crop from like the
pixels nearby or whatever if you want to
pixels nearby or whatever if you want to
think of it that way little 11 by 11
think of it that way little 11 by 11
window so it's technically 11 * 11 uh
window so it's technically 11 * 11 uh
numbers and when you collect that data
numbers and when you collect that data
by interacting with the environment
by interacting with the environment
you're running the lstm for one time
you're running the lstm for one time
step so you have a state you get the
step so you have a state you get the
state from the last step you feed that
state from the last step you feed that
into the LST lstm you feed the current
into the LST lstm you feed the current
observation in which is the current set
observation in which is the current set
of crops of Windows you feed that in and
of crops of Windows you feed that in and
then you get the updated state to use
then you get the updated state to use
after you've updated the simulator and
after you've updated the simulator and
then when you train this thing you
then when you train this thing you
collect all the data that you've
collect all the data that you've
uh you've gotten from the simulator and
uh you've gotten from the simulator and
you put it together into
you put it together into
sequences and then you run the lstm over
sequences and then you run the lstm over
those
those
sequences the source code for this is on
sequences the source code for this is on
puffer lib you can also see a slightly
puffer lib you can also see a slightly
simpler example example in clean
simpler example example in clean
RL but if you look at uh like clean puff
RL but if you look at uh like clean puff
RL here this is my main
RL here this is my main
transcript that I wrote for this it's
transcript that I wrote for this it's
based on clean
RL the lstm call is is
right where is
it yeah right here so the lstm calls
it yeah right here so the lstm calls
right
right
here the data shaping stuff is mostly in
here the data shaping stuff is mostly in
the experience buffer but um you know we
the experience buffer but um you know we
have the flattening of experience here
have the flattening of experience here
an experience buffer and then the
an experience buffer and then the
collection of data with the lstm and the
collection of data with the lstm and the
simulator is you run the simulator
simulator is you run the simulator
you run the policy with the lstm and
you run the policy with the lstm and
then you go back to run the simulator
then you go back to run the simulator
run the policy with the lstm because the
run the policy with the lstm because the
data comes from the simulator don't know
data comes from the simulator don't know
if that answers your
question okay so this trained something
question okay so this trained something
and the reward is pretty static but the
and the reward is pretty static but the
episode return has gone up which is
episode return has gone up which is
encouraging
let's go grab the model from
let's go grab the model from
here should have saved by now
here should have saved by now
right is there a starter Jack
lstm uh this is all in py torch this is
lstm uh this is all in py torch this is
million step per second reinforcement
million step per second reinforcement
learning in pytorch with CPU
learning in pytorch with CPU
environments so not entirely sure with
environments so not entirely sure with
the Jack
stuff I kind of did write a uh big
stuff I kind of did write a uh big
granty post the other day about Jack's
granty post the other day about Jack's
pissing me off on uh Twitter whever that
pissing me off on uh Twitter whever that
went right
went right
freaking where is this rant
post is there
post is there
articles yeah
articles yeah
here so this is on Twitter and a lot of
here so this is on Twitter and a lot of
this is annoying Jack stuff so
this is annoying Jack stuff so
probably not the person to ask about uh
probably not the person to ask about uh
you know what tools are there in Jack I
you know what tools are there in Jack I
know um if you are using Jack though my
know um if you are using Jack though my
suggested implementation is the clean RL
suggested implementation is the clean RL
Jax
Jax
versions those are pretty
nice where was that model file it should
nice where was that model file it should
have been done by now
have been done by now
right here we go
what's the distinction with CPU
what's the distinction with CPU
environment does this mean it is one
environment does this mean it is one
environment per CPU it could be multiple
environment per CPU it could be multiple
environments per CPU the distinction is
environments per CPU the distinction is
that the simulation itself occurs on the
that the simulation itself occurs on the
CPU so I'm not uh the Sim itself is on
CPU so I'm not uh the Sim itself is on
uh CPU you could have multiple Sims per
uh CPU you could have multiple Sims per
core you can have multiple agents per
core you can have multiple agents per
Sim multiple agents Pere whatever you
Sim multiple agents Pere whatever you
want puffer lib gives you very very fast
want puffer lib gives you very very fast
factorization um but the difference in
factorization um but the difference in
the way that I'm approaching
the way that I'm approaching
environments is instead of writing
environments is instead of writing
domain specific language stuff in Jacks
domain specific language stuff in Jacks
where you have you're very constrained
where you have you're very constrained
in what you can write by the Jax
in what you can write by the Jax
language uh I'm just writing arbitrary
language uh I'm just writing arbitrary
environments in scon which are still
environments in scon which are still
really fast millions of steps per second
really fast millions of steps per second
and then I have good vectorization code
and then I have good vectorization code
to get all that data onto the GPU
to get all that data onto the GPU
relatively
relatively
efficiently very very flexible
efficiently very very flexible
approach and I this is the last thing
approach and I this is the last thing
that I've got time to do today because I
that I've got time to do today because I
have a meeting in 10 got to finish the
have a meeting in 10 got to finish the
shake got to do a couple
things what happened with the the
things what happened with the the
URL
oh all this code by the way is open
oh all this code by the way is open
source in puffer lib so you're free to
source in puffer lib so you're free to
look at the way it works there's like um
look at the way it works there's like um
this project is still very new but there
this project is still very new but there
is a massively multi-agent snake
is a massively multi-agent snake
environment that's a really fast and a
environment that's a really fast and a
really good demo of this Tech and I have
really good demo of this Tech and I have
a lot more stuff releasing very soon as
a lot more stuff releasing very soon as
well training stuff as we speak for
well training stuff as we speak for
it um I wanted to change the RO that
it um I wanted to change the RO that
length didn't
length didn't
I no it's it's good
ah I
ah I
the well the spawn is wrong
the well the spawn is wrong
but let me fix the spawn because that
but let me fix the spawn because that
might actually bias
might actually bias
it come
on so this colon colon 4 has to go uh on
here Jack's limited because environment
here Jack's limited because environment
complexity is limited due to GPU like
complexity is limited due to GPU like
branching yes precisely uh not only is
branching yes precisely uh not only is
the GPU restrictive in what it's good at
the GPU restrictive in what it's good at
Computing with branching but you have to
Computing with branching but you have to
think of you essentially have to solve a
think of you essentially have to solve a
clever puzzle every time you want to
clever puzzle every time you want to
write something because you have to
write something because you have to
twist your head around how to write
twist your head around how to write
everything into array operations whereas
everything into array operations whereas
in syon you can just write arbitrary
in syon you can just write arbitrary
code yeah your state is usually still
code yeah your state is usually still
going to be in arrays but some of it can
going to be in arrays but some of it can
be in struct some of it can be in other
be in struct some of it can be in other
things and you can write arbitrary logic
things and you can write arbitrary logic
over it you can write Loops you can
over it you can write Loops you can
write conditionals you don't have have
write conditionals you don't have have
to like do everything in this weird
to like do everything in this weird
functional domain specific
functional domain specific
language let me do that one more time
here okay so this is a little bit better
here okay so this is a little bit better
actually we have some agents that go in
actually we have some agents that go in
all directions it's a little bit less
all directions it's a little bit less
directionally biased I think we'd have
directionally biased I think we'd have
to train it for longer to get it fully
to train it for longer to get it fully
unbiased uh useful info but I'm just
unbiased uh useful info but I'm just
selftaught trying to learn lstm models
selftaught trying to learn lstm models
to shamelessly predict things like
to shamelessly predict things like
stocks you're going to lose a lot of
stocks you're going to lose a lot of
money doing that uh so the way your
money doing that uh so the way your
sequence is prob different but I'm
sequence is prob different but I'm
looking for overl yeah so this is not um
looking for overl yeah so this is not um
classic machine learning this is not
classic machine learning this is not
like train set test set this is your
like train set test set this is your
data comes from a simulator and anytime
data comes from a simulator and anytime
you want new data you have to give
you want new data you have to give
actions to the simulator like you have
actions to the simulator like you have
to press the buttons on the keyboard to
to press the buttons on the keyboard to
determine what happens next right it's
determine what happens next right it's
like playing a game so the data setup is
like playing a game so the data setup is
very different the infrastructure setup
very different the infrastructure setup
is very different yes technically the
is very different yes technically the
data has to go into an lstm and the way
data has to go into an lstm and the way
that you're going to do that is going to
that you're going to do that is going to
be roughly the same for class machine
be roughly the same for class machine
learning or for reinforcement learning
learning or for reinforcement learning
but uh it's there a lot more moving
but uh it's there a lot more moving
Parts
here okay
here okay
so decent little uh decent little
so decent little uh decent little
progress for
progress for
today we
today we
did let's actually commit all this code
did let's actually commit all this code
up
so that you guys can look at it if you
so that you guys can look at it if you
want
why is Jax become so popular in RL not
why is Jax become so popular in RL not
pytorch because in order to get uh CPU
pytorch because in order to get uh CPU
Sims to run fast you have to do some
Sims to run fast you have to do some
vectorization work and other things that
vectorization work and other things that
nobody bothered to do I did it it is now
nobody bothered to do I did it it is now
in puffer lib this is very recent this
in puffer lib this is very recent this
is been released within the last couple
is been released within the last couple
of months it is incredibly efficient and
of months it is incredibly efficient and
it makes reinforcement learning so so
it makes reinforcement learning so so
much simpler and this is also what I'm
much simpler and this is also what I'm
working on fulltime now so this is just
working on fulltime now so this is just
the beginning
the beginning
um I genuinely think that reinforcement
um I genuinely think that reinforcement
learning is going to be a stable
learning is going to be a stable
consistent easy to work in field within
consistent easy to work in field within
a few months directly because of puffer
a few months directly because of puffer
lib and because all of these simulators
lib and because all of these simulators
I'm building like the default in
I'm building like the default in
reinforcement learning is going to be
reinforcement learning is going to be
you train it hundreds of thousands to a
you train it hundreds of thousands to a
million steps per second you have access
million steps per second you have access
to a variety of simulators of various
to a variety of simulators of various
complexity for uh all of your research
complexity for uh all of your research
needs some of which are more complex
needs some of which are more complex
than some of the most complex simulators
than some of the most complex simulators
out there now you have access to large
out there now you have access to large
scale hyperparameter sweeps that run
scale hyperparameter sweeps that run
overnight on a single GPU and you have
overnight on a single GPU and you have
access to Quality infrastructure backing
access to Quality infrastructure backing
all of this all of this code is
all of this all of this code is
incredibly simple it's like a few
incredibly simple it's like a few
thousand Total Lines there are not 500
thousand Total Lines there are not 500
useless abstractions it's going to look
useless abstractions it's going to look
like clean RL it's going to have the
like clean RL it's going to have the
efficiency of sample sample Factory or
efficiency of sample sample Factory or
higher this is where RL is
higher this is where RL is
going take some work to get there though
going take some work to get there though
and with that uh I have got to go to a
and with that uh I have got to go to a
meeting in a couple minutes so I'm gonna
meeting in a couple minutes so I'm gonna
sign off uh I will probably be back
sign off uh I will probably be back
streaming more Dev on this
streaming more Dev on this
tomorrow and uh yeah I've been I've been
tomorrow and uh yeah I've been I've been
enjoying this so I'm just going to keep
enjoying this so I'm just going to keep
going with it and uh hopefully we'll get
going with it and uh hopefully we'll get
some better policies as well as uh we'll
some better policies as well as uh we'll
start doing some continuous control
start doing some continuous control
versus discreet experiments tomorrow see
versus discreet experiments tomorrow see
you all and uh star the puffer if you
you all and uh star the puffer if you
haven't already helps me out a whole ton
haven't already helps me out a whole ton
bye

Kind: captions
Language: en
sat down this morning did a little bit
sat down this morning did a little bit
of little bit of work fix some of the
of little bit of work fix some of the
puffer box that were down
puffer box that were down
um and then I realized
um and then I realized
that uh I wasn't really feeling like
that uh I wasn't really feeling like
doing some of the low-level nitty-gritty
doing some of the low-level nitty-gritty
stuff uh that I was going to have to do
stuff uh that I was going to have to do
so I figured I'd just flip things around
so I figured I'd just flip things around
and work a little bit more on this now I
and work a little bit more on this now I
thought of a way to get around the uh
thought of a way to get around the uh
annoying model loading thing that I was
annoying model loading thing that I was
looking at yesterday will require some
looking at yesterday will require some
kind of substantial changes to puffer
kind of substantial changes to puffer
lib so it's going to take a few days
lib so it's going to take a few days
before all this gets merged
before all this gets merged
into I can probably merge it into Dev
into I can probably merge it into Dev
but before it gets merged into main
but before it gets merged into main
will'll be a little
will'll be a little
bit um but yeah let's just start from
bit um but yeah let's just start from
there uh I found a
okay Chris you can fight me on Twitter
okay Chris you can fight me on Twitter
as much as you want but this is just
as much as you want but this is just
this is just wrong this is because the
this is just wrong this is because the
baselines are horribly
baselines are horribly
tuned
um you do good work though I will give
um you do good work though I will give
you
that where's the thing that I posted cuz
that where's the thing that I posted cuz
I found a nice way of putting it this is
I found a nice way of putting it this is
what I want to answer with this project
what I want to answer with this project
object is continuous control
object is continuous control
fundamentally harder to learn than a
fundamentally harder to learn than a
discret action
discret action
space and I think that the way to answer
space and I think that the way to answer
this problem is to sort of separate
this problem is to sort of separate
continuous control from what people
continuous control from what people
think of as you know as continuous
think of as you know as continuous
control being
control being
robotics so just fundamentally if you
robotics so just fundamentally if you
make the action space continuous does
make the action space continuous does
that make the problem harder because
that make the problem harder because
like are robots hard to are robots hard
like are robots hard to are robots hard
to deal with is a different problem from
to deal with is a different problem from
uh is continuous control
uh is continuous control
hard so this is kind of like the the
hard so this is kind of like the the
essence of the problem and if these M's
essence of the problem and if these M's
can be made hyperf enough then we're
can be made hyperf enough then we're
going to just be able to exhaustively
going to just be able to exhaustively
run everything uh run everything against
run everything uh run everything against
this and it will work give quick a Chris
this and it will work give quick a Chris
reply on this and then start
parallel for
whatever let's go back to
whatever let's go back to
this so the thing I realized is that uh
this so the thing I realized is that uh
we need a way to support multiple
we need a way to support multiple
different policies within each um each
different policies within each um each
environment or Suite because like ocean
environment or Suite because like ocean
for instance puffer environments we want
for instance puffer environments we want
to have different policies for the
to have different policies for the
different environments in there and
different environments in there and
conceivably you could want to have
conceivably you could want to have
different architectures for um different
different architectures for um different
environments in other environment Suites
environments in other environment Suites
as well so what we're going to
do go into
do go into
puffer and we're going to add
puffer and we're going to add
a uh this policy here uh argument we
a uh this policy here uh argument we
need to add like a policy name or
need to add like a policy name or
whatever policy
whatever policy
name and this is just going to be uh
policy and we can
policy and we can
do
do
uh this can be RNN
name
name
oops none
oops none
or actually it's just this and then
or actually it's just this and then
we're going to have to adjust some
we're going to have to adjust some
annoying configurations in order to make
annoying configurations in order to make
all of this work but the idea is going
all of this work but the idea is going
to be that we're going to have uh
to be that we're going to have uh
multiple different policy names
multiple different policy names
specified
specified
here so if I go into where is it o
here so if I go into where is it o
ocean these ones will have the
ocean these ones will have the
default um but then grid continuous will
default um but then grid continuous will
have
our that name is recurrent we'll have
our that name is recurrent we'll have
policy name is going to be
grid and then instead of having these
grid and then instead of having these
all named
all named
policy where is it this one will be
policy where is it this one will be
policy the default
policy the default
model okay and then this one will be
model okay and then this one will be
uh this one's
snake and this one is
grid and in order to fix this hold on
grid and in order to fix this hold on
let me check the stream real
let me check the stream real
quick cool yeah this works so in order
quick cool yeah this works so in order
to fix
to fix
this uh we're going to have to just
this uh we're going to have to just
modify the way that the config loader
modify the way that the config loader
Works a little
Works a little
bit which is I hate having to edit this
bit which is I hate having to edit this
thing I really do um I wish there were a
thing I really do um I wish there were a
simpler way to do this
simpler way to do this
but this is like a multi-stage config
but this is like a multi-stage config
loader and this is as simple as I could
loader and this is as simple as I could
get it and it's still a pain to edit um
get it and it's still a pain to edit um
so we're going to do policy
name and where did they get RNN arcs
yeah so policy name
yeah so policy name
equals not big de yet policy
name policy name
none well no it has to have this so it's
none well no it has to have this so it's
just going to be
n how do we how did we do the uh the
n how do we how did we do the uh the
override thing before
end of
policy
policy
uh so obnoxious I'm trying to do like
uh so obnoxious I'm trying to do like
this cascading inherited thing I I don't
this cascading inherited thing I I don't
want to go rewrite the whole config now
want to go rewrite the whole config now
I've been tempted to for a while but
I've been tempted to for a while but
then that's going to be like a 5H hour
then that's going to be like a 5H hour
Rabbit Hole uh and I don't want to do
Rabbit Hole uh and I don't want to do
that
that
so for now
so for now
we will
do isn't there one that we did this
already yeah right like this this type
already yeah right like this this type
of a thing so we're just going to add
of a thing so we're just going to add
this as a hack for now and um we'll go
this as a hack for now and um we'll go
from here so this is going to be policy
name policy name
okay so now we have the policy
name and then we have uh policy what is
name and then we have uh policy what is
it policy class class
it policy class class
equals get Adder
policy
class
name we're actually going to be able to
name we're actually going to be able to
get rid of the use RNN thing so that's
get rid of the use RNN thing so that's
kind of
kind of
nice RNN name RNN class RN
args we'll get rid of just use
RNN oh wait shoot we have to
RNN oh wait shoot we have to
add do we have to add stuff
add do we have to add stuff
here we use
RNN yes we
do all right
hopefully that's
good let's just start running stuff and
good let's just start running stuff and
debugging that way
okay has no attribute
okay has no attribute
grid uh get Adder end module policy
name do
torch okay invalid argument CNN channels
torch okay invalid argument CNN channels
to
default that is
default that is
reasonable do we actually have that in
reasonable do we actually have that in
there that shouldn't be
there that shouldn't be
there oh wait no because we
there oh wait no because we
have what's policy
name what is policy
name it's grid
name it's grid
unrelated but is loss of network
unrelated but is loss of network
plasticity something that has cropped up
plasticity something that has cropped up
as an
as an
issue it's hard to
issue it's hard to
say and it depends what you mean yes
say and it depends what you mean yes
there's been some evidence of that in
there's been some evidence of that in
reinforcement learning in general
reinforcement learning in general
there's this thing called rerun that
there's this thing called rerun that
I've been wanting to try out for a while
I've been wanting to try out for a while
now that's like that some of the I have
now that's like that some of the I have
this plan that's like make everything
this plan that's like make everything
fast make a bunch of good environments
fast make a bunch of good environments
and then be able to run lots of fast
and then be able to run lots of fast
experiments to determine stuff like that
experiments to determine stuff like that
conclusively rather than the thing what
conclusively rather than the thing what
that is happening now where like you run
that is happening now where like you run
a very small number of experiments and
a very small number of experiments and
get very weak evidence for something um
get very weak evidence for something um
but there are things like rerun that is
but there are things like rerun that is
something that is potentially cropped up
something that is potentially cropped up
and uh it definitely I can get I can
and uh it definitely I can get I can
tell you one place for sure that it is
tell you one place for sure that it is
an issue let's say that you have an
an issue let's say that you have an
environment where you have a big action
environment where you have a big action
space and you don't need half of it
space and you don't need half of it
until later on in the game like maybe
until later on in the game like maybe
you can use a bow right there's an
you can use a bow right there's an
action space portion that's for a bow
action space portion that's for a bow
but you don't get bow until halfway
but you don't get bow until halfway
through the game well by the time you
through the game well by the time you
get the bow the model is going to have
get the bow the model is going to have
learned to never take those actions so
learned to never take those actions so
that type of a thing is absolutely a
that type of a thing is absolutely a
problem um more generally we'll have to
problem um more generally we'll have to
wait for the experiments check out the
wait for the experiments check out the
rerun
paper was definitely a good idea to set
paper was definitely a good idea to set
the multi streaming on this to uh to
the multi streaming on this to uh to
YouTube as well since it seems like
YouTube as well since it seems like
folks do actually see it from there as
folks do actually see it from there as
well so that's
cool goal for today is to fix up a bunch
cool goal for today is to fix up a bunch
of the mess that I made with these
of the mess that I made with these
multiple policies and then start on
multiple policies and then start on
getting like actual training done with
getting like actual training done with
uh some of these grid environments I
uh some of these grid environments I
have a spare puffer box reserved
have a spare puffer box reserved
specifically for this purpose today I
specifically for this purpose today I
have one of them running a 10 billion
have one of them running a 10 billion
experiment on a fun environment that I'm
experiment on a fun environment that I'm
working on and one of them reserved for
working on and one of them reserved for
Dev
today so I think it's just
um oh that is you linky I thought that
um oh that is you linky I thought that
was from
was from
yesterday fair
yesterday fair
enough I think the video quality is
enough I think the video quality is
actually better on YouTube than on X as
actually better on YouTube than on X as
well because I'm streaming at 1080p but
well because I'm streaming at 1080p but
X goes uh only up to
720 but most of my audience is on is on
720 but most of my audience is on is on
X
X
right though I actually I guess with the
right though I actually I guess with the
uh the thesis video I did maybe not so
uh the thesis video I did maybe not so
much um I got to stop starting all these
much um I got to stop starting all these
new projects because I have promised a
new projects because I have promised a
high quality video on puffer 10 um I
high quality video on puffer 10 um I
have a really cool idea for a video and
have a really cool idea for a video and
it's it's just going to take me like
it's it's just going to take me like
several days to do
several days to do
it yeah I know you were here yesterday
it yeah I know you were here yesterday
well I mean this will be good for you to
well I mean this will be good for you to
have on in the background because you'll
have on in the background because you'll
get uh you know if you're looking for
get uh you know if you're looking for
Dev stuff this will absolutely teach you
Dev stuff this will absolutely teach you
a few
a few
things uh link is one of our
things uh link is one of our
contributors on uh the Pokemon Red
contributors on uh the Pokemon Red
project he's done awesome over there
I hate this code right here so
much but if the code does not bring you
much but if the code does not bring you
pain you never go make it
pain you never go make it
better um where's this make policy
function wait why am I returning
function wait why am I returning
make why am I returning make policy that
make why am I returning make policy that
makes absolutely no
makes absolutely no
sense I'm sure that this was like
sense I'm sure that this was like
leftover from something else right this
leftover from something else right this
should be
should be
um what
um what
policy no I don't need I because I have
policy no I don't need I because I have
it in config now
it in config now
right yeah so let's not touch this for
right yeah so let's not touch this for
now even though it's stupid just so I
now even though it's stupid just so I
don't break stuff uh and then this is
don't break stuff uh and then this is
going to
be this is args of I think this is like
be this is args of I think this is like
args of policy class
args of policy class
right yeah and then this
right yeah and then this
is
is
args here so if args
args here so if args
of RNN class is not none yeah there you
of RNN class is not none yeah there you
go thanks super
Maven invalid
arguments one
102 ah right here this
102 ah right here this
is validate args is going to
is validate args is going to
be end
class or no policy class what is it
class or no policy class what is it
policy class yeah
did I not add this to the
did I not add this to the
ARs what do I call this
ARs what do I call this
policy policy name
policy policy name
ooh so I do actually need to return this
ooh so I do actually need to return this
that's
that's
obnoxious okay
we'll do
we'll do
policy policy class and RNN class we'll
policy policy class and RNN class we'll
just say this is going to be EnV en
just say this is going to be EnV en
module policy class RN class and then
module policy class RN class and then
this will be
this will be
if policy class super Maven will
if policy class super Maven will
probably get the
probably get the
idea
idea
yeah wait
yeah wait
what no it did not get this whatsoever
what no it did not get this whatsoever
did
did
it that just was uh
bad so we do policy is
bad so we do policy is
equaly isal to policy
equaly isal to policy
class
class
this if RNN class is not none then we
this if RNN class is not none then we
make it an
RNN
RNN
else yeah there we go this is what we
else yeah there we go this is what we
want uh and we do not need end module
want uh and we do not need end module
here anymore
here anymore
right don't need end
right don't need end
module so we do let's go get the
module so we do let's go get the
signature for load
config policy class RN
config policy class RN
class and
then I guess all this junk has to go
then I guess all this junk has to go
into train
into train
right make
right make
n policy class
policy class and RN
class I really hate this demo file
class I really hate this demo file
everybody hates this demo file to be
everybody hates this demo file to be
fair but so far nobody has given me a
fair but so far nobody has given me a
better like a better way to do this file
better like a better way to do this file
I've tried very hard to make this file
I've tried very hard to make this file
reasonable just nobody's come up with a
reasonable just nobody's come up with a
good way of doing it
of policy class RNN class args
policy
class
so you're going to be very dis appointed
so you're going to be very dis appointed
by my Vim
config
config
um that's such a bait question oh my
um that's such a bait question oh my
gosh I it is on here I think
right yeah so I think that I just forgot
right yeah so I think that I just forgot
to
to
change oh no yeah this is updated
change oh no yeah this is updated
um I used Vim with no plugin nothing for
um I used Vim with no plugin nothing for
like six years now I have two plugins I
like six years now I have two plugins I
have super Maven and I have SEI for some
have super Maven and I have SEI for some
extra syntax highlights I don't really
extra syntax highlights I don't really
have any extra commands or anything on
have any extra commands or anything on
here and then all of this is just my uh
here and then all of this is just my uh
the color scheme that I made for
the color scheme that I made for
this that's literally it and as it turns
this that's literally it and as it turns
out we actually fixed training while I
out we actually fixed training while I
was talking that talking about that but
was talking that talking about that but
yeah puffer tank comes with my uh my Vim
yeah puffer tank comes with my uh my Vim
config and neim set up in it just
config and neim set up in it just
cuz
convenient okay so this is nice that
convenient okay so this is nice that
this actually
this actually
trains let's make sure um eval works
does this
does this
work make
policy no because it needs
the policy class and RNN class right
so ar. M policy
class I've always thought you're kind of
class I've always thought you're kind of
missing the point if you spend a whole
missing the point if you spend a whole
bunch of
bunch of
time like hyper fixating on
time like hyper fixating on
your on like making a very complicated
your on like making a very complicated
editor cont fig or like system setup or
something on the flip side you know
something on the flip side you know
buying a Mac and just using it with a
buying a Mac and just using it with a
default is also probably just not giving
default is also probably just not giving
enough fcks
see class class yeah there we
see class class yeah there we
go I want to make sure that eval and
go I want to make sure that eval and
train
work how's it missing two keyword ARS or
work how's it missing two keyword ARS or
positional ARS I just added
them oh cuz I added it to this stupid
them oh cuz I added it to this stupid
Pokemon function that shouldn't even be
Pokemon function that shouldn't even be
here
do we still not do we still need this in
do we still not do we still need this in
here linky is there still like a custom
here linky is there still like a custom
roll out thing or is there some way that
roll out thing or is there some way that
I can get this out of the main
file sick of having like custom
file sick of having like custom
environment stuff in the demo
file I'm really tempted to just
file I'm really tempted to just
like move it okay I'm really tempted to
like move it okay I'm really tempted to
just give you guys um cuz like you guys
just give you guys um cuz like you guys
hate the demo file too I know like why
hate the demo file too I know like why
don't we just give you guys like a
don't we just give you guys like a
simple demo script like one of these
simple demo script like one of these
this one's designed to be edited you
this one's designed to be edited you
know the real demo script isn't designed
know the real demo script isn't designed
to be edited as often this one is like
to be edited as often this one is like
you hardcode whatever single environment
you hardcode whatever single environment
you want into it it's like 100 line
you want into it it's like 100 line
shorter it doesn't use it doesn't use
shorter it doesn't use it doesn't use
the configs something like
that oh well
that would make
sense love deleting code that is not
required and
module make
module make
policy got unexpected
M policy class R and N class arcs
M policy class R and N class arcs
right well this isn't supposed to be
right well this isn't supposed to be
star star ARS this is just supposed to
star star ARS this is just supposed to
be
be
args isn't
it like
this dick object has no attribute
policy what
does actually make any sense
whatsoever stop policy
and module
ARS you just need this let actually get
simpler very nice
it is a good day when your code
it is a good day when your code
works and chocolate chip cookies are a
works and chocolate chip cookies are a
legitimate part of your
diet for
I think we can Commit This to Dev
I think we can Commit This to Dev
right let me make sure I didn't break
right let me make sure I didn't break
snake cuz there are actually people
snake cuz there are actually people
using uh I know there are people who
using uh I know there are people who
actually want to use the snake
en yeah let's just fix this nen
en yeah let's just fix this nen
everything else can be broken in Dev for
everything else can be broken in Dev for
now I don't care but the snake en needs
now I don't care but the snake en needs
to be working
well that
well that
works here's your snake
works here's your snake
end you can play it it's very
end you can play it it's very
nice it's actually kind of
fun make sure train
fun make sure train
Works joined a little late what are you
Works joined a little late what are you
up to now what did you decide to do for
up to now what did you decide to do for
the continuous control engine
the continuous control engine
discretization yes uh I'm literally just
discretization yes uh I'm literally just
about to do that stuff so it ended up
about to do that stuff so it ended up
being way simpler than I thought um
being way simpler than I thought um
literally all you have to do is
literally all you have to do is
discretize uh the the state into a grid
discretize uh the the state into a grid
for the purpose of of computing
for the purpose of of computing
observations that's all you have to
observations that's all you have to
do um and then you have an identical
do um and then you have an identical
environment for discreet and continuous
environment for discreet and continuous
action
action
spaces we're actually about to talk
spaces we're actually about to talk
about that let me commit all this stuff
about that let me commit all this stuff
up and then we're going to go through
up and then we're going to go through
some stuff
pu RL config
demo
environments is ocean meant to be overly
environments is ocean meant to be overly
simplified environments that can be all
simplified environments that can be all
trained side by side wondering if it can
trained side by side wondering if it can
be used for generalized training
be used for generalized training
approach as an entrance to curricul
approach as an entrance to curricul
learning or with neural MMO be better um
learning or with neural MMO be better um
ocean is just that's what it started as
ocean is just that's what it started as
but ocean is now just the blanket repo
but ocean is now just the blanket repo
for or the B the blanket term for puffer
for or the B the blanket term for puffer
Liv's first party environments so
Liv's first party environments so
anything that I have made or I have you
anything that I have made or I have you
know personally done for puffer
know personally done for puffer
lib
lib
um the classic ocean tasks which we're
um the classic ocean tasks which we're
going to have to like find some sub
going to have to like find some sub
package name for or whatever are probe
package name for or whatever are probe
tasks that are designed to test very
tasks that are designed to test very
specific deficiencies in uh
specific deficiencies in uh
implementations they're meant as quick
implementations they're meant as quick
sanity checks they train in 10 seconds
sanity checks they train in 10 seconds
the snake environment and neural MMO and
the snake environment and neural MMO and
like other things that are going to be
like other things that are going to be
in
in
Ocean are uh well they're pretty much
Ocean are uh well they're pretty much
anything that I've done for it um the
anything that I've done for it um the
general themes are ultra high
general themes are ultra high
performance simulation variety of
performance simulation variety of
different complexities often massively
different complexities often massively
multi-agent cuz that's my main Jam but
multi-agent cuz that's my main Jam but
not necessarily
not necessarily
yeah it doesn't really derail me much
like actually kind of keeps me on track
like actually kind of keeps me on track
when
when
I go interact with the
I go interact with the
chat
chat
um let me commit all this stuff up
welcome red
ey I've always done my my commits
ey I've always done my my commits
manually like this just because I want
manually like this just because I want
to know exactly what I am uh what I am
to know exactly what I am uh what I am
messing
with like in here I can see that there
is
is
D.P
D.P
IX PNG
and that's all I
need I'm just committing up some files
need I'm just committing up some files
you're going to see I actually I have a
you're going to see I actually I have a
cool segment coming up just in a second
cool segment coming up just in a second
here let me just push
here let me just push
this latest Dev
this latest Dev
Massi break some Ms snake
works okay so now what we're going to do
works okay so now what we're going to do
is I'm going to go back to The
is I'm going to go back to The
Continuous control engine I'm going to
Continuous control engine I'm going to
talk about what I want to do with it
talk about what I want to do with it
we're going to decide on some tasks to
we're going to decide on some tasks to
implement hopefully that's going to be
implement hopefully that's going to be
easier than yesterday since I'm fresh
relatively
so
so
yeah the guy doing everything in Vim is
yeah the guy doing everything in Vim is
not using get desktop I'm stubborn
so we had this original environment
so we had this original environment
that's discreet uh a discret environment
that's discreet uh a discret environment
that supports multi-agent particles like
that supports multi-agent particles like
every environment is uh every agent is a
every environment is uh every agent is a
particle it can move around you can
particle it can move around you can
instruct them to do different things via
instruct them to do different things via
the reward signal um there is a little
the reward signal um there is a little
bit of infrastructure behind doing this
bit of infrastructure behind doing this
incredibly quickly so it simulates
incredibly quickly so it simulates
multiple millions of steps per second
multiple millions of steps per second
the general idea is that you can either
the general idea is that you can either
just simulate a 3D grid and then every
just simulate a 3D grid and then every
property you need of an environment can
property you need of an environment can
live on a 3D Grid or if you want to be a
live on a 3D Grid or if you want to be a
little fancier and a little bit more
little fancier and a little bit more
efficient you just store a grid of
efficient you just store a grid of
player IDs where each ID is an integer
player IDs where each ID is an integer
that tells you what agent it controls
that tells you what agent it controls
and then you index use this to index
and then you index use this to index
into a a struct essentially of different
into a a struct essentially of different
players so you know element zero is
players so you know element zero is
player one and it has various different
player one and it has various different
properties etc
properties etc
etc fairly simple you don't really need
etc fairly simple you don't really need
this cuz this environment is so simple
this cuz this environment is so simple
but whatever I wanted to then make this
but whatever I wanted to then make this
environment
environment
continuous so I thought about different
continuous so I thought about different
ways to do this and I came to the
ways to do this and I came to the
conclusion that it's actually really
conclusion that it's actually really
easy and all you have to do is you allow
easy and all you have to do is you allow
the agents to move a uh continuous non-
the agents to move a uh continuous non-
integer value so they can move however
integer value so they can move however
much they want you store their position
much they want you store their position
as a float but then when you write to
as a float but then when you write to
the observations you discretize to
the observations you discretize to
whatever the resolution is of the image
whatever the resolution is of the image
so you have continuous actions
so you have continuous actions
continuous movement you could even do uh
continuous movement you could even do uh
acceleration and stuff like that if you
acceleration and stuff like that if you
want to though I didn't add that yet but
want to though I didn't add that yet but
uh you get the same exact observation
uh you get the same exact observation
space so this produces tasks that are
space so this produces tasks that are
one to one the same between the
one to one the same between the
continuous and the discret case which
continuous and the discret case which
means that if we have several of these
means that if we have several of these
tasks that are kind of reasonable and we
tasks that are kind of reasonable and we
can train discret agents we can train
can train discret agents we can train
continuous agents we can see what
continuous agents we can see what
whether our continuous learning setup
whether our continuous learning setup
works as well as our discret learning
works as well as our discret learning
setup which will be a huge Boon to
setup which will be a huge Boon to
continuous control people that have
continuous control people that have
problems with janky
algorithms as well as being a very nice
algorithms as well as being a very nice
test case a way to show that puffer is
test case a way to show that puffer is
provably working on these types of
provably working on these types of
environments as well uh I started this
environments as well uh I started this
doc where we started looking at
doc where we started looking at
different
tasks I think some of these are good
tasks I think some of these are good
some of these are not so good
some of these are not so good
good so these two are classic problems
good so these two are classic problems
that have been used for a long time in
that have been used for a long time in
multi-agent sometimes they're posed in
multi-agent sometimes they're posed in
weird ways but I think some variation of
weird ways but I think some variation of
foraging where you have to spread out
foraging where you have to spread out
and eat food and some version of
and eat food and some version of
Predator prey where you want some agents
Predator prey where you want some agents
to chase others these are
to chase others these are
good this one was a little bit more
good this one was a little bit more
questionable I'll think about it
questionable I'll think about it
um grouping up so so you have several
um grouping up so so you have several
different types of agents that have to
different types of agents that have to
group up with their own groups and stay
group up with their own groups and stay
away from other groups uh that is
away from other groups uh that is
slightly different from the Predator
slightly different from the Predator
prey one this one is purely Cooperative
prey one this one is purely Cooperative
it's kind of interesting in that
it's kind of interesting in that
sense and then I felt like doing a promo
sense and then I felt like doing a promo
task in which we just the agents have to
task in which we just the agents have to
assemble to spell out the words puff or
assemble to spell out the words puff or
Li I thought that would be fun and uh
Li I thought that would be fun and uh
it's a decent little uh test case as
it's a decent little uh test case as
well so that's four
well so that's four
tasks I don't know
tasks I don't know
let me think if I want to do this
let me think if I want to do this
optimized task how I would do
optimized task how I would do
it it might not be that
it it might not be that
hard if I make the food continuous
hard if I make the food continuous
rather than discreet the food is just a
rather than discreet the food is just a
continuous amount that would
continuous amount that would
work jungle basketball from Eric Jang
[Music]
[Music]
nope
uh robotics guy oh this one oh oh yeah
uh robotics guy oh this one oh oh yeah
yeah the Olympics I saw this thing
yeah the Olympics I saw this thing
yesterday this thing here
right is this this guy's work or is this
right is this this guy's work or is this
just something he's reposting though
just something he's reposting though
because I saw this online
yeah it's cool we could potentially add
yeah it's cool we could potentially add
a binding for it do they have a do they
a binding for it do they have a do they
tell you how fast it
is man people really got to say how fast
is man people really got to say how fast
stuff is initial code release code
stuff is initial code release code
trainable repo under construction so
trainable repo under construction so
we'll give this a little bit of time to
we'll give this a little bit of time to
cook uh if it's basically I'm going to
cook uh if it's basically I'm going to
add bindings for anything that is
add bindings for anything that is
reasonable enough to work with and is
reasonable enough to work with and is
fast
let's see
this is not true
okay whatever
Chris let's go look at this
welcome I can't quite see the
welcome I can't quite see the
name yanero if I'm pronouncing that
name yanero if I'm pronouncing that
welcome let's get into some actual Dev
welcome let's get into some actual Dev
instead of just freaking scrolling
instead of just freaking scrolling
Twitter
Twitter
endlessly the thing with Chris is
endlessly the thing with Chris is
somewhat annoying because like Chris is
somewhat annoying because like Chris is
somebody who I actually I respect a lot
somebody who I actually I respect a lot
and does very good work um
and does very good work um
I disagree with him on Jax but I think
I disagree with him on Jax but I think
he's mostly just mad that the article
he's mostly just mad that the article
was written in a sort of inflammatory
was written in a sort of inflammatory
way um I actually I held back a lot on
way um I actually I held back a lot on
the way that I would post stuff during
the way that I would post stuff during
my PhD because I didn't want it to
my PhD because I didn't want it to
reflect badly on my lab or on MIT or
reflect badly on my lab or on MIT or
stuff but the state of RL is just
stuff but the state of RL is just
pathetic we need to fix it it needs to
pathetic we need to fix it it needs to
be fixed now it's not that hard just do
be fixed now it's not that hard just do
it uh what we're working on today is a
it uh what we're working on today is a
continuous control engine for a bunch of
continuous control engine for a bunch of
simple tasks that can be done obviously
simple tasks that can be done obviously
continuously but also discreet this is
continuously but also discreet this is
going to be a very nice way of seeing
going to be a very nice way of seeing
whether or not uh continuous
whether or not uh continuous
optimization losses are as good as
optimization losses are as good as
discreet ones or whether they're like
discreet ones or whether they're like
fundamentally different and one is
fundamentally different and one is
harder to tune than the other this will
harder to tune than the other this will
be very useful for continuous control
be very useful for continuous control
people and it will be very useful as a
people and it will be very useful as a
test case in puffer as well as for other
test case in puffer as well as for other
people who want to use puffer on this
people who want to use puffer on this
stuff because these environment should
stuff because these environment should
be trainable within a few minutes at
be trainable within a few minutes at
most and we'll run at millions of steps
most and we'll run at millions of steps
per second that is what we are currently
per second that is what we are currently
building
building
so let me think where I'm going to get
so let me think where I'm going to get
started here I think we can start
started here I think we can start
locally we can start with this list of
locally we can start with this list of
tasks um we're going to have to
tasks um we're going to have to
add the only two features that I have to
add the only two features that I have to
add to this environment to make this
add to this environment to make this
work
work
are there needs to be a layer for
are there needs to be a layer for
food ability for them to see
food ability for them to see
food
food
and there has to be um the ability for
and there has to be um the ability for
the individual agents to have different
the individual agents to have different
colors associated with them so let's
colors associated with them so let's
look at that that's going to be slightly
look at that that's going to be slightly
awkward because one of those values is
awkward because one of those values is
potentially continuous and the other is
potentially continuous and the other is
discret
[Music]
[Music]
right maybe
not starting from more RL people that
not starting from more RL people that
are major problems with the seal why do
are major problems with the seal why do
you starting from more RL
you starting from more RL
people that there are major problems in
people that there are major problems in
the field why do you also say it's
the field why do you also say it's
pathetic I say it's pathetic because if
pathetic I say it's pathetic because if
the field were if this were like yeah
the field were if this were like yeah
the problems are just really hard and
the problems are just really hard and
you know we've just and we've done a
you know we've just and we've done a
good job then I would not be able to
good job then I would not be able to
like sit down write a few hundred lines
like sit down write a few hundred lines
of code and solve something that's been
of code and solve something that's been
unsolved for 10 years which has been
unsolved for 10 years which has been
happening like regularly over the last
happening like regularly over the last
couple of months um
couple of months um
Academia heavily
Academia heavily
disincentivizes core infrastructure work
disincentivizes core infrastructure work
that is required for the field to
that is required for the field to
actually function heavily heavily do
actually function heavily heavily do
incentivizes core infrastructure work so
incentivizes core infrastructure work so
just nobody's done it so everybody works
just nobody's done it so everybody works
with really slow janky environments
with really slow janky environments
doesn't have like proper ways to tune
doesn't have like proper ways to tune
baselines it's work that I'm not going
baselines it's work that I'm not going
to say it's necessarily simple but it is
to say it's necessarily simple but it is
absolutely something that can be done at
absolutely something that can be done at
the scale of a single individual and has
the scale of a single individual and has
never been done like this continuous
never been done like this continuous
control environment I mean this seems
control environment I mean this seems
like a really basic question to ask
like a really basic question to ask
right hey if we swap the action space
right hey if we swap the action space
from discreet to continuous does the
from discreet to continuous does the
problem fundamentally change I've never
problem fundamentally change I've never
seen anybody ask that and I've certainly
seen anybody ask that and I've certainly
never seen anybody like do a
never seen anybody like do a
comprehensive evaluation of that like
comprehensive evaluation of that like
we're doing right here this is like
we're doing right here this is like
something that is going to be a I mean
something that is going to be a I mean
I'd be shocked if this takes me more
I'd be shocked if this takes me more
than a few days to complete
than a few days to complete
fully if it does take me more than a few
fully if it does take me more than a few
days to complete fully it will be
days to complete fully it will be
because Contin control is Jank that's
because Contin control is Jank that's
another way of putting it
so right now I have the grid as a u
8 if I want to add continuous amounts of
8 if I want to add continuous amounts of
food I'm not going to be able to have it
food I'm not going to be able to have it
be a u and 8 I'd have to make the
be a u and 8 I'd have to make the
observations
observations
continuous or the food layer at least
continuous or the food layer at least
continuous this would be
continuous this would be
bad yeah the academic environments
bad yeah the academic environments
are I've done more in the last two
are I've done more in the last two
months than I've done in the last two
months than I've done in the last two
years um and that's with a good lab like
years um and that's with a good lab like
it's just the publishing cycle is a mess
um
um
yeah
so I can't do if I just add the so this
so I can't do if I just add the so this
would be basically the same as the snake
would be basically the same as the snake
environment if I just add a few
environment if I just add a few
different agent colors to this that
different agent colors to this that
would support this group up task I could
would support this group up task I could
support the puffer lib task I could
support the puffer lib task I could
support Predator can I do Predator prey
support Predator can I do Predator prey
with that yeah I can do Predator prey
with that yeah I can do Predator prey
with that uh I could do foraging with
with that uh I could do foraging with
that as well I just have to add food as
that as well I just have to add food as
a channel the only one I wouldn't be
a channel the only one I wouldn't be
able to do is this optimize
able to do is this optimize
task
task
which you know we really don't need
which you know we really don't need
that I think I'm good with there being
that I think I'm good with there being
four tasks because they have to be
four tasks because they have to be
optimized continuous and discreet so
optimized continuous and discreet so
having eight different environments
having eight different environments
essentially eight different things to to
essentially eight different things to to
test that's a nice
test that's a nice
start and these
start and these
are yeah these are pretty
are yeah these are pretty
nice so let's just add
nice so let's just add
that so we're going to
do wall will be
do wall will be
here wall will be one agent will
be uh we need to choose some uh some
so what did I do for snake I did red
so what did I do for snake I did red
white and blue for snake didn't I I
white and blue for snake didn't I I
think I didn't just copy
think I didn't just copy
that that'd be funny we just make all
that that'd be funny we just make all
the the environments America
the the environments America
themed since I did the snake one on 4th
themed since I did the snake one on 4th
of July and already have the color
codes yeah there we go
codes yeah there we go
it's Reds whites and blues
it's Reds whites and blues
okay and did I have the I think I just
okay and did I have the I think I just
called them like agent One agent two
called them like agent One agent two
agent three
agent three
right
right
one
one
here agent
here agent
two three and
two three and
four we also need
food USA
all right that's seven different
all right that's seven different
things playing around with
things playing around with
generalization experiments in Sp sp3 had
generalization experiments in Sp sp3 had
to expose a lot of core sp3 code to fit
to expose a lot of core sp3 code to fit
my use cases not to bash on it but it
my use cases not to bash on it but it
does feel junky yeah and SP sp3 is one
does feel junky yeah and SP sp3 is one
of the better ones as well but the
of the better ones as well but the
design philosophy is just fundamentally
design philosophy is just fundamentally
different so the way I approach stuff
different so the way I approach stuff
with puffer is I assume everything is
with puffer is I assume everything is
white box
white box
software um
software um
which means that you're going to have to
which means that you're going to have to
edit every part of the software at some
edit every part of the software at some
point because you're going to be doing
point because you're going to be doing
some crazy thing that I can't anticipate
some crazy thing that I can't anticipate
like reinforcement learning is not a
like reinforcement learning is not a
mature space right this is all research
mature space right this is all research
so what I do is instead of trying to
so what I do is instead of trying to
like box up all these components so that
like box up all these components so that
they snap together like Legos I just
they snap together like Legos I just
make the code as simple and as short as
make the code as simple and as short as
possible so that it's easy for you to
possible so that it's easy for you to
onboard and edit that's how I write
onboard and edit that's how I write
stuff here if it doesn't have rk4 it's
stuff here if it doesn't have rk4 it's
not continuous control
the action space is
continuous if the implementation works
continuous if the implementation works
well with continuous action spaces then
well with continuous action spaces then
the problem of whether this specific
the problem of whether this specific
robot and is hard or not is going to be
robot and is hard or not is going to be
separate from whether or not continuous
separate from whether or not continuous
control is
hard that's the
hard that's the
idea what time it 12:49 I think I have a
idea what time it 12:49 I think I have a
400 p.m. call so I'm good for 3 hours
400 p.m. call so I'm good for 3 hours
I have three hours in
me okay so we need to add food
me okay so we need to add food
fundamentally we need to add food uh to
fundamentally we need to add food uh to
this
environment how are we going
environment how are we going
to well the agents aren't going to die
to well the agents aren't going to die
if they don't get food right it's just
if they don't get food right it's just
going to be a reward for food so I can
going to be a reward for food so I can
just add like food reward
right so reward function introverts and
right so reward function introverts and
I can just add a food reward as well as
I can just add a food reward as well as
an extra thing right food reward
an extra thing right food reward
equals uh
equals uh
0.1
okay oh and then there's this other
okay oh and then there's this other
trick that I like to do
where I think I did this with snake
where I think I did this with snake
right didn't I do this thing with snake
where no I think as fine as this never
where no I think as fine as this never
mind I thought something was applicable
mind I thought something was applicable
and it isn't
okay so we add self. food
reward oh yeah this is where I was going
reward oh yeah this is where I was going
to do it
to do it
so the trick here is instead of doing
so the trick here is instead of doing
like equal agent one or equal agent 2 or
like equal agent one or equal agent 2 or
equal agent
equal agent
3 we just do greater than equal to agent
3 we just do greater than equal to agent
one that was the
one that was the
trick because you define the indices
trick because you define the indices
like that
self. food
self. food
reward okay and now we go
into I'm going to actually just go over
into I'm going to actually just go over
to here it's going to be easier
uh float no not
add food reward
add food reward
here up size agent
here up size agent
speed load food
speed load food
reward
okay now what we do is we add
okay now what we do is we add
unfortunately we do have to dupc at the
unfortunately we do have to dupc at the
enums here which is slightly
annoying so we're going to put these
annoying so we're going to put these
here
okay and now uh it's very easy I think
okay and now uh it's very easy I think
for us to
just yeah it should
just yeah it should
be very easy for us here
I'm actually tempted to move this
I'm actually tempted to move this
into python
into python
but it's
but it's
okay where's the move
function okay so if this is this is
function okay so if this is this is
supposed to be empty not food got to be
supposed to be empty not food got to be
careful with the hardcoded indices so if
careful with the hardcoded indices so if
it's food then what we do is
we we set it to
empty for a second and then we do self.
empty for a second and then we do self.
rewards it's going to be food reward
rewards it's going to be food reward
does this thing have rewards no it
does this thing have rewards no it
doesn't have rewards so we need to uh we
doesn't have rewards so we need to uh we
need to add uh the reward function into
need to add uh the reward function into
here which is somewhat annoying but not
here which is somewhat annoying but not
too bad
too bad
agent positions observations and we do
rewards where's the
rewards where's the
observations Lo
observations Lo
rewards load
rewards load
array
array
and rewards is equal to rewards
good pass this after observations
where is
it self UP up. Rewards right
it self UP up. Rewards right
here so what the reason for this is that
here so what the reason for this is that
if we pass this array it's the same
if we pass this array it's the same
trick we're doing with observations if
trick we're doing with observations if
we pass this as a buffer into the C
we pass this as a buffer into the C
environment uh then we have access to
environment uh then we have access to
mem shared data from both Python and
mem shared data from both Python and
from C so we have very very fast ability
from C so we have very very fast ability
to update um rewards and to read rewards
to update um rewards and to read rewards
from Python and from
from Python and from
C so this is now done
and now I have to change the way that
and now I have to change the way that
rewards are um rewards are applied for
rewards are um rewards are applied for
sure
so the way to do that is
so the way to do that is
uh you just have to add the reward
uh you just have to add the reward
function to the buffer so these come
function to the buffer so these come
from C these come from python
oh wait can you even do this like that
oh wait can you even do this like that
no wait you can't there's no point in
no wait you can't there's no point in
doing the fill operation from scon
doing the fill operation from scon
because it's a numpy operation anyways
because it's a numpy operation anyways
so it's just going to call back to
so it's just going to call back to
python to do that anyways so let's just
python to do that anyways so let's just
do uh right here we'll just do self
dot fill zero good
uh okay so this is now
uh okay so this is now
reasonable where's the
reasonable where's the
renderer yes this is the renderer
here I kind of want to one file this
here I kind of want to one file this
whole
thing
thing
there was this where it was wait am I
there was this where it was wait am I
yeah it's
yeah it's
it's let's just take all this
code 100
code 100
lines we're just going to put it in this
file okay now we got 300 lines total for
file okay now we got 300 lines total for
the environment with
the environment with
no additional stuff floating around
now we can just have this is just make
now we can just have this is just make
render and we'll apply some of the
render and we'll apply some of the
cleanups from the snake en in a bit
cleanups from the snake en in a bit
here
here
but as for the asset
map we kind of want this snake one here
map we kind of want this snake one here
don't
we didn't we actually make like a way
we didn't we actually make like a way
better renderer here as well
I think we want the snake grid
renderer yeah because this is the entire
renderer yeah because this is the entire
snake renderer right
snake renderer right
here so we don't need this grid render
here so we don't need this grid render
thing at all it's just a
thing at all it's just a
waste
waste
um make render so this make renderer
um make render so this make renderer
thing is a total waste as
thing is a total waste as
well
well
um so
let's just update
let's just update
that oops where is
it so if render mode is human self.
it so if render mode is human self.
client be equal to
client be equal to
this uh asset map with
this uh asset map with
height I think we just do this
yeah cuz we're not going to use this
yeah cuz we're not going to use this
mode for a bit
mode for a bit
anyways so self. render
anyways so self. render
mode RGB array then we can do frame
mode RGB array then we can do frame
equal
colors
grid actually we can cut the oh this is
grid actually we can cut the oh this is
well
let's do Vision where is it
Vision self. Vision
Vision self. Vision
range GD
of self
of self
grid
grid
no grid equals self.
grid yeah and we'll just leave like this
grid yeah and we'll just leave like this
for now and and now we just go get
for now and and now we just go get
colors from
snake grab all
these I could make the uh the asy render
these I could make the uh the asy render
for this as well you know this is the
for this as well you know this is the
snake render is kind of
snake render is kind of
good I might just use uh something
good I might just use uh something
similar to that and then I might end up
similar to that and then I might end up
making like a just a uniform render for
making like a just a uniform render for
you know all these different little
you know all these different little
projects of mine but for now it makes
projects of mine but for now it makes
more sense to just one file stuff and
more sense to just one file stuff and
just see if that's actually the same
just see if that's actually the same
code or
not yeah so this is fine turn
frame for
what did I do
here 11 positional arcs
did I like forget to pass
something wait
ah se grid
ah se grid
continuous what's the signature here
continuous what's the signature here
grid AG agent position spawn position
grid AG agent position spawn position
cans observations rewards width height
cans observations rewards width height
num agents Horizon
num agents Horizon
Vision speed discretize food reward this
Vision speed discretize food reward this
is correct oh did I just not I didn't
is correct oh did I just not I didn't
compile it right
that's
fine and uh
agent going do agent one for now like
this and we will edit this in a little
this and we will edit this in a little
bit to support different
colors for
right we have to edit the uh we have to
right we have to edit the uh we have to
edit the torch model just a little
bit because this now takes uh a few
bit because this now takes uh a few
additional
channels believe seven additional
channels believe seven additional
channels
channels
and is this still three you know this is
and is this still three you know this is
still three
still three
cool I think we're set here um we'll see
cool I think we're set here um we'll see
if there are any little additional
errors expected to have three channels
errors expected to have three channels
oh it right the observation space we
oh it right the observation space we
also have to update so that it correctly
also have to update so that it correctly
reflects
reflects
the no wait this is
the no wait this is
fine um
base yeah no this is totally fine
base yeah no this is totally fine
um what is wrong
um what is wrong
here given weights of size
three didn't I just edit this did I edit
three didn't I just edit this did I edit
the wrong one or
something CNN
something CNN
channels oh this was three right what
channels oh this was three right what
was
was
this yeah this this was five up
this yeah this this was five up
here this is the kernel size this is the
here this is the kernel size this is the
one that had to be
seven puffer grid has no attribute
rewards this is self. buff. Rewards
Okay cool so this now runs and we have
Okay cool so this now runs and we have
our nice puffer en uh now we can make
our nice puffer en uh now we can make
this have different agent colors or
this have different agent colors or
whatever and we can add food and we can
whatever and we can add food and we can
do all sorts of nice
things I think we're going to start by
we're going to have to add a uh an
argument we're going to have to add like
argument we're going to have to add like
agent colors or something right to the
agent colors or something right to the
the arcs of the in
it yeah that would be the easiest right
we just add
like
like
H let's do it that way for now I have
H let's do it that way for now I have
some ideas on how we can make this a
some ideas on how we can make this a
like a slightly more General and useful
like a slightly more General and useful
thing in a bit but for now let's just do
thing in a bit but for now let's just do
like
uh self. agent
uh self. agent
colors p. random.
colors p. random.
randant and it's going to be
randant and it's going to be
from 3 to six so is that 0 to 4 + three
from 3 to six so is that 0 to 4 + three
or
or
whatever but we can just do
whatever but we can just do
three
three
agents and we will pass this
to the Sea environment
maybe does that make sense to pass this
maybe does that make sense to pass this
to the Sea
to the Sea
environment I don't know I can't really
environment I don't know I can't really
think all that well today let's do uh
think all that well today let's do uh
let's just do it the dumb way for now
let's just do it the dumb way for now
and then we'll fix it if it needs to be
fixed spawn position
fixed spawn position
cans agent
cans agent
colors agent
colors agent
colors C grid
continuous agent
continuous agent
colors and we'll do
colors and we'll do
uh float agent no in agent colors right
uh float agent no in agent colors right
in agent colors
did I give this thing a d type
did I give this thing a d type
before no it's probably an
INT and we'll do self. agent
colors assign
colors assign
that and now when we spawn the
that and now when we spawn the
agents and when we spawn the
agents this is the agent spawn code
agents this is the agent spawn code
right here
instead of being agent one this is going
instead of being agent one this is going
to be self. agent colors of agent
idx now this is also going to
idx now this is also going to
be s. agent colors of agent
be s. agent colors of agent
idx
idx
okay let's see if that does oops well we
okay let's see if that does oops well we
forgot to compile it so that's not
help and if I've done this correctly we
help and if I've done this correctly we
should
get various different colored
get various different colored
agents expected in got long okay that I
agents expected in got long okay that I
was somewhat suspicious of
that D
that D
type and 32
hey very
hey very
nice so we've
nice so we've
got red and white agents
got red and white agents
here and we will go make
here and we will go make
some blue food to add to
some blue food to add to
this
this
right then this will be
set so let's do
it's food indexed uh
two let's actually make the wall two and
two let's actually make the wall two and
food
food
one uh one second and you'll see
one uh one second and you'll see
why it's just going to make it slightly
easier it's going to be slightly easier
easier it's going to be slightly easier
to do
numpy random. randint
02 and that would give us food
not seeing the
food should be
blue oh it's because of this grid
fill
fill
okay we'll just do like
okay we'll just do like
this we'll just remake
this we'll just remake
the oh no we can't do that you have to
the oh no we can't do that you have to
fill
fill
it so uh here self.
it so uh here self.
grid so we make it zeros and then we
grid so we make it zeros and then we
fill it
fill it
here so self. grid uh star
here so self. grid uh star
star you can copy stuff into the
star you can copy stuff into the
grid that's
grid that's
acceptable kind of slow um we might find
acceptable kind of slow um we might find
some other way of doing
some other way of doing
it soft. height soft. width
okay there we
go so now are they eating the
go so now are they eating the
food yeah right like whenever they hit
food yeah right like whenever they hit
they walk on the food it disappears
they walk on the food it disappears
yeah awesome
so this is uh our basic environment that
so this is uh our basic environment that
is going to be required to implement
is going to be required to implement
whatever we
want we
want we
need a few things from
need a few things from
here we need to implement the tasks and
here we need to implement the tasks and
we would like to have a clean way of
we would like to have a clean way of
implementing tasks on top of this grid
implementing tasks on top of this grid
type environment
type environment
is what we would like to
have I think that in order to do
that it's actually a fair number of
that it's actually a fair number of
things you need in order to do
things you need in order to do
that
that
um you need a reward function I guess a
um you need a reward function I guess a
task consists of an environment
task consists of an environment
initialization and a reward function
initialization and a reward function
right yeah
environment initialization and a reward
function let me think how I want to do
that really don't want to make a bunch
that really don't want to make a bunch
of sub classes of this environment
I could give it the functions to
I could give it the functions to
call um I could give it a nit function
call um I could give it a nit function
to call and I could give it a reward
to call and I could give it a reward
function that's kind of how we have it
function that's kind of how we have it
at the moment
that wouldn't be
that wouldn't be
bad
right man it's
right man it's
like I'm so used to just subclassing
like I'm so used to just subclassing
stuff and making a mess of things in the
stuff and making a mess of things in the
long run that I have to like get my head
long run that I have to like get my head
out of that that space we're just going
out of that that space we're just going
to do it how we're going to do it for
now so we're going to take a uh we're
now so we're going to take a uh we're
going to just Define some reward
going to just Define some reward
functions so we're going to move these
functions so we're going to move these
outside up top Maybe
takes
observations think this is just n
right you full axess to the
end okay and uh we need to make some
end okay and uh we need to make some
init functions
init functions
right so this is going to take reward
right so this is going to take reward
function init function reward function
right
and instead of doing string we can just
and instead of doing string we can just
make this the reward introverts function
make this the reward introverts function
right init
and it
and it
introverts like
so and then it's just going to be self
so and then it's just going to be self
dot nit function reward
dot nit function reward
function that's actually kind of
function that's actually kind of
clean that's really not that
bad so we make uh an init function for
bad so we make uh an init function for
introverts now the NP
function should take care
function should take care
of uh
of uh
spawning and the placement
spawning and the placement
of
of
any obstacles or food or whatever
right so these spawn position cans
we'll leave them like this for now but
we'll leave them like this for now but
they're probably going to have to
change I don't think we have to do
change I don't think we have to do
anything for this specific
end we call this inside of reset
self. AIT
function and we call this
function and we call this
here and yeah so this would be inside of
here and yeah so this would be inside of
like the reward food one or whatever in
like the reward food one or whatever in
it forging
yeah let's see how this
yeah let's see how this
does still
work yeah
no self. reward function
yeah one little
change okay so now we have this particle
change okay so now we have this particle
n we can Define tasks in it we're going
n we can Define tasks in it we're going
to have to come up with some slightly
to have to come up with some slightly
cleaner ways of doing that but I can
cleaner ways of doing that but I can
sort of think about that over time uh
sort of think about that over time uh
let's get all of the tasks that we are
let's get all of the tasks that we are
interested in implemented at the very
interested in implemented at the very
least let's get all of the tasks
least let's get all of the tasks
implemented
so we would like to do the forging
task so we have init forging which is um
task so we have init forging which is um
you make some
food
food
and this is kind of too much food isn't
and this is kind of too much food isn't
it
I think what you do is you do like RNG
I think what you do is you do like RNG
equal numpy random.
equal numpy random.
Rand of uh m.
Rand of uh m.
height height m. width and then you do
height height m. width and then you do
m. grid of RNG less
than
prob food cr1
we prob is
we prob is
food and then otherwise it's
empty reward
empty reward
forging uh what do you have to
forging uh what do you have to
return you don't really have to return
return you don't really have to return
anything
anything
right so this is inefficient but I'm
right so this is inefficient but I'm
going to return just zeros for
now okay so we have the forging spec
now okay so we have the forging spec
here
here
um we actually kind of want to do the
um we actually kind of want to do the
agent colors in here as well don't
we you do want to put the agent colors
we you do want to put the agent colors
in
in
here but I can do it inside of um
here but I can do it inside of um
because it's memory shared I can do it
because it's memory shared I can do it
inside
inside
here so I can do um it's fine for this
here so I can do um it's fine for this
one for them to all be just random
right
right
Predator
right uh I think we're not going to mix
right uh I think we're not going to mix
it we're not going to make like the
it we're not going to make like the
Predators ALS the prey also have to get
Predators ALS the prey also have to get
food we're just going to do a pure uh
food we're just going to do a pure uh
stay away from other agents task
so how do I have the color set
up this is from snake right so I have
up this is from snake right so I have
red white red so
yeah um
yeah um
agents so let's do m.
agents so let's do m.
agent colors so
so over
4 n is going to be agent
4 n is going to be agent
one and it's going to this is going to
one and it's going to this is going to
be agent three which is going to be
Predators
um
and well
and well
I just use the first two
I just use the first two
colors maybe I should just use the first
colors maybe I should just use the first
two
two
colors first two colors are bright red
colors first two colors are bright red
and bright white yeah that's fine we'll
and bright white yeah that's fine we'll
just use the first two
colors so this is going to be agent one
colors so this is going to be agent one
and then this is Agent
two and what else do we have to do to
two and what else do we have to do to
initialize these
we're probably going to have to like
we're probably going to have to like
move them apart or something but we'll
move them apart or something but we'll
handle the spawn position differences in
handle the spawn position differences in
a little
a little
bit I think that's all we have to do for
bit I think that's all we have to do for
a nit is just make sure that they are
a nit is just make sure that they are
defined this
defined this
way reward Predator
prey
prey
W is going to be
W is going to be
zeros and then um
reward introverts here
right so n is going to be num agents
right so n is going to be num agents
over two and then we do wordss
of so the prey reward is going to be
it stay away from the Predator
it stay away from the Predator
specifically
so so this is the Predator agent
here we're going to still clip the
here we're going to still clip the
rewards I think just like
rewards I think just like
before and uh we need the
before and uh we need the
the the Predator reward
the the Predator reward
right which is going to
be this doesn't need to be one minus
be this doesn't need to be one minus
anymore
anymore
because uh the current agent is not
because uh the current agent is not
included so this is just going to be
included so this is just going to be
positive
right yes so
so Predator wants
so Predator wants
to uh keep prey in
to uh keep prey in
sight
sight
prey wants to keep Predator out of
prey wants to keep Predator out of
sight and that's the reward for predator
prey optimize is not happening for
prey optimize is not happening for
now um group up is going to be simpler I
think in it
group we're going to
group we're going to
do I think the default just like
do I think the default just like
randomly
randomly
assigning agent colors is actually good
assigning agent colors is actually good
here
here
right yeah
right yeah
so uh the reward actually should be very
so uh the reward actually should be very
easy did you Define insight if is it
easy did you Define insight if is it
just a check of surrounding area yeah so
just a check of surrounding area yeah so
the observations here this is a uh a 11
the observations here this is a uh a 11
by1 window that is centered on the agent
by1 window that is centered on the agent
so by checking just the local window of
so by checking just the local window of
observations right you can sum over that
observations right you can sum over that
window so this is comparing the window
window so this is comparing the window
all of the stuff in that window to the
all of the stuff in that window to the
specific agent type and then we can see
specific agent type and then we can see
how many agents of that type are in the
how many agents of that type are in the
window the init group one is going to be
window the init group one is going to be
very
very
similar um it's going to be even a
similar um it's going to be even a
little easier so it's going to be
little easier so it's going to be
rewards
equal no not zeros it's going to
be it's well it's going to be like same
equal
to what this thing
here m. buff.
observations equal
to we need the agent types don't
to we need the agent types don't
we self. agent
colors is that going to
colors is that going to
be well we might have to
be well we might have to
broadcast
broadcast
colors m. agent colors so you get
colors m. agent colors so you get
rewarded based on the number that you
rewarded based on the number that you
can see of your
own this you have to subtract one
own this you have to subtract one
because you can always see
because you can always see
yourself and diff reward is going to
yourself and diff reward is going to
be
be
one is going to be negative of this so
one is going to be negative of this so
you get rewarded one for each AG that
you get rewarded one for each AG that
you can see of your same
you can see of your same
population you get penalized for each
population you get penalized for each
agent uh that is in a different
agent uh that is in a different
population then you
return so rewards is going to be equal
return so rewards is going to be equal
to same plus
to same plus
diff and actually we can get rid of this
diff and actually we can get rid of this
negative and we can just put minus here
negative and we can just put minus here
and we can just clip
and we can just clip
it this is not supposed to be the NIT
it this is not supposed to be the NIT
this should be reward group
this should be reward group
and then in it
and then in it
group is actually just going to be a
group is actually just going to be a
pass don't need to do
anything and then the puffer Li task is
anything and then the puffer Li task is
going to be a little harder because I'm
going to be a little harder because I'm
going to have to go and uh figure out
going to have to go and uh figure out
how to get a bit map of puffer lib maybe
how to get a bit map of puffer lib maybe
even the puffer logo we'll see we're
even the puffer logo we'll see we're
going to have to get a bit map of that
going to have to get a bit map of that
and see if they will uh match
it for
so we maybe we can even start trying
so we maybe we can even start trying
these out already
um easiest one of these is going to be
um easiest one of these is going to be
forging this m is parti is this uh envis
forging this m is parti is this uh envis
partially observed how will agents
partially observed how will agents
complete puffer task if the logo is
complete puffer task if the logo is
bigger than their window so yes the m is
bigger than their window so yes the m is
partially observed um the starting
partially observed um the starting
positions are going to be appropriate
positions are going to be appropriate
for the task so for the puffer lib task
for the task so for the puffer lib task
I'm going to spread out uh lean key this
I'm going to spread out uh lean key this
is right before I started this I
is right before I started this I
committed the sum version of this to Dev
committed the sum version of this to Dev
you can look at it
you can look at it
um yeah I'm I'm trying to make it easier
um yeah I'm I'm trying to make it easier
to add tasks at the moment
to add tasks at the moment
um JBL
um JBL
uh yeah so for the puffer task for
uh yeah so for the puffer task for
instance I'm just going to start all the
instance I'm just going to start all the
agents at the top of the map in like a
agents at the top of the map in like a
blob type thing that covers the top of
blob type thing that covers the top of
the map and the hope is that they're
the map and the hope is that they're
going to learn to just Cascade downward
going to learn to just Cascade downward
and then they'll just sort of fill in
and then they'll just sort of fill in
pixels based on where they're getting
pixels based on where they're getting
reward um you know the task has to make
reward um you know the task has to make
sense if it's partially observed uh and
sense if it's partially observed uh and
there's no way to get there then they're
there's no way to get there then they're
not going to be able to do it um but
not going to be able to do it um but
like the Predator prey ones make sense
like the Predator prey ones make sense
part partially observed um the forging
part partially observed um the forging
ones make sense partially observed
ones make sense partially observed
though you're going to have to learn to
though you're going to have to learn to
explore a little bit right everything
explore a little bit right everything
else sort of make sense the reward for
else sort of make sense the reward for
groups maybe that you won't get one
groups maybe that you won't get one
group for each agent type right maybe
group for each agent type right maybe
they'll split into multiple groups so
they'll split into multiple groups so
they won't get like as much reward as
they won't get like as much reward as
they could but it should still be able
they could but it should still be able
to learn something very reasonable that
to learn something very reasonable that
we should be able to evaluate and uh
we should be able to evaluate and uh
it's very important with these types of
it's very important with these types of
M to get the reward on a scale that
M to get the reward on a scale that
makes sense so hopefully we're going to
makes sense so hopefully we're going to
scale stuff so that like the maximum
scale stuff so that like the maximum
reward or whatever is going to be like
reward or whatever is going to be like
one and the minimum will be zero or
one and the minimum will be zero or
something like that or negative 1
something like that or negative 1
whatever across all the different
whatever across all the different
environments so we're going to have very
environments so we're going to have very
nice easy to evaluate metrics on all of
nice easy to evaluate metrics on all of
this
stuff let's
stuff let's
do foraging in it
foraging first does the eval make sense
EV makes
EV makes
sense is this
sense is this
10% seems like a lot of
10% seems like a lot of
food whatever should be very easy to
food whatever should be very easy to
learn
learn
right uh so let's do mode
right uh so let's do mode
train dash dash ulti
pross and let me make sure I have a
pross and let me make sure I have a
reasonable whoops
reasonable whoops
reasonable
config
grid yeah so I do I have a reasonable
grid yeah so I do I have a reasonable
configuration
here let's just see if this does
here let's just see if this does
anything remotely
anything remotely
reasonable I think it was being a little
reasonable I think it was being a little
bit obnoxious yesterday
it is a little bit annoying that this
it is a little bit annoying that this
does not give you rewards back
consistently I'm going to go ahead and
consistently I'm going to go ahead and
fix
that oh interesting that I made it
that oh interesting that I made it
episode episode Rewards
H
H
tick
percent o
we'll do like
we'll do like
this we get any rewards
here yeah so this is obnoxious that this
here yeah so this is obnoxious that this
doesn't work the way I'd like it to so
doesn't work the way I'd like it to so
let's just recast uh redo this
let's just recast uh redo this
experiment and uh this should give
experiment and uh this should give
us something a little bit more
us something a little bit more
reasonable
pretty much same network as
snake still not seeing stuff showing up
snake still not seeing stuff showing up
in uh user stats which is
in uh user stats which is
weird I did Commit This I mean I did
weird I did Commit This I mean I did
yeah episode
yeah episode
return did I just messed this up uh if
return did I just messed this up uh if
tick percent 32
tick percent 32
oh am I not increasing self.
oh am I not increasing self.
tick no I
tick no I
am
am
[Music]
[Music]
so maybe it shows up in
here very
weird there a little bit weird how this
weird there a little bit weird how this
thing logs
um episode
um episode
return yeah it's a little weird how this
return yeah it's a little weird how this
thing
thing
logs I'll have to mess with
logs I'll have to mess with
that how did I do it in snake that it
that how did I do it in snake that it
worked I was just running it at much
worked I was just running it at much
larger
scale this is zero
scale this is zero
regardless cuz that's not
helpful yeah so here there's a special
helpful yeah so here there's a special
case um to see if Episode return is in
case um to see if Episode return is in
here so it prioritizes logging data that
here so it prioritizes logging data that
has episode return in
has episode return in
it but I didn't do that for snake and it
it but I didn't do that for snake and it
still
worked I do
it so for snake I
did I did every 128 steps which is not
did I did every 128 steps which is not
very often and it still worked
so is
something is something off here they
something is something off here they
didn't do episode return
here oh right here this
else so what we're going to do is we're
else so what we're going to do is we're
going to just put this down
here yeah that'll do
here yeah that'll do
it now we'll be able to see
[Music]
[Music]
cool so let's see if this is actually
cool so let's see if this is actually
zero or if there's um a slight positive
zero or if there's um a slight positive
reward
reward
here
here
uhoh I'm going to have to delete some uh
uhoh I'm going to have to delete some uh
some runs
apparently should be okay for a little
apparently should be okay for a little
bit
okay the reward is actually
okay the reward is actually
zero which leads me to believe that
zero which leads me to believe that
we're just like not updating it
correctly so where's the reward for
correctly so where's the reward for
food okay so this is zero reward for
food okay so this is zero reward for
food but you should be getting a reward
food but you should be getting a reward
based on food eaten from the environment
based on food eaten from the environment
itself provided that we've passed the
itself provided that we've passed the
rewards
rewards
correctly which where did we do
that yes so we pass buff rewards
that yes so we pass buff rewards
here
and we fill it with zeros here but then
and we fill it with zeros here but then
we step it right
continuous food reward
this looks
this looks
reasonable food is one wall is two
right there's one walls two did I just
right there's one walls two did I just
forget to recompile it like since I last
forget to recompile it like since I last
changed
that looks like it's building something
that looks like it's building something
new so something
changed we should hopefully get
changed we should hopefully get
something that is not
zero
really welcome back Samurai
we are currently fixing this environment
we are currently fixing this environment
up so that it will actually train some
up so that it will actually train some
cool
tasks hey curious have you faced any
tasks hey curious have you faced any
limitations with scyon if you've
limitations with scyon if you've
answered this apologies scyon is
answered this apologies scyon is
absolutely amazing like it is so
absolutely amazing like it is so
freaking good scyon has single-handedly
freaking good scyon has single-handedly
fixed like most of the problems that I
fixed like most of the problems that I
have had uh limitations that I've had in
have had uh limitations that I've had in
the field because you can give it to
the field because you can give it to
researchers it looks like python it it
researchers it looks like python it it
is python with a couple types added um
is python with a couple types added um
it's super fast to write and you can get
it's super fast to write and you can get
C native speeds and you can get shared
C native speeds and you can get shared
data access between Python and scyon
data access between Python and scyon
read and write shared access very very
read and write shared access very very
easily um so so many good things I've
easily um so so many good things I've
had like a couple very small nitpicks
had like a couple very small nitpicks
with it but so far it's like use syon
with it but so far it's like use syon
it's amazing
love me some
scon the few nitpicks would be they kept
scon the few nitpicks would be they kept
the C behavior of passing uh structs by
the C behavior of passing uh structs by
value so you have to use pointers where
value so you have to use pointers where
if they just passed by reference you
if they just passed by reference you
wouldn't I wish they would have kept
wouldn't I wish they would have kept
that from python um you cannot Define
that from python um you cannot Define
variables inside of conditionals and
variables inside of conditionals and
Loops you have to Define them at the top
Loops you have to Define them at the top
that's slightly annoying and um the
that's slightly annoying and um the
compile times could be better those are
compile times could be better those are
my main
my main
[Music]
annoyances okay so seriously what is
annoyances okay so seriously what is
wrong with this food thing I think we
wrong with this food thing I think we
need to just go put a print somewhere in
need to just go put a print somewhere in
here oh yeah and you can't run pdb
here oh yeah and you can't run pdb
inside of scon I really wish could run
inside of scon I really wish could run
pdb inside of cython and like debug at
pdb inside of cython and like debug at
the python level I know that would be
the python level I know that would be
slow but maybe you would be able to do
slow but maybe you would be able to do
it
food yeah cython is like keys to the
food yeah cython is like keys to the
kingdom for high performance simulation
kingdom for high performance simulation
engineering without spending months
engineering without spending months
having to do
having to do
it it's like I can build stuff so fast
it it's like I can build stuff so fast
now and actually have it be high
now and actually have it be high
perf in fact it can actually be faster
perf in fact it can actually be faster
to build in scon because I can write the
to build in scon because I can write the
code really stupid you know I don't have
code really stupid you know I don't have
to spend time uh doing I don't have to
to spend time uh doing I don't have to
spend time doing like optimization crap
spend time doing like optimization crap
that I would in Python like I just do it
that I would in Python like I just do it
in scon and it's fast it's so
in scon and it's fast it's so
nice love me
scon okay so right here
scon okay so right here
this should
this should
be
something that was a lot of
foodprints that was like a lot of
foodprints that was like a lot of
foodprints but okay so
says that we're getting rewards but I'm
says that we're getting rewards but I'm
not seeing
not seeing
any which is very
any which is very
weird um my guess would be that we
weird um my guess would be that we
accidentally assigned rewards somehow
accidentally assigned rewards somehow
did we overwrite reward
somewhere I have a fill here the fill
somewhere I have a fill here the fill
should be fine I don't know why this uh
should be fine I don't know why this uh
rewards is not
rewards is not
synced self. rewards equals
synced self. rewards equals
rewards self rewards of agent
rewards self rewards of agent
idx this looks fine to
me is it the
fill I don't think that the fill would
fill I don't think that the fill would
do it
do it
right oops
WS did I do like m. rewards equals or
something now this returns
zeros I do like
this maybe the plus equals is not in
this maybe the plus equals is not in
place is I think it
is
really uh it's occurring to me that I'm
really uh it's occurring to me that I'm
not seeing actually getting food
here it should be getting some
food
so what is up with
so what is up with
that knit
function oh is it deleting all the
function oh is it deleting all the
agents let me see
yeah yeah yeah
um so it needs to be
like
times
times
n. grid
grid
uh I think I was overriding the
agents
right thoughts on Jack
right thoughts on Jack
is this a
is this a
troll I literally wrote like a super
troll I literally wrote like a super
ranty post about
ranty post about
this I mean this Artic I literally wrote
this I mean this Artic I literally wrote
like this stupid rant article that blew
like this stupid rant article that blew
up here this thing has 140,000 views on
up here this thing has 140,000 views on
it on my relatively small
it on my relatively small
Twitter um not a jack
fan main reason for it is the problems
fan main reason for it is the problems
that it Sol solving are way more easy to
that it Sol solving are way more easy to
solve directly
solve directly
with relatively basic infrastructure and
with relatively basic infrastructure and
the price of jacks is introducing a
the price of jacks is introducing a
domain specific language to everything
domain specific language to everything
so especially for environment
so especially for environment
simulations I can write dead simple code
simulations I can write dead simple code
and have stuff run at millions of steps
and have stuff run at millions of steps
per second or I can be forced to
per second or I can be forced to
implement stuff in a domain specific
implement stuff in a domain specific
language and now be heavily constrained
language and now be heavily constrained
in the types of environments I can
in the types of environments I can
Implement in the first place
a lot of people do like Jack but wait do
a lot of people do like Jack but wait do
you like it for training or do you like
you like it for training or do you like
it for
it for
environments these are two different
environments these are two different
things I don't really like it for either
things I don't really like it for either
I'm slightly more tolerant of it for
I'm slightly more tolerant of it for
training than for environments in most
training than for environments in most
cases though there are some environments
cases though there are some environments
some like Niche cases where it makes a
some like Niche cases where it makes a
lot of sense for
environments but I think it's definitely
environments but I think it's definitely
being overused right now heavily heavily
being overused right now heavily heavily
overused and it's causing RL a lot of
overused and it's causing RL a lot of
problems
mostly for OD yeah that's totally fine
mostly for OD yeah that's totally fine
if you're using it for like scientific
if you're using it for like scientific
Computing stuff and you just want like
Computing stuff and you just want like
an easy way to do array Ops on GPU
an easy way to do array Ops on GPU
that's totally
that's totally
fine um the problem is people are trying
fine um the problem is people are trying
to implement full Stacks where the
to implement full Stacks where the
environment which is a complex simulator
environment which is a complex simulator
is end to end Jacks the reinforcement
is end to end Jacks the reinforcement
learning libraries end to end Jacks with
learning libraries end to end Jacks with
tons of third-party poorly developed
tons of third-party poorly developed
libraries like flax which are like god-
libraries like flax which are like god-
awful hybrid of overly functional and
awful hybrid of overly functional and
overly object-oriented programming and
overly object-oriented programming and
the whole stack essentially grinds to a
the whole stack essentially grinds to a
halt because they've introduce all this
halt because they've introduce all this
complexity in the name of performance
complexity in the name of performance
instead of just like not writing stuff
instead of just like not writing stuff
in in playing python like domain
in in playing python like domain
specific language inside of python
specific language inside of python
versus add a couple types in just syon
versus add a couple types in just syon
like I don't understand how this is a
like I don't understand how this is a
comparison right totally fine for other
comparison right totally fine for other
applications the post was for Riel
applications the post was for Riel
infrastructure
specifically can I do and personand
here
here
really what about Plus
what's the Boolean mask op in
what's the Boolean mask op in
um isn't there Boolean mask op in numpy
um isn't there Boolean mask op in numpy
that just works or do I just do I really
that just works or do I just do I really
have to do like this
have to do like this
times like
times like
bu what ites
bu what ites
type
type
w truth value of an array with
w truth value of an array with
am I doing this
wrong
what
huh oh is it the
huh oh is it the
parenthesis it's the parenthesis right
parenthesis it's the parenthesis right
that's so obnoxious
oops I would have thought that the
oops I would have thought that the
uh I guess it makes sense why that
uh I guess it makes sense why that
happens right if you think about
it yeah of course it's going to bind it
it yeah of course it's going to bind it
binds those operators last
okay I'm something is really Jank here
okay I'm something is really Jank here
and I don't know what it is
um can I just like evil this
yes yes I've spoken uh with Alexi quite
yes yes I've spoken uh with Alexi quite
a bit um so sample Factory gives you
a bit um so sample Factory gives you
very good
very good
performance the code is much more
performance the code is much more
complicated than something like puffer
complicated than something like puffer
so I am trying to get as close to that
so I am trying to get as close to that
level of performance as possible
level of performance as possible
actually in some cases I think I have
actually in some cases I think I have
better um but he has a lot of
better um but he has a lot of
optimizations that I don't but I'm not
optimizations that I don't but I'm not
willing to complicate the code bases I
willing to complicate the code bases I
want reinforcement learning to be like
want reinforcement learning to be like
dead simple and performant by default
dead simple and performant by default
that's the whole thing that I'm going
for like clean ourl Simplicity sample
for like clean ourl Simplicity sample
Factory performance if you want to look
Factory performance if you want to look
at it that way that is the that is the
objective cleaner Simplicity sample
objective cleaner Simplicity sample
Factory
performance and
performance and
actually sample Factory performance only
actually sample Factory performance only
takes into account the um the learning
takes into account the um the learning
library uh I'm also re-engineering the
library uh I'm also re-engineering the
environments themselves to be a thousand
environments themselves to be a thousand
times faster so like together you can
times faster so like together you can
get stuff that is way way way faster
get stuff that is way way way faster
than sample Factory is now because
than sample Factory is now because
you're training on like equally complex
you're training on like equally complex
environments to what you'd be using
environments to what you'd be using
anyways but the environments just run
anyways but the environments just run
way
faster like the snake environment runs
faster like the snake environment runs
at 14 million steps per second on One
at 14 million steps per second on One
Core
Core
and trains it a
million linky it makes the RL more
million linky it makes the RL more
accessible to everybody like PhD
accessible to everybody like PhD
researchers don't understand what the
researchers don't understand what the
hell is going on in this space either
hell is going on in this space either
it's just all a mess it's not like oh
it's just all a mess it's not like oh
you need to take the time to learn it
you need to take the time to learn it
it's just everything is freaking hard
it's just everything is freaking hard
and a mess
sample factor is good though sample
sample factor is good though sample
factor is very very
good I mean the main objective of puffer
good I mean the main objective of puffer
is not to make RL easier for beginners
is not to make RL easier for beginners
the main objective of puffer is to push
the main objective of puffer is to push
the boundaries of what RL can do um but
the boundaries of what RL can do um but
the thing is that I think that the best
the thing is that I think that the best
way to do that is also to make it simple
way to do that is also to make it simple
which happens to have this nice side
which happens to have this nice side
effect of making like a lot more people
effect of making like a lot more people
able to get into RL
easily but yeah like a good thing to do
easily but yeah like a good thing to do
is like with sample Factory it's just
is like with sample Factory it's just
like open up the source code for sample
like open up the source code for sample
Factory try to figure out where all the
Factory try to figure out where all the
components that you would expect are
components that you would expect are
open up the code for puffer see where
open up the code for puffer see where
all the components are and see which one
all the components are and see which one
is easier to figure
is easier to figure
out right puffer doesn't have all the
out right puffer doesn't have all the
stuff that sample Factory
though the link key you're rapidly
though the link key you're rapidly
running out of Noob credits you know
running out of Noob credits you know
after it's been a year you're going to
after it's been a year you're going to
just be expected to know things that's
just be expected to know things that's
how it works
[Music]
right what is going on here this is so
right what is going on here this is so
weird is it this thing
something is
wonk okay so the rewards are just not
wonk okay so the rewards are just not
coming through and I don't get why
coming through and I don't get why
not self. buff.
not self. buff.
rewards it's
rewards it's
here gets
here gets
assigned oh let's put this like this
assigned oh let's put this like this
maybe
interesting so when you you do that
fill does that like screw something
up wait did I mess
up where is
food reward did I put it in the wrong
food reward did I put it in the wrong
spot oh man I put this
spot oh man I put this
on I put this on the
on I put this on the
spawn
o that was stupid that was very very
o that was stupid that was very very
stupid supposed to be
stupid supposed to be
there my bad
one of the side effects of starting your
one of the side effects of starting your
day with two to three hours of Hard
day with two to three hours of Hard
Exercise um is that occasionally your
Exercise um is that occasionally your
brain will just decide no I'm not going
brain will just decide no I'm not going
to work today
are these eating
food they're moving around the food
food they're moving around the food
aren't
they wait
they wait
if disk this is d y it needs to be here
if disk this is d y it needs to be here
right disk
right disk
dest d x yeah yeah yeah I just I have
dest d x yeah yeah yeah I just I have
the I have the names wrong here
cool yeah I thought that was a pretty
cool yeah I thought that was a pretty
good line I'm sure that's going to piss
good line I'm sure that's going to piss
off a lot of a lot of
people I swear like heavy functional
people I swear like heavy functional
programming is just like computer
programming is just like computer
science's
science's
world of like intellectual Naval gazing
world of like intellectual Naval gazing
it's just what the
hell heavily functional heavily oop are
hell heavily functional heavily oop are
just both insane and like trying to like
just both insane and like trying to like
oh yeah well we're going to make a
oh yeah well we're going to make a
functional library and the crazy thing
functional library and the crazy thing
about it is like you know I'll post like
about it is like you know I'll post like
some absolute garbage snippet of code
some absolute garbage snippet of code
that's like makes no sense and then I'll
that's like makes no sense and then I'll
have like three or four people online
have like three or four people online
telling me well it avoids side effects
telling me well it avoids side effects
and you avoid bugs because it's
and you avoid bugs because it's
declarative or whatever and it's just
declarative or whatever and it's just
like look at the code it's garbage like
like look at the code it's garbage like
I don't literally it's just doing random
I don't literally it's just doing random
stuff that's not even written there what
stuff that's not even written there what
on
Earth okay that's
Earth okay that's
good this is work this is uh working
now e
there we go these things get reward they
there we go these things get reward they
eat food I think that there was
eat food I think that there was
something weird happening in reset so
something weird happening in reset so
we're just going to watch this for a
we're just going to watch this for a
second until it resets and does the
second until it resets and does the
weird thing
yep
yep
okay we
okay we
do self. grid.
fill and hopefully this is no
good you know the sad thing about that
good you know the sad thing about that
Jack's rant is like I get mad because I
Jack's rant is like I get mad because I
waste an hour and a half on stupid
waste an hour and a half on stupid
Jack's things and I'm like this is not
Jack's things and I'm like this is not
the way forward so I write this really
the way forward so I write this really
low effort post
low effort post
right and then the next day I write an
right and then the next day I write an
actually good
post and everybody gets mad as well
post and everybody gets mad as well
everybody's mad about this stupid post
everybody's mad about this stupid post
and then the next day I write a good
and then the next day I write a good
post which is
post which is
like actual infr tear down of the puffer
like actual infr tear down of the puffer
cluster that has a lot of really cool
cluster that has a lot of really cool
information in it and it gets less than
information in it and it gets less than
a tenth of the well actually no this is
a tenth of the well actually no this is
a hund this is like a h hundredth of the
a hund this is like a h hundredth of the
exposure
so I don't know
interesting we actually got some cool
interesting we actually got some cool
replies on this
so search
space I don't buy
space I don't buy
this I don't buy this take
this
this
is sometimes
true small just
yeah maybe
yeah maybe
yeah is this yeah okay this is the Sony
yeah is this yeah okay this is the Sony
AI
guy cool
oo very nice this is old but
uh
what oh that's nothing all right let's
what oh that's nothing all right let's
see yeah this is now correct there's no
see yeah this is now correct there's no
like old artifacting
here let's uh let's train
leave a trainer on it for a couple
leave a trainer on it for a couple
seconds let me read chat uh been super
seconds let me read chat uh been super
bullish on Jacks realizing how much
bullish on Jacks realizing how much
effort I put into hackly massaging
effort I put into hackly massaging
things just to get them jetable or yes
things just to get them jetable or yes
so that is the problem with Jacks and if
so that is the problem with Jacks and if
you don't know that you need GPU and you
you don't know that you need GPU and you
just need things to be not
just need things to be not
python try cython it's awesome what if
python try cython it's awesome what if
you could just write loops and
you could just write loops and
conditionals and like arbitrary code and
conditionals and like arbitrary code and
just have it be
just have it be
c-speed but not actually have to do c
c-speed but not actually have to do c
things and like you know not like get
things and like you know not like get
actual reasonable error messages when
actual reasonable error messages when
you go to compile stuff or when runtime
you go to compile stuff or when runtime
stuff happens like it's really really
stuff happens like it's really really
nice for
nice for
that it also has very nice integration
that it also has very nice integration
with numpy so if you're doing um
with numpy so if you're doing um
scientific Computing stuff and you need
scientific Computing stuff and you need
to do like slice operations over arrays
to do like slice operations over arrays
that can be done very efficiently
but yeah the thing that you described
but yeah the thing that you described
that is the precise problem with
ja now mind you it's a little different
ja now mind you it's a little different
in my case right because we actually are
in my case right because we actually are
doing something with the GPU like we'd
doing something with the GPU like we'd
like to use the GPU
like to use the GPU
for simulating neural networks not for
for simulating neural networks not for
simulating the environment
simulating the environment
so with puffer I have the core spinning
so with puffer I have the core spinning
at 100 doing the environment and then
at 100 doing the environment and then
the GPU spinning at you know 100 doing
the GPU spinning at you know 100 doing
uh the neural net so there's a
uh the neural net so there's a
efficiency there and uh this is actually
efficiency there and uh this is actually
already training this is already
already training this is already
optimizing into
peers doesn't look like it's necessarily
peers doesn't look like it's necessarily
performing great but it's better than
performing great but it's better than
better than nothing
better than nothing
maybe is this continuous mode or
maybe is this continuous mode or
discreet
mode I forgot what I left it
mode I forgot what I left it
on uh this is discret mode
okay not
amazing do we get episode return
amazing do we get episode return
though o episode return went up and then
though o episode return went up and then
it goes back
it goes back
down that could just be hyper parameters
down that could just be hyper parameters
though let's get the uh the other tasks
though let's get the uh the other tasks
to appear to be working and uh do a few
to appear to be working and uh do a few
more things from there
yeah so this crashed
other
environments I don't know why I'm so
environments I don't know why I'm so
tired today holy
I've got two evening meetings so that's
I've got two evening meetings so that's
chill at
least uh
reward where did the dock
reward where did the dock
go you have the dock
go you have the dock
somewhere
somewhere
yeah forging Predator pre optimize group
yeah forging Predator pre optimize group
okay
so that
so that
was
was
foring we
go Predator
go Predator
prey we'll see how this does if this
prey we'll see how this does if this
um makes any sense whatsoever
oops slice IND Dees must be
integers probably should have before I
integers probably should have before I
tried to track the run I should have
tried to track the run I should have
just done um train like this
this one as well
oh did it not post this
oh did it not post this
stream maybe it didn't post the stream
stream maybe it didn't post the stream
on X I'm trying to figure out why like I
on X I'm trying to figure out why like I
was doing the same stuff yesterday and
was doing the same stuff yesterday and
it went and it was like 10 times more
it went and it was like 10 times more
views did it post it to my
views did it post it to my
timeline no it did it did that's fine I
timeline no it did it did that's fine I
guess just day two of the same project
guess just day two of the same project
as
well okay so uh reward here
that's
that's
interesting you're not necessarily going
interesting you're not necessarily going
to know what the uh thing looks like
to know what the uh thing looks like
until we have ELO of some type because
until we have ELO of some type because
Predator pray is an adversarial
task but it does train so that's
nice let's see if it looks reasonable
yes it
yes it
does uh the red ones are on the
outside okay that's interesting I didn't
outside okay that's interesting I didn't
expect that
so as is probably
fine and so Predator prey and then was
fine and so Predator prey and then was
group up is the third
one
grp which I would hope would be one of
grp which I would hope would be one of
the easier tasks
quick reminder for folks if you haven't
quick reminder for folks if you haven't
starred a puffer lib please go ahead and
starred a puffer lib please go ahead and
do that it helps me out a whole
ton helps me get all this work out there
that's really
that's really
weird
weird
observations.
observations.
shape colors.
shape colors.
shape
buff
same
group
group
some diff reward
can I just do like knot like
this
this
not I don't know if the the
not I don't know if the the
unary you got to do like this right
yeah and we get all the different pops
cool and let's just open uh
cool and let's just open uh
config Rd
config Rd
continuous we'll do this like
continuous we'll do this like
20 mil steps or
whatever and we'll see if it actually
trains let's see if this trains anything
flat negative one means I'm Computing
flat negative one means I'm Computing
the reward wrong guaranteed
the reward wrong guaranteed
let's go figure that out
oops guaranteed means that I'm Computing
oops guaranteed means that I'm Computing
it
wrong what on Earth
okay oh the rewards are being clipped
stupid
right yeah this
right yeah this
is we can clip them between
is we can clip them between
-1 and one
yeah that's
fine
um this still might not work but we'll
see this will be better at least
so it's stuck at negative
-1 because it can
see it can see um other agents around it
see it can see um other agents around it
a lot of other agents around it that are
pop let me see if it's actually flat
pop let me see if it's actually flat
negative one I think it probably
negative one I think it probably
is yeah it's flat negative one okay so
is yeah it's flat negative one okay so
we need to
do can I do like divide by 50 or
do can I do like divide by 50 or
something or do I do divide by 10 and do
something or do I do divide by 10 and do
like Nega five to
like Nega five to
five for now
now it's just stuck at neg five
all
right
H well it shouldn't be just stationary
H well it shouldn't be just stationary
right if I do buy if I divide by 50
gets worse over
time
well weird
well weird
least it's not stuck
stationary let's do over
stationary let's do over
100 and then let's just clip it from
100 and then let's just clip it from
negative 1 to
negative 1 to
one it's probably the best
thing well whether this actually helps
thing well whether this actually helps
we'll we'll see
we'll we'll see
I think it's probably getting two large
rewards oh well doesn't have to train
rewards oh well doesn't have to train
immediately we can always think about
immediately we can always think about
why it's not
why it's not
training uh we do have the the tasks
training uh we do have the the tasks
though which is
though which is
nice only one I'm missing is the puffer
nice only one I'm missing is the puffer
lib
task um I'm going to have to
task um I'm going to have to
decide a few things before I can do
that yeah I'm going to have to decide on
that yeah I'm going to have to decide on
a few
things
so thousand by th000
map let's let's change some defaults I
map let's let's change some defaults I
think we want 4096 agents not
think we want 4096 agents not
1,24 um if I do some Division if I
1,24 um if I do some Division if I
do like 1
E6 for
E6 for
4096 200 Tils per agent that's plenty
and what we do is we say Map size is
and what we do is we say Map size is
going to
going to
be 1080
oops
uh I thought I didn't have render size
uh I thought I didn't have render size
anymore do I
wait what
wait what
I'm hold
on
on
wait width height n agent
Horizon
Horizon
oh it's just going to be
with hey was talking to some dude about
with hey was talking to some dude about
your and whether or not your platform
your and whether or not your platform
could be used for agent-based
could be used for agent-based
computational economics
computational economics
um yeah I've had a lot of people ask me
um yeah I've had a lot of people ask me
that
that
um you can kind of do
um you can kind of do
it it you can get some interesting stuff
it it you can get some interesting stuff
it's very hard to train agents to
it's very hard to train agents to
actually engage with a market system uh
actually engage with a market system uh
I do have some upcoming work in this
I do have some upcoming work in this
area though um the the inspiration for
area though um the the inspiration for
is not necessarily economic I guess it
is not necessarily economic I guess it
will be somewhat interesting for that
will be somewhat interesting for that
purpose
purpose
um yeah definitely with puffer you could
um yeah definitely with puffer you could
very very easily write like an RL
very very easily write like an RL
environment that has those types of
environment that has those types of
action spaces that would be fast enough
action spaces that would be fast enough
to train to get some interesting
to train to get some interesting
behaviors for sure
should the Horizon be for this
task it's 100 ticks per
minute
so well a good way to decide that would
so well a good way to decide that would
be it takes
be it takes
512 from the
512 from the
center it's going to take like a th
center it's going to take like a th
steps to get to the
edges Y at Ker's machine learning
Discord
um you can get some I there were some
um you can get some I there were some
things in neural MMO in the
things in neural MMO in the
competition and with some of my other
competition and with some of my other
results where we actually got kind of
results where we actually got kind of
like reasonable looking supply and
like reasonable looking supply and
demand stuff to
demand stuff to
emerge where like items would actually
emerge where like items would actually
just be priced reasonably by the
just be priced reasonably by the
agents I think some of it was because
agents I think some of it was because
people were scripting it in but some of
people were scripting it in but some of
it was definitely emergent and
it was definitely emergent and
um the problems that we have with a lot
um the problems that we have with a lot
of this are less to do with the
of this are less to do with the
difficulty of that problem specifically
difficulty of that problem specifically
and more to do with the fact that like
and more to do with the fact that like
the market is one small part of a much
the market is one small part of a much
larger environment and getting agents to
larger environment and getting agents to
use a market when it's a lot one small
use a market when it's a lot one small
part of a larger environment and it's
part of a larger environment and it's
not immediately clear how it's important
not immediately clear how it's important
is difficult if you just wanted to have
is difficult if you just wanted to have
like agents to optimize on a market to
like agents to optimize on a market to
like observe interesting trends that
like observe interesting trends that
would be substantially substantially
would be substantially substantially
easier
okay so this is a nice little frame this
okay so this is a nice little frame this
has this 4096 agents in it now does that
has this 4096 agents in it now does that
look like 4096
agents guess so right
and some of them are stuck like they
and some of them are stuck like they
cannot
cannot
move because they're blocked
in and they're slowly diffusing over
in and they're slowly diffusing over
time you can
see okay so this is perfectly fine for
now one aspect we discussed was the
now one aspect we discussed was the
dictator game Nash equilibrium whether
dictator game Nash equilibrium whether
not agents engaging in sanctions
not agents engaging in sanctions
policies would lead to more peaceful
policies would lead to more peaceful
behaviors
uh so the really obnoxious thing about
uh so the really obnoxious thing about
Game Theory and RL is
Game Theory and RL is
like we solve a lot of these problems in
like we solve a lot of these problems in
the real world like we break Nash
the real world like we break Nash
equilibri in the real world by cheating
equilibri in the real world by cheating
outside of the game right like we Sol
outside of the game right like we Sol
one of the things that uh that's common
one of the things that uh that's common
like iterated prisoners dilemma for
like iterated prisoners dilemma for
inance
inance
we don't necessarily solve that problem
we don't necessarily solve that problem
in the real world we solve it by like
in the real world we solve it by like
you know making promises or threats
you know making promises or threats
outside of that game environment right
outside of that game environment right
it's not like oh tip for Tad in prison
it's not like oh tip for Tad in prison
it's hey uh if you you know if you wrap
it's hey uh if you you know if you wrap
me out you're going to get it when I get
me out you're going to get it when I get
out right
out right
um so what I would say for that type of
um so what I would say for that type of
a thing
a thing
is is the behavior you're looking for
is is the behavior you're looking for
something that is reasonably learnable
something that is reasonably learnable
in the first place cuz you don't break
in the first place cuz you don't break
Nash equilibrio without having external
Nash equilibrio without having external
factors that allow you to do so
generally there's some work with like
generally there's some work with like
Alpha star and exploiter agents um
Alpha star and exploiter agents um
that's probably relevant
here but it still relies on the game
here but it still relies on the game
having like some sort of way around it
did I get this right hold on I think I
did I get this right hold on I think I
got to update the
got to update the
config yeah cuz we're going to replace
config yeah cuz we're going to replace
uh
uh
eight 512 agent environments with 1
eight 512 agent environments with 1
14096 agent environment so this is going
14096 agent environment so this is going
to
to
be 2
M's one M fat
size this should be substantially better
size this should be substantially better
I still don't know why they're not
I still don't know why they're not
learning uh we should probably
learning uh we should probably
do the diffusion task
right do we have the diffusion task in
right do we have the diffusion task in
here
here
still we should put like scatter back in
here cuz that was kind of an interesting
here cuz that was kind of an interesting
task oops
is it reward
introverts I know I like calling it
introverts I know I like calling it
introverts it's kind of
funny
introverts yeah let's add introverts
introverts yeah let's add introverts
back to the task pool
see if it
runs okay so this does run does it
train okay so this does
train okay so this does
train now what we're going to do is
train now what we're going to do is
we're going to commit all this stuff up
we're going to commit all this stuff up
and we're train it on the big machion
and we're train it on the big machion
and we're going to figure out why this
and we're going to figure out why this
thing is not working the way we want it
thing is not working the way we want it
to
okay this is the I9 14900 uh 13900 K
okay this is the I9 14900 uh 13900 K
Machine slightly worse than the other
Machine slightly worse than the other
boxes I have and the 4090 box
so skibbidy fortnite
I don't know why that's funny to me but
I don't know why that's funny to me but
that is funny
[Music]
we run this 120 I think 125 mil was the
we run this 120 I think 125 mil was the
original setting
right let's
do Rd continuous
if this does not hit a million uh steps
if this does not hit a million uh steps
per second then we have some
per second then we have some
optimization that's
wrong okay it's close but we probably
wrong okay it's close but we probably
have some a little a little bit more
have some a little a little bit more
optimization to do because it should
optimization to do because it should
hit a million and I can see that the
hit a million and I can see that the
environment is not at
environment is not at
0% though it is it is decreasing so
0% though it is it is decreasing so
maybe
maybe ah the reward is actually maybe
maybe ah the reward is actually maybe
improving let's
see puff
grid little bit of patience
right the reward is going to end up
right the reward is going to end up
being cyclic I think
being cyclic I think
because
because
um at the start of the environment it's
um at the start of the environment it's
bad and at the end of the environment
bad and at the end of the environment
it's good so at the start of the episode
it's good so at the start of the episode
um all the agents are stuck together and
um all the agents are stuck together and
then they spread out so this is going to
then they spread out so this is going to
like be pretty Jagged but
like be pretty Jagged but
um hopefully
reasonable is this sp3 based
reasonable is this sp3 based
definitely not sp3 based this is puffer
VEC in fact this whole environment would
VEC in fact this whole environment would
be like 20 times slower just because of
be like 20 times slower just because of
the uh the petting zoo API if I were
the uh the petting zoo API if I were
using that this is the advanced puffer
using that this is the advanced puffer
version nope the model is not compiled
version nope the model is not compiled
uh I can make it faster than this
uh I can make it faster than this
anyways I'm sure even without doing that
anyways I'm sure even without doing that
cuz I have 1 million plus on snake I'm
cuz I have 1 million plus on snake I'm
sure it just needs a little bit of
sure it just needs a little bit of
tuning in fact let me just look what's
tuning in fact let me just look what's
what do I do different with snake
what do I do different with snake
here snake
has uh it looks like snake just has a
has uh it looks like snake just has a
bigger mini batch
bigger mini batch
size is that
size is that
all
so num Ms num
so num Ms num
workers yeah bigger Mini bch which
workers yeah bigger Mini bch which
actually means I could probably make it
actually means I could probably make it
even
even
faster substantially over a
million what did I do
million what did I do
here yeah so 4096 would be the limit I'm
here yeah so 4096 would be the limit I'm
assuming
assuming
um 42% forward pass that's why
maybe this is not
learning yeah this reward function
learning yeah this reward function
doesn't look too
good so the question is what did I
good so the question is what did I
break we definitely had this learning
break we definitely had this learning
before and this should be a very easy
before and this should be a very easy
task
well we'll go look at that right
first of
all let's just get the speed of this to
all let's just get the speed of this to
be something you know a little bit more
be something you know a little bit more
reasonable and let's just borrow the
reasonable and let's just borrow the
snake uh the snake settings because I'm
snake uh the snake settings because I'm
assuming they're probably going to be
assuming they're probably going to be
decent
right so for
right so for
snake we do a batch size
snake we do a batch size
of seems dated I'm pretty sure it was
of seems dated I'm pretty sure it was
larger than this but 131 we'll use 131k
larger than this but 131 we'll use 131k
for now this should be simpler than
for now this should be simpler than
snake anyways we'll do
snake anyways we'll do
32k batch
32k batch
here gamma's fine this is fine entropy
here gamma's fine this is fine entropy
is really high
is really high
here do
here do
o 01 or O2
o 01 or O2
maybe do
O2 learning rate of 0.003
it's actually
it's actually
fine so let's start with
this okay there you go there's your 1
this okay there you go there's your 1
million steps per
second it's funny we can actually make
second it's funny we can actually make
this way way faster if we just use more
this way way faster if we just use more
agents
I'm kind of
tempted
um 32k
um 32k
reasonable 32,000
reasonable 32,000
agents I don't know what would be a
agents I don't know what would be a
good or optimal batch size for this
just for
just for
fun yeah let's not do this tracking
fun yeah let's not do this tracking
stuff
right
sh let's say that we're going to make
sh let's say that we're going to make
this 32,000 agents right
this 32,000 agents right
7 uh 8 768 like this
how fast do this
train only
1.2 it's kind of weird isn't
1.2 it's kind of weird isn't
it Forward pass is still slow
very
slow I wonder if that's the data
slow I wonder if that's the data
transfer
transfer
right but I thought I'd optimized it
right but I thought I'd optimized it
pretty well
well yeah it's TR we're going to fix the
well yeah it's TR we're going to fix the
training
separately I want to make sure that I
separately I want to make sure that I
have the Sim running at a good speed
have the Sim running at a good speed
first and then I have it op like
first and then I have it op like
Hardware
optimal uh 32k agents right
why would that be
why would that be
slow
continuous I think I defined this as un8
continuous I think I defined this as un8
didn't
didn't
I the OB
slices yeah Vision range is
slices yeah Vision range is
five so these observations are very
tiny I mean I can't say it's very tiny
tiny I mean I can't say it's very tiny
cuz it's
cuz it's
32,000 t
32,000 t
time what's that
time what's that
100ish by
100ish by
11 so 32 KB 3
MB where's the Ford pass time
I no wait this is in MK right so the two
I no wait this is in MK right so the two
device time is actually just in
MK
um Place update should be faster
is
is
14% which is still
high why is
it why is it that the Ford passes this
slow it was was made substantially
slow it was was made substantially
faster right before training was made
faster right before training was made
substantially
faster well
definitely some room for
optimizations this was faster than
optimizations this was faster than
before or
no I don't actually want to mess with
no I don't actually want to mess with
the we're going to take the uh the one
the we're going to take the uh the one
mil version for
mil version for
now because this this number of Agents
now because this this number of Agents
can actually start to mess stuff up like
can actually start to mess stuff up like
this this can actually start to mess up
this this can actually start to mess up
a lot of uh tricky to track down
a lot of uh tricky to track down
things just the way the memory works
out
continuous mini batch is
fine uh Eight's probably fine
frankly
frankly
grid
grid
entropy yeah so all these things are fun
fine
fine
so next debug
check orops next debug check is going to
be the
Horizon so we're going to set a nice
Horizon so we're going to set a nice
very short Horizon
let's do Dash Dash Track as well and to
let's do Dash Dash Track as well and to
set a nice short
Horizon and that should make it easier
Horizon and that should make it easier
to learn a little bit
we will see whether this is the case uh
we will see whether this is the case uh
as a matter of
fact the reward does seem to be
fact the reward does seem to be
improving quite consistently
right yes
is it getting worse
again or is it just
stable all just
say so regardless though it is learning
something entropy curve is
something entropy curve is
fine clip Frack is a little high but
fine clip Frack is a little high but
probably
fine uh this is L
crashing odd for an environment like
crashing odd for an environment like
this to be
this to be
unstable very
odd environment like should this should
odd environment like should this should
be very
be very
stable does seem to be learning back a
stable does seem to be learning back a
little
bit let's see the curves
it does learn back a little bit so it's
it does learn back a little bit so it's
very weird how it's not fully stable
very weird how it's not fully stable
like
like
this um you can see the entropy crash
this um you can see the entropy crash
down pretty
down pretty
bad we'll have to do a sweep on
this and this is on the easy
version easy scatter environment
Let's uh let's see what this policy
Let's uh let's see what this policy
actually does
right this
model e
okay that's actually very
okay that's actually very
reasonable wait hold on that's very
reasonable wait hold on that's very
reasonable
reasonable
holy what the
[Laughter]
[Laughter]
heck that's
heck that's
sweet what the heck
y
y
uh okay and then like they screw up
uh okay and then like they screw up
because they all accumulate on the edge
because they all accumulate on the edge
but
uh that's kind of
cool it's so funny that they just chill
cool it's so funny that they just chill
down there as well they don't they
down there as well they don't they
didn't learn anything else other than
didn't learn anything else other than
that
okay let's let's actually get a
okay let's let's actually get a
recording of
recording of
that yeah but did you see the way it
that yeah but did you see the way it
separated it was kind of
separated it was kind of
cool it like they kind of like did this
cool it like they kind of like did this
diagonal separation so they had like
diagonal separation so they had like
they were moving across each other it
they were moving across each other it
was kind of cool
but I will steal that for
but I will steal that for
uh I will steal that as a
quote so that is going to be what 50
frames
frames
at 15
at 15
FPS we'll do
FPS we'll do
grid grid
grid grid
that
GI just want to watch this thing it's so
cool
like I mean they that actually really
like I mean they that actually really
looks like
sand you're right that's that's kind of
cool that's like a really convincing
cool that's like a really convincing
sand
sand
simulation isn't it it's like if you
simulation isn't it it's like if you
just hooked a pile of sand isn't
it
for e
this actually yeah okay cool
that looks really
sweet I mean it's like a nothing but it
sweet I mean it's like a nothing but it
it's still
funny people got to stop liking the
funny people got to stop liking the
stupid
post
post
bad okay uh
I really didn't expect that I like I
I really didn't expect that I like I
really didn't expect that to happen
really didn't expect that to happen
that's kind of weird that that worked
that's kind of weird that that worked
that
that
way
way
yeah
um what do we do next
um what do we do next
here well
here well
because I'll show you why so before I
because I'll show you why so before I
made this I I messed with this
made this I I messed with this
environment right I had like a similar
environment right I had like a similar
version of the same thing
well first of all the reward the reward
well first of all the reward the reward
curves didn't look very good
curves didn't look very good
right
right
and where is
it my darn
posts freaking thing
is there a command like I don't think
is there a command like I don't think
there's a Twitter command on YouTube is
there it's j Wars
there it's j Wars
5341 okay so this was
5341 okay so this was
the the file you this grid environment
the the file you this grid environment
so this is what happened when I trained
so this is what happened when I trained
it
it
before you see like they end up in these
before you see like they end up in these
weird
clusters so I don't know what happened
clusters so I don't know what happened
like what I changed that makes them not
like what I changed that makes them not
end up in these
clusters but this is what happened like
clusters but this is what happened like
after I tuned it a little bit um with
after I tuned it a little bit um with
the original version of the
project there are a few different things
project there are a few different things
like this this is uh a slightly higher
like this this is uh a slightly higher
per version of the above one so you can
per version of the above one so you can
see they spread out in two directions
see they spread out in two directions
but they still end up in these clusters
but they still end up in these clusters
these clumps and then if you use a
these clumps and then if you use a
really expensive to compute reward
really expensive to compute reward
function then you get something that
function then you get something that
looks a little bit more like this but I
looks a little bit more like this but I
didn't want to do this one cuz this
didn't want to do this one cuz this
one's really really
slow yeah and centralized ukan
slow yeah and centralized ukan
distance you get like this
thing and you can sometimes get him to
thing and you can sometimes get him to
like move in this amorphous mass as well
like move in this amorphous mass as well
with weird Rewards
um I guess we can try training one of
um I guess we can try training one of
the other
the other
ones just to see like how far off base
ones just to see like how far off base
we
we
are I also want to think about the time
are I also want to think about the time
Horizon a little
bit I think it's kind of fine as
bit I think it's kind of fine as
is it would be more stable if I cut the
is it would be more stable if I cut the
timer Horizon to 512
timer Horizon to 512
though I'm going to cut it to 512 just
though I'm going to cut it to 512 just
while I'm
deving it'll just make it a little bit
deving it'll just make it a little bit
easier because like they won't they're
easier because like they won't they're
wasting half of their training data
wasting half of their training data
now oh wow that's funny so this was the
now oh wow that's funny so this was the
one that I trained with 64 Horizon and I
one that I trained with 64 Horizon and I
guess it just
guess it just
generalized huh that's
generalized huh that's
cool we'll leave it on this for now and
cool we'll leave it on this for now and
we'll do
we'll do
[Music]
um
um
ocean uh great
ocean uh great
continuous
slash
slash
continuous and we're going to change the
continuous and we're going to change the
task
task
to let's do the foraging task
all
all
right we'll see how this one
goes okay reward goes
up reward does go
up so this will be done this is 125
up so this will be done this is 125
million steps training to put this into
million steps training to put this into
perspective and it finishes in 2 minutes
perspective and it finishes in 2 minutes
um when I was starting in reinforcement
um when I was starting in reinforcement
learning we would train stuff on
learning we would train stuff on
Atari people still do this but we would
Atari people still do this but we would
train stuff on Atari that was like the
train stuff on Atari that was like the
most common test suite and it would
most common test suite and it would
train
train
at like 1,000th maybe a couple
at like 1,000th maybe a couple
thousandths of this speed so You' do a
thousandths of this speed so You' do a
10 million step run and it would take
10 million step run and it would take
like multiple hours to do a 10 million
like multiple hours to do a 10 million
step Run 100 million step run in 2
step Run 100 million step run in 2
minutes not Apples to Apples but very
minutes not Apples to Apples but very
very nice uh this one actually appears
very nice uh this one actually appears
to be very stable
to be very stable
at least with the very short Horizon
at least with the very short Horizon
that I've given
that I've given
it so I'm hoping that this gives us some
it so I'm hoping that this gives us some
sort of interesting
sort of interesting
policy this should be a very easy task
policy this should be a very easy task
to be fair no reason for this task to be
to be fair no reason for this task to be
expected to be hard but um yeah we
expected to be hard but um yeah we
should just get sort of get an idea of
should just get sort of get an idea of
uh get an idea out of
uh get an idea out of
this and then we'll run some sweeps and
this and then we'll run some sweeps and
we'll debug the environments and stuff
we'll debug the environments and stuff
over the next couple of days and uh
over the next couple of days and uh
we'll figure out
we'll figure out
all the questions that we wanted to
all the questions that we wanted to
answer with it about continuous versus
answer with it about continuous versus
discreet was a discreet version just
discreet was a discreet version just
because I know it's going to be easier
because I know it's going to be easier
in
testing but we'll be able to do a lot
testing but we'll be able to do a lot
from
here and that is
here and that is
completed let's just make sure it didn't
completed let's just make sure it didn't
um
crash no it didn't crash actually got a
crash no it didn't crash actually got a
little better towards the end even
little better towards the end even
so pretty nice
so pretty nice
policy learns in 2
minutes and we have a model that is
minutes and we have a model that is
ready for
ready for
us this is public by the way so you can
us this is public by the way so you can
just grab my models if you
want uh this one's going to to be forg
PT I got to change the task to
PT I got to change the task to
forge haven't added a hook for this yet
forge haven't added a hook for this yet
so we'll just do it in the code
I think just G continuous
holy well that's
cool
huh for
so weird how it does this
so weird how it does this
like yeah I don't know it's
fun let's do
fun let's do
um let's do one other one a herting
um let's do one other one a herting
task uh your wish is my command let me
task uh your wish is my command let me
try that one cuz we actually already
try that one cuz we actually already
have
have
it it's a predator prey which is
it it's a predator prey which is
basically that
now the initial conditions for this are
now the initial conditions for this are
a little
screwy cuz the enemies start like around
screwy cuz the enemies start like around
the prey so I don't know if it's going
the prey so I don't know if it's going
to do anything cool or not like I might
to do anything cool or not like I might
have just screwed up this environment
reward goes
up ah so reward going like this is not
up ah so reward going like this is not
necessarily bad
necessarily bad
because it's an adversarial task
because it's an adversarial task
so if the prey does better the Predator
so if the prey does better the Predator
does worse and if the Predator does
does worse and if the Predator does
better then the prey does worse and
better then the prey does worse and
actually the rewards are like one is the
actually the rewards are like one is the
negative of the other so in order for
negative of the other so in order for
one of them to do better the other has
one of them to do better the other has
to do worse
to do worse
it's I don't know if it's quite Zero Sum
it's I don't know if it's quite Zero Sum
I think it might be Zero
Sum anyways I have no idea what this is
Sum anyways I have no idea what this is
going to do
going to do
so let's see
one of my favorite things about this
one of my favorite things about this
type of work is you can be really stupid
type of work is you can be really stupid
about
about
it like I don't have to know what the
it like I don't have to know what the
hell's going to happen um I just have to
hell's going to happen um I just have to
make a fast Sim make sure the Sim is
make a fast Sim make sure the Sim is
right and then I mean this is just me
right and then I mean this is just me
playing around with it because it's fun
playing around with it because it's fun
um since I just built it and kind of
um since I just built it and kind of
want to get a feel for it but I'm going
want to get a feel for it but I'm going
to go run a bunch of hyper parameter
to go run a bunch of hyper parameter
sweeps on this stuff and we're going to
sweeps on this stuff and we're going to
figure out
figure out
exactly how we should be running the
exactly how we should be running the
bigger tasks like what what settings and
bigger tasks like what what settings and
stuff we're going to get way better
stuff we're going to get way better
policies out of it and then we're going
policies out of it and then we're going
to do some actual science based off of
to do some actual science based off of
that so that's going to be real
cool million steps per second training
all I guess I should emphasize just like
all I guess I should emphasize just like
how unheard of it would have
how unheard of it would have
been up to very very recently for me to
been up to very very recently for me to
be doing like hundred some odd million
be doing like hundred some odd million
step experiments live because they take
step experiments live because they take
two minutes
technically I could load like the forge
technically I could load like the forge
model with the Predator pre task or
model with the Predator pre task or
whatever but I don't think it would do
whatever but I don't think it would do
anything cool
[Laughter]
[Laughter]
did you see
did you see
that so I guess I did it backwards so I
that so I guess I did it backwards so I
guess the prey is the
guess the prey is the
red
red
um which is kind of better so the prey
um which is kind of better so the prey
they just like they run off this
they just like they run off this
way and then the Predators go wait
way and then the Predators go wait
they're escaping
oops
yeah yeah well it's an undertrained
yeah yeah well it's an undertrained
model right like undertrained models
model right like undertrained models
develop all these weird directional
biases um the one I would like to do
biases um the one I would like to do
actually there is one that would maybe
actually there is one that would maybe
be a little bit more interesting because
be a little bit more interesting because
we didn't do there's one other
we didn't do there's one other
one yeah you're going to get way better
one yeah you're going to get way better
results with longer training with better
results with longer training with better
hyper parameters just all sorts of stuff
hyper parameters just all sorts of stuff
that we can this is just like we're
that we can this is just like we're
having fun here this is just like
having fun here this is just like
getting a sense of whether I made you
getting a sense of whether I made you
know remotely reasonable environments in
know remotely reasonable environments in
the first place that kind of do
the first place that kind of do
something that looks decent before I go
something that looks decent before I go
burn some compute on sweeps it's not
burn some compute on sweeps it's not
even burning that much compute on this
even burning that much compute on this
like what 10 minutes for 500 mil
like what 10 minutes for 500 mil
something like that so I can
something like that so I can
do uh 12 experiments per
do uh 12 experiments per
hour so I mean I could do you know 100x
hour so I mean I could do you know 100x
sweep uh over these in a few hours
sweep uh over these in a few hours
whatever
whatever
it's yeah we're going to be able to do
it's yeah we're going to be able to do
lots of cool
lots of cool
experiments like I said on Twitter we're
experiments like I said on Twitter we're
going to run like a 100 billion sweep or
going to run like a 100 billion sweep or
something on this and that's going to be
something on this and that's going to be
very easily doable a million at a
very easily doable a million at a
million steps per second
train uh where is it reward group
yeah
group so this one they shouldn't run
group so this one they shouldn't run
away from each other but I wouldn't be
away from each other but I wouldn't be
surprised if they do run
surprised if they do run
away
away
um I don't know we'll
um I don't know we'll
see I don't have to know I can just run
see I don't have to know I can just run
the experiment and find out
right what are you building I am
right what are you building I am
building a an engine for continuous
building a an engine for continuous
control with uh reinforcement learning
control with uh reinforcement learning
it is you well I have some examples on
it is you well I have some examples on
Twitter here that I just posted cool to
Twitter here that I just posted cool to
see the first twitch M uh twitch chat
see the first twitch M uh twitch chat
message as well I just added that as a
message as well I just added that as a
streaming platform so I've got these
streaming platform so I've got these
tasks that look like this where each of
tasks that look like this where each of
these pixels is an agent this is
these pixels is an agent this is
predator prey with the gray ones trying
predator prey with the gray ones trying
to chase the red ones this one they're
to chase the red ones this one they're
trying to eat all the blue food this one
trying to eat all the blue food this one
they're just trying to get away from
they're just trying to get away from
each other and uh really brand new
each other and uh really brand new
project just started this in the last
project just started this in the last
couple of days but the goal is ultra
couple of days but the goal is ultra
high performance reinforcement learning
high performance reinforcement learning
and uh there's some additional
and uh there's some additional
scientific stuff here like I want to do
scientific stuff here like I want to do
continuous and discreet uh with the same
continuous and discreet uh with the same
environments this will let us uh gain
environments this will let us uh gain
some insights as to whether training
some insights as to whether training
robots is fundamentally harder than like
robots is fundamentally harder than like
training RL to play games stuff like
training RL to play games stuff like
that um but we're going to get to that
that um but we're going to get to that
over the next couple of days once this
over the next couple of days once this
gets a little bit more wellb built
out it's fun fun
stuff probably is the first follow on uh
stuff probably is the first follow on uh
on Twitch I think you can see the chat
on Twitch I think you can see the chat
overlay this is multi streamed to Twitch
overlay this is multi streamed to Twitch
YouTube and uh
YouTube and uh
Twitter so the twitch is uh entirely
Twitter so the twitch is uh entirely
new so thank you for the
interest I don't know how the twitch
interest I don't know how the twitch
page looks I just set it up real quick
page looks I just set it up real quick
the other day
and this things they take uh about 2
and this things they take uh about 2
minutes to train 125 million simulation
minutes to train 125 million simulation
steps with a little neural network and
steps with a little neural network and
little
little
lstm this one's almost
lstm this one's almost
done let's actually open up the logs on
done let's actually open up the logs on
this to
see did this train anything
reasonable yeah the reward went up
reasonable yeah the reward went up
did something it looks
did something it looks
like so actually this not so bad for
like so actually this not so bad for
first try haven't optimized anything um
first try haven't optimized anything um
the models trained for like four
the models trained for like four
different tasks that I set up I kind of
different tasks that I set up I kind of
guessed on everything and it still works
guessed on everything and it still works
at least somewhat
Works uh group. PT
think Wan has like Cloud flare or
think Wan has like Cloud flare or
something because it takes way too long
something because it takes way too long
to curl
stuff it's 12 second
timeout okay so we're going to do
R and do they do anything
what
what
uh what how do you decide a agent
uh what how do you decide a agent
architecture I will show you the agent
architecture I will show you the agent
architecture it's mainly common sense
architecture it's mainly common sense
um but it's highly optimized for Speed
um but it's highly optimized for Speed
this is this this is almost The
this is this this is almost The
Identical architecture that I use for
Identical architecture that I use for
multi snake actually we've got the uh
multi snake actually we've got the uh
the fellow still on Twitch
the fellow still on Twitch
I'll give you a reason to check out this
I'll give you a reason to check out this
content uh the snake project that I
content uh the snake project that I
built this is what something I've done
built this is what something I've done
that this took like an actual week worth
that this took like an actual week worth
of work if I can find my snake
of work if I can find my snake
project on I posted it just a few days
project on I posted it just a few days
ago
ago
a yeah Okay so we've got snake where you
a yeah Okay so we've got snake where you
have reinforcement learning with snake
have reinforcement learning with snake
but lots of snakes
and this here is is reinforcement
and this here is is reinforcement
learning with 4096 snakes lots of snakes
learning with 4096 snakes lots of snakes
do I have the local one it would look
do I have the local one it would look
way cooler if I just open the
way cooler if I just open the
local this do it this is one of the
local this do it this is one of the
gifts
gifts
nope yeah there we
go snake with 496 snakes trained with
go snake with 496 snakes trained with
reinforcement
learning so the agent architecture I
learning so the agent architecture I
came up with for that was reused for
came up with for that was reused for
this project that I that uh I'm using
this project that I that uh I'm using
now which is to answer the
question it's right here yes
question it's right here yes
architecture choice is very important
architecture choice is very important
within some some bounds of acceptable
within some some bounds of acceptable
stuff so this is two
stuff so this is two
comps comp with size five filters comp
comps comp with size five filters comp
with size three
with size three
filters um and then there is a linear
filters um and then there is a linear
layer they're in
layer they're in
between the actor and the value function
between the actor and the value function
are both uh individual just linear
are both uh individual just linear
layers very simple and the nice part
layers very simple and the nice part
about this is this is recurrent it does
about this is this is recurrent it does
have an lstm on here I believe it is a
have an lstm on here I believe it is a
128 dimensional lstm so very very small
128 dimensional lstm so very very small
policy 150,000 or so parameters um but
policy 150,000 or so parameters um but
the nice thing with it is I can train it
the nice thing with it is I can train it
at a million steps per second so I can
at a million steps per second so I can
just this thing just inhales data I
just this thing just inhales data I
trained snake on 10 billion steps of
trained snake on 10 billion steps of
course it was a pretty good policy after
course it was a pretty good policy after
like 5 minutes of training but you know
like 5 minutes of training but you know
for the sake of it I just kept training
for the sake of it I just kept training
it for 10 billion to see if it would
it for 10 billion to see if it would
learn anything else cool and it kind of
learn anything else cool and it kind of
did and um we're using basically the
did and um we're using basically the
same architecture for this right now as
well so what the heck just happened with
this I did not expect
this I did not expect
this um the task at at least I thought
this um the task at at least I thought
the task was for the ones that are the
the task was for the ones that are the
same color to group together and the
same color to group together and the
ones that are different color to spread
ones that are different color to spread
apart
apart
and instead we have created a water
and instead we have created a water
spout
thing I and the reward is going up as
thing I and the reward is going up as
well
so is it just me or they like locally
so is it just me or they like locally
assembling themselves to get
close to the same
H well it's probably the case that
H well it's probably the case that
you're that I'm really crunching
you're that I'm really crunching
learning here by
learning here by
um making them spawn right next to each
um making them spawn right next to each
other
other
huh why use lstm versus RNN versus
huh why use lstm versus RNN versus
Transformer you never use a vanilla
Transformer you never use a vanilla
recurrent neural network um it very
recurrent neural network um it very
Vanishing gradients problem very bad it
Vanishing gradients problem very bad it
it essentially has no ability to train
it essentially has no ability to train
over reasonable contexts Transformer is
over reasonable contexts Transformer is
heavier and uh very hard to implement in
heavier and uh very hard to implement in
an RL context because of the way that
an RL context because of the way that
you collect data so some people have had
you collect data so some people have had
a little bit of success with them for uh
a little bit of success with them for uh
like small scale online RL but generally
like small scale online RL but generally
the standard is uh lstm
the standard is uh lstm
here yeah they can't occupy the same
here yeah they can't occupy the same
space Lan key so the same color ones are
space Lan key so the same color ones are
blocking I'm going to make a little
tweaks yeah I'm going to make a couple
tweaks yeah I'm going to make a couple
tweaks we're going to
do times it 324 still
good did I forget lunch
good did I forget lunch
again I'm going
again I'm going
to get this experiment set up to train
to get this experiment set up to train
and then I'm going to go make my myself
and then I'm going to go make my myself
a shake for like 2 minutes keep
a shake for like 2 minutes keep
forgetting to eat damn
it man's got to
eat but I'm going to get this thing to
eat but I'm going to get this thing to
train so that it'll take two minutes to
train so that it'll take two minutes to
train and then I'll come back and I'll
train and then I'll come back and I'll
have um you know I'll have it done and
have um you know I'll have it done and
then we'll finish up the
then we'll finish up the
stream if I don't do it now I'm not
stream if I don't do it now I'm not
going to have time before my
meeting where was the gen was it spawn
positions so this is going to be every
positions so this is going to be every
fourth
position okay perfect be right back
position okay perfect be right back
going to go make my shake one
sec
e
e
e
e
e e
okay missing meals normally is bad
okay missing meals normally is bad
missing meals when you start your day
missing meals when you start your day
with two to three hours of
with two to three hours of
exercise is absolutely atrocious
let's go grab that
policy
policy
o well this is a little bit
o well this is a little bit
odd do we have a crash we had an entropy
odd do we have a crash we had an entropy
crash this might not be particularly
crash this might not be particularly
good because it looks like it was doing
good because it looks like it was doing
well and then we got the entropy crash
well and then we got the entropy crash
but I'll I'll open up the
but I'll I'll open up the
model uh anyways and then what I'm going
model uh anyways and then what I'm going
to do is while I'm opening the model I'm
to do is while I'm opening the model I'm
going to just train this for
longer so we're going to
longer so we're going to
do uh 300
do uh 300
mil while I'm doing the analysis on this
mil while I'm doing the analysis on this
one yeah I should have time for that
one yeah I should have time for that
that's
fine let's do
oh it's the same yeah
um
yeah the multi stream thing is cool
let's see if this does
anything um
what is it just me or did they oh wait
what is it just me or did they oh wait
they didn't spawn correctly because I
they didn't spawn correctly because I
didn't pull that change down
didn't pull that change down
right what you passed for the Target
right what you passed for the Target
features on unseen new data when the
features on unseen new data when the
target is
target is
unknown what do you mean the target
feature um I I don't know what you mean
feature um I I don't know what you mean
by the Target feature
uh I did what was it star star uh col
uh I did what was it star star uh col
col 4
col 4
right yeah that'll work
oh wow that's
oh wow that's
weird uh
weird uh
okay I guess that's the starting
pattern did not expect that but okay
let's see if we have a better
let's see if we have a better
model this one has been training
model this one has been training
right does the crash get
reproduced yes the crash
reproduced yes the crash
reproduces it's interesting that it does
then it starts going back
up 9
up 9
38 yeah so it's about where it is now
38 yeah so it's about where it is now
doesn't really seem to be improving much
doesn't really seem to be improving much
from here
from here
though maybe 9
though maybe 9
28 yeah it was improving back
okay oh yeah there
okay oh yeah there
goes it's a little
goes it's a little
jumpy I'm wondering if I specified this
jumpy I'm wondering if I specified this
task
correctly this was the group task
correctly this was the group task
right
right
so you get rewarded
for seeing agents of the same
color and you get Negative ly rewarded
color and you get Negative ly rewarded
for seeing agents of a different
color but the conditioning on this
color but the conditioning on this
reward is very
reward is very
weird so the probably this the probably
weird so the probably this the probably
the reason that it's so unstable is that
the reason that it's so unstable is that
the reward magnitude is too
the reward magnitude is too
big um I had to divide by 100 and it's
big um I had to divide by 100 and it's
still clipping to negative 1 and one
still clipping to negative 1 and one
very
very
often like this negative .9 is way too
often like this negative .9 is way too
big for a reward though it does seem
big for a reward though it does seem
like it has gone back up somewhat and if
like it has gone back up somewhat and if
it doesn't crash right at the end
it doesn't crash right at the end
here yeah this should
be 8 n
be 8 n
oops yeah yeah yeah this should be
oops yeah yeah yeah this should be
good to
good to
there perfect if I refresh
this oh yeah look at that perfect so
let's see what I have no idea what this
let's see what I have no idea what this
model does let's see if it learned
model does let's see if it learned
anything remotely
anything remotely
reasonable um and then I'll think about
reasonable um and then I'll think about
what I'm going to do with this next and
what I'm going to do with this next and
uh we'll go from
there quick
continuous
files copy for
okay so again I didn't mean to have them
okay so again I didn't mean to have them
start like this
start like this
but that's weird that they do
that well not exactly what I was hoping
that well not exactly what I was hoping
for
um they clearly haven't learned this one
um they clearly haven't learned this one
so
so
the other tasks may be performed
the other tasks may be performed
somewhat reasonably this one does not uh
somewhat reasonably this one does not uh
the forging one in particular leads me
the forging one in particular leads me
to believe that they are you know kind
to believe that they are you know kind
of
of
reasonable
um
yeah I think what I'm going to have to
yeah I think what I'm going to have to
do with this is I'm going to have to
do with this is I'm going to have to
fiddle with it a little bit I'm going to
fiddle with it a little bit I'm going to
have to fiddle with the different
have to fiddle with the different
Horizons I'm going to have to do a
Horizons I'm going to have to do a
couple more of these like experiments
couple more of these like experiments
type things offline and um then I'm
type things offline and um then I'm
going to have to run some hyper
going to have to run some hyper
parameter
parameter
sweeps get some reasonable
sweeps get some reasonable
defaults and then
defaults and then
hopefully that will be enough to get
hopefully that will be enough to get
some
some
visually compelling results and we can
visually compelling results and we can
do the continuous
do the continuous
versions you know just for the sake of
versions you know just for the sake of
it let's just see if anything happens to
it let's just see if anything happens to
work immediately with the uh The
work immediately with the uh The
Continuous forging one
Continuous forging one
right so I have a little bit of time
right so I have a little bit of time
that'll still give me a few minutes
that'll still give me a few minutes
before my next
meeting let's
do
do
foraging so we're going to do the
foraging so we're going to do the
foraging task CU I think that's going to
foraging task CU I think that's going to
be the most consistent one
and we're going to set the
and we're going to set the
continuous we're going to set continuous
continuous we're going to set continuous
to true so discretise be
false action
false action
spaces oh right I didn't add it to this
spaces oh right I didn't add it to this
model okay so that's going to be hard
I thought I was going to be able to do
I thought I was going to be able to do
that easily I have to change a couple
that easily I have to change a couple
things to support it in the new model
things to support it in the new model
that's fine
that's fine
um okay in that case then let's do it
um okay in that case then let's do it
something slightly different let's do
something slightly different let's do
Horizon 512 300 mil forging and let's
Horizon 512 300 mil forging and let's
see if it learns anything and I want to
see if it learns anything and I want to
fix
the I'd like to also fix the uh the the
the I'd like to also fix the uh the the
initial condition
how the heck did it happen to be
this colon colon
4 yeah this thing how this happen
H
H
-12 I think if I put it here it'll do
-12 I think if I put it here it'll do
it yeah this this should do what I want
it yeah this this should do what I want
if I put it here I
think
think
yeah let's try this
and then we'll commit everything
up for new folks here if you'd like to
up for new folks here if you'd like to
support my work all I ask is that you
support my work all I ask is that you
star the puffer repo on GitHub helps me
star the puffer repo on GitHub helps me
out a whole bunch I work on
out a whole bunch I work on
reinforcement learning stuff making
reinforcement learning stuff making
everything a lot easier and more
everything a lot easier and more
consistent and faster fulltime that
consistent and faster fulltime that
helps me quite a
bit and I also post a bunch of this
bit and I also post a bunch of this
stuff on
stuff on
Twitter under JS 5341 same
handle let's see if we've got any cool
graphs okay so the reward goes up and
graphs okay so the reward goes up and
then I think it might crash back
then I think it might crash back
down is
that
that
maybe we'll just have to wait on it for
maybe we'll just have to wait on it for
a second
I made this longer Horizon and I made
I made this longer Horizon and I made
better initial conditions uh when you
better initial conditions uh when you
spawn all the agents right next to each
spawn all the agents right next to each
other and they can't move that makes it
other and they can't move that makes it
really
hard I think I might want to change this
hard I think I might want to change this
environment up quite a bit as well
um the best the best thing you can do
um the best the best thing you can do
for learning stability with these like
for learning stability with these like
big multi-agent Sims is to make it so
big multi-agent Sims is to make it so
that the uh the agents respawn
that the uh the agents respawn
periodically so that you get uh
periodically so that you get uh
constantly diverse data like when you
constantly diverse data like when you
have a big chunk of data that's like all
have a big chunk of data that's like all
the agents at the start of the n and
the agents at the start of the n and
then all the agents in the middle all
then all the agents in the middle all
the agents at the end that destabilizes
the agents at the end that destabilizes
stuff if you can make like a nice smooth
stuff if you can make like a nice smooth
thing where you have different agents at
thing where you have different agents at
different parts all the time that makes
different parts all the time that makes
it a lot easier to
learn so like every time it resets here
learn so like every time it resets here
it screws it up if I were to just like
it screws it up if I were to just like
respawn food or whatever and just
respawn food or whatever and just
randomly spawn agents it would be a lot
randomly spawn agents it would be a lot
easier
though it is cool to see them spread out
though it is cool to see them spread out
like this
sand I'm seeing o1 reward constantly
sand I'm seeing o1 reward constantly
here how is it that this is so far
here how is it that this is so far
behind maybe it's rounding
X features
X features
equals hold on let me see I just saw
equals hold on let me see I just saw
your thing X train equals
features wait X train equals
features wait X train equals
features X test
features X test
equals features to predict has same
equals features to predict has same
shape prediction
equals okay
equals okay
so the syntax that you're
so the syntax that you're
using
using
um reminds me of like ml courses and
um reminds me of like ml courses and
stuff so I'm not sure if this is
stuff so I'm not sure if this is
intended specifically for re enforcement
intended specifically for re enforcement
or for learning or for machine learning
or for learning or for machine learning
in
in
general um but the train and test thing
general um but the train and test thing
is a little bit different here so you're
is a little bit different here so you're
interacting with an
interacting with an
environment all of your data comes from
environment all of your data comes from
this simulator this game it has a
this simulator this game it has a
specific shape the shape of the data is
specific shape the shape of the data is
in this case uh a crop from like the
in this case uh a crop from like the
pixels nearby or whatever if you want to
pixels nearby or whatever if you want to
think of it that way little 11 by 11
think of it that way little 11 by 11
window so it's technically 11 * 11 uh
window so it's technically 11 * 11 uh
numbers and when you collect that data
numbers and when you collect that data
by interacting with the environment
by interacting with the environment
you're running the lstm for one time
you're running the lstm for one time
step so you have a state you get the
step so you have a state you get the
state from the last step you feed that
state from the last step you feed that
into the LST lstm you feed the current
into the LST lstm you feed the current
observation in which is the current set
observation in which is the current set
of crops of Windows you feed that in and
of crops of Windows you feed that in and
then you get the updated state to use
then you get the updated state to use
after you've updated the simulator and
after you've updated the simulator and
then when you train this thing you
then when you train this thing you
collect all the data that you've
collect all the data that you've
uh you've gotten from the simulator and
uh you've gotten from the simulator and
you put it together into
you put it together into
sequences and then you run the lstm over
sequences and then you run the lstm over
those
those
sequences the source code for this is on
sequences the source code for this is on
puffer lib you can also see a slightly
puffer lib you can also see a slightly
simpler example example in clean
simpler example example in clean
RL but if you look at uh like clean puff
RL but if you look at uh like clean puff
RL here this is my main
RL here this is my main
transcript that I wrote for this it's
transcript that I wrote for this it's
based on clean
RL the lstm call is is
right where is
it yeah right here so the lstm calls
it yeah right here so the lstm calls
right
right
here the data shaping stuff is mostly in
here the data shaping stuff is mostly in
the experience buffer but um you know we
the experience buffer but um you know we
have the flattening of experience here
have the flattening of experience here
an experience buffer and then the
an experience buffer and then the
collection of data with the lstm and the
collection of data with the lstm and the
simulator is you run the simulator
simulator is you run the simulator
you run the policy with the lstm and
you run the policy with the lstm and
then you go back to run the simulator
then you go back to run the simulator
run the policy with the lstm because the
run the policy with the lstm because the
data comes from the simulator don't know
data comes from the simulator don't know
if that answers your
question okay so this trained something
question okay so this trained something
and the reward is pretty static but the
and the reward is pretty static but the
episode return has gone up which is
episode return has gone up which is
encouraging
let's go grab the model from
let's go grab the model from
here should have saved by now
here should have saved by now
right is there a starter Jack
lstm uh this is all in py torch this is
lstm uh this is all in py torch this is
million step per second reinforcement
million step per second reinforcement
learning in pytorch with CPU
learning in pytorch with CPU
environments so not entirely sure with
environments so not entirely sure with
the Jack
stuff I kind of did write a uh big
stuff I kind of did write a uh big
granty post the other day about Jack's
granty post the other day about Jack's
pissing me off on uh Twitter whever that
pissing me off on uh Twitter whever that
went right
went right
freaking where is this rant
post is there
post is there
articles yeah
articles yeah
here so this is on Twitter and a lot of
here so this is on Twitter and a lot of
this is annoying Jack stuff so
this is annoying Jack stuff so
probably not the person to ask about uh
probably not the person to ask about uh
you know what tools are there in Jack I
you know what tools are there in Jack I
know um if you are using Jack though my
know um if you are using Jack though my
suggested implementation is the clean RL
suggested implementation is the clean RL
Jax
Jax
versions those are pretty
nice where was that model file it should
nice where was that model file it should
have been done by now
have been done by now
right here we go
what's the distinction with CPU
what's the distinction with CPU
environment does this mean it is one
environment does this mean it is one
environment per CPU it could be multiple
environment per CPU it could be multiple
environments per CPU the distinction is
environments per CPU the distinction is
that the simulation itself occurs on the
that the simulation itself occurs on the
CPU so I'm not uh the Sim itself is on
CPU so I'm not uh the Sim itself is on
uh CPU you could have multiple Sims per
uh CPU you could have multiple Sims per
core you can have multiple agents per
core you can have multiple agents per
Sim multiple agents Pere whatever you
Sim multiple agents Pere whatever you
want puffer lib gives you very very fast
want puffer lib gives you very very fast
factorization um but the difference in
factorization um but the difference in
the way that I'm approaching
the way that I'm approaching
environments is instead of writing
environments is instead of writing
domain specific language stuff in Jacks
domain specific language stuff in Jacks
where you have you're very constrained
where you have you're very constrained
in what you can write by the Jax
in what you can write by the Jax
language uh I'm just writing arbitrary
language uh I'm just writing arbitrary
environments in scon which are still
environments in scon which are still
really fast millions of steps per second
really fast millions of steps per second
and then I have good vectorization code
and then I have good vectorization code
to get all that data onto the GPU
to get all that data onto the GPU
relatively
relatively
efficiently very very flexible
efficiently very very flexible
approach and I this is the last thing
approach and I this is the last thing
that I've got time to do today because I
that I've got time to do today because I
have a meeting in 10 got to finish the
have a meeting in 10 got to finish the
shake got to do a couple
things what happened with the the
things what happened with the the
URL
oh all this code by the way is open
oh all this code by the way is open
source in puffer lib so you're free to
source in puffer lib so you're free to
look at the way it works there's like um
look at the way it works there's like um
this project is still very new but there
this project is still very new but there
is a massively multi-agent snake
is a massively multi-agent snake
environment that's a really fast and a
environment that's a really fast and a
really good demo of this Tech and I have
really good demo of this Tech and I have
a lot more stuff releasing very soon as
a lot more stuff releasing very soon as
well training stuff as we speak for
well training stuff as we speak for
it um I wanted to change the RO that
it um I wanted to change the RO that
length didn't
length didn't
I no it's it's good
ah I
ah I
the well the spawn is wrong
the well the spawn is wrong
but let me fix the spawn because that
but let me fix the spawn because that
might actually bias
might actually bias
it come
on so this colon colon 4 has to go uh on
here Jack's limited because environment
here Jack's limited because environment
complexity is limited due to GPU like
complexity is limited due to GPU like
branching yes precisely uh not only is
branching yes precisely uh not only is
the GPU restrictive in what it's good at
the GPU restrictive in what it's good at
Computing with branching but you have to
Computing with branching but you have to
think of you essentially have to solve a
think of you essentially have to solve a
clever puzzle every time you want to
clever puzzle every time you want to
write something because you have to
write something because you have to
twist your head around how to write
twist your head around how to write
everything into array operations whereas
everything into array operations whereas
in syon you can just write arbitrary
in syon you can just write arbitrary
code yeah your state is usually still
code yeah your state is usually still
going to be in arrays but some of it can
going to be in arrays but some of it can
be in struct some of it can be in other
be in struct some of it can be in other
things and you can write arbitrary logic
things and you can write arbitrary logic
over it you can write Loops you can
over it you can write Loops you can
write conditionals you don't have have
write conditionals you don't have have
to like do everything in this weird
to like do everything in this weird
functional domain specific
functional domain specific
language let me do that one more time
here okay so this is a little bit better
here okay so this is a little bit better
actually we have some agents that go in
actually we have some agents that go in
all directions it's a little bit less
all directions it's a little bit less
directionally biased I think we'd have
directionally biased I think we'd have
to train it for longer to get it fully
to train it for longer to get it fully
unbiased uh useful info but I'm just
unbiased uh useful info but I'm just
selftaught trying to learn lstm models
selftaught trying to learn lstm models
to shamelessly predict things like
to shamelessly predict things like
stocks you're going to lose a lot of
stocks you're going to lose a lot of
money doing that uh so the way your
money doing that uh so the way your
sequence is prob different but I'm
sequence is prob different but I'm
looking for overl yeah so this is not um
looking for overl yeah so this is not um
classic machine learning this is not
classic machine learning this is not
like train set test set this is your
like train set test set this is your
data comes from a simulator and anytime
data comes from a simulator and anytime
you want new data you have to give
you want new data you have to give
actions to the simulator like you have
actions to the simulator like you have
to press the buttons on the keyboard to
to press the buttons on the keyboard to
determine what happens next right it's
determine what happens next right it's
like playing a game so the data setup is
like playing a game so the data setup is
very different the infrastructure setup
very different the infrastructure setup
is very different yes technically the
is very different yes technically the
data has to go into an lstm and the way
data has to go into an lstm and the way
that you're going to do that is going to
that you're going to do that is going to
be roughly the same for class machine
be roughly the same for class machine
learning or for reinforcement learning
learning or for reinforcement learning
but uh it's there a lot more moving
but uh it's there a lot more moving
Parts
here okay
here okay
so decent little uh decent little
so decent little uh decent little
progress for
progress for
today we
today we
did let's actually commit all this code
did let's actually commit all this code
up
so that you guys can look at it if you
so that you guys can look at it if you
want
why is Jax become so popular in RL not
why is Jax become so popular in RL not
pytorch because in order to get uh CPU
pytorch because in order to get uh CPU
Sims to run fast you have to do some
Sims to run fast you have to do some
vectorization work and other things that
vectorization work and other things that
nobody bothered to do I did it it is now
nobody bothered to do I did it it is now
in puffer lib this is very recent this
in puffer lib this is very recent this
is been released within the last couple
is been released within the last couple
of months it is incredibly efficient and
of months it is incredibly efficient and
it makes reinforcement learning so so
it makes reinforcement learning so so
much simpler and this is also what I'm
much simpler and this is also what I'm
working on fulltime now so this is just
working on fulltime now so this is just
the beginning
the beginning
um I genuinely think that reinforcement
um I genuinely think that reinforcement
learning is going to be a stable
learning is going to be a stable
consistent easy to work in field within
consistent easy to work in field within
a few months directly because of puffer
a few months directly because of puffer
lib and because all of these simulators
lib and because all of these simulators
I'm building like the default in
I'm building like the default in
reinforcement learning is going to be
reinforcement learning is going to be
you train it hundreds of thousands to a
you train it hundreds of thousands to a
million steps per second you have access
million steps per second you have access
to a variety of simulators of various
to a variety of simulators of various
complexity for uh all of your research
complexity for uh all of your research
needs some of which are more complex
needs some of which are more complex
than some of the most complex simulators
than some of the most complex simulators
out there now you have access to large
out there now you have access to large
scale hyperparameter sweeps that run
scale hyperparameter sweeps that run
overnight on a single GPU and you have
overnight on a single GPU and you have
access to Quality infrastructure backing
access to Quality infrastructure backing
all of this all of this code is
all of this all of this code is
incredibly simple it's like a few
incredibly simple it's like a few
thousand Total Lines there are not 500
thousand Total Lines there are not 500
useless abstractions it's going to look
useless abstractions it's going to look
like clean RL it's going to have the
like clean RL it's going to have the
efficiency of sample sample Factory or
efficiency of sample sample Factory or
higher this is where RL is
higher this is where RL is
going take some work to get there though
going take some work to get there though
and with that uh I have got to go to a
and with that uh I have got to go to a
meeting in a couple minutes so I'm gonna
meeting in a couple minutes so I'm gonna
sign off uh I will probably be back
sign off uh I will probably be back
streaming more Dev on this
streaming more Dev on this
tomorrow and uh yeah I've been I've been
tomorrow and uh yeah I've been I've been
enjoying this so I'm just going to keep
enjoying this so I'm just going to keep
going with it and uh hopefully we'll get
going with it and uh hopefully we'll get
some better policies as well as uh we'll
some better policies as well as uh we'll
start doing some continuous control
start doing some continuous control
versus discreet experiments tomorrow see
versus discreet experiments tomorrow see
you all and uh star the puffer if you
you all and uh star the puffer if you
haven't already helps me out a whole ton
haven't already helps me out a whole ton
bye
