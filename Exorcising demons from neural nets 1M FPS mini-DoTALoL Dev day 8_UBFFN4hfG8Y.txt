Kind: captions
Language: en
okay we are
okay we are
live is this day eight or day
live is this day eight or day
nine I think I forgot to update the
nine I think I forgot to update the
title to nine
title to nine
right
right
wow we'll see
wow we'll see
anyways um let me post this and post a
anyways um let me post this and post a
couple things and then I'll go over what
couple things and then I'll go over what
is going to happen
today uh one
today uh one
second Dev day seven nope we're
second Dev day seven nope we're
good
good
okay perfect so uh let me tweet this
okay perfect so uh let me tweet this
real
quick for
cool so we're all set here um the main
cool so we're all set here um the main
plan for today is to actually buckle
plan for today is to actually buckle
down and get these networ to work a
down and get these networ to work a
little better
little better
rested had some ideas and we're
rested had some ideas and we're
basically just going to start uh start
basically just going to start uh start
going through this
going through this
so uh the first thing that I wanted to
so uh the first thing that I wanted to
do was I wanted to grab the parameters
do was I wanted to grab the parameters
roughly from
roughly from
snake because this is like just a decent
snake because this is like just a decent
and robust set of parameters at least
and robust set of parameters at least
some of
some of
them so I went ahead and did
them so I went ahead and did
that let me see which machine we on uh
that let me see which machine we on uh
I yeah okay this
I yeah okay this
one so if I look at the parameters that
one so if I look at the parameters that
I have at the
I have at the
moment
moment
[Music]
[Music]
NOA config where is it where am I
oh I'll show you what I updated these
two so I set these to a learning rate of
two so I set these to a learning rate of
0.0
0.0
O2 which is very aggressive but we
O2 which is very aggressive but we
should be able to get away with this for
should be able to get away with this for
what we're
what we're
doing uh numm two num workers two each
doing uh numm two num workers two each
core is actually running 400 copies of
core is actually running 400 copies of
the environment so really this is uh
the environment so really this is uh
environments gamma point9 Lambda point9
environments gamma point9 Lambda point9
these are very good starter values for
these are very good starter values for
um for problems where like it's
um for problems where like it's
essentially easy and you get pretty nice
essentially easy and you get pretty nice
extense reward
extense reward
signals entry coefficient of
signals entry coefficient of
.01 this is sometimes enough to prevent
.01 this is sometimes enough to prevent
models from collapsing but uh not high
models from collapsing but uh not high
enough to totally crash learning Horizon
enough to totally crash learning Horizon
of6 is just default and then we did
of6 is just default and then we did
essentially four mini batches of updates
essentially four mini batches of updates
32k very large stable updates batch size
32k very large stable updates batch size
of
128k uh and I tried this and this did
128k uh and I tried this and this did
not learn anything so
not learn anything so
this leads me to believe and we really
this leads me to believe and we really
haven't seen the agents learning
haven't seen the agents learning
anything reasonable so I think it's most
anything reasonable so I think it's most
likely that there's a data problem of
likely that there's a data problem of
some type because they really should be
some type because they really should be
learning something yeah this is what I
learning something yeah this is what I
ran before um they get this reward of
ran before um they get this reward of
like
like
Point uh
Point uh
0.14 or whatever which is technically
0.14 or whatever which is technically
better than nothing but like this is a
better than nothing but like this is a
very very easy problem for them to be
very very easy problem for them to be
learning so we should see this go up
learning so we should see this go up
to let me
think let me see what I have the reward
think let me see what I have the reward
scaled to
actually what do I have this scaled too
where did I Define the
where did I Define the
reward I think it was scaled to 0.1 but
reward I think it was scaled to 0.1 but
let me double check that that's very
let me double check that that's very
important the reward
scaling in order to get these uh these
scaling in order to get these uh these
models training
correctly okay right here
correctly okay right here
so yeah looks like the reward is scaled
so yeah looks like the reward is scaled
uh not at all so it should just be one
uh not at all so it should just be one
or uh you know between zero and one I'm
or uh you know between zero and one I'm
actually going to double check on this
actually going to double check on this
locally this is on the other box so add
locally this is on the other box so add
puffer
puffer
lib ocean MOA MOBA
Ranch get
push and we'll I want to pull this down
push and we'll I want to pull this down
on
on
local oh am I on the
local oh am I on the
right I'm not on the right box hold
right I'm not on the right box hold
on yeah I'm on the wrong box great um
that's my
bad what's the reward scale two now here
bad what's the reward scale two now here
so okay it's the same thing yeah it's
so okay it's the same thing yeah it's
the same thing
the same thing
so what did I change on this
box 400 copies
yep right and I just I added a thing to
yep right and I just I added a thing to
randomly respawn players uh that's just
randomly respawn players uh that's just
like a common debug technique in RL it
like a common debug technique in RL it
prevents degenerate situations from
prevents degenerate situations from
crashing learning we're obviously going
crashing learning we're obviously going
to have to take that out after we've
to have to take that out after we've
debug this
debug this
but since this seems reasonable for now
but since this seems reasonable for now
let's uh let's see if we can get this
let's uh let's see if we can get this
to to show us what's
wrong so we're just going to print out
wrong so we're just going to print out
the reward for the agent that we're
the reward for the agent that we're
playing we're going to play one of the
playing we're going to play one of the
agents and we're going to see how the
agents and we're going to see how the
rewards are
rewards are
scaled and if they match up with what we
expect uh right here
K this looks
good the game render
correctly yeah it does render okay so we
correctly yeah it does render okay so we
have a reward of 0.005
I thought we' rescaled this thing to be
I thought we' rescaled this thing to be
substantially
larger did we not rescale
this player. reward
this player. reward
equals previous distance to ancient
equals previous distance to ancient
minus distance to
ancient how do you make the rendering
ancient how do you make the rendering
it's with rayb if you haven't seen RB
it's with rayb if you haven't seen RB
it's like it's an absolutely fantastic
it's like it's an absolutely fantastic
project it gives you uh just low-level
project it gives you uh just low-level
rendering utilities and it makes it
rendering utilities and it makes it
really easy to just render nice stuff in
really easy to just render nice stuff in
code without having to deal with like a
code without having to deal with like a
big framework of [ __ ] or like an IDE
big framework of [ __ ] or like an IDE
or uh you know like a game engine or
or uh you know like a game engine or
anything like that
anything like that
the whole rendering for um
the whole rendering for um
Coba
Coba
oops the whole rendering for uh this
oops the whole rendering for uh this
thing here this is the draw health bars
thing here this is the draw health bars
function and then this block right here
function and then this block right here
which most of is commented yeah that's
which most of is commented yeah that's
like the entire rendering that's it it's
like the entire rendering that's it it's
very very simple and
very very simple and
easy you can write like nice little
easy you can write like nice little
renderers in a 100 lines of code couple
renderers in a 100 lines of code couple
hundred lines of code
hundred lines of code
absolutely love
absolutely love
RB has bindings for every language as
RB has bindings for every language as
well the original is a single file C
well the original is a single file C
library which
library which
is I mean freaking God dear is what that
is why am I getting rewards of
0.05 is this not compiled recently
we should be getting rewards of 0 five
right okay it was just not compiled
right okay it was just not compiled
that's fine so now we correctly get
that's fine so now we correctly get
rewards of 0.5 to 1
rewards of 0.5 to 1
right this looks good
now this is
now this is
fine so do we just have like some weird
fine so do we just have like some weird
issue where we forgot to pull the
new did we just like forget to pull the
new did we just like forget to pull the
new one or something or is it not
new one or something or is it not
learning this is good there's no scaling
learning this is good there's no scaling
quantity
here no it looks like this is
here no it looks like this is
legitimately not working which is what
legitimately not working which is what
we expected but okay it's good that we
we expected but okay it's good that we
know that this is the case now at least
so I'm going to run this just to make
so I'm going to run this just to make
sure that I didn't totally screw
sure that I didn't totally screw
something up somehow while I was half
something up somehow while I was half
asleep yesterday and I think we should
asleep yesterday and I think we should
see a curve that looks like this
see a curve that looks like this
so if uh you get 0. five for moving in
so if uh you get 0. five for moving in
the general direction of the opponent
the general direction of the opponent
and one if you move along a diagonal I
and one if you move along a diagonal I
would expect the average reward to be
would expect the average reward to be
upward of
upward of
0.5 uh if it's doing anything remotely
0.5 uh if it's doing anything remotely
reasonable so the fact that it's this
reasonable so the fact that it's this
low means that it is not
low means that it is not
technically the average reward should be
technically the average reward should be
what would the average reward be
well it
depends I think it's negative 1: one
depends I think it's negative 1: one
isn't
it let me double check that I think it
it let me double check that I think it
should be negative 1 to
one so if you go towards it's it's one
one so if you go towards it's it's one
yeah negative 1 okay so this is like a
yeah negative 1 okay so this is like a
perfectly scaled reward value right 1
perfectly scaled reward value right 1
one yeah that's like
one yeah that's like
perfect so if this doesn't work then
perfect so if this doesn't work then
there's just something
screwy and it says right here yeah this
screwy and it says right here yeah this
is where it was stuck before at like
is where it was stuck before at like
0.15 and if we look at the radiant uh
0.15 and if we look at the radiant uh
the positions
the positions
here dire X it's all the way on the
here dire X it's all the way on the
right side of the map so it's not
right side of the map so it's not
moving uh radiant Y is all the way on
moving uh radiant Y is all the way on
the bottom dire Y is all the way at the
the bottom dire Y is all the way at the
top and and then
top and and then
radiant or is radiant X yeah so these
radiant or is radiant X yeah so these
guys are not really
guys are not really
moving they should be like moving
moving they should be like moving
towards the center of the map or
whatever so we've got lots of different
whatever so we've got lots of different
potential uh candidates for issues to go
potential uh candidates for issues to go
through and attempt to rule out here
I'm going to grab 10 copies of the
environment I'm going to go into the
environment I'm going to go into the
forward pass right
forward pass right
here which is where the features
here which is where the features
are and then what we're going to do is
are and then what we're going to do is
we're going to
render we're going to run train
and we're going to see if the data looks
and we're going to see if the data looks
reasonable as it is going into the
network okay CNN features.
network okay CNN features.
shape
800 that is more than I was
expecting how did I end up with 800
ah CU I ran eight copies and 8times
10 now this end batch size wait a second
10 now this end batch size wait a second
this n batch
this n batch
size does not match up
if n batch size is
if n batch size is
four this should
four this should
be this should be
be this should be
400 did I mess this up
somehow this is potentially
something oh in
something oh in
demo let's look where I make the
environments train of end batch SI so
environments train of end batch SI so
right here
Argus of train gives
Argus of train gives
you m batch Siz is
four so it looks like this is giving you
four so it looks like this is giving you
the
the
correct
H the way that we're making this
H the way that we're making this
function appears to be correct
let's double check on
this welcome folks see the stream's
this welcome folks see the stream's
warmed up a
bit we're doing some hardcore debugging
bit we're doing some hardcore debugging
today we're finding yesterday I got some
today we're finding yesterday I got some
progress made but kind of messed around
progress made but kind of messed around
chatted bu about a bunch of stuff still
chatted bu about a bunch of stuff still
going to answer some questions and have
going to answer some questions and have
some fun but uh we're absolutely fixing
some fun but uh we're absolutely fixing
this
today can relax once we have good models
today can relax once we have good models
training
all right let's see what
all right let's see what
um what this says
um what this says
so num workers M's per worker batch
so num workers M's per worker batch
size
size
right I didn't run this with
right I didn't run this with
multiprocessing
this could be a degenerate case because
this could be a degenerate case because
of the serial back
of the serial back
end
end
right let me double check that I think
right let me double check that I think
that I had accounted for
this but it's possible that I could be
this but it's possible that I could be
running this differently uh locally
running this differently uh locally
versus when I'm training properly
appears so because when I run this
appears so because when I run this
without
without
cereal I mean if I run this
cereal I mean if I run this
multiprocessing I get 400
batch and here I get a 800
batch and here I get a 800
batch
so quars of batch size right
this should be caught right
here yeah n is
here yeah n is
four
right num
right num
workers oh because they're eight workers
that's an interesting conundrum
um I'll add a to do here
so not a huge deal um we are going to
so not a huge deal um we are going to
use the multiprocess version for
use the multiprocess version for
debugging for
debugging for
now just to make
sure okay here we have our features
it's a lot of
features well this is weird right
maybe maybe
maybe maybe
not let me check before the uh the end
code okay so this gives
you this is weird
right three one h
what are these
what are these
indices so three is the agent which is
indices so three is the agent which is
correct one is a
correct one is a
tower oh so it's spawning near its own
tower oh so it's spawning near its own
Tower that's
Tower that's
fine yeah that's actually kind of
fine yeah that's actually kind of
fine how about this one and then this
fine how about this one and then this
one is spawning closer to its base cuz
one is spawning closer to its base cuz
two is uh two is a wall so you have a
two is uh two is a wall so you have a
couple of your agents spawning up near
couple of your agents spawning up near
the wall here it's like random around
the wall here it's like random around
the fountain area you can see the wall
the fountain area you can see the wall
here and then the other agent is over
here and then the other agent is over
here maybe this one or something not
here maybe this one or something not
this one it can't be must be this
one so these features actually do look
one so these features actually do look
good these look good to
me would be very weird not to be able to
me would be very weird not to be able to
learn off of this data
unless so four gives you three in the
unless so four gives you three in the
center five this should change yep so
center five this should change yep so
this is now the other
this is now the other
team and then if we go up to nine it
team and then if we go up to nine it
should still be four and then if we go
should still be four and then if we go
to 10 it should be
three
and this is different uh a different set
and this is different uh a different set
of features as well
from from this zero right yeah
from from this zero right yeah
so they are spawning
so they are spawning
differently which is
good the random seating should actually
good the random seating should actually
be different here which is what we
be different here which is what we
want what else could it be
hello welcome and thank you
oops okay we are getting some good
oops okay we are getting some good
unique data in
here do I see any
patterns what's the plan I'm looking for
patterns what's the plan I'm looking for
weird data quirk and discrepancies that
weird data quirk and discrepancies that
would prevent this thing from learning
would prevent this thing from learning
properly we're trying to debug the
properly we're trying to debug the
policies and get them to actually do
stuff the observation data looks fine to
stuff the observation data looks fine to
me it really
does like
yeah this is all exactly what I would
yeah this is all exactly what I would
expect this data looks good so if this
expect this data looks good so if this
data is good the data is good going into
data is good the data is good going into
the
network how do I run that snake game and
network how do I run that snake game and
all the other games you created they're
all the other games you created they're
in puffer
in puffer
lib um
lib um
the command I'm using for these it's
the command I'm using for these it's
there's a demo file if you just do das
there's a demo file if you just do das
dasel you get options but if you run the
dasel you get options but if you run the
demo with like D- mode eval D- render
demo with like D- mode eval D- render
mode um like well there's probably a
mode um like well there's probably a
reasonable default for render mode even
reasonable default for render mode even
uh but they're in puffer lib they
uh but they're in puffer lib they
haven't been merged up to 10 yet but
haven't been merged up to 10 yet but
snake specifically is in a relatively
snake specifically is in a relatively
stable one Dev branch is relatively
stable one Dev branch is relatively
stable if you want to play around with
stable if you want to play around with
it it's in uh
it it's in uh
ocean right
ocean right
there and you can run it just from uh
there and you can run it just from uh
you can run it right from here with the
you can run it right from here with the
demo file demo. py d-m snake D- modiv
demo file demo. py d-m snake D- modiv
Val there's a little tutorial
here dang the repo got another bunch of
here dang the repo got another bunch of
stars overnight that's awesome
now
um did I mess up the network somehow let
um did I mess up the network somehow let
me
me
see you get the
observations you encode them
observations you encode them
you one hun
them they get put into this convet which
them they get put into this convet which
is the reasonable reasonable Network
is the reasonable reasonable Network
structure get your flat
structure get your flat
features you put them into self flat
features you put them into self flat
which is just a
projection you can catenate
projection you can catenate
them and then
them and then
you re Pro re
you re Pro re
features which is exactly what I did
features which is exactly what I did
with snake
right snake didn't have these extra
right snake didn't have these extra
relis at the end
relis at the end
actually and have this extra projection
stuff grid did though the grid grid
stuff grid did though the grid grid
environments
environments
did it's
did it's
identical so this should be fine
I could just return here to see if this
I could just return here to see if this
makes a difference though I don't think
makes a difference though I don't think
it
it
would me make one change here
I just want to see if this like this
I just want to see if this like this
thing is somehow screwing it
thing is somehow screwing it
up features is cat
up features is cat
uh2 let's just do CNN
channels are you using WSL
channels are you using WSL
Yep this is WSL and the other terminal
Yep this is WSL and the other terminal
that I have is an SSH into my uh well
that I have is an SSH into my uh well
this is technically this one that you're
this is technically this one that you're
looking at now is sshed into a server um
looking at now is sshed into a server um
and the local Dev setup is WSL they're
and the local Dev setup is WSL they're
all inside of the containers though it's
all inside of the containers though it's
all a puffer
tank which is our Docker stack
what editor this is
neovim NM is pretty
neovim NM is pretty
nice it is pretty
nice it's basically just Vim with a
nice it's basically just Vim with a
custom color scheme and like I have two
custom color scheme and like I have two
plugins total so it's pretty close to
plugins total so it's pretty close to
Vim uh the main reason I use neovim is
Vim uh the main reason I use neovim is
just it has uh easier integration with
just it has uh easier integration with
um language model like
autocomplete that's pretty much it I
autocomplete that's pretty much it I
have one syntax highlighting plugin for
have one syntax highlighting plugin for
python cuz that's a little bit weaker uh
python cuz that's a little bit weaker uh
by default and then uh super
Maven it's just super Maven and SEI
Maven it's just super Maven and SEI
that's literally
at I used uh no plugins like default
at I used uh no plugins like default
config Vim for seven
years I wish modern IDs would borrow
years I wish modern IDs would borrow
some of the more advanced macros that
some of the more advanced macros that
neov has I don't even use I don't even
neov has I don't even use I don't even
use any of the advanced stuff honestly
use any of the advanced stuff honestly
I've been using Vim for like 10 years
I've been using Vim for like 10 years
and I just use it in the dumbest way
and I just use it in the dumbest way
possible
possible
like I use a pretty small set of basic
like I use a pretty small set of basic
shortcuts and uh I mainly like it
shortcuts and uh I mainly like it
because it's like it's portable and
because it's like it's portable and
doesn't get in my
way so like I have my whole editor
way so like I have my whole editor
inside of the docker container so like
inside of the docker container so like
if I'm deing on a server or on local I
if I'm deing on a server or on local I
don't have to connect an IDE to the SSH
don't have to connect an IDE to the SSH
session or anything it's just there it's
session or anything it's just there it's
just on whatever box I'm doing and I
just on whatever box I'm doing and I
have an identical uh setup it's really
have an identical uh setup it's really
nice which version of Vim did you I have
nice which version of Vim did you I have
no
idea this is just this is just
Neo yeah I used I used
Neo yeah I used I used
um I used VSS code for like a year or so
um I used VSS code for like a year or so
with the Vim plugin and like it's the
with the Vim plugin and like it's the
second best thing out there but I still
second best thing out there but I still
hated
it I like this so much more
people have been trying to get me to tr
people have been trying to get me to tr
like to use cursor um I might do a
like to use cursor um I might do a
stream day at one po at one point but
stream day at one po at one point but
like I've warned folks that it's
like I've warned folks that it's
probably just going to be me [ __ ] on
probably just going to be me [ __ ] on
it because I have not found aggressive
it because I have not found aggressive
language model integration to be helpful
whatsoever did you download
whatsoever did you download
the this is on a container so this is
the this is on a container so this is
downloaded on a bunch to or Debian or
downloaded on a bunch to or Debian or
whatever the heck container I have here
whatever the heck container I have here
so no it's not it's not the gooey
so no it's not it's not the gooey
version or whatever it's the setup is in
version or whatever it's the setup is in
if you go check out puffer tank you can
if you go check out puffer tank you can
see the setup in there because it's
see the setup in there because it's
included it's on the GitHub including
included it's on the GitHub including
the config for this
the config for this
um let me think
um let me think
so I want to rerun this
so I want to rerun this
[Music]
[Music]
with oops I want to just see if this
with oops I want to just see if this
does anything different all I did was
does anything different all I did was
cut out a couple of linear
cut out a couple of linear
layers apparently I did that
incorrectly this
incorrectly this
is output shape right I forgot one thing
is output shape right I forgot one thing
oops
assuming this runs I'll check something
assuming this runs I'll check something
for you now yeah okay so while that's
for you now yeah okay so while that's
running I'll just show you um I've been
running I'll just show you um I've been
actually considering merging this into
actually considering merging this into
the puffer lib repo cuz I'm just it's
the puffer lib repo cuz I'm just it's
annoying maintaining multiple repos mono
annoying maintaining multiple repos mono
repo is actually pretty good um
right here is puffer
right here is puffer
tank and puffer tank has our Docker
tank and puffer tank has our Docker
files these are also these are pushed to
files these are also these are pushed to
Docker Hub as pre-built images so you
Docker Hub as pre-built images so you
don't have to like spend forever
don't have to like spend forever
building stuff but if you look in in
building stuff but if you look in in
some of these um I think
some of these um I think
the puffer depths one has
the puffer depths one has
my yeah here's the neovim setup right
my yeah here's the neovim setup right
here
and then uh it actually has my config in
and then uh it actually has my config in
it here like this is literally right
it here like this is literally right
here this is
here this is
it it's got like a couple very basic you
it it's got like a couple very basic you
know defaults and then the rest of this
know defaults and then the rest of this
is just hardcoded color
scheme do I just clone puffer tank uh
scheme do I just clone puffer tank uh
you can you don't have to build the
you can you don't have to build the
files though like if you want to use
files though like if you want to use
this container this Docker shell has
this container this Docker shell has
just a convenient utility you can just
just a convenient utility you can just
like bash doer. sh
like bash doer. sh
test and it will pull down for you
test and it will pull down for you
um the 1.0 version of puffer tank this
um the 1.0 version of puffer tank this
is like a just so you know this is like
is like a just so you know this is like
a 10 to 13 gigabyte image but this is
a 10 to 13 gigabyte image but this is
the whole Dev setup for puffer lib and
the whole Dev setup for puffer lib and
all of the reinforcement learning
all of the reinforcement learning
environments that are bound to puffer
environments that are bound to puffer
lib it includes pytorch with like Cuda
lib it includes pytorch with like Cuda
12.1 so it's assuming you have a GPU
12.1 so it's assuming you have a GPU
machine yeah lots of stuff there's a
machine yeah lots of stuff there's a
little setup here there's also a vs code
little setup here there's also a vs code
integration for
integration for
people I try to make this stuff pretty
people I try to make this stuff pretty
nice and easy for
nice and easy for
people okay so right here I can see that
people okay so right here I can see that
this network architecture change did not
this network architecture change did not
help so this is increasing my confidence
help so this is increasing my confidence
that um this issue is not just a network
that um this issue is not just a network
config
config
issue so
I think as a result the smart thing to
do p torch the only Cuda
dependency
um I I don't think it would be easy to
um I I don't think it would be easy to
like
like
so technically puffer lib doesn't use P
so technically puffer lib doesn't use P
torch for stuff but like the training
torch for stuff but like the training
all the training Integrations make a
all the training Integrations make a
pretty hard assumption on that um
zuda I don't think that there's anything
zuda I don't think that there's anything
in ml that's easily swapped for those
in ml that's easily swapped for those
frankly you could change it to CPU you'd
frankly you could change it to CPU you'd
probably have to swap the uh the P torch
probably have to swap the uh the P torch
install though because like the
install though because like the
container is built assuming GPU it's
container is built assuming GPU it's
built off of a a GPU base so you'd
built off of a a GPU base so you'd
probably have to swap some stuff
around you also don't need to use the
around you also don't need to use the
whole container right like the container
whole container right like the container
is for heavy RL environments like puffer
is for heavy RL environments like puffer
is not that hard to install you can just
is not that hard to install you can just
pip install puffer lib or you can just
pip install puffer lib or you can just
Co clone puffer Li if you want the dev
Co clone puffer Li if you want the dev
branch check out the dev branch and pip
branch check out the dev branch and pip
install d e do and that will just
work hey linky
work hey linky
thank you for the reminder for the
thank you for the reminder for the
advertisement there if you find any of
advertisement there if you find any of
this useful please go ahead and Star
this useful please go ahead and Star
Puffer lib it helps me out a whole ton
Puffer lib it helps me out a whole ton
really helps me get this work on the
board link is one of the Pokemon uh
board link is one of the Pokemon uh
Pokemon RL
Pokemon RL
contributors doing great work over there
I know I've been kind of a wall linky
I know I've been kind of a wall linky
I've been working on this
project trying to make this actually
something literally install poer is a
something literally install poer is a
dependency yeah it's supposed to be easy
dependency yeah it's supposed to be easy
and if it's not let me know because it
and if it's not let me know because it
should be
did you find out what the problem was
did you find out what the problem was
yesterday no that's what we're doing
yesterday no that's what we're doing
right
now can I just run a sweep on this while
now can I just run a sweep on this while
I try to debug
I try to debug
locally I really should have done this
locally I really should have done this
yesterday but I was just exhausted
I guess this does something we'll see
I guess this does something we'll see
what it's doing but in the meantime
what it's doing but in the meantime
let's debug more from here so the
let's debug more from here so the
observations coming into the network
observations coming into the network
look good this is usually the biggest
look good this is usually the biggest
source of issues but uh it looks pretty
source of issues but uh it looks pretty
darn solid
here
so how would I swap and what would I
so how would I swap and what would I
swap you can just pip in like you if
swap you can just pip in like you if
you're if you have something where
you're if you have something where
puffer tank isn't useful just just clone
puffer tank isn't useful just just clone
the puffer lib repo check out the dev
the puffer lib repo check out the dev
Branch pip install Dash e
Dot there are instructions on the
Dot there are instructions on the
website and on uh the GitHub and
everything let me know if it let me know
everything let me know if it let me know
if they don't work for
you um there's also discord. Discord uh
you um there's also discord. Discord uh
cord I can't type. GP Puffer that is the
cord I can't type. GP Puffer that is the
Discord for
Discord for
this nice and
this nice and
simple um
I guess we got to check the environment
I guess we got to check the environment
file very closely now
right see what's going on with
it call
reset this is where you assign the
reset this is where you assign the
actions into the actions
actions into the actions
buffer we know that this actually
buffer we know that this actually
works we know that this works because we
works we know that this works because we
can play the
game Let's double check right here that
game Let's double check right here that
we're actually getting actions from the
we're actually getting actions from the
model let me fix the AC as well real
model let me fix the AC as well real
quick
so
oh we can't do multiprocessing here this
oh we can't do multiprocessing here this
has to be
serial 100
actions why are there 100
actions because I have 10 environments
actions because I have 10 environments
per uh per instance
per uh per instance
right
right
yeah that's correct then
so
wait why did
64 does that Break
64 does that Break
Stuff no that can't break stuff because
Stuff no that can't break stuff because
we've double checked before that
yeah gave me an error something I can't
yeah gave me an error something I can't
create or remove file and install
directory I think that's that's
directory I think that's that's
typically a permissioning
typically a permissioning
error um that means that wherever you're
error um that means that wherever you're
trying to do this uh you're you're in
trying to do this uh you're you're in
some folder that's you need permissions
some folder that's you need permissions
on uh that should be not puffer lib
on uh that should be not puffer lib
doing that you should just you look up
doing that you should just you look up
like you're going to have to like Chown
like you're going to have to like Chown
that or
whatever I don't know linky we've never
whatever I don't know linky we've never
had like weird permissioning errors with
had like weird permissioning errors with
puffer
puffer
right that aren't just system
errors no yeah okay I I'm guessing those
errors no yeah okay I I'm guessing those
are system
are system
errors if you think that they're puffer
errors if you think that they're puffer
errors then I will definitely fix those
though the step is the step logic is
though the step is the step logic is
totally
totally
fine we're able to get rewards back from
fine we're able to get rewards back from
stuff observations rewards terminals
stuff observations rewards terminals
truncations
infos which are always false
right yeah they're always
false and again we know that buff
false and again we know that buff
observations is correct because we know
observations is correct because we know
the data going to the neural network is
correct and there you go
this looks very
this looks very
reasonable like I'm actually not seeing
reasonable like I'm actually not seeing
any of the usual red flags whatsoever
any of the usual red flags whatsoever
for like common screw-ups
for like common screw-ups
here if I check this
sweep WSL loves to throw permissioning
sweep WSL loves to throw permissioning
errors it's really obnoxious
what the heck is wrong with
this these nice dense
this these nice dense
rewards you have what should be a
rewards you have what should be a
trivial
trivial
task every indication is that the data
task every indication is that the data
going into the network is is correct and
going into the network is is correct and
that the actions are being
applied the hyper parameters are all
reasonable should definitely be working
we know puffer lib is not like fully
broken how much stuff have I trained
broken how much stuff have I trained
since the
refactor I have trained stuff on this
refactor I have trained stuff on this
Branch haven't
Branch haven't
I non-trivial stuff
snake EG bolts really
why well that's sketchy
why well that's sketchy
right didn't SE fault
before no this is a known good
before no this is a known good
environment
well it's trying to launch a client
well it's trying to launch a client
which is very
weird though potentially I messed with
weird though potentially I messed with
the renderer the environment is known
the renderer the environment is known
good
I'm just wondering if I like really
I'm just wondering if I like really
broke some core stuff in puffer with the
broke some core stuff in puffer with the
latest config I don't think I did though
why is it still on render mode
human e
okay so it was literally just the
okay so it was literally just the
renderer that's
fine that's clearly
training that's on my local Box by the
training that's on my local Box by the
way that it get 700k
train Crazy
Fast yeah that's definitely training so
Fast yeah that's definitely training so
it's impossible then that the whole I
it's impossible then that the whole I
didn't break anything critical I mean
didn't break anything critical I mean
there's no way I would have with the
there's no way I would have with the
latest refactor I just changed config
latest refactor I just changed config
stuff
so that learns to play Snake in like a
minute
so how's the sweep going
yeah
no why would this environment be
no why would this environment be
hard there's nothing hard about it
right I mean
oh yeah okay
what if we only spawned one
team are we both we spawned them uh
team are we both we spawned them uh
spawn player
if we just made them all be on this
team it's not go to Mid it's go to the
team it's not go to Mid it's go to the
enemy ancient which isn't really even a
enemy ancient which isn't really even a
debug task if you think about it like
debug task if you think about it like
technically that could be enough to just
technically that could be enough to just
play the
game okay so this now spawns 10 players
game okay so this now spawns 10 players
on the radiant team
I just want to see if this does anything
different how does it man man to be that
bad I
bad I
rebuild yeah I did rebuild
is the learning rate just way too high
is the learning rate just way too high
like there should be there should be
like there should be there should be
other signs though if that's the case
no it just very quickly
learns how bad does the initial entropy
crash you have very very clear learning
crash you have very very clear learning
signals
here very clear learning signals
What GPU are you running
What GPU are you running
my local box has a 3080 in it and each
my local box has a 3080 in it and each
of the puffer boxes has a
4090 wasn't the recurrent net being
weird what the
heck what's the interpretation of Lambda
heck what's the interpretation of Lambda
equals z Lambda equals 1 again hold on
equals z Lambda equals 1 again hold on
it's been so long with po looking at the
it's been so long with po looking at the
paper
where's the
thing e
it's the PO
it's the PO
paper uh
zero this do anything
no none of that does
no none of that does
anything so
weird I was trying to just do a one step
weird I was trying to just do a one step
[Music]
optimization what the heck would make it
optimization what the heck would make it
this
bad uh you can use one the commands like
this you don't have MOA in that one but
this you don't have MOA in that one but
snake for
snake for
instance we'll pull it
up these puffers are clearly just C
up these puffers are clearly just C
they're capable of moving around right
they behave what looks like randomly
they behave what looks like randomly
when they have random neuron net
when they have random neuron net
weights they receive what look like good
weights they receive what look like good
observations the policy collapses even
observations the policy collapses even
though they should be getting good
though they should be getting good
Rewards
maybe um
evv pathfinding is entirely learned or
evv pathfinding is entirely learned or
is there any hard-coded
is there any hard-coded
constraints pathf finding is learned the
constraints pathf finding is learned the
creeps have precomputed pathf finding
creeps have precomputed pathf finding
the agents are entirely learning the
the agents are entirely learning the
path finding but the thing is like
path finding but the thing is like
they're not like it's really weird cuz
they're not like it's really weird cuz
they're not learning anything and like
they're not learning anything and like
you get a reward of plus one if you go
you get a reward of plus one if you go
straight towards the enemy ancient like
straight towards the enemy ancient like
plus one plus one if you take a step in
plus one plus one if you take a step in
the opposite direction you get negative
the opposite direction you get negative
one if you stand in place you get zero
one if you stand in place you get zero
if you take a step like half in the
if you take a step like half in the
right direction you get 0. five it's
right direction you get 0. five it's
like the easiest thing in the world to
like the easiest thing in the world to
learn so like there's just something
learn so like there's just something
that's screwy I think and I don't know
that's screwy I think and I don't know
what it is
I mean I have a pretty robust set of
I mean I have a pretty robust set of
checks I normally go through and they've
checks I normally go through and they've
all come up clean so
far occasionally this is just like
far occasionally this is just like
egregiously bad hyper parameters but I'm
egregiously bad hyper parameters but I'm
starting with a default set that I've
starting with a default set that I've
used for many other similar uh simple
used for many other similar uh simple
tasks that have been
fine you're going to have to change the
fine you're going to have to change the
defaults for CPU and stuff most likely
defaults for CPU and stuff most likely
undefined symbol is going to be a bad
undefined symbol is going to be a bad
torch version uh I think the latest
torch version uh I think the latest
puffer uses like 2.4 whatever I'd say go
puffer uses like 2.4 whatever I'd say go
download the like the nightly or
download the like the nightly or
whatever uh just CPU version it'll
whatever uh just CPU version it'll
probably fix
it e
there's something else I'm missing
there's something else I'm missing
here I mean there's there's something
here I mean there's there's something
clearly but I think I'm looking in the
clearly but I think I'm looking in the
wrong place like I shouldn't be
wrong place like I shouldn't be
spending time suspecting the stable
spending time suspecting the stable
implementation that we've used for like
implementation that we've used for like
everything for the past
year what's unique about this
year what's unique about this
project and we don't have very many
project and we don't have very many
agents per environment
really that shouldn't
matter you get
matter you get
very consistent reward
what's the issue it's not learning
a very simple debug task with the Moa
a very simple debug task with the Moa
and it's not learning
it that's the
issue and the sweeps so far haven't
issue and the sweeps so far haven't
found anything
found anything
better so I don't think it's just hyper
better so I don't think it's just hyper
parameters
h
sweeping Epoch as well no
shouldn't have to but you're right that
shouldn't have to but you're right that
there was like a weird thing I remember
there was like a weird thing I remember
with
that I do remember there being something
that I do remember there being something
weird about
EPO doesn't seem to
EPO doesn't seem to
help very very quickly
crashes what was the starting
entropy still bouncing around as
well I don't understand how it says that
well I don't understand how it says that
they're getting levels as well
when they have an X of
6.4 radiant Y at 69 means that they've
6.4 radiant Y at 69 means that they've
gone like straight
gone like straight
up doesn't it
have the agent resetting this copy hold
have the agent resetting this copy hold
on I'll leave this for for
now e
this is such an easy task
now this is where it gets stuck right
yeah they do better
yeah they do better
when you reset
when you reset
them not by much
though I do like
32 I didn't rebuild
so they can optimize it looks
like I mean you have to make the task so
like I mean you have to make the task so
stupidly simple
stupidly simple
though what about why
though what about why
what do you mean what about
why this does learn over time now it
why this does learn over time now it
looks to
me but it's
me but it's
so why do you have to do this
the cords is all internal Sim yeah
I mean at least this tells me it's
I mean at least this tells me it's
capable of
capable of
learning like this rules out a bunch of
learning like this rules out a bunch of
other
[ __ ] yeah this rules out a whole
[ __ ] yeah this rules out a whole
bunch of other
[ __ ] e
the entropy is really low though
h
I have a custom RL environment that's
I have a custom RL environment that's
only partially
only partially
observed same to deal with this I wanted
observed same to deal with this I wanted
to use a recurrent poo
to use a recurrent poo
implementation but all the examples I
implementation but all the examples I
found use convolutional nails for
found use convolutional nails for
Atari can I just replace them with a
Atari can I just replace them with a
linear layer if you have a flat if you
linear layer if you have a flat if you
have flat observations yes are other is
have flat observations yes are other is
necessary
necessary
no you like puffer lib does that as
well all you have to do is replace the
well all you have to do is replace the
um comms with
um comms with
linear we do that in some debug
linear we do that in some debug
environments as well for Puffer
replace all cers with one linear or one
replace all cers with one linear or one
linear depends on the complexity of your
linear depends on the complexity of your
environment
I'd usually say if you have some like
I'd usually say if you have some like
basic
basic
thing
thing
um fully connected reu fully connected
um fully connected reu fully connected
maybe is a good entry
maybe is a good entry
to your
to your
lstm may or may not want to put a re at
lstm may or may not want to put a re at
the end I think I have reos at the
end
for e
I mean this is what we wanted them to
I mean this is what we wanted them to
learn
I'm so weird
there's got to be a reason for
this
for e
e e
very bizarre Dynamic that's happening
so I can get them to learn the thing
so I can get them to learn the thing
that I want but only in a ridiculously
that I want but only in a ridiculously
stupidly simplified version
why they're getting zero reward
entropy
bonus I don't believe it would be
bonus I don't believe it would be
entropy
bonus that'd be really funny
bonus that'd be really funny
welcome
back deep in thought here trying to
back deep in thought here trying to
figure out what is wrong with this found
figure out what is wrong with this found
some Clues this
morning definitely found some Clues
set this to
zero how the gray guys
zero how the gray guys
doing
so if I let them play the game
so if I let them play the game
normally they just learn to go run up
normally they just learn to go run up
against a wall and chill there it's
against a wall and chill there it's
bizarre if I reset the agents every 128
bizarre if I reset the agents every 128
steps on average so they respawn them
steps on average so they respawn them
randomly they still just go into a wall
randomly they still just go into a wall
if I respawn them randomly every 32
if I respawn them randomly every 32
steps they learn to run at each other
steps they learn to run at each other
down
mid so there's like some incentive that
mid so there's like some incentive that
I'm not seeing here like there's some
I'm not seeing here like there's some
incentive that I'm not seeing here it's
weird no
weird no
well that is actually better than
before not really little
before not really little
bit oh
bit oh
wait that's
wait that's
progress that is progress
there like your P oh I missed a message
there like your P oh I missed a message
Lone Wolf design hi there I really liked
Lone Wolf design hi there I really liked
your PhD presentation was extremely
your PhD presentation was extremely
interesting thank you I am considering
interesting thank you I am considering
doing some additional long form content
doing some additional long form content
for the
for the
channel um tree they do not lose reward
channel um tree they do not lose reward
for dying that's what makes it
for dying that's what makes it
weird
weird
um yeah I will potentially do some
um yeah I will potentially do some
additional long form content
additional long form content
um nothing as ridiculously high effort
um nothing as ridiculously high effort
as my PhD defense obviously but I'm
as my PhD defense obviously but I'm
thinking of putting some cool stuff
thinking of putting some cool stuff
together for now I've been just doing a
together for now I've been just doing a
bunch of Dev live because uh
bunch of Dev live because uh
well there's been a lot of progress
well there's been a lot of progress
since my PhD defense I'll say like
since my PhD defense I'll say like
reinforcement learning is practically a
reinforcement learning is practically a
different field since since then because
different field since since then because
of all this
stuff I think I figured something out
stuff I think I figured something out
hold on
oh what's wait uh hold on let me
there you go okay I actually got this
there you go okay I actually got this
thing to somewhat train it's not perfect
thing to somewhat train it's not perfect
but uh I will explain this in a second
but uh I will explain this in a second
let me answer these questions so are you
let me answer these questions so are you
still in Academia after your
still in Academia after your
PhD uh no I am currently doing puffer
PhD uh no I am currently doing puffer
full-time puffer is technically a
full-time puffer is technically a
startup but for the time being I'm just
startup but for the time being I'm just
doing a whole bunch of Open Source
doing a whole bunch of Open Source
reinforcement learning Dev mostly infra
reinforcement learning Dev mostly infra
and high performance infrastructure and
and high performance infrastructure and
tools trying to just fix all of this
tools trying to just fix all of this
stuff that makes RL a hard and unfun
stuff that makes RL a hard and unfun
place to work in uh so that's what
place to work in uh so that's what
you're seeing here ultra high
you're seeing here ultra high
performance simulation development live
performance simulation development live
and fixing all sorts of things that are
and fixing all sorts of things that are
broken with
broken with
RL um so not not exactly industry not
RL um so not not exactly industry not
exactly Academia I don't really plan on
exactly Academia I don't really plan on
publishing stuff so much anymore because
publishing stuff so much anymore because
it's just there's a whole rant on that
it's just there's a whole rant on that
topic but um I do release stuff I
topic but um I do release stuff I
release a lot of code I do blog posts I
release a lot of code I do blog posts I
tweet a bunch of stuff so everything is
tweet a bunch of stuff so everything is
still out in the open and all this is
still out in the open and all this is
open
open
source all what stuff why is it
source all what stuff why is it
different
different
now so uh RL is different because of the
now so uh RL is different because of the
level of performance that puffer lib can
level of performance that puffer lib can
introduce and the level of Simplicity it
introduce and the level of Simplicity it
can introduce like I would not believe
can introduce like I would not believe
this was possible three months ago and I
this was possible three months ago and I
think that the majority of the field
think that the majority of the field
like they might tell you now oh of
like they might tell you now oh of
course you can do that but I guarantee
course you can do that but I guarantee
you if you just pose them the question
you if you just pose them the question
neutrally 3 months ago they would tell
neutrally 3 months ago they would tell
you this is
impossible like hey can you write a like
impossible like hey can you write a like
a MOBA simulator in a week and a half
a MOBA simulator in a week and a half
that runs at over a million frames per
that runs at over a million frames per
second and trains at like a half a
second and trains at like a half a
million plus on one GPU they'd be like
million plus on one GPU they'd be like
are you crazy no yeah we can just do
are you crazy no yeah we can just do
that
now Okay so
now Okay so
I'm really surprised that the sweep
I'm really surprised that the sweep
didn't find this though I probably
didn't find this though I probably
should have just ran the sweep overnight
should have just ran the sweep overnight
because if I let it run for longer it
because if I let it run for longer it
would have solved it
would have solved it
um actually has it solved it while I've
um actually has it solved it while I've
been talking that'd be really funny if
been talking that'd be really funny if
the algorithm be beat me to it
the algorithm be beat me to it
surprisingly it
surprisingly it
didn't very surprised that it
didn't what's this thing tried for
didn't what's this thing tried for
entropy
I would have expected this one to
work oh but it's sampling stupid lambdas
work oh but it's sampling stupid lambdas
okay so this would take a while to find
okay so this would take a while to find
anything
good with puffer AI how many agents can
good with puffer AI how many agents can
you
you
have wait what uh puffer fish
have wait what uh puffer fish
go yeah puffer's fun with puffer AI how
go yeah puffer's fun with puffer AI how
many agents can you
many agents can you
have well
have well
it depends what you mean by that but uh
it depends what you mean by that but uh
the training skill that we're typically
the training skill that we're typically
looking at for a puffer that I found to
looking at for a puffer that I found to
be very effective is I'll usually
be very effective is I'll usually
simulate on the order of 16,000 agents
simulate on the order of 16,000 agents
um so for DOTA that is 1,600 copies of
um so for DOTA that is 1,600 copies of
the game and I can do that on two CPU
the game and I can do that on two CPU
cores typically we use two CPU cores
cores typically we use two CPU cores
instead of one because then we can
instead of one because then we can
buffer them so that you're never waiting
buffer them so that you're never waiting
on data um and the 16,000 numbers become
on data um and the 16,000 numbers become
because mini batch size 8192 is
because mini batch size 8192 is
typically what you want to saturate
typically what you want to saturate
Hardware you can go much higher if you
Hardware you can go much higher if you
would
would
like um sometimes there are reasons to
like um sometimes there are reasons to
do
do
that but yeah um you're looking at the
that but yeah um you're looking at the
level of performance we have now this on
level of performance we have now this on
one CPU core this little mini MOBA runs
one CPU core this little mini MOBA runs
five uh like five and a half hours worth
five uh like five and a half hours worth
of games per
second it's very very fast
second it's very very fast
and it's very simple as
well oh I missed this uh spinning up in
well oh I missed this uh spinning up in
puffer would be good long form
puffer would be good long form
content yeah
content yeah
I keep changing it so frequently that
I keep changing it so frequently that
like I need to do that once the user
like I need to do that once the user
experience stabilizes a bit more um like
experience stabilizes a bit more um like
I just redid the configuration system
I just redid the configuration system
now and I've got to like rewrite stuff
now and I've got to like rewrite stuff
for that um the other thing that's like
for that um the other thing that's like
spinning up with puffer Li is kind of
spinning up with puffer Li is kind of
awkward is um puffer lib isn't a
awkward is um puffer lib isn't a
framework like it's not some big heavy
framework like it's not some big heavy
thing where we give you a whole bunch of
thing where we give you a whole bunch of
first party algorithms and stuff it's
first party algorithms and stuff it's
just like here's a library with some
just like here's a library with some
tools to make your stuff fast here's
tools to make your stuff fast here's
some fast Sims and like we have one
some fast Sims and like we have one
training demo with a PO that we have
training demo with a PO that we have
souped up but like there's not a ton of
souped up but like there's not a ton of
stuff to
it I did a cool um I did a a like somewh
it I did a cool um I did a a like somewh
a somewhat prepared demo video like I
a somewhat prepared demo video like I
did a demo Day video uh that's the
did a demo Day video uh that's the
tagged post on my Twitter and is also on
tagged post on my Twitter and is also on
the YouTube where I go through like all
the YouTube where I go through like all
of the nice technical advancements in
of the nice technical advancements in
puffer 10
puffer 10
it's a less accessible video but it's um
it's a less accessible video but it's um
you know it shows a lot of what I
did
e e
let's try
this put this to One update Epoch as
this put this to One update Epoch as
well
one
second
second
88 200
try
this what care do you take for your
this what care do you take for your
hair it's like Trader Joe's three in
[Music]
[Music]
one what's the end goal for this DOTA
one what's the end goal for this DOTA
style project or is it more a test of
style project or is it more a test of
the
the
libraries the goal is to have a very
libraries the goal is to have a very
complicated and interesting and stupidly
complicated and interesting and stupidly
high performance environment that you
high performance environment that you
can use for research like what type of
can use for research like what type of
research can you do when you have an
research can you do when you have an
environment that's more complex and
environment that's more complex and
interesting than literally anything else
interesting than literally anything else
out there that also runs a thousand
out there that also runs a thousand
times
times
faster like that's the that's the goal
that's fair I remember you don't like
that's fair I remember you don't like
Minecraft ml I mean okay I know the guy
Minecraft ml I mean okay I know the guy
who did that one and he's awesome like
who did that one and he's awesome like
my problem with Minecraft as an RL
my problem with Minecraft as an RL
environment like here there's an
environment like here there's an
environment that's called uh crafter
environment that's called uh crafter
that I also think is a terrible
that I also think is a terrible
environment that's like 2D Minecraft
environment that's like 2D Minecraft
okay and now there's an environment
okay and now there's an environment
called craft ax which is the exact same
called craft ax which is the exact same
environment 2D
environment 2D
Minecraft but like a thousand times
Minecraft but like a thousand times
faster and that environment is awesome
faster and that environment is awesome
you see it's like it just has to be
you see it's like it just has to be
fast like your environment has to be
fast like your environment has to be
fast it has has to be complex and
fast it has has to be complex and
interesting if your environment is slow
interesting if your environment is slow
I can't do anything with
it yeah we love okay that's the
it yeah we love okay that's the
exception that proves the rule though
exception that proves the rule though
like the craft a environment in Jack
like the craft a environment in Jack
that's like 3,000 lines of puzzle
that's like 3,000 lines of puzzle
solving Madness to make that thing work
solving Madness to make that thing work
in Jack like you can actually look at
in Jack like you can actually look at
that and say hey does this look like a
that and say hey does this look like a
reasonable thing to have to do to
reasonable thing to have to do to
implement every new environment no no it
implement every new environment no no it
doesn't like just because that one guy
doesn't like just because that one guy
that did that is really really good
that did that is really really good
doesn't mean I'm smart enough to figure
doesn't mean I'm smart enough to figure
out how to do
it I like to write dumb simple code that
it I like to write dumb simple code that
is fast
interesting that this one doesn't seem
interesting that this one doesn't seem
to be optimizing anywhere near as
well this is pretty
well this is pretty
weird on this
weird on this
works yeah it's way faster than
works yeah it's way faster than
mineral yeah
like the new standard is going to be M's
like the new standard is going to be M's
run at a million steps per second per
run at a million steps per second per
CPU core training runs at at least
CPU core training runs at at least
several hundred, steps per second bare
minimum get out of here with your slow
minimum get out of here with your slow
ass learning
stuff like the thing is you don't even
stuff like the thing is you don't even
have to make stuff complicated in order
have to make stuff complicated in order
to do this right like it's not like oh
to do this right like it's not like oh
yeah but in order to make it that
yeah but in order to make it that
complicated you need like a 20,000 line
complicated you need like a 20,000 line
RL stack with all sorts of complicated
RL stack with all sorts of complicated
little components no like the
little components no like the
environment implementation puffer lib
environment implementation puffer lib
puffer Li's vectorization like the whole
puffer Li's vectorization like the whole
training code everything together is 3
training code everything together is 3
to 4,000
to 4,000
lines the whole stack is three to 4,000
lines the whole stack is three to 4,000
lines
lines
deep very very basic and
doable and try this
when are you going to write an efficient
when are you going to write an efficient
Pokemon
Pokemon
H how about I teach you how to write an
H how about I teach you how to write an
efficient Pokemon and then you do
it can we do that
instead I've been grinding like
instead I've been grinding like
simulation engineering stuff for the
simulation engineering stuff for the
last two months straight so
it's going to be finish this block
it's going to be finish this block
finish up this work block get right on
finish up this work block get right on
the scon B yeah pretty much it's pretty
the scon B yeah pretty much it's pretty
much for me it's going to be like finish
much for me it's going to be like finish
this work block get these Sims in a good
this work block get these Sims in a good
spot
spot
um figure out like what I'm going to
um figure out like what I'm going to
decide needs to be done for release like
decide needs to be done for release like
polish stuff up for release this mobile
polish stuff up for release this mobile
I'm going to hang on to for a little
I'm going to hang on to for a little
longer you know chill for a few days
longer you know chill for a few days
write a few blog posts maybe make a
write a few blog posts maybe make a
couple of videos like and then we're
couple of videos like and then we're
going to go to the next phase of the
going to go to the next phase of the
master plan for RL from there um the
master plan for RL from there um the
goal is pretty much at the moment just
goal is pretty much at the moment just
to finish all of these
first interesting that this one doesn't
first interesting that this one doesn't
seem to work with the other one did what
seem to work with the other one did what
did I do differently
8843 10
8843 10
M's that
M's that
works
works
huh it's really weird
um do you have any tips on making
um do you have any tips on making
Sims
Sims
yes
yes
um scon is essentially god
um scon is essentially god
tier and if you want a very nice
tier and if you want a very nice
example of how to make something that
example of how to make something that
runs at like 10 million plus steps per
runs at like 10 million plus steps per
second single
thread start on your way in helps me out
thread start on your way in helps me out
a
bunch so I
made have you seen the snake
environment let me find it
I've been tweeting a lot of
stuff where the heck is the snake
en okay here
en okay here
so snake EnV it's snake with 4096
so snake EnV it's snake with 4096
snakes right you can also do smaller
snakes right you can also do smaller
versions of it here's gigantic version
versions of it here's gigantic version
of it with lots of
of it with lots of
snakes you can do whatever you want in
snakes you can do whatever you want in
there puffer Li puffer lib environments
there puffer Li puffer lib environments
ocean
ocean
snake it
snake it
is 230 lines of wrapper code in
is 230 lines of wrapper code in
Python and 230 lines of scyon for the
Python and 230 lines of scyon for the
simulation Logic the thing runs over 10
simulation Logic the thing runs over 10
million steps per second single thread
million steps per second single thread
and trains it over 10 uh over a million
and trains it over 10 uh over a million
steps per second on one
steps per second on one
GPU so this is basically the whole this
GPU so this is basically the whole this
is the simple version of how to make
is the simple version of how to make
Sims
fast I added Xfinity for my internet
fast I added Xfinity for my internet
outage yesterday and they actually reply
outage yesterday and they actually reply
with this I'm sure automated
with this I'm sure automated
account that's so funny
are there any Sim examples you would
are there any Sim examples you would
suggest I
suggest I
create just to
create just to
learn I mean if you can think of any
learn I mean if you can think of any
cool like retro game type things like
cool like retro game type things like
simple retro games if you want to add
simple retro games if you want to add
something to puffer lib that would be
something to puffer lib that would be
awesome um I did snake as an example cuz
awesome um I did snake as an example cuz
like people know slyther iio right and I
like people know slyther iio right and I
could basically make slyther iio but
could basically make slyther iio but
make it stupidly fast for RL I don't
make it stupidly fast for RL I don't
know if there are any other ones that
know if there are any other ones that
are like nice in multiplayer like that
are like nice in multiplayer like that
you might have to do single player which
you might have to do single player which
those are a little harder to make fast
those are a little harder to make fast
though you can still get in the millions
though you can still get in the millions
of steps per second pretty easily
of steps per second pretty easily
um I'd have to think about
um I'd have to think about
that are there any old like arcade games
that are there any old like arcade games
that would be interesting to do for
that I was going to do Tetris I think
that I was going to do Tetris I think
that would be slightly harder though
that would be slightly harder though
and I don't know if that's a super
and I don't know if that's a super
useful n either for
RL anybody have ideas for like
RL anybody have ideas for like
relatively simple Sims that would be
relatively simple Sims that would be
useful for RL to be very fast
okay this is actually training now which
okay this is actually training now which
is
is
interesting so it doesn't train if you
interesting so it doesn't train if you
have too many M's that's
weird trying to think of some Sims that
weird trying to think of some Sims that
I would like to have people
I would like to have people
make open TTD bit simpler yeah that's
make open TTD bit simpler yeah that's
just a hard place to
just a hard place to
start I've been wanting to do an RTS
start I've been wanting to do an RTS
those are
hard scaling mazm
hard scaling mazm
maybe that's kind of hard to make
maybe that's kind of hard to make
efficient though because when you reset
efficient though because when you reset
it you have to generate a new maze
it you have to generate a new maze
unless you're going to keep the same
unless you're going to keep the same
maze over and over
and that requires a little bit of M uh
and that requires a little bit of M uh
generation work as
well I mean there's like this open Spiel
well I mean there's like this open Spiel
thing from Deep
thing from Deep
Mind where like these Ms are fast but
Mind where like these Ms are fast but
they're not fast when you paralyze them
they're not fast when you paralyze them
as
as
well some of these would be easy I don't
well some of these would be easy I don't
know how useful these would be cuz this
know how useful these would be cuz this
is kind kind of welldeveloped if a bit
is kind kind of welldeveloped if a bit
of a pain to work with I do like the
of a pain to work with I do like the
idea of having an RTS but that's
idea of having an RTS but that's
probably a much more
probably a much more
complicated project
complicated project
um Checkers and board games fall into
um Checkers and board games fall into
open Spiel yeah
open Spiel yeah
um man there's so many things I'd like
um man there's so many things I'd like
to
to
have in this but like practically
speaking the easiest thing to
speaking the easiest thing to
do actually here's a really good project
do actually here's a really good project
here's a really good project that if you
here's a really good project that if you
do it well I'd be happy to promote and
do it well I'd be happy to promote and
I'd be very happy to integrate into
I'd be very happy to integrate into
puffer lip um breakout is a very classic
puffer lip um breakout is a very classic
Atari game right it's used for like a
Atari game right it's used for like a
lot of it's like the cover for a lot of
lot of it's like the cover for a lot of
old RL research if you were to make
old RL research if you were to make
breakout at uh a million steps plus per
breakout at uh a million steps plus per
second then uh we got it training like
second then uh we got it training like
in 30 seconds or something I think that
in 30 seconds or something I think that
that would probably make people very
that would probably make people very
interested that was something I was
interested that was something I was
thinking about doing that's like pretty
thinking about doing that's like pretty
easy but I don't know if it's worth
easy but I don't know if it's worth
spending a few days on it um that would
spending a few days on it um that would
probably be a pretty good learner
probably be a pretty good learner
project assuming that you're like a
project assuming that you're like a
competent programmer may or may not have
competent programmer may or may not have
tons of RL experience but want to do
tons of RL experience but want to do
super like hyper
simd that would probably be my
simd that would probably be my
suggestion to do
and uh yeah if you look at the snake
and uh yeah if you look at the snake
stack as well like you don't render
stack as well like you don't render
breakout during training like you would
breakout during training like you would
just give it like the board would just
just give it like the board would just
be represented where like each like
be represented where like each like
pixel or whatever is a block and then
pixel or whatever is a block and then
your paddle is a certain like length or
your paddle is a certain like length or
whatever and then uh you would just
whatever and then uh you would just
render it at at test time you'd like
render it at at test time you'd like
make it playable it that wouldn't be
make it playable it that wouldn't be
that hard I would expect that that
that hard I would expect that that
project would
project would
be probably about the same length the
be probably about the same length the
snake that's probably a whole like you
snake that's probably a whole like you
can probably do that whole project maybe
can probably do that whole project maybe
a little bit more for the rendering
a little bit more for the rendering
that's probably a 500 line of code
that's probably a 500 line of code
project what are the current steps per
project what are the current steps per
second for breakout
second for breakout
um it's actually hard to say because
um it's actually hard to say because
we've got all these crappy python
we've got all these crappy python
rappers that make it slower I think it's
rappers that make it slower I think it's
actually like in the tens of thousands
actually like in the tens of thousands
or like high thousands maybe but then
or like high thousands maybe but then
also because it's rendered like you have
also because it's rendered like you have
just a whole bunch of data that has to
just a whole bunch of data that has to
go through the neural net um that slows
go through the neural net um that slows
it down whereas if you could get more
it down whereas if you could get more
efficient State based that would be more
efficient State based that would be more
useful like there's Ram based stuff but
useful like there's Ram based stuff but
nobody really trains from Ram because
nobody really trains from Ram because
that's kind of
weird but yeah if you're interested in
weird but yeah if you're interested in
doing stuff with puffer Li that's like
doing stuff with puffer Li that's like
the exact type of thing that I suggest
the exact type of thing that I suggest
to people like if you want to like get
to people like if you want to like get
your feet wet with uh development m in
your feet wet with uh development m in
this like and you want to contribute to
this like and you want to contribute to
this space like new M's for Puffer that
this space like new M's for Puffer that
are like you know Built Well stable High
are like you know Built Well stable High
perf that's a really awesome way to get
perf that's a really awesome way to get
involved with
it what tools should I learn to make
it what tools should I learn to make
such environments I just linked the the
such environments I just linked the the
snake project in puffer if you just look
snake project in puffer if you just look
in the dev branch of puffer puffer
in the dev branch of puffer puffer
environments ocean um the snake game is
environments ocean um the snake game is
an end to end example of making snake in
an end to end example of making snake in
450 lines of code that runs at over 10
450 lines of code that runs at over 10
million steps per second it's just scon
million steps per second it's just scon
plus python that's all it is and like
plus python that's all it is and like
you'll see the way that I use cython to
you'll see the way that I use cython to
mirror arrays back and forth between
mirror arrays back and forth between
Python and C
Python and C
um yeah it's a pretty simple stack it's
um yeah it's a pretty simple stack it's
like actually shockingly
simple okay so here you can see
simple okay so here you can see
different games that have been one at
different games that have been one at
looks
like the reward capping out here is a
like the reward capping out here is a
little
odd check out the snake
odd check out the snake
implementation helps I can reach out on
implementation helps I can reach out on
Twitter if I get any progress yeah of
Twitter if I get any progress yeah of
course you can DM me on Twitter you can
course you can DM me on Twitter you can
me you can add me in the puffer Discord
me you can add me in the puffer Discord
you can find me here I'm not hard to get
you can find me here I'm not hard to get
a hold
a hold
of 10 million steps per
of 10 million steps per
second I think it's a little more than
second I think it's a little more than
than that but
yeah 10 mil for something like snake is
yeah 10 mil for something like snake is
very achievable for something much more
very achievable for something much more
complex like the project I'm doing now
complex like the project I'm doing now
you can still get over a million and
you can still get over a million and
like I think you could get AAA level
like I think you could get AAA level
games uh still potentially running in
games uh still potentially running in
the hundreds of
thousands you just have to build for
thousands you just have to build for
it
2.89 weird how this gets more stuck
2.89 weird how this gets more stuck
isn't
it
0.24 why does this get stuck
oh you have a very different h
very
finicky check out
finicky check out
those
those
this
second e
is puffer lib meant to be
is puffer lib meant to be
cloned like clean RL or installed via
cloned like clean RL or installed via
pip so uh if you want to use our
pip so uh if you want to use our
training utils they're based on clean RL
training utils they're based on clean RL
so you clone it if you just want to use
so you clone it if you just want to use
the environments out of the box then
the environments out of the box then
it's a pip install with the
it's a pip install with the
understanding that like all the new Sims
understanding that like all the new Sims
that I've been building are still in Dev
that I've been building are still in Dev
branches so like those aren't in the pit
branches so like those aren't in the pit
package yet right but like all the
package yet right but like all the
existing bindings and stuff are
existing bindings and stuff are
accessible via
pip so it's a little bit of
both I want there to be a stable package
both I want there to be a stable package
that you can use for like our core
that you can use for like our core
vectorization bindings functionality
vectorization bindings functionality
right and then uh for training it's it's
right and then uh for training it's it's
got to be
got to be
uh a it's got to be a clone because we
uh a it's got to be a clone because we
base it off of cleanar Al so you know
base it off of cleanar Al so you know
that stuff is in user
that stuff is in user
space and really like the idea behind
space and really like the idea behind
puffer lib is if you're trying to do
puffer lib is if you're trying to do
advanced stuff with it it's intended to
advanced stuff with it it's intended to
be white box software so like the goal
be white box software so like the goal
is that you should be able to read
is that you should be able to read
through any bit of puffer lib that you
through any bit of puffer lib that you
want to like look at extend extend or do
want to like look at extend extend or do
stuff with and your eyes should not
stuff with and your eyes should not
bleed
interesting how this
works four update Epoch did I just like
works four update Epoch did I just like
find the YOLO config somehow that just
find the YOLO config somehow that just
works and nothing else does it's a
works and nothing else does it's a
really ridiculous amount of data
really ridiculous amount of data
reuse so when I want to run a custom EnV
reuse so when I want to run a custom EnV
I should clone um it depends right if
I should clone um it depends right if
you want to train a custom end with
you want to train a custom end with
puffer Libs training stuff then yes if
puffer Libs training stuff then yes if
you want to just use a custom end with
you want to just use a custom end with
puffer Libs emulation layers and
puffer Libs emulation layers and
vectorization then you can do that via
vectorization then you can do that via
pip because you just use like puffer li.
pip because you just use like puffer li.
emulation gymnasium puffer EnV of your
emulation gymnasium puffer EnV of your
environment and that gives you uh a
environment and that gives you uh a
puffer Li compatible one and then you
puffer Li compatible one and then you
can use like puffer lib vectorization
can use like puffer lib vectorization
multiprocessing and then you have your
multiprocessing and then you have your
simulated your hyper simulated
simulated your hyper simulated
environment just like
that what does it mean when when an
that what does it mean when when an
array has an
array has an
in okay it's not inhomogeneous it's
in okay it's not inhomogeneous it's
heterogeneous um and what that means
heterogeneous um and what that means
well first of all the thing that you ask
well first of all the thing that you ask
doesn't make sense I assume that you
doesn't make sense I assume that you
mean
mean
nonhomogeneous uh or heterogeneous
nonhomogeneous uh or heterogeneous
observation and action spaces and that
observation and action spaces and that
means when different agents have
means when different agents have
different shaped observations and
different shaped observations and
different shaped actions that is
different shaped actions that is
something that is fundamentally very uh
something that is fundamentally very uh
obnoxious to support because you get
obnoxious to support because you get
variable sized arrays
variable sized arrays
everywhere and uh yeah I'm I don't want
everywhere and uh yeah I'm I don't want
to have to deal with that if
possible
setting okay so you're just I don't know
setting okay so you're just I don't know
where you're getting that from linky
horrible
OBS I
OBS I
mean that stuff is part of puffer's
mean that stuff is part of puffer's
Advanced API for a reason
Advanced API for a reason
right it's because it's not
right it's because it's not
easy
um dire X dire y radiant
x what is this thing actually doing if I
x what is this thing actually doing if I
look at it
okay this is actually fairly reasonable
this one being stuck is
weird and then not figuring their way
weird and then not figuring their way
around
the main thing was
entropy very
weird well this definitely appears to be
weird well this definitely appears to be
working
professional bottom left
oh look he went this way for once that's
funny
okay I
okay I
memory something weird happened with
this okay so these things do train
this okay so these things do train
now it's just some weird Quirk with the
now it's just some weird Quirk with the
uh the render Library don't worry about
uh the render Library don't worry about
it I'll find it at some point before
it I'll find it at some point before
this is launched the leak is not in the
this is launched the leak is not in the
simulation I checked and it's impossible
simulation I checked and it's impossible
for the leak to be in the simulation
anyways can you layer
anyways can you layer
games linky I have 1600 games going on
games linky I have 1600 games going on
at the same time not three
though actually oddly enough it doesn't
though actually oddly enough it doesn't
work well with 1600 games it works
work well with 1600 games it works
better with um what's
this I think 80
this I think 80
games yeah it works better when I have
games yeah it works better when I have
like 80 games it's weird
there should be no reason for that as
well so I'm going to try that
well so I'm going to try that
next after
this we're not going to spend like all
this we're not going to spend like all
day screwing with uh just the really
day screwing with uh just the really
really basic version um
really basic version um
the plan is to finish this do a little
the plan is to finish this do a little
bit of analysis fiddle with one or two
bit of analysis fiddle with one or two
more things and then we're going to work
more things and then we're going to work
on like
on like
proper observations and stuff for this
proper observations and stuff for this
game and then like code improvements
game and then like code improvements
balance like actual moving forward on
balance like actual moving forward on
the project and then like the rest of
the project and then like the rest of
the experiments will basically just be
the experiments will basically just be
done via
done via
sweeps you just you need to get the
sweeps you just you need to get the
environment to a point where uh where
environment to a point where uh where
you are mostly getting something
you are mostly getting something
reasonable
reasonable
as a starting point and then from there
as a starting point and then from there
you can automate everything
you can automate everything
else getting the reasonable starting
else getting the reasonable starting
point is just sometimes very
obnoxious
e
e e
who it does actually look like the
who it does actually look like the
model's getting a little bit better over
model's getting a little bit better over
time maybe let me
[Music]
[Music]
see a little bit Improvement right there
see a little bit Improvement right there
that's
that's
weird yeah definitely you can see it
weird yeah definitely you can see it
spiking
spiking
so we'll finish this we'll pull this
so we'll finish this we'll pull this
model we'll see what it looks like we
model we'll see what it looks like we
will uh fiddle with a few
parameters I'm only logging the stats
parameters I'm only logging the stats
these stats except for the reward the
these stats except for the reward the
reward is everything but these stats are
reward is everything but these stats are
only from the first game so these like
only from the first game so these like
spikes here whenever it finishes a game
spikes here whenever it finishes a game
so it looks like each separate
so it looks like each separate
environment is only playing like
environment is only playing like
whatever this is 10 to 20 games
we
crashing no we're
good there's definitely some weird weird
good there's definitely some weird weird
Dynamics going on here ARL still got
Dynamics going on here ARL still got
some demons I haven't purged all of them
some demons I haven't purged all of them
yet it's what the exorcism is
for the other thing I wanted to do was
for the other thing I wanted to do was
just train with more Ms right yeah
and from here
um be a lot of stuff to figure
out
out
puffer does Google use RL does Google RL
puffer does Google use RL does Google RL
at what point they use RL
at what point they use RL
um yeah pretty much a lot a lot of the
um yeah pretty much a lot a lot of the
big tech companies use RL at least for
big tech companies use RL at least for
some
stuff a lot of it's R lhf these days
stuff a lot of it's R lhf these days
which is
which is
like I don't know baby mode RL for
like I don't know baby mode RL for
noobs no seriously it's it's a hard but
noobs no seriously it's it's a hard but
different problem that has different
different problem that has different
characteristics that doesn't really make
characteristics that doesn't really make
it RL in the traditional sense um
it RL in the traditional sense um
but yeah it's been used for a bunch of
but yeah it's been used for a bunch of
applications at different companies
applications at different companies
obviously nowhere near as much as
obviously nowhere near as much as
language models but RL has not had the
language models but RL has not had the
same level investment in maturity
yet Deep Mind part of Google yep Deep
yet Deep Mind part of Google yep Deep
Mind is uses a bunch I'm going to be
Mind is uses a bunch I'm going to be
right back while this job finishes up um
right back while this job finishes up um
there's a restroom be right back and
there's a restroom be right back and
then I mean at least we have something
then I mean at least we have something
working somewhat now
working somewhat now
right once you have something at least
right once you have something at least
you know is training and you know isn't
you know is training and you know isn't
just like horribly
just like horribly
bugged it's pretty easy to improve from
bugged it's pretty easy to improve from
here so I'll be right
back
e
e
e
e
e e
this finished
I don't think this is necessarily any
I don't think this is necessarily any
better than the other
models actually his entropy full
models actually his entropy full
crashed let me
see well to be fair if entropy is full
see well to be fair if entropy is full
crashed it doesn't matter because we
crashed it doesn't matter because we
literally have no entropy
bonus no it's not full crashed
pretty reasonable
pretty reasonable
model it actually does occasionally path
model it actually does occasionally path
around walls sometimes it gets stuck but
around walls sometimes it gets stuck but
usually passed around the
usually passed around the
walls sends it down
walls sends it down
mid pretty reasonable
mid pretty reasonable
overall doesn't really stop to fight
overall doesn't really stop to fight
because we turn the XP rewards off
because we turn the XP rewards off
sure but overall I'm pretty happy with
sure but overall I'm pretty happy with
that I've even seen them go down a
that I've even seen them go down a
different Lane once or twice
different Lane once or twice
yeah like that one right there on the
yeah like that one right there on the
left went down the Top Lane and then
left went down the Top Lane and then
went into the jungle pretty
went into the jungle pretty
interesting okay so the only other thing
interesting okay so the only other thing
I really wanted to do was um I wanted to
I really wanted to do was um I wanted to
see
see
if it was really the case that
if it was really the case that
um where did this
um where did this
go I wanted to see if it was really the
go I wanted to see if it was really the
case this this just doesn't work with
case this this just doesn't work with
more environments cuz there really
more environments cuz there really
should no be there really should not be
should no be there really should not be
a reason that it wouldn't so this is
a reason that it wouldn't so this is
going to be now 200 M I think right uh
going to be now 200 M I think right uh
config
config
mooba let me just back to the
mooba let me just back to the
envelope MOA so we've got eight M's
envelope MOA so we've got eight M's
we've got four per
we've got four per
batch four per batch means that we need
batch four per batch means that we need
200 M's each yes because that gets
200 M's each yes because that gets
you 800 yeah that gives you your 8K mini
you 800 yeah that gives you your 8K mini
batch you have 8K times
batch you have 8K times
128 that should give you your BPT
128 that should give you your BPT
Horizon of
Horizon of
16 it's probably very low for
16 it's probably very low for
generalized Advantage estimation um that
generalized Advantage estimation um that
could be an
issue yeah but let's try this for now
issue yeah but let's try this for now
why not right
okay so regardless of this uh we need to
okay so regardless of this uh we need to
start on the rest of this project now
start on the rest of this project now
before I get bored and tired of
before I get bored and tired of
this so let's see
um we definitely need to add some stuff
um we definitely need to add some stuff
into the observations
into the observations
right right now you can
right right now you can
observe you can essentially observe the
observe you can essentially observe the
nearby entities what team they're on and
nearby entities what team they're on and
what type they are you can't observe
what type they are you can't observe
Health
Health
Mana anything like that
Ley did lots of work today so s good
Ley did lots of work today so s good
night thanks for stopping
night thanks for stopping
by I'm going just keep hacking on this
by I'm going just keep hacking on this
and trying to make uh add some stuff to
and trying to make uh add some stuff to
the observations so that the models can
the observations so that the models can
be better mess with some rewards stuff
be better mess with some rewards stuff
like that
Chief puffing
officer see them out
officer see them out
[Music]
[Music]
uh what do I do
here do you want to be able to observe X
here do you want to be able to observe X
and
Y not
Y not
really there's not that that much stuff
really there's not that that much stuff
you need to be able to observe
you need to be able to observe
right what did they do for
right what did they do for
DOTA model architect should
DOTA model architect should
have CU they did a whole bunch of stuff
have CU they did a whole bunch of stuff
but I don't think a lot of it
but I don't think a lot of it
is crazy
relevant you need to know something
relevant you need to know something
about your skills
right
right
so Heroes only
okay so they've got like modifiers
okay so they've got like modifiers
abilities and
abilities and
[Music]
[Music]
items yeah
that would be nice to have
right and then they have unit
type whether it's under
type whether it's under
attack stats
attack stats
Health
Health
distance they have an
distance they have an
orientation absolute position and
orientation absolute position and
animation
animation
that's because they don't do a
CNN interesting that they went with this
CNN interesting that they went with this
I know
I know
why it's very hard to do something like
why it's very hard to do something like
this we're going to go a little simpler
this we're going to go a little simpler
than this for
than this for
sure um I think we can
sure um I think we can
probably the one thing that's a little
probably the one thing that's a little
bit obnoxious here is we have
bit obnoxious here is we have
to we have have to like normalize stuff
to we have have to like normalize stuff
to fit in one
to fit in one
bite the way I have this at the
bite the way I have this at the
moment
so
yeah how many of these things are
yeah how many of these things are
discreet
you definitely need to
observe
observe
Health
Health
Mana damage
modifiers
modifiers
maybe
timers you only get to see your own cool
timers you only get to see your own cool
your own cooldown
timers this is not that
timers this is not that
bad how's this experiment
bad how's this experiment
going not well that's funny
going not well that's funny
that's very
that's very
weird
weird
yeah potentially there's still some
yeah potentially there's still some
issue
then though I think uh I have some
then though I think uh I have some
better ideas as to this okay we're going
better ideas as to this okay we're going
to actually wake up a little bit here
to actually wake up a little bit here
and do
and do
um do some proper Dev on this um so for
um do some proper Dev on this um so for
observations we're going to have to
observations we're going to have to
change the way that observ work a little
change the way that observ work a little
bit observations
map observations
map
map
yeah observations map observations
extra right the way up observation Map
extra right the way up observation Map
works at the
works at the
moment is that each player
gets each player gets
gets each player gets
a 2d
grid now we need to give each player a
grid now we need to give each player a
3D
grid I have to be careful not to blow up
grid I have to be careful not to blow up
the data here too much though
right
right
o That's potentially rough
o That's potentially rough
not blowing up the amount of
not blowing up the amount of
data because right now it's
data because right now it's
150ish 200
bytes we got some continuous values
I can't add that many channels to
I can't add that many channels to
this so they're going to be very
this so they're going to be very
expensive in terms of
memory it's 11 by 11 right now it's 121
if I'm really selective with these
right you do need to see their class
right you do need to see their class
their class
right so let's
right so let's
say let's say we do classes like uh we
say let's say we do classes like uh we
roll that into type or
whatever that's just one
whatever that's just one
bite we'll discretize
Health do we need to discretize
Health do we need to discretize
Mana probably should be able to see the
Mana probably should be able to see the
opponent
Mana say we discretize Health Mana
Mana say we discretize Health Mana
damage do you need to be able to see
damage do you need to be able to see
damage not
damage not
really you know how when you're getting
really you know how when you're getting
hit if you know their health you know
hit if you know their health you know
roughly how strong they
roughly how strong they
are do we need
are do we need
level do the enemy
level again not really you should know
level again not really you should know
based on well no because if you don't
based on well no because if you don't
know Max Health right if you don't know
know Max Health right if you don't know
their Max Health if you just know their
their Max Health if you just know their
current
current
health I think we do Health Mana level
what modifiers
not
really maybe you want to know remaining
really maybe you want to know remaining
stun duration you can't even see that in
stun duration you can't even see that in
Dota can
you you just know how long your stuns
you you just know how long your stuns
last don't you there's there isn't like
last don't you there's there isn't like
a is there a cooldown timer on
them I thought you had to keep track
them I thought you had to keep track
yeah you
yeah you
can is that the case in Dota and League
can is that the case in Dota and League
that you can see remaining
stun it's a white bar okay so I can add
stun it's a white bar okay so I can add
them I just don't know if it's worth
them I just don't know if it's worth
it so let me make this clear here
it so let me make this clear here
um everything that we add
um everything that we add
oh it's a newer feature okay I don't
oh it's a newer feature okay I don't
think we need that then so here's the
think we need that then so here's the
here's the thing that we are we got to
here's the thing that we are we got to
deal with right this project is
deal with right this project is
complicated enough as is I have really
complicated enough as is I have really
fancy encoder Tech I can use to compress
fancy encoder Tech I can use to compress
observations uh I'd rather not have to
observations uh I'd rather not have to
use it here this project has enough
use it here this project has enough
other stuff going on so naively and I'm
other stuff going on so naively and I'm
going to do a little bit of compression
going to do a little bit of compression
already so naively every new feature
already so naively every new feature
that we want to add adds 21 bytes to the
that we want to add adds 21 bytes to the
observation size so right now that you
observation size so right now that you
can assume that the observation is going
can assume that the observation is going
to be between 150 to 200 bytes at the
to be between 150 to 200 bytes at the
moment and every additional feature we
moment and every additional feature we
add adds an additional 121 bytes this is
add adds an additional 121 bytes this is
not going to be a problem for the neural
not going to be a problem for the neural
network but it is potentially going to
network but it is potentially going to
add bandd like data transfer bandwidth
add bandd like data transfer bandwidth
that can slow stuff down so we really
that can slow stuff down so we really
want to limit this to like what do you
want to limit this to like what do you
need to play the game
need to play the game
you definitely need to be able to
you definitely need to be able to
see what type of uh what type of agent
see what type of uh what type of agent
everything is you need to be able to see
everything is you need to be able to see
like the class type and like the minion
like the class type and like the minion
type the minion team that sort of a
type the minion team that sort of a
thing I can do that that's
thing I can do that that's
fine um you need to be able to
see
see
health and
Mana I'm going to emit Max health and
Mana I'm going to emit Max health and
Max Mana as
Max Mana as
unnecessary need to be able to see the
unnecessary need to be able to see the
opponent's
level and probably that's about
level and probably that's about
it so we had four features or was that
it so we had four features or was that
do I say so
do I say so
Health
Health
[Music]
[Music]
Mana I said Health mana and level right
Mana I said Health mana and level right
so that's a to plus the uh the agent
so that's a to plus the uh the agent
type is four
type is four
features that's a reasonable size
features that's a reasonable size
observation that's a relatively small
observation that's a relatively small
amount of information for me to have to
amount of information for me to have to
deal
deal
with that gives
with that gives
you a lot of data
actually shouldn't be too difficult to
actually shouldn't be too difficult to
deal with in the neuronet
deal with in the neuronet
side and then for yourself you'll get a
side and then for yourself you'll get a
lot more information about
yourself
yeah how's this experiment
going well it doesn't
matter e
let going to go get something with a
let going to go get something with a
little bit of caffeine in it I'll be
little bit of caffeine in it I'll be
back in uh I'm going to be back in like
back in uh I'm going to be back in like
two minutes I'm going to just wake
two minutes I'm going to just wake
myself up because we're going to do um
myself up because we're going to do um
we're going to right now what we're
we're going to right now what we're
going to do is we're going to get the
going to do is we're going to get the
observations that we actually intend to
observations that we actually intend to
use for the game um that should enable
use for the game um that should enable
the models to do way better and then
the models to do way better and then
it'll just be a matter of like reward
it'll just be a matter of like reward
game balance and that sort of a stuff
game balance and that sort of a stuff
and cleaning up all the game code so
and cleaning up all the game code so
this actually will have us in a pretty
this actually will have us in a pretty
good spot if we just do this today be
good spot if we just do this today be
right
back
e
e
e
e
e e
okay let's lock
in for
we update this to be four dimensional
first obnoxiously we have to zero this
first obnoxiously we have to zero this
whole freaking thing don't we
observations extra we're going to leave
observations extra we're going to leave
this here for
now we go
now we go
through these
through these
nearby agents
which I think we can literally do a scan
which I think we can literally do a scan
on
right regardless
right regardless
though Target is get
though Target is get
entity Target OB we don't need
this we do need DX and and Dy here
this we do need DX and and Dy here
actually I remember why I needed those
actually I remember why I needed those
now so we'll do int
now so we'll do int
DX and
Dy
o kind of gross the way this works
R is going to be equal
R is going to be equal
to y
to y
+ Dy right
and then we need to
know Target PID is going to be PID map
know Target PID is going to be PID map
at this
at this
location we actually know R is going to
location we actually know R is going to
be a little different from this it's
be a little different from this it's
going to be plus
going to be plus
Dy it's just going to be
Dy it's just going to be
Dy plus self. Vision range I believe
Dy plus self. Vision range I believe
right because we want Ard in X hi again
right because we want Ard in X hi again
Kawa we're doing features we're doing
Kawa we're doing features we're doing
proper features for uh DOTA now so we're
proper features for uh DOTA now so we're
expanding the observation map to include
expanding the observation map to include
additional
data okay so now we have the location we
data okay so now we have the location we
have the local
have the local
location in the observations where we
location in the observations where we
want to add this thing let's do get the
want to add this thing let's do get the
target uh we can filter here
don't need to do this processing for
don't need to do this processing for
non-existent
non-existent
targets and then what we do
is self. observations map of
PID row
PID row
call one
call one
is going to be equal to
is going to be equal to
Target DOT
Target DOT
health and
health and
then
mana and then we want
mana and then we want
level you can see their health their
level you can see their health their
Mana their
level rest of this is
level rest of this is
garbage we break
we do not need this idx garbage
anymore we do not need to sort by
distance we will find out an alternative
distance we will find out an alternative
targeting mechanism I'm
targeting mechanism I'm
sure this is quite
sure this is quite
clean observations extra is still fine
clean observations extra is still fine
at the
at the
moment I would like to
moment I would like to
modify a few small things here
potentially but I think we'll start with
potentially but I think we'll start with
this yeah let's start with
this we have to modify the network
this we have to modify the network
architecture just a little bit as
well this is now 8 plus three channels
right so we do map
right so we do map
features is it going to be the
features is it going to be the
observations or CNN what is
it why aren't you using an IDE I got
it why aren't you using an IDE I got
neovim what do I need an ID
for it's not like I've never used an ID
we only want the first channel of
this
this
yes so now we have map features extra
yes so now we have map features extra
map
map
features so oops
features so oops
CNN
CNN
features we're going to concatenate
features we're going to concatenate
these and then we're going to pass these
these and then we're going to pass these
through the
through the
CNN so now we have something reasonable
here would you say that clean code isn't
here would you say that clean code isn't
always if by Clean code you mean the
always if by Clean code you mean the
clean code like the book that's an anti-
clean code like the book that's an anti-
guide that's like how to develop
guide that's like how to develop
terrible
terrible
software for
cannot reshape array
I really should stop screwing around
I really should stop screwing around
with these like hardcoded in four places
with these like hardcoded in four places
array shapes we're just going to do OBS
array shapes we're just going to do OBS
map byes is going to be equal
map byes is going to be equal
to what did we say OB
size it's also a hyper a parameter okay
size it's also a hyper a parameter okay
we'll just do self. map B
we'll just do self. map B
OBS map byes equals self. OB size time
OBS map byes equals self. OB size time
OB size Time
4 and then wherever I'm using this I can
4 and then wherever I'm using this I can
do self.
OBS
OBS
map map
bites for
what I'm doing here is I'm adjusting the
what I'm doing here is I'm adjusting the
observation
spaces to accommodate the additional
spaces to accommodate the additional
data we're now going to have 484 bytes
data we're now going to have 484 bytes
worth of observation data per agent from
worth of observation data per agent from
the local map that tells you what agents
the local map that tells you what agents
are nearby uh and now we've added to
are nearby uh and now we've added to
that that we have the health of nearby
that that we have the health of nearby
agents for Heroes you get to see their
agents for Heroes you get to see their
mana and level as well so those are
mana and level as well so those are
quite expensive in terms of memory with
quite expensive in terms of memory with
this represent
this represent
which we're going to have to see how bad
which we're going to have to see how bad
that is but the benefit of representing
that is but the benefit of representing
it this way is we don't have to do any
it this way is we don't have to do any
decoding or additional stuff in the uh
decoding or additional stuff in the uh
the forward pass of the neural network
the forward pass of the neural network
it's very very simple to use the data
it's very very simple to use the data
from there and we don't have to offload
from there and we don't have to offload
any processing over there so I think
any processing over there so I think
that this should be should be good um
that this should be should be good um
there maybe some Oddities we'll see
dot observations
dot observations
shape 487 which is perfect
right cannot
reshape into
reshape into
shape
right or
we good here
we good here
yes okay now we get buffer
yes okay now we get buffer
has wrong number of
has wrong number of
Dimensions expected
Dimensions expected
three got
four that's just uh we didn't recompile
four that's just uh we didn't recompile
our code no big deal
right I forgot some data type
Shenanigans we need to do um
Shenanigans we need to do um
Target mana and stuff now don't
Target mana and stuff now don't
we do we want to do this as a
percentage we probably want to do this
percentage we probably want to do this
as like a percentage
right hard to
right hard to
say how do I want to encode health and
say how do I want to encode health and
Mana
I think as a percentage will be cleaner
I think as a percentage will be cleaner
and then they'll just have to learn that
and then they'll just have to learn that
like hey if this thing has a giant level
like hey if this thing has a giant level
advantage on you you're probably going
advantage on you you're probably going
to get
merked we do FL
merked we do FL
flat target.
health is Health a float or an INT oh
health is Health a float or an INT oh
it's already a float so we're actually
it's already a float so we're actually
we're we're
we're we're
chilling so we do Target health over Max
chilling so we do Target health over Max
health and we just do 255
health and we just do 255
times
um 255 times this then we'll just cast
um 255 times this then we'll just cast
to unsigned
Char so now what we get is we get a
Char so now what we get is we get a
value from 0 to 255 that tells you the
value from 0 to 255 that tells you the
percentage of the health and the reason
percentage of the health and the reason
that we're doing this is that we're
that we're doing this is that we're
trying to uh pack this value into one
trying to uh pack this value into one
bite
bite
so we can do the same thing
so we can do the same thing
here and then we can
here and then we can
do the same thing
do the same thing
here uh unsign Char target. level
here uh unsign Char target. level
divided by 30.0 which is the max
divided by 30.0 which is the max
level
perfect and connect
perfect and connect
cast type double to unsign char that's
cast type double to unsign char that's
funny I forgot that you have to do
funny I forgot that you have to do
f are you not allowed to do F like
this
30f no we say a floating
30f no we say a floating
scon sure it was just
F it's kind of weird
this
this
work cannot assign type
work cannot assign type
double uh and how would this possibly
be assign CH
type oh is it just
type oh is it just
this I think I think I just needed this
this I think I think I just needed this
set of parens here
set of parens here
right yeah and now we have cannot assign
right yeah and now we have cannot assign
float I forgot that this is like it
float I forgot that this is like it
prefix it binds tightly I
prefix it binds tightly I
forgot I haven't done sea style cast in
forgot I haven't done sea style cast in
a
while makes sense
okay this
okay this
compiles what happens when we run a
train differing
train differing
extents in copy
contents 4 and 11
this needs to be
oops we need to add this as
well
rebuild we're going to have to redo some
rebuild we're going to have to redo some
optimization after this but that's fine
optimization after this but that's fine
so much stuff to
fix view size is not
compatible
yeah oh well I think I just did this
yeah I just did this
yeah I just did this
wrong
wrong
right W minus1
right W minus1
111 yeah I just did it
wrong this is now- 1114 I believe
okay we get a
okay we get a
Cuda cuda's mad
right before the one
hot because you didn't uh
hot because you didn't uh
slice it's
okay we got the one
okay we got the one
Hots we got our
Hots we got our
extras must must match exactly P CNN
extras must must match exactly P CNN
features
features
shape 012 this now has to be
shape 012 this now has to be
concatenated along
concatenated along
Dimension this is dimension
three and I think that fixes
it apparently
it apparently
not well I might be missing a per
right extra map
right extra map
features this is totally wrong colon
features this is totally wrong colon
colon colon
negative
star
star
yeah um but then this there's a channels
yeah um but then this there's a channels
perut here as
well just tensor
Shenanigans I'm be sure to have this
Shenanigans I'm be sure to have this
nicely cleaned up for the uh the release
nicely cleaned up for the uh the release
of it too many indices for tensor of
of it too many indices for tensor of
Dimension
two yeah so this is actually got to be
two yeah so this is actually got to be
this is CNN
this is CNN
features colon colon
features colon colon
colon
colon
three star.
float okay so now we have our extra map
float okay so now we have our extra map
features and our map features.
shape and we still need to permute this
shape and we still need to permute this
it looks like because we do channels
it looks like because we do channels
first so this is
first so this is
actually
prute which means that we're stacking
prute which means that we're stacking
along the First
along the First
Dimension let's see if this gives us our
Dimension let's see if this gives us our
features we're looking for 11 channels
features we're looking for 11 channels
worth of features I
believe must match in all Dimensions
believe must match in all Dimensions
okay apparently
not this looks good to
me map features
me map features
CNN
features oh
wait so we want to concat map features
wait so we want to concat map features
and extra map
and extra map
features so I had it right I just had
features so I had it right I just had
the wrong
names okay perfect
names okay perfect
CNN so this is now a very good
CNN so this is now a very good
representation for uh our
representation for uh our
Network we have 11 channels of 11 by 11
Network we have 11 channels of 11 by 11
there's nothing special about having it
there's nothing special about having it
be a cube like that it just happened to
be a cube like that it just happened to
work out that way
work out that way
um but now how much data is that
um but now how much data is that
actually 800
times 96
times 96
KB 96 kb per batch not
bad and this seems to now this seems to
run yeah cool our network runs
run yeah cool our network runs
now and uh I'm not going to start
now and uh I'm not going to start
training just yet what we're going to do
training just yet what we're going to do
is we're going to clean this up and
is we're going to clean this up and
allocate um some additional extra
allocate um some additional extra
variables so the architecture for this
variables so the architecture for this
at the moment is that you have some
at the moment is that you have some
number of things that you can observe
number of things that you can observe
about all of the agents around you you
about all of the agents around you you
can observe Everybody's Health
can observe Everybody's Health
everybody's mana and everybody's level
everybody's mana and everybody's level
but then you of course you have some
but then you of course you have some
extra information like your cool Downs
extra information like your cool Downs
aren't visible to everybody and stuff so
aren't visible to everybody and stuff so
we're going to add those into an extra
we're going to add those into an extra
Vector that you just append on at the
Vector that you just append on at the
end
we're going to have to change the shapes
we're going to have to change the shapes
as well we might as well just do
that I don't know
that I don't know
10 we'll see how many we end up
using
so yeah this is your uh your shape here
so yeah this is your uh your shape here
it's a little bit obnoxious that they
it's a little bit obnoxious that they
have to
have to
be
be
U like this but it's fine we bite
pack where else do I use this I have it
pack where else do I use this I have it
in the observation space should be in
in the observation space should be in
the buffer right yeah right
here okay and then there are a couple
here okay and then there are a couple
other places where I have this thing
other places where I have this thing
hardcoded so we'll just swap that out
hardcoded so we'll just swap that out
now I think underscore Phil yeah this is
now I think underscore Phil yeah this is
a dead method
a dead method
right cuz we have this in C which is
right cuz we have this in C which is
better better faster
stronger
stronger
yeah do I have threes anywhere
else according to this I
don't right here yeah there was one
more cool so now we get to allocate
more cool so now we get to allocate
these we'll have to pick which variables
these we'll have to pick which variables
we want to
include I think that for yourself you
include I think that for yourself you
get to see your pretty much most of
get to see your pretty much most of
these
we'll find a better way to copy them
we'll find a better way to copy them
later I'm sure but to start
with these are right now they're just
with these are right now they're just
added right
added right
here
here
so we're going to add in your
so we're going to add in your
X your
X your
y your reward which this is wrong at the
y your reward which this is wrong at the
moment
moment
um o how do we do
um o how do we do
this
this
27 255
values if we limit rewards to negative
values if we limit rewards to negative
-1 to one hold
-1 to one hold
on let's
on let's
say -1
say -1
time 128 Plus
can't do plus
can't do plus
256 wait negative 1 needs to map to Z so
256 wait negative 1 needs to map to Z so
it should be plus
it should be plus
obviously then 1 *
obviously then 1 *
128 this is 256 which is too high
this is just an asymmetric problem right
this is just an asymmetric problem right
there're
there're
values so you can't have 256 values and
values so you can't have 256 values and
distribute those evenly
I would like to scale this
perfectly kind of important
this gives you negative one oh no gives
this gives you negative one oh no gives
you
you
one that's perfect
right you never get zero
but this is
but this is
fine so we just do
fine so we just do
127 time rewards plus 128
X and Y are also
X and Y are also
fine I think you can even multiply these
fine I think you can even multiply these
by
by
two two times because they're 0 to
128 and they'll never actually be 128 so
128 and they'll never actually be 128 so
you're safe here
so now we're going to do uh
so now we're going to do uh
health and uh health and everything
right level's fine actually
here uh we don't really need XP
we want to see Max
health I think that we're fine just
health I think that we're fine just
doing it this
way we give you
way we give you
Mana we've already done
level we can do
damage plus six times player
damage plus six times player
level 30 is 180 so your damage is never
level 30 is 180 so your damage is never
over 255 in the current
over 255 in the current
format we'll make sure that we add
format we'll make sure that we add
bounds
checks so unsign car Char player damage
checks so unsign car Char player damage
it's approximately in the correct bound
move
speed move mod mod ifier as
well we do the stun timer
is that
is that
everything is hit
everything is hit
Level attack cool
Level attack cool
down basic attack cool down which is
down basic attack cool down which is
like going to be 50 times
like going to be 50 times
this basic
this basic
attack timer is also 50 times
attack timer is also 50 times
this qwe timers these are
this qwe timers these are
fine okay I think these are like
fine okay I think these are like
reasonable to to start with and um
reasonable to to start with and um
potentially we will do auto scaling
potentially we will do auto scaling
instead of manual scaling here but the
instead of manual scaling here but the
thing is
thing is
like because we're
like because we're
discretizing
discretizing
we yeah because we're discretizing here
we yeah because we're discretizing here
it kind of makes sense to scale it
it kind of makes sense to scale it
myself because some of these values are
myself because some of these values are
floats though maybe not maybe I don't
floats though maybe not maybe I don't
have to we'll
have to we'll
see but this is all nice data
so we ended up with 17 different values
so we ended up with 17 different values
here so we'll set this to
17 and we
will edit the
will edit the
uh we'll edit the network as
uh we'll edit the network as
well to take 17
well to take 17
input
variables
variables
okay appears to work
this is
this is
minus
minus
17 of minus 3
okay so that was relatively
painless now in theory we have all of
painless now in theory we have all of
the data that we need for
the data that we need for
training in practice we're going to add
training in practice we're going to add
a few additional values I'm
sure um but this should make quite a big
sure um but this should make quite a big
difference
already now it's not going to make a big
already now it's not going to make a big
difference in um
difference in um
the current training task because it's
the current training task because it's
very
basic we also I think the next thing
basic we also I think the next thing
that we need to do is we need to make
that we need to do is we need to make
sure
sure
that um players can tell their hero
that um players can tell their hero
class that's a big
thing and that's also going to be
thing and that's also going to be
important for the
important for the
renderer
so let's do that next
we'll do int
um radiant reap it's
um radiant reap it's
three dire creep it's going to be
four neutral going be five
uh
radiant what order did I Define these
radiant what order did I Define these
things
in support assassin first tank
carry w
what I say
what I say
radiant support assassin first tank
radiant support assassin first tank
carry
okay and we forgot the tower as well the
okay and we forgot the tower as well the
tower needs to be int
um should I distinguish these two
or should it just be like
Tower probably
easier
easier
no the icons on the map can be the same
right oh wait we already have Tower we
right oh wait we already have Tower we
have got Tower
have got Tower
empty really it should be empty let's do
empty really it should be empty let's do
wall is one Tower is
wall is one Tower is
two creep three four neutral 6 7 8 9
dire what's up with the auto complete
today this actually fits perfectly into
today this actually fits perfectly into
values that's very nice to have so this
values that's very nice to have so this
fits
fits
perfectly minus the debug one which
perfectly minus the debug one which
we'll ignore for
now here are all the different creep
types this will allow us to do a bunch
types this will allow us to do a bunch
of stuff
we'll do support
we'll do support
zero we'll use these as
zero we'll use these as
well
well
type
type
class um can't do
class um can't do
class
class
uh hero
Co yeah let's do that
if player.
type weird how I have this
done player. type
is entity player
oh I
oh I
see yeah there's like a lot of different
see yeah there's like a lot of different
typing information we got to get right
typing information we got to get right
here so we've got int PID let's make
here so we've got int PID let's make
sure we get this right so this is going
sure we get this right so this is going
to
to
be entity
type
type
um hero
type grid
type red ID hero type
so this
is then
is then
team
type any type hero type grid ID team
type any type hero type grid ID team
this is fine
and we're going to have to change quite
and we're going to have to change quite
a bit of stuff from
a bit of stuff from
that I think it's probably best to just
that I think it's probably best to just
go through very
slowly nothing should be
slowly nothing should be
here we're going to have to change a lot
here we're going to have to change a lot
of stuff though probably in the move
of stuff though probably in the move
functions mostly but I don't want to
functions mostly but I don't want to
miss anything
so first of all in the observations
um does anything change
um does anything change
here not yet we're going to add a few
here not yet we're going to add a few
things to it
later spawn
later spawn
Tower this is now Tower at entity type
Tower this is now Tower at entity type
is going to be entity
is going to be entity
Tower tower.
Tower tower.
grid idea is going to be Tower
how much stuff do I want to give
how much stuff do I want to give
different IDs
too what stuff would I need to have
too what stuff would I need to have
different
different
Graphics that's kind of the
Graphics that's kind of the
test I think you only need the 16 grid
test I think you only need the 16 grid
idas I
defined so this is Tower this is
defined so this is Tower this is
fine it's got PID should be up
top yeah that's
top yeah that's
fine so now reset
fine so now reset
does player. PID player. entity type
does player. PID player. entity type
player. grid ID should
be player.
be player.
grid I oh I actually have to do this
grid I oh I actually have to do this
normally don't
normally don't
I player
I player
dot grid type has got to
dot grid type has got to
be
radiant that's a
radiant that's a
support it's not radiant player is
support it's not radiant player is
it now it's radiant
support I don't know why it can't
support I don't know why it can't
autocomplete this
an
an
assassin carry at Le at least it auto
assassin carry at Le at least it auto
completes off of
this dire support assassin burst tank
this dire support assassin burst tank
carry
perfect that takes care of
perfect that takes care of
that and
uh we'll probably adjust the values as
uh we'll probably adjust the values as
well for all of these in there
well for all of these in there
because we're going to have different
because we're going to have different
Max healths and stuff per
Max healths and stuff per
class we might even add different
class we might even add different
scaling per class probably
will that'll be
will that'll be
fun and I
fun and I
actually we're probably going to move
actually we're probably going to move
where this code is for now but um this
where this code is for now but um this
is not bad to have like on
is not bad to have like on
initialization to just have like all of
initialization to just have like all of
the hard-coded values for all the
the hard-coded values for all the
different types of things you're
different types of things you're
initializing
initializing
there's really not a bad way of doing
there's really not a bad way of doing
it
it
yeah I'm actually tempted even to
yeah I'm actually tempted even to
like I could go through and initialize
like I could go through and initialize
all the creeps like this even
all the creeps like this even
maybe that might not be a bad idea then
maybe that might not be a bad idea then
when you reset creeps you literally just
when you reset creeps you literally just
move
them we'll start like this though
team
team
okay I think we're jamming this
now hard code the lanes
this is just for the scripted AIS but
this is just for the scripted AIS but
we're going to do
we're going to do
um what did we do zero and one is
um what did we do zero and one is
two so we'll
two so we'll
do uh player.
do uh player.
lane
two these are all
two these are all
twos and then
twos and then
is
is
four this gets a
one and three is a zero
perfect layer Lane is 3
39 I don't know why it can't copy data
like Seven's got to be a
like Seven's got to be a
four and then eight should be
five so these Lane assignments don't
five so these Lane assignments don't
actually do anything when we're not
actually do anything when we're not
using the scripted model but uh very
using the scripted model but uh very
nice for the scripted
nice for the scripted
model okay that's actually a lot nicer
model okay that's actually a lot nicer
I'm way happier with that
I'm way happier with that
now so we're actually kind of cleaning
now so we're actually kind of cleaning
stuff up in the process of doing this
huh and all this agent type stuff we
huh and all this agent type stuff we
literally no longer have to care about
literally no longer have to care about
because it's literally now just um
because it's literally now just um
player. grid
ID this gets deleted so this logic that
ID this gets deleted so this logic that
runs every single time anything moves
runs every single time anything moves
anywhere no longer needs to
anywhere no longer needs to
happen and uh now the move to function
happen and uh now the move to function
is actually very simple it's just a
is actually very simple it's just a
sanity check move on grid and then move
sanity check move on grid and then move
on the PID map
um
I don't see anything that needs to be
I don't see anything that needs to be
updated here the spawn needs to be
updated here the spawn needs to be
updated entity type is entity neutral
updated entity type is entity neutral
right
right
um Rd ID gets
neutral and that is
neutral and that is
fine then it gets to be team two
which is not anybody's
team I don't see any logic to update
team I don't see any logic to update
there there will be a little bit to
there there will be a little bit to
update and spawn
update and spawn
creep entity type is entity
creep we'll do
creep we'll do
creep grid
creep grid
idea is going to
be.
be.
team.
team and this goes
away respawn
away respawn
player doesn't touch any of those
player doesn't touch any of those
attributes respawn creep doesn't touch
attributes respawn creep doesn't touch
any of those
any of those
attributes target. type no longer exists
attributes target. type no longer exists
this is now entity
this is now entity
type it's entity
type it's entity
player entity
type this is fine
entity
type there may be some additional type
type there may be some additional type
checks in here right like over
here power type so I assume there's a
here power type so I assume there's a
get type somewhere
get type somewhere
here
here
type uh here we need this is entity
type
and that might be all of
them these are just
them these are just
skills they shouldn't be checking any of
skills they shouldn't be checking any of
this except this one apparently does
entity
types
type
DOT layer.
DOT layer.
type is not in here entity. type is not
type is not in here entity. type is not
in here uh creep. type and neutral. type
in here uh creep. type and neutral. type
target.
type okay I think we're good this is a
type okay I think we're good this is a
substantial change so this might take a
substantial change so this might take a
little bit of
little bit of
figuring unless I oneshot
it likely the rendering is not going to
it likely the rendering is not going to
make sense for a little bit until I
make sense for a little bit until I
update the graphics and the uh the
update the graphics and the uh the
colors
colors
assignments mode
assignments mode
render or
render or
eval render mode
RB I think actually this will crash
RB I think actually this will crash
because yep don't have enough colors
because yep don't have enough colors
which is
fine so let's just take these grid IDs
fine so let's just take these grid IDs
and let's go
and let's go
to
to
um
um
environments
environments
and let's just get some nice colors for
and let's just get some nice colors for
stuff
right and this will just
right and this will just
B 0 to
B 0 to
55 this is the walls are
55 this is the walls are
blue I
suppose don't really like the walls
suppose don't really like the walls
being blue like
that I would rather use
that I would rather use
um is it 178
78 this is the puffer
color uh Tower is going to
be Tower is going to be an objective
be Tower is going to be an objective
marker let's go get a a Color Picker
up uh what do we want the towers to be
up uh what do we want the towers to be
like orange
green green for the objective markers
green green for the objective markers
would stand out nicely
right so it's
Tower uh radiant
creeps oh we need a color identity for
creeps oh we need a color identity for
each side don't
each side don't
we what do they use in Dota it's it's
we what do they use in Dota it's it's
green and blue I mean green and red
green and blue I mean green and red
isn't
it for the uh the
it for the uh the
creeps yeah so maybe we don't use that
creeps yeah so maybe we don't use that
for the
for the
tower
um green plus red is orange so
orange FF
six
six
five so this is the
five so this is the
tower uh radiant creeps will be
tower uh radiant creeps will be
in I say darkish green right darkish
in I say darkish green right darkish
greens and reds
maybe dire creep will be
maybe dire creep will be
dark
red neutral creep will be
gray and then uh
gray and then uh
initially for radiant
I think what we do is we
use do we use tones
this is nope
need like some good colors for the uh
need like some good colors for the uh
the creeps oh we did
um Let's do let's actually change this
um Let's do let's actually change this
up a little
up a little
bit we're going to change the default
bit we're going to change the default
colors a bit from the uh The Originals
colors a bit from the uh The Originals
cuz red green is bad for color blind and
cuz red green is bad for color blind and
also it's like we're going to do red
also it's like we're going to do red
white and blue because of course we
are so radiant creeps will
be uh they can
be uh they can
stay no these need to be dark blue like
stay no these need to be dark blue like
this 12
eight yeah so it's red versus
eight yeah so it's red versus
blue that's
better and then we'll basically we'll
better and then we'll basically we'll
just give you
like yeah Canan versus that's even
like yeah Canan versus that's even
better
we do need to get you a color palette
we do need to get you a color palette
though for the
though for the
heroes they need to stand out quite a
bit still doesn't solve this
problem I do get to use all of the
problem I do get to use all of the
bright colors now though
I could do
I could do
um dire creeping 128 like this like dark
um dire creeping 128 like this like dark
and I could just make these for now I'm
and I could just make these for now I'm
going to just make them um like the pure
colors
colors
okay and then for now we'll just do is
okay and then for now we'll just do is
it0 0 255
all
right this should be a reasonable color
right this should be a reasonable color
palette see if we can see
anything Q to error
anything Q to error
device side
device side
assert yes because now we have to change
assert yes because now we have to change
this to
be6 we have 16 possible channels instead
be6 we have 16 possible channels instead
of
eight 16 +
3 it's one hot with a Max of
16 it looks like I messed up some color
16 it looks like I messed up some color
codes
codes
because this is not what I intended
because this is not what I intended
though it's very
funny so let's see how did this
happen oh uh because I think we're using
happen oh uh because I think we're using
these Maybe
where's the wall thing wall is now
where's the wall thing wall is now
one and Tower is two
not need to do
this okay let's see about this
now there we go
so the
so the
walls the colors did not come out
great yeah
because we flip first of all we flip the
because we flip first of all we flip the
uh the radiant and the
uh the radiant and the
dire so let's undo that
dire so let's undo that
be0 to
55 and this one is
55 and this one is
255 Z
see how this
looks okay red versus
looks okay red versus
blue way
better
better
yeah creep colors are
off let's do dire creeps are just going
off let's do dire creeps are just going
to be
to be
128 radiant creeps will
be this see if this looks
better oh yeah there we
go and we'll have to come up with a
go and we'll have to come up with a
better color palette but for now you've
better color palette but for now you've
got dark red dark blue
can you give a quick rundown on what is
can you give a quick rundown on what is
being solved here if you've uh this is
being solved here if you've uh this is
the first time that you're seeing this
the first time that you're seeing this
project this is a miniature version of
project this is a miniature version of
am MOA it's based off of DOTA uh except
am MOA it's based off of DOTA uh except
that it runs at over a million frames
that it runs at over a million frames
per second and the goal is to train
per second and the goal is to train
reinforcement learning agents on it to
reinforcement learning agents on it to
play the game so you essentially have uh
play the game so you essentially have uh
if you've seen the open AI 5 project
if you've seen the open AI 5 project
where open AI trained well they used a
where open AI trained well they used a
ton of compute like tens of millions of
ton of compute like tens of millions of
dollars of compute to train uh agents to
dollars of compute to train uh agents to
play the full game of DOTA we're doing a
play the full game of DOTA we're doing a
mini version of that but we're going to
mini version of that but we're going to
make it like 10,000 times faster so
make it like 10,000 times faster so
you're going to be able to train this on
you're going to be able to train this on
One desktop and I'm building the
One desktop and I'm building the
simulator I am uh figuring out what data
simulator I am uh figuring out what data
to pass from the simulator to the
to pass from the simulator to the
networks and uh what I'm doing right now
networks and uh what I'm doing right now
at the this particular moment is I redid
at the this particular moment is I redid
the way that the environment represents
the way that the environment represents
different types of Agents different
different types of Agents different
Heroes and such and uh I had to fix the
Heroes and such and uh I had to fix the
renderer so I could actually see what's
renderer so I could actually see what's
going on and then now that this is fixed
going on and then now that this is fixed
I'm going to be passing all that new
I'm going to be passing all that new
data into the neural network which will
data into the neural network which will
hopefully allow the agents to better
hopefully allow the agents to better
learn how to play the
game that's what's going on here ultra
game that's what's going on here ultra
high performance simulation engineering
high performance simulation engineering
for reinforcement
learning ultra high per
is this RGB array render take
colors one
colors one
sec just want to make sure the other
sec just want to make sure the other
render is not broken by my new
change RGB array render does take colors
yeah okay so this one works as well just
yeah okay so this one works as well just
the same
the same
perfect we fixed the
renderer
now let's see if there's anything else
now let's see if there's anything else
we want include in the agent
we want include in the agent
observations
here only thing I can think
here only thing I can think
of is do we want to include the hero
class it's not really a format of
class it's not really a format of
data a form a data formula that's going
data a form a data formula that's going
to be
to be
useful I could add it as five separate
useful I could add it as five separate
variables
maybe how far are you going with
maybe how far are you going with
modeling how many champs they have a
modeling how many champs they have a
simplified kit how many players uh it's
simplified kit how many players uh it's
a 5v5 the map is currently modeled like
a 5v5 the map is currently modeled like
one to one with the DOTA map app uh
one to one with the DOTA map app uh
except I've only implemented obstacles
except I've only implemented obstacles
so far I haven't implemented the forest
so far I haven't implemented the forest
or Vision or anything like that uh they
or Vision or anything like that uh they
do have a simplified kit they're not one
do have a simplified kit they're not one
to one with the DOTA Heroes because the
to one with the DOTA Heroes because the
abilities that many DOTA Heroes Have are
abilities that many DOTA Heroes Have are
very complicated and really require you
very complicated and really require you
to implement a lot more of the game so
to implement a lot more of the game so
they do have simplified kits I currently
they do have simplified kits I currently
have three skills per hero um I'm
have three skills per hero um I'm
thinking about adding you know ultimates
thinking about adding you know ultimates
and stuff though they're harder to learn
and stuff though they're harder to learn
to use um so they're five unique Heroes
to use um so they're five unique Heroes
it's mirror match and it runs really
it's mirror match and it runs really
fast um and we can continue I mean I can
fast um and we can continue I mean I can
continue to build out from here I've
continue to build out from here I've
only been developing this thing for less
only been developing this thing for less
than two weeks it's mainly going to be
than two weeks it's mainly going to be
based on how interested people are in
based on how interested people are in
this project so the current plan is to
this project so the current plan is to
make this like mini MOBA that is uh that
make this like mini MOBA that is uh that
can be learned with reinforcement
can be learned with reinforcement
learning get some baseline agents that
learning get some baseline agents that
train to do some interesting stuff in
train to do some interesting stuff in
the environment make sure it's very fast
the environment make sure it's very fast
very clean and very easy to use and then
very clean and very easy to use and then
release it if people think that this is
release it if people think that this is
uh if people are very interested in this
uh if people are very interested in this
project and want to see like more
project and want to see like more
specific stuff added to it then you know
specific stuff added to it then you know
I will go ahead and do
that okay so so far this is pretty
that okay so so far this is pretty
reasonable um
this rewards
this rewards
variable I think we want to move
this I'm going to move this down here
this I'm going to move this down here
we're going to have to reindex
everything thank you it's all open
everything thank you it's all open
source as well you're free to uh
source as well you're free to uh
download it and play with it
oops
this
works so we have to
works so we have to
um maybe just add some indicator values
um maybe just add some indicator values
right
they should know their team as
well so let's
do let's add some let's just add a few
do let's add some let's just add a few
additional
things team we didn't add team right
things team we didn't add team right
yeah let's add team
255 18
19 Okay so we've got some
19 Okay so we've got some
indicators we now have 22
indicators we now have 22
variables associated with the player we
variables associated with the player we
should probably put a little bit more
should probably put a little bit more
effort into the scaling
here unsign Char player.
here unsign Char player.
Lev divided by
30 be 255 times player. level over 30
30 be 255 times player. level over 30
right
yeah and then the timers we don't know
yeah and then the timers we don't know
yet player is hit player.
yet player is hit player.
team these look
fine so 22
fine so 22
variables oh
variables oh
23 it's got to be 22 right
23 it's got to be 22 right
here we're probably going to want some
here we're probably going to want some
different reward components
huh yeah we're going to definitely want
huh yeah we're going to definitely want
some different reward
some different reward
components maybe what we'll do is we'll
components maybe what we'll do is we'll
make each individual reward component
make each individual reward component
between um negative 1 and
one
one
yeah and then you can always scale the
yeah and then you can always scale the
reward
right well no that doesn't make any
right well no that doesn't make any
sense come to think of it
I think we leave it here for now we
I think we leave it here for now we
start with
this
welcome we are
welcome we are
currently uh adding a whole bunch of
currently uh adding a whole bunch of
better observation data we're adding a
better observation data we're adding a
bunch of better observation data into
bunch of better observation data into
the model
the model
we currently have um four four channels
we currently have um four four channels
worth of map
worth of map
data that contains information about
data that contains information about
nearby about the map itself and nearby
nearby about the map itself and nearby
Heroes and now agents have
Heroes and now agents have
23 variables about themselves that they
23 variables about themselves that they
can
see so this now goes up to
23 and this is player OBS n
where is
it so this is
it so this is
now
three
three this run
this does
this does
run about the same speed as
run about the same speed as
before
before
perfect let's Commit This up and uh see
perfect let's Commit This up and uh see
if we
if we
can we can get something to train on
can we can get something to train on
this
now upper lib
right
okay so now we have the latest
code uh
diff 200 M's okay I know 10m worked
diff 200 M's okay I know 10m worked
before
I'm going to set this to 10 even though
I'm going to set this to 10 even though
it's going to be
slow that's
fine yeah this is fine so let's run
fine yeah this is fine so let's run
this we have to run
setup does this run
train it's very slow but it's very slow
train it's very slow but it's very slow
because the the small number of
because the the small number of
environments that's
acceptable it does appear to be training
acceptable it does appear to be training
like it was
before uh
yeah oh yeah there it goes
so this should just run down mid it's a
so this should just run down mid it's a
very dumb model that we just trained I'm
very dumb model that we just trained I'm
hoping it just runs down mid if so that
hoping it just runs down mid if so that
just means I didn't break
just means I didn't break
anything that's all that
means a reward yeah perfect
let's watch
them and then what we're going to have
them and then what we're going to have
to do is we're basically going to have
to do is we're basically going to have
to figure out
to figure out
like how to get this thing training High
like how to get this thing training High
perf and consistently for this dumb task
perf and consistently for this dumb task
before we try to make it run on
before we try to make it run on
something more fancy you know because we
something more fancy you know because we
we can start giving them rewards for
we can start giving them rewards for
like levels and XP and stuff
like levels and XP and stuff
um but it's not really going to make a
um but it's not really going to make a
difference if the training is not well
difference if the training is not well
set up in the first place
so we're going to look at this and then
so we're going to look at this and then
I'm going to take a quick second we're
I'm going to take a quick second we're
going to come back and finish that side
going to come back and finish that side
of things um
of things um
okay these guys aren't doing anything
because model PA
because model PA
ET there we go
okay they go down Lane
right these guys
right these guys
don't that's funny that Dyer didn't
don't that's funny that Dyer didn't
figure it out
yet
yet
H well I think they will improve over
H well I think they will improve over
time
time
right here I'm going to launch a
right here I'm going to launch a
slightly longer
slightly longer
run and uh I'm going to go use the
run and uh I'm going to go use the
restroom I'm going to go make myself a
shake and then we will come back and
shake and then we will come back and
um yeah we'll see if we can get this
um yeah we'll see if we can get this
thing training way better and more
consistently maybe hopefully yeah they
consistently maybe hopefully yeah they
should
should
but the thing is like this is a really
but the thing is like this is a really
dumb sanity check
anyways here we'll run this for 100
anyways here we'll run this for 100
million steps the problem is at the
million steps the problem is at the
moment the uh the configuration I'm
moment the uh the configuration I'm
running is very slow because the larger
running is very slow because the larger
one seems to be unstable so as soon as I
one seems to be unstable so as soon as I
get the uh the larger configuration to
get the uh the larger configuration to
be more stable it'll just chug so much
be more stable it'll just chug so much
more training data that it should work
more training data that it should work
but it's going to take a little bit of
but it's going to take a little bit of
effort to get it to there so I'm going
effort to get it to there so I'm going
to leave this up I'm going to get the uh
to leave this up I'm going to get the uh
the experiment up for you to watch while
the experiment up for you to watch while
I take a quick minute to go make myself
I take a quick minute to go make myself
a
a
shake few minutes there and this run
shake few minutes there and this run
only takes s six minutes so that's good
timing here
reward commercial break more like I
reward commercial break more like I
haven't had lunch break let me go make a
shake
e
e
e
e
e
e
e
e
e e
okay so uh the training speed halfed
okay so uh the training speed halfed
that's
new and uh it also refreshed so that you
new and uh it also refreshed so that you
couldn't see it
couldn't see it
whoops okay
whoops okay
um
interesting it goes up to 0.25
dire level mean radiant level the fact
dire level mean radiant level the fact
that the radiant and the dire are the
that the radiant and the dire are the
same levels mean I would think that
same levels mean I would think that
they're both going towards the center
they're both going towards the center
we'll see when this finishes training of
course but this looks
course but this looks
decent yeah yeah so this was 10 mil
decent yeah yeah so this was 10 mil
before right this is where we stopped
before right this is where we stopped
training before and then it takes a
training before and then it takes a
little longer to get better
little longer to get better
okay it's not
bad of course this is training
bad of course this is training
incredibly slowly so let's just
incredibly slowly so let's just
preemptively start thinking about why
preemptively start thinking about why
why that is
um when I try to simulate so the issue
um when I try to simulate so the issue
is that when I try to simulate way more
is that when I try to simulate way more
copies of the environment which you need
copies of the environment which you need
to simulate more copies of the
to simulate more copies of the
environment if you want want to train
environment if you want want to train
more quickly has nothing to do with the
more quickly has nothing to do with the
environment it has to do with um optimal
environment it has to do with um optimal
data batch sizes for the GPU essentially
data batch sizes for the GPU essentially
like you want you always want at least
like you want you always want at least
4,000 but preferably 8,000 like
4,000 but preferably 8,000 like
8192 uh steps worth of
8192 uh steps worth of
data that's a good mini batch size for
data that's a good mini batch size for
small reinforcement learning networks to
small reinforcement learning networks to
saturate your
saturate your
GPU but in order to get that many
GPU but in order to get that many
batches well 8,000 steps that's 10
batches well 8,000 steps that's 10
agents per environment so that's 800
agents per environment so that's 800
environments you need to simulate but in
environments you need to simulate but in
practice you need to simulate at least
practice you need to simulate at least
twice that number of environments so
twice that number of environments so
that while the GPU is running for one
that while the GPU is running for one
set of environments the other ones are
set of environments the other ones are
running in the background so that you're
running in the background so that you're
never wasting time that's how this
never wasting time that's how this
environment here is like taking no time
environment here is like taking no time
whatsoever
whatsoever
um so yeah that's the awkward part about
um so yeah that's the awkward part about
this uh the other thing that's kind of
this uh the other thing that's kind of
weird is that this doesn't seem to train
weird is that this doesn't seem to train
correctly unless you you run multiple
correctly unless you you run multiple
updates through the same
updates through the same
data that doesn't make much sense to me
either so we're going to have to play
either so we're going to have to play
with those we're going to have to play
with those we're going to have to play
with those things this is technically
with those things this is technically
one of the areas where I probably should
one of the areas where I probably should
just run a hyperparameter sweep and
just run a hyperparameter sweep and
later on in this project that's
later on in this project that's
absolutely what I'll be doing I'll just
absolutely what I'll be doing I'll just
kind of be automating everything but
kind of be automating everything but
early on it's good to gain an intuition
early on it's good to gain an intuition
for like what things in the environment
for like what things in the environment
are wonky and what aren't so it's wor
are wonky and what aren't so it's wor
spending a little bit of time on this
spending a little bit of time on this
just a little bit and then after that we
just a little bit and then after that we
can do uh you know we can do a little
can do uh you know we can do a little
bit of reward shaping a little bit of
bit of reward shaping a little bit of
like just get a bunch of rewards
like just get a bunch of rewards
extracted from the environment we'll
extracted from the environment we'll
look at the DOTA paper we'll see what
look at the DOTA paper we'll see what
rewards they used um and that'll
rewards they used um and that'll
probably be what we'll get done for the
probably be what we'll get done for the
rest of
rest of
today
yeah cheers though
almost done
all of this is
all of this is
oops right here all of this is open
oops right here all of this is open
source right
here go ahead and start the repo on your
here go ahead and start the repo on your
way
way
in helps me a whole
in helps me a whole
bunch and uh the latest Dev is right
bunch and uh the latest Dev is right
here in any config you can go look at
here in any config you can go look at
where the changes are it's in puffer lib
where the changes are it's in puffer lib
environments ocean
environments ocean
MOBA so all this is available pretty
MOBA so all this is available pretty
much in real time as I'm developing
it as soon as this finishes
it as soon as this finishes
synchronizing yeah we got our
model e
okay very nice they run at each other
okay very nice they run at each other
down
down
mid so this is what we
wanted sometimes they do weird stuff but
wanted sometimes they do weird stuff but
for the most
for the most
part like there's like one agent that's
part like there's like one agent that's
weird but for the most part they do
weird but for the most part they do
reasonable
things some of them they even path
things some of them they even path
around the obstacles on the way there
around the obstacles on the way there
yeah nice basic sanity check but
reasonable so let's see how long it took
reasonable so let's see how long it took
them to learn that because we have some
them to learn that because we have some
additional stats that we can rely on
additional stats that we can rely on
here
uh
oh welcome
okay so we can actually
see if we look at the training
see if we look at the training
here we can see dire level mean and
here we can see dire level mean and
radiant level mean
radiant level mean
here
here
so they're actually pretty even in this
so they're actually pretty even in this
training run so it might just not be
training run so it might just not be
very
consistent but definitely you only need
consistent but definitely you only need
to train for like 30 million or so steps
to train for like 30 million or so steps
it looks like before this becomes uh
it looks like before this becomes uh
reasonably
reasonably
consistent and that matches with the
consistent and that matches with the
reward curves over here even 20 million
reward curves over here even 20 million
looks reasonable enough I guess 10 just
looks reasonable enough I guess 10 just
wasn't quite there but you train a
wasn't quite there but you train a
little longer in your set so that's not
little longer in your set so that's not
bad
um I need to think about why when you
um I need to think about why when you
run more copies of the environment it
run more copies of the environment it
screws up
screws up
training well does It screw up training
training well does It screw up training
let's I haven't tested it
lately let's change this from 10
lately let's change this from 10
environments to 200 environments
right what happens
that looked
that looked
good hold
good hold
on are we uh are we back
here is actually working
now hasn't logged
maybe that
logged okay it only logged us one data
logged okay it only logged us one data
point but
logs reward at
logs reward at
0.13 this is not sufficient here uh this
0.13 this is not sufficient here uh this
is only 8 mil steps into training
though this is substantially faster than
though this is substantially faster than
before we can actually we can go over
before we can actually we can go over
500k here quite
500k here quite
easily but
easily but
um we're debugging and then there
um we're debugging and then there
there's like some weirdness with the
there's like some weirdness with the
scaling right now we're going to get
scaling right now we're going to get
this over a mil okay so this is not
this over a mil okay so this is not
actually working then because that 0.14
actually working then because that 0.14
is insufficient so then the question is
is insufficient so then the question is
going to be why
um I do kind of have this janky
um I do kind of have this janky
generalized Advantage estimation
implementation maybe we need a larger
implementation maybe we need a larger
batch size
batch size
maybe that
helps it is increasing maybe I'm going
helps it is increasing maybe I'm going
to let this run for like a little longer
to let this run for like a little longer
but then I have some
but then I have some
ideas on how I would want to address
ideas on how I would want to address
this it involves ludicrously large batch
sizes how long would it take to do a
sizes how long would it take to do a
sweep on wand
sweep on wand
B um it depends
B um it depends
this is an environment for sure that we
this is an environment for sure that we
are going to run overnight sweeps on it
are going to run overnight sweeps on it
is fast enough for that it's just that
is fast enough for that it's just that
like you kind of need to have an initial
like you kind of need to have an initial
starting point that's
reasonable
yeah I'm really only doing very coarse
yeah I'm really only doing very coarse
tuning at the moment to like get the
tuning at the moment to like get the
highle picture of this
highle picture of this
thing okay so this thing is it's not 50
thing okay so this thing is it's not 50
million steps .16 is not good enough so
one thing I'm suspecting here
one thing I'm suspecting here
is possible your reward signal no the
is possible your reward signal no the
reward signal is very nice it's the
reward signal is very nice it's the
easiest reward signal in the in the
easiest reward signal in the in the
world you get one every time you go
world you get one every time you go
closer to the enemy's ancient diagonally
closer to the enemy's ancient diagonally
you get 0 five every time you go like
you get 0 five every time you go like
lengthwise closer to the enemy's ancient
lengthwise closer to the enemy's ancient
zero if you stand in place Nega .1 if
zero if you stand in place Nega .1 if
you go away this way or this way away
you go away this way or this way away
from the enemy anent and negative one if
from the enemy anent and negative one if
you go directly away from the enemy
you go directly away from the enemy
anent
anent
so yeah it's it's a very very easy
so yeah it's it's a very very easy
reward
signal should be like
signal should be like
trivial 8,000
so this is only 16 steps this batch size
so this is only 16 steps this batch size
only gives you 16
steps no
steps no
wait yeah batch size tuning
wait yeah batch size tuning
so I have eight environments hold
on I've got eight I've got 800
on I've got eight I've got 800
environments so 8,000
environments so 8,000
agents right
agents right
no I
have I do this
have I do this
right 2,000 agents per
environment and I have eight
environment and I have eight
environments so I have 16,000 agents
environments so I have 16,000 agents
total
right and the problem is that this
right and the problem is that this
number is only eight
number is only eight
so you're really not getting very much
so you're really not getting very much
data per
data per
agent at
all
all
well we can cut this down a little bit
well we can cut this down a little bit
to start with
right let's start by just cutting this
right let's start by just cutting this
down a little
bit do we need 8 8K
bit do we need 8 8K
no snake
no snake
runs snake runs a million steps per
runs snake runs a million steps per
second with 4K we should only need
4K so let's say we Run 100 We Run 100
4K so let's say we Run 100 We Run 100
environments per
environments per
core so now we are running
800 800 environments which is 8,000
800 800 environments which is 8,000
agents
we're going to do now mini batch size
we're going to do now mini batch size
will be
4K that's slightly better
right and
then this is now 16 steps per agent
this is a huge huge number of updates
this is a huge huge number of updates
per batch at this
per batch at this
point let's see what happens when I do
point let's see what happens when I do
this and if I can get away with not well
this and if I can get away with not well
with doing things a little differently
with doing things a little differently
let's
let's
say am I on the wrong file hold
on yeah I'm on the wrong
one uh
one uh
well
e e
well that didn't run for long
well that didn't run for long
enough and also that's kind of awkward
enough and also that's kind of awkward
the way that I did
that I'm going to set this to
that I'm going to set this to
one I'm going to set this to 100 Mil
we should at least see the speed come
we should at least see the speed come
back
back
now only
400k definitely would expect more than
400k some optimization to be done
what's clip Frack clip fraction is very
what's clip Frack clip fraction is very
low
so this big number of updates really is
so this big number of updates really is
not taking us off
policy bizarre to me that this doesn't
learn
learn
so that's weird
why is the training speed that
why is the training speed that
variable do I have other stuff running
variable do I have other stuff running
on this box I
don't it really there should not be
don't it really there should not be
anything variable whatsoever about the
anything variable whatsoever about the
training
speed it's not the environment the
speed it's not the environment the
environment is not taking up any
environment is not taking up any
additional time
profile it
profile it
yeah yeah well initially I want to get
yeah yeah well initially I want to get
it learning
it learning
correctly because the fact that this is
correctly because the fact that this is
stuck like the only setting that I've
stuck like the only setting that I've
been able to get to
been able to get to
train is when you run very few copies of
train is when you run very few copies of
the environment and you use a lot of
the environment and you use a lot of
stale data which if you think about it
stale data which if you think about it
just that doesn't make any
just that doesn't make any
sense it straight up just does not make
sense it straight up just does not make
any
any
sense
um I can dramatically increase the mini
um I can dramatically increase the mini
batch size for more stable
batch size for more stable
updates in
theory fact that Ford is taking that
theory fact that Ford is taking that
much time as well
stale data like I'll show
you
you
so I had four updates before but like
so I had four updates before but like
the thing is if you have mini batch size
the thing is if you have mini batch size
4,000 right and batch size
4,000 right and batch size
256,000 then you're you're actually
256,000 then you're you're actually
doing 64 updates
doing 64 updates
times another four so you're doing 256
times another four so you're doing 256
updates per Epoch which is
insane
insane
so I'm going set this to like what
so I'm going set this to like what
32 32k and see what
32 32k and see what
happens just give you stable
happens just give you stable
updates I think I have GPU memory for
updates I think I have GPU memory for
that
this is a task that you should be able
this is a task that you should be able
to learn in like a million steps with
to learn in like a million steps with
batch size like I don't know 128 or
batch size like I don't know 128 or
something it's very easy
having absolutely no entropy bonus is
having absolutely no entropy bonus is
very
Jank but when I remove the entropy bonus
Jank but when I remove the entropy bonus
before it actually let it
learn what did I end up using for
learn what did I end up using for
snake was the entropy
so snake had an entropy a final entropy
so snake had an entropy a final entropy
tuned value of
tuned value of
01 and grid had a final entropy value of
01 and grid had a final entropy value of
very
[Music]
[Music]
low reward is sum of side one plus sum
low reward is sum of side one plus sum
of side
two some of side the reward is per agent
two some of side the reward is per agent
each agent is rewarded for going towards
each agent is rewarded for going towards
the enemy's ancient that reward is
the enemy's ancient that reward is
scaled between negative one for going
scaled between negative one for going
directly away from the enemy ancient and
directly away from the enemy ancient and
one for going directly towards the enemy
one for going directly towards the enemy
ancient so you literally have a one-step
ancient so you literally have a one-step
reward like except for like obstacles
reward like except for like obstacles
you have a one-step problem it's the
you have a one-step problem it's the
easiest thing
imaginable or it should
imaginable or it should
be but clearly something is wonky here
be but clearly something is wonky here
I'm going to put a little bit of entropy
I'm going to put a little bit of entropy
back in see what
happens nope
H that was just a flicker
okay I'm assuming your multi-agent logic
okay I'm assuming your multi-agent logic
is
is
correct it's tested on a ton of other
correct it's tested on a ton of other
environments same code if you want I can
environments same code if you want I can
train multi-agent snake for in like 30
train multi-agent snake for in like 30
seconds I tried that earlier
seconds I tried that earlier
today the snake environment that I have
today the snake environment that I have
in this uh in here you can train like
in this uh in here you can train like
slither.io basically 4096 agent snake
slither.io basically 4096 agent snake
you can get a reasonable policy in about
you can get a reasonable policy in about
a
minute trains it over a million steps
minute trains it over a million steps
per second
and that policy is basically identical
and that policy is basically identical
to this one as
to this one as
well it's just this one has a few more
well it's just this one has a few more
com
channels entropy not helping
it's so
weird blue team goes One Direction red
weird blue team goes One Direction red
team goes the other yeah now the thing
team goes the other yeah now the thing
is I think that the way like okay I
is I think that the way like okay I
here's the the really weird thing right
here's the the really weird thing right
I'm pretty sure this
works ocean uh
works ocean uh
MOA see
I've got this logic in here that um
I've got this logic in here that um
randomly respawns players every 128
randomly respawns players every 128
steps it's helps with training it's like
steps it's helps with training it's like
curriculum smoothing obviously you have
curriculum smoothing obviously you have
to take this out later but if I
just if I just rebuild with this rewards
just if I just rebuild with this rewards
are relative to their respective goals
are relative to their respective goals
right
right
yes and you can actually you can play
yes and you can actually you can play
this game in human mode right and then
this game in human mode right and then
you can print out the rewards and you
you can print out the rewards and you
can see that when you go towards their
can see that when you go towards their
base uh you get positive reward and when
base uh you get positive reward and when
you go away you get Negative reward and
you go away you get Negative reward and
also we've seen it train successfully
also we've seen it train successfully
before just with really Jank
before just with really Jank
settings now if I train this
one I bet this one works
yeah so the reward is going
up and all I did is say Okay agents
up and all I did is say Okay agents
respawn very frequently and now you can
respawn very frequently and now you can
see you get stably increasing reward
see you get stably increasing reward
with reasonable
with reasonable
hyperparameters all I
hyperparameters all I
did so it's pretty bizarre
and we'll be able to pull this policy
and we'll be able to pull this policy
down as well and see this uh and watch
down as well and see this uh and watch
this
happen you need shorter episodes but why
right why does it only work when you
right why does it only work when you
randomly respawn agents on average every
randomly respawn agents on average every
steps that's really janky
right reset sequels
training it doesn't collect full
training it doesn't collect full
episodes just collect segments
really shouldn't be it's very
weird now oddly I don't see them getting
levels hopefully this policy is actually
levels hopefully this policy is actually
working as we uh we expect it to
working as we uh we expect it to
I think it is this is a high enough
I think it is this is a high enough
reward that it should be mostly doing
reward that it should be mostly doing
something
something
reasonable isn't most RL heavily
reasonable isn't most RL heavily
episodic it depends on the environment
episodic it depends on the environment
right if your
right if your
episodes if you have like episodes where
episodes if you have like episodes where
you have diverse data within each
you have diverse data within each
episode then yes but like you have a
episode then yes but like you have a
dense reward that's essentially just
dense reward that's essentially just
saying go in a specific
saying go in a specific
direction that should be fine however
direction that should be fine however
you do it
okay so this now is
okay so this now is
trained and go grab ourselves a free
trained and go grab ourselves a free
policy
policy
here we'll just render it
episode versus batch size balance is
crucial
um it should just be bigger batch sizes
um it should just be bigger batch sizes
are better but it's in practice not
always here let's see what this looks
always here let's see what this looks
like
funny that they all go down
funny that they all go down
top mid would be
better things are very dumb e
it does something at least it does do
it does something at least it does do
something
now technically I can just this is like
now technically I can just this is like
passible enough I can run a hyper pram
passible enough I can run a hyper pram
sweep on it
but not particularly satisfying is it
can start adding more log variables I
can start adding more log variables I
can start adding better
can start adding better
rewards maybe more difference between
rewards maybe more difference between
diagonal and one step more like 1.1 and
diagonal and one step more like 1.1 and
0.25 I mean I don't really care which
0.25 I mean I don't really care which
lane they go down it's just
lane they go down it's just
like the fact that they're having this
like the fact that they're having this
much trouble with such an easy problem
much trouble with such an easy problem
is weird right
shouldn't be having this much
trouble have you tried decreasing
trouble have you tried decreasing
learning
learning
rate I did before
rate I did before
I got these defaults from snake
I got these defaults from snake
though and they didn't
work snake should be a very similar even
work snake should be a very similar even
actually snake has a harder problem if
actually snake has a harder problem if
you think about it doesn't have his nice
you think about it doesn't have his nice
and dense of a reward as
and dense of a reward as
this I mean obviously not the full game
this I mean obviously not the full game
of DOTA that's way harder but just like
of DOTA that's way harder but just like
go down mid come on
well I guess we should just add in let's
well I guess we should just add in let's
I don't want to like completely stall on
I don't want to like completely stall on
dev today doing dumb experiments so
dev today doing dumb experiments so
let's just
let's just
um let's actually add the types of
um let's actually add the types of
learning uh of rewards that we need so
learning uh of rewards that we need so
that we can like autotune everything and
that we can like autotune everything and
we'll go from there I'm guessing they're
we'll go from there I'm guessing they're
just some dumb data bugs hold
on I'm guessing that they're just like
on I'm guessing that they're just like
some dumb data bugs where we're like
some dumb data bugs where we're like
corrupting observations every so often
corrupting observations every so often
or something stupid that's like the
or something stupid that's like the
usual
usual
thing if reward is so easy we don't need
thing if reward is so easy we don't need
much exploration yeah we shouldn't need
much exploration yeah we shouldn't need
much exploration we should just be able
much exploration we should just be able
to like install learn it and be done
okay I'll I'll like I'll cut it in 10
okay I'll I'll like I'll cut it in 10
let's see if it does
anything
anything
you're you're probably right
there I would think it would be fine
there I would think it would be fine
with 32k batch
though 32k mini batch I mean
yeah but this is still the uh the
yeah but this is still the uh the
version from before right with 32 step
version from before right with 32 step
Horizons
we'll run the other one after for you
we'll run the other one after for you
want to see what rewards they did for
want to see what rewards they did for
DOTA steal their Rewards
okay
okay
win negative uh win
win negative uh win
five Hero death negative
five Hero death negative
1 corer death
1 corer death
-2 have an XP
-2 have an XP
gain gold gain
gain gold gain
we don't have
we don't have
gold Health
gold Health
change so they get a reward for
change so they get a reward for
healing Mana
Change Hero uh killed
Change Hero uh killed
hero gold and XP are very high so this
hero gold and XP are very high so this
reduces the total
reward last hit
deny AIS ancient
HP for Defending Your ancient I
HP for Defending Your ancient I
guess then they've got Tower
rewards Shrine Barrack
and then this Lane
and then this Lane
assignment the
item okay so we
item okay so we
have randomly assign each hero to a
have randomly assign each hero to a
subset of
subset of
lanes and penalize them for
lanes and penalize them for
leaving those
leaving those
Lanes this may have not been necessary
Lanes this may have not been necessary
in the
end prefer not to do a lane assignment
end prefer not to do a lane assignment
if I can avoid it let's see what did we
if I can avoid it let's see what did we
get out of
this but we're going to implement the
this but we're going to implement the
tower rewards and
stuff okay so
visualize
visualize
none here are the last two
runs the latest run did worse it
appears not really any more stable
either now we'll run this on the uh
either now we'll run this on the uh
we'll do the lower learning rate on this
we'll do the lower learning rate on this
one
right the original
task and then we'll do this while I look
task and then we'll do this while I look
more at the rewards and start
more at the rewards and start
implementing them
so I think then we do uh the XP gain
so I think then we do uh the XP gain
reward will
be will be
this 0.002
oh there's also the the gold gain reward
oh there's also the the gold gain reward
so gold XP is very high so this reduces
so gold XP is very high so this reduces
the total reward to
the total reward to
0.4 is what this
0.4 is what this
says because this is 0
point how's that
point how's that
possible doesn't a minion give like 50
possible doesn't a minion give like 50
XP or
XP or
something 0 point was it 08
[Music]
H killed hero
well let's just implement the uh the
well let's just implement the uh the
rewards and we'll scale
rewards and we'll scale
them I think ne5 to5
them I think ne5 to5
overall looks like something
overall looks like something
reasonable how's this doing
nope doesn't
work I told you it's weird right
I don't know there could just be
I don't know there could just be
something Jank about this like this
something Jank about this like this
reward structure though I
reward structure though I
don't who knows maybe we just implement
don't who knows maybe we just implement
the full thing and it suddenly dissolves
DOTA let's add some Rewards
uh and we also have to cap the rewards
uh and we also have to cap the rewards
right compute
right compute
let me do that first cap the reward
observation
so
so
plus gives
plus gives
you is it5
you is it5
time 20
oops yeah this is good initial thought
oops yeah this is good initial thought
is rewards need to be
is rewards need to be
tuned one for correct and zero for
incorrect I can do
that I don't know why that would be
that I don't know why that would be
needed
but this pretty odd
but this pretty odd
right not having a negative
right not having a negative
reward I mean unless it's
reward I mean unless it's
like unless I did something really
like unless I did something really
stupid like use a uent for rewards but I
stupid like use a uent for rewards but I
don't think
don't think
so you
so you
see no it's a
see no it's a
float should be signed
okay
rebuild so I've clipped the reward now
rebuild so I've clipped the reward now
to be uh 0er to
to be uh 0er to
one see about
one see about
that
that
meantime we're going to change this
meantime we're going to change this
logic to
logic to
be this is just the observation of the
be this is just the observation of the
reward are no big
deal we'll clip the reward
to we should do the clipping all at the
to we should do the clipping all at the
end
right they said they they get about 0.4
right they said they they get about 0.4
for a last hit on something
so 3560 for a creep
kill 006
it seems
reasonable so we add this to the reward
reasonable so we add this to the reward
here and get reward based on XP
and then
and then
um what about the tower
what's there
what's there
roughly 2.25 3
roughly 2.25 3
4.5 I think I can just use three for all
4.5 I think I can just use three for all
of
of
these a baseline since they're not going
these a baseline since they're not going
to be the same
they also have Team base rewards
they also have Team base rewards
here which is
interesting Shrine
Barracks they have uh Hero death is
Barracks they have uh Hero death is
negative one
is at the
top clear
this did this do
this did this do
anything Let's see we tried your
anything Let's see we tried your
experiment here
oh
oh
uh this might have done
uh this might have done
something be real
interesting though it's scaled
interesting though it's scaled
differently so I can't quite tell
right you're at 0.5 but nobody's gotten
right you're at 0.5 but nobody's gotten
any
any
levels is a little
sketchy for
H this is really funny do you see what
H this is really funny do you see what
they're
they're
doing they're
doing they're
vibrating so that they are getting a
vibrating so that they are getting a
reward every other time
reward every other time
step isn't that weird
that was pretty
weird they're actually they're
weird they're actually they're
like they're vibrating along the
like they're vibrating along the
diagonal as well which is weird I don't
diagonal as well which is weird I don't
think that should give them any
reward yeah this shows as uh
huh very
huh very
weird we probably have some
weird we probably have some
bugs I have some
bugs I have some
bugs demons those demons
negative one for
dying Health
dying Health
changed Mana changed
we've got
we've got
Tower we've got Tower
Tower we've got Tower
rewards we've got XP rewards and we've
rewards we've got XP rewards and we've
got don't die
got don't die
reward and we also have the
reward and we also have the
uh you know the the Jank reward that
uh you know the the Jank reward that
doesn't seem to be
doesn't seem to be
working okay let's figure out why this
working okay let's figure out why this
Jank forward is not working we got a
Jank forward is not working we got a
clue out of this I think which is that
clue out of this I think which is that
this Behavior says that it's getting .
this Behavior says that it's getting .
five reward oops not this one the one we
five reward oops not this one the one we
just watched says it's getting 0 five
just watched says it's getting 0 five
reward right but I don't understand
how
how
so distance to ancient
player Yus ancient y plus player xus
player Yus ancient y plus player xus
ancient
X move
really looks like that behavior should
really looks like that behavior should
get zero
reward I'm going to implement it locally
what did we say it was like
what did we say it was like
um if player. reward less than zero
um if player. reward less than zero
player reward is
player reward is
zero that's what we said we're going to
do and according to this they get 0 five
reward that behavior looks like it gets
reward that behavior looks like it gets
no reward to
me seg
Vault I probably just screwed this up
right yeah
seg
VA
VA
okay okay so this is saying we're
okay okay so this is saying we're
getting point4
getting point4
reward it's about what we found
reward it's about what we found
before I do not see how this is
before I do not see how this is
point for a
point for a
reward
reward
so that's interesting right
invalid PID
huh
there record has no object
there record has no object
type uh fine
type
key
key
error I guess I have to update this uh
error I guess I have to update this uh
this asset map a little bit
this asset map a little bit
huh so we can render
it e
where is this even
getting tile is equal to grid okay so it
getting tile is equal to grid okay so it
is the grid ID
that's
that's
fine we just have to update this asset
fine we just have to update this asset
map a little bit and we'll be able to
map a little bit and we'll be able to
render it and maybe we'll get some
render it and maybe we'll get some
insight just
insight just
maybe we'll get a little bit of insight
maybe we'll get a little bit of insight
so this is now two Tower
so this is now two Tower
here two is
here two is
Tower three
I need to open up this file don't
I not this one
okay perfect
so this
so this
is
is
dire this is
dire this is
12 13
14 and 15 is going to be the red
14 and 15 is going to be the red
puff uh the was it 1280 yeah so this is
puff uh the was it 1280 yeah so this is
the radiance so this is
six 7 8 n
six 7 8 n
and
10 and then this one 256 this is the
10 and then this one 256 this is the
dire creep so this is four this this is
dire creep so this is four this this is
the radiant creep which is
three and then this
three and then this
is uh this is
is uh this is
nothing cuz seven is already covered
nothing cuz seven is already covered
perfect
why is there no
tower towers is two now
right where do we draw the the uh the
walls be
walls be
here now we should be able to do it
y
y
okay
so this is one this is 05 this is
n so this is literally zero reward right
how are they getting reward for this
participation trophy yeah seriously like
participation trophy yeah seriously like
okay what does it think that the first
okay what does it think that the first
agent is getting his
agent is getting his
reward let's try
that
that
zeros so it's under no impression that
zeros so it's under no impression that
this this behavior is rewarded
this this behavior is rewarded
right what about the dire are the dire
right what about the dire are the dire
getting rewarded for this [ __ ]
the dire are not getting rewarded for
the dire are not getting rewarded for
this
[ __ ]
[ __ ]
right
nine so nobody's getting rewarded for
nine so nobody's getting rewarded for
this
this
[ __ ] yet we're still doing
[ __ ] yet we're still doing
[ __ ] that the game here
how are we getting rewarded for this
[ __ ] zero we're not getting rewarded
[ __ ] zero we're not getting rewarded
for this
for this
[ __ ] wait a second wait just one
[ __ ] wait a second wait just one
goddamn
minute
minute
10 go
now we're suddenly getting rewarded for
now we're suddenly getting rewarded for
this [ __ ]
this [ __ ]
again
ooh what the
hell so there's definitely a bug with
hell so there's definitely a bug with
the uh multiple environments
the uh multiple environments
then definitely a
bug which makes sense right
bug which makes sense right
reinforcement learning Is Us usually
reinforcement learning Is Us usually
good like RL kind of kicks ass it's
good like RL kind of kicks ass it's
usually a data
problem I thought I'd checked for this
problem I thought I'd checked for this
dumb thing as well
let's see if these agents are doing
let's see if these agents are doing
anything different first of all right
now you're getting zero again
now you're getting
reward if I is equal to
reward if I is equal to
1 grid is the original grid
otherwise it's a copy of the
grid for
I'm not going to be able to see it if I
I'm not going to be able to see it if I
do
this yeah so this render
this yeah so this render
blank and then it well this render blank
blank and then it well this render blank
it rendered blank I can tell you that
so if I is equal to zero hold
so if I is equal to zero hold
on this gets reward
and this does not get
reward does that make any
sense e
yeah know I'm not I it's this is
yeah know I'm not I it's this is
complicated I was it was more or less
complicated I was it was more or less
rhetorical if you happen to know then
rhetorical if you happen to know then
great
great
but there's something there's
but there's something there's
essentially there's something weird with
essentially there's something weird with
multiple environments going on like when
multiple environments going on like when
you have multiple simulated
you have multiple simulated
environments and it has something to do
environments and it has something to do
with the way that I'm passing the
map
for e
you set up the Grid on a
knit render just takes self duck
red for
oh
wait blocking that
freaking
Bots you don't sell me Bots I make the
Bots that's how it
works so I think I found the issue
so when you pass all this data in
right what do we do with the
grid we start adding [ __ ] to the Grid on
grid we start adding [ __ ] to the Grid on
a reset
right oh I know how we can do this is
right oh I know how we can do this is
going to be funny actually this is going
going to be funny actually this is going
to be like horribly horribly broken
well
well that's a weird number of Agents Now
well that's a weird number of Agents Now
isn't
it
e e
that's so funny that's like a top tier
that's so funny that's like a top tier
bug [ __ ] that is a top top tier
bug [ __ ] that is a top top tier
bug jeez
that is
nuts shouldn't be anywhere else that
nuts shouldn't be anywhere else that
could happen though so um easiest fix
could happen though so um easiest fix
for this I think
that a top tier
bug it's always game problems I swear
bug it's always game problems I swear
it's always problems with the game
data well this still has some demons it
data well this still has some demons it
looks like
Toopy actual freaking demons I
swear you don't need a doctor you need a
priest initial Bowie
priest initial Bowie
wait is this
wait is this
bet oh wait I just realized that's you
bet oh wait I just realized that's you
bet isn't it Bo image
bet isn't it Bo image
shows
shows
shape
shape
24 I thought it might be I wasn't
24 I thought it might be I wasn't
positive um torch
positive um torch
shape yeah hi bet
um adjusted shape
um adjusted shape
image after CNN
shape is there a bug you're trying to
shape is there a bug you're trying to
help me find oh it's working it's
help me find oh it's working it's
functioning good very good get on
oh
Jesus [ __ ] straight demons I swear
just [ __ ]
demons great [ __ ]
demons honestly like getting good at
demons honestly like getting good at
tensor manips is one of the it's like
tensor manips is one of the it's like
one of the first things you have to do
one of the first things you have to do
in ml and it's one of the things you
in ml and it's one of the things you
will use forever in
ml like that right there was probably
ml like that right there was probably
like my first two years worth of
like my first two years worth of
research on architecture Dev was just
research on architecture Dev was just
like getting really good at shape
like getting really good at shape
issues banishing
issues banishing
demons you know
boom it
boom it
trains speaking of demon boxes for are
trains speaking of demon boxes for are
you kidding
me
[ __ ] four and seven
sh e
looking more at
box stress
test his code also
test his code also
yeah [ __ ] I'll get rebooted tomorrow I'm
yeah [ __ ] I'll get rebooted tomorrow I'm
rebooted in the
morning I'm so obnoxious
man too many demons
man too many demons
we're not doing Intel no more unless
we're not doing Intel no more unless
they get some good
chips uh yeah this actually freaking
chips uh yeah this actually freaking
slaps now doesn't
it 10
it 10
m let's do 100
m 200
m 200
M boom
do 8K mini
do 8K mini
batch
batch
28k is still ridiculous that
that's I'm working on this thing man
that's I'm working on this thing man
it's
hard 128
oops this is eight right yeah this is
oops this is eight right yeah this is
eight duh I don't know why I'm dividing
eight duh I don't know why I'm dividing
powers of two on
powers of two on
calculator
um this is so sketchy the way this works
um this is so sketchy the way this works
honestly you kind of start needing
honestly you kind of start needing
gradient accumulation don't
you now
you now
[ __ ] we going do 32k
[ __ ] we going do 32k
Right One update
Epoch give us 100 million
Epoch give us 100 million
steps and let's run uh a
steps and let's run uh a
nice nicely tracked
run oh yeah look at that SPS [ __ ] yeah
run oh yeah look at that SPS [ __ ] yeah
I'll use a restro be right back and uh
I'll use a restro be right back and uh
we'll see what
we'll see what
happens got no another hour and a half
happens got no another hour and a half
of
Dev
e
e e
I have to go run soon run is
I have to go run soon run is
good run is kicking my ass at the moment
good run is kicking my ass at the moment
it's 50 miles per week of run
I'm going to attempt to do 22 Mile run
I'm going to attempt to do 22 Mile run
this weekend and possibly
this weekend and possibly
survive we will
see so this doesn't work all that well
see so this doesn't work all that well
that's
funny knees
can't KNE shouldn't be the thing that
can't KNE shouldn't be the thing that
hurt
only thing if it hurts May knees maybe
only thing if it hurts May knees maybe
is if I try running down Hills I don't
is if I try running down Hills I don't
run down steep
hills it's usually Cales or whatever
hills it's usually Cales or whatever
that or tendons or stuff that gets hurt
everything
hurts eventually everything just gets
hurts eventually everything just gets
stronger takes time
you can very safely train from a
you can very safely train from a
marathon from being able to run like a
marathon from being able to run like a
few 5Ks a week in about 6
few 5Ks a week in about 6
months it sucks but you can do it you
months it sucks but you can do it you
can very consistently do that
safely the training does absolutely suck
though e
Marathon isn't that bad for
Marathon isn't that bad for
body 100 miles is bad for body marathon
body 100 miles is bad for body marathon
is fine for
body half marathon definitely isn't bad
body half marathon definitely isn't bad
for body
also good for mind
yeah people in shape can just do like
yeah people in shape can just do like
you can do several halfs a week and be
you can do several halfs a week and be
like chilling
body's capable of a
lot interesting that we're still having
lot interesting that we're still having
these weird stability
issues do I bump the entropy coefficient
issues do I bump the entropy coefficient
back up
back up
now it's pretty well crashed right
entropy coefficient
big learning rate
big reinforcement learning doesn't want
big reinforcement learning doesn't want
you to have fast MOBA
big RL wants you to think RL is hard and
big RL wants you to think RL is hard and
takes lots of
GPU but that's just listening to the man
we do have
we do have
32k 32k
32k 32k
MB 32,000
okay two
M's we got Lambda Gamma
M's we got Lambda Gamma
clip learning rate is O2
should be a very easy
task
e
e e
very funny how this
very funny how this
works we fixed the bug didn't we and
works we fixed the bug didn't we and
like forget to pull it right
like forget to pull it right
pull
no yeah
h
yeah it's
FAL fix the bug with the other thing
FAL fix the bug with the other thing
right
we saw this get solved like instantly
right could just be batch
uh the first run we did with this it
uh the first run we did with this it
actually it solved the um it got like
actually it solved the um it got like
reward 0.9 or whatever it so it works
reward 0.9 or whatever it so it works
it's just it doesn't work with good
settings let me try with just way larger
batch e
that's way way
that's way way
nicer I am back how's it
nicer I am back how's it
going oh man we found some demons yeah
going oh man we found some demons yeah
bet this fixed it it was the it was the
bet this fixed it it was the it was the
effective J Horizon is what it was
effective J Horizon is what it was
remember that thing I told you about
remember that thing I told you about
that yeah that
that yeah that
matters um we we found so well F we
matters um we we found so well F we
found some demons we found some real
found some demons we found some real
demons
demons
so this is one of the craziest bugs in
so this is one of the craziest bugs in
recent memory um I was passing a view of
recent memory um I was passing a view of
the grid into each
the grid into each
environment and that means that like as
environment and that means that like as
you added more environments it was
you added more environments it was
adding ghost copies of Agents into the
adding ghost copies of Agents into the
environment that could get rewards so
environment that could get rewards so
like the first environment which was the
like the first environment which was the
one that I was watching was normal so
one that I was watching was normal so
everything looked fine but then like the
everything looked fine but then like the
later environment got progressively more
later environment got progressively more
ghosts into
them
yeah
demons now this is actually very stable
demons now this is actually very stable
right here
here and it makes total sense
here and it makes total sense
why like big
batch this is an actually beautiful RL
batch this is an actually beautiful RL
curve look at
curve look at
this look how nice this is ready
boom
perfect e
is
is
solid yeah it
solid yeah it
works we're going to let this finish
works we're going to let this finish
we're going to watch the policy we're
we're going to watch the policy we're
going to admire the policy for a little
going to admire the policy for a little
bit hopefully it's
bit hopefully it's
good uh it looks like it's good right
good uh it looks like it's good right
because look at the level means
because look at the level means
right they're doing good
right they're doing good
see they got reasonable levels they're
good they're kung fu fighting in the
good they're kung fu fighting in the
middle we got the training going at
middle we got the training going at
500,000 plus steps per
500,000 plus steps per
second bada
boom whoops thank you for minimizing all
boom whoops thank you for minimizing all
my windows
my windows
it's exactly what I
wanted that is
clean RL is easy okay RL is easy getting
clean RL is easy okay RL is easy getting
making Sims that actually work correctly
making Sims that actually work correctly
and are fast is
and are fast is
hard RL is
hard RL is
easy you run po you run with some big
easy you run po you run with some big
batch you run a little hyperparameter
batch you run a little hyperparameter
sweep and you're good to go
let's go look at it
let's go look at it
the perfect training
the perfect training
curve which is exactly what should
curve which is exactly what should
happen because this is an a dead simple
happen because this is an a dead simple
task we train them to go Kung Fu
task we train them to go Kung Fu
Fighting is good
have you got them purchasing upgrades
have you got them purchasing upgrades
man there no freaking upgrades there's
man there no freaking upgrades there's
no shop system in this thing at the
no shop system in this thing at the
moment let's get them actually playing
moment let's get them actually playing
the game first all
the game first all
right
right
Jesus what's a man got to do solo Dev
Jesus what's a man got to do solo Dev
all of DOTA in a we
all of DOTA in a we
weak how about this this looks clean
weak how about this this looks clean
enough
right
fo everyone Rush
fo everyone Rush
mid
ha looks
good everyone feed mid
that's pretty
cool see you
cool see you
bet I'm going to uh keep on this for
bet I'm going to uh keep on this for
another hour or so and then go get
another hour or so and then go get
dinner this nice though
how we feeling about the tower colors I
how we feeling about the tower colors I
think they're nice
right would be cool to have a 3D visz of
right would be cool to have a 3D visz of
it I mean we've got you've seen the
it I mean we've got you've seen the
human playable one
right we can play we can play dodo with
right we can play we can play dodo with
uh with the Bots
uh with the Bots
now kind of
whoop key error
whoop key error
five what's supposed to be on
five what's supposed to be on
five Tower or
five Tower or
something well we'll have to figure that
out yeah we have it it is actually human
out yeah we have it it is actually human
playable and you you're going to be able
playable and you you're going to be able
to play it alongside the
to play it alongside the
Bots what was the fix oh man
Bots what was the fix oh man
tree like the stream title could not be
tree like the stream title could not be
more accurate we actually we found some
more accurate we actually we found some
demons so I was passing a view of the
demons so I was passing a view of the
grid the environment grid to the Sea uh
grid the environment grid to the Sea uh
to the SE process for each environment
to the SE process for each environment
and like each successive environment was
and like each successive environment was
adding ghost agents into it that were're
adding ghost agents into it that were're
getting reward or something weird so
getting reward or something weird so
like when I visualize the first
like when I visualize the first
environment for rendering it looks fine
environment for rendering it looks fine
but then like with the every environment
but then like with the every environment
after that has more and more ghosts that
after that has more and more ghosts that
are just generating garbage reward data
are just generating garbage reward data
and like eventually we got this weird
and like eventually we got this weird
ass training run where like they were
ass training run where like they were
going back and forth in a direction that
going back and forth in a direction that
I noticed shouldn't be generating any
I noticed shouldn't be generating any
reward because they were going
reward because they were going
perpendicular to the ancient like they
perpendicular to the ancient like they
were going on the DI the off diagonal
were going on the DI the off diagonal
and it was showing High reward so I was
and it was showing High reward so I was
like okay something's totally wrong here
like okay something's totally wrong here
so I ran it with one environment and it
so I ran it with one environment and it
gets no reward like huh okay that's
gets no reward like huh okay that's
weird the training run says it gets
weird the training run says it gets
reward but it doesn't so then I ran it
reward but it doesn't so then I ran it
with more environment and it's getting
with more environment and it's getting
reward I say oh okay so then there's
reward I say oh okay so then there's
something weird with like the multi-
something weird with like the multi-
environment code and then like I made a
environment code and then like I made a
couple changes by mistake and like the
couple changes by mistake and like the
reward went back so then I like I kind
reward went back so then I like I kind
of managed to figure it out from there
of managed to figure it out from there
it was
it was
ridiculous hell of a day but yo tree
ridiculous hell of a day but yo tree
we've got um we've got like proper
we've got um we've got like proper
observation now as well they can see uh
observation now as well they can see uh
they can see the type of all nearby
they can see the type of all nearby
agents they can distinguish enemy hero
agents they can distinguish enemy hero
types they can uh see nearby Health mana
types they can uh see nearby Health mana
and level of agents and they can see
and level of agents and they can see
pretty much all of their own stats like
pretty much all of their own stats like
full set of their own stats
full set of their own stats
so we got some cool stuff
so we got some cool stuff
now we got some real cool stuff
now we got some real cool stuff
now yeah for
yo I got a cool
idea do we want to
do I could
do I could
change I don't know if I want to do Lane
change I don't know if I want to do Lane
assignment rewards as the
thing I don't know if I want to do Lane
thing I don't know if I want to do Lane
assignment
rewards onto some features
rewards onto some features
Maybe we're doing some
thinking let's see if we don't have to
thinking let's see if we don't have to
do resets
do resets
anymore I bet you we don't have to do
anymore I bet you we don't have to do
resets anymore
so this garbage should be able to go
away let's try this
debugging rewards from
debugging rewards from
ghosts yeah dude
it's like it's not PhD doctor philosophy
it's like it's not PhD doctor philosophy
it's like Paranormal heuristic
detective I got to find myself like a
detective I got to find myself like a
fun title that's like PhD and ials or
fun title that's like PhD and ials or
something cuz this is like freaking
something cuz this is like freaking
straight up RL don't need a doctor it
straight up RL don't need a doctor it
needs a
needs a
priest yeah instead of a moth in the
priest yeah instead of a moth in the
core memory it's ghost it's freaking
core memory it's ghost it's freaking
ghosts and demons and
ghosts and demons and
[ __ ]
[ __ ]
H is rough
but I got to say we got these real nice
but I got to say we got these real nice
stable train curves now and um oh yeah
stable train curves now and um oh yeah
it took a while to get it to actually
it took a while to get it to actually
train even after that until I like up
train even after that until I like up
the batch size massively I think we've
the batch size massively I think we've
got do we have like 256 or 500k batch
got do we have like 256 or 500k batch
size now yeah we got big
size now yeah we got big
batch we got big
batch and I know exactly why as well and
batch and I know exactly why as well and
it's not the reason that people
think you have to go compute the
think you have to go compute the
effective you have to go compute the
effective you have to go compute the
average number of samples that are
average number of samples that are
obtained per
obtained per
environment 30k before yeah you have to
environment 30k before yeah you have to
like go compute the expected number of
like go compute the expected number of
samples obtained per environment per
samples obtained per environment per
batch and make sure that that number is
batch and make sure that that number is
long enough to compute generalized
long enough to compute generalized
Advantage estimation over it's wonky so
Advantage estimation over it's wonky so
essentially like just by virtue of the
essentially like just by virtue of the
hardware scaling I have to run I'm
hardware scaling I have to run I'm
running 1,600 copies of this game right
running 1,600 copies of this game right
now on uh like eight CPU cores total and
now on uh like eight CPU cores total and
in order to actually make
in order to actually make
that work because I have so many copies
that work because I have so many copies
of the environment and in order to get
of the environment and in order to get
long enough sections of data from each
long enough sections of data from each
environment in every batch you need like
environment in every batch you need like
500k batch it's nutty
we getting some dire level mean increase
we getting some dire level mean increase
radiant
levels next thing is going to be to
levels next thing is going to be to
start logging uh rewards
start logging uh rewards
separately we're going to start we're
separately we're going to start we're
going to separate all the reward
going to separate all the reward
components out it's going to be cool and
components out it's going to be cool and
then we're going to have um yeah we're
then we're going to have um yeah we're
going to have separately logged reward
going to have separately logged reward
components would be nice
would you still need huge batches with
would you still need huge batches with
hypers sweeps yeah we
hypers sweeps yeah we
would we'll need the huge batches with
would we'll need the huge batches with
Hyper pram sweeps too we're going to
Hyper pram sweeps too we're going to
need the huge batches with
everything we'll just have 1 million
everything we'll just have 1 million
batch
batch
size yes
okay reward going down a
little levels are going up
though more dat more no not more data
though more dat more no not more data
more
DOTA now I want to play some DOTA and
DOTA now I want to play some DOTA and
and I know if I boot that game up I'm
and I know if I boot that game up I'm
not finishing this project so I can't
not finishing this project so I can't
boot that game
up straight up banned myself from
up straight up banned myself from
playing anything like I allow myself to
playing anything like I allow myself to
play slay the spire on my phone once in
play slay the spire on my phone once in
a while that's literally
a while that's literally
it swear games are
heroin it's easier to quit drinking than
heroin it's easier to quit drinking than
it was to quit playing RuneScape so
much oh there it goes it figured
much oh there it goes it figured
something
out I also had to after over 404,000
out I also had to after over 404,000
little open TTD yeah yeah yeah
0.04 where's our reward freaking
0.04 where's our reward freaking
curve oh there it goes boom RL
curve oh there it goes boom RL
learned RL found a
way RL
way RL
stons invest now
take full advantage of the RL
stons that is some reward
I swear osrs is like uniquely heroin as
I swear osrs is like uniquely heroin as
a
game you know it was the inspiration for
game you know it was the inspiration for
like my PhD work on neural MMO that was
like my PhD work on neural MMO that was
the original inspiration I got it from
the original inspiration I got it from
[Music]
osrs yeah I'll go spend seven years
osrs yeah I'll go spend seven years
implementing this thing that sounds like
implementing this thing that sounds like
a reasonable thing to do
all right how are the uh the models
doing and at 0.1% the cost of alphastar
doing and at 0.1% the cost of alphastar
this is less than 0.1% the cost of alpha
this is less than 0.1% the cost of alpha
star this is probably less than
star this is probably less than
0.01% the cost of alpha star mate
0.01% the cost of alpha star mate
nothing osr yeah
oops not this one we want the uh the
oops not this one we want the uh the
highle
render rib
[Music]
[Music]
render this guy's got to get lazy there
render this guy's got to get lazy there
we
go fight
do like to do their Rush
mid oh wait this isn't the full thing
mid oh wait this isn't the full thing
because this one isn't updated hold
on e
to the
to the
Moon buffer to the Moon
Moon buffer to the Moon
you too can help puffer go to the
you too can help puffer go to the
Moon all you have to do is Star
Moon all you have to do is Star
github.com Puffer a/p puffer Li to send
github.com Puffer a/p puffer Li to send
the puffer to the Moon helps me out a
ton H they learned that they can cut
ton H they learned that they can cut
through they learned that they can
through they learned that they can
glitch through right
here let some survival a little
here let some survival a little
longer that's
funny oh I'm dancing in your
fountain star and followed your stuff
fountain star and followed your stuff
why thank
why thank
you that helps me out a whole
ton so I imagine this is right here is
ton so I imagine this is right here is
probably what is
probably what is
um what's screwing up the learning a
bit they're literally in the
fountain we'll put the um
we'll put the the reset thing
back we'll put it back for now at like a
back we'll put it back for now at like a
much lower rate
we'll do like
1024 that ought to help
I remember like one of the first games
I remember like one of the first games
of DOTA I played I managed to feed
of DOTA I played I managed to feed
somebody like six levels and I was like
somebody like six levels and I was like
what you this is a game where if you
what you this is a game where if you
don't know what you're doing you can
don't know what you're doing you can
literally turn the opponent into a
God leave ultimate rage bait
game thank you for the puffer star we've
game thank you for the puffer star we've
got it's been going real
well we're up now to 684 which is
awesome puffer stons are
good
good
okay DOTA is super unforgiving even more
okay DOTA is super unforgiving even more
than League yeah
cool game though okay uh we're going to
cool game though okay uh we're going to
show you off some more
show you off some more
Tech want to do some more struct
Tech want to do some more struct
Shenanigans we're going to do some more
Shenanigans we're going to do some more
stru
Shenanigans upper
all right C type def struct entity
yeah oh we don't use this
yeah oh we don't use this
[ __ ] so this can be a c def not a c type
[ __ ] so this can be a c def not a c type
Def
what else do we give it float uh we gave
what else do we give it float uh we gave
it XP we gave it a death
it XP we gave it a death
reward I played
reward I played
Luna I really liked uh Lena when I
Luna I really liked uh Lena when I
played DotA
you I would buy it's such a dumb build
you I would buy it's such a dumb build
but I would buy that stupid staff that
but I would buy that stupid staff that
basically gives you a second
basically gives you a second
ultimate and just like double burst
ultimate and just like double burst
people but the thing is I was bad enough
people but the thing is I was bad enough
that I would just like screw up sausage
that I would just like screw up sausage
fingered like miss all my buttons and
fingered like miss all my buttons and
just
die and people would be like dude press
die and people would be like dude press
your buttons like I can't I can't press
your buttons like I can't I can't press
the buttons fast enough
I just I can't press
I just I can't press
them like why didn't you ult cuz cuz
them like why didn't you ult cuz cuz
he's stunned me before I could all cuz
he's stunned me before I could all cuz
cuz he was he's like I don't know 14 and
cuz he was he's like I don't know 14 and
on like his fifth Red
Bull good times
dude he's just I can't do anything he's
dude he's just I can't do anything he's
14 he's on his fifth Red Bull what do
14 he's on his fifth Red Bull what do
you want from
me he's got those twitch reactions from
me he's got those twitch reactions from
like swiping Tik Tok so
much e
reactions are gone at
reactions are gone at
ah you know that's the thing I really
ah you know that's the thing I really
liked with old school was like you
liked with old school was like you
didn't have to have super fast reactions
didn't have to have super fast reactions
you just had to have fast like Fast
you just had to have fast like Fast
movements with good muscle memory you
movements with good muscle memory you
know like if you can one tick a
know like if you can one tick a
four-way which is always in the same
four-way which is always in the same
position every time you're
position every time you're
good that took me a long time to be able
good that took me a long time to be able
to do as
well but the
well but the
reactions I don't know I I peaked like
reactions I don't know I I peaked like
silver in every other game I've touched
silver in every other game I've touched
like and actually even like in triat
like and actually even like in triat
like peaked silver and like valerent and
like peaked silver and like valerent and
like everything
like everything
else I didn't play enough league but I
else I didn't play enough league but I
have bronze there was ridiculous
I don't know there's no decision making
I don't know there's no decision making
involved when you can't hit the
involved when you can't hit the
broadside of the bar and if you just
broadside of the bar and if you just
miss you just miss it's like well you
miss you just miss it's like well you
should have like you should have like
should have like you should have like
made got a better angle it's like I
made got a better angle it's like I
would have still
missed I'm just that bad I don't know
missed I'm just that bad I don't know
what to tell
[Music]
you oh sorry s
yeah it's a lot more um well that's a
yeah it's a lot more um well that's a
freaking awesome
Game Stop talk I gotta stop talking
Game Stop talk I gotta stop talking
about before I want to play it and then
about before I want to play it and then
I stop deving stuff I have stuff to
I stop deving stuff I have stuff to
build got to feed the
puffer uh so what we're going to do now
puffer uh so what we're going to do now
is
we're going to pull what's called a Pro
we're going to pull what's called a Pro
Gamer move and make the reward destruct
Gamer move and make the reward destruct
so that we can get access to the
so that we can get access to the
individual reward components in the
individual reward components in the
python
python
code what are you adding here I'm making
code what are you adding here I'm making
I'm breaking up the reward into
I'm breaking up the reward into
components so that we can get the
components so that we can get the
components of the reward in here so we
components of the reward in here so we
can log them to w b to see actually what
can log them to w b to see actually what
is going on and if they learn to fight
is going on and if they learn to fight
and stuff okay and then we're gon to add
and stuff okay and then we're gon to add
a bunch more logging and stuff and then
a bunch more logging and stuff and then
we're going to balance stuff and we're
we're going to balance stuff and we're
going to like change like tweak things
going to like change like tweak things
from there but it's like metrics
from there but it's like metrics
visibility right balance lots of lots of
visibility right balance lots of lots of
things from
there we also got to put skills back in
there we also got to put skills back in
because I disabled the skills while I
because I disabled the skills while I
was trying to
debug T type
team spirit reward for assist and
team spirit reward for assist and
grouping I think yeah yeah yeah that was
grouping I think yeah yeah yeah that was
pretty
major that's for down the line
major that's for down the line
though once we manage to get him to do
though once we manage to get him to do
something reasonable to
something reasonable to
start e
okay bro is it working now is there an
okay bro is it working now is there an
agent that gets good
agent that gets good
reward oh God I got to tell the demon
reward oh God I got to tell the demon
story
story
again yeah so will um the thing that
again yeah so will um the thing that
wasn't working so when I rendered the
wasn't working so when I rendered the
first environment in the stack it looked
first environment in the stack it looked
fine everything was good but the thing
fine everything was good but the thing
was you see when I was constructing all
was you see when I was constructing all
the environments I was passing a
the environments I was passing a
reference by mistake to the main grid of
reference by mistake to the main grid of
the EnV so what was happening is that
the EnV so what was happening is that
each subsequent environment was adding
each subsequent environment was adding
more and more ghost agents into the
more and more ghost agents into the
simulation and these ghost agents Were
simulation and these ghost agents Were
Somehow triggering rewards so when I
Somehow triggering rewards so when I
would look at it I had a re perfectly
would look at it I had a re perfectly
reasonable environment but you actually
reasonable environment but you actually
had this like ghost infested Insanity
had this like ghost infested Insanity
environment that was corrupting all of
environment that was corrupting all of
the training data and as soon as I fixed
the training data and as soon as I fixed
that and increased the batch size you
that and increased the batch size you
get perfect learning curves and the
get perfect learning curves and the
thing actually freaking
works and well I have to rebuild this
works and well I have to rebuild this
thing
but demons
ha the agents shall fly to each
ha the agents shall fly to each
other one or two of them get stuck
other one or two of them get stuck
occasionally
occasionally
but they run it down
but they run it down
mid there you
mid there you
go they run down
mid and eventually it seems that they do
mid and eventually it seems that they do
take over your
fountain yeah this is learned I just
fountain yeah this is learned I just
told them to go to the enemy ancient and
told them to go to the enemy ancient and
they learned to do it I mean it's very
they learned to do it I mean it's very
simple it's like a really really easy
simple it's like a really really easy
thing to learn but still it
works oh Towers can still be pulled I
works oh Towers can still be pulled I
didn't change that there's a there's an
didn't change that there's a there's an
ability in the game that lets you move
ability in the game that lets you move
around Towers
see I didn't make I didn't add a check
see I didn't make I didn't add a check
on the pull like the hook ability like
on the pull like the hook ability like
Pudge hook I didn't add a check to see
Pudge hook I didn't add a check to see
if you're uh trying to Target a tower
if you're uh trying to Target a tower
with that so you can just straight up
with that so you can just straight up
hook a
tower I saw it happen one time they
tower I saw it happen one time they
pulled their uh they pulled their tier
pulled their uh they pulled their tier
two all the way back to their tier three
get this in Dota ASAP yeah right yeah
get this in Dota ASAP yeah right yeah
call me up valve for all of your
call me up valve for all of your
reinforcement learning and exorcism
reinforcement learning and exorcism
[Laughter]
[Laughter]
needs how long till training on a better
needs how long till training on a better
objective it is already technically
objective it is already technically
training on a better objective there
training on a better objective there
additional reward components for Tower
additional reward components for Tower
kills XP gained dying and one or two
kills XP gained dying and one or two
other things um and I'm currently
other things um and I'm currently
exposing these so that we can graft them
exposing these so that we can graft them
independently that's what I was doing
independently that's what I was doing
before I was so rudely distracted by
demons e
so what we're doing is we're putting the
so what we're doing is we're putting the
rewards into a struct
rewards into a struct
here put the rewards into a
struct and then
it player
OBS
Rewards
e
e
e
e
e
e e
whoops do
reward forgot to add that
boom death XP distance
to that's pretty good
we'll fix this up
we'll fix this up
now cannot
assign type float
assign type float
to
reward
reward
ah we don't need to do this
did I not Define reward up top
did I not Define reward up top
somewhere or did I double Define it
no well this is [ __ ] right here so
no well this is [ __ ] right here so
this is
this is
reward reward.
reward reward.
death that's
better get reward of PID
better get reward of PID
storing see
storing see
derivative I've PID defined don't
derivative I've PID defined don't
I I totally have PID
defined well let's fix these first these
defined well let's fix these first these
are
easy
for
e e
okay now we have 26 values here for
reward e
get reward is not
defined Undeclared name not built-in get
defined Undeclared name not built-in get
reward cdef reward star get
reward cdef reward star get
reward what
it's right
there
there
[Music]
so C def
so C def
struct
struct
reward the death struct
player well
cannot
convert so basically all of these are
convert so basically all of these are
like we don't know what a reward is
see def reward star get
see def reward star get
reward turn and self. rewards of
PID self. rewards equal
PID self. rewards equal
rewards
entities
yeah I don't understand what the hell is
wrong Undeclared name not built-in get
reward I literally have it declared
reward I literally have it declared
right
here oh well this one's easy this is
here oh well this one's easy this is
self Dot
there we
go let's hurry up and make this work so
go let's hurry up and make this work so
that we can run an overnight
that we can run an overnight
sweep that' be
nice we just train 10 trillion steps of
nice we just train 10 trillion steps of
DOTA that' be nice and then we uh we
DOTA that' be nice and then we uh we
solve
solve
it out of
it out of
bounds on Buffer axis zero
get reward of
PID
H
H
print well
zero
zero
out of bound on axis
out of bound on axis
zero well that is
zero well that is
ridiculous that does not make any sense
ridiculous that does not make any sense
they're all zero through
nine so assumedly something is getting
nine so assumedly something is getting
passed
incorrectly num MS
this is num agents not num
M's invalid input
we have 26 here I
we have 26 here I
believe
26 26 is what we
have seg vault
really seg vault
second FS before
second FS before
step
okay like faults before that
np. zeros oh wait hold
np. zeros oh wait hold
on yeah yeah I just did this
wrong
e
e e
pigmentation vault
okay we don't say fault
okay we don't say fault
there though it's possible if I'm
there though it's possible if I'm
putting the uh the wrong data shape in
putting the uh the wrong data shape in
that would still be a seg bolt
and not assign float
pointer for
but before this even happens
I think that's the seg fault right
there seg faults are
there seg faults are
dumb you shouldn't
exist I wish we had czig that would be
exist I wish we had czig that would be
cool or Pi
cool or Pi
Zig be fun
I might take some time to play around
I might take some time to play around
with Zig that might be fun but the thing
with Zig that might be fun but the thing
is without this uh numpy view slicing
is without this uh numpy view slicing
thing nothing's going to be the same you
thing nothing's going to be the same you
really need the numpy view
thing all right invalid
tensors why is I got invalid tensors huh
briefly looked at
Zig is it like see but with things that
Zig is it like see but with things that
make you not
make you not
cry like uh I saw that they had a really
cry like uh I saw that they had a really
sweet like debug memory allocator that
sweet like debug memory allocator that
does all the bounce checking and stuff
does all the bounce checking and stuff
and then you just swap it when you're
and then you just swap it when you're
like when you're set you just swap it
like when you're set you just swap it
out for the normal one that's fast
out for the normal one that's fast
that's the type of stuff I want
it's pretty much what it
it's pretty much what it
is yeah that's kind of what I want um of
is yeah that's kind of what I want um of
like trendy programming languages that
like trendy programming languages that
have been going around I have absolutely
have been going around I have absolutely
no interest in Rust it's [ __ ] stupid
no interest in Rust it's [ __ ] stupid
for this um go is
for this um go is
like maybe but I don't I haven't seen
like maybe but I don't I haven't seen
perect on it I don't think it's as fast
perect on it I don't think it's as fast
as like be C from what I've seen and Zig
as like be C from what I've seen and Zig
looks kind of cool zig's the only one
looks kind of cool zig's the only one
that looks kind of
cool but then it's like the thing is you
cool but then it's like the thing is you
need
need
the like the python integration here is
the like the python integration here is
so crucial
Odin is too much of
Odin is too much of
itself
itself
yeah haven't seen too much of it
but I don't want more people Reinventing
but I don't want more people Reinventing
the wheel I just want like it's very
the wheel I just want like it's very
simple I want C with stuff I can compile
simple I want C with stuff I can compile
with that makes it not throw garbage
with that makes it not throw garbage
errors that's all I want they give me
errors that's all I want they give me
fast compile mode safe compile mode
fast compile mode safe compile mode
that's all I
that's all I
want ideally I wouldn't even have to
want ideally I wouldn't even have to
like put in the code you know memory
like put in the code you know memory
like area allocator or something I
like area allocator or something I
should just be able to compile it with a
should just be able to compile it with a
different allocator back
different allocator back
end that would be very
nice that's where they want to take
nice that's where they want to take
Zig if they do that well I still can't
Zig if they do that well I still can't
say I'd use it because
say I'd use it because
like the memory view thing like the
like the memory view thing like the
killer app for scyon is not needing
killer app for scyon is not needing
pbind and having um memory
pbind and having um memory
views those two things are just god tier
so like having easy ways to share memory
so like having easy ways to share memory
even structured memory like entity
even structured memory like entity
component systems between Python and c
component systems between Python and c
is awesome and also in this like I don't
is awesome and also in this like I don't
even have to write pure C right or stuff
even have to write pure C right or stuff
that compiles to Pure C I can write slow
that compiles to Pure C I can write slow
and it functions in
and it functions in
Python I'm allowed to do that
let me fix the uh the AC real quick and
let me fix the uh the AC real quick and
then we're going to finish this
of course you know they have to call
of course you know they have to call
they need a name for people that uh that
they need a name for people that uh that
write Zig I suggested Zig zons
510 that
510 that
flat invalid input tensor side did I
flat invalid input tensor side did I
just forget to change
just forget to change
something Beast to reimplement in Zig as
something Beast to reimplement in Zig as
is yeah yeah like it's
is yeah yeah like it's
not yeah it's really not worth it at the
not yeah it's really not worth it at the
moment like the scyon is Best in Class
moment like the scyon is Best in Class
by a
by a
mile for Sim Dev like this it's Best in
mile for Sim Dev like this it's Best in
Class by a
mile like you know that there are no
mile like you know that there are no
malx in this ithon right now there's
malx in this ithon right now there's
literally no possibility for memory
literally no possibility for memory
leaks the errors I get are out of bounds
leaks the errors I get are out of bounds
things because like it's missing a few
things because like it's missing a few
compile Flags um but all the memory
compile Flags um but all the memory
allocation is done from numpy arrays in
allocation is done from numpy arrays in
Python it's easier than CNC yeah yeah
Python it's easier than CNC yeah yeah
yeah it's like python almost python ease
yeah it's like python almost python ease
and raw C performance it com it
and raw C performance it com it
transpiles to C and compiles from
there it's free really
there it's free really
good invalid input tensor shape why is
good invalid input tensor shape why is
there invalid input tensor
shape this is
stupid player
stupid player
Oben did I Cal wrong
this looks fine what the [ __ ]
oh it's because I'm trying to load a
oh it's because I'm trying to load a
model that's
dumb yeah
bunk tried to
bunk tried to
add all right I figured that it would
add all right I figured that it would
get stuck there though that's fine
why is this 200 by
e e
nice
e
e
e
e e
could not broadcast
yeah that's going to be a problem isn't
yeah that's going to be a problem isn't
it
um slightly obnoxious
really just to some shape
really just to some shape
errors different ways of they
errors different ways of they
representing
representing
this num
this num
agents do like this
maybe e
out of bounds AIS zero
on attack yes this is going to be an
on attack yes this is going to be an
error it's easily
resolved
um e
what's wrong with
what's wrong with
this declare it after it's
used
well that's a little
awkward I guess technically what you can
awkward I guess technically what you can
do is you can put this up
do is you can put this up
here
here
right and uh now that's a null
right and uh now that's a null
pointer what you do is you
pointer what you do is you
do reward is going to
be target.
PID not initializing yeah so the thing
PID not initializing yeah so the thing
is it's one kind of awkward thing with
is it's one kind of awkward thing with
scon is you can't declare variables
scon is you can't declare variables
inside of conditionals or Loops so I
inside of conditionals or Loops so I
have to declare it up here and then
have to declare it up here and then
initialize it down here
strs work the same as they do in uh in
C so this is a pointer I've declared a
C so this is a pointer I've declared a
pointer and now here I get the pointer
pointer and now here I get the pointer
to the
to the
reward and now I set
reward and now I set
it and then down here right this won't
it and then down here right this won't
have been initialized so I have to do
have been initialized so I have to do
reward is going to be get reward of this
reward is going to be get reward of this
time the player PID and then I can set
time the player PID and then I can set
that stuff there so I reuse the same
that stuff there so I reuse the same
pointer
I love strs strs are great I wish python
I love strs strs are great I wish python
had
had
strs it's one of the worst things about
strs it's one of the worst things about
the language is not having good
strs this looks good though we've gotten
strs this looks good though we've gotten
uh a bunch of additional stuff
a bunch of additional stuff
here strs are great type defs are
here strs are great type defs are
great type def
great type def
strs I do not know the difference
strs I do not know the difference
between a struct and a type def struct
between a struct and a type def struct
actually I actually do not know the
actually I actually do not know the
difference
uh this is nice
so now we get to rerun that experiment
so now we get to rerun that experiment
from
before e
okay
look at
that we get
that we get
graphs isn't that
graphs isn't that
nice all these things are getting
nice all these things are getting
graphed we've got a four component
graphed we've got a four component
reward it's going to be
reward it's going to be
graphed makes the struct an anonymous
graphed makes the struct an anonymous
type so you don't have to just click the
type so you don't have to just click the
struct type when using the
struct type when using the
struct
huh and that
huh and that
works I guess that
works yeah I could see how that would be
works yeah I could see how that would be
useful
oh I guess I didn't notice that because
oh I guess I didn't notice that because
that's how it works by default in uh in
that's how it works by default in uh in
scon yeah in scon you don't have to type
scon yeah in scon you don't have to type
the Redundant uh stru keyword anyways
yeah I'd forgotten about
yeah I'd forgotten about
that from see okay I didn't know there
that from see okay I didn't know there
was a way around that that's
cool redundant keywords are just dumb I
cool redundant keywords are just dumb I
swear like languages with tons of
swear like languages with tons of
redundant keywords are just bad almost
redundant keywords are just bad almost
by default because they just clutter
by default because they just clutter
stuff
the other thing that drives me
the other thing that drives me
absolutely
absolutely
nuts um
nuts um
is typed python code drives me
is typed python code drives me
absolutely insane because it's like
absolutely insane because it's like
you're using the slowest language in
you're using the slowest language in
existence where the main strength is
existence where the main strength is
being really succinct and versatile and
being really succinct and versatile and
you're adding all the bloat of a
you're adding all the bloat of a
strongly typed language with Zero
strongly typed language with Zero
Performance benefits whatsoever
that's my personal just
that's my personal just
like ride me
nuts I like absolutely hate having to
nuts I like absolutely hate having to
read typed python
code win 32 API confusion
code win 32 API confusion
jeez no win 32 API please
look at
look at
that we sort
these look at
that we do some uh we take some
that we do some uh we take some
Towers we go get some
Towers we go get some
ancient we get some Rewards
we do a little less uh we do some dying
we do a little less uh we do some dying
I guess we get some XP
yeah typing in Python doesn't even no it
yeah typing in Python doesn't even no it
doesn't and in fact it doesn't even
doesn't and in fact it doesn't even
here's the thing not only does it not
here's the thing not only does it not
increase performance the types aren't
increase performance the types aren't
even enforced so they literally they do
nothing and it's worse than a comment
nothing and it's worse than a comment
that's like it's even worse than a
that's like it's even worse than a
comment on the type of the variable
comment on the type of the variable
because most like many people don't even
because most like many people don't even
know that they're not
enforced the only way that they're
enforced the only way that they're
enforced is if you run mypie in strict
enforced is if you run mypie in strict
mode which nobody does and uh even if
mode which nobody does and uh even if
you do that you can still technically
you do that you can still technically
leak if the entire code base well you
leak if the entire code base well you
have to type the entire code base
have to type the entire code base
essentially strict like you can't have
essentially strict like you can't have
mixed typed and not typed it's just
mixed typed and not typed it's just
horrible it's just the worst stuff
imaginable like if you're going to type
imaginable like if you're going to type
your python you could write it in scyon
your python you could write it in scyon
it would be identical the types would be
it would be identical the types would be
enforced and your code would be 100
enforced and your code would be 100
times
faster literally that easy
I don't know why I don't use sion's
I don't know why I don't use sion's
awesome there a few little rough edges
awesome there a few little rough edges
here and there but it's so
good I mean it's the thing that's really
good I mean it's the thing that's really
funny to me about this stack That should
funny to me about this stack That should
kind of show you like I wrote The Snake
kind of show you like I wrote The Snake
Sim in a week it's 450 lines of code it
Sim in a week it's 450 lines of code it
runs at like I think 14 million maybe 12
runs at like I think 14 million maybe 12
million steps per second something like
million steps per second something like
that single thr Ed um you can run you
that single thr Ed um you can run you
can run in like uh you can run like
can run in like uh you can run like
4,000 One agent snake environment or one
4,000 One agent snake environment or one
4096 agent snake environment you can go
4096 agent snake environment you can go
bigger than that if you want there are
bigger than that if you want there are
zero dynamic memory
zero dynamic memory
allocations
allocations
um and there's no pie bind and
um and there's no pie bind and
everything works just perfectly with
everything works just perfectly with
python
all right
all right
cool this was the first game win so we
cool this was the first game win so we
took a little bit of a Down Spike after
took a little bit of a Down Spike after
the first game win but look at
that and then we can go grab our nice
that and then we can go grab our nice
model
training agents to knock down Towers
syon is very
syon is very
good syon is very very
good syon is very very
good
like let's
watch oh look at that they Rush
watch oh look at that they Rush
mid
mid
ha and they they farmed the some agents
ha and they they farmed the some agents
yeah look at that that's
clean it's funny they they don't bother
clean it's funny they they don't bother
taking the
towers I wonder what happens if we train
towers I wonder what happens if we train
without the uh the objective to go
without the uh the objective to go
mid if they'll like actually learn to
mid if they'll like actually learn to
play the
play the
game it's really freaking cool either
game it's really freaking cool either
way
we should uh let's let's post this gift
we should uh let's let's post this gift
and let's get a sweep
going not getting stuck yeah
going not getting stuck yeah
solid Pro
mid better than some of my
mid better than some of my
teammates what people
teammates what people
say yeah let's see
[Music]
uh
e
e e
very
nice the response to this Project's been
nice the response to this Project's been
decent so far it hasn't quite blown up
decent so far it hasn't quite blown up
on Twitter just yet it's gotten some
on Twitter just yet it's gotten some
decent some decent views nothing
crazy really needed to like hit Twitter
crazy really needed to like hit Twitter
proper I mean if people like keep seeing
proper I mean if people like keep seeing
it and I keep getting people looking at
it and I keep getting people looking at
this project I'll keep working on it
this project I'll keep working on it
right we're at least going to make a
right we're at least going to make a
stable environment out of it that have
stable environment out of it that have
some def decent policies right but like
some def decent policies right but like
whether I go past like yeah the agents
whether I go past like yeah the agents
all kind of go mid they fight they look
all kind of go mid they fight they look
reasonable in the way way they fight uh
reasonable in the way way they fight uh
they like press their skill buttons and
they like press their skill buttons and
stuff like whether I go past that that's
stuff like whether I go past that that's
going to depend on uh engagement with
going to depend on uh engagement with
the project pretty
much
e e
oh I already have this perfect
oh I literally already set this up
oh I literally already set this up
perfect
let's see if this
works
e e
invalid hyper pram configuration
invalid hyper pram configuration
Train Sweet parameters
oh what I'm trying to do is set up a
oh what I'm trying to do is set up a
nice hyper pram sweep before dinner
nice hyper pram sweep before dinner
there you
there you
go that seems to
work okay nice so
um
yeah t-x
so that's pretty cool we've actually got
so that's pretty cool we've actually got
a a hyper parameter sweep set up on this
a a hyper parameter sweep set up on this
now hopefully it does something cool for
us possible I don't have it set up
us possible I don't have it set up
perfectly right but um we literally have
perfectly right but um we literally have
the best hper parameter algorithm out
the best hper parameter algorithm out
there now running against this
there now running against this
environment and we're going to get all
environment and we're going to get all
the results nicely
aggregated actually that's already kind
aggregated actually that's already kind
of working fast isn't it
of working fast isn't it
huh
huh
so I'll explain this and then I'm going
so I'll explain this and then I'm going
to go to
dinner what we have
dinner what we have
here oops not not this one it's in
carbs carbs
carbs carbs
sweeps okay so what we have here is a
sweeps okay so what we have here is a
carbs hyperparameter sweep we get all
carbs hyperparameter sweep we get all
our usual metrics but we're also going
our usual metrics but we're also going
to get the summary metrics and this is
to get the summary metrics and this is
going to do Paro optimal basian
going to do Paro optimal basian
optimization over hyperparameter space
optimization over hyperparameter space
it's going to do a 100 different
it's going to do a 100 different
experiments
experiments
um or however many I until I get bored
um or however many I until I get bored
or whatever and it's going to give us
or whatever and it's going to give us
all the results nicely and we're going
all the results nicely and we're going
to be able to see if it learns anything
to be able to see if it learns anything
cool I think it's pretty unlikely it
cool I think it's pretty unlikely it
learns anything all that cool cuz we
learns anything all that cool cuz we
give it a pretty big reward for rushing
give it a pretty big reward for rushing
down mid so really all I'm hoping to get
down mid so really all I'm hoping to get
out of this that like is that we get
out of this that like is that we get
like really nice stable hyper parameters
like really nice stable hyper parameters
that learn to do this very quickly and
that learn to do this very quickly and
consistently um but who knows maybe they
consistently um but who knows maybe they
solve DotA
I think actually um paradoxically I
I think actually um paradoxically I
think it's going to do more once we get
think it's going to do more once we get
good hyper parameters I think it's going
good hyper parameters I think it's going
to learn more once we remove or
to learn more once we remove or
substantially decrease the go to ancient
substantially decrease the go to ancient
reward because that reward is just like
reward because that reward is just like
preventing it from focusing on farming
preventing it from focusing on farming
or like learning anything else
or like learning anything else
yeah but hey this is going to be cool
yeah but hey this is going to be cool
and this is like one of those things
and this is like one of those things
that open AI could not do this
that open AI could not do this
because like their experiments did not
because like their experiments did not
run fast enough to do this right like
run fast enough to do this right like
they could do some hyper parameter
they could do some hyper parameter
tuning yeah but they couldn't do like
tuning yeah but they couldn't do like
okay we're going to just queue up a
okay we're going to just queue up a
hundred jobs right and try to like tune
hundred jobs right and try to like tune
that way that's really freaking hard to
that way that's really freaking hard to
do
do
so we can do
so we can do
it and uh maybe we get some cool stuff
it and uh maybe we get some cool stuff
out of it I will be back streaming this
out of it I will be back streaming this
stuff about the usual around noonish
stuff about the usual around noonish
again tomorrow
again tomorrow
Pacific um we're going to continue on
Pacific um we're going to continue on
this we're gonna hopefully keep getting
this we're gonna hopefully keep getting
the models better it's really now at
the models better it's really now at
this point it's more of a data problem
this point it's more of a data problem
right we just have to get better
right we just have to get better
training data so like better uh
training data so like better uh
observations maybe a few small Network
observations maybe a few small Network
tweaks hyperparameter changes and then
tweaks hyperparameter changes and then
we're going to clean up and balance the
we're going to clean up and balance the
environment a whole bunch and start
environment a whole bunch and start
reintroducing additional features like
reintroducing additional features like
reintroducing the skills you know
reintroducing the skills you know
reintroducing
reintroducing
uh various other things and also
uh various other things and also
starting to clean up the environment and
starting to clean up the environment and
hopefully catching a bunch of bugs along
hopefully catching a bunch of bugs along
the way that is the goal anyways uh
the way that is the goal anyways uh
thank you folks for tuning in one last
thank you folks for tuning in one last
appeal if you haven't already please go
appeal if you haven't already please go
ahead and give this repository a Star
ahead and give this repository a Star
Puffer ipuff lib all the stuff I do is
Puffer ipuff lib all the stuff I do is
open source um there is currently no
open source um there is currently no
revenue from any of this and it helps me
revenue from any of this and it helps me
a whole bunch to get this project on the
a whole bunch to get this project on the
board just like the stars the Twitter
board just like the stars the Twitter
follow like the retweets on stuff it
follow like the retweets on stuff it
really really helps so that's all I ask
really really helps so that's all I ask
uh anyways have a nice evening and see
uh anyways have a nice evening and see
you tomorrow

Kind: captions
Language: en
okay we are
okay we are
live is this day eight or day
live is this day eight or day
nine I think I forgot to update the
nine I think I forgot to update the
title to nine
title to nine
right
right
wow we'll see
wow we'll see
anyways um let me post this and post a
anyways um let me post this and post a
couple things and then I'll go over what
couple things and then I'll go over what
is going to happen
today uh one
today uh one
second Dev day seven nope we're
second Dev day seven nope we're
good
good
okay perfect so uh let me tweet this
okay perfect so uh let me tweet this
real
quick for
cool so we're all set here um the main
cool so we're all set here um the main
plan for today is to actually buckle
plan for today is to actually buckle
down and get these networ to work a
down and get these networ to work a
little better
little better
rested had some ideas and we're
rested had some ideas and we're
basically just going to start uh start
basically just going to start uh start
going through this
going through this
so uh the first thing that I wanted to
so uh the first thing that I wanted to
do was I wanted to grab the parameters
do was I wanted to grab the parameters
roughly from
roughly from
snake because this is like just a decent
snake because this is like just a decent
and robust set of parameters at least
and robust set of parameters at least
some of
some of
them so I went ahead and did
them so I went ahead and did
that let me see which machine we on uh
that let me see which machine we on uh
I yeah okay this
I yeah okay this
one so if I look at the parameters that
one so if I look at the parameters that
I have at the
I have at the
moment
moment
[Music]
[Music]
NOA config where is it where am I
oh I'll show you what I updated these
two so I set these to a learning rate of
two so I set these to a learning rate of
0.0
0.0
O2 which is very aggressive but we
O2 which is very aggressive but we
should be able to get away with this for
should be able to get away with this for
what we're
what we're
doing uh numm two num workers two each
doing uh numm two num workers two each
core is actually running 400 copies of
core is actually running 400 copies of
the environment so really this is uh
the environment so really this is uh
environments gamma point9 Lambda point9
environments gamma point9 Lambda point9
these are very good starter values for
these are very good starter values for
um for problems where like it's
um for problems where like it's
essentially easy and you get pretty nice
essentially easy and you get pretty nice
extense reward
extense reward
signals entry coefficient of
signals entry coefficient of
.01 this is sometimes enough to prevent
.01 this is sometimes enough to prevent
models from collapsing but uh not high
models from collapsing but uh not high
enough to totally crash learning Horizon
enough to totally crash learning Horizon
of6 is just default and then we did
of6 is just default and then we did
essentially four mini batches of updates
essentially four mini batches of updates
32k very large stable updates batch size
32k very large stable updates batch size
of
128k uh and I tried this and this did
128k uh and I tried this and this did
not learn anything so
not learn anything so
this leads me to believe and we really
this leads me to believe and we really
haven't seen the agents learning
haven't seen the agents learning
anything reasonable so I think it's most
anything reasonable so I think it's most
likely that there's a data problem of
likely that there's a data problem of
some type because they really should be
some type because they really should be
learning something yeah this is what I
learning something yeah this is what I
ran before um they get this reward of
ran before um they get this reward of
like
like
Point uh
Point uh
0.14 or whatever which is technically
0.14 or whatever which is technically
better than nothing but like this is a
better than nothing but like this is a
very very easy problem for them to be
very very easy problem for them to be
learning so we should see this go up
learning so we should see this go up
to let me
think let me see what I have the reward
think let me see what I have the reward
scaled to
actually what do I have this scaled too
where did I Define the
where did I Define the
reward I think it was scaled to 0.1 but
reward I think it was scaled to 0.1 but
let me double check that that's very
let me double check that that's very
important the reward
scaling in order to get these uh these
scaling in order to get these uh these
models training
correctly okay right here
correctly okay right here
so yeah looks like the reward is scaled
so yeah looks like the reward is scaled
uh not at all so it should just be one
uh not at all so it should just be one
or uh you know between zero and one I'm
or uh you know between zero and one I'm
actually going to double check on this
actually going to double check on this
locally this is on the other box so add
locally this is on the other box so add
puffer
puffer
lib ocean MOA MOBA
Ranch get
push and we'll I want to pull this down
push and we'll I want to pull this down
on
on
local oh am I on the
local oh am I on the
right I'm not on the right box hold
right I'm not on the right box hold
on yeah I'm on the wrong box great um
that's my
bad what's the reward scale two now here
bad what's the reward scale two now here
so okay it's the same thing yeah it's
so okay it's the same thing yeah it's
the same thing
the same thing
so what did I change on this
box 400 copies
yep right and I just I added a thing to
yep right and I just I added a thing to
randomly respawn players uh that's just
randomly respawn players uh that's just
like a common debug technique in RL it
like a common debug technique in RL it
prevents degenerate situations from
prevents degenerate situations from
crashing learning we're obviously going
crashing learning we're obviously going
to have to take that out after we've
to have to take that out after we've
debug this
debug this
but since this seems reasonable for now
but since this seems reasonable for now
let's uh let's see if we can get this
let's uh let's see if we can get this
to to show us what's
wrong so we're just going to print out
wrong so we're just going to print out
the reward for the agent that we're
the reward for the agent that we're
playing we're going to play one of the
playing we're going to play one of the
agents and we're going to see how the
agents and we're going to see how the
rewards are
rewards are
scaled and if they match up with what we
expect uh right here
K this looks
good the game render
correctly yeah it does render okay so we
correctly yeah it does render okay so we
have a reward of 0.005
I thought we' rescaled this thing to be
I thought we' rescaled this thing to be
substantially
larger did we not rescale
this player. reward
this player. reward
equals previous distance to ancient
equals previous distance to ancient
minus distance to
ancient how do you make the rendering
ancient how do you make the rendering
it's with rayb if you haven't seen RB
it's with rayb if you haven't seen RB
it's like it's an absolutely fantastic
it's like it's an absolutely fantastic
project it gives you uh just low-level
project it gives you uh just low-level
rendering utilities and it makes it
rendering utilities and it makes it
really easy to just render nice stuff in
really easy to just render nice stuff in
code without having to deal with like a
code without having to deal with like a
big framework of [ __ ] or like an IDE
big framework of [ __ ] or like an IDE
or uh you know like a game engine or
or uh you know like a game engine or
anything like that
anything like that
the whole rendering for um
the whole rendering for um
Coba
Coba
oops the whole rendering for uh this
oops the whole rendering for uh this
thing here this is the draw health bars
thing here this is the draw health bars
function and then this block right here
function and then this block right here
which most of is commented yeah that's
which most of is commented yeah that's
like the entire rendering that's it it's
like the entire rendering that's it it's
very very simple and
very very simple and
easy you can write like nice little
easy you can write like nice little
renderers in a 100 lines of code couple
renderers in a 100 lines of code couple
hundred lines of code
hundred lines of code
absolutely love
absolutely love
RB has bindings for every language as
RB has bindings for every language as
well the original is a single file C
well the original is a single file C
library which
library which
is I mean freaking God dear is what that
is why am I getting rewards of
0.05 is this not compiled recently
we should be getting rewards of 0 five
right okay it was just not compiled
right okay it was just not compiled
that's fine so now we correctly get
that's fine so now we correctly get
rewards of 0.5 to 1
rewards of 0.5 to 1
right this looks good
now this is
now this is
fine so do we just have like some weird
fine so do we just have like some weird
issue where we forgot to pull the
new did we just like forget to pull the
new did we just like forget to pull the
new one or something or is it not
new one or something or is it not
learning this is good there's no scaling
learning this is good there's no scaling
quantity
here no it looks like this is
here no it looks like this is
legitimately not working which is what
legitimately not working which is what
we expected but okay it's good that we
we expected but okay it's good that we
know that this is the case now at least
so I'm going to run this just to make
so I'm going to run this just to make
sure that I didn't totally screw
sure that I didn't totally screw
something up somehow while I was half
something up somehow while I was half
asleep yesterday and I think we should
asleep yesterday and I think we should
see a curve that looks like this
see a curve that looks like this
so if uh you get 0. five for moving in
so if uh you get 0. five for moving in
the general direction of the opponent
the general direction of the opponent
and one if you move along a diagonal I
and one if you move along a diagonal I
would expect the average reward to be
would expect the average reward to be
upward of
upward of
0.5 uh if it's doing anything remotely
0.5 uh if it's doing anything remotely
reasonable so the fact that it's this
reasonable so the fact that it's this
low means that it is not
low means that it is not
technically the average reward should be
technically the average reward should be
what would the average reward be
well it
depends I think it's negative 1: one
depends I think it's negative 1: one
isn't
it let me double check that I think it
it let me double check that I think it
should be negative 1 to
one so if you go towards it's it's one
one so if you go towards it's it's one
yeah negative 1 okay so this is like a
yeah negative 1 okay so this is like a
perfectly scaled reward value right 1
perfectly scaled reward value right 1
one yeah that's like
one yeah that's like
perfect so if this doesn't work then
perfect so if this doesn't work then
there's just something
screwy and it says right here yeah this
screwy and it says right here yeah this
is where it was stuck before at like
is where it was stuck before at like
0.15 and if we look at the radiant uh
0.15 and if we look at the radiant uh
the positions
the positions
here dire X it's all the way on the
here dire X it's all the way on the
right side of the map so it's not
right side of the map so it's not
moving uh radiant Y is all the way on
moving uh radiant Y is all the way on
the bottom dire Y is all the way at the
the bottom dire Y is all the way at the
top and and then
top and and then
radiant or is radiant X yeah so these
radiant or is radiant X yeah so these
guys are not really
guys are not really
moving they should be like moving
moving they should be like moving
towards the center of the map or
whatever so we've got lots of different
whatever so we've got lots of different
potential uh candidates for issues to go
potential uh candidates for issues to go
through and attempt to rule out here
I'm going to grab 10 copies of the
environment I'm going to go into the
environment I'm going to go into the
forward pass right
forward pass right
here which is where the features
here which is where the features
are and then what we're going to do is
are and then what we're going to do is
we're going to
render we're going to run train
and we're going to see if the data looks
and we're going to see if the data looks
reasonable as it is going into the
network okay CNN features.
network okay CNN features.
shape
800 that is more than I was
expecting how did I end up with 800
ah CU I ran eight copies and 8times
10 now this end batch size wait a second
10 now this end batch size wait a second
this n batch
this n batch
size does not match up
if n batch size is
if n batch size is
four this should
four this should
be this should be
be this should be
400 did I mess this up
somehow this is potentially
something oh in
something oh in
demo let's look where I make the
environments train of end batch SI so
environments train of end batch SI so
right here
Argus of train gives
Argus of train gives
you m batch Siz is
four so it looks like this is giving you
four so it looks like this is giving you
the
the
correct
H the way that we're making this
H the way that we're making this
function appears to be correct
let's double check on
this welcome folks see the stream's
this welcome folks see the stream's
warmed up a
bit we're doing some hardcore debugging
bit we're doing some hardcore debugging
today we're finding yesterday I got some
today we're finding yesterday I got some
progress made but kind of messed around
progress made but kind of messed around
chatted bu about a bunch of stuff still
chatted bu about a bunch of stuff still
going to answer some questions and have
going to answer some questions and have
some fun but uh we're absolutely fixing
some fun but uh we're absolutely fixing
this
today can relax once we have good models
today can relax once we have good models
training
all right let's see what
all right let's see what
um what this says
um what this says
so num workers M's per worker batch
so num workers M's per worker batch
size
size
right I didn't run this with
right I didn't run this with
multiprocessing
this could be a degenerate case because
this could be a degenerate case because
of the serial back
of the serial back
end
end
right let me double check that I think
right let me double check that I think
that I had accounted for
this but it's possible that I could be
this but it's possible that I could be
running this differently uh locally
running this differently uh locally
versus when I'm training properly
appears so because when I run this
appears so because when I run this
without
without
cereal I mean if I run this
cereal I mean if I run this
multiprocessing I get 400
batch and here I get a 800
batch and here I get a 800
batch
so quars of batch size right
this should be caught right
here yeah n is
here yeah n is
four
right num
right num
workers oh because they're eight workers
that's an interesting conundrum
um I'll add a to do here
so not a huge deal um we are going to
so not a huge deal um we are going to
use the multiprocess version for
use the multiprocess version for
debugging for
debugging for
now just to make
sure okay here we have our features
it's a lot of
features well this is weird right
maybe maybe
maybe maybe
not let me check before the uh the end
code okay so this gives
you this is weird
right three one h
what are these
what are these
indices so three is the agent which is
indices so three is the agent which is
correct one is a
correct one is a
tower oh so it's spawning near its own
tower oh so it's spawning near its own
Tower that's
Tower that's
fine yeah that's actually kind of
fine yeah that's actually kind of
fine how about this one and then this
fine how about this one and then this
one is spawning closer to its base cuz
one is spawning closer to its base cuz
two is uh two is a wall so you have a
two is uh two is a wall so you have a
couple of your agents spawning up near
couple of your agents spawning up near
the wall here it's like random around
the wall here it's like random around
the fountain area you can see the wall
the fountain area you can see the wall
here and then the other agent is over
here and then the other agent is over
here maybe this one or something not
here maybe this one or something not
this one it can't be must be this
one so these features actually do look
one so these features actually do look
good these look good to
me would be very weird not to be able to
me would be very weird not to be able to
learn off of this data
unless so four gives you three in the
unless so four gives you three in the
center five this should change yep so
center five this should change yep so
this is now the other
this is now the other
team and then if we go up to nine it
team and then if we go up to nine it
should still be four and then if we go
should still be four and then if we go
to 10 it should be
three
and this is different uh a different set
and this is different uh a different set
of features as well
from from this zero right yeah
from from this zero right yeah
so they are spawning
so they are spawning
differently which is
good the random seating should actually
good the random seating should actually
be different here which is what we
be different here which is what we
want what else could it be
hello welcome and thank you
oops okay we are getting some good
oops okay we are getting some good
unique data in
here do I see any
patterns what's the plan I'm looking for
patterns what's the plan I'm looking for
weird data quirk and discrepancies that
weird data quirk and discrepancies that
would prevent this thing from learning
would prevent this thing from learning
properly we're trying to debug the
properly we're trying to debug the
policies and get them to actually do
stuff the observation data looks fine to
stuff the observation data looks fine to
me it really
does like
yeah this is all exactly what I would
yeah this is all exactly what I would
expect this data looks good so if this
expect this data looks good so if this
data is good the data is good going into
data is good the data is good going into
the
network how do I run that snake game and
network how do I run that snake game and
all the other games you created they're
all the other games you created they're
in puffer
in puffer
lib um
lib um
the command I'm using for these it's
the command I'm using for these it's
there's a demo file if you just do das
there's a demo file if you just do das
dasel you get options but if you run the
dasel you get options but if you run the
demo with like D- mode eval D- render
demo with like D- mode eval D- render
mode um like well there's probably a
mode um like well there's probably a
reasonable default for render mode even
reasonable default for render mode even
uh but they're in puffer lib they
uh but they're in puffer lib they
haven't been merged up to 10 yet but
haven't been merged up to 10 yet but
snake specifically is in a relatively
snake specifically is in a relatively
stable one Dev branch is relatively
stable one Dev branch is relatively
stable if you want to play around with
stable if you want to play around with
it it's in uh
it it's in uh
ocean right
ocean right
there and you can run it just from uh
there and you can run it just from uh
you can run it right from here with the
you can run it right from here with the
demo file demo. py d-m snake D- modiv
demo file demo. py d-m snake D- modiv
Val there's a little tutorial
here dang the repo got another bunch of
here dang the repo got another bunch of
stars overnight that's awesome
now
um did I mess up the network somehow let
um did I mess up the network somehow let
me
me
see you get the
observations you encode them
observations you encode them
you one hun
them they get put into this convet which
them they get put into this convet which
is the reasonable reasonable Network
is the reasonable reasonable Network
structure get your flat
structure get your flat
features you put them into self flat
features you put them into self flat
which is just a
projection you can catenate
projection you can catenate
them and then
them and then
you re Pro re
you re Pro re
features which is exactly what I did
features which is exactly what I did
with snake
right snake didn't have these extra
right snake didn't have these extra
relis at the end
relis at the end
actually and have this extra projection
stuff grid did though the grid grid
stuff grid did though the grid grid
environments
environments
did it's
did it's
identical so this should be fine
I could just return here to see if this
I could just return here to see if this
makes a difference though I don't think
makes a difference though I don't think
it
it
would me make one change here
I just want to see if this like this
I just want to see if this like this
thing is somehow screwing it
thing is somehow screwing it
up features is cat
up features is cat
uh2 let's just do CNN
channels are you using WSL
channels are you using WSL
Yep this is WSL and the other terminal
Yep this is WSL and the other terminal
that I have is an SSH into my uh well
that I have is an SSH into my uh well
this is technically this one that you're
this is technically this one that you're
looking at now is sshed into a server um
looking at now is sshed into a server um
and the local Dev setup is WSL they're
and the local Dev setup is WSL they're
all inside of the containers though it's
all inside of the containers though it's
all a puffer
tank which is our Docker stack
what editor this is
neovim NM is pretty
neovim NM is pretty
nice it is pretty
nice it's basically just Vim with a
nice it's basically just Vim with a
custom color scheme and like I have two
custom color scheme and like I have two
plugins total so it's pretty close to
plugins total so it's pretty close to
Vim uh the main reason I use neovim is
Vim uh the main reason I use neovim is
just it has uh easier integration with
just it has uh easier integration with
um language model like
autocomplete that's pretty much it I
autocomplete that's pretty much it I
have one syntax highlighting plugin for
have one syntax highlighting plugin for
python cuz that's a little bit weaker uh
python cuz that's a little bit weaker uh
by default and then uh super
Maven it's just super Maven and SEI
Maven it's just super Maven and SEI
that's literally
at I used uh no plugins like default
at I used uh no plugins like default
config Vim for seven
years I wish modern IDs would borrow
years I wish modern IDs would borrow
some of the more advanced macros that
some of the more advanced macros that
neov has I don't even use I don't even
neov has I don't even use I don't even
use any of the advanced stuff honestly
use any of the advanced stuff honestly
I've been using Vim for like 10 years
I've been using Vim for like 10 years
and I just use it in the dumbest way
and I just use it in the dumbest way
possible
possible
like I use a pretty small set of basic
like I use a pretty small set of basic
shortcuts and uh I mainly like it
shortcuts and uh I mainly like it
because it's like it's portable and
because it's like it's portable and
doesn't get in my
way so like I have my whole editor
way so like I have my whole editor
inside of the docker container so like
inside of the docker container so like
if I'm deing on a server or on local I
if I'm deing on a server or on local I
don't have to connect an IDE to the SSH
don't have to connect an IDE to the SSH
session or anything it's just there it's
session or anything it's just there it's
just on whatever box I'm doing and I
just on whatever box I'm doing and I
have an identical uh setup it's really
have an identical uh setup it's really
nice which version of Vim did you I have
nice which version of Vim did you I have
no
idea this is just this is just
Neo yeah I used I used
Neo yeah I used I used
um I used VSS code for like a year or so
um I used VSS code for like a year or so
with the Vim plugin and like it's the
with the Vim plugin and like it's the
second best thing out there but I still
second best thing out there but I still
hated
it I like this so much more
people have been trying to get me to tr
people have been trying to get me to tr
like to use cursor um I might do a
like to use cursor um I might do a
stream day at one po at one point but
stream day at one po at one point but
like I've warned folks that it's
like I've warned folks that it's
probably just going to be me [ __ ] on
probably just going to be me [ __ ] on
it because I have not found aggressive
it because I have not found aggressive
language model integration to be helpful
whatsoever did you download
whatsoever did you download
the this is on a container so this is
the this is on a container so this is
downloaded on a bunch to or Debian or
downloaded on a bunch to or Debian or
whatever the heck container I have here
whatever the heck container I have here
so no it's not it's not the gooey
so no it's not it's not the gooey
version or whatever it's the setup is in
version or whatever it's the setup is in
if you go check out puffer tank you can
if you go check out puffer tank you can
see the setup in there because it's
see the setup in there because it's
included it's on the GitHub including
included it's on the GitHub including
the config for this
the config for this
um let me think
um let me think
so I want to rerun this
so I want to rerun this
[Music]
[Music]
with oops I want to just see if this
with oops I want to just see if this
does anything different all I did was
does anything different all I did was
cut out a couple of linear
cut out a couple of linear
layers apparently I did that
incorrectly this
incorrectly this
is output shape right I forgot one thing
is output shape right I forgot one thing
oops
assuming this runs I'll check something
assuming this runs I'll check something
for you now yeah okay so while that's
for you now yeah okay so while that's
running I'll just show you um I've been
running I'll just show you um I've been
actually considering merging this into
actually considering merging this into
the puffer lib repo cuz I'm just it's
the puffer lib repo cuz I'm just it's
annoying maintaining multiple repos mono
annoying maintaining multiple repos mono
repo is actually pretty good um
right here is puffer
right here is puffer
tank and puffer tank has our Docker
tank and puffer tank has our Docker
files these are also these are pushed to
files these are also these are pushed to
Docker Hub as pre-built images so you
Docker Hub as pre-built images so you
don't have to like spend forever
don't have to like spend forever
building stuff but if you look in in
building stuff but if you look in in
some of these um I think
some of these um I think
the puffer depths one has
the puffer depths one has
my yeah here's the neovim setup right
my yeah here's the neovim setup right
here
and then uh it actually has my config in
and then uh it actually has my config in
it here like this is literally right
it here like this is literally right
here this is
here this is
it it's got like a couple very basic you
it it's got like a couple very basic you
know defaults and then the rest of this
know defaults and then the rest of this
is just hardcoded color
scheme do I just clone puffer tank uh
scheme do I just clone puffer tank uh
you can you don't have to build the
you can you don't have to build the
files though like if you want to use
files though like if you want to use
this container this Docker shell has
this container this Docker shell has
just a convenient utility you can just
just a convenient utility you can just
like bash doer. sh
like bash doer. sh
test and it will pull down for you
test and it will pull down for you
um the 1.0 version of puffer tank this
um the 1.0 version of puffer tank this
is like a just so you know this is like
is like a just so you know this is like
a 10 to 13 gigabyte image but this is
a 10 to 13 gigabyte image but this is
the whole Dev setup for puffer lib and
the whole Dev setup for puffer lib and
all of the reinforcement learning
all of the reinforcement learning
environments that are bound to puffer
environments that are bound to puffer
lib it includes pytorch with like Cuda
lib it includes pytorch with like Cuda
12.1 so it's assuming you have a GPU
12.1 so it's assuming you have a GPU
machine yeah lots of stuff there's a
machine yeah lots of stuff there's a
little setup here there's also a vs code
little setup here there's also a vs code
integration for
integration for
people I try to make this stuff pretty
people I try to make this stuff pretty
nice and easy for
nice and easy for
people okay so right here I can see that
people okay so right here I can see that
this network architecture change did not
this network architecture change did not
help so this is increasing my confidence
help so this is increasing my confidence
that um this issue is not just a network
that um this issue is not just a network
config
config
issue so
I think as a result the smart thing to
do p torch the only Cuda
dependency
um I I don't think it would be easy to
um I I don't think it would be easy to
like
like
so technically puffer lib doesn't use P
so technically puffer lib doesn't use P
torch for stuff but like the training
torch for stuff but like the training
all the training Integrations make a
all the training Integrations make a
pretty hard assumption on that um
zuda I don't think that there's anything
zuda I don't think that there's anything
in ml that's easily swapped for those
in ml that's easily swapped for those
frankly you could change it to CPU you'd
frankly you could change it to CPU you'd
probably have to swap the uh the P torch
probably have to swap the uh the P torch
install though because like the
install though because like the
container is built assuming GPU it's
container is built assuming GPU it's
built off of a a GPU base so you'd
built off of a a GPU base so you'd
probably have to swap some stuff
around you also don't need to use the
around you also don't need to use the
whole container right like the container
whole container right like the container
is for heavy RL environments like puffer
is for heavy RL environments like puffer
is not that hard to install you can just
is not that hard to install you can just
pip install puffer lib or you can just
pip install puffer lib or you can just
Co clone puffer Li if you want the dev
Co clone puffer Li if you want the dev
branch check out the dev branch and pip
branch check out the dev branch and pip
install d e do and that will just
work hey linky
work hey linky
thank you for the reminder for the
thank you for the reminder for the
advertisement there if you find any of
advertisement there if you find any of
this useful please go ahead and Star
this useful please go ahead and Star
Puffer lib it helps me out a whole ton
Puffer lib it helps me out a whole ton
really helps me get this work on the
board link is one of the Pokemon uh
board link is one of the Pokemon uh
Pokemon RL
Pokemon RL
contributors doing great work over there
I know I've been kind of a wall linky
I know I've been kind of a wall linky
I've been working on this
project trying to make this actually
something literally install poer is a
something literally install poer is a
dependency yeah it's supposed to be easy
dependency yeah it's supposed to be easy
and if it's not let me know because it
and if it's not let me know because it
should be
did you find out what the problem was
did you find out what the problem was
yesterday no that's what we're doing
yesterday no that's what we're doing
right
now can I just run a sweep on this while
now can I just run a sweep on this while
I try to debug
I try to debug
locally I really should have done this
locally I really should have done this
yesterday but I was just exhausted
I guess this does something we'll see
I guess this does something we'll see
what it's doing but in the meantime
what it's doing but in the meantime
let's debug more from here so the
let's debug more from here so the
observations coming into the network
observations coming into the network
look good this is usually the biggest
look good this is usually the biggest
source of issues but uh it looks pretty
source of issues but uh it looks pretty
darn solid
here
so how would I swap and what would I
so how would I swap and what would I
swap you can just pip in like you if
swap you can just pip in like you if
you're if you have something where
you're if you have something where
puffer tank isn't useful just just clone
puffer tank isn't useful just just clone
the puffer lib repo check out the dev
the puffer lib repo check out the dev
Branch pip install Dash e
Dot there are instructions on the
Dot there are instructions on the
website and on uh the GitHub and
everything let me know if it let me know
everything let me know if it let me know
if they don't work for
you um there's also discord. Discord uh
you um there's also discord. Discord uh
cord I can't type. GP Puffer that is the
cord I can't type. GP Puffer that is the
Discord for
Discord for
this nice and
this nice and
simple um
I guess we got to check the environment
I guess we got to check the environment
file very closely now
right see what's going on with
it call
reset this is where you assign the
reset this is where you assign the
actions into the actions
actions into the actions
buffer we know that this actually
buffer we know that this actually
works we know that this works because we
works we know that this works because we
can play the
game Let's double check right here that
game Let's double check right here that
we're actually getting actions from the
we're actually getting actions from the
model let me fix the AC as well real
model let me fix the AC as well real
quick
so
oh we can't do multiprocessing here this
oh we can't do multiprocessing here this
has to be
serial 100
actions why are there 100
actions because I have 10 environments
actions because I have 10 environments
per uh per instance
per uh per instance
right
right
yeah that's correct then
so
wait why did
64 does that Break
64 does that Break
Stuff no that can't break stuff because
Stuff no that can't break stuff because
we've double checked before that
yeah gave me an error something I can't
yeah gave me an error something I can't
create or remove file and install
directory I think that's that's
directory I think that's that's
typically a permissioning
typically a permissioning
error um that means that wherever you're
error um that means that wherever you're
trying to do this uh you're you're in
trying to do this uh you're you're in
some folder that's you need permissions
some folder that's you need permissions
on uh that should be not puffer lib
on uh that should be not puffer lib
doing that you should just you look up
doing that you should just you look up
like you're going to have to like Chown
like you're going to have to like Chown
that or
whatever I don't know linky we've never
whatever I don't know linky we've never
had like weird permissioning errors with
had like weird permissioning errors with
puffer
puffer
right that aren't just system
errors no yeah okay I I'm guessing those
errors no yeah okay I I'm guessing those
are system
are system
errors if you think that they're puffer
errors if you think that they're puffer
errors then I will definitely fix those
though the step is the step logic is
though the step is the step logic is
totally
totally
fine we're able to get rewards back from
fine we're able to get rewards back from
stuff observations rewards terminals
stuff observations rewards terminals
truncations
infos which are always false
right yeah they're always
false and again we know that buff
false and again we know that buff
observations is correct because we know
observations is correct because we know
the data going to the neural network is
correct and there you go
this looks very
this looks very
reasonable like I'm actually not seeing
reasonable like I'm actually not seeing
any of the usual red flags whatsoever
any of the usual red flags whatsoever
for like common screw-ups
for like common screw-ups
here if I check this
sweep WSL loves to throw permissioning
sweep WSL loves to throw permissioning
errors it's really obnoxious
what the heck is wrong with
this these nice dense
this these nice dense
rewards you have what should be a
rewards you have what should be a
trivial
trivial
task every indication is that the data
task every indication is that the data
going into the network is is correct and
going into the network is is correct and
that the actions are being
applied the hyper parameters are all
reasonable should definitely be working
we know puffer lib is not like fully
broken how much stuff have I trained
broken how much stuff have I trained
since the
refactor I have trained stuff on this
refactor I have trained stuff on this
Branch haven't
Branch haven't
I non-trivial stuff
snake EG bolts really
why well that's sketchy
why well that's sketchy
right didn't SE fault
before no this is a known good
before no this is a known good
environment
well it's trying to launch a client
well it's trying to launch a client
which is very
weird though potentially I messed with
weird though potentially I messed with
the renderer the environment is known
the renderer the environment is known
good
I'm just wondering if I like really
I'm just wondering if I like really
broke some core stuff in puffer with the
broke some core stuff in puffer with the
latest config I don't think I did though
why is it still on render mode
human e
okay so it was literally just the
okay so it was literally just the
renderer that's
fine that's clearly
training that's on my local Box by the
training that's on my local Box by the
way that it get 700k
train Crazy
Fast yeah that's definitely training so
Fast yeah that's definitely training so
it's impossible then that the whole I
it's impossible then that the whole I
didn't break anything critical I mean
didn't break anything critical I mean
there's no way I would have with the
there's no way I would have with the
latest refactor I just changed config
latest refactor I just changed config
stuff
so that learns to play Snake in like a
minute
so how's the sweep going
yeah
no why would this environment be
no why would this environment be
hard there's nothing hard about it
right I mean
oh yeah okay
what if we only spawned one
team are we both we spawned them uh
team are we both we spawned them uh
spawn player
if we just made them all be on this
team it's not go to Mid it's go to the
team it's not go to Mid it's go to the
enemy ancient which isn't really even a
enemy ancient which isn't really even a
debug task if you think about it like
debug task if you think about it like
technically that could be enough to just
technically that could be enough to just
play the
game okay so this now spawns 10 players
game okay so this now spawns 10 players
on the radiant team
I just want to see if this does anything
different how does it man man to be that
bad I
bad I
rebuild yeah I did rebuild
is the learning rate just way too high
is the learning rate just way too high
like there should be there should be
like there should be there should be
other signs though if that's the case
no it just very quickly
learns how bad does the initial entropy
crash you have very very clear learning
crash you have very very clear learning
signals
here very clear learning signals
What GPU are you running
What GPU are you running
my local box has a 3080 in it and each
my local box has a 3080 in it and each
of the puffer boxes has a
4090 wasn't the recurrent net being
weird what the
heck what's the interpretation of Lambda
heck what's the interpretation of Lambda
equals z Lambda equals 1 again hold on
equals z Lambda equals 1 again hold on
it's been so long with po looking at the
it's been so long with po looking at the
paper
where's the
thing e
it's the PO
it's the PO
paper uh
zero this do anything
no none of that does
no none of that does
anything so
weird I was trying to just do a one step
weird I was trying to just do a one step
[Music]
optimization what the heck would make it
optimization what the heck would make it
this
bad uh you can use one the commands like
this you don't have MOA in that one but
this you don't have MOA in that one but
snake for
snake for
instance we'll pull it
up these puffers are clearly just C
up these puffers are clearly just C
they're capable of moving around right
they behave what looks like randomly
they behave what looks like randomly
when they have random neuron net
when they have random neuron net
weights they receive what look like good
weights they receive what look like good
observations the policy collapses even
observations the policy collapses even
though they should be getting good
though they should be getting good
Rewards
maybe um
evv pathfinding is entirely learned or
evv pathfinding is entirely learned or
is there any hard-coded
is there any hard-coded
constraints pathf finding is learned the
constraints pathf finding is learned the
creeps have precomputed pathf finding
creeps have precomputed pathf finding
the agents are entirely learning the
the agents are entirely learning the
path finding but the thing is like
path finding but the thing is like
they're not like it's really weird cuz
they're not like it's really weird cuz
they're not learning anything and like
they're not learning anything and like
you get a reward of plus one if you go
you get a reward of plus one if you go
straight towards the enemy ancient like
straight towards the enemy ancient like
plus one plus one if you take a step in
plus one plus one if you take a step in
the opposite direction you get negative
the opposite direction you get negative
one if you stand in place you get zero
one if you stand in place you get zero
if you take a step like half in the
if you take a step like half in the
right direction you get 0. five it's
right direction you get 0. five it's
like the easiest thing in the world to
like the easiest thing in the world to
learn so like there's just something
learn so like there's just something
that's screwy I think and I don't know
that's screwy I think and I don't know
what it is
I mean I have a pretty robust set of
I mean I have a pretty robust set of
checks I normally go through and they've
checks I normally go through and they've
all come up clean so
far occasionally this is just like
far occasionally this is just like
egregiously bad hyper parameters but I'm
egregiously bad hyper parameters but I'm
starting with a default set that I've
starting with a default set that I've
used for many other similar uh simple
used for many other similar uh simple
tasks that have been
fine you're going to have to change the
fine you're going to have to change the
defaults for CPU and stuff most likely
defaults for CPU and stuff most likely
undefined symbol is going to be a bad
undefined symbol is going to be a bad
torch version uh I think the latest
torch version uh I think the latest
puffer uses like 2.4 whatever I'd say go
puffer uses like 2.4 whatever I'd say go
download the like the nightly or
download the like the nightly or
whatever uh just CPU version it'll
whatever uh just CPU version it'll
probably fix
it e
there's something else I'm missing
there's something else I'm missing
here I mean there's there's something
here I mean there's there's something
clearly but I think I'm looking in the
clearly but I think I'm looking in the
wrong place like I shouldn't be
wrong place like I shouldn't be
spending time suspecting the stable
spending time suspecting the stable
implementation that we've used for like
implementation that we've used for like
everything for the past
year what's unique about this
year what's unique about this
project and we don't have very many
project and we don't have very many
agents per environment
really that shouldn't
matter you get
matter you get
very consistent reward
what's the issue it's not learning
a very simple debug task with the Moa
a very simple debug task with the Moa
and it's not learning
it that's the
issue and the sweeps so far haven't
issue and the sweeps so far haven't
found anything
found anything
better so I don't think it's just hyper
better so I don't think it's just hyper
parameters
h
sweeping Epoch as well no
shouldn't have to but you're right that
shouldn't have to but you're right that
there was like a weird thing I remember
there was like a weird thing I remember
with
that I do remember there being something
that I do remember there being something
weird about
EPO doesn't seem to
EPO doesn't seem to
help very very quickly
crashes what was the starting
entropy still bouncing around as
well I don't understand how it says that
well I don't understand how it says that
they're getting levels as well
when they have an X of
6.4 radiant Y at 69 means that they've
6.4 radiant Y at 69 means that they've
gone like straight
gone like straight
up doesn't it
have the agent resetting this copy hold
have the agent resetting this copy hold
on I'll leave this for for
now e
this is such an easy task
now this is where it gets stuck right
yeah they do better
yeah they do better
when you reset
when you reset
them not by much
though I do like
32 I didn't rebuild
so they can optimize it looks
like I mean you have to make the task so
like I mean you have to make the task so
stupidly simple
stupidly simple
though what about why
though what about why
what do you mean what about
why this does learn over time now it
why this does learn over time now it
looks to
me but it's
me but it's
so why do you have to do this
the cords is all internal Sim yeah
I mean at least this tells me it's
I mean at least this tells me it's
capable of
capable of
learning like this rules out a bunch of
learning like this rules out a bunch of
other
[ __ ] yeah this rules out a whole
[ __ ] yeah this rules out a whole
bunch of other
[ __ ] e
the entropy is really low though
h
I have a custom RL environment that's
I have a custom RL environment that's
only partially
only partially
observed same to deal with this I wanted
observed same to deal with this I wanted
to use a recurrent poo
to use a recurrent poo
implementation but all the examples I
implementation but all the examples I
found use convolutional nails for
found use convolutional nails for
Atari can I just replace them with a
Atari can I just replace them with a
linear layer if you have a flat if you
linear layer if you have a flat if you
have flat observations yes are other is
have flat observations yes are other is
necessary
necessary
no you like puffer lib does that as
well all you have to do is replace the
well all you have to do is replace the
um comms with
um comms with
linear we do that in some debug
linear we do that in some debug
environments as well for Puffer
replace all cers with one linear or one
replace all cers with one linear or one
linear depends on the complexity of your
linear depends on the complexity of your
environment
I'd usually say if you have some like
I'd usually say if you have some like
basic
basic
thing
thing
um fully connected reu fully connected
um fully connected reu fully connected
maybe is a good entry
maybe is a good entry
to your
to your
lstm may or may not want to put a re at
lstm may or may not want to put a re at
the end I think I have reos at the
end
for e
I mean this is what we wanted them to
I mean this is what we wanted them to
learn
I'm so weird
there's got to be a reason for
this
for e
e e
very bizarre Dynamic that's happening
so I can get them to learn the thing
so I can get them to learn the thing
that I want but only in a ridiculously
that I want but only in a ridiculously
stupidly simplified version
why they're getting zero reward
entropy
bonus I don't believe it would be
bonus I don't believe it would be
entropy
bonus that'd be really funny
bonus that'd be really funny
welcome
back deep in thought here trying to
back deep in thought here trying to
figure out what is wrong with this found
figure out what is wrong with this found
some Clues this
morning definitely found some Clues
set this to
zero how the gray guys
zero how the gray guys
doing
so if I let them play the game
so if I let them play the game
normally they just learn to go run up
normally they just learn to go run up
against a wall and chill there it's
against a wall and chill there it's
bizarre if I reset the agents every 128
bizarre if I reset the agents every 128
steps on average so they respawn them
steps on average so they respawn them
randomly they still just go into a wall
randomly they still just go into a wall
if I respawn them randomly every 32
if I respawn them randomly every 32
steps they learn to run at each other
steps they learn to run at each other
down
mid so there's like some incentive that
mid so there's like some incentive that
I'm not seeing here like there's some
I'm not seeing here like there's some
incentive that I'm not seeing here it's
weird no
weird no
well that is actually better than
before not really little
before not really little
bit oh
bit oh
wait that's
wait that's
progress that is progress
there like your P oh I missed a message
there like your P oh I missed a message
Lone Wolf design hi there I really liked
Lone Wolf design hi there I really liked
your PhD presentation was extremely
your PhD presentation was extremely
interesting thank you I am considering
interesting thank you I am considering
doing some additional long form content
doing some additional long form content
for the
for the
channel um tree they do not lose reward
channel um tree they do not lose reward
for dying that's what makes it
for dying that's what makes it
weird
weird
um yeah I will potentially do some
um yeah I will potentially do some
additional long form content
additional long form content
um nothing as ridiculously high effort
um nothing as ridiculously high effort
as my PhD defense obviously but I'm
as my PhD defense obviously but I'm
thinking of putting some cool stuff
thinking of putting some cool stuff
together for now I've been just doing a
together for now I've been just doing a
bunch of Dev live because uh
bunch of Dev live because uh
well there's been a lot of progress
well there's been a lot of progress
since my PhD defense I'll say like
since my PhD defense I'll say like
reinforcement learning is practically a
reinforcement learning is practically a
different field since since then because
different field since since then because
of all this
stuff I think I figured something out
stuff I think I figured something out
hold on
oh what's wait uh hold on let me
there you go okay I actually got this
there you go okay I actually got this
thing to somewhat train it's not perfect
thing to somewhat train it's not perfect
but uh I will explain this in a second
but uh I will explain this in a second
let me answer these questions so are you
let me answer these questions so are you
still in Academia after your
still in Academia after your
PhD uh no I am currently doing puffer
PhD uh no I am currently doing puffer
full-time puffer is technically a
full-time puffer is technically a
startup but for the time being I'm just
startup but for the time being I'm just
doing a whole bunch of Open Source
doing a whole bunch of Open Source
reinforcement learning Dev mostly infra
reinforcement learning Dev mostly infra
and high performance infrastructure and
and high performance infrastructure and
tools trying to just fix all of this
tools trying to just fix all of this
stuff that makes RL a hard and unfun
stuff that makes RL a hard and unfun
place to work in uh so that's what
place to work in uh so that's what
you're seeing here ultra high
you're seeing here ultra high
performance simulation development live
performance simulation development live
and fixing all sorts of things that are
and fixing all sorts of things that are
broken with
broken with
RL um so not not exactly industry not
RL um so not not exactly industry not
exactly Academia I don't really plan on
exactly Academia I don't really plan on
publishing stuff so much anymore because
publishing stuff so much anymore because
it's just there's a whole rant on that
it's just there's a whole rant on that
topic but um I do release stuff I
topic but um I do release stuff I
release a lot of code I do blog posts I
release a lot of code I do blog posts I
tweet a bunch of stuff so everything is
tweet a bunch of stuff so everything is
still out in the open and all this is
still out in the open and all this is
open
open
source all what stuff why is it
source all what stuff why is it
different
different
now so uh RL is different because of the
now so uh RL is different because of the
level of performance that puffer lib can
level of performance that puffer lib can
introduce and the level of Simplicity it
introduce and the level of Simplicity it
can introduce like I would not believe
can introduce like I would not believe
this was possible three months ago and I
this was possible three months ago and I
think that the majority of the field
think that the majority of the field
like they might tell you now oh of
like they might tell you now oh of
course you can do that but I guarantee
course you can do that but I guarantee
you if you just pose them the question
you if you just pose them the question
neutrally 3 months ago they would tell
neutrally 3 months ago they would tell
you this is
impossible like hey can you write a like
impossible like hey can you write a like
a MOBA simulator in a week and a half
a MOBA simulator in a week and a half
that runs at over a million frames per
that runs at over a million frames per
second and trains at like a half a
second and trains at like a half a
million plus on one GPU they'd be like
million plus on one GPU they'd be like
are you crazy no yeah we can just do
are you crazy no yeah we can just do
that
now Okay so
now Okay so
I'm really surprised that the sweep
I'm really surprised that the sweep
didn't find this though I probably
didn't find this though I probably
should have just ran the sweep overnight
should have just ran the sweep overnight
because if I let it run for longer it
because if I let it run for longer it
would have solved it
would have solved it
um actually has it solved it while I've
um actually has it solved it while I've
been talking that'd be really funny if
been talking that'd be really funny if
the algorithm be beat me to it
the algorithm be beat me to it
surprisingly it
surprisingly it
didn't very surprised that it
didn't what's this thing tried for
didn't what's this thing tried for
entropy
I would have expected this one to
work oh but it's sampling stupid lambdas
work oh but it's sampling stupid lambdas
okay so this would take a while to find
okay so this would take a while to find
anything
good with puffer AI how many agents can
good with puffer AI how many agents can
you
you
have wait what uh puffer fish
have wait what uh puffer fish
go yeah puffer's fun with puffer AI how
go yeah puffer's fun with puffer AI how
many agents can you
many agents can you
have well
have well
it depends what you mean by that but uh
it depends what you mean by that but uh
the training skill that we're typically
the training skill that we're typically
looking at for a puffer that I found to
looking at for a puffer that I found to
be very effective is I'll usually
be very effective is I'll usually
simulate on the order of 16,000 agents
simulate on the order of 16,000 agents
um so for DOTA that is 1,600 copies of
um so for DOTA that is 1,600 copies of
the game and I can do that on two CPU
the game and I can do that on two CPU
cores typically we use two CPU cores
cores typically we use two CPU cores
instead of one because then we can
instead of one because then we can
buffer them so that you're never waiting
buffer them so that you're never waiting
on data um and the 16,000 numbers become
on data um and the 16,000 numbers become
because mini batch size 8192 is
because mini batch size 8192 is
typically what you want to saturate
typically what you want to saturate
Hardware you can go much higher if you
Hardware you can go much higher if you
would
would
like um sometimes there are reasons to
like um sometimes there are reasons to
do
do
that but yeah um you're looking at the
that but yeah um you're looking at the
level of performance we have now this on
level of performance we have now this on
one CPU core this little mini MOBA runs
one CPU core this little mini MOBA runs
five uh like five and a half hours worth
five uh like five and a half hours worth
of games per
second it's very very fast
second it's very very fast
and it's very simple as
well oh I missed this uh spinning up in
well oh I missed this uh spinning up in
puffer would be good long form
puffer would be good long form
content yeah
content yeah
I keep changing it so frequently that
I keep changing it so frequently that
like I need to do that once the user
like I need to do that once the user
experience stabilizes a bit more um like
experience stabilizes a bit more um like
I just redid the configuration system
I just redid the configuration system
now and I've got to like rewrite stuff
now and I've got to like rewrite stuff
for that um the other thing that's like
for that um the other thing that's like
spinning up with puffer Li is kind of
spinning up with puffer Li is kind of
awkward is um puffer lib isn't a
awkward is um puffer lib isn't a
framework like it's not some big heavy
framework like it's not some big heavy
thing where we give you a whole bunch of
thing where we give you a whole bunch of
first party algorithms and stuff it's
first party algorithms and stuff it's
just like here's a library with some
just like here's a library with some
tools to make your stuff fast here's
tools to make your stuff fast here's
some fast Sims and like we have one
some fast Sims and like we have one
training demo with a PO that we have
training demo with a PO that we have
souped up but like there's not a ton of
souped up but like there's not a ton of
stuff to
it I did a cool um I did a a like somewh
it I did a cool um I did a a like somewh
a somewhat prepared demo video like I
a somewhat prepared demo video like I
did a demo Day video uh that's the
did a demo Day video uh that's the
tagged post on my Twitter and is also on
tagged post on my Twitter and is also on
the YouTube where I go through like all
the YouTube where I go through like all
of the nice technical advancements in
of the nice technical advancements in
puffer 10
puffer 10
it's a less accessible video but it's um
it's a less accessible video but it's um
you know it shows a lot of what I
did
e e
let's try
this put this to One update Epoch as
this put this to One update Epoch as
well
one
second
second
88 200
try
this what care do you take for your
this what care do you take for your
hair it's like Trader Joe's three in
[Music]
[Music]
one what's the end goal for this DOTA
one what's the end goal for this DOTA
style project or is it more a test of
style project or is it more a test of
the
the
libraries the goal is to have a very
libraries the goal is to have a very
complicated and interesting and stupidly
complicated and interesting and stupidly
high performance environment that you
high performance environment that you
can use for research like what type of
can use for research like what type of
research can you do when you have an
research can you do when you have an
environment that's more complex and
environment that's more complex and
interesting than literally anything else
interesting than literally anything else
out there that also runs a thousand
out there that also runs a thousand
times
times
faster like that's the that's the goal
that's fair I remember you don't like
that's fair I remember you don't like
Minecraft ml I mean okay I know the guy
Minecraft ml I mean okay I know the guy
who did that one and he's awesome like
who did that one and he's awesome like
my problem with Minecraft as an RL
my problem with Minecraft as an RL
environment like here there's an
environment like here there's an
environment that's called uh crafter
environment that's called uh crafter
that I also think is a terrible
that I also think is a terrible
environment that's like 2D Minecraft
environment that's like 2D Minecraft
okay and now there's an environment
okay and now there's an environment
called craft ax which is the exact same
called craft ax which is the exact same
environment 2D
environment 2D
Minecraft but like a thousand times
Minecraft but like a thousand times
faster and that environment is awesome
faster and that environment is awesome
you see it's like it just has to be
you see it's like it just has to be
fast like your environment has to be
fast like your environment has to be
fast it has has to be complex and
fast it has has to be complex and
interesting if your environment is slow
interesting if your environment is slow
I can't do anything with
it yeah we love okay that's the
it yeah we love okay that's the
exception that proves the rule though
exception that proves the rule though
like the craft a environment in Jack
like the craft a environment in Jack
that's like 3,000 lines of puzzle
that's like 3,000 lines of puzzle
solving Madness to make that thing work
solving Madness to make that thing work
in Jack like you can actually look at
in Jack like you can actually look at
that and say hey does this look like a
that and say hey does this look like a
reasonable thing to have to do to
reasonable thing to have to do to
implement every new environment no no it
implement every new environment no no it
doesn't like just because that one guy
doesn't like just because that one guy
that did that is really really good
that did that is really really good
doesn't mean I'm smart enough to figure
doesn't mean I'm smart enough to figure
out how to do
it I like to write dumb simple code that
it I like to write dumb simple code that
is fast
interesting that this one doesn't seem
interesting that this one doesn't seem
to be optimizing anywhere near as
well this is pretty
well this is pretty
weird on this
weird on this
works yeah it's way faster than
works yeah it's way faster than
mineral yeah
like the new standard is going to be M's
like the new standard is going to be M's
run at a million steps per second per
run at a million steps per second per
CPU core training runs at at least
CPU core training runs at at least
several hundred, steps per second bare
minimum get out of here with your slow
minimum get out of here with your slow
ass learning
stuff like the thing is you don't even
stuff like the thing is you don't even
have to make stuff complicated in order
have to make stuff complicated in order
to do this right like it's not like oh
to do this right like it's not like oh
yeah but in order to make it that
yeah but in order to make it that
complicated you need like a 20,000 line
complicated you need like a 20,000 line
RL stack with all sorts of complicated
RL stack with all sorts of complicated
little components no like the
little components no like the
environment implementation puffer lib
environment implementation puffer lib
puffer Li's vectorization like the whole
puffer Li's vectorization like the whole
training code everything together is 3
training code everything together is 3
to 4,000
to 4,000
lines the whole stack is three to 4,000
lines the whole stack is three to 4,000
lines
lines
deep very very basic and
doable and try this
when are you going to write an efficient
when are you going to write an efficient
Pokemon
Pokemon
H how about I teach you how to write an
H how about I teach you how to write an
efficient Pokemon and then you do
it can we do that
instead I've been grinding like
instead I've been grinding like
simulation engineering stuff for the
simulation engineering stuff for the
last two months straight so
it's going to be finish this block
it's going to be finish this block
finish up this work block get right on
finish up this work block get right on
the scon B yeah pretty much it's pretty
the scon B yeah pretty much it's pretty
much for me it's going to be like finish
much for me it's going to be like finish
this work block get these Sims in a good
this work block get these Sims in a good
spot
spot
um figure out like what I'm going to
um figure out like what I'm going to
decide needs to be done for release like
decide needs to be done for release like
polish stuff up for release this mobile
polish stuff up for release this mobile
I'm going to hang on to for a little
I'm going to hang on to for a little
longer you know chill for a few days
longer you know chill for a few days
write a few blog posts maybe make a
write a few blog posts maybe make a
couple of videos like and then we're
couple of videos like and then we're
going to go to the next phase of the
going to go to the next phase of the
master plan for RL from there um the
master plan for RL from there um the
goal is pretty much at the moment just
goal is pretty much at the moment just
to finish all of these
first interesting that this one doesn't
first interesting that this one doesn't
seem to work with the other one did what
seem to work with the other one did what
did I do differently
8843 10
8843 10
M's that
M's that
works
works
huh it's really weird
um do you have any tips on making
um do you have any tips on making
Sims
Sims
yes
yes
um scon is essentially god
um scon is essentially god
tier and if you want a very nice
tier and if you want a very nice
example of how to make something that
example of how to make something that
runs at like 10 million plus steps per
runs at like 10 million plus steps per
second single
thread start on your way in helps me out
thread start on your way in helps me out
a
bunch so I
made have you seen the snake
environment let me find it
I've been tweeting a lot of
stuff where the heck is the snake
en okay here
en okay here
so snake EnV it's snake with 4096
so snake EnV it's snake with 4096
snakes right you can also do smaller
snakes right you can also do smaller
versions of it here's gigantic version
versions of it here's gigantic version
of it with lots of
of it with lots of
snakes you can do whatever you want in
snakes you can do whatever you want in
there puffer Li puffer lib environments
there puffer Li puffer lib environments
ocean
ocean
snake it
snake it
is 230 lines of wrapper code in
is 230 lines of wrapper code in
Python and 230 lines of scyon for the
Python and 230 lines of scyon for the
simulation Logic the thing runs over 10
simulation Logic the thing runs over 10
million steps per second single thread
million steps per second single thread
and trains it over 10 uh over a million
and trains it over 10 uh over a million
steps per second on one
steps per second on one
GPU so this is basically the whole this
GPU so this is basically the whole this
is the simple version of how to make
is the simple version of how to make
Sims
fast I added Xfinity for my internet
fast I added Xfinity for my internet
outage yesterday and they actually reply
outage yesterday and they actually reply
with this I'm sure automated
with this I'm sure automated
account that's so funny
are there any Sim examples you would
are there any Sim examples you would
suggest I
suggest I
create just to
create just to
learn I mean if you can think of any
learn I mean if you can think of any
cool like retro game type things like
cool like retro game type things like
simple retro games if you want to add
simple retro games if you want to add
something to puffer lib that would be
something to puffer lib that would be
awesome um I did snake as an example cuz
awesome um I did snake as an example cuz
like people know slyther iio right and I
like people know slyther iio right and I
could basically make slyther iio but
could basically make slyther iio but
make it stupidly fast for RL I don't
make it stupidly fast for RL I don't
know if there are any other ones that
know if there are any other ones that
are like nice in multiplayer like that
are like nice in multiplayer like that
you might have to do single player which
you might have to do single player which
those are a little harder to make fast
those are a little harder to make fast
though you can still get in the millions
though you can still get in the millions
of steps per second pretty easily
of steps per second pretty easily
um I'd have to think about
um I'd have to think about
that are there any old like arcade games
that are there any old like arcade games
that would be interesting to do for
that I was going to do Tetris I think
that I was going to do Tetris I think
that would be slightly harder though
that would be slightly harder though
and I don't know if that's a super
and I don't know if that's a super
useful n either for
RL anybody have ideas for like
RL anybody have ideas for like
relatively simple Sims that would be
relatively simple Sims that would be
useful for RL to be very fast
okay this is actually training now which
okay this is actually training now which
is
is
interesting so it doesn't train if you
interesting so it doesn't train if you
have too many M's that's
weird trying to think of some Sims that
weird trying to think of some Sims that
I would like to have people
I would like to have people
make open TTD bit simpler yeah that's
make open TTD bit simpler yeah that's
just a hard place to
just a hard place to
start I've been wanting to do an RTS
start I've been wanting to do an RTS
those are
hard scaling mazm
hard scaling mazm
maybe that's kind of hard to make
maybe that's kind of hard to make
efficient though because when you reset
efficient though because when you reset
it you have to generate a new maze
it you have to generate a new maze
unless you're going to keep the same
unless you're going to keep the same
maze over and over
and that requires a little bit of M uh
and that requires a little bit of M uh
generation work as
well I mean there's like this open Spiel
well I mean there's like this open Spiel
thing from Deep
thing from Deep
Mind where like these Ms are fast but
Mind where like these Ms are fast but
they're not fast when you paralyze them
they're not fast when you paralyze them
as
as
well some of these would be easy I don't
well some of these would be easy I don't
know how useful these would be cuz this
know how useful these would be cuz this
is kind kind of welldeveloped if a bit
is kind kind of welldeveloped if a bit
of a pain to work with I do like the
of a pain to work with I do like the
idea of having an RTS but that's
idea of having an RTS but that's
probably a much more
probably a much more
complicated project
complicated project
um Checkers and board games fall into
um Checkers and board games fall into
open Spiel yeah
open Spiel yeah
um man there's so many things I'd like
um man there's so many things I'd like
to
to
have in this but like practically
speaking the easiest thing to
speaking the easiest thing to
do actually here's a really good project
do actually here's a really good project
here's a really good project that if you
here's a really good project that if you
do it well I'd be happy to promote and
do it well I'd be happy to promote and
I'd be very happy to integrate into
I'd be very happy to integrate into
puffer lip um breakout is a very classic
puffer lip um breakout is a very classic
Atari game right it's used for like a
Atari game right it's used for like a
lot of it's like the cover for a lot of
lot of it's like the cover for a lot of
old RL research if you were to make
old RL research if you were to make
breakout at uh a million steps plus per
breakout at uh a million steps plus per
second then uh we got it training like
second then uh we got it training like
in 30 seconds or something I think that
in 30 seconds or something I think that
that would probably make people very
that would probably make people very
interested that was something I was
interested that was something I was
thinking about doing that's like pretty
thinking about doing that's like pretty
easy but I don't know if it's worth
easy but I don't know if it's worth
spending a few days on it um that would
spending a few days on it um that would
probably be a pretty good learner
probably be a pretty good learner
project assuming that you're like a
project assuming that you're like a
competent programmer may or may not have
competent programmer may or may not have
tons of RL experience but want to do
tons of RL experience but want to do
super like hyper
simd that would probably be my
simd that would probably be my
suggestion to do
and uh yeah if you look at the snake
and uh yeah if you look at the snake
stack as well like you don't render
stack as well like you don't render
breakout during training like you would
breakout during training like you would
just give it like the board would just
just give it like the board would just
be represented where like each like
be represented where like each like
pixel or whatever is a block and then
pixel or whatever is a block and then
your paddle is a certain like length or
your paddle is a certain like length or
whatever and then uh you would just
whatever and then uh you would just
render it at at test time you'd like
render it at at test time you'd like
make it playable it that wouldn't be
make it playable it that wouldn't be
that hard I would expect that that
that hard I would expect that that
project would
project would
be probably about the same length the
be probably about the same length the
snake that's probably a whole like you
snake that's probably a whole like you
can probably do that whole project maybe
can probably do that whole project maybe
a little bit more for the rendering
a little bit more for the rendering
that's probably a 500 line of code
that's probably a 500 line of code
project what are the current steps per
project what are the current steps per
second for breakout
second for breakout
um it's actually hard to say because
um it's actually hard to say because
we've got all these crappy python
we've got all these crappy python
rappers that make it slower I think it's
rappers that make it slower I think it's
actually like in the tens of thousands
actually like in the tens of thousands
or like high thousands maybe but then
or like high thousands maybe but then
also because it's rendered like you have
also because it's rendered like you have
just a whole bunch of data that has to
just a whole bunch of data that has to
go through the neural net um that slows
go through the neural net um that slows
it down whereas if you could get more
it down whereas if you could get more
efficient State based that would be more
efficient State based that would be more
useful like there's Ram based stuff but
useful like there's Ram based stuff but
nobody really trains from Ram because
nobody really trains from Ram because
that's kind of
weird but yeah if you're interested in
weird but yeah if you're interested in
doing stuff with puffer Li that's like
doing stuff with puffer Li that's like
the exact type of thing that I suggest
the exact type of thing that I suggest
to people like if you want to like get
to people like if you want to like get
your feet wet with uh development m in
your feet wet with uh development m in
this like and you want to contribute to
this like and you want to contribute to
this space like new M's for Puffer that
this space like new M's for Puffer that
are like you know Built Well stable High
are like you know Built Well stable High
perf that's a really awesome way to get
perf that's a really awesome way to get
involved with
it what tools should I learn to make
it what tools should I learn to make
such environments I just linked the the
such environments I just linked the the
snake project in puffer if you just look
snake project in puffer if you just look
in the dev branch of puffer puffer
in the dev branch of puffer puffer
environments ocean um the snake game is
environments ocean um the snake game is
an end to end example of making snake in
an end to end example of making snake in
450 lines of code that runs at over 10
450 lines of code that runs at over 10
million steps per second it's just scon
million steps per second it's just scon
plus python that's all it is and like
plus python that's all it is and like
you'll see the way that I use cython to
you'll see the way that I use cython to
mirror arrays back and forth between
mirror arrays back and forth between
Python and C
Python and C
um yeah it's a pretty simple stack it's
um yeah it's a pretty simple stack it's
like actually shockingly
simple okay so here you can see
simple okay so here you can see
different games that have been one at
different games that have been one at
looks
like the reward capping out here is a
like the reward capping out here is a
little
odd check out the snake
odd check out the snake
implementation helps I can reach out on
implementation helps I can reach out on
Twitter if I get any progress yeah of
Twitter if I get any progress yeah of
course you can DM me on Twitter you can
course you can DM me on Twitter you can
me you can add me in the puffer Discord
me you can add me in the puffer Discord
you can find me here I'm not hard to get
you can find me here I'm not hard to get
a hold
a hold
of 10 million steps per
of 10 million steps per
second I think it's a little more than
second I think it's a little more than
than that but
yeah 10 mil for something like snake is
yeah 10 mil for something like snake is
very achievable for something much more
very achievable for something much more
complex like the project I'm doing now
complex like the project I'm doing now
you can still get over a million and
you can still get over a million and
like I think you could get AAA level
like I think you could get AAA level
games uh still potentially running in
games uh still potentially running in
the hundreds of
thousands you just have to build for
thousands you just have to build for
it
2.89 weird how this gets more stuck
2.89 weird how this gets more stuck
isn't
it
0.24 why does this get stuck
oh you have a very different h
very
finicky check out
finicky check out
those
those
this
second e
is puffer lib meant to be
is puffer lib meant to be
cloned like clean RL or installed via
cloned like clean RL or installed via
pip so uh if you want to use our
pip so uh if you want to use our
training utils they're based on clean RL
training utils they're based on clean RL
so you clone it if you just want to use
so you clone it if you just want to use
the environments out of the box then
the environments out of the box then
it's a pip install with the
it's a pip install with the
understanding that like all the new Sims
understanding that like all the new Sims
that I've been building are still in Dev
that I've been building are still in Dev
branches so like those aren't in the pit
branches so like those aren't in the pit
package yet right but like all the
package yet right but like all the
existing bindings and stuff are
existing bindings and stuff are
accessible via
pip so it's a little bit of
both I want there to be a stable package
both I want there to be a stable package
that you can use for like our core
that you can use for like our core
vectorization bindings functionality
vectorization bindings functionality
right and then uh for training it's it's
right and then uh for training it's it's
got to be
got to be
uh a it's got to be a clone because we
uh a it's got to be a clone because we
base it off of cleanar Al so you know
base it off of cleanar Al so you know
that stuff is in user
that stuff is in user
space and really like the idea behind
space and really like the idea behind
puffer lib is if you're trying to do
puffer lib is if you're trying to do
advanced stuff with it it's intended to
advanced stuff with it it's intended to
be white box software so like the goal
be white box software so like the goal
is that you should be able to read
is that you should be able to read
through any bit of puffer lib that you
through any bit of puffer lib that you
want to like look at extend extend or do
want to like look at extend extend or do
stuff with and your eyes should not
stuff with and your eyes should not
bleed
interesting how this
works four update Epoch did I just like
works four update Epoch did I just like
find the YOLO config somehow that just
find the YOLO config somehow that just
works and nothing else does it's a
works and nothing else does it's a
really ridiculous amount of data
really ridiculous amount of data
reuse so when I want to run a custom EnV
reuse so when I want to run a custom EnV
I should clone um it depends right if
I should clone um it depends right if
you want to train a custom end with
you want to train a custom end with
puffer Libs training stuff then yes if
puffer Libs training stuff then yes if
you want to just use a custom end with
you want to just use a custom end with
puffer Libs emulation layers and
puffer Libs emulation layers and
vectorization then you can do that via
vectorization then you can do that via
pip because you just use like puffer li.
pip because you just use like puffer li.
emulation gymnasium puffer EnV of your
emulation gymnasium puffer EnV of your
environment and that gives you uh a
environment and that gives you uh a
puffer Li compatible one and then you
puffer Li compatible one and then you
can use like puffer lib vectorization
can use like puffer lib vectorization
multiprocessing and then you have your
multiprocessing and then you have your
simulated your hyper simulated
simulated your hyper simulated
environment just like
that what does it mean when when an
that what does it mean when when an
array has an
array has an
in okay it's not inhomogeneous it's
in okay it's not inhomogeneous it's
heterogeneous um and what that means
heterogeneous um and what that means
well first of all the thing that you ask
well first of all the thing that you ask
doesn't make sense I assume that you
doesn't make sense I assume that you
mean
mean
nonhomogeneous uh or heterogeneous
nonhomogeneous uh or heterogeneous
observation and action spaces and that
observation and action spaces and that
means when different agents have
means when different agents have
different shaped observations and
different shaped observations and
different shaped actions that is
different shaped actions that is
something that is fundamentally very uh
something that is fundamentally very uh
obnoxious to support because you get
obnoxious to support because you get
variable sized arrays
variable sized arrays
everywhere and uh yeah I'm I don't want
everywhere and uh yeah I'm I don't want
to have to deal with that if
possible
setting okay so you're just I don't know
setting okay so you're just I don't know
where you're getting that from linky
horrible
OBS I
OBS I
mean that stuff is part of puffer's
mean that stuff is part of puffer's
Advanced API for a reason
Advanced API for a reason
right it's because it's not
right it's because it's not
easy
um dire X dire y radiant
x what is this thing actually doing if I
x what is this thing actually doing if I
look at it
okay this is actually fairly reasonable
this one being stuck is
weird and then not figuring their way
weird and then not figuring their way
around
the main thing was
entropy very
weird well this definitely appears to be
weird well this definitely appears to be
working
professional bottom left
oh look he went this way for once that's
funny
okay I
okay I
memory something weird happened with
this okay so these things do train
this okay so these things do train
now it's just some weird Quirk with the
now it's just some weird Quirk with the
uh the render Library don't worry about
uh the render Library don't worry about
it I'll find it at some point before
it I'll find it at some point before
this is launched the leak is not in the
this is launched the leak is not in the
simulation I checked and it's impossible
simulation I checked and it's impossible
for the leak to be in the simulation
anyways can you layer
anyways can you layer
games linky I have 1600 games going on
games linky I have 1600 games going on
at the same time not three
though actually oddly enough it doesn't
though actually oddly enough it doesn't
work well with 1600 games it works
work well with 1600 games it works
better with um what's
this I think 80
this I think 80
games yeah it works better when I have
games yeah it works better when I have
like 80 games it's weird
there should be no reason for that as
well so I'm going to try that
well so I'm going to try that
next after
this we're not going to spend like all
this we're not going to spend like all
day screwing with uh just the really
day screwing with uh just the really
really basic version um
really basic version um
the plan is to finish this do a little
the plan is to finish this do a little
bit of analysis fiddle with one or two
bit of analysis fiddle with one or two
more things and then we're going to work
more things and then we're going to work
on like
on like
proper observations and stuff for this
proper observations and stuff for this
game and then like code improvements
game and then like code improvements
balance like actual moving forward on
balance like actual moving forward on
the project and then like the rest of
the project and then like the rest of
the experiments will basically just be
the experiments will basically just be
done via
done via
sweeps you just you need to get the
sweeps you just you need to get the
environment to a point where uh where
environment to a point where uh where
you are mostly getting something
you are mostly getting something
reasonable
reasonable
as a starting point and then from there
as a starting point and then from there
you can automate everything
you can automate everything
else getting the reasonable starting
else getting the reasonable starting
point is just sometimes very
obnoxious
e
e e
who it does actually look like the
who it does actually look like the
model's getting a little bit better over
model's getting a little bit better over
time maybe let me
[Music]
[Music]
see a little bit Improvement right there
see a little bit Improvement right there
that's
that's
weird yeah definitely you can see it
weird yeah definitely you can see it
spiking
spiking
so we'll finish this we'll pull this
so we'll finish this we'll pull this
model we'll see what it looks like we
model we'll see what it looks like we
will uh fiddle with a few
parameters I'm only logging the stats
parameters I'm only logging the stats
these stats except for the reward the
these stats except for the reward the
reward is everything but these stats are
reward is everything but these stats are
only from the first game so these like
only from the first game so these like
spikes here whenever it finishes a game
spikes here whenever it finishes a game
so it looks like each separate
so it looks like each separate
environment is only playing like
environment is only playing like
whatever this is 10 to 20 games
we
crashing no we're
good there's definitely some weird weird
good there's definitely some weird weird
Dynamics going on here ARL still got
Dynamics going on here ARL still got
some demons I haven't purged all of them
some demons I haven't purged all of them
yet it's what the exorcism is
for the other thing I wanted to do was
for the other thing I wanted to do was
just train with more Ms right yeah
and from here
um be a lot of stuff to figure
out
out
puffer does Google use RL does Google RL
puffer does Google use RL does Google RL
at what point they use RL
at what point they use RL
um yeah pretty much a lot a lot of the
um yeah pretty much a lot a lot of the
big tech companies use RL at least for
big tech companies use RL at least for
some
stuff a lot of it's R lhf these days
stuff a lot of it's R lhf these days
which is
which is
like I don't know baby mode RL for
like I don't know baby mode RL for
noobs no seriously it's it's a hard but
noobs no seriously it's it's a hard but
different problem that has different
different problem that has different
characteristics that doesn't really make
characteristics that doesn't really make
it RL in the traditional sense um
it RL in the traditional sense um
but yeah it's been used for a bunch of
but yeah it's been used for a bunch of
applications at different companies
applications at different companies
obviously nowhere near as much as
obviously nowhere near as much as
language models but RL has not had the
language models but RL has not had the
same level investment in maturity
yet Deep Mind part of Google yep Deep
yet Deep Mind part of Google yep Deep
Mind is uses a bunch I'm going to be
Mind is uses a bunch I'm going to be
right back while this job finishes up um
right back while this job finishes up um
there's a restroom be right back and
there's a restroom be right back and
then I mean at least we have something
then I mean at least we have something
working somewhat now
working somewhat now
right once you have something at least
right once you have something at least
you know is training and you know isn't
you know is training and you know isn't
just like horribly
just like horribly
bugged it's pretty easy to improve from
bugged it's pretty easy to improve from
here so I'll be right
back
e
e
e
e
e e
this finished
I don't think this is necessarily any
I don't think this is necessarily any
better than the other
models actually his entropy full
models actually his entropy full
crashed let me
see well to be fair if entropy is full
see well to be fair if entropy is full
crashed it doesn't matter because we
crashed it doesn't matter because we
literally have no entropy
bonus no it's not full crashed
pretty reasonable
pretty reasonable
model it actually does occasionally path
model it actually does occasionally path
around walls sometimes it gets stuck but
around walls sometimes it gets stuck but
usually passed around the
usually passed around the
walls sends it down
walls sends it down
mid pretty reasonable
mid pretty reasonable
overall doesn't really stop to fight
overall doesn't really stop to fight
because we turn the XP rewards off
because we turn the XP rewards off
sure but overall I'm pretty happy with
sure but overall I'm pretty happy with
that I've even seen them go down a
that I've even seen them go down a
different Lane once or twice
different Lane once or twice
yeah like that one right there on the
yeah like that one right there on the
left went down the Top Lane and then
left went down the Top Lane and then
went into the jungle pretty
went into the jungle pretty
interesting okay so the only other thing
interesting okay so the only other thing
I really wanted to do was um I wanted to
I really wanted to do was um I wanted to
see
see
if it was really the case that
if it was really the case that
um where did this
um where did this
go I wanted to see if it was really the
go I wanted to see if it was really the
case this this just doesn't work with
case this this just doesn't work with
more environments cuz there really
more environments cuz there really
should no be there really should not be
should no be there really should not be
a reason that it wouldn't so this is
a reason that it wouldn't so this is
going to be now 200 M I think right uh
going to be now 200 M I think right uh
config
config
mooba let me just back to the
mooba let me just back to the
envelope MOA so we've got eight M's
envelope MOA so we've got eight M's
we've got four per
we've got four per
batch four per batch means that we need
batch four per batch means that we need
200 M's each yes because that gets
200 M's each yes because that gets
you 800 yeah that gives you your 8K mini
you 800 yeah that gives you your 8K mini
batch you have 8K times
batch you have 8K times
128 that should give you your BPT
128 that should give you your BPT
Horizon of
Horizon of
16 it's probably very low for
16 it's probably very low for
generalized Advantage estimation um that
generalized Advantage estimation um that
could be an
issue yeah but let's try this for now
issue yeah but let's try this for now
why not right
okay so regardless of this uh we need to
okay so regardless of this uh we need to
start on the rest of this project now
start on the rest of this project now
before I get bored and tired of
before I get bored and tired of
this so let's see
um we definitely need to add some stuff
um we definitely need to add some stuff
into the observations
into the observations
right right now you can
right right now you can
observe you can essentially observe the
observe you can essentially observe the
nearby entities what team they're on and
nearby entities what team they're on and
what type they are you can't observe
what type they are you can't observe
Health
Health
Mana anything like that
Ley did lots of work today so s good
Ley did lots of work today so s good
night thanks for stopping
night thanks for stopping
by I'm going just keep hacking on this
by I'm going just keep hacking on this
and trying to make uh add some stuff to
and trying to make uh add some stuff to
the observations so that the models can
the observations so that the models can
be better mess with some rewards stuff
be better mess with some rewards stuff
like that
Chief puffing
officer see them out
officer see them out
[Music]
[Music]
uh what do I do
here do you want to be able to observe X
here do you want to be able to observe X
and
Y not
Y not
really there's not that that much stuff
really there's not that that much stuff
you need to be able to observe
you need to be able to observe
right what did they do for
right what did they do for
DOTA model architect should
DOTA model architect should
have CU they did a whole bunch of stuff
have CU they did a whole bunch of stuff
but I don't think a lot of it
but I don't think a lot of it
is crazy
relevant you need to know something
relevant you need to know something
about your skills
right
right
so Heroes only
okay so they've got like modifiers
okay so they've got like modifiers
abilities and
abilities and
[Music]
[Music]
items yeah
that would be nice to have
right and then they have unit
type whether it's under
type whether it's under
attack stats
attack stats
Health
Health
distance they have an
distance they have an
orientation absolute position and
orientation absolute position and
animation
animation
that's because they don't do a
CNN interesting that they went with this
CNN interesting that they went with this
I know
I know
why it's very hard to do something like
why it's very hard to do something like
this we're going to go a little simpler
this we're going to go a little simpler
than this for
than this for
sure um I think we can
sure um I think we can
probably the one thing that's a little
probably the one thing that's a little
bit obnoxious here is we have
bit obnoxious here is we have
to we have have to like normalize stuff
to we have have to like normalize stuff
to fit in one
to fit in one
bite the way I have this at the
bite the way I have this at the
moment
so
yeah how many of these things are
yeah how many of these things are
discreet
you definitely need to
observe
observe
Health
Health
Mana damage
modifiers
modifiers
maybe
timers you only get to see your own cool
timers you only get to see your own cool
your own cooldown
timers this is not that
timers this is not that
bad how's this experiment
bad how's this experiment
going not well that's funny
going not well that's funny
that's very
that's very
weird
weird
yeah potentially there's still some
yeah potentially there's still some
issue
then though I think uh I have some
then though I think uh I have some
better ideas as to this okay we're going
better ideas as to this okay we're going
to actually wake up a little bit here
to actually wake up a little bit here
and do
and do
um do some proper Dev on this um so for
um do some proper Dev on this um so for
observations we're going to have to
observations we're going to have to
change the way that observ work a little
change the way that observ work a little
bit observations
map observations
map
map
yeah observations map observations
extra right the way up observation Map
extra right the way up observation Map
works at the
works at the
moment is that each player
gets each player gets
gets each player gets
a 2d
grid now we need to give each player a
grid now we need to give each player a
3D
grid I have to be careful not to blow up
grid I have to be careful not to blow up
the data here too much though
right
right
o That's potentially rough
o That's potentially rough
not blowing up the amount of
not blowing up the amount of
data because right now it's
data because right now it's
150ish 200
bytes we got some continuous values
I can't add that many channels to
I can't add that many channels to
this so they're going to be very
this so they're going to be very
expensive in terms of
memory it's 11 by 11 right now it's 121
if I'm really selective with these
right you do need to see their class
right you do need to see their class
their class
right so let's
right so let's
say let's say we do classes like uh we
say let's say we do classes like uh we
roll that into type or
whatever that's just one
whatever that's just one
bite we'll discretize
Health do we need to discretize
Health do we need to discretize
Mana probably should be able to see the
Mana probably should be able to see the
opponent
Mana say we discretize Health Mana
Mana say we discretize Health Mana
damage do you need to be able to see
damage do you need to be able to see
damage not
damage not
really you know how when you're getting
really you know how when you're getting
hit if you know their health you know
hit if you know their health you know
roughly how strong they
roughly how strong they
are do we need
are do we need
level do the enemy
level again not really you should know
level again not really you should know
based on well no because if you don't
based on well no because if you don't
know Max Health right if you don't know
know Max Health right if you don't know
their Max Health if you just know their
their Max Health if you just know their
current
current
health I think we do Health Mana level
what modifiers
not
really maybe you want to know remaining
really maybe you want to know remaining
stun duration you can't even see that in
stun duration you can't even see that in
Dota can
you you just know how long your stuns
you you just know how long your stuns
last don't you there's there isn't like
last don't you there's there isn't like
a is there a cooldown timer on
them I thought you had to keep track
them I thought you had to keep track
yeah you
yeah you
can is that the case in Dota and League
can is that the case in Dota and League
that you can see remaining
stun it's a white bar okay so I can add
stun it's a white bar okay so I can add
them I just don't know if it's worth
them I just don't know if it's worth
it so let me make this clear here
it so let me make this clear here
um everything that we add
um everything that we add
oh it's a newer feature okay I don't
oh it's a newer feature okay I don't
think we need that then so here's the
think we need that then so here's the
here's the thing that we are we got to
here's the thing that we are we got to
deal with right this project is
deal with right this project is
complicated enough as is I have really
complicated enough as is I have really
fancy encoder Tech I can use to compress
fancy encoder Tech I can use to compress
observations uh I'd rather not have to
observations uh I'd rather not have to
use it here this project has enough
use it here this project has enough
other stuff going on so naively and I'm
other stuff going on so naively and I'm
going to do a little bit of compression
going to do a little bit of compression
already so naively every new feature
already so naively every new feature
that we want to add adds 21 bytes to the
that we want to add adds 21 bytes to the
observation size so right now that you
observation size so right now that you
can assume that the observation is going
can assume that the observation is going
to be between 150 to 200 bytes at the
to be between 150 to 200 bytes at the
moment and every additional feature we
moment and every additional feature we
add adds an additional 121 bytes this is
add adds an additional 121 bytes this is
not going to be a problem for the neural
not going to be a problem for the neural
network but it is potentially going to
network but it is potentially going to
add bandd like data transfer bandwidth
add bandd like data transfer bandwidth
that can slow stuff down so we really
that can slow stuff down so we really
want to limit this to like what do you
want to limit this to like what do you
need to play the game
need to play the game
you definitely need to be able to
you definitely need to be able to
see what type of uh what type of agent
see what type of uh what type of agent
everything is you need to be able to see
everything is you need to be able to see
like the class type and like the minion
like the class type and like the minion
type the minion team that sort of a
type the minion team that sort of a
thing I can do that that's
thing I can do that that's
fine um you need to be able to
see
see
health and
Mana I'm going to emit Max health and
Mana I'm going to emit Max health and
Max Mana as
Max Mana as
unnecessary need to be able to see the
unnecessary need to be able to see the
opponent's
level and probably that's about
level and probably that's about
it so we had four features or was that
it so we had four features or was that
do I say so
do I say so
Health
Health
[Music]
[Music]
Mana I said Health mana and level right
Mana I said Health mana and level right
so that's a to plus the uh the agent
so that's a to plus the uh the agent
type is four
type is four
features that's a reasonable size
features that's a reasonable size
observation that's a relatively small
observation that's a relatively small
amount of information for me to have to
amount of information for me to have to
deal
deal
with that gives
with that gives
you a lot of data
actually shouldn't be too difficult to
actually shouldn't be too difficult to
deal with in the neuronet
deal with in the neuronet
side and then for yourself you'll get a
side and then for yourself you'll get a
lot more information about
yourself
yeah how's this experiment
going well it doesn't
matter e
let going to go get something with a
let going to go get something with a
little bit of caffeine in it I'll be
little bit of caffeine in it I'll be
back in uh I'm going to be back in like
back in uh I'm going to be back in like
two minutes I'm going to just wake
two minutes I'm going to just wake
myself up because we're going to do um
myself up because we're going to do um
we're going to right now what we're
we're going to right now what we're
going to do is we're going to get the
going to do is we're going to get the
observations that we actually intend to
observations that we actually intend to
use for the game um that should enable
use for the game um that should enable
the models to do way better and then
the models to do way better and then
it'll just be a matter of like reward
it'll just be a matter of like reward
game balance and that sort of a stuff
game balance and that sort of a stuff
and cleaning up all the game code so
and cleaning up all the game code so
this actually will have us in a pretty
this actually will have us in a pretty
good spot if we just do this today be
good spot if we just do this today be
right
back
e
e
e
e
e e
okay let's lock
in for
we update this to be four dimensional
first obnoxiously we have to zero this
first obnoxiously we have to zero this
whole freaking thing don't we
observations extra we're going to leave
observations extra we're going to leave
this here for
now we go
now we go
through these
through these
nearby agents
which I think we can literally do a scan
which I think we can literally do a scan
on
right regardless
right regardless
though Target is get
though Target is get
entity Target OB we don't need
this we do need DX and and Dy here
this we do need DX and and Dy here
actually I remember why I needed those
actually I remember why I needed those
now so we'll do int
now so we'll do int
DX and
Dy
o kind of gross the way this works
R is going to be equal
R is going to be equal
to y
to y
+ Dy right
and then we need to
know Target PID is going to be PID map
know Target PID is going to be PID map
at this
at this
location we actually know R is going to
location we actually know R is going to
be a little different from this it's
be a little different from this it's
going to be plus
going to be plus
Dy it's just going to be
Dy it's just going to be
Dy plus self. Vision range I believe
Dy plus self. Vision range I believe
right because we want Ard in X hi again
right because we want Ard in X hi again
Kawa we're doing features we're doing
Kawa we're doing features we're doing
proper features for uh DOTA now so we're
proper features for uh DOTA now so we're
expanding the observation map to include
expanding the observation map to include
additional
data okay so now we have the location we
data okay so now we have the location we
have the local
have the local
location in the observations where we
location in the observations where we
want to add this thing let's do get the
want to add this thing let's do get the
target uh we can filter here
don't need to do this processing for
don't need to do this processing for
non-existent
non-existent
targets and then what we do
is self. observations map of
PID row
PID row
call one
call one
is going to be equal to
is going to be equal to
Target DOT
Target DOT
health and
health and
then
mana and then we want
mana and then we want
level you can see their health their
level you can see their health their
Mana their
level rest of this is
level rest of this is
garbage we break
we do not need this idx garbage
anymore we do not need to sort by
distance we will find out an alternative
distance we will find out an alternative
targeting mechanism I'm
targeting mechanism I'm
sure this is quite
sure this is quite
clean observations extra is still fine
clean observations extra is still fine
at the
at the
moment I would like to
moment I would like to
modify a few small things here
potentially but I think we'll start with
potentially but I think we'll start with
this yeah let's start with
this we have to modify the network
this we have to modify the network
architecture just a little bit as
well this is now 8 plus three channels
right so we do map
right so we do map
features is it going to be the
features is it going to be the
observations or CNN what is
it why aren't you using an IDE I got
it why aren't you using an IDE I got
neovim what do I need an ID
for it's not like I've never used an ID
we only want the first channel of
this
this
yes so now we have map features extra
yes so now we have map features extra
map
map
features so oops
features so oops
CNN
CNN
features we're going to concatenate
features we're going to concatenate
these and then we're going to pass these
these and then we're going to pass these
through the
through the
CNN so now we have something reasonable
here would you say that clean code isn't
here would you say that clean code isn't
always if by Clean code you mean the
always if by Clean code you mean the
clean code like the book that's an anti-
clean code like the book that's an anti-
guide that's like how to develop
guide that's like how to develop
terrible
terrible
software for
cannot reshape array
I really should stop screwing around
I really should stop screwing around
with these like hardcoded in four places
with these like hardcoded in four places
array shapes we're just going to do OBS
array shapes we're just going to do OBS
map byes is going to be equal
map byes is going to be equal
to what did we say OB
size it's also a hyper a parameter okay
size it's also a hyper a parameter okay
we'll just do self. map B
we'll just do self. map B
OBS map byes equals self. OB size time
OBS map byes equals self. OB size time
OB size Time
4 and then wherever I'm using this I can
4 and then wherever I'm using this I can
do self.
OBS
OBS
map map
bites for
what I'm doing here is I'm adjusting the
what I'm doing here is I'm adjusting the
observation
spaces to accommodate the additional
spaces to accommodate the additional
data we're now going to have 484 bytes
data we're now going to have 484 bytes
worth of observation data per agent from
worth of observation data per agent from
the local map that tells you what agents
the local map that tells you what agents
are nearby uh and now we've added to
are nearby uh and now we've added to
that that we have the health of nearby
that that we have the health of nearby
agents for Heroes you get to see their
agents for Heroes you get to see their
mana and level as well so those are
mana and level as well so those are
quite expensive in terms of memory with
quite expensive in terms of memory with
this represent
this represent
which we're going to have to see how bad
which we're going to have to see how bad
that is but the benefit of representing
that is but the benefit of representing
it this way is we don't have to do any
it this way is we don't have to do any
decoding or additional stuff in the uh
decoding or additional stuff in the uh
the forward pass of the neural network
the forward pass of the neural network
it's very very simple to use the data
it's very very simple to use the data
from there and we don't have to offload
from there and we don't have to offload
any processing over there so I think
any processing over there so I think
that this should be should be good um
that this should be should be good um
there maybe some Oddities we'll see
dot observations
dot observations
shape 487 which is perfect
right cannot
reshape into
reshape into
shape
right or
we good here
we good here
yes okay now we get buffer
yes okay now we get buffer
has wrong number of
has wrong number of
Dimensions expected
Dimensions expected
three got
four that's just uh we didn't recompile
four that's just uh we didn't recompile
our code no big deal
right I forgot some data type
Shenanigans we need to do um
Shenanigans we need to do um
Target mana and stuff now don't
Target mana and stuff now don't
we do we want to do this as a
percentage we probably want to do this
percentage we probably want to do this
as like a percentage
right hard to
right hard to
say how do I want to encode health and
say how do I want to encode health and
Mana
I think as a percentage will be cleaner
I think as a percentage will be cleaner
and then they'll just have to learn that
and then they'll just have to learn that
like hey if this thing has a giant level
like hey if this thing has a giant level
advantage on you you're probably going
advantage on you you're probably going
to get
merked we do FL
merked we do FL
flat target.
health is Health a float or an INT oh
health is Health a float or an INT oh
it's already a float so we're actually
it's already a float so we're actually
we're we're
we're we're
chilling so we do Target health over Max
chilling so we do Target health over Max
health and we just do 255
health and we just do 255
times
um 255 times this then we'll just cast
um 255 times this then we'll just cast
to unsigned
Char so now what we get is we get a
Char so now what we get is we get a
value from 0 to 255 that tells you the
value from 0 to 255 that tells you the
percentage of the health and the reason
percentage of the health and the reason
that we're doing this is that we're
that we're doing this is that we're
trying to uh pack this value into one
trying to uh pack this value into one
bite
bite
so we can do the same thing
so we can do the same thing
here and then we can
here and then we can
do the same thing
do the same thing
here uh unsign Char target. level
here uh unsign Char target. level
divided by 30.0 which is the max
divided by 30.0 which is the max
level
perfect and connect
perfect and connect
cast type double to unsign char that's
cast type double to unsign char that's
funny I forgot that you have to do
funny I forgot that you have to do
f are you not allowed to do F like
this
30f no we say a floating
30f no we say a floating
scon sure it was just
F it's kind of weird
this
this
work cannot assign type
work cannot assign type
double uh and how would this possibly
be assign CH
type oh is it just
type oh is it just
this I think I think I just needed this
this I think I think I just needed this
set of parens here
set of parens here
right yeah and now we have cannot assign
right yeah and now we have cannot assign
float I forgot that this is like it
float I forgot that this is like it
prefix it binds tightly I
prefix it binds tightly I
forgot I haven't done sea style cast in
forgot I haven't done sea style cast in
a
while makes sense
okay this
okay this
compiles what happens when we run a
train differing
train differing
extents in copy
contents 4 and 11
this needs to be
oops we need to add this as
well
rebuild we're going to have to redo some
rebuild we're going to have to redo some
optimization after this but that's fine
optimization after this but that's fine
so much stuff to
fix view size is not
compatible
yeah oh well I think I just did this
yeah I just did this
yeah I just did this
wrong
wrong
right W minus1
right W minus1
111 yeah I just did it
wrong this is now- 1114 I believe
okay we get a
okay we get a
Cuda cuda's mad
right before the one
hot because you didn't uh
hot because you didn't uh
slice it's
okay we got the one
okay we got the one
Hots we got our
Hots we got our
extras must must match exactly P CNN
extras must must match exactly P CNN
features
features
shape 012 this now has to be
shape 012 this now has to be
concatenated along
concatenated along
Dimension this is dimension
three and I think that fixes
it apparently
it apparently
not well I might be missing a per
right extra map
right extra map
features this is totally wrong colon
features this is totally wrong colon
colon colon
negative
star
star
yeah um but then this there's a channels
yeah um but then this there's a channels
perut here as
well just tensor
Shenanigans I'm be sure to have this
Shenanigans I'm be sure to have this
nicely cleaned up for the uh the release
nicely cleaned up for the uh the release
of it too many indices for tensor of
of it too many indices for tensor of
Dimension
two yeah so this is actually got to be
two yeah so this is actually got to be
this is CNN
this is CNN
features colon colon
features colon colon
colon
colon
three star.
float okay so now we have our extra map
float okay so now we have our extra map
features and our map features.
shape and we still need to permute this
shape and we still need to permute this
it looks like because we do channels
it looks like because we do channels
first so this is
first so this is
actually
prute which means that we're stacking
prute which means that we're stacking
along the First
along the First
Dimension let's see if this gives us our
Dimension let's see if this gives us our
features we're looking for 11 channels
features we're looking for 11 channels
worth of features I
believe must match in all Dimensions
believe must match in all Dimensions
okay apparently
not this looks good to
me map features
me map features
CNN
features oh
wait so we want to concat map features
wait so we want to concat map features
and extra map
and extra map
features so I had it right I just had
features so I had it right I just had
the wrong
names okay perfect
names okay perfect
CNN so this is now a very good
CNN so this is now a very good
representation for uh our
representation for uh our
Network we have 11 channels of 11 by 11
Network we have 11 channels of 11 by 11
there's nothing special about having it
there's nothing special about having it
be a cube like that it just happened to
be a cube like that it just happened to
work out that way
work out that way
um but now how much data is that
um but now how much data is that
actually 800
times 96
times 96
KB 96 kb per batch not
bad and this seems to now this seems to
run yeah cool our network runs
run yeah cool our network runs
now and uh I'm not going to start
now and uh I'm not going to start
training just yet what we're going to do
training just yet what we're going to do
is we're going to clean this up and
is we're going to clean this up and
allocate um some additional extra
allocate um some additional extra
variables so the architecture for this
variables so the architecture for this
at the moment is that you have some
at the moment is that you have some
number of things that you can observe
number of things that you can observe
about all of the agents around you you
about all of the agents around you you
can observe Everybody's Health
can observe Everybody's Health
everybody's mana and everybody's level
everybody's mana and everybody's level
but then you of course you have some
but then you of course you have some
extra information like your cool Downs
extra information like your cool Downs
aren't visible to everybody and stuff so
aren't visible to everybody and stuff so
we're going to add those into an extra
we're going to add those into an extra
Vector that you just append on at the
Vector that you just append on at the
end
we're going to have to change the shapes
we're going to have to change the shapes
as well we might as well just do
that I don't know
that I don't know
10 we'll see how many we end up
using
so yeah this is your uh your shape here
so yeah this is your uh your shape here
it's a little bit obnoxious that they
it's a little bit obnoxious that they
have to
have to
be
be
U like this but it's fine we bite
pack where else do I use this I have it
pack where else do I use this I have it
in the observation space should be in
in the observation space should be in
the buffer right yeah right
here okay and then there are a couple
here okay and then there are a couple
other places where I have this thing
other places where I have this thing
hardcoded so we'll just swap that out
hardcoded so we'll just swap that out
now I think underscore Phil yeah this is
now I think underscore Phil yeah this is
a dead method
a dead method
right cuz we have this in C which is
right cuz we have this in C which is
better better faster
stronger
stronger
yeah do I have threes anywhere
else according to this I
don't right here yeah there was one
more cool so now we get to allocate
more cool so now we get to allocate
these we'll have to pick which variables
these we'll have to pick which variables
we want to
include I think that for yourself you
include I think that for yourself you
get to see your pretty much most of
get to see your pretty much most of
these
we'll find a better way to copy them
we'll find a better way to copy them
later I'm sure but to start
with these are right now they're just
with these are right now they're just
added right
added right
here
here
so we're going to add in your
so we're going to add in your
X your
X your
y your reward which this is wrong at the
y your reward which this is wrong at the
moment
moment
um o how do we do
um o how do we do
this
this
27 255
values if we limit rewards to negative
values if we limit rewards to negative
-1 to one hold
-1 to one hold
on let's
on let's
say -1
say -1
time 128 Plus
can't do plus
can't do plus
256 wait negative 1 needs to map to Z so
256 wait negative 1 needs to map to Z so
it should be plus
it should be plus
obviously then 1 *
obviously then 1 *
128 this is 256 which is too high
this is just an asymmetric problem right
this is just an asymmetric problem right
there're
there're
values so you can't have 256 values and
values so you can't have 256 values and
distribute those evenly
I would like to scale this
perfectly kind of important
this gives you negative one oh no gives
this gives you negative one oh no gives
you
you
one that's perfect
right you never get zero
but this is
but this is
fine so we just do
fine so we just do
127 time rewards plus 128
X and Y are also
X and Y are also
fine I think you can even multiply these
fine I think you can even multiply these
by
by
two two times because they're 0 to
128 and they'll never actually be 128 so
128 and they'll never actually be 128 so
you're safe here
so now we're going to do uh
so now we're going to do uh
health and uh health and everything
right level's fine actually
here uh we don't really need XP
we want to see Max
health I think that we're fine just
health I think that we're fine just
doing it this
way we give you
way we give you
Mana we've already done
level we can do
damage plus six times player
damage plus six times player
level 30 is 180 so your damage is never
level 30 is 180 so your damage is never
over 255 in the current
over 255 in the current
format we'll make sure that we add
format we'll make sure that we add
bounds
checks so unsign car Char player damage
checks so unsign car Char player damage
it's approximately in the correct bound
move
speed move mod mod ifier as
well we do the stun timer
is that
is that
everything is hit
everything is hit
Level attack cool
Level attack cool
down basic attack cool down which is
down basic attack cool down which is
like going to be 50 times
like going to be 50 times
this basic
this basic
attack timer is also 50 times
attack timer is also 50 times
this qwe timers these are
this qwe timers these are
fine okay I think these are like
fine okay I think these are like
reasonable to to start with and um
reasonable to to start with and um
potentially we will do auto scaling
potentially we will do auto scaling
instead of manual scaling here but the
instead of manual scaling here but the
thing is
thing is
like because we're
like because we're
discretizing
discretizing
we yeah because we're discretizing here
we yeah because we're discretizing here
it kind of makes sense to scale it
it kind of makes sense to scale it
myself because some of these values are
myself because some of these values are
floats though maybe not maybe I don't
floats though maybe not maybe I don't
have to we'll
have to we'll
see but this is all nice data
so we ended up with 17 different values
so we ended up with 17 different values
here so we'll set this to
17 and we
will edit the
will edit the
uh we'll edit the network as
uh we'll edit the network as
well to take 17
well to take 17
input
variables
variables
okay appears to work
this is
this is
minus
minus
17 of minus 3
okay so that was relatively
painless now in theory we have all of
painless now in theory we have all of
the data that we need for
the data that we need for
training in practice we're going to add
training in practice we're going to add
a few additional values I'm
sure um but this should make quite a big
sure um but this should make quite a big
difference
already now it's not going to make a big
already now it's not going to make a big
difference in um
difference in um
the current training task because it's
the current training task because it's
very
basic we also I think the next thing
basic we also I think the next thing
that we need to do is we need to make
that we need to do is we need to make
sure
sure
that um players can tell their hero
that um players can tell their hero
class that's a big
thing and that's also going to be
thing and that's also going to be
important for the
important for the
renderer
so let's do that next
we'll do int
um radiant reap it's
um radiant reap it's
three dire creep it's going to be
four neutral going be five
uh
radiant what order did I Define these
radiant what order did I Define these
things
in support assassin first tank
carry w
what I say
what I say
radiant support assassin first tank
radiant support assassin first tank
carry
okay and we forgot the tower as well the
okay and we forgot the tower as well the
tower needs to be int
um should I distinguish these two
or should it just be like
Tower probably
easier
easier
no the icons on the map can be the same
right oh wait we already have Tower we
right oh wait we already have Tower we
have got Tower
have got Tower
empty really it should be empty let's do
empty really it should be empty let's do
wall is one Tower is
wall is one Tower is
two creep three four neutral 6 7 8 9
dire what's up with the auto complete
today this actually fits perfectly into
today this actually fits perfectly into
values that's very nice to have so this
values that's very nice to have so this
fits
fits
perfectly minus the debug one which
perfectly minus the debug one which
we'll ignore for
now here are all the different creep
types this will allow us to do a bunch
types this will allow us to do a bunch
of stuff
we'll do support
we'll do support
zero we'll use these as
zero we'll use these as
well
well
type
type
class um can't do
class um can't do
class
class
uh hero
Co yeah let's do that
if player.
type weird how I have this
done player. type
is entity player
oh I
oh I
see yeah there's like a lot of different
see yeah there's like a lot of different
typing information we got to get right
typing information we got to get right
here so we've got int PID let's make
here so we've got int PID let's make
sure we get this right so this is going
sure we get this right so this is going
to
to
be entity
type
type
um hero
type grid
type red ID hero type
so this
is then
is then
team
type any type hero type grid ID team
type any type hero type grid ID team
this is fine
and we're going to have to change quite
and we're going to have to change quite
a bit of stuff from
a bit of stuff from
that I think it's probably best to just
that I think it's probably best to just
go through very
slowly nothing should be
slowly nothing should be
here we're going to have to change a lot
here we're going to have to change a lot
of stuff though probably in the move
of stuff though probably in the move
functions mostly but I don't want to
functions mostly but I don't want to
miss anything
so first of all in the observations
um does anything change
um does anything change
here not yet we're going to add a few
here not yet we're going to add a few
things to it
later spawn
later spawn
Tower this is now Tower at entity type
Tower this is now Tower at entity type
is going to be entity
is going to be entity
Tower tower.
Tower tower.
grid idea is going to be Tower
how much stuff do I want to give
how much stuff do I want to give
different IDs
too what stuff would I need to have
too what stuff would I need to have
different
different
Graphics that's kind of the
Graphics that's kind of the
test I think you only need the 16 grid
test I think you only need the 16 grid
idas I
defined so this is Tower this is
defined so this is Tower this is
fine it's got PID should be up
top yeah that's
top yeah that's
fine so now reset
fine so now reset
does player. PID player. entity type
does player. PID player. entity type
player. grid ID should
be player.
be player.
grid I oh I actually have to do this
grid I oh I actually have to do this
normally don't
normally don't
I player
I player
dot grid type has got to
dot grid type has got to
be
radiant that's a
radiant that's a
support it's not radiant player is
support it's not radiant player is
it now it's radiant
support I don't know why it can't
support I don't know why it can't
autocomplete this
an
an
assassin carry at Le at least it auto
assassin carry at Le at least it auto
completes off of
this dire support assassin burst tank
this dire support assassin burst tank
carry
perfect that takes care of
perfect that takes care of
that and
uh we'll probably adjust the values as
uh we'll probably adjust the values as
well for all of these in there
well for all of these in there
because we're going to have different
because we're going to have different
Max healths and stuff per
Max healths and stuff per
class we might even add different
class we might even add different
scaling per class probably
will that'll be
will that'll be
fun and I
fun and I
actually we're probably going to move
actually we're probably going to move
where this code is for now but um this
where this code is for now but um this
is not bad to have like on
is not bad to have like on
initialization to just have like all of
initialization to just have like all of
the hard-coded values for all the
the hard-coded values for all the
different types of things you're
different types of things you're
initializing
initializing
there's really not a bad way of doing
there's really not a bad way of doing
it
it
yeah I'm actually tempted even to
yeah I'm actually tempted even to
like I could go through and initialize
like I could go through and initialize
all the creeps like this even
all the creeps like this even
maybe that might not be a bad idea then
maybe that might not be a bad idea then
when you reset creeps you literally just
when you reset creeps you literally just
move
them we'll start like this though
team
team
okay I think we're jamming this
now hard code the lanes
this is just for the scripted AIS but
this is just for the scripted AIS but
we're going to do
we're going to do
um what did we do zero and one is
um what did we do zero and one is
two so we'll
two so we'll
do uh player.
do uh player.
lane
two these are all
two these are all
twos and then
twos and then
is
is
four this gets a
one and three is a zero
perfect layer Lane is 3
39 I don't know why it can't copy data
like Seven's got to be a
like Seven's got to be a
four and then eight should be
five so these Lane assignments don't
five so these Lane assignments don't
actually do anything when we're not
actually do anything when we're not
using the scripted model but uh very
using the scripted model but uh very
nice for the scripted
nice for the scripted
model okay that's actually a lot nicer
model okay that's actually a lot nicer
I'm way happier with that
I'm way happier with that
now so we're actually kind of cleaning
now so we're actually kind of cleaning
stuff up in the process of doing this
huh and all this agent type stuff we
huh and all this agent type stuff we
literally no longer have to care about
literally no longer have to care about
because it's literally now just um
because it's literally now just um
player. grid
ID this gets deleted so this logic that
ID this gets deleted so this logic that
runs every single time anything moves
runs every single time anything moves
anywhere no longer needs to
anywhere no longer needs to
happen and uh now the move to function
happen and uh now the move to function
is actually very simple it's just a
is actually very simple it's just a
sanity check move on grid and then move
sanity check move on grid and then move
on the PID map
um
I don't see anything that needs to be
I don't see anything that needs to be
updated here the spawn needs to be
updated here the spawn needs to be
updated entity type is entity neutral
updated entity type is entity neutral
right
right
um Rd ID gets
neutral and that is
neutral and that is
fine then it gets to be team two
which is not anybody's
team I don't see any logic to update
team I don't see any logic to update
there there will be a little bit to
there there will be a little bit to
update and spawn
update and spawn
creep entity type is entity
creep we'll do
creep we'll do
creep grid
creep grid
idea is going to
be.
be.
team.
team and this goes
away respawn
away respawn
player doesn't touch any of those
player doesn't touch any of those
attributes respawn creep doesn't touch
attributes respawn creep doesn't touch
any of those
any of those
attributes target. type no longer exists
attributes target. type no longer exists
this is now entity
this is now entity
type it's entity
type it's entity
player entity
type this is fine
entity
type there may be some additional type
type there may be some additional type
checks in here right like over
here power type so I assume there's a
here power type so I assume there's a
get type somewhere
get type somewhere
here
here
type uh here we need this is entity
type
and that might be all of
them these are just
them these are just
skills they shouldn't be checking any of
skills they shouldn't be checking any of
this except this one apparently does
entity
types
type
DOT layer.
DOT layer.
type is not in here entity. type is not
type is not in here entity. type is not
in here uh creep. type and neutral. type
in here uh creep. type and neutral. type
target.
type okay I think we're good this is a
type okay I think we're good this is a
substantial change so this might take a
substantial change so this might take a
little bit of
little bit of
figuring unless I oneshot
it likely the rendering is not going to
it likely the rendering is not going to
make sense for a little bit until I
make sense for a little bit until I
update the graphics and the uh the
update the graphics and the uh the
colors
colors
assignments mode
assignments mode
render or
render or
eval render mode
RB I think actually this will crash
RB I think actually this will crash
because yep don't have enough colors
because yep don't have enough colors
which is
fine so let's just take these grid IDs
fine so let's just take these grid IDs
and let's go
and let's go
to
to
um
um
environments
environments
and let's just get some nice colors for
and let's just get some nice colors for
stuff
right and this will just
right and this will just
B 0 to
B 0 to
55 this is the walls are
55 this is the walls are
blue I
suppose don't really like the walls
suppose don't really like the walls
being blue like
that I would rather use
that I would rather use
um is it 178
78 this is the puffer
color uh Tower is going to
be Tower is going to be an objective
be Tower is going to be an objective
marker let's go get a a Color Picker
up uh what do we want the towers to be
up uh what do we want the towers to be
like orange
green green for the objective markers
green green for the objective markers
would stand out nicely
right so it's
Tower uh radiant
creeps oh we need a color identity for
creeps oh we need a color identity for
each side don't
each side don't
we what do they use in Dota it's it's
we what do they use in Dota it's it's
green and blue I mean green and red
green and blue I mean green and red
isn't
it for the uh the
it for the uh the
creeps yeah so maybe we don't use that
creeps yeah so maybe we don't use that
for the
for the
tower
um green plus red is orange so
orange FF
six
six
five so this is the
five so this is the
tower uh radiant creeps will be
tower uh radiant creeps will be
in I say darkish green right darkish
in I say darkish green right darkish
greens and reds
maybe dire creep will be
maybe dire creep will be
dark
red neutral creep will be
gray and then uh
gray and then uh
initially for radiant
I think what we do is we
use do we use tones
this is nope
need like some good colors for the uh
need like some good colors for the uh
the creeps oh we did
um Let's do let's actually change this
um Let's do let's actually change this
up a little
up a little
bit we're going to change the default
bit we're going to change the default
colors a bit from the uh The Originals
colors a bit from the uh The Originals
cuz red green is bad for color blind and
cuz red green is bad for color blind and
also it's like we're going to do red
also it's like we're going to do red
white and blue because of course we
are so radiant creeps will
be uh they can
be uh they can
stay no these need to be dark blue like
stay no these need to be dark blue like
this 12
eight yeah so it's red versus
eight yeah so it's red versus
blue that's
better and then we'll basically we'll
better and then we'll basically we'll
just give you
like yeah Canan versus that's even
like yeah Canan versus that's even
better
we do need to get you a color palette
we do need to get you a color palette
though for the
though for the
heroes they need to stand out quite a
bit still doesn't solve this
problem I do get to use all of the
problem I do get to use all of the
bright colors now though
I could do
I could do
um dire creeping 128 like this like dark
um dire creeping 128 like this like dark
and I could just make these for now I'm
and I could just make these for now I'm
going to just make them um like the pure
colors
colors
okay and then for now we'll just do is
okay and then for now we'll just do is
it0 0 255
all
right this should be a reasonable color
right this should be a reasonable color
palette see if we can see
anything Q to error
anything Q to error
device side
device side
assert yes because now we have to change
assert yes because now we have to change
this to
be6 we have 16 possible channels instead
be6 we have 16 possible channels instead
of
eight 16 +
3 it's one hot with a Max of
16 it looks like I messed up some color
16 it looks like I messed up some color
codes
codes
because this is not what I intended
because this is not what I intended
though it's very
funny so let's see how did this
happen oh uh because I think we're using
happen oh uh because I think we're using
these Maybe
where's the wall thing wall is now
where's the wall thing wall is now
one and Tower is two
not need to do
this okay let's see about this
now there we go
so the
so the
walls the colors did not come out
great yeah
because we flip first of all we flip the
because we flip first of all we flip the
uh the radiant and the
uh the radiant and the
dire so let's undo that
dire so let's undo that
be0 to
55 and this one is
55 and this one is
255 Z
see how this
looks okay red versus
looks okay red versus
blue way
better
better
yeah creep colors are
off let's do dire creeps are just going
off let's do dire creeps are just going
to be
to be
128 radiant creeps will
be this see if this looks
better oh yeah there we
go and we'll have to come up with a
go and we'll have to come up with a
better color palette but for now you've
better color palette but for now you've
got dark red dark blue
can you give a quick rundown on what is
can you give a quick rundown on what is
being solved here if you've uh this is
being solved here if you've uh this is
the first time that you're seeing this
the first time that you're seeing this
project this is a miniature version of
project this is a miniature version of
am MOA it's based off of DOTA uh except
am MOA it's based off of DOTA uh except
that it runs at over a million frames
that it runs at over a million frames
per second and the goal is to train
per second and the goal is to train
reinforcement learning agents on it to
reinforcement learning agents on it to
play the game so you essentially have uh
play the game so you essentially have uh
if you've seen the open AI 5 project
if you've seen the open AI 5 project
where open AI trained well they used a
where open AI trained well they used a
ton of compute like tens of millions of
ton of compute like tens of millions of
dollars of compute to train uh agents to
dollars of compute to train uh agents to
play the full game of DOTA we're doing a
play the full game of DOTA we're doing a
mini version of that but we're going to
mini version of that but we're going to
make it like 10,000 times faster so
make it like 10,000 times faster so
you're going to be able to train this on
you're going to be able to train this on
One desktop and I'm building the
One desktop and I'm building the
simulator I am uh figuring out what data
simulator I am uh figuring out what data
to pass from the simulator to the
to pass from the simulator to the
networks and uh what I'm doing right now
networks and uh what I'm doing right now
at the this particular moment is I redid
at the this particular moment is I redid
the way that the environment represents
the way that the environment represents
different types of Agents different
different types of Agents different
Heroes and such and uh I had to fix the
Heroes and such and uh I had to fix the
renderer so I could actually see what's
renderer so I could actually see what's
going on and then now that this is fixed
going on and then now that this is fixed
I'm going to be passing all that new
I'm going to be passing all that new
data into the neural network which will
data into the neural network which will
hopefully allow the agents to better
hopefully allow the agents to better
learn how to play the
game that's what's going on here ultra
game that's what's going on here ultra
high performance simulation engineering
high performance simulation engineering
for reinforcement
learning ultra high per
is this RGB array render take
colors one
colors one
sec just want to make sure the other
sec just want to make sure the other
render is not broken by my new
change RGB array render does take colors
yeah okay so this one works as well just
yeah okay so this one works as well just
the same
the same
perfect we fixed the
renderer
now let's see if there's anything else
now let's see if there's anything else
we want include in the agent
we want include in the agent
observations
here only thing I can think
here only thing I can think
of is do we want to include the hero
class it's not really a format of
class it's not really a format of
data a form a data formula that's going
data a form a data formula that's going
to be
to be
useful I could add it as five separate
useful I could add it as five separate
variables
maybe how far are you going with
maybe how far are you going with
modeling how many champs they have a
modeling how many champs they have a
simplified kit how many players uh it's
simplified kit how many players uh it's
a 5v5 the map is currently modeled like
a 5v5 the map is currently modeled like
one to one with the DOTA map app uh
one to one with the DOTA map app uh
except I've only implemented obstacles
except I've only implemented obstacles
so far I haven't implemented the forest
so far I haven't implemented the forest
or Vision or anything like that uh they
or Vision or anything like that uh they
do have a simplified kit they're not one
do have a simplified kit they're not one
to one with the DOTA Heroes because the
to one with the DOTA Heroes because the
abilities that many DOTA Heroes Have are
abilities that many DOTA Heroes Have are
very complicated and really require you
very complicated and really require you
to implement a lot more of the game so
to implement a lot more of the game so
they do have simplified kits I currently
they do have simplified kits I currently
have three skills per hero um I'm
have three skills per hero um I'm
thinking about adding you know ultimates
thinking about adding you know ultimates
and stuff though they're harder to learn
and stuff though they're harder to learn
to use um so they're five unique Heroes
to use um so they're five unique Heroes
it's mirror match and it runs really
it's mirror match and it runs really
fast um and we can continue I mean I can
fast um and we can continue I mean I can
continue to build out from here I've
continue to build out from here I've
only been developing this thing for less
only been developing this thing for less
than two weeks it's mainly going to be
than two weeks it's mainly going to be
based on how interested people are in
based on how interested people are in
this project so the current plan is to
this project so the current plan is to
make this like mini MOBA that is uh that
make this like mini MOBA that is uh that
can be learned with reinforcement
can be learned with reinforcement
learning get some baseline agents that
learning get some baseline agents that
train to do some interesting stuff in
train to do some interesting stuff in
the environment make sure it's very fast
the environment make sure it's very fast
very clean and very easy to use and then
very clean and very easy to use and then
release it if people think that this is
release it if people think that this is
uh if people are very interested in this
uh if people are very interested in this
project and want to see like more
project and want to see like more
specific stuff added to it then you know
specific stuff added to it then you know
I will go ahead and do
that okay so so far this is pretty
that okay so so far this is pretty
reasonable um
this rewards
this rewards
variable I think we want to move
this I'm going to move this down here
this I'm going to move this down here
we're going to have to reindex
everything thank you it's all open
everything thank you it's all open
source as well you're free to uh
source as well you're free to uh
download it and play with it
oops
this
works so we have to
works so we have to
um maybe just add some indicator values
um maybe just add some indicator values
right
they should know their team as
well so let's
do let's add some let's just add a few
do let's add some let's just add a few
additional
things team we didn't add team right
things team we didn't add team right
yeah let's add team
255 18
19 Okay so we've got some
19 Okay so we've got some
indicators we now have 22
indicators we now have 22
variables associated with the player we
variables associated with the player we
should probably put a little bit more
should probably put a little bit more
effort into the scaling
here unsign Char player.
here unsign Char player.
Lev divided by
30 be 255 times player. level over 30
30 be 255 times player. level over 30
right
yeah and then the timers we don't know
yeah and then the timers we don't know
yet player is hit player.
yet player is hit player.
team these look
fine so 22
fine so 22
variables oh
variables oh
23 it's got to be 22 right
23 it's got to be 22 right
here we're probably going to want some
here we're probably going to want some
different reward components
huh yeah we're going to definitely want
huh yeah we're going to definitely want
some different reward
some different reward
components maybe what we'll do is we'll
components maybe what we'll do is we'll
make each individual reward component
make each individual reward component
between um negative 1 and
one
one
yeah and then you can always scale the
yeah and then you can always scale the
reward
right well no that doesn't make any
right well no that doesn't make any
sense come to think of it
I think we leave it here for now we
I think we leave it here for now we
start with
this
welcome we are
welcome we are
currently uh adding a whole bunch of
currently uh adding a whole bunch of
better observation data we're adding a
better observation data we're adding a
bunch of better observation data into
bunch of better observation data into
the model
the model
we currently have um four four channels
we currently have um four four channels
worth of map
worth of map
data that contains information about
data that contains information about
nearby about the map itself and nearby
nearby about the map itself and nearby
Heroes and now agents have
Heroes and now agents have
23 variables about themselves that they
23 variables about themselves that they
can
see so this now goes up to
23 and this is player OBS n
where is
it so this is
it so this is
now
three
three this run
this does
this does
run about the same speed as
run about the same speed as
before
before
perfect let's Commit This up and uh see
perfect let's Commit This up and uh see
if we
if we
can we can get something to train on
can we can get something to train on
this
now upper lib
right
okay so now we have the latest
code uh
diff 200 M's okay I know 10m worked
diff 200 M's okay I know 10m worked
before
I'm going to set this to 10 even though
I'm going to set this to 10 even though
it's going to be
slow that's
fine yeah this is fine so let's run
fine yeah this is fine so let's run
this we have to run
setup does this run
train it's very slow but it's very slow
train it's very slow but it's very slow
because the the small number of
because the the small number of
environments that's
acceptable it does appear to be training
acceptable it does appear to be training
like it was
before uh
yeah oh yeah there it goes
so this should just run down mid it's a
so this should just run down mid it's a
very dumb model that we just trained I'm
very dumb model that we just trained I'm
hoping it just runs down mid if so that
hoping it just runs down mid if so that
just means I didn't break
just means I didn't break
anything that's all that
means a reward yeah perfect
let's watch
them and then what we're going to have
them and then what we're going to have
to do is we're basically going to have
to do is we're basically going to have
to figure out
to figure out
like how to get this thing training High
like how to get this thing training High
perf and consistently for this dumb task
perf and consistently for this dumb task
before we try to make it run on
before we try to make it run on
something more fancy you know because we
something more fancy you know because we
we can start giving them rewards for
we can start giving them rewards for
like levels and XP and stuff
like levels and XP and stuff
um but it's not really going to make a
um but it's not really going to make a
difference if the training is not well
difference if the training is not well
set up in the first place
so we're going to look at this and then
so we're going to look at this and then
I'm going to take a quick second we're
I'm going to take a quick second we're
going to come back and finish that side
going to come back and finish that side
of things um
of things um
okay these guys aren't doing anything
because model PA
because model PA
ET there we go
okay they go down Lane
right these guys
right these guys
don't that's funny that Dyer didn't
don't that's funny that Dyer didn't
figure it out
yet
yet
H well I think they will improve over
H well I think they will improve over
time
time
right here I'm going to launch a
right here I'm going to launch a
slightly longer
slightly longer
run and uh I'm going to go use the
run and uh I'm going to go use the
restroom I'm going to go make myself a
shake and then we will come back and
shake and then we will come back and
um yeah we'll see if we can get this
um yeah we'll see if we can get this
thing training way better and more
consistently maybe hopefully yeah they
consistently maybe hopefully yeah they
should
should
but the thing is like this is a really
but the thing is like this is a really
dumb sanity check
anyways here we'll run this for 100
anyways here we'll run this for 100
million steps the problem is at the
million steps the problem is at the
moment the uh the configuration I'm
moment the uh the configuration I'm
running is very slow because the larger
running is very slow because the larger
one seems to be unstable so as soon as I
one seems to be unstable so as soon as I
get the uh the larger configuration to
get the uh the larger configuration to
be more stable it'll just chug so much
be more stable it'll just chug so much
more training data that it should work
more training data that it should work
but it's going to take a little bit of
but it's going to take a little bit of
effort to get it to there so I'm going
effort to get it to there so I'm going
to leave this up I'm going to get the uh
to leave this up I'm going to get the uh
the experiment up for you to watch while
the experiment up for you to watch while
I take a quick minute to go make myself
I take a quick minute to go make myself
a
a
shake few minutes there and this run
shake few minutes there and this run
only takes s six minutes so that's good
timing here
reward commercial break more like I
reward commercial break more like I
haven't had lunch break let me go make a
shake
e
e
e
e
e
e
e
e
e e
okay so uh the training speed halfed
okay so uh the training speed halfed
that's
new and uh it also refreshed so that you
new and uh it also refreshed so that you
couldn't see it
couldn't see it
whoops okay
whoops okay
um
interesting it goes up to 0.25
dire level mean radiant level the fact
dire level mean radiant level the fact
that the radiant and the dire are the
that the radiant and the dire are the
same levels mean I would think that
same levels mean I would think that
they're both going towards the center
they're both going towards the center
we'll see when this finishes training of
course but this looks
course but this looks
decent yeah yeah so this was 10 mil
decent yeah yeah so this was 10 mil
before right this is where we stopped
before right this is where we stopped
training before and then it takes a
training before and then it takes a
little longer to get better
little longer to get better
okay it's not
bad of course this is training
bad of course this is training
incredibly slowly so let's just
incredibly slowly so let's just
preemptively start thinking about why
preemptively start thinking about why
why that is
um when I try to simulate so the issue
um when I try to simulate so the issue
is that when I try to simulate way more
is that when I try to simulate way more
copies of the environment which you need
copies of the environment which you need
to simulate more copies of the
to simulate more copies of the
environment if you want want to train
environment if you want want to train
more quickly has nothing to do with the
more quickly has nothing to do with the
environment it has to do with um optimal
environment it has to do with um optimal
data batch sizes for the GPU essentially
data batch sizes for the GPU essentially
like you want you always want at least
like you want you always want at least
4,000 but preferably 8,000 like
4,000 but preferably 8,000 like
8192 uh steps worth of
8192 uh steps worth of
data that's a good mini batch size for
data that's a good mini batch size for
small reinforcement learning networks to
small reinforcement learning networks to
saturate your
saturate your
GPU but in order to get that many
GPU but in order to get that many
batches well 8,000 steps that's 10
batches well 8,000 steps that's 10
agents per environment so that's 800
agents per environment so that's 800
environments you need to simulate but in
environments you need to simulate but in
practice you need to simulate at least
practice you need to simulate at least
twice that number of environments so
twice that number of environments so
that while the GPU is running for one
that while the GPU is running for one
set of environments the other ones are
set of environments the other ones are
running in the background so that you're
running in the background so that you're
never wasting time that's how this
never wasting time that's how this
environment here is like taking no time
environment here is like taking no time
whatsoever
whatsoever
um so yeah that's the awkward part about
um so yeah that's the awkward part about
this uh the other thing that's kind of
this uh the other thing that's kind of
weird is that this doesn't seem to train
weird is that this doesn't seem to train
correctly unless you you run multiple
correctly unless you you run multiple
updates through the same
updates through the same
data that doesn't make much sense to me
either so we're going to have to play
either so we're going to have to play
with those we're going to have to play
with those we're going to have to play
with those things this is technically
with those things this is technically
one of the areas where I probably should
one of the areas where I probably should
just run a hyperparameter sweep and
just run a hyperparameter sweep and
later on in this project that's
later on in this project that's
absolutely what I'll be doing I'll just
absolutely what I'll be doing I'll just
kind of be automating everything but
kind of be automating everything but
early on it's good to gain an intuition
early on it's good to gain an intuition
for like what things in the environment
for like what things in the environment
are wonky and what aren't so it's wor
are wonky and what aren't so it's wor
spending a little bit of time on this
spending a little bit of time on this
just a little bit and then after that we
just a little bit and then after that we
can do uh you know we can do a little
can do uh you know we can do a little
bit of reward shaping a little bit of
bit of reward shaping a little bit of
like just get a bunch of rewards
like just get a bunch of rewards
extracted from the environment we'll
extracted from the environment we'll
look at the DOTA paper we'll see what
look at the DOTA paper we'll see what
rewards they used um and that'll
rewards they used um and that'll
probably be what we'll get done for the
probably be what we'll get done for the
rest of
rest of
today
yeah cheers though
almost done
all of this is
all of this is
oops right here all of this is open
oops right here all of this is open
source right
here go ahead and start the repo on your
here go ahead and start the repo on your
way
way
in helps me a whole
in helps me a whole
bunch and uh the latest Dev is right
bunch and uh the latest Dev is right
here in any config you can go look at
here in any config you can go look at
where the changes are it's in puffer lib
where the changes are it's in puffer lib
environments ocean
environments ocean
MOBA so all this is available pretty
MOBA so all this is available pretty
much in real time as I'm developing
it as soon as this finishes
it as soon as this finishes
synchronizing yeah we got our
model e
okay very nice they run at each other
okay very nice they run at each other
down
down
mid so this is what we
wanted sometimes they do weird stuff but
wanted sometimes they do weird stuff but
for the most
for the most
part like there's like one agent that's
part like there's like one agent that's
weird but for the most part they do
weird but for the most part they do
reasonable
things some of them they even path
things some of them they even path
around the obstacles on the way there
around the obstacles on the way there
yeah nice basic sanity check but
reasonable so let's see how long it took
reasonable so let's see how long it took
them to learn that because we have some
them to learn that because we have some
additional stats that we can rely on
additional stats that we can rely on
here
uh
oh welcome
okay so we can actually
see if we look at the training
see if we look at the training
here we can see dire level mean and
here we can see dire level mean and
radiant level mean
radiant level mean
here
here
so they're actually pretty even in this
so they're actually pretty even in this
training run so it might just not be
training run so it might just not be
very
consistent but definitely you only need
consistent but definitely you only need
to train for like 30 million or so steps
to train for like 30 million or so steps
it looks like before this becomes uh
it looks like before this becomes uh
reasonably
reasonably
consistent and that matches with the
consistent and that matches with the
reward curves over here even 20 million
reward curves over here even 20 million
looks reasonable enough I guess 10 just
looks reasonable enough I guess 10 just
wasn't quite there but you train a
wasn't quite there but you train a
little longer in your set so that's not
little longer in your set so that's not
bad
um I need to think about why when you
um I need to think about why when you
run more copies of the environment it
run more copies of the environment it
screws up
screws up
training well does It screw up training
training well does It screw up training
let's I haven't tested it
lately let's change this from 10
lately let's change this from 10
environments to 200 environments
right what happens
that looked
that looked
good hold
good hold
on are we uh are we back
here is actually working
now hasn't logged
maybe that
logged okay it only logged us one data
logged okay it only logged us one data
point but
logs reward at
logs reward at
0.13 this is not sufficient here uh this
0.13 this is not sufficient here uh this
is only 8 mil steps into training
though this is substantially faster than
though this is substantially faster than
before we can actually we can go over
before we can actually we can go over
500k here quite
500k here quite
easily but
easily but
um we're debugging and then there
um we're debugging and then there
there's like some weirdness with the
there's like some weirdness with the
scaling right now we're going to get
scaling right now we're going to get
this over a mil okay so this is not
this over a mil okay so this is not
actually working then because that 0.14
actually working then because that 0.14
is insufficient so then the question is
is insufficient so then the question is
going to be why
um I do kind of have this janky
um I do kind of have this janky
generalized Advantage estimation
implementation maybe we need a larger
implementation maybe we need a larger
batch size
batch size
maybe that
helps it is increasing maybe I'm going
helps it is increasing maybe I'm going
to let this run for like a little longer
to let this run for like a little longer
but then I have some
but then I have some
ideas on how I would want to address
ideas on how I would want to address
this it involves ludicrously large batch
sizes how long would it take to do a
sizes how long would it take to do a
sweep on wand
sweep on wand
B um it depends
B um it depends
this is an environment for sure that we
this is an environment for sure that we
are going to run overnight sweeps on it
are going to run overnight sweeps on it
is fast enough for that it's just that
is fast enough for that it's just that
like you kind of need to have an initial
like you kind of need to have an initial
starting point that's
reasonable
yeah I'm really only doing very coarse
yeah I'm really only doing very coarse
tuning at the moment to like get the
tuning at the moment to like get the
highle picture of this
highle picture of this
thing okay so this thing is it's not 50
thing okay so this thing is it's not 50
million steps .16 is not good enough so
one thing I'm suspecting here
one thing I'm suspecting here
is possible your reward signal no the
is possible your reward signal no the
reward signal is very nice it's the
reward signal is very nice it's the
easiest reward signal in the in the
easiest reward signal in the in the
world you get one every time you go
world you get one every time you go
closer to the enemy's ancient diagonally
closer to the enemy's ancient diagonally
you get 0 five every time you go like
you get 0 five every time you go like
lengthwise closer to the enemy's ancient
lengthwise closer to the enemy's ancient
zero if you stand in place Nega .1 if
zero if you stand in place Nega .1 if
you go away this way or this way away
you go away this way or this way away
from the enemy anent and negative one if
from the enemy anent and negative one if
you go directly away from the enemy
you go directly away from the enemy
anent
anent
so yeah it's it's a very very easy
so yeah it's it's a very very easy
reward
signal should be like
signal should be like
trivial 8,000
so this is only 16 steps this batch size
so this is only 16 steps this batch size
only gives you 16
steps no
steps no
wait yeah batch size tuning
wait yeah batch size tuning
so I have eight environments hold
on I've got eight I've got 800
on I've got eight I've got 800
environments so 8,000
environments so 8,000
agents right
agents right
no I
have I do this
have I do this
right 2,000 agents per
environment and I have eight
environment and I have eight
environments so I have 16,000 agents
environments so I have 16,000 agents
total
right and the problem is that this
right and the problem is that this
number is only eight
number is only eight
so you're really not getting very much
so you're really not getting very much
data per
data per
agent at
all
all
well we can cut this down a little bit
well we can cut this down a little bit
to start with
right let's start by just cutting this
right let's start by just cutting this
down a little
bit do we need 8 8K
bit do we need 8 8K
no snake
no snake
runs snake runs a million steps per
runs snake runs a million steps per
second with 4K we should only need
4K so let's say we Run 100 We Run 100
4K so let's say we Run 100 We Run 100
environments per
environments per
core so now we are running
800 800 environments which is 8,000
800 800 environments which is 8,000
agents
we're going to do now mini batch size
we're going to do now mini batch size
will be
4K that's slightly better
right and
then this is now 16 steps per agent
this is a huge huge number of updates
this is a huge huge number of updates
per batch at this
per batch at this
point let's see what happens when I do
point let's see what happens when I do
this and if I can get away with not well
this and if I can get away with not well
with doing things a little differently
with doing things a little differently
let's
let's
say am I on the wrong file hold
on yeah I'm on the wrong
one uh
one uh
well
e e
well that didn't run for long
well that didn't run for long
enough and also that's kind of awkward
enough and also that's kind of awkward
the way that I did
that I'm going to set this to
that I'm going to set this to
one I'm going to set this to 100 Mil
we should at least see the speed come
we should at least see the speed come
back
back
now only
400k definitely would expect more than
400k some optimization to be done
what's clip Frack clip fraction is very
what's clip Frack clip fraction is very
low
so this big number of updates really is
so this big number of updates really is
not taking us off
policy bizarre to me that this doesn't
learn
learn
so that's weird
why is the training speed that
why is the training speed that
variable do I have other stuff running
variable do I have other stuff running
on this box I
don't it really there should not be
don't it really there should not be
anything variable whatsoever about the
anything variable whatsoever about the
training
speed it's not the environment the
speed it's not the environment the
environment is not taking up any
environment is not taking up any
additional time
profile it
profile it
yeah yeah well initially I want to get
yeah yeah well initially I want to get
it learning
it learning
correctly because the fact that this is
correctly because the fact that this is
stuck like the only setting that I've
stuck like the only setting that I've
been able to get to
been able to get to
train is when you run very few copies of
train is when you run very few copies of
the environment and you use a lot of
the environment and you use a lot of
stale data which if you think about it
stale data which if you think about it
just that doesn't make any
just that doesn't make any
sense it straight up just does not make
sense it straight up just does not make
any
any
sense
um I can dramatically increase the mini
um I can dramatically increase the mini
batch size for more stable
batch size for more stable
updates in
theory fact that Ford is taking that
theory fact that Ford is taking that
much time as well
stale data like I'll show
you
you
so I had four updates before but like
so I had four updates before but like
the thing is if you have mini batch size
the thing is if you have mini batch size
4,000 right and batch size
4,000 right and batch size
256,000 then you're you're actually
256,000 then you're you're actually
doing 64 updates
doing 64 updates
times another four so you're doing 256
times another four so you're doing 256
updates per Epoch which is
insane
insane
so I'm going set this to like what
so I'm going set this to like what
32 32k and see what
32 32k and see what
happens just give you stable
happens just give you stable
updates I think I have GPU memory for
updates I think I have GPU memory for
that
this is a task that you should be able
this is a task that you should be able
to learn in like a million steps with
to learn in like a million steps with
batch size like I don't know 128 or
batch size like I don't know 128 or
something it's very easy
having absolutely no entropy bonus is
having absolutely no entropy bonus is
very
Jank but when I remove the entropy bonus
Jank but when I remove the entropy bonus
before it actually let it
learn what did I end up using for
learn what did I end up using for
snake was the entropy
so snake had an entropy a final entropy
so snake had an entropy a final entropy
tuned value of
tuned value of
01 and grid had a final entropy value of
01 and grid had a final entropy value of
very
[Music]
[Music]
low reward is sum of side one plus sum
low reward is sum of side one plus sum
of side
two some of side the reward is per agent
two some of side the reward is per agent
each agent is rewarded for going towards
each agent is rewarded for going towards
the enemy's ancient that reward is
the enemy's ancient that reward is
scaled between negative one for going
scaled between negative one for going
directly away from the enemy ancient and
directly away from the enemy ancient and
one for going directly towards the enemy
one for going directly towards the enemy
ancient so you literally have a one-step
ancient so you literally have a one-step
reward like except for like obstacles
reward like except for like obstacles
you have a one-step problem it's the
you have a one-step problem it's the
easiest thing
imaginable or it should
imaginable or it should
be but clearly something is wonky here
be but clearly something is wonky here
I'm going to put a little bit of entropy
I'm going to put a little bit of entropy
back in see what
happens nope
H that was just a flicker
okay I'm assuming your multi-agent logic
okay I'm assuming your multi-agent logic
is
is
correct it's tested on a ton of other
correct it's tested on a ton of other
environments same code if you want I can
environments same code if you want I can
train multi-agent snake for in like 30
train multi-agent snake for in like 30
seconds I tried that earlier
seconds I tried that earlier
today the snake environment that I have
today the snake environment that I have
in this uh in here you can train like
in this uh in here you can train like
slither.io basically 4096 agent snake
slither.io basically 4096 agent snake
you can get a reasonable policy in about
you can get a reasonable policy in about
a
minute trains it over a million steps
minute trains it over a million steps
per second
and that policy is basically identical
and that policy is basically identical
to this one as
to this one as
well it's just this one has a few more
well it's just this one has a few more
com
channels entropy not helping
it's so
weird blue team goes One Direction red
weird blue team goes One Direction red
team goes the other yeah now the thing
team goes the other yeah now the thing
is I think that the way like okay I
is I think that the way like okay I
here's the the really weird thing right
here's the the really weird thing right
I'm pretty sure this
works ocean uh
works ocean uh
MOA see
I've got this logic in here that um
I've got this logic in here that um
randomly respawns players every 128
randomly respawns players every 128
steps it's helps with training it's like
steps it's helps with training it's like
curriculum smoothing obviously you have
curriculum smoothing obviously you have
to take this out later but if I
just if I just rebuild with this rewards
just if I just rebuild with this rewards
are relative to their respective goals
are relative to their respective goals
right
right
yes and you can actually you can play
yes and you can actually you can play
this game in human mode right and then
this game in human mode right and then
you can print out the rewards and you
you can print out the rewards and you
can see that when you go towards their
can see that when you go towards their
base uh you get positive reward and when
base uh you get positive reward and when
you go away you get Negative reward and
you go away you get Negative reward and
also we've seen it train successfully
also we've seen it train successfully
before just with really Jank
before just with really Jank
settings now if I train this
one I bet this one works
yeah so the reward is going
up and all I did is say Okay agents
up and all I did is say Okay agents
respawn very frequently and now you can
respawn very frequently and now you can
see you get stably increasing reward
see you get stably increasing reward
with reasonable
with reasonable
hyperparameters all I
hyperparameters all I
did so it's pretty bizarre
and we'll be able to pull this policy
and we'll be able to pull this policy
down as well and see this uh and watch
down as well and see this uh and watch
this
happen you need shorter episodes but why
right why does it only work when you
right why does it only work when you
randomly respawn agents on average every
randomly respawn agents on average every
steps that's really janky
right reset sequels
training it doesn't collect full
training it doesn't collect full
episodes just collect segments
really shouldn't be it's very
weird now oddly I don't see them getting
levels hopefully this policy is actually
levels hopefully this policy is actually
working as we uh we expect it to
working as we uh we expect it to
I think it is this is a high enough
I think it is this is a high enough
reward that it should be mostly doing
reward that it should be mostly doing
something
something
reasonable isn't most RL heavily
reasonable isn't most RL heavily
episodic it depends on the environment
episodic it depends on the environment
right if your
right if your
episodes if you have like episodes where
episodes if you have like episodes where
you have diverse data within each
you have diverse data within each
episode then yes but like you have a
episode then yes but like you have a
dense reward that's essentially just
dense reward that's essentially just
saying go in a specific
saying go in a specific
direction that should be fine however
direction that should be fine however
you do it
okay so this now is
okay so this now is
trained and go grab ourselves a free
trained and go grab ourselves a free
policy
policy
here we'll just render it
episode versus batch size balance is
crucial
um it should just be bigger batch sizes
um it should just be bigger batch sizes
are better but it's in practice not
always here let's see what this looks
always here let's see what this looks
like
funny that they all go down
funny that they all go down
top mid would be
better things are very dumb e
it does something at least it does do
it does something at least it does do
something
now technically I can just this is like
now technically I can just this is like
passible enough I can run a hyper pram
passible enough I can run a hyper pram
sweep on it
but not particularly satisfying is it
can start adding more log variables I
can start adding more log variables I
can start adding better
can start adding better
rewards maybe more difference between
rewards maybe more difference between
diagonal and one step more like 1.1 and
diagonal and one step more like 1.1 and
0.25 I mean I don't really care which
0.25 I mean I don't really care which
lane they go down it's just
lane they go down it's just
like the fact that they're having this
like the fact that they're having this
much trouble with such an easy problem
much trouble with such an easy problem
is weird right
shouldn't be having this much
trouble have you tried decreasing
trouble have you tried decreasing
learning
learning
rate I did before
rate I did before
I got these defaults from snake
I got these defaults from snake
though and they didn't
work snake should be a very similar even
work snake should be a very similar even
actually snake has a harder problem if
actually snake has a harder problem if
you think about it doesn't have his nice
you think about it doesn't have his nice
and dense of a reward as
and dense of a reward as
this I mean obviously not the full game
this I mean obviously not the full game
of DOTA that's way harder but just like
of DOTA that's way harder but just like
go down mid come on
well I guess we should just add in let's
well I guess we should just add in let's
I don't want to like completely stall on
I don't want to like completely stall on
dev today doing dumb experiments so
dev today doing dumb experiments so
let's just
let's just
um let's actually add the types of
um let's actually add the types of
learning uh of rewards that we need so
learning uh of rewards that we need so
that we can like autotune everything and
that we can like autotune everything and
we'll go from there I'm guessing they're
we'll go from there I'm guessing they're
just some dumb data bugs hold
on I'm guessing that they're just like
on I'm guessing that they're just like
some dumb data bugs where we're like
some dumb data bugs where we're like
corrupting observations every so often
corrupting observations every so often
or something stupid that's like the
or something stupid that's like the
usual
usual
thing if reward is so easy we don't need
thing if reward is so easy we don't need
much exploration yeah we shouldn't need
much exploration yeah we shouldn't need
much exploration we should just be able
much exploration we should just be able
to like install learn it and be done
okay I'll I'll like I'll cut it in 10
okay I'll I'll like I'll cut it in 10
let's see if it does
anything
anything
you're you're probably right
there I would think it would be fine
there I would think it would be fine
with 32k batch
though 32k mini batch I mean
yeah but this is still the uh the
yeah but this is still the uh the
version from before right with 32 step
version from before right with 32 step
Horizons
we'll run the other one after for you
we'll run the other one after for you
want to see what rewards they did for
want to see what rewards they did for
DOTA steal their Rewards
okay
okay
win negative uh win
win negative uh win
five Hero death negative
five Hero death negative
1 corer death
1 corer death
-2 have an XP
-2 have an XP
gain gold gain
gain gold gain
we don't have
we don't have
gold Health
gold Health
change so they get a reward for
change so they get a reward for
healing Mana
Change Hero uh killed
Change Hero uh killed
hero gold and XP are very high so this
hero gold and XP are very high so this
reduces the total
reward last hit
deny AIS ancient
HP for Defending Your ancient I
HP for Defending Your ancient I
guess then they've got Tower
rewards Shrine Barrack
and then this Lane
and then this Lane
assignment the
item okay so we
item okay so we
have randomly assign each hero to a
have randomly assign each hero to a
subset of
subset of
lanes and penalize them for
lanes and penalize them for
leaving those
leaving those
Lanes this may have not been necessary
Lanes this may have not been necessary
in the
end prefer not to do a lane assignment
end prefer not to do a lane assignment
if I can avoid it let's see what did we
if I can avoid it let's see what did we
get out of
this but we're going to implement the
this but we're going to implement the
tower rewards and
stuff okay so
visualize
visualize
none here are the last two
runs the latest run did worse it
appears not really any more stable
either now we'll run this on the uh
either now we'll run this on the uh
we'll do the lower learning rate on this
we'll do the lower learning rate on this
one
right the original
task and then we'll do this while I look
task and then we'll do this while I look
more at the rewards and start
more at the rewards and start
implementing them
so I think then we do uh the XP gain
so I think then we do uh the XP gain
reward will
be will be
this 0.002
oh there's also the the gold gain reward
oh there's also the the gold gain reward
so gold XP is very high so this reduces
so gold XP is very high so this reduces
the total reward to
the total reward to
0.4 is what this
0.4 is what this
says because this is 0
point how's that
point how's that
possible doesn't a minion give like 50
possible doesn't a minion give like 50
XP or
XP or
something 0 point was it 08
[Music]
H killed hero
well let's just implement the uh the
well let's just implement the uh the
rewards and we'll scale
rewards and we'll scale
them I think ne5 to5
them I think ne5 to5
overall looks like something
overall looks like something
reasonable how's this doing
nope doesn't
work I told you it's weird right
I don't know there could just be
I don't know there could just be
something Jank about this like this
something Jank about this like this
reward structure though I
reward structure though I
don't who knows maybe we just implement
don't who knows maybe we just implement
the full thing and it suddenly dissolves
DOTA let's add some Rewards
uh and we also have to cap the rewards
uh and we also have to cap the rewards
right compute
right compute
let me do that first cap the reward
observation
so
so
plus gives
plus gives
you is it5
you is it5
time 20
oops yeah this is good initial thought
oops yeah this is good initial thought
is rewards need to be
is rewards need to be
tuned one for correct and zero for
incorrect I can do
that I don't know why that would be
that I don't know why that would be
needed
but this pretty odd
but this pretty odd
right not having a negative
right not having a negative
reward I mean unless it's
reward I mean unless it's
like unless I did something really
like unless I did something really
stupid like use a uent for rewards but I
stupid like use a uent for rewards but I
don't think
don't think
so you
so you
see no it's a
see no it's a
float should be signed
okay
rebuild so I've clipped the reward now
rebuild so I've clipped the reward now
to be uh 0er to
to be uh 0er to
one see about
one see about
that
that
meantime we're going to change this
meantime we're going to change this
logic to
logic to
be this is just the observation of the
be this is just the observation of the
reward are no big
deal we'll clip the reward
to we should do the clipping all at the
to we should do the clipping all at the
end
right they said they they get about 0.4
right they said they they get about 0.4
for a last hit on something
so 3560 for a creep
kill 006
it seems
reasonable so we add this to the reward
reasonable so we add this to the reward
here and get reward based on XP
and then
and then
um what about the tower
what's there
what's there
roughly 2.25 3
roughly 2.25 3
4.5 I think I can just use three for all
4.5 I think I can just use three for all
of
of
these a baseline since they're not going
these a baseline since they're not going
to be the same
they also have Team base rewards
they also have Team base rewards
here which is
interesting Shrine
Barracks they have uh Hero death is
Barracks they have uh Hero death is
negative one
is at the
top clear
this did this do
this did this do
anything Let's see we tried your
anything Let's see we tried your
experiment here
oh
oh
uh this might have done
uh this might have done
something be real
interesting though it's scaled
interesting though it's scaled
differently so I can't quite tell
right you're at 0.5 but nobody's gotten
right you're at 0.5 but nobody's gotten
any
any
levels is a little
sketchy for
H this is really funny do you see what
H this is really funny do you see what
they're
they're
doing they're
doing they're
vibrating so that they are getting a
vibrating so that they are getting a
reward every other time
reward every other time
step isn't that weird
that was pretty
weird they're actually they're
weird they're actually they're
like they're vibrating along the
like they're vibrating along the
diagonal as well which is weird I don't
diagonal as well which is weird I don't
think that should give them any
reward yeah this shows as uh
huh very
huh very
weird we probably have some
weird we probably have some
bugs I have some
bugs I have some
bugs demons those demons
negative one for
dying Health
dying Health
changed Mana changed
we've got
we've got
Tower we've got Tower
Tower we've got Tower
rewards we've got XP rewards and we've
rewards we've got XP rewards and we've
got don't die
got don't die
reward and we also have the
reward and we also have the
uh you know the the Jank reward that
uh you know the the Jank reward that
doesn't seem to be
doesn't seem to be
working okay let's figure out why this
working okay let's figure out why this
Jank forward is not working we got a
Jank forward is not working we got a
clue out of this I think which is that
clue out of this I think which is that
this Behavior says that it's getting .
this Behavior says that it's getting .
five reward oops not this one the one we
five reward oops not this one the one we
just watched says it's getting 0 five
just watched says it's getting 0 five
reward right but I don't understand
how
how
so distance to ancient
player Yus ancient y plus player xus
player Yus ancient y plus player xus
ancient
X move
really looks like that behavior should
really looks like that behavior should
get zero
reward I'm going to implement it locally
what did we say it was like
what did we say it was like
um if player. reward less than zero
um if player. reward less than zero
player reward is
player reward is
zero that's what we said we're going to
do and according to this they get 0 five
reward that behavior looks like it gets
reward that behavior looks like it gets
no reward to
me seg
Vault I probably just screwed this up
right yeah
seg
VA
VA
okay okay so this is saying we're
okay okay so this is saying we're
getting point4
getting point4
reward it's about what we found
reward it's about what we found
before I do not see how this is
before I do not see how this is
point for a
point for a
reward
reward
so that's interesting right
invalid PID
huh
there record has no object
there record has no object
type uh fine
type
key
key
error I guess I have to update this uh
error I guess I have to update this uh
this asset map a little bit
this asset map a little bit
huh so we can render
it e
where is this even
getting tile is equal to grid okay so it
getting tile is equal to grid okay so it
is the grid ID
that's
that's
fine we just have to update this asset
fine we just have to update this asset
map a little bit and we'll be able to
map a little bit and we'll be able to
render it and maybe we'll get some
render it and maybe we'll get some
insight just
insight just
maybe we'll get a little bit of insight
maybe we'll get a little bit of insight
so this is now two Tower
so this is now two Tower
here two is
here two is
Tower three
I need to open up this file don't
I not this one
okay perfect
so this
so this
is
is
dire this is
dire this is
12 13
14 and 15 is going to be the red
14 and 15 is going to be the red
puff uh the was it 1280 yeah so this is
puff uh the was it 1280 yeah so this is
the radiance so this is
six 7 8 n
six 7 8 n
and
10 and then this one 256 this is the
10 and then this one 256 this is the
dire creep so this is four this this is
dire creep so this is four this this is
the radiant creep which is
three and then this
three and then this
is uh this is
is uh this is
nothing cuz seven is already covered
nothing cuz seven is already covered
perfect
why is there no
tower towers is two now
right where do we draw the the uh the
walls be
walls be
here now we should be able to do it
y
y
okay
so this is one this is 05 this is
n so this is literally zero reward right
how are they getting reward for this
participation trophy yeah seriously like
participation trophy yeah seriously like
okay what does it think that the first
okay what does it think that the first
agent is getting his
agent is getting his
reward let's try
that
that
zeros so it's under no impression that
zeros so it's under no impression that
this this behavior is rewarded
this this behavior is rewarded
right what about the dire are the dire
right what about the dire are the dire
getting rewarded for this [ __ ]
the dire are not getting rewarded for
the dire are not getting rewarded for
this
[ __ ]
[ __ ]
right
nine so nobody's getting rewarded for
nine so nobody's getting rewarded for
this
this
[ __ ] yet we're still doing
[ __ ] yet we're still doing
[ __ ] that the game here
how are we getting rewarded for this
[ __ ] zero we're not getting rewarded
[ __ ] zero we're not getting rewarded
for this
for this
[ __ ] wait a second wait just one
[ __ ] wait a second wait just one
goddamn
minute
minute
10 go
now we're suddenly getting rewarded for
now we're suddenly getting rewarded for
this [ __ ]
this [ __ ]
again
ooh what the
hell so there's definitely a bug with
hell so there's definitely a bug with
the uh multiple environments
the uh multiple environments
then definitely a
bug which makes sense right
bug which makes sense right
reinforcement learning Is Us usually
reinforcement learning Is Us usually
good like RL kind of kicks ass it's
good like RL kind of kicks ass it's
usually a data
problem I thought I'd checked for this
problem I thought I'd checked for this
dumb thing as well
let's see if these agents are doing
let's see if these agents are doing
anything different first of all right
now you're getting zero again
now you're getting
reward if I is equal to
reward if I is equal to
1 grid is the original grid
otherwise it's a copy of the
grid for
I'm not going to be able to see it if I
I'm not going to be able to see it if I
do
this yeah so this render
this yeah so this render
blank and then it well this render blank
blank and then it well this render blank
it rendered blank I can tell you that
so if I is equal to zero hold
so if I is equal to zero hold
on this gets reward
and this does not get
reward does that make any
sense e
yeah know I'm not I it's this is
yeah know I'm not I it's this is
complicated I was it was more or less
complicated I was it was more or less
rhetorical if you happen to know then
rhetorical if you happen to know then
great
great
but there's something there's
but there's something there's
essentially there's something weird with
essentially there's something weird with
multiple environments going on like when
multiple environments going on like when
you have multiple simulated
you have multiple simulated
environments and it has something to do
environments and it has something to do
with the way that I'm passing the
map
for e
you set up the Grid on a
knit render just takes self duck
red for
oh
wait blocking that
freaking
Bots you don't sell me Bots I make the
Bots that's how it
works so I think I found the issue
so when you pass all this data in
right what do we do with the
grid we start adding [ __ ] to the Grid on
grid we start adding [ __ ] to the Grid on
a reset
right oh I know how we can do this is
right oh I know how we can do this is
going to be funny actually this is going
going to be funny actually this is going
to be like horribly horribly broken
well
well that's a weird number of Agents Now
well that's a weird number of Agents Now
isn't
it
e e
that's so funny that's like a top tier
that's so funny that's like a top tier
bug [ __ ] that is a top top tier
bug [ __ ] that is a top top tier
bug jeez
that is
nuts shouldn't be anywhere else that
nuts shouldn't be anywhere else that
could happen though so um easiest fix
could happen though so um easiest fix
for this I think
that a top tier
bug it's always game problems I swear
bug it's always game problems I swear
it's always problems with the game
data well this still has some demons it
data well this still has some demons it
looks like
Toopy actual freaking demons I
swear you don't need a doctor you need a
priest initial Bowie
priest initial Bowie
wait is this
wait is this
bet oh wait I just realized that's you
bet oh wait I just realized that's you
bet isn't it Bo image
bet isn't it Bo image
shows
shows
shape
shape
24 I thought it might be I wasn't
24 I thought it might be I wasn't
positive um torch
positive um torch
shape yeah hi bet
um adjusted shape
um adjusted shape
image after CNN
shape is there a bug you're trying to
shape is there a bug you're trying to
help me find oh it's working it's
help me find oh it's working it's
functioning good very good get on
oh
Jesus [ __ ] straight demons I swear
just [ __ ]
demons great [ __ ]
demons honestly like getting good at
demons honestly like getting good at
tensor manips is one of the it's like
tensor manips is one of the it's like
one of the first things you have to do
one of the first things you have to do
in ml and it's one of the things you
in ml and it's one of the things you
will use forever in
ml like that right there was probably
ml like that right there was probably
like my first two years worth of
like my first two years worth of
research on architecture Dev was just
research on architecture Dev was just
like getting really good at shape
like getting really good at shape
issues banishing
issues banishing
demons you know
boom it
boom it
trains speaking of demon boxes for are
trains speaking of demon boxes for are
you kidding
me
[ __ ] four and seven
sh e
looking more at
box stress
test his code also
test his code also
yeah [ __ ] I'll get rebooted tomorrow I'm
yeah [ __ ] I'll get rebooted tomorrow I'm
rebooted in the
morning I'm so obnoxious
man too many demons
man too many demons
we're not doing Intel no more unless
we're not doing Intel no more unless
they get some good
chips uh yeah this actually freaking
chips uh yeah this actually freaking
slaps now doesn't
it 10
it 10
m let's do 100
m 200
m 200
M boom
do 8K mini
do 8K mini
batch
batch
28k is still ridiculous that
that's I'm working on this thing man
that's I'm working on this thing man
it's
hard 128
oops this is eight right yeah this is
oops this is eight right yeah this is
eight duh I don't know why I'm dividing
eight duh I don't know why I'm dividing
powers of two on
powers of two on
calculator
um this is so sketchy the way this works
um this is so sketchy the way this works
honestly you kind of start needing
honestly you kind of start needing
gradient accumulation don't
you now
you now
[ __ ] we going do 32k
[ __ ] we going do 32k
Right One update
Epoch give us 100 million
Epoch give us 100 million
steps and let's run uh a
steps and let's run uh a
nice nicely tracked
run oh yeah look at that SPS [ __ ] yeah
run oh yeah look at that SPS [ __ ] yeah
I'll use a restro be right back and uh
I'll use a restro be right back and uh
we'll see what
we'll see what
happens got no another hour and a half
happens got no another hour and a half
of
Dev
e
e e
I have to go run soon run is
I have to go run soon run is
good run is kicking my ass at the moment
good run is kicking my ass at the moment
it's 50 miles per week of run
I'm going to attempt to do 22 Mile run
I'm going to attempt to do 22 Mile run
this weekend and possibly
this weekend and possibly
survive we will
see so this doesn't work all that well
see so this doesn't work all that well
that's
funny knees
can't KNE shouldn't be the thing that
can't KNE shouldn't be the thing that
hurt
only thing if it hurts May knees maybe
only thing if it hurts May knees maybe
is if I try running down Hills I don't
is if I try running down Hills I don't
run down steep
hills it's usually Cales or whatever
hills it's usually Cales or whatever
that or tendons or stuff that gets hurt
everything
hurts eventually everything just gets
hurts eventually everything just gets
stronger takes time
you can very safely train from a
you can very safely train from a
marathon from being able to run like a
marathon from being able to run like a
few 5Ks a week in about 6
few 5Ks a week in about 6
months it sucks but you can do it you
months it sucks but you can do it you
can very consistently do that
safely the training does absolutely suck
though e
Marathon isn't that bad for
Marathon isn't that bad for
body 100 miles is bad for body marathon
body 100 miles is bad for body marathon
is fine for
body half marathon definitely isn't bad
body half marathon definitely isn't bad
for body
also good for mind
yeah people in shape can just do like
yeah people in shape can just do like
you can do several halfs a week and be
you can do several halfs a week and be
like chilling
body's capable of a
lot interesting that we're still having
lot interesting that we're still having
these weird stability
issues do I bump the entropy coefficient
issues do I bump the entropy coefficient
back up
back up
now it's pretty well crashed right
entropy coefficient
big learning rate
big reinforcement learning doesn't want
big reinforcement learning doesn't want
you to have fast MOBA
big RL wants you to think RL is hard and
big RL wants you to think RL is hard and
takes lots of
GPU but that's just listening to the man
we do have
we do have
32k 32k
32k 32k
MB 32,000
okay two
M's we got Lambda Gamma
M's we got Lambda Gamma
clip learning rate is O2
should be a very easy
task
e
e e
very funny how this
very funny how this
works we fixed the bug didn't we and
works we fixed the bug didn't we and
like forget to pull it right
like forget to pull it right
pull
no yeah
h
yeah it's
FAL fix the bug with the other thing
FAL fix the bug with the other thing
right
we saw this get solved like instantly
right could just be batch
uh the first run we did with this it
uh the first run we did with this it
actually it solved the um it got like
actually it solved the um it got like
reward 0.9 or whatever it so it works
reward 0.9 or whatever it so it works
it's just it doesn't work with good
settings let me try with just way larger
batch e
that's way way
that's way way
nicer I am back how's it
nicer I am back how's it
going oh man we found some demons yeah
going oh man we found some demons yeah
bet this fixed it it was the it was the
bet this fixed it it was the it was the
effective J Horizon is what it was
effective J Horizon is what it was
remember that thing I told you about
remember that thing I told you about
that yeah that
that yeah that
matters um we we found so well F we
matters um we we found so well F we
found some demons we found some real
found some demons we found some real
demons
demons
so this is one of the craziest bugs in
so this is one of the craziest bugs in
recent memory um I was passing a view of
recent memory um I was passing a view of
the grid into each
the grid into each
environment and that means that like as
environment and that means that like as
you added more environments it was
you added more environments it was
adding ghost copies of Agents into the
adding ghost copies of Agents into the
environment that could get rewards so
environment that could get rewards so
like the first environment which was the
like the first environment which was the
one that I was watching was normal so
one that I was watching was normal so
everything looked fine but then like the
everything looked fine but then like the
later environment got progressively more
later environment got progressively more
ghosts into
them
yeah
demons now this is actually very stable
demons now this is actually very stable
right here
here and it makes total sense
here and it makes total sense
why like big
batch this is an actually beautiful RL
batch this is an actually beautiful RL
curve look at
curve look at
this look how nice this is ready
boom
perfect e
is
is
solid yeah it
solid yeah it
works we're going to let this finish
works we're going to let this finish
we're going to watch the policy we're
we're going to watch the policy we're
going to admire the policy for a little
going to admire the policy for a little
bit hopefully it's
bit hopefully it's
good uh it looks like it's good right
good uh it looks like it's good right
because look at the level means
because look at the level means
right they're doing good
right they're doing good
see they got reasonable levels they're
good they're kung fu fighting in the
good they're kung fu fighting in the
middle we got the training going at
middle we got the training going at
500,000 plus steps per
500,000 plus steps per
second bada
boom whoops thank you for minimizing all
boom whoops thank you for minimizing all
my windows
my windows
it's exactly what I
wanted that is
clean RL is easy okay RL is easy getting
clean RL is easy okay RL is easy getting
making Sims that actually work correctly
making Sims that actually work correctly
and are fast is
and are fast is
hard RL is
hard RL is
easy you run po you run with some big
easy you run po you run with some big
batch you run a little hyperparameter
batch you run a little hyperparameter
sweep and you're good to go
let's go look at it
let's go look at it
the perfect training
the perfect training
curve which is exactly what should
curve which is exactly what should
happen because this is an a dead simple
happen because this is an a dead simple
task we train them to go Kung Fu
task we train them to go Kung Fu
Fighting is good
have you got them purchasing upgrades
have you got them purchasing upgrades
man there no freaking upgrades there's
man there no freaking upgrades there's
no shop system in this thing at the
no shop system in this thing at the
moment let's get them actually playing
moment let's get them actually playing
the game first all
the game first all
right
right
Jesus what's a man got to do solo Dev
Jesus what's a man got to do solo Dev
all of DOTA in a we
all of DOTA in a we
weak how about this this looks clean
weak how about this this looks clean
enough
right
fo everyone Rush
fo everyone Rush
mid
ha looks
good everyone feed mid
that's pretty
cool see you
cool see you
bet I'm going to uh keep on this for
bet I'm going to uh keep on this for
another hour or so and then go get
another hour or so and then go get
dinner this nice though
how we feeling about the tower colors I
how we feeling about the tower colors I
think they're nice
right would be cool to have a 3D visz of
right would be cool to have a 3D visz of
it I mean we've got you've seen the
it I mean we've got you've seen the
human playable one
right we can play we can play dodo with
right we can play we can play dodo with
uh with the Bots
uh with the Bots
now kind of
whoop key error
whoop key error
five what's supposed to be on
five what's supposed to be on
five Tower or
five Tower or
something well we'll have to figure that
out yeah we have it it is actually human
out yeah we have it it is actually human
playable and you you're going to be able
playable and you you're going to be able
to play it alongside the
to play it alongside the
Bots what was the fix oh man
Bots what was the fix oh man
tree like the stream title could not be
tree like the stream title could not be
more accurate we actually we found some
more accurate we actually we found some
demons so I was passing a view of the
demons so I was passing a view of the
grid the environment grid to the Sea uh
grid the environment grid to the Sea uh
to the SE process for each environment
to the SE process for each environment
and like each successive environment was
and like each successive environment was
adding ghost agents into it that were're
adding ghost agents into it that were're
getting reward or something weird so
getting reward or something weird so
like when I visualize the first
like when I visualize the first
environment for rendering it looks fine
environment for rendering it looks fine
but then like with the every environment
but then like with the every environment
after that has more and more ghosts that
after that has more and more ghosts that
are just generating garbage reward data
are just generating garbage reward data
and like eventually we got this weird
and like eventually we got this weird
ass training run where like they were
ass training run where like they were
going back and forth in a direction that
going back and forth in a direction that
I noticed shouldn't be generating any
I noticed shouldn't be generating any
reward because they were going
reward because they were going
perpendicular to the ancient like they
perpendicular to the ancient like they
were going on the DI the off diagonal
were going on the DI the off diagonal
and it was showing High reward so I was
and it was showing High reward so I was
like okay something's totally wrong here
like okay something's totally wrong here
so I ran it with one environment and it
so I ran it with one environment and it
gets no reward like huh okay that's
gets no reward like huh okay that's
weird the training run says it gets
weird the training run says it gets
reward but it doesn't so then I ran it
reward but it doesn't so then I ran it
with more environment and it's getting
with more environment and it's getting
reward I say oh okay so then there's
reward I say oh okay so then there's
something weird with like the multi-
something weird with like the multi-
environment code and then like I made a
environment code and then like I made a
couple changes by mistake and like the
couple changes by mistake and like the
reward went back so then I like I kind
reward went back so then I like I kind
of managed to figure it out from there
of managed to figure it out from there
it was
it was
ridiculous hell of a day but yo tree
ridiculous hell of a day but yo tree
we've got um we've got like proper
we've got um we've got like proper
observation now as well they can see uh
observation now as well they can see uh
they can see the type of all nearby
they can see the type of all nearby
agents they can distinguish enemy hero
agents they can distinguish enemy hero
types they can uh see nearby Health mana
types they can uh see nearby Health mana
and level of agents and they can see
and level of agents and they can see
pretty much all of their own stats like
pretty much all of their own stats like
full set of their own stats
full set of their own stats
so we got some cool stuff
so we got some cool stuff
now we got some real cool stuff
now we got some real cool stuff
now yeah for
yo I got a cool
idea do we want to
do I could
do I could
change I don't know if I want to do Lane
change I don't know if I want to do Lane
assignment rewards as the
thing I don't know if I want to do Lane
thing I don't know if I want to do Lane
assignment
rewards onto some features
rewards onto some features
Maybe we're doing some
thinking let's see if we don't have to
thinking let's see if we don't have to
do resets
do resets
anymore I bet you we don't have to do
anymore I bet you we don't have to do
resets anymore
so this garbage should be able to go
away let's try this
debugging rewards from
debugging rewards from
ghosts yeah dude
it's like it's not PhD doctor philosophy
it's like it's not PhD doctor philosophy
it's like Paranormal heuristic
detective I got to find myself like a
detective I got to find myself like a
fun title that's like PhD and ials or
fun title that's like PhD and ials or
something cuz this is like freaking
something cuz this is like freaking
straight up RL don't need a doctor it
straight up RL don't need a doctor it
needs a
needs a
priest yeah instead of a moth in the
priest yeah instead of a moth in the
core memory it's ghost it's freaking
core memory it's ghost it's freaking
ghosts and demons and
ghosts and demons and
[ __ ]
[ __ ]
H is rough
but I got to say we got these real nice
but I got to say we got these real nice
stable train curves now and um oh yeah
stable train curves now and um oh yeah
it took a while to get it to actually
it took a while to get it to actually
train even after that until I like up
train even after that until I like up
the batch size massively I think we've
the batch size massively I think we've
got do we have like 256 or 500k batch
got do we have like 256 or 500k batch
size now yeah we got big
size now yeah we got big
batch we got big
batch and I know exactly why as well and
batch and I know exactly why as well and
it's not the reason that people
think you have to go compute the
think you have to go compute the
effective you have to go compute the
effective you have to go compute the
average number of samples that are
average number of samples that are
obtained per
obtained per
environment 30k before yeah you have to
environment 30k before yeah you have to
like go compute the expected number of
like go compute the expected number of
samples obtained per environment per
samples obtained per environment per
batch and make sure that that number is
batch and make sure that that number is
long enough to compute generalized
long enough to compute generalized
Advantage estimation over it's wonky so
Advantage estimation over it's wonky so
essentially like just by virtue of the
essentially like just by virtue of the
hardware scaling I have to run I'm
hardware scaling I have to run I'm
running 1,600 copies of this game right
running 1,600 copies of this game right
now on uh like eight CPU cores total and
now on uh like eight CPU cores total and
in order to actually make
in order to actually make
that work because I have so many copies
that work because I have so many copies
of the environment and in order to get
of the environment and in order to get
long enough sections of data from each
long enough sections of data from each
environment in every batch you need like
environment in every batch you need like
500k batch it's nutty
we getting some dire level mean increase
we getting some dire level mean increase
radiant
levels next thing is going to be to
levels next thing is going to be to
start logging uh rewards
start logging uh rewards
separately we're going to start we're
separately we're going to start we're
going to separate all the reward
going to separate all the reward
components out it's going to be cool and
components out it's going to be cool and
then we're going to have um yeah we're
then we're going to have um yeah we're
going to have separately logged reward
going to have separately logged reward
components would be nice
would you still need huge batches with
would you still need huge batches with
hypers sweeps yeah we
hypers sweeps yeah we
would we'll need the huge batches with
would we'll need the huge batches with
Hyper pram sweeps too we're going to
Hyper pram sweeps too we're going to
need the huge batches with
everything we'll just have 1 million
everything we'll just have 1 million
batch
batch
size yes
okay reward going down a
little levels are going up
though more dat more no not more data
though more dat more no not more data
more
DOTA now I want to play some DOTA and
DOTA now I want to play some DOTA and
and I know if I boot that game up I'm
and I know if I boot that game up I'm
not finishing this project so I can't
not finishing this project so I can't
boot that game
up straight up banned myself from
up straight up banned myself from
playing anything like I allow myself to
playing anything like I allow myself to
play slay the spire on my phone once in
play slay the spire on my phone once in
a while that's literally
a while that's literally
it swear games are
heroin it's easier to quit drinking than
heroin it's easier to quit drinking than
it was to quit playing RuneScape so
much oh there it goes it figured
much oh there it goes it figured
something
out I also had to after over 404,000
out I also had to after over 404,000
little open TTD yeah yeah yeah
0.04 where's our reward freaking
0.04 where's our reward freaking
curve oh there it goes boom RL
curve oh there it goes boom RL
learned RL found a
way RL
way RL
stons invest now
take full advantage of the RL
stons that is some reward
I swear osrs is like uniquely heroin as
I swear osrs is like uniquely heroin as
a
game you know it was the inspiration for
game you know it was the inspiration for
like my PhD work on neural MMO that was
like my PhD work on neural MMO that was
the original inspiration I got it from
the original inspiration I got it from
[Music]
osrs yeah I'll go spend seven years
osrs yeah I'll go spend seven years
implementing this thing that sounds like
implementing this thing that sounds like
a reasonable thing to do
all right how are the uh the models
doing and at 0.1% the cost of alphastar
doing and at 0.1% the cost of alphastar
this is less than 0.1% the cost of alpha
this is less than 0.1% the cost of alpha
star this is probably less than
star this is probably less than
0.01% the cost of alpha star mate
0.01% the cost of alpha star mate
nothing osr yeah
oops not this one we want the uh the
oops not this one we want the uh the
highle
render rib
[Music]
[Music]
render this guy's got to get lazy there
render this guy's got to get lazy there
we
go fight
do like to do their Rush
mid oh wait this isn't the full thing
mid oh wait this isn't the full thing
because this one isn't updated hold
on e
to the
to the
Moon buffer to the Moon
Moon buffer to the Moon
you too can help puffer go to the
you too can help puffer go to the
Moon all you have to do is Star
Moon all you have to do is Star
github.com Puffer a/p puffer Li to send
github.com Puffer a/p puffer Li to send
the puffer to the Moon helps me out a
ton H they learned that they can cut
ton H they learned that they can cut
through they learned that they can
through they learned that they can
glitch through right
here let some survival a little
here let some survival a little
longer that's
funny oh I'm dancing in your
fountain star and followed your stuff
fountain star and followed your stuff
why thank
why thank
you that helps me out a whole
ton so I imagine this is right here is
ton so I imagine this is right here is
probably what is
probably what is
um what's screwing up the learning a
bit they're literally in the
fountain we'll put the um
we'll put the the reset thing
back we'll put it back for now at like a
back we'll put it back for now at like a
much lower rate
we'll do like
1024 that ought to help
I remember like one of the first games
I remember like one of the first games
of DOTA I played I managed to feed
of DOTA I played I managed to feed
somebody like six levels and I was like
somebody like six levels and I was like
what you this is a game where if you
what you this is a game where if you
don't know what you're doing you can
don't know what you're doing you can
literally turn the opponent into a
God leave ultimate rage bait
game thank you for the puffer star we've
game thank you for the puffer star we've
got it's been going real
well we're up now to 684 which is
awesome puffer stons are
good
good
okay DOTA is super unforgiving even more
okay DOTA is super unforgiving even more
than League yeah
cool game though okay uh we're going to
cool game though okay uh we're going to
show you off some more
show you off some more
Tech want to do some more struct
Tech want to do some more struct
Shenanigans we're going to do some more
Shenanigans we're going to do some more
stru
Shenanigans upper
all right C type def struct entity
yeah oh we don't use this
yeah oh we don't use this
[ __ ] so this can be a c def not a c type
[ __ ] so this can be a c def not a c type
Def
what else do we give it float uh we gave
what else do we give it float uh we gave
it XP we gave it a death
it XP we gave it a death
reward I played
reward I played
Luna I really liked uh Lena when I
Luna I really liked uh Lena when I
played DotA
you I would buy it's such a dumb build
you I would buy it's such a dumb build
but I would buy that stupid staff that
but I would buy that stupid staff that
basically gives you a second
basically gives you a second
ultimate and just like double burst
ultimate and just like double burst
people but the thing is I was bad enough
people but the thing is I was bad enough
that I would just like screw up sausage
that I would just like screw up sausage
fingered like miss all my buttons and
fingered like miss all my buttons and
just
die and people would be like dude press
die and people would be like dude press
your buttons like I can't I can't press
your buttons like I can't I can't press
the buttons fast enough
I just I can't press
I just I can't press
them like why didn't you ult cuz cuz
them like why didn't you ult cuz cuz
he's stunned me before I could all cuz
he's stunned me before I could all cuz
cuz he was he's like I don't know 14 and
cuz he was he's like I don't know 14 and
on like his fifth Red
Bull good times
dude he's just I can't do anything he's
dude he's just I can't do anything he's
14 he's on his fifth Red Bull what do
14 he's on his fifth Red Bull what do
you want from
me he's got those twitch reactions from
me he's got those twitch reactions from
like swiping Tik Tok so
much e
reactions are gone at
reactions are gone at
ah you know that's the thing I really
ah you know that's the thing I really
liked with old school was like you
liked with old school was like you
didn't have to have super fast reactions
didn't have to have super fast reactions
you just had to have fast like Fast
you just had to have fast like Fast
movements with good muscle memory you
movements with good muscle memory you
know like if you can one tick a
know like if you can one tick a
four-way which is always in the same
four-way which is always in the same
position every time you're
position every time you're
good that took me a long time to be able
good that took me a long time to be able
to do as
well but the
well but the
reactions I don't know I I peaked like
reactions I don't know I I peaked like
silver in every other game I've touched
silver in every other game I've touched
like and actually even like in triat
like and actually even like in triat
like peaked silver and like valerent and
like peaked silver and like valerent and
like everything
like everything
else I didn't play enough league but I
else I didn't play enough league but I
have bronze there was ridiculous
I don't know there's no decision making
I don't know there's no decision making
involved when you can't hit the
involved when you can't hit the
broadside of the bar and if you just
broadside of the bar and if you just
miss you just miss it's like well you
miss you just miss it's like well you
should have like you should have like
should have like you should have like
made got a better angle it's like I
made got a better angle it's like I
would have still
missed I'm just that bad I don't know
missed I'm just that bad I don't know
what to tell
[Music]
you oh sorry s
yeah it's a lot more um well that's a
yeah it's a lot more um well that's a
freaking awesome
Game Stop talk I gotta stop talking
Game Stop talk I gotta stop talking
about before I want to play it and then
about before I want to play it and then
I stop deving stuff I have stuff to
I stop deving stuff I have stuff to
build got to feed the
puffer uh so what we're going to do now
puffer uh so what we're going to do now
is
we're going to pull what's called a Pro
we're going to pull what's called a Pro
Gamer move and make the reward destruct
Gamer move and make the reward destruct
so that we can get access to the
so that we can get access to the
individual reward components in the
individual reward components in the
python
python
code what are you adding here I'm making
code what are you adding here I'm making
I'm breaking up the reward into
I'm breaking up the reward into
components so that we can get the
components so that we can get the
components of the reward in here so we
components of the reward in here so we
can log them to w b to see actually what
can log them to w b to see actually what
is going on and if they learn to fight
is going on and if they learn to fight
and stuff okay and then we're gon to add
and stuff okay and then we're gon to add
a bunch more logging and stuff and then
a bunch more logging and stuff and then
we're going to balance stuff and we're
we're going to balance stuff and we're
going to like change like tweak things
going to like change like tweak things
from there but it's like metrics
from there but it's like metrics
visibility right balance lots of lots of
visibility right balance lots of lots of
things from
there we also got to put skills back in
there we also got to put skills back in
because I disabled the skills while I
because I disabled the skills while I
was trying to
debug T type
team spirit reward for assist and
team spirit reward for assist and
grouping I think yeah yeah yeah that was
grouping I think yeah yeah yeah that was
pretty
major that's for down the line
major that's for down the line
though once we manage to get him to do
though once we manage to get him to do
something reasonable to
something reasonable to
start e
okay bro is it working now is there an
okay bro is it working now is there an
agent that gets good
agent that gets good
reward oh God I got to tell the demon
reward oh God I got to tell the demon
story
story
again yeah so will um the thing that
again yeah so will um the thing that
wasn't working so when I rendered the
wasn't working so when I rendered the
first environment in the stack it looked
first environment in the stack it looked
fine everything was good but the thing
fine everything was good but the thing
was you see when I was constructing all
was you see when I was constructing all
the environments I was passing a
the environments I was passing a
reference by mistake to the main grid of
reference by mistake to the main grid of
the EnV so what was happening is that
the EnV so what was happening is that
each subsequent environment was adding
each subsequent environment was adding
more and more ghost agents into the
more and more ghost agents into the
simulation and these ghost agents Were
simulation and these ghost agents Were
Somehow triggering rewards so when I
Somehow triggering rewards so when I
would look at it I had a re perfectly
would look at it I had a re perfectly
reasonable environment but you actually
reasonable environment but you actually
had this like ghost infested Insanity
had this like ghost infested Insanity
environment that was corrupting all of
environment that was corrupting all of
the training data and as soon as I fixed
the training data and as soon as I fixed
that and increased the batch size you
that and increased the batch size you
get perfect learning curves and the
get perfect learning curves and the
thing actually freaking
works and well I have to rebuild this
works and well I have to rebuild this
thing
but demons
ha the agents shall fly to each
ha the agents shall fly to each
other one or two of them get stuck
other one or two of them get stuck
occasionally
occasionally
but they run it down
but they run it down
mid there you
mid there you
go they run down
mid and eventually it seems that they do
mid and eventually it seems that they do
take over your
fountain yeah this is learned I just
fountain yeah this is learned I just
told them to go to the enemy ancient and
told them to go to the enemy ancient and
they learned to do it I mean it's very
they learned to do it I mean it's very
simple it's like a really really easy
simple it's like a really really easy
thing to learn but still it
works oh Towers can still be pulled I
works oh Towers can still be pulled I
didn't change that there's a there's an
didn't change that there's a there's an
ability in the game that lets you move
ability in the game that lets you move
around Towers
see I didn't make I didn't add a check
see I didn't make I didn't add a check
on the pull like the hook ability like
on the pull like the hook ability like
Pudge hook I didn't add a check to see
Pudge hook I didn't add a check to see
if you're uh trying to Target a tower
if you're uh trying to Target a tower
with that so you can just straight up
with that so you can just straight up
hook a
tower I saw it happen one time they
tower I saw it happen one time they
pulled their uh they pulled their tier
pulled their uh they pulled their tier
two all the way back to their tier three
get this in Dota ASAP yeah right yeah
get this in Dota ASAP yeah right yeah
call me up valve for all of your
call me up valve for all of your
reinforcement learning and exorcism
reinforcement learning and exorcism
[Laughter]
[Laughter]
needs how long till training on a better
needs how long till training on a better
objective it is already technically
objective it is already technically
training on a better objective there
training on a better objective there
additional reward components for Tower
additional reward components for Tower
kills XP gained dying and one or two
kills XP gained dying and one or two
other things um and I'm currently
other things um and I'm currently
exposing these so that we can graft them
exposing these so that we can graft them
independently that's what I was doing
independently that's what I was doing
before I was so rudely distracted by
demons e
so what we're doing is we're putting the
so what we're doing is we're putting the
rewards into a struct
rewards into a struct
here put the rewards into a
struct and then
it player
OBS
Rewards
e
e
e
e
e
e e
whoops do
reward forgot to add that
boom death XP distance
to that's pretty good
we'll fix this up
we'll fix this up
now cannot
assign type float
assign type float
to
reward
reward
ah we don't need to do this
did I not Define reward up top
did I not Define reward up top
somewhere or did I double Define it
no well this is [ __ ] right here so
no well this is [ __ ] right here so
this is
this is
reward reward.
reward reward.
death that's
better get reward of PID
better get reward of PID
storing see
storing see
derivative I've PID defined don't
derivative I've PID defined don't
I I totally have PID
defined well let's fix these first these
defined well let's fix these first these
are
easy
for
e e
okay now we have 26 values here for
reward e
get reward is not
defined Undeclared name not built-in get
defined Undeclared name not built-in get
reward cdef reward star get
reward cdef reward star get
reward what
it's right
there
there
[Music]
so C def
so C def
struct
struct
reward the death struct
player well
cannot
convert so basically all of these are
convert so basically all of these are
like we don't know what a reward is
see def reward star get
see def reward star get
reward turn and self. rewards of
PID self. rewards equal
PID self. rewards equal
rewards
entities
yeah I don't understand what the hell is
wrong Undeclared name not built-in get
reward I literally have it declared
reward I literally have it declared
right
here oh well this one's easy this is
here oh well this one's easy this is
self Dot
there we
go let's hurry up and make this work so
go let's hurry up and make this work so
that we can run an overnight
that we can run an overnight
sweep that' be
nice we just train 10 trillion steps of
nice we just train 10 trillion steps of
DOTA that' be nice and then we uh we
DOTA that' be nice and then we uh we
solve
solve
it out of
it out of
bounds on Buffer axis zero
get reward of
PID
H
H
print well
zero
zero
out of bound on axis
out of bound on axis
zero well that is
zero well that is
ridiculous that does not make any sense
ridiculous that does not make any sense
they're all zero through
nine so assumedly something is getting
nine so assumedly something is getting
passed
incorrectly num MS
this is num agents not num
M's invalid input
we have 26 here I
we have 26 here I
believe
26 26 is what we
have seg vault
really seg vault
second FS before
second FS before
step
okay like faults before that
np. zeros oh wait hold
np. zeros oh wait hold
on yeah yeah I just did this
wrong
e
e e
pigmentation vault
okay we don't say fault
okay we don't say fault
there though it's possible if I'm
there though it's possible if I'm
putting the uh the wrong data shape in
putting the uh the wrong data shape in
that would still be a seg bolt
and not assign float
pointer for
but before this even happens
I think that's the seg fault right
there seg faults are
there seg faults are
dumb you shouldn't
exist I wish we had czig that would be
exist I wish we had czig that would be
cool or Pi
cool or Pi
Zig be fun
I might take some time to play around
I might take some time to play around
with Zig that might be fun but the thing
with Zig that might be fun but the thing
is without this uh numpy view slicing
is without this uh numpy view slicing
thing nothing's going to be the same you
thing nothing's going to be the same you
really need the numpy view
thing all right invalid
tensors why is I got invalid tensors huh
briefly looked at
Zig is it like see but with things that
Zig is it like see but with things that
make you not
make you not
cry like uh I saw that they had a really
cry like uh I saw that they had a really
sweet like debug memory allocator that
sweet like debug memory allocator that
does all the bounce checking and stuff
does all the bounce checking and stuff
and then you just swap it when you're
and then you just swap it when you're
like when you're set you just swap it
like when you're set you just swap it
out for the normal one that's fast
out for the normal one that's fast
that's the type of stuff I want
it's pretty much what it
it's pretty much what it
is yeah that's kind of what I want um of
is yeah that's kind of what I want um of
like trendy programming languages that
like trendy programming languages that
have been going around I have absolutely
have been going around I have absolutely
no interest in Rust it's [ __ ] stupid
no interest in Rust it's [ __ ] stupid
for this um go is
for this um go is
like maybe but I don't I haven't seen
like maybe but I don't I haven't seen
perect on it I don't think it's as fast
perect on it I don't think it's as fast
as like be C from what I've seen and Zig
as like be C from what I've seen and Zig
looks kind of cool zig's the only one
looks kind of cool zig's the only one
that looks kind of
cool but then it's like the thing is you
cool but then it's like the thing is you
need
need
the like the python integration here is
the like the python integration here is
so crucial
Odin is too much of
Odin is too much of
itself
itself
yeah haven't seen too much of it
but I don't want more people Reinventing
but I don't want more people Reinventing
the wheel I just want like it's very
the wheel I just want like it's very
simple I want C with stuff I can compile
simple I want C with stuff I can compile
with that makes it not throw garbage
with that makes it not throw garbage
errors that's all I want they give me
errors that's all I want they give me
fast compile mode safe compile mode
fast compile mode safe compile mode
that's all I
that's all I
want ideally I wouldn't even have to
want ideally I wouldn't even have to
like put in the code you know memory
like put in the code you know memory
like area allocator or something I
like area allocator or something I
should just be able to compile it with a
should just be able to compile it with a
different allocator back
different allocator back
end that would be very
nice that's where they want to take
nice that's where they want to take
Zig if they do that well I still can't
Zig if they do that well I still can't
say I'd use it because
say I'd use it because
like the memory view thing like the
like the memory view thing like the
killer app for scyon is not needing
killer app for scyon is not needing
pbind and having um memory
pbind and having um memory
views those two things are just god tier
so like having easy ways to share memory
so like having easy ways to share memory
even structured memory like entity
even structured memory like entity
component systems between Python and c
component systems between Python and c
is awesome and also in this like I don't
is awesome and also in this like I don't
even have to write pure C right or stuff
even have to write pure C right or stuff
that compiles to Pure C I can write slow
that compiles to Pure C I can write slow
and it functions in
and it functions in
Python I'm allowed to do that
let me fix the uh the AC real quick and
let me fix the uh the AC real quick and
then we're going to finish this
of course you know they have to call
of course you know they have to call
they need a name for people that uh that
they need a name for people that uh that
write Zig I suggested Zig zons
510 that
510 that
flat invalid input tensor side did I
flat invalid input tensor side did I
just forget to change
just forget to change
something Beast to reimplement in Zig as
something Beast to reimplement in Zig as
is yeah yeah like it's
is yeah yeah like it's
not yeah it's really not worth it at the
not yeah it's really not worth it at the
moment like the scyon is Best in Class
moment like the scyon is Best in Class
by a
by a
mile for Sim Dev like this it's Best in
mile for Sim Dev like this it's Best in
Class by a
mile like you know that there are no
mile like you know that there are no
malx in this ithon right now there's
malx in this ithon right now there's
literally no possibility for memory
literally no possibility for memory
leaks the errors I get are out of bounds
leaks the errors I get are out of bounds
things because like it's missing a few
things because like it's missing a few
compile Flags um but all the memory
compile Flags um but all the memory
allocation is done from numpy arrays in
allocation is done from numpy arrays in
Python it's easier than CNC yeah yeah
Python it's easier than CNC yeah yeah
yeah it's like python almost python ease
yeah it's like python almost python ease
and raw C performance it com it
and raw C performance it com it
transpiles to C and compiles from
there it's free really
there it's free really
good invalid input tensor shape why is
good invalid input tensor shape why is
there invalid input tensor
shape this is
stupid player
stupid player
Oben did I Cal wrong
this looks fine what the [ __ ]
oh it's because I'm trying to load a
oh it's because I'm trying to load a
model that's
dumb yeah
bunk tried to
bunk tried to
add all right I figured that it would
add all right I figured that it would
get stuck there though that's fine
why is this 200 by
e e
nice
e
e
e
e e
could not broadcast
yeah that's going to be a problem isn't
yeah that's going to be a problem isn't
it
um slightly obnoxious
really just to some shape
really just to some shape
errors different ways of they
errors different ways of they
representing
representing
this num
this num
agents do like this
maybe e
out of bounds AIS zero
on attack yes this is going to be an
on attack yes this is going to be an
error it's easily
resolved
um e
what's wrong with
what's wrong with
this declare it after it's
used
well that's a little
awkward I guess technically what you can
awkward I guess technically what you can
do is you can put this up
do is you can put this up
here
here
right and uh now that's a null
right and uh now that's a null
pointer what you do is you
pointer what you do is you
do reward is going to
be target.
PID not initializing yeah so the thing
PID not initializing yeah so the thing
is it's one kind of awkward thing with
is it's one kind of awkward thing with
scon is you can't declare variables
scon is you can't declare variables
inside of conditionals or Loops so I
inside of conditionals or Loops so I
have to declare it up here and then
have to declare it up here and then
initialize it down here
strs work the same as they do in uh in
C so this is a pointer I've declared a
C so this is a pointer I've declared a
pointer and now here I get the pointer
pointer and now here I get the pointer
to the
to the
reward and now I set
reward and now I set
it and then down here right this won't
it and then down here right this won't
have been initialized so I have to do
have been initialized so I have to do
reward is going to be get reward of this
reward is going to be get reward of this
time the player PID and then I can set
time the player PID and then I can set
that stuff there so I reuse the same
that stuff there so I reuse the same
pointer
I love strs strs are great I wish python
I love strs strs are great I wish python
had
had
strs it's one of the worst things about
strs it's one of the worst things about
the language is not having good
strs this looks good though we've gotten
strs this looks good though we've gotten
uh a bunch of additional stuff
a bunch of additional stuff
here strs are great type defs are
here strs are great type defs are
great type def
great type def
strs I do not know the difference
strs I do not know the difference
between a struct and a type def struct
between a struct and a type def struct
actually I actually do not know the
actually I actually do not know the
difference
uh this is nice
so now we get to rerun that experiment
so now we get to rerun that experiment
from
before e
okay
look at
that we get
that we get
graphs isn't that
graphs isn't that
nice all these things are getting
nice all these things are getting
graphed we've got a four component
graphed we've got a four component
reward it's going to be
reward it's going to be
graphed makes the struct an anonymous
graphed makes the struct an anonymous
type so you don't have to just click the
type so you don't have to just click the
struct type when using the
struct type when using the
struct
huh and that
huh and that
works I guess that
works yeah I could see how that would be
works yeah I could see how that would be
useful
oh I guess I didn't notice that because
oh I guess I didn't notice that because
that's how it works by default in uh in
that's how it works by default in uh in
scon yeah in scon you don't have to type
scon yeah in scon you don't have to type
the Redundant uh stru keyword anyways
yeah I'd forgotten about
yeah I'd forgotten about
that from see okay I didn't know there
that from see okay I didn't know there
was a way around that that's
cool redundant keywords are just dumb I
cool redundant keywords are just dumb I
swear like languages with tons of
swear like languages with tons of
redundant keywords are just bad almost
redundant keywords are just bad almost
by default because they just clutter
by default because they just clutter
stuff
the other thing that drives me
the other thing that drives me
absolutely
absolutely
nuts um
nuts um
is typed python code drives me
is typed python code drives me
absolutely insane because it's like
absolutely insane because it's like
you're using the slowest language in
you're using the slowest language in
existence where the main strength is
existence where the main strength is
being really succinct and versatile and
being really succinct and versatile and
you're adding all the bloat of a
you're adding all the bloat of a
strongly typed language with Zero
strongly typed language with Zero
Performance benefits whatsoever
that's my personal just
that's my personal just
like ride me
nuts I like absolutely hate having to
nuts I like absolutely hate having to
read typed python
code win 32 API confusion
code win 32 API confusion
jeez no win 32 API please
look at
look at
that we sort
these look at
that we do some uh we take some
that we do some uh we take some
Towers we go get some
Towers we go get some
ancient we get some Rewards
we do a little less uh we do some dying
we do a little less uh we do some dying
I guess we get some XP
yeah typing in Python doesn't even no it
yeah typing in Python doesn't even no it
doesn't and in fact it doesn't even
doesn't and in fact it doesn't even
here's the thing not only does it not
here's the thing not only does it not
increase performance the types aren't
increase performance the types aren't
even enforced so they literally they do
nothing and it's worse than a comment
nothing and it's worse than a comment
that's like it's even worse than a
that's like it's even worse than a
comment on the type of the variable
comment on the type of the variable
because most like many people don't even
because most like many people don't even
know that they're not
enforced the only way that they're
enforced the only way that they're
enforced is if you run mypie in strict
enforced is if you run mypie in strict
mode which nobody does and uh even if
mode which nobody does and uh even if
you do that you can still technically
you do that you can still technically
leak if the entire code base well you
leak if the entire code base well you
have to type the entire code base
have to type the entire code base
essentially strict like you can't have
essentially strict like you can't have
mixed typed and not typed it's just
mixed typed and not typed it's just
horrible it's just the worst stuff
imaginable like if you're going to type
imaginable like if you're going to type
your python you could write it in scyon
your python you could write it in scyon
it would be identical the types would be
it would be identical the types would be
enforced and your code would be 100
enforced and your code would be 100
times
faster literally that easy
I don't know why I don't use sion's
I don't know why I don't use sion's
awesome there a few little rough edges
awesome there a few little rough edges
here and there but it's so
good I mean it's the thing that's really
good I mean it's the thing that's really
funny to me about this stack That should
funny to me about this stack That should
kind of show you like I wrote The Snake
kind of show you like I wrote The Snake
Sim in a week it's 450 lines of code it
Sim in a week it's 450 lines of code it
runs at like I think 14 million maybe 12
runs at like I think 14 million maybe 12
million steps per second something like
million steps per second something like
that single thr Ed um you can run you
that single thr Ed um you can run you
can run in like uh you can run like
can run in like uh you can run like
4,000 One agent snake environment or one
4,000 One agent snake environment or one
4096 agent snake environment you can go
4096 agent snake environment you can go
bigger than that if you want there are
bigger than that if you want there are
zero dynamic memory
zero dynamic memory
allocations
allocations
um and there's no pie bind and
um and there's no pie bind and
everything works just perfectly with
everything works just perfectly with
python
all right
all right
cool this was the first game win so we
cool this was the first game win so we
took a little bit of a Down Spike after
took a little bit of a Down Spike after
the first game win but look at
that and then we can go grab our nice
that and then we can go grab our nice
model
training agents to knock down Towers
syon is very
syon is very
good syon is very very
good syon is very very
good
like let's
watch oh look at that they Rush
watch oh look at that they Rush
mid
mid
ha and they they farmed the some agents
ha and they they farmed the some agents
yeah look at that that's
clean it's funny they they don't bother
clean it's funny they they don't bother
taking the
towers I wonder what happens if we train
towers I wonder what happens if we train
without the uh the objective to go
without the uh the objective to go
mid if they'll like actually learn to
mid if they'll like actually learn to
play the
play the
game it's really freaking cool either
game it's really freaking cool either
way
we should uh let's let's post this gift
we should uh let's let's post this gift
and let's get a sweep
going not getting stuck yeah
going not getting stuck yeah
solid Pro
mid better than some of my
mid better than some of my
teammates what people
teammates what people
say yeah let's see
[Music]
uh
e
e e
very
nice the response to this Project's been
nice the response to this Project's been
decent so far it hasn't quite blown up
decent so far it hasn't quite blown up
on Twitter just yet it's gotten some
on Twitter just yet it's gotten some
decent some decent views nothing
crazy really needed to like hit Twitter
crazy really needed to like hit Twitter
proper I mean if people like keep seeing
proper I mean if people like keep seeing
it and I keep getting people looking at
it and I keep getting people looking at
this project I'll keep working on it
this project I'll keep working on it
right we're at least going to make a
right we're at least going to make a
stable environment out of it that have
stable environment out of it that have
some def decent policies right but like
some def decent policies right but like
whether I go past like yeah the agents
whether I go past like yeah the agents
all kind of go mid they fight they look
all kind of go mid they fight they look
reasonable in the way way they fight uh
reasonable in the way way they fight uh
they like press their skill buttons and
they like press their skill buttons and
stuff like whether I go past that that's
stuff like whether I go past that that's
going to depend on uh engagement with
going to depend on uh engagement with
the project pretty
much
e e
oh I already have this perfect
oh I literally already set this up
oh I literally already set this up
perfect
let's see if this
works
e e
invalid hyper pram configuration
invalid hyper pram configuration
Train Sweet parameters
oh what I'm trying to do is set up a
oh what I'm trying to do is set up a
nice hyper pram sweep before dinner
nice hyper pram sweep before dinner
there you
there you
go that seems to
work okay nice so
um
yeah t-x
so that's pretty cool we've actually got
so that's pretty cool we've actually got
a a hyper parameter sweep set up on this
a a hyper parameter sweep set up on this
now hopefully it does something cool for
us possible I don't have it set up
us possible I don't have it set up
perfectly right but um we literally have
perfectly right but um we literally have
the best hper parameter algorithm out
the best hper parameter algorithm out
there now running against this
there now running against this
environment and we're going to get all
environment and we're going to get all
the results nicely
aggregated actually that's already kind
aggregated actually that's already kind
of working fast isn't it
of working fast isn't it
huh
huh
so I'll explain this and then I'm going
so I'll explain this and then I'm going
to go to
dinner what we have
dinner what we have
here oops not not this one it's in
carbs carbs
carbs carbs
sweeps okay so what we have here is a
sweeps okay so what we have here is a
carbs hyperparameter sweep we get all
carbs hyperparameter sweep we get all
our usual metrics but we're also going
our usual metrics but we're also going
to get the summary metrics and this is
to get the summary metrics and this is
going to do Paro optimal basian
going to do Paro optimal basian
optimization over hyperparameter space
optimization over hyperparameter space
it's going to do a 100 different
it's going to do a 100 different
experiments
experiments
um or however many I until I get bored
um or however many I until I get bored
or whatever and it's going to give us
or whatever and it's going to give us
all the results nicely and we're going
all the results nicely and we're going
to be able to see if it learns anything
to be able to see if it learns anything
cool I think it's pretty unlikely it
cool I think it's pretty unlikely it
learns anything all that cool cuz we
learns anything all that cool cuz we
give it a pretty big reward for rushing
give it a pretty big reward for rushing
down mid so really all I'm hoping to get
down mid so really all I'm hoping to get
out of this that like is that we get
out of this that like is that we get
like really nice stable hyper parameters
like really nice stable hyper parameters
that learn to do this very quickly and
that learn to do this very quickly and
consistently um but who knows maybe they
consistently um but who knows maybe they
solve DotA
I think actually um paradoxically I
I think actually um paradoxically I
think it's going to do more once we get
think it's going to do more once we get
good hyper parameters I think it's going
good hyper parameters I think it's going
to learn more once we remove or
to learn more once we remove or
substantially decrease the go to ancient
substantially decrease the go to ancient
reward because that reward is just like
reward because that reward is just like
preventing it from focusing on farming
preventing it from focusing on farming
or like learning anything else
or like learning anything else
yeah but hey this is going to be cool
yeah but hey this is going to be cool
and this is like one of those things
and this is like one of those things
that open AI could not do this
that open AI could not do this
because like their experiments did not
because like their experiments did not
run fast enough to do this right like
run fast enough to do this right like
they could do some hyper parameter
they could do some hyper parameter
tuning yeah but they couldn't do like
tuning yeah but they couldn't do like
okay we're going to just queue up a
okay we're going to just queue up a
hundred jobs right and try to like tune
hundred jobs right and try to like tune
that way that's really freaking hard to
that way that's really freaking hard to
do
do
so we can do
so we can do
it and uh maybe we get some cool stuff
it and uh maybe we get some cool stuff
out of it I will be back streaming this
out of it I will be back streaming this
stuff about the usual around noonish
stuff about the usual around noonish
again tomorrow
again tomorrow
Pacific um we're going to continue on
Pacific um we're going to continue on
this we're gonna hopefully keep getting
this we're gonna hopefully keep getting
the models better it's really now at
the models better it's really now at
this point it's more of a data problem
this point it's more of a data problem
right we just have to get better
right we just have to get better
training data so like better uh
training data so like better uh
observations maybe a few small Network
observations maybe a few small Network
tweaks hyperparameter changes and then
tweaks hyperparameter changes and then
we're going to clean up and balance the
we're going to clean up and balance the
environment a whole bunch and start
environment a whole bunch and start
reintroducing additional features like
reintroducing additional features like
reintroducing the skills you know
reintroducing the skills you know
reintroducing
reintroducing
uh various other things and also
uh various other things and also
starting to clean up the environment and
starting to clean up the environment and
hopefully catching a bunch of bugs along
hopefully catching a bunch of bugs along
the way that is the goal anyways uh
the way that is the goal anyways uh
thank you folks for tuning in one last
thank you folks for tuning in one last
appeal if you haven't already please go
appeal if you haven't already please go
ahead and give this repository a Star
ahead and give this repository a Star
Puffer ipuff lib all the stuff I do is
Puffer ipuff lib all the stuff I do is
open source um there is currently no
open source um there is currently no
revenue from any of this and it helps me
revenue from any of this and it helps me
a whole bunch to get this project on the
a whole bunch to get this project on the
board just like the stars the Twitter
board just like the stars the Twitter
follow like the retweets on stuff it
follow like the retweets on stuff it
really really helps so that's all I ask
really really helps so that's all I ask
uh anyways have a nice evening and see
uh anyways have a nice evening and see
you tomorrow
