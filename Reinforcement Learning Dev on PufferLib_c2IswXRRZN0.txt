Kind: captions
Language: en
goal for the rest of today is to get the
goal for the rest of today is to get the
uh two new sample
uh two new sample
environments commented, cleaned up,
environments commented, cleaned up,
documented, and also to put a tutorial
documented, and also to put a tutorial
on our homepage for
on our homepage for
that. Do that and then we're probably
that. Do that and then we're probably
going to see if we can get some sweeps
going to see if we can get some sweeps
going on uh some of the more complicated
going on uh some of the more complicated
MS, try to stress test some things, try
MS, try to stress test some things, try
to fix some user issues like some user
to fix some user issues like some user
submitted bugs, just variety of
submitted bugs, just variety of
different things.
different things.
So, so speaking of which, Captain had
So, so speaking of which, Captain had
some uh some issues. So, let's see if
some uh some issues. So, let's see if
he's
around and helps.
We'll see if he's uh he's here. And in
We'll see if he's uh he's here. And in
the meantime, we
the meantime, we
will that over
there and uh keep on this little
there and uh keep on this little
refactor.
Okay, that gets rid of this function and
Okay, that gets rid of this function and
this function. Done.
and that's 228
and that's 228
lines including
lines including
comments. Let's see if there's anything
comments. Let's see if there's anything
in here that seems redundant.
Got our
Got our
strus. Client has a strct.
Yeah, I think that's
fine. Have the extra client strct like
fine. Have the extra client strct like
that.
I am a little bit knowledge impaired
I am a little bit knowledge impaired
real RL learner. Could you explain if
real RL learner. Could you explain if
not taking too much time what is the
not taking too much time what is the
exact issue that puffer lib is
exact issue that puffer lib is
targeting? My exam understanding the
targeting? My exam understanding the
direct goal is to sit between the
direct goal is to sit between the
environments and learning how them to
environments and learning how them to
deal with vectorization. Therefore am I
deal with vectorization. Therefore am I
misinformed? So that is what we started
misinformed? So that is what we started
off doing in puffer. So like in the
off doing in puffer. So like in the
first few months of dev that was the
first few months of dev that was the
original goal was that multipprocessing
original goal was that multipprocessing
as it is typically used is very janky.
as it is typically used is very janky.
It only supports certain types of m
It only supports certain types of m
doesn't have m native multi- aent
doesn't have m native multi- aent
support doesn't support complex
support doesn't support complex
observation and action spaces whole
observation and action spaces whole
bunch of problems and it's incredibly
bunch of problems and it's incredibly
slow. So the first thing that puffer did
slow. So the first thing that puffer did
was fix that. So we have very fast uh
was fix that. So we have very fast uh
parallel processing. It works with our
parallel processing. It works with our
own environments. It works with
own environments. It works with
gymnasium environments. It works with
gymnasium environments. It works with
petting zoo environments. It uses much
petting zoo environments. It uses much
much more optimized shared memory and a
much more optimized shared memory and a
whole bunch of other tricks uh
whole bunch of other tricks uh
asynchronous simulation, double
asynchronous simulation, double
buffering, lots of different tricks that
buffering, lots of different tricks that
are in the paper uh to make
are in the paper uh to make
vectorization up to like 10x
vectorization up to like 10x
faster. So that was the first thing we
faster. So that was the first thing we
did, but that's like we've had we have
did, but that's like we've had we have
two major releases basically since then.
two major releases basically since then.
So now we also have uh 20 ultra high
So now we also have uh 20 ultra high
performance environments written in C
performance environments written in C
that we use for all our own research
that we use for all our own research
that run at millions of steps per second
that run at millions of steps per second
and we have a trainer that actually can
and we have a trainer that actually can
train at millions of steps per second.
train at millions of steps per second.
So I mean if you compare to like the
So I mean if you compare to like the
libraries that we were trying to build
libraries that we were trying to build
infrastructure to support before like
infrastructure to support before like
SP3 and RLIB and all those we just train
SP3 and RLIB and all those we just train
a 100 to a thousand times faster now. So
a 100 to a thousand times faster now. So
we still provide these compatibility
we still provide these compatibility
tools for use with other libraries, but
tools for use with other libraries, but
really at this point you should consider
really at this point you should consider
using our tools because everything is
using our tools because everything is
just so so dramatically faster. We're
just so so dramatically faster. We're
also doing our own algorithm research
also doing our own algorithm research
now. So we have a better PO like PO with
now. So we have a better PO like PO with
additional things that is just better
additional things that is just better
across the board.
across the board.
Um we have benchmarks on all of our
Um we have benchmarks on all of our
environments. We have we're setting like
environments. We have we're setting like
soda on other people's environments. Uh
soda on other people's environments. Uh
so it's become much much more than what
so it's become much much more than what
we started essentially. It's really it's
we started essentially. It's really it's
a comprehensive effort now to just make
a comprehensive effort now to just make
reinforcement learning ultra fast, ultra
reinforcement learning ultra fast, ultra
simple and very very stable and sane.
simple and very very stable and sane.
That's what Puffer
does. And I stream all of the dev
does. And I stream all of the dev
because why not? It's all open source
because why not? It's all open source
and we're always looking to get more
and we're always looking to get more
people uh interested in contributing
people uh interested in contributing
building environments, helping us run
building environments, helping us run
experiments, helping on science side,
experiments, helping on science side,
all the different things that we do.
trainers departed from torch tracks. No,
trainers departed from torch tracks. No,
they're still implemented in uh our
they're still implemented in uh our
trainer still in torch. But the thing is
trainer still in torch. But the thing is
it's just like it's so much more
it's just like it's so much more
efficient than what was being done
efficient than what was being done
before. Like there were a half dozen
before. Like there were a half dozen
different places where people were just
different places where people were just
losing 99% of their PF in training. Um
losing 99% of their PF in training. Um
to give you an idea, right? Like the
to give you an idea, right? Like the
vast majority of training that's done in
vast majority of training that's done in
reinforcement learning, like you look at
reinforcement learning, like you look at
your average lab, a lot of them are
your average lab, a lot of them are
still on singledigit thousand step per
still on singledigit thousand step per
second trainers. Um, some of the better
second trainers. Um, some of the better
ones are now in like double digits. And
ones are now in like double digits. And
then the ones with full GPUms sometimes
then the ones with full GPUms sometimes
get triple digit like thousand. So like
get triple digit like thousand. So like
100,000 or something, but that's like a
100,000 or something, but that's like a
very very few labs with a very few
very very few labs with a very few
environments that can even do that. So
environments that can even do that. So
our training we run typically between
our training we run typically between
400,000 and 4 million steps per second
400,000 and 4 million steps per second
depending on the architecture and the um
depending on the architecture and the um
the model size. Uh a lot of our simpler
the model size. Uh a lot of our simpler
tests that we use for primary research
tests that we use for primary research
run like 2 to four million steps per
run like 2 to four million steps per
second training on one
second training on one
GPU. And our training code is simpler
GPU. And our training code is simpler
than the slower libraries that it
than the slower libraries that it
replaces. Um it works out of the box
replaces. Um it works out of the box
with multi- aent. It works out of the
with multi- aent. It works out of the
box with a whole bunch of things that
box with a whole bunch of things that
aren't even supported generally
aren't even supported generally
elsewhere. Uh the environment code is
elsewhere. Uh the environment code is
way simpler. It's way way easier to
way simpler. It's way way easier to
build environments in Puffer Lib than it
build environments in Puffer Lib than it
is in Jax, for instance, or like PyTorch
is in Jax, for instance, or like PyTorch
accelerated GPUms. Way way faster to
accelerated GPUms. Way way faster to
build our stuff because you just write
build our stuff because you just write
arbitrary code and it's fast. Um yeah,
arbitrary code and it's fast. Um yeah,
it's like a really comprehensive effort,
it's like a really comprehensive effort,
but we're not like we're not giving you
but we're not like we're not giving you
our own trainer API that's like you have
our own trainer API that's like you have
to go learn. It's just PyTorch. It's
to go learn. It's just PyTorch. It's
very vanilla pietorch. It's just that
very vanilla pietorch. It's just that
our code is way better optimized, right?
our code is way better optimized, right?
Um, we didn't come up with like a crazy
Um, we didn't come up with like a crazy
new interface. It's almost identical to
new interface. It's almost identical to
the gymnasium vector interface, just
the gymnasium vector interface, just
faster, right? So, we really try to make
faster, right? So, we really try to make
stuff just
stuff just
simple. Working through SMB now. Haven't
simple. Working through SMB now. Haven't
finished P1 yet. Notice the proper
finished P1 yet. Notice the proper
vectorization matters a lot in
vectorization matters a lot in
responsiveness. Yes, it does.
responsiveness. Yes, it does.
Multi-agent sounds like a nightmare to
Multi-agent sounds like a nightmare to
Marshall. It's not is the thing. It's
Marshall. It's not is the thing. It's
literally the way that we do it.
literally the way that we do it.
Multi-agent is no more complicated than
Multi-agent is no more complicated than
single agent at all.
single agent at all.
So, this end that I built
So, this end that I built
here actually. Can I can I eval this for
here actually. Can I can I eval this for
you?
Let me see if I can just remember which
Let me see if I can just remember which
of these checkpoints is any good. Okay,
of these checkpoints is any good. Okay,
so here's a multi-agent environment
so here's a multi-agent environment
about Puffer Libs eating stars. I built
about Puffer Libs eating stars. I built
this just as a tutorial as like a really
this just as a tutorial as like a really
simple thing to get people started. This
simple thing to get people started. This
thing trains 100 million steps in 25
thing trains 100 million steps in 25
seconds on my local GPU. Um, it worked
seconds on my local GPU. Um, it worked
instantly with our just our default
instantly with our just our default
training setup and I built this thing
training setup and I built this thing
yesterday on stream. So there was no
yesterday on stream. So there was no
code for this yesterday and now it just
code for this yesterday and now it just
works.
Welcome, sub. What's
up? So, yeah, this thing trains four
up? So, yeah, this thing trains four
million steps a second. And like it's a
million steps a second. And like it's a
very simple end, but this is really
very simple end, but this is really
really clean as far as like multi- aent
really clean as far as like multi- aent
uh multi- aent just like forging type
uh multi- aent just like forging type
behavior
goes. Yeah, the evals are on the website
goes. Yeah, the evals are on the website
as well. So, you can literally watch
as well. So, you can literally watch
agents playing the games in your
agents playing the games in your
browser.
browser.
and they're running locally as
well. I will be honest with you also I
well. I will be honest with you also I
have I've never read this on an embaro
book. I do a lot more engineering side
book. I do a lot more engineering side
stuff that really seems to matter a lot.
stuff that really seems to matter a lot.
I do some algorithm stuff to be fair. I
I do some algorithm stuff to be fair. I
occasionally we have like some algorithm
occasionally we have like some algorithm
advancements in puffer that make a big
advancements in puffer that make a big
difference
difference
but so so so much in RL is just getting
but so so so much in RL is just getting
the engineering right.
the engineering right.
The way I look at it is that there have
The way I look at it is that there have
been a lot of mathematically very smart
been a lot of mathematically very smart
people in RL. Um, that's not why the
people in RL. Um, that's not why the
field is stuck right now. That's not why
field is stuck right now. That's not why
like everything is so difficult. Because
like everything is so difficult. Because
all the code is slow.
Aren't some parts of environments
Aren't some parts of environments
inherently serial? Such as environments
inherently serial? Such as environments
that get more complicated, they will be
that get more complicated, they will be
less easily vectorized. Well, here's the
less easily vectorized. Well, here's the
thing. So, we just write stuff in C, but
thing. So, we just write stuff in C, but
we don't care if things are serial or
we don't care if things are serial or
not. It's not running on GPU. Um, our
not. It's not running on GPU. Um, our
code runs like a million steps a second.
code runs like a million steps a second.
So, we're not just like running one
So, we're not just like running one
environment per core. We're running like
environment per core. We're running like
a thousand environments per core. Some
a thousand environments per core. Some
of those are slower than others. It
of those are slower than others. It
doesn't matter because it gets amortized
doesn't matter because it gets amortized
out anyways. And like we literally end
out anyways. And like we literally end
up saturating the GPU with one or two
up saturating the GPU with one or two
CPU cores with most of our
environments. So like when you get to a
environments. So like when you get to a
certain speed threshold, it really
certain speed threshold, it really
doesn't
doesn't
matter. More headless environments.
matter. More headless environments.
Well, that's one of the things we do,
Well, that's one of the things we do,
right? like it's one of the main ways
right? like it's one of the main ways
that we scale out the open- source
that we scale out the open- source
contributor side because the easiest way
contributor side because the easiest way
to get new people into RL is say hey you
to get new people into RL is say hey you
know if you want to learn RL come up
know if you want to learn RL come up
with a new environment that you would
with a new environment that you would
think would be interesting build it and
think would be interesting build it and
go through the process of building the
go through the process of building the
environment and getting RL working on it
environment and getting RL working on it
so that you see the full end to end loop
so that you see the full end to end loop
so that's how we get a lot of these
so that's how we get a lot of these
environments contributed it's from
environments contributed it's from
people that are trying to learn RL and
people that are trying to learn RL and
are like building cool environments I
are like building cool environments I
build a few of them for myself for
build a few of them for myself for
various research purposes as well those
various research purposes as well those
are also there and and the library grows
are also there and and the library grows
that way. Um, we're also we have our
that way. Um, we're also we have our
work that we do on infrastructure and
work that we do on infrastructure and
good training. We have our own
good training. We have our own
algorithms in this next update. We're
algorithms in this next update. We're
shipping our own uh really high quality
shipping our own uh really high quality
hyperparameter tuning algorithm. We're
hyperparameter tuning algorithm. We're
shipping our new trainer which has a new
shipping our new trainer which has a new
advantage function. It's got a whole
advantage function. It's got a whole
bunch of additional improvements. It's
bunch of additional improvements. It's
very fast. It's got very nice logging
very fast. It's got very nice logging
integration. So, we really do everything
integration. So, we really do everything
um in RL. Like this is a comprehensive
um in RL. Like this is a comprehensive
effort to like fix all the things that
effort to like fix all the things that
are wrong with this
are wrong with this
field. Port working on Flappy Bird.
field. Port working on Flappy Bird.
Awesome. We would like a Flappy Bird M.
Awesome. We would like a Flappy Bird M.
Maybe a Flappy Puffer
Maybe a Flappy Puffer
M. Maybe load the Puffer
M. I have no idea how nobody has
M. I have no idea how nobody has
submitted one of those yet, but um yeah,
submitted one of those yet, but um yeah,
go for
go for
it. What do you do for logging? I notice
it. What do you do for logging? I notice
logging is very underdeveloped in RL
logging is very underdeveloped in RL
space. So we just have a very nice way
space. So we just have a very nice way
where anything that you have in this log
where anything that you have in this log
strct
strct
uh anything that you put in this log
uh anything that you put in this log
strct there is a very clean way to just
strct there is a very clean way to just
get it integrated with well aggregated
get it integrated with well aggregated
across like a thousand environments at
across like a thousand environments at
reasonable intervals and then we just
reasonable intervals and then we just
have backend integrations with WY and
have backend integrations with WY and
Neptune and it's like 20 lines of code
Neptune and it's like 20 lines of code
if you want to add your own of those. So
if you want to add your own of those. So
like when I do my experiments um I'm not
like when I do my experiments um I'm not
just looking at like one plot, right? If
just looking at like one plot, right? If
I go to Neptune
here, I have 30,000
here, I have 30,000
experiments from the last few months and
experiments from the last few months and
I can go to any of these tags and like I
I can go to any of these tags and like I
can go to this maze tag here. This is a
can go to this maze tag here. This is a
hyperparameter sweep and I can see all
hyperparameter sweep and I can see all
these graphs that are getting logged
these graphs that are getting logged
from um from C. Hang on.
from um from C. Hang on.
So I have all these graphs from C but
So I have all these graphs from C but
then I also have these dashboards which
then I also have these dashboards which
I can actually see like hyperparameter
I can actually see like hyperparameter
sensitivity. So I can see like this is
sensitivity. So I can see like this is
the stable regime for learning rate.
the stable regime for learning rate.
This is a stable regime for gamuts very
This is a stable regime for gamuts very
high. I can see like lambda. I have all
high. I can see like lambda. I have all
sorts of different views. I can see
sorts of different views. I can see
sweep progress. So I can see like how
sweep progress. So I can see like how
the parameters were fit over the course
the parameters were fit over the course
of a sweep and sensitivity therein. Um I
of a sweep and sensitivity therein. Um I
just have a ton a ton of insights here
just have a ton a ton of insights here
and it mostly it just comes from the
and it mostly it just comes from the
speed frankly because like these aren't
speed frankly because like these aren't
necessarily things that people couldn't
necessarily things that people couldn't
do before. The difference is that these
do before. The difference is that these
100 experiments or these like 178
100 experiments or these like 178
experiments this is like I don't know 30
experiments this is like I don't know 30
billion steps of training data or
billion steps of training data or
something and uh this ran overnight on
something and uh this ran overnight on
one GPU. So like we just have so so so
one GPU. So like we just have so so so
much more data and so much uh so much
much more data and so much uh so much
faster at such smaller scale that we can
faster at such smaller scale that we can
just do things that would previously
just do things that would previously
have been reserved for Google which is
have been reserved for Google which is
why we've been able to make so much
why we've been able to make so much
progress so
progress so
quickly. Why do you prefer C? Why you
quickly. Why do you prefer C? Why you
prefer using C
prefer using C
inside I I assume that means instead of
inside I I assume that means instead of
C++ it's just simpler and easier like I
C++ it's just simpler and easier like I
don't want to deal with the giant C
don't want to deal with the giant C
standard library. I don't want to deal
standard library. I don't want to deal
with people with boost. I don't want to
with people with boost. I don't want to
deal with like all the crazy things with
deal with like all the crazy things with
PE that things do in C++. C is just
PE that things do in C++. C is just
really easy. That's
really easy. That's
all. I like simple
things. It's just so
easy. Like I any of the environments
easy. Like I any of the environments
that I've worked on, I can just give
that I've worked on, I can just give
people the code file for it. Like I can
people the code file for it. Like I can
give a first year undergrad the code for
give a first year undergrad the code for
neural MMO 3 which is arguably one of
neural MMO 3 which is arguably one of
the most sophisticated RL environments
the most sophisticated RL environments
out there and like a first year
out there and like a first year
undergrad who's taken their first
undergrad who's taken their first
systems course can just read that code
systems course can just read that code
from top to bottom and we'll understand
from top to bottom and we'll understand
everything because it's just that easy.
everything because it's just that easy.
Like why do why do hard thing when
Like why do why do hard thing when
simple things
simple things
suffice and runs at one and a half
suffice and runs at one and a half
million steps per second on one CPU
million steps per second on one CPU
core.
But what I'm currently working on here
But what I'm currently working on here
is this is the this is a multi- aent
is this is the this is a multi- aent
tutorial M. I just built this yesterday
tutorial M. I just built this yesterday
to be like, you know, the sample
to be like, you know, the sample
project where you can like get a sense
project where you can like get a sense
of how a couple different like basic Ms
of how a couple different like basic Ms
are written. So, this is like, you know,
are written. So, this is like, you know,
I tried to like really just pair it down
I tried to like really just pair it down
to a a very basic end, and it's like
to a a very basic end, and it's like
going to be way more commented than uh
going to be way more commented than uh
the other end files are.
I'm mostly interested in
I'm mostly interested in
interrun logging. I was more talk per
interrun logging. I was more talk per
run logging in and viz how learned
run logging in and viz how learned
behaviors interact similar to how ice
behaviors interact similar to how ice
plots work for forest.
plots work for forest.
Um yeah, I mean we kind of just have
Um yeah, I mean we kind of just have
standard metrics for that, right? It's
standard metrics for that, right? It's
just like you log whatever you want from
just like you log whatever you want from
the end and then the key for a lot of
the end and then the key for a lot of
our uh interpretability stuff is just
our uh interpretability stuff is just
trying to make as many of the M's as
trying to make as many of the M's as
possible human playable uh and having
possible human playable uh and having
like a really quick feedback cycle on
like a really quick feedback cycle on
evals, right? Like let me just show you
evals, right? Like let me just show you
how let me show you how we train an
how let me show you how we train an
endbon puffer, right? So puffer
endbon puffer, right? So puffer
train. Hopefully I haven't broken
train. Hopefully I haven't broken
it. Okay, puffer train and puffer
it. Okay, puffer train and puffer
target. This literally just runs our
target. This literally just runs our
main training script. So it's this is
main training script. So it's this is
the same as doing like puffer python-m
the same as doing like puffer python-m
pufferlib like pufferl train. It's like
pufferlib like pufferl train. It's like
the same thing just a convenience. Okay.
the same thing just a convenience. Okay.
So now here we have this thing training
So now here we have this thing training
at
at
3539 million steps per
3539 million steps per
second. Okay. And you get all of this
second. Okay. And you get all of this
data in real time like this. But also if
data in real time like this. But also if
you just pass Neptune or 1B, it'll also
you just pass Neptune or 1B, it'll also
log all this stuff online with full
log all this stuff online with full
graphs.
Okay. And now after 20 to 30 seconds,
Okay. And now after 20 to 30 seconds,
this thing will have trained 100 million
this thing will have trained 100 million
steps, which is typically like an
steps, which is typically like an
overnight run for most of RL
overnight run for most of RL
researchers. Um, and
researchers. Um, and
then experiments just see what it named
then experiments just see what it named
it. And then we just do
eval which runs the exact same script
eval which runs the exact same script
just know not you doing the training
just know not you doing the training
part and it returns rendering arm load
part and it returns rendering arm load
model
model
path
8. And this is the model that we just
8. And this is the model that we just
trained.
And we have like experiments that take
And we have like experiments that take
on the order of seconds to minutes for
on the order of seconds to minutes for
like a pretty surprising range of
like a pretty surprising range of
environment complexity. Like things that
environment complexity. Like things that
you would normally expect to take
you would normally expect to take
multiple days to train, we just do it in
multiple days to train, we just do it in
like five minutes.
like five minutes.
So you kind of just have a lot more
So you kind of just have a lot more
interpretability in the sense that you
interpretability in the sense that you
you just get faster feedback loops of
you just get faster feedback loops of
all. You can just go watch your agent
all. You can just go watch your agent
play the game a
bunch. And I don't have it set up in
bunch. And I don't have it set up in
this end, but for most of them you can
this end, but for most of them you can
actually like hold shift and you can
actually like hold shift and you can
take over for the AI. So you can like
take over for the AI. So you can like
try to like shift around the way it
try to like shift around the way it
plays.
Um yeah, there's a bunch of stuff you
Um yeah, there's a bunch of stuff you
can do.
Python based and Majoko definitely can't
Python based and Majoko definitely can't
keep up. Yeah, I mean it's not even
keep up. Yeah, I mean it's not even
close, right? To be fair, even if you
close, right? To be fair, even if you
give us a Python end, we normally still
give us a Python end, we normally still
make it like
make it like
between I'd say around like 50 times
between I'd say around like 50 times
faster than you'd normally get in like
faster than you'd normally get in like
SP3. Like we've been handed unoptimized
SP3. Like we've been handed unoptimized
Python ends and we still get them to run
Python ends and we still get them to run
like 200,000 steps per second training.
like 200,000 steps per second training.
But like in our C version, it runs 2
But like in our C version, it runs 2
million steps per second,
right? But the thing that's like the
right? But the thing that's like the
emphasis here is just how easy this is.
emphasis here is just how easy this is.
Like this isn't hard. This is that env
Like this isn't hard. This is that env
is 200 lines of C. That's it, including
is 200 lines of C. That's it, including
the
renderer with a very thin binding
renderer with a very thin binding
layer. Do I need to run overwork equals
layer. Do I need to run overwork equals
true to run 96 Python M?
true to run 96 Python M?
recommend keeping both workers and batch
recommend keeping both workers and batch
sets to
sets to
auto. Those are the same as what they
auto. Those are the same as what they
were in 2.0. So link key the way that I
were in 2.0. So link key the way that I
would suggest including the ren well the
would suggest including the ren well the
render is super look at this it's just
render is super look at this it's just
ray this is the whole
ray this is the whole
renderer ray's awesome also if you want
renderer ray's awesome also if you want
to do python uh ray has an identical API
to do python uh ray has an identical API
for python you can basically write the
for python you can basically write the
exact same code and see in python
um yeah lean key so I wouldn't suggest
um yeah lean key so I wouldn't suggest
doing overwork like that parameter is
doing overwork like that parameter is
off by default because you shouldn't do
off by default because you shouldn't do
that um generally what you do is you
that um generally what you do is you
take your cores. All right, you get like
take your cores. All right, you get like
and then you run a number of
and then you run a number of
environments that's going to be
environments that's going to be
divisible by your number of cores, not
divisible by your number of cores, not
threads. So like if you have 16 cores,
threads. So like if you have 16 cores,
then you do 96 ms on on 16 cores. So
then you do 96 ms on on 16 cores. So
what is that? Is that like six each? I
what is that? Is that like six each? I
believe it'll just end up running six
believe it'll just end up running six
per core.
Uh, and the way that the auto things
Uh, and the way that the auto things
work
is
is
well, you might have to you might have
well, you might have to you might have
to set some of those params manually. I
to set some of those params manually. I
can help you with that today, though.
can help you with that today, though.
Like, if you're stuck on this, I can
Like, if you're stuck on this, I can
like look at your config and I can just
like look at your config and I can just
fix it for
fix it for
you. That's just the easiest
thing. I'll start learning Rayb right
thing. I'll start learning Rayb right
now. Yeah, it's really nice. I like it's
now. Yeah, it's really nice. I like it's
rendering is one of those things where
rendering is one of those things where
it's just like you have like pygame and
it's just like you have like pygame and
just these awful libraries. This is so
just these awful libraries. This is so
so much better and it just works
so much better and it just works
effortlessly and see this is all of it.
effortlessly and see this is all of it.
It's just got like really basic stuff
It's just got like really basic stuff
like texture loading and low-level
like texture loading and low-level
things that you would want and then you
things that you would want and then you
just write loops
just write loops
easy. So yeah, I mean this is this
easy. So yeah, I mean this is this
environment here is already committed. I
environment here is already committed. I
will be pushing these comments up today
will be pushing these comments up today
and then this is going to be part of a
and then this is going to be part of a
tutorial on the website for the new
tutorial on the website for the new
version of pufferlib. Uh so it's going
version of pufferlib. Uh so it's going
to be really easy for people to write
to be really easy for people to write
and contribute environments uh to puffer
and contribute environments uh to puffer
lib learn some RL in the process and
lib learn some RL in the process and
then you know from there once you get
then you know from there once you get
comfortable with all the things if you
comfortable with all the things if you
don't have a huge amount of RL
don't have a huge amount of RL
background right like this helps you get
background right like this helps you get
comfortable and then you can actually
comfortable and then you can actually
start helping on our research side right
start helping on our research side right
like there's so many different research
like there's so many different research
problems that we are interested in first
problems that we are interested in first
of all just like new types of
of all just like new types of
environments that pose interesting
environments that pose interesting
problems for reinforcement learning is a
problems for reinforcement learning is a
great way to contribute we have
great way to contribute we have
algorithm side for people that are more
algorithm side for people that are more
mathematically inclined we've that all
mathematically inclined we've that all
this RL infrastructure and optimization
this RL infrastructure and optimization
for people who have like the more
for people who have like the more
engineering side of things, right?
engineering side of things, right?
There's like tons and tons of stuff to
There's like tons and tons of stuff to
do. It's a comprehensive effort to make
do. It's a comprehensive effort to make
reinforcement learning fast and
reinforcement learning fast and
sane. Gives me warning saying numm
sane. Gives me warning saying numm
should be h
uh yeah, link that linky like just paste
uh yeah, link that linky like just paste
that in discord and let me know because
that in discord and let me know because
I'll look at that. It's possible I have
I'll look at that. It's possible I have
the warnings misleading.
and Captain's going to be here to have
and Captain's going to be here to have
uh his stuff fixed in a second as well.
We don't need this. We do not need
We don't need this. We do not need
this. Make this
this. Make this
end. Do
end. Do
init
allocate here.
You don't have to join the discord. It's
You don't have to join the discord. It's
just it's there if you want it. Looking
just it's there if you want it. Looking
to go towards meta RL. If I ever think
to go towards meta RL. If I ever think
of some kinds of M which makes sense,
of some kinds of M which makes sense,
I'll try to contribute. I'm fully
I'll try to contribute. I'm fully
formalized myself yet so it be another
formalized myself yet so it be another
year.
year.
opinionated current neural architecture
opinionated current neural architecture
search is quite naive. It definitely is.
search is quite naive. It definitely is.
The tricky thing that I think is
The tricky thing that I think is
like the thing that's I think difficult
like the thing that's I think difficult
with the neural architecture search is
with the neural architecture search is
like if you want to make this thing
like if you want to make this thing
really work, right? You want to make it
really work, right? You want to make it
fast. And the tough thing is neural
fast. And the tough thing is neural
architecture search tends to generate
architecture search tends to generate
like really
like really
slow uh really slow models because like
slow uh really slow models because like
you'd almost have to generate and then
you'd almost have to generate and then
like run optimization on all the kernels
like run optimization on all the kernels
or something weird like fuse kernels or
or something weird like fuse kernels or
something. There'd be like a lot of
something. There'd be like a lot of
low-level stuff, I think, to make it
low-level stuff, I think, to make it
really
good. I think you'd also be surprised at
good. I think you'd also be surprised at
how quickly you can learn um some of
how quickly you can learn um some of
these things. We've had people come in
these things. We've had people come in
with no reinforcement learning
with no reinforcement learning
background whatsoever, just like a
background whatsoever, just like a
software engine background. Um, who
software engine background. Um, who
haven't even done a ton of low-level dev
haven't even done a ton of low-level dev
and uh they've been like very very
and uh they've been like very very
capable at like helping and doing all
capable at like helping and doing all
sorts of interesting things around
sorts of interesting things around
Puffer within just a few months. We
Puffer within just a few months. We
really really try to make it as low
really really try to make it as low
friction as possible and like you can
friction as possible and like you can
just learn things quicker when you have
just learn things quicker when you have
lower turnaround time for like checking
lower turnaround time for like checking
things that you want to check, right?
things that you want to check, right?
running ideas, experiments, and things.
running ideas, experiments, and things.
It's just it's a way lower turnaround
It's just it's a way lower turnaround
time for
everything. That's the
idea. Definitely naive for a good
idea. Definitely naive for a good
reason. Defining a search space is hard.
reason. Defining a search space is hard.
It's hard. It's also
It's hard. It's also
like I don't know what what flavor of
like I don't know what what flavor of
neural architecture search you're
neural architecture search you're
interested in, right? Like if you're
interested in, right? Like if you're
talking if you're looking at like old
talking if you're looking at like old
like neat or hyper neat or like any of
like neat or hyper neat or like any of
the um like CPPNs or stuff, right? Like
the um like CPPNs or stuff, right? Like
the really small evolved networks that
the really small evolved networks that
do really interesting things just
do really interesting things just
basically via function composition.
basically via function composition.
They're really cool, but I haven't seen
They're really cool, but I haven't seen
them scale very well. And they're also
them scale very well. And they're also
like if you do scale them, it's very
like if you do scale them, it's very
difficult to make them
difficult to make them
fast, which is pretty tough. Generally,
fast, which is pretty tough. Generally,
I think I like the space of EVO in
I think I like the space of EVO in
general. The thing is like you want to
general. The thing is like you want to
make everything like nicely shaped
make everything like nicely shaped
networks that run fast and like run on
networks that run fast and like run on
big
big
batches. Yeah, Captain. I'll hop on
batches. Yeah, Captain. I'll hop on
Discord. We'll do that right now. I've
Discord. We'll do that right now. I've
been waiting for you.
It's not letting me click the
It's not letting me click the
channel. Hang on. Oh, there it is.
Hey, in ENS and darts. Um, yeah, meet
This is like all stuff like Ken Stanley
This is like all stuff like Ken Stanley
adjacent
adjacent
stuff. Yeah, we're always happy to have
stuff. Yeah, we're always happy to have
contributors. It's pretty much if you're
contributors. It's pretty much if you're
pushing uh if you're contributing code
pushing uh if you're contributing code
to Puffer, you'll see that you get lots
to Puffer, you'll see that you get lots
of uh lots of feedback on stuff. All
of uh lots of feedback on stuff. All
right, Captain, you got bugs to
right, Captain, you got bugs to
fix or I have bugs. Let's see here.
Boom. All right. What do we
got? No idea. You might I don't
got? No idea. You might I don't
know, man. This Python packaging is
know, man. This Python packaging is
cursed. Okay, I'm working um best I can.
We're going to have the the demo m done
We're going to have the the demo m done
today. That is the goal. And then I will
today. That is the goal. And then I will
go from
there. Let's
see. Okay. So, I have n there's ninja
see. Okay. So, I have n there's ninja
there. It's built.
Okay, using your correct. It's I see
Okay, using your correct. It's I see
it's using the Python in your PM. So,
it's using the Python in your PM. So,
it's
good. Ah, okay.
good. Ah, okay.
Um, so I see what happened there. Uh,
Um, so I see what happened there. Uh,
can
can
you you run
NVCC? You don't have NVCC.
Boom. So basically what happened there,
Boom. So basically what happened there,
it's a good report because that should
it's a good report because that should
be I should have that working so that
be I should have that working so that
it's an automatic fallback to that.
it's an automatic fallback to that.
Um, but what basically happened is it
Um, but what basically happened is it
compiled the CPU version of that kernel
compiled the CPU version of that kernel
for you because you don't have CUDA dev
tools. So that is still a good error
tools. So that is still a good error
report because it should not fail in
report because it should not fail in
that manner. It should just it should
that manner. It should just it should
just be slower basically. It should just
just be slower basically. It should just
build the CPU back end and just use it
build the CPU back end and just use it
for you.
It's probably well it's it goes in your
It's probably well it's it goes in your
container.
container.
Um, it needs to be installed in the
Um, it needs to be installed in the
container and it's like CUDA dev
container and it's like CUDA dev
tools
something. It doesn't need to be. It's
something. It doesn't need to be. It's
not on the host that you need it
though. Are you not in Oh, you're not in
though. Are you not in Oh, you're not in
a
a
container. Yeah. Then you Well, then you
container. Yeah. Then you Well, then you
just need CUDA dev tools. Yeah.
Uh well, we use the base container,
Uh well, we use the base container,
right? We use the Nvidia
base, but it's yeah, it's just like food
base, but it's yeah, it's just like food
dev tools or whatever if you look it up.
So I the goal is that this should not
So I the goal is that this should not
prevent you from running puffer lib.
prevent you from running puffer lib.
It'll just prevent you from having the
It'll just prevent you from having the
GPU kernel. So I will I will go add that
GPU kernel. So I will I will go add that
fallback um after I fix all the other
fallback um after I fix all the other
stuff
today cuz it actually it correctly
today cuz it actually it correctly
compiled the CPU version. It just didn't
compiled the CPU version. It just didn't
detect that there was no CUDA version
detect that there was no CUDA version
for it. So uh
for it. So uh
yeah, that's all.
You don't have
nvcc. You should you should have the
nvcc. You should you should have the
like you need the you should have a CUDA
like you need the you should have a CUDA
compiler if it's correctly set
up. We do not have a 1-800 puff
number. We should get one of those.
number. We should get one of those.
Can we get 1800 puff?
All
right, you take a second to figure that.
right, you take a second to figure that.
I'm going to go take the quick
I'm going to go take the quick
call back.
You figure it out,
Captain. Let's see.
So I added a warning for that in the
So I added a warning for that in the
latest
version. Is it synced? So there should
version. Is it synced? So there should
be there is an explicit check against
be there is an explicit check against
total mini batches being
zero like yesterday.
If you're on a fork, you would have had
If you're on a fork, you would have had
to fold from upstream or synced
it. I will be right back.
Hey,
Hey,
quicky. We were live earlier. What
quicky. We were live earlier. What
happened? We went for breakfast. I just
happened? We went for breakfast. I just
uh I just do a quick stream in the
uh I just do a quick stream in the
morning because I have breakfast at
morning because I have breakfast at
10:00. So,
10:00. So,
it's some days I run, some days I don't.
it's some days I run, some days I don't.
So, it's like I'm on for half hour to
So, it's like I'm on for half hour to
two hours depending on the day and then
two hours depending on the day and then
I'm back for the rest of the
I'm back for the rest of the
day. Captain, how's it
going? Okay. Is this old Syon or
So run it with serial back
So run it with serial back
end if it if you see CSX
fault.
Yes.
Yes.
Yes. It also works from
Yes. It also works from
CLI like vec.backend serial will do it
CLI like vec.backend serial will do it
as well.
capital S serial. That's this time
though. Um well, I'd have to like
though. Um well, I'd have to like
postprocess the string otherwise. It's
postprocess the string otherwise. It's
because it uses
because it uses
um like it it just like it looks for the
um like it it just like it looks for the
vectorzation thing that matches that
vectorzation thing that matches that
string. Basically, it's a str it's a
string. Basically, it's a str it's a
string match on the classes in vec.
string match on the classes in vec.
Yeah.
uh okay so that is
because you have num m set to
Uh, you shouldn't do that. So, what's
Uh, you shouldn't do that. So, what's
happening there is that you still have
happening there is that you still have
your mini batch size set to
your mini batch size set to
8192. All right. And the batch size is
8192. All right. And the batch size is
BPT Horizon times the number of
ends. There you go. That's
ends. There you go. That's
why. So, the good thing is we've
why. So, the good thing is we've
actually made it so you can debug that
actually made it so you can debug that
now. Um, if you just uh just head uh
setup.py. No, no, no. If you Okay. Yeah,
setup.py. No, no, no. If you Okay. Yeah,
you can do it that way, too. Yeah, but
you can do it that way, too. Yeah, but
there's another command in there you'll
there's another command in there you'll
need. So, debug equals one. You can
need. So, debug equals one. You can
compile that. Yeah, that's fine. But
compile that. Yeah, that's fine. But
then to actually run the thing, you're
then to actually run the thing, you're
going to have to get the command from uh
going to have to get the command from uh
from the setup.py.
from the setup.py.
It's just a comment at the top for
now. Okay. So, CUDA visible device.
now. Okay. So, CUDA visible device.
Yeah. All that stuff. And then you can
Yeah. All that stuff. And then you can
just set that to run your
just set that to run your
end. I would do it with serial back end
end. I would do it with serial back end
and run train.
You get trace out of any of these.
So when I do this, I normally get a with
So when I do this, I normally get a with
that command, I normally get a stack
that command, I normally get a stack
trace that goes into the
C. Well, I'm not I don't have anything
C. Well, I'm not I don't have anything
set. It's so it's however
set. It's so it's however
setup.py like setup
setup.py like setup
tools. Yeah. I don't know.
tools. Yeah. I don't know.
I just have uh the uh the dev container
I just have uh the uh the dev container
is just set up with with
clang. I don't know why there could be a
clang. I don't know why there could be a
stack trace at the top of all that spam
stack trace at the top of all that spam
for all I
know. All right.
So the just thing just pops up is like
So the just thing just pops up is like
an operation not permitted. That's like
an operation not permitted. That's like
some weird permissioning [ __ ] with
some weird permissioning [ __ ] with
my OS has nothing to do with it.
And that only that like that happens
And that only that like that happens
with debugger specifically. Play.
What
doesn't? Well, then that means whatever
doesn't? Well, then that means whatever
compiler I have is working.
Uh, the Python ecosystem is not known
Uh, the Python ecosystem is not known
for being particularly
smart. Yeah, I'm seeing this uh I'm
smart. Yeah, I'm seeing this uh I'm
seeing this linky.
That doesn't exit cleanly.
multi-discrete
outputs. We do multi-disipre outputs.
Action dim is off.
See, I bought
You
argument 13.3.0
which is uh default Abuntu
for I'm pretty sure unless we find that
for I'm pretty sure unless we find that
everything's screwed at the last minute.
Like, yeah, I mean,
Like, yeah, I mean,
it's it just makes more sense to go in
it's it just makes more sense to go in
the major
versions. There's like version inflation
versions. There's like version inflation
because of um all the AI companies just
because of um all the AI companies just
like skipping normal versioning. So,
like skipping normal versioning. So,
it's like fine, screw it.
H. Oh,
yeah. Well, then every update's just
yeah. Well, then every update's just
going to be major
version. Like, oh no, it's a breakage.
version. Like, oh no, it's a breakage.
It's like, "Shut up. You fix our L."
The thing is nobody actually follows the
The thing is nobody actually follows the
versioning and everybody just has it
versioning and everybody just has it
stuck into their config files without
stuck into their config files without
like a pin to a major version anyways.
like a pin to a major version anyways.
So like it just will automatically
So like it just will automatically
upgrade to breaking versions like the
upgrade to breaking versions like the
whole system is wrong for
that. Yes.
The thing is, Python's really not built
The thing is, Python's really not built
for it though because like the correct
for it though because like the correct
way to do this, right, would be that you
way to do this, right, would be that you
would have isolated like let's say you
would have isolated like let's say you
have two different dependencies that
have two different dependencies that
both have numpy they would both build
both have numpy they would both build
their own versions of numpy like
their own versions of numpy like
statically or what however they want to
statically or what however they want to
do it and then you wouldn't get
do it and then you wouldn't get
conflicts but because everything is
conflicts but because everything is
global like you can't pin versions
global like you can't pin versions
otherwise your pin numpy breaks the
otherwise your pin numpy breaks the
minimum pin version of the other library
minimum pin version of the other library
because we don't know what static
because we don't know what static
linking is apparently um and like
linking is apparently um and like
everyone has to share the same bloody
everyone has to share the same bloody
version of everything But it's just a
mess. Like literally, this should not be
mess. Like literally, this should not be
a problem, right? Literally, versioning
a problem, right? Literally, versioning
would not be a problem if it's just
would not be a problem if it's just
like, okay, your library basically takes
like, okay, your library basically takes
all the code. It's like vendored
all the code. It's like vendored
versions. Everything is linked
versions. Everything is linked
reasonably so that you're not sharing
reasonably so that you're not sharing
the name space of your depths with other
the name space of your depths with other
things that need different versions of
things that need different versions of
that depths. And you're done.
I mean, go go go generally seems like
I mean, go go go generally seems like
fine.
I mean, we can literally do all of this
I mean, we can literally do all of this
and
Now, you don't blame Don't blame C for
Now, you don't blame Don't blame C for
this. This is Python being dumb as hell.
this. This is Python being dumb as hell.
All
right. Okay. But like if you just run
right. Okay. But like if you just run
this sanitizer from the C build, it
this sanitizer from the C build, it
works.
works.
So, right.
We use clang as well.
I don't know, man. I'm working as fast
I don't know, man. I'm working as fast
as I can on it. I kind of don't want to
as I can on it. I kind of don't want to
think about it. I just want to like get
think about it. I just want to like get
it
it
done. It's like it's just it
done. It's like it's just it
sucks. I hate releases.
It's really close, but it's like if I
It's really close, but it's like if I
think about like even if I think like,
think about like even if I think like,
oh, there like, you know, 20 more hours
oh, there like, you know, 20 more hours
of like real work to do, it's like I
of like real work to do, it's like I
can't do those 20 hours. It's like I'm
can't do those 20 hours. It's like I'm
just so
bored. I want to get back to doing real
bored. I want to get back to doing real
things, you know?
I haven't ridden one of these like
I haven't ridden one of these like
networks in a while and they're kind of
networks in a while and they're kind of
annoying.
Yeah, this is the
Yeah, this is the
rare the rare watch neural nets in pure
rare the rare watch neural nets in pure
content.
have to make the network a slightly
have to make the network a slightly
different way. I think.
Okay. So, we're going to try this
training split sizes.
I figured it would be good to at least
I figured it would be good to at least
have the tutorial on like how we do our
have the tutorial on like how we do our
um how we do like custom nuts and stuff.
wrong with you.
our model.
So I guess then it will just
be can make this a fair bit
simpler.
Totally make this simpler.
And this will
Forward
linear still not working for
See, make it work suddenly.
Hey,
Hey,
man. That's such a pain in the ass
man. That's such a pain in the ass
because
because
like it's I hate when it's like one
like it's I hate when it's like one
stupid heavy tool makes you use another
stupid heavy tool makes you use another
stupid heavy
stupid heavy
tool because it's it's like this is how
tool because it's it's like this is how
you get this is how C turns into React.
you get this is how C turns into React.
All right, we used to be we used to
All right, we used to be we used to
write real code, let me tell you.
I mean, that's people doing weird
I mean, that's people doing weird
[ __ ] It's pretty damn easy when you
[ __ ] It's pretty damn easy when you
just write loops and conditionals and to
just write loops and conditionals and to
watch your code be a thousandx faster
watch your code be a thousandx faster
and shorter than everything written in
and shorter than everything written in
the modern
era.
era.
Like, you know, I just I like see just
Like, you know, I just I like see just
it's easy, you know?
it's easy, you know?
It's just Easy.
What's it say?
That's where this the debug commands
That's where this the debug commands
were supposed
were supposed
to fix that.
to fix that.
like it's supposed to fix this asand
like it's supposed to fix this asand
linking or whatever.
this the action
space and five
This runs out of weights
Yeah, it's obnoxious. I mean, this is
Yeah, it's obnoxious. I mean, this is
why we have it so that it should be it
why we have it so that it should be it
should be easy. I mean, to be fair, this
should be easy. I mean, to be fair, this
is also kind of like why we ship the dev
is also kind of like why we ship the dev
container. So, it's like there are
container. So, it's like there are
million freaking stupid setup things
million freaking stupid setup things
that can be different. Here's a damn
that can be different. Here's a damn
entire computer that's the same as
mine. Let's use You have access to a
mine. Let's use You have access to a
box, don't you? Do it there.
boxes for you to like fiddle
with. Probably
not. If you need if you get stuff the
not. If you need if you get stuff the
way you need
way you need
it, I can run a thing.
it, I can run a thing.
But I don't know. Does that actually It
But I don't know. Does that actually It
depends what box you have. I will also
depends what box you have. I will also
just have the
just have the
uh the new containers done pretty soon
here. Just finish these darn tutorials.
What? It's input.
What? It's input.
It's state.
137K parameters.
That seems
correct. Something's just slightly off,
correct. Something's just slightly off,
I guess.
Okay,
Okay,
now get
now get
a overflow of some type.
and I'm agents.
Oh, wait. Numb
Oh, wait. Numb
goals. This is it.
We got it
work still now.
Breast sanitizer.
This is you have to
order. Here we are.
These puffers don't seem as smart as the
These puffers don't seem as smart as the
other puffers.
Why don't I get um
Why don't I get um
symbols? I don't get like symbols here.
That's
crazy. Actually version made a
crazy. Actually version made a
difference.
Oh, that sucks.
me retrain. I think I am loading this
me retrain. I think I am loading this
correctly. I think I just trained a bad
correctly. I think I just trained a bad
model.
Rain this Oh,
Right. Uh [ __ ] I broke something in the
Right. Uh [ __ ] I broke something in the
end in the process of refactoring, I
end in the process of refactoring, I
think.
Oh yeah, I kind of forgot about
Oh yeah, I kind of forgot about
that.
Um, that's weird cuz I I can definitely
Um, that's weird cuz I I can definitely
run with device CPU. So
run with device CPU. So
like Huh.
Don't set
Don't set
that. Maybe don't set
that. I That should fix that. Either
way, was the other
That's
That's
bizarre. There's so many of these little
bizarre. There's so many of these little
like stupid things, man.
Oh yeah.
I have a feeling I'm going to have to go
I have a feeling I'm going to have to go
through line by line like the old
through line by line like the old
version and see what I messed up.
Um, I mean, you get that for a ton of
things. Like I just fixed one of those
things. Like I just fixed one of those
not
not
initializing Ray Lib before calling load
initializing Ray Lib before calling load
texture.
Yep. Got the puffers vibing
again. It's like it's cool, right? I
again. It's like it's cool, right? I
honestly I think we should do way more
honestly I think we should do way more
with stuff like this in the future, but
with stuff like this in the future, but
I got to get this update out
I got to get this update out
first and I probably have to
first and I probably have to
go recover from all this stuff I've been
go recover from all this stuff I've been
doing. Get my head on straight and go
doing. Get my head on straight and go
like I don't know, lift some rocks
like I don't know, lift some rocks
outside and watch the birds or
whatever. I You think I'm joking? I have
whatever. I You think I'm joking? I have
uh I've got stones 70 to I don't know
uh I've got stones 70 to I don't know
how much the big one is. It's between
how much the big one is. It's between
150 and
150 and
200. I've got like, you know,
200. I've got like, you know,
uh I've got an overhead pressing stone.
uh I've got an overhead pressing stone.
I've got like a loading
stone. It's just it's fun, you know. I
stone. It's just it's fun, you know. I
got all the barbells behind me. Speaking
got all the barbells behind me. Speaking
of which, thanks for reminding me. I got
of which, thanks for reminding me. I got
to do my first set of squats because I'm
I got a screwy knee, so I got to be
I got a screwy knee, so I got to be
careful. So, this is just going to
careful. So, this is just going to
be that the
cardinal. Going to see if that's the
cardinal. Going to see if that's the
cardinal I've been looking
cardinal I've been looking
for. No, it's
for. No, it's
not.
not.
Um, what do I put
on? I think we'll try
Uh, we'll try like
Uh, we'll try like
two right back.
knees didn't like it. Everything else is
fine. This is the funniest thing. So,
fine. This is the funniest thing. So,
there are I put up a bird house and I've
there are I put up a bird house and I've
got two bluebirds in the uh the
got two bluebirds in the uh the
birdhouse, but every so often some other
birdhouse, but every so often some other
bird comes over to like, "Oh, that looks
bird comes over to like, "Oh, that looks
like a nice box you've got there." And
like a nice box you've got there." And
then there two very angry
then there two very angry
bluebirds chase off whatever it
is. This is my spot.
And I have it like maybe 30 feet outside
And I have it like maybe 30 feet outside
the window. 25 ft. 30 ft outside the
the window. 25 ft. 30 ft outside the
window. That was very
amusing.
Ah, I have
Ah, I have
I've got uh two 27 inch monitors. That's
I've got uh two 27 inch monitors. That's
it. One's verd on the left and then my
it. One's verd on the left and then my
main monitor is in the center. But then
main monitor is in the center. But then
I have I do have to look around this big
I have I do have to look around this big
freaking
camera. The camera's on a tripod where I
camera. The camera's on a tripod where I
have my desk like two feet off the wall
have my desk like two feet off the wall
and the camera tripod has two legs on
and the camera tripod has two legs on
the desk and one leg like extended all
the desk and one leg like extended all
the way down to the ground.
And then the camera is actually mounted
And then the camera is actually mounted
upside down like on the front side. It's
upside down like on the front side. It's
a whole
thing. Uh-huh.
What random
number. What's the data
type? Then that seems like that sounds
type? Then that seems like that sounds
like it's getting overwritten. Not by
like it's getting overwritten. Not by
like that's like some random memory
overwrite. Is it before or
overwrite. Is it before or
after like
after like
um I mean OBS has to be the right size.
um I mean OBS has to be the right size.
There's no check on that, right?
There's no check on that, right?
Like all these things have to be the
Like all these things have to be the
right
size. No, because it's all your
memory. like that's you're overwriting
memory. like that's you're overwriting
within your own
within your own
strct. So that's actually one of the
strct. So that's actually one of the
things where like where people say, "Oh,
things where like where people say, "Oh,
you should use like area allocators."
you should use like area allocators."
That's why that's dumb.
It's not the same
It's not the same
um in uh
um in uh
PyTorch as in
C. Like the model does better in PyTorch
C. Like the model does better in PyTorch
than it does in the C export.
Mhm. Would have to
Mhm. Would have to
be pain in the ass
be pain in the ass
though. I've definitely seen this type
though. I've definitely seen this type
of thing happen before where
of thing happen before where
like behavior degrades somehow.
I mean, but it's like the thing is it
does like these guys do go for
does like these guys do go for
stars. They're just not as good.
Ah, I
see. Well, the log is the log field is
see. Well, the log is the log field is
just for things that are supposed to be
just for things that are supposed to be
logged and they should all be floats.
Mhm.
Well, yeah. No, I think so. I that
Well, yeah. No, I think so. I that
doesn't seem like a pretty a good way to
doesn't seem like a pretty a good way to
scale it anyways. Like if you have a
scale it anyways. Like if you have a
scripted bot that you want stats for
scripted bot that you want stats for
separately versus a trained bot, that's
separately versus a trained bot, that's
one thing. But if they're like all
one thing. But if they're like all
trained bots, then you should just log
trained bots, then you should just log
them all. Like you shouldn't have
them all. Like you shouldn't have
different stats per drone, I wouldn't
think. Yeah, you can have that.
think. Yeah, you can have that.
technically, but then if you really want
technically, but then if you really want
to check that for now, just like put
to check that for now, just like put
four agents worth of logs and call it a
four agents worth of logs and call it a
day. Put four agents worth of log spots
day. Put four agents worth of log spots
and call it a day for
now.
now.
Yeah. Well, then just have two
Yeah. Well, then just have two
till we get your good policy.
There's something screwed up in the net
There's something screwed up in the net
itself, I believe.
Well, I mean, I can always fix this as
Well, I mean, I can always fix this as
part of
part of
um as part of fixing
the the demo.
This mallet.
the ASM.
Yeah, this is installed.
I think there's anything else that
I think there's anything else that
uh allocate Ite.
Do we use
clang? We do, right? Yeah.
playing.
playing.
Oh.
Oh.
Um, why
Um, why
does why is platform not defined?
Oh no, it is
Oh no, it is
never
platform. Trying to fix the debugging
platform. Trying to fix the debugging
experience for this.
Oh, it's GPU driver stuff. Okay.
All right, whatever. I think this is
All right, whatever. I think this is
enough for
now. Just add a couple comments.
Careful. I'm going to find a way to
Careful. I'm going to find a way to
delete setup tools and not include
delete setup tools and not include
CMake. And the whole entire project
CMake. And the whole entire project
setup is going to be a random shell
script. I don't know. I tend to build
script. I don't know. I tend to build
okay stuff.
Hey Aaron, what's up? We're on stream.
Hey Aaron, what's up? We're on stream.
How's it
going? Okay, awesome. I am
going? Okay, awesome. I am
fixing our demos.
So, we will have uh very nice
So, we will have uh very nice
uh M docks. And then I might even send
uh M docks. And then I might even send
these to you to see how bad these look
these to you to see how bad these look
for
you. I don't know. Maybe we'll uh we'll
you. I don't know. Maybe we'll uh we'll
turn you into a C engineer just just
turn you into a C engineer just just
yet.
I mean, it's not that far off from this
I mean, it's not that far off from this
thing that I just built. So, why don't I
thing that I just built. So, why don't I
just give you this? It's like I'll
just give you this? It's like I'll
finish. You can beta test our doc. So, I
finish. You can beta test our doc. So, I
mean, if you can figure this out having
mean, if you can figure this out having
done probably mostly pure Python for the
done probably mostly pure Python for the
last decade, um, then it should be
last decade, um, then it should be
pretty easy.
builds. Yeah. So, this is pretty much
builds. Yeah. So, this is pretty much
ready to go. Um, here, check this out. I
ready to go. Um, here, check this out. I
The one thing is I have to fix our
The one thing is I have to fix our
library, like our neural net library,
library, like our neural net library,
because it's there's a gap with torch
because it's there's a gap with torch
because the networks don't look quite as
because the networks don't look quite as
good in uh like it's loading the
good in uh like it's loading the
network, but I think there's like some
network, but I think there's like some
drift or something. The puffer still get
drift or something. The puffer still get
the stars, just not quite as well as in
the stars, just not quite as well as in
uh Python. This doesn't look that
uh Python. This doesn't look that
bad. So, this is running in pure C right
bad. So, this is running in pure C right
now.
These ones are not as uh as snappy as
These ones are not as uh as snappy as
they so they'll be way snappy when I fix
they so they'll be way snappy when I fix
the uh the torch library.
But yeah, so that's just like drift with
But yeah, so that's just like drift with
our network. But if I just do for
instance, if I do this, that was running
instance, if I do this, that was running
pure C. Now this is running with
pure C. Now this is running with
PyTorch.
see the
puffers. Yeah. So, there's just like
puffers. Yeah. So, there's just like
some numerical stuff going on with our
some numerical stuff going on with our
uh RC library, but it's pretty cool
uh RC library, but it's pretty cool
because let's say I fix this. Okay. This
because let's say I fix this. Okay. This
is a 60line C demo file and then the the
is a 60line C demo file and then the the
source code for this environment this
source code for this environment this
whole with all of the comments included
whole with all of the comments included
and it's like nicely commented up is 230
and it's like nicely commented up is 230
lines and that's literally it. Now the
lines and that's literally it. Now the
one thing we use ray as our renderer and
one thing we use ray as our renderer and
then for that neural net demo
then for that neural net demo
specifically if you want to do that we
specifically if you want to do that we
have extensions puffernet.h
have extensions puffernet.h
page. And I've been thinking of
page. And I've been thinking of
replacing this thing with like the tiny
replacing this thing with like the tiny
red C back end, but uh it's also just
red C back end, but uh it's also just
kind of cool that we have this thing.
kind of cool that we have this thing.
It's like a
It's like a
single it's a single file like no
single it's a single file like no
dependencies C neural net and French
dependencies C neural net and French
library. That's really
library. That's really
easy. So that is the target env done.
easy. So that is the target env done.
I'm going to comment up uh the squared
I'm going to comment up uh the squared
env. I'm going to put them online and
env. I'm going to put them online and
then I can give you docs for that and
then I can give you docs for that and
you can see how that goes because the
you can see how that goes because the
chase thing you can train that at
chase thing you can train that at
probably 4 million steps a
second. Yeah, just insta
solve from the reinforcement learning
solve from the reinforcement learning
conference to actually have
conference to actually have
reinforcement learning.
Okay, there will be plenty of it to go
Okay, there will be plenty of it to go
around with a
around with a
puffer. Let me see the thing that you
puffer. Let me see the thing that you
put in
chat like
chat like
LP. So the are these two different
graphs and then is
graphs and then is
this is this perf this isn't evalf
this is this perf this isn't evalf
though this is this is the training
curve okay so
curve okay so
there's so according to the eval which
there's so according to the eval which
isn't really quite long enough, but
isn't really quite long enough, but
according to that
according to that
eval, it looks like they dip about the
eval, it looks like they dip about the
same amount
same amount
despite Yeah, they dip about the same
despite Yeah, they dip about the same
amount.
amount.
Okay. Yeah. So, I mean, just put all
Okay. Yeah. So, I mean, just put all
this stuff together and we will uh we
this stuff together and we will uh we
will uh you know, send over the final
will uh you know, send over the final
stuff. I mean, you've basically done
stuff. I mean, you've basically done
everything that we could have asked for
everything that we could have asked for
on this,
right? Yeah. It's like it's like it
right? Yeah. It's like it's like it
turns out that the baseline got
turns out that the baseline got
ridiculously good in the meanwhile,
ridiculously good in the meanwhile,
which is kind of what we were trying to
which is kind of what we were trying to
do. Um, but like it's pretty much like
do. Um, but like it's pretty much like
this helps whenever it is
possible. All right, good LP stuff for
possible. All right, good LP stuff for
puffer and hopefully I think that there
puffer and hopefully I think that there
will be there should be a way to get
will be there should be a way to get
some version of this into like more
some version of this into like more
puffer experiments as well. Maybe you'll
puffer experiments as well. Maybe you'll
be interested doing that side of stuff
be interested doing that side of stuff
when uh you have more like M's that are
when uh you have more like M's that are
easy to work with and interpret and
easy to work with and interpret and
randomize over and all
that. Oh yeah, you can of course of
that. Oh yeah, you can of course of
course finish everything. Um I here let
course finish everything. Um I here let
me
just I can commit this anyways.
I can show you this
code. Oh, that's actually way
easier. I mean, this is the code like
easier. I mean, this is the code like
this is the type of code that you write
this is the type of code that you write
for puffer ms, right? So I have
for puffer ms, right? So I have
commented here like which of these
commented here like which of these
fields are required and most of them
fields are required and most of them
aren't. So it's like this is a required
aren't. So it's like this is a required
log structure. You have a couple strrus
log structure. You have a couple strrus
which are literally just data classes,
which are literally just data classes,
right?
right?
Um here's your
Um here's your
env. And then this is the thing setting
env. And then this is the thing setting
new goals for itself. It's literally
new goals for itself. It's literally
just I mean if you just like replace the
just I mean if you just like replace the
parentheses with like white spacing and
parentheses with like white spacing and
stuff and like
stuff and like
other than the fact that there are
other than the fact that there are
pointers for things that in Python like
pointers for things that in Python like
everything that is not a primitive is
everything that is not a primitive is
just passed by reference anyways like
just passed by reference anyways like
this is basically
this is basically
Python like because you can write this
Python like because you can write this
in any
language. It really can be. And like
language. It really can be. And like
this is the observation function right
this is the observation function right
here. This is it.
It's not bad
either. All right. And then this is
either. All right. And then this is
here's how you reset the end. You just
here's how you reset the end. You just
randomize the
positions. All right. Basic stuff like
positions. All right. Basic stuff like
clipping is not built into C. So you
clipping is not built into C. So you
just write your own clipping function,
just write your own clipping function,
but
whatever. And then here's the step. Here
whatever. And then here's the step. Here
are the dynamics.
Yep. Little bit of memory management,
Yep. Little bit of memory management,
right? Little tiny bit of memory
right? Little tiny bit of memory
management right here. But um and then
management right here. But um and then
this is the whole renderer. This is the
this is the whole renderer. This is the
probably the coolest bit. You can't even
probably the coolest bit. You can't even
make the renderer this good in
Python. That's the whole
renderer. And this draws the puffers. It
renderer. And this draws the puffers. It
draws the stars. it would be even
draws the stars. it would be even
shorter except for the fact that I'm
shorter except for the fact that I'm
downscaling the textures as
downscaling the textures as
well. Oh, no. I It's not that I'm
well. Oh, no. I It's not that I'm
downing. It's I'm uh I'm text I have a
downing. It's I'm uh I'm text I have a
uh it's a like texture sheet. So, it's
uh it's a like texture sheet. So, it's
like a texture
atlas. And that's
atlas. And that's
everything that
everything that
is. And then the only thing you need to
is. And then the only thing you need to
do, it's a little bit
do, it's a little bit
annoying. This is the binding file.
This is what you write to bind your end
This is what you write to bind your end
to
Python. So until you start doing crazy
Python. So until you start doing crazy
things that require you to really get
things that require you to really get
into like the Python C API and stuff
into like the Python C API and stuff
like I mean this is pretty simple for
like I mean this is pretty simple for
just binding your end, right?
We will get you riding C
yet. I got to tell you, it's so much
yet. I got to tell you, it's so much
cooler now just to be able to like try
cooler now just to be able to like try
out whatever the heck I want and not
out whatever the heck I want and not
have like some big engineering stack I'm
have like some big engineering stack I'm
dependent
on. But it's but the thing is it's super
on. But it's but the thing is it's super
thin, right? So basically like I've got
thin, right? So basically like I've got
a 500 line script that replaces the need
a 500 line script that replaces the need
for like having some Python thing and it
for like having some Python thing and it
just uses the Python. It's just like a
just uses the Python. It's just like a
small wrapper over the Python C API.
small wrapper over the Python C API.
Like it hardly even does
anything. It's really nice that it's C,
anything. It's really nice that it's C,
not C++.
Um, I mean if we need to, we
Um, I mean if we need to, we
can. I mean that but the thing is it's
can. I mean that but the thing is it's
legitimately it's like whoopde it's not
legitimately it's like whoopde it's not
that
hard and like you know like these M's
hard and like you know like these M's
you can pretty much the way that these
you can pretty much the way that these
M's are written you can almost just copy
M's are written you can almost just copy
paste them into CUDA and like you could
paste them into CUDA and like you could
just run this to
envel you have to set up like the
envel you have to set up like the
parallelism flag but it's basically you
parallelism flag but it's basically you
just parallelize across like one you
just parallelize across like one you
just do one thread per end copy And
just do one thread per end copy And
you're
done. Well, I mean, you can paste this
done. Well, I mean, you can paste this
into CUDA, right? Like apparently nobody
into CUDA, right? Like apparently nobody
else can figure out how to get basic C
else can figure out how to get basic C
to run on a GPU reasonably. Um but look
to run on a GPU reasonably. Um but look
like this is where is it? Yeah, this is
like this is where is it? Yeah, this is
our extension our CUDA. This is our
our extension our CUDA. This is our
whole CUDA extension
whole CUDA extension
file. This is the puffer advantage
file. This is the puffer advantage
function right
function right
here. That's
it. We had
it. We had
to This gets called on This gets called
to This gets called on This gets called
on everything every mini batch.
It's actually really obnoxious that we
It's actually really obnoxious that we
have to ship kernels because it makes
have to ship kernels because it makes
the whole build stuff like way more
the whole build stuff like way more
obnoxious. That's the one thing that
obnoxious. That's the one thing that
really sucks is dealing with setup tools
really sucks is dealing with setup tools
and like Python compilation garb. Like
and like Python compilation garb. Like
that sucks. But like the actual language
that sucks. But like the actual language
itself, like it's it's all easy.
Oh, cool. Yeah, I should chat with
him. Yeah, send
him. Yeah, send
uh and where where were you chatting
uh and where where were you chatting
with him? Send send us a message because
with him? Send send us a message because
I was working on that as
well. Okay. Yeah, if it makes sense,
well. Okay. Yeah, if it makes sense,
send over. Um, I'd be happy to chat with
send over. Um, I'd be happy to chat with
him because I mean, I'd be very happy if
him because I mean, I'd be very happy if
somebody just helps with that and does
somebody just helps with that and does
that.
that.
Um, that's
like doesn't have to go back to the
like doesn't have to go back to the
drawing board. But if he already has
drawing board. But if he already has
stuff like at least like try at least
stuff like at least like try at least
use our ends to eval the thing so it
use our ends to eval the thing so it
actually make so you make sure it
actually make so you make sure it
actually works and it's not like yet
actually works and it's not like yet
another Atari
overfit. I mean at this point I just
overfit. I mean at this point I just
want all the algorithm stuff to just be
want all the algorithm stuff to just be
like done on that and it's like I'm very
like done on that and it's like I'm very
happy to have more people just like
happy to have more people just like
doing things and just providing MS and
doing things and just providing MS and
tools and things. Honestly, at this
tools and things. Honestly, at this
point, like I'm gonna have to spend a
point, like I'm gonna have to spend a
month probably just going through all
month probably just going through all
the algorithm things, but I kind of
the algorithm things, but I kind of
would just like to spend a month instead
would just like to spend a month instead
just building crazy sims and
just building crazy sims and
uh it's but I think it's makes more
uh it's but I think it's makes more
sense to do like split between
sense to do like split between
algorithms and business side
stuff. Yeah, I uh know the priorities
stuff. Yeah, I uh know the priorities
here at Puffer's got to be the
here at Puffer's got to be the
technically the best. that includes the
technically the best. that includes the
algorithms. But then also uh you know
algorithms. But then also uh you know
it's it does it is very important to
it's it does it is very important to
keep everything growing at a reasonable
keep everything growing at a reasonable
uh pace just because it like it opens so
uh pace just because it like it opens so
many more opportunities for us to do
stuff. All
stuff. All
right. Yeah, sure. Send us uh cuz like I
right. Yeah, sure. Send us uh cuz like I
because I got to honestly apologize to
because I got to honestly apologize to
those guys. Like I really wanted to do
those guys. Like I really wanted to do
way more with their lab and then like I
way more with their lab and then like I
I don't even think the pneumonia is an
I don't even think the pneumonia is an
excuse like cuz I could have just
excuse like cuz I could have just
started on it afterwards. I don't know.
started on it afterwards. I don't know.
My whole like mental like my whole like
My whole like mental like my whole like
the whole way I like like framed and
the whole way I like like framed and
like looked at the work I was doing kind
like looked at the work I was doing kind
of just shifted a bit I guess. I don't
know. I don't know. I kind of just like
know. I don't know. I kind of just like
after that instead of like doing a bunch
after that instead of like doing a bunch
of collabs and stuff, it was just like
of collabs and stuff, it was just like
now I'm going to just like focus up and
now I'm going to just like focus up and
like freaking shove RL as much as I can
like freaking shove RL as much as I can
solo with like the smallest amount of
solo with like the smallest amount of
code plus a few awesome open source
contributors. Okay, cool. So plan for
contributors. Okay, cool. So plan for
today is we'll have tutorials done. So
today is we'll have tutorials done. So
if you want to look at these things, you
if you want to look at these things, you
will have like a very nice like onepage
will have like a very nice like onepage
doc plus two super short ends probably
doc plus two super short ends probably
like 500 lines of code total that you
like 500 lines of code total that you
can read. Um one basic mult one basic
can read. Um one basic mult one basic
single agent en one basic multi- aent n.
single agent en one basic multi- aent n.
There will be a demo for the single
There will be a demo for the single
agent one with the um native API uh or
agent one with the um native API uh or
with the python version as well. But
with the python version as well. But
like we could probably train your Python
like we could probably train your Python
end at 100 or 200,000 steps per second
end at 100 or 200,000 steps per second
without even going to C. Um, and then
without even going to C. Um, and then
there's the C to get to like 2
there's the C to get to like 2
million. All right,
million. All right,
cool. Back to
back. Yep, sounds good. You're getting
back. Yep, sounds good. You're getting
good
good
results. All
right. Oh, yeah. I forgot that I wrote
right. Oh, yeah. I forgot that I wrote
the terminal renderer for pi
the terminal renderer for pi
squared. Funny.
So when you do eval um we forcibly set
So when you do eval um we forcibly set
your be back end to serial because
your be back end to serial because
render gets called on the first end.
So you're not
So you're not
rendering is it with serial back end.
rendering is it with serial back end.
So the only thing I can think of is the
So the only thing I can think of is the
first end in serial is the it gets
first end in serial is the it gets
treated as a driver end.
[Music]
Only thing I can think of is that I've
Only thing I can think of is that I've
never seen it. The only thing I can
never seen it. The only thing I can
think of is when we call render, we mess
think of is when we call render, we mess
specifically with the first amp. That's
specifically with the first amp. That's
about it
though.
though.
Yeah, that's weird.
I will be back after
info should be a list of
I actually don't even know why we have
I actually don't even know why we have
it as a list of dictionaries.
All
All
right. This is
right. This is
the your Python end commented up.
refactor and square a little bit here.
Yeah, this code's going to get a lot
Yeah, this code's going to get a lot
easier.
I think we're I'm definitely going to
I think we're I'm definitely going to
have these tutorials done today unless I
have these tutorials done today unless I
like run out of
like run out of
steam. That should be at least done.
can actually just copy this. I think
Perf score return on. Oh,
good. Some of these comments are just
good. Some of these comments are just
going to be the same.
actually rethe this as well. While we're
here, buffer colors.
We have
We have
a we just don't have a computer
a we just don't have a computer
observation. That's
fine. Is just a required phone.
This is your third first time on
stream. Third first time. That's a new
stream. Third first time. That's a new
one.
You're funny,
man. Are you here for any of the
man. Are you here for any of the
technical content and just like trolling
technical content and just like trolling
too? Or you just here to troll? I got to
too? Or you just here to troll? I got to
know.
anything else that I forgot to comment.
All right, this is pretty solid.
PC
PC
specs. Uh, I mean, this one's got a 4090
specs. Uh, I mean, this one's got a 4090
and
and
9950X and 128 gigs DR5.
9950X and 128 gigs DR5.
Uh the one in the back that's on the
Uh the one in the back that's on the
shelf way back there has got a 5090 and
shelf way back there has got a 5090 and
the same
the same
otherwise. And then we've got like eight
otherwise. And then we've got like eight
more that are 4090s with Intel chips
more that are 4090s with Intel chips
that I don't recommend. And
that I don't recommend. And
uh let's see. We're getting two tiny
uh let's see. We're getting two tiny
boxes in like a week or two that are
boxes in like a week or two that are
6490s a piece.
6490s a piece.
Um and then hopefully we'll be getting
Um and then hopefully we'll be getting
more after that.
We got a lot of PCs.
Now, can you send me a
Now, can you send me a
PC? No. PCs are for reinforcement
learning. You can get PC access if you
learning. You can get PC access if you
do reinforcement
learning. All
right, cool. This seems uh this seems
decent. Got to do reinforcement
decent. Got to do reinforcement
learning. I don't know what you mean.
learning. I don't know what you mean.
You need to do the
RL. You got 55090s? No, I only have one
RL. You got 55090s? No, I only have one
5090 at the moment, unfortunately. I
5090 at the moment, unfortunately. I
would like many more 5090s, but
would like many more 5090s, but
unfortunately
tariffs. I want to buy like another 30
tariffs. I want to buy like another 30
of them. 30 or 40.
I think the play might just be to wait
I think the play might just be to wait
for a little bit more revenue and then
for a little bit more revenue and then
just buy them anyways. We'll see. Tiny
just buy them anyways. We'll see. Tiny
boxes are not here yet. Actually, that
boxes are not here yet. Actually, that
is a good point. Let me go check
is a good point. Let me go check
on where are my tiny boxes.
Where the heck is uh I put these on
Where the heck is uh I put these on
my can't find the
email. Oh yeah, there it
email. Oh yeah, there it
is. So this was on
your
your
order. You've received your
order. They actually should be here.
What programming languages do you know?
What programming languages do you know?
All of
them. But I can kind of write code in
them. But I can kind of write code in
any like if you give me like a day, I
any like if you give me like a day, I
can write code in just about
anything. I mostly write C and
Python. So you do see my comments.
I don't know why you do that for free,
I don't know why you do that for free,
man. You can make good money doing that.
All right, this seems like it's good. We
All right, this seems like it's good. We
just got to make sure I didn't break a
just got to make sure I didn't break a
ton of
things. This actually uh cleanly
exits. Target has mem leaks.
Tiny box single board. Now it's um
uh it is
uh George Hotz's like six
uh George Hotz's like six
GPU
box. Let me do let me finish this and
box. Let me do let me finish this and
then there's and then I'll go finish my
then there's and then I'll go finish my
my
my
set. Figure out what else from there.
thought that it's just like unload
texture. Unload texture. Close window.
texture. Unload texture. Close window.
Oh, did I forget to close the window?
I did forget to close the window.
Perfect. No mem links.
What happened
here? End of creator must be a list of
here? End of creator must be a list of
callables. Oh, I know what it is
callables. Oh, I know what it is
actually.
by. So we have from
upper pi squared.
Why is it on size
Why is it on size
squared? Oh, I
squared? Oh, I
know. Oops.
Oh, that's right.
A little bit more work it seems to be
A little bit more work it seems to be
done to get these things to like rain
done to get these things to like rain
correctly.
I'm going to go get my set done and then
I'm going to go get my set done and then
uh Oops. and kill this bug. And then we
uh Oops. and kill this bug. And then we
will be back. I will finish cleaning up
will be back. I will finish cleaning up
all these MS and then we will make the
all these MS and then we will make the
ducks.
Decent enough leg training for today.
See, Linky, this is the type of these
See, Linky, this is the type of these
are the type of improvements we
are the type of improvements we
need. All right, I'll think
need. All right, I'll think
of puffing alternatives.
Why is this thing not train?
How's there no puffer squared?
All the
balls, Python
version and then the
solves takes it 40 million
steps. Hang on.
steps. Hang on.
This is 8192
This is 8192
total. Oh, wait. No, this is because
total. Oh, wait. No, this is because
that's
that's
eval. So, it'll do 20
eval. So, it'll do 20
mil in solved in pretty well. Yeah, get
mil in solved in pretty well. Yeah, get
solved in 20 mil. And then just runs
solved in 20 mil. And then just runs
extra
ebells. Yeah. And this is fine that we
ebells. Yeah. And this is fine that we
have it this way because this is like
have it this way because this is like
our
our
standard standard
configuration. So this should
be should really be
be should really be
8192 on the same two workers would be
Okay. So, this is 400 something K.
Oh, and this does train. Cool. This
Oh, and this does train. Cool. This
totally trains just the same.
So, we can actually just start
So, we can actually just start
um we can start putting these
um we can start putting these
into uh into tutorials and docs, right?
into uh into tutorials and docs, right?
2 p.m. I've got plenty of time. Let's
2 p.m. I've got plenty of time. Let's
commit all this stuff up.
commit all this stuff up.
Uh, we should
Uh, we should
probably There's one other thing we
should do like a speed test thing.
Oh yeah, this is not vectorzed, huh?
All
right, this is pretty good. So, this is
right, this is pretty good. So, this is
a
270K like this.
We'll put this test
on
on
whereh pi Okay.
Oh, that does make it substantially
Oh, that does make it substantially
faster. Cool.
We got to at least be fair to our
We got to at least be fair to our
Python,
right? We can't make the Python like
right? We can't make the Python like
even worse than it already is. That
even worse than it already is. That
would just be
would just be
needlessly
needlessly
silly. Uh what else?
F.
This should give us some crazy number.
Okay. So, I think that um 113 million
Okay. So, I think that um 113 million
steps per second is pretty good. I don't
steps per second is pretty good. I don't
know about
you, I think that's pretty good. That
you, I think that's pretty good. That
put this in
target. Some
M's agents number four.
Python perf
test back
reset. Boom.
could not
broadcast. 16,000
two mains, buddy.
What's
this? I think we just like break point.
That's pretty good.
Low, high, and then
size.
broadcast output
shape. All right, this is cool.
shape. All right, this is cool.
So we
do
do
action environ.
All right. So, this one gives
All right. So, this one gives
you 35 million SPS. Still
you 35 million SPS. Still
decent. Decent 35 million, you
decent. Decent 35 million, you
know. And then you have your pain points
know. And then you have your pain points
listed
listed
here. I'm going to just put these
into We'll put these into here for
into We'll put these into here for
now. I can commit this cleanly.
There we go.
I change
there. That's fine.
And this is where I wanted to cach some
And this is where I wanted to cach some
credentials. And we can push
this command.
Okay, now we have all our nice new
Okay, now we have all our nice new
tutorial items and
uh we should be able to just open
uh we should be able to just open
up the uh hang
up the uh hang
on should be able to just
on should be able to just
open buffer tank buffer
open buffer tank buffer
AI
docs ML
file and look at that actually it has
file and look at that actually it has
um that's me right there doing like I
um that's me right there doing like I
have the stream on
have the stream on
a new site. And then what we get to do
a new site. And then what we get to do
is we get to add this to our docs
page. I'm going to put this
page. I'm going to put this
here. Uh we are pretty well set up with
here. Uh we are pretty well set up with
the code. So I'm going to do a couple
the code. So I'm going to do a couple
quick sets so I get some sort of
quick sets so I get some sort of
exercise in
exercise in
today. and then I will be back in a
today. and then I will be back in a
couple
couple
minutes and we will um write the
minutes and we will um write the
documentation section for these M.
documentation section for these M.
You'll have feedback on what you'd like
You'll have feedback on what you'd like
to see or uh making it easier for other
to see or uh making it easier for other
folks to write cool ultra high perf sims
folks to write cool ultra high perf sims
for RL, you let me know.
I'll be back.
All
right. I do not like writing
right. I do not like writing
docs and I especially don't like writing
docs and I especially don't like writing
docs for the sake of writing docs.
So I'm just going to try to look at this
So I'm just going to try to look at this
and figure out what is the
and figure out what is the
shortest simplest thing we can give
shortest simplest thing we can give
people so that they will be able to
people so that they will be able to
write M's.
The main thing I want to decide on is
The main thing I want to decide on is
whether I want to actually put the code
whether I want to actually put the code
here or just link to it.
honestly the stuff I have here is pretty
honestly the stuff I have here is pretty
well thought out.
well thought out.
Um,
Okay.
I don't think what I even
I don't think what I even
put in here.
I'm just trying to think how much of the
I'm just trying to think how much of the
code I want to inline.
I'm trying to think like what is the
I'm trying to think like what is the
amount of stuff I need to write here to
amount of stuff I need to write here to
make this clear without just writing a
make this clear without just writing a
whole bunch of like stuff people only
whole bunch of like stuff people only
need to read.
It might be better to just inline the
It might be better to just inline the
code. Now
I think it would make sense too.
I kind of know all the things that I
I kind of know all the things that I
want. I just noticed I don't know how I
want. I just noticed I don't know how I
want to structure this at
want to structure this at
all. Also don't want to agonize over it
all. Also don't want to agonize over it
for too long. It's like pretty basic
for too long. It's like pretty basic
ultimately, right? Like I have good
ultimately, right? Like I have good
commented code for how to do this.
It's a good idea to just paste it in
here. Kind of fine
too. It's a lot of code to put like on a
too. It's a lot of code to put like on a
doc page though.
I want to just put no code here and link
I want to just put no code here and link
to it, but like I something's telling me
to it, but like I something's telling me
that's a dumb
idea. If I just say I'm going to paste
idea. If I just say I'm going to paste
the code and do it like then it's a lot
the code and do it like then it's a lot
easier, right?
like I don't think this that was it. No.
Yeah, let's paste. Let's see what this
Yeah, let's paste. Let's see what this
looks like if I um I just like start
looks like if I um I just like start
pasting this in
Got a a whole bunch of these nice
Got a a whole bunch of these nice
bluebirds
bluebirds
outside. Very peaceful.
I think I will go for a good walk after
I think I will go for a good walk after
finishing all these docks.
just makes up random stuff. The model
just makes up random stuff. The model
does
I think what I'll do is I'll put this
I think what I'll do is I'll put this
one online and I'll put I'll leave the
one online and I'll put I'll leave the
other one on GitHub.
really was 400k,
right?
Yeah. Something like that.
Add this
Add this
here. And bugs, man.
Wham!
Oops. here.
the macro. Thank
Okay. So, let's see what this looks like
Okay. So, let's see what this looks like
so far. If I uh open this
so far. If I uh open this
thing. If I open this thing
up. So that Okay, that italics doesn't
up. So that Okay, that italics doesn't
help.
How much code this
Hey Spencer, I'm trying to figure out
Hey Spencer, I'm trying to figure out
what the dock should look like
here. Oh, this was totally fine. Hang
here. Oh, this was totally fine. Hang
on.
There we go.
I think that's decent. And I think what
I think that's decent. And I think what
I'll do is I'll put
I'll do is I'll put
um I'll put this as like the first M.
um I'll put this as like the first M.
I'll give some additional tips with like
I'll give some additional tips with like
debugging and
debugging and
um I will link the uh the other target
um I will link the uh the other target
ends.
ends.
And then that will be
good. That sounds
good. That sounds
good. That's what we will do.
This is
cool. See, the memory obsession though
cool. See, the memory obsession though
is a bit weirder than the perf
obsession.
also true.
Okay, let's see what the uh the docks
Okay, let's see what the uh the docks
look like.
All right, let's see how this does.
So we now
So we now
have a
have a
tutorial writing custom environments
tutorial writing custom environments
that run 1 mil plus steps per
that run 1 mil plus steps per
second. We have the square end which is
second. We have the square end which is
page and a half of code and then we
page and a half of code and then we
have a little explanation. It's farewell
have a little explanation. It's farewell
commented. Then we have the C version.
Uh, I messed up a
Uh, I messed up a
parenthesy was a paragraph or whatever.
Not bad.
probably clean up links and things,
but probably put some like bunch of
but probably put some like bunch of
things. But this
things. But this
is I mean this is like a nicely
is I mean this is like a nicely
commented full code example.
That's decent
progress. Decent
progress. Got the M's done. They're
progress. Got the M's done. They're
clean. Um, there might still be a better
clean. Um, there might still be a better
way I can present it on the site.
way I can present it on the site.
I think it'll be easier once I have like
I think it'll be easier once I have like
the inlinable demos or
whatever. I could also put a GIF, but
whatever. I could also put a GIF, but
probably just inline a demo.
Let me see.
Um, I should probably just add some
Um, I should probably just add some
debug tips actually. Let's do that.
Yeah, I know what we're going to do.
Yeah, I know what we're going to do.
We're not a checklist.
I think it does reset, does
I think it does reset, does
it? I honestly
forget. I don't think it does.
That's
pretty good.
All
right, that ought to be useful,
right? This
Very nice. All right, I'm happy with
that.
Cool. Um, it is 400
Cool. Um, it is 400
p.m. Happy Thursday as well. There's a
p.m. Happy Thursday as well. There's a
restroom real quick and then I think we
restroom real quick and then I think we
um should probably like go check on and
um should probably like go check on and
set up some sweeps instead of just
set up some sweeps instead of just
working on docks all day cuz we got to
working on docks all day cuz we got to
check on our experiments and make sure
check on our experiments and make sure
we're
we're
uh we're making progress towards the
uh we're making progress towards the
final final experiments on here. So, no
final final experiments on here. So, no
more freaking boring docs for today. I
more freaking boring docs for today. I
got the portion of that that I wanted
got the portion of that that I wanted
done, which is we have a
done, which is we have a
decent environment tutorial, which is
decent environment tutorial, which is
mostly just a bunch of nicely commented
mostly just a bunch of nicely commented
code.
code.
Um, but also, you know, some additional
Um, but also, you know, some additional
things on debugging and whatnot.
Yeah, I will be back. Here's the
Yeah, I will be back. Here's the
restroom. Get a couple sets in real
restroom. Get a couple sets in real
quick and then we will uh yeah, we'll
quick and then we will uh yeah, we'll
actually get some uh experiments going.
actually get some uh experiments going.
We'll see how that goes.
All
right. Experiment's going. Unless
right. Experiment's going. Unless
there's
anything. All right. Nothing to deal
anything. All right. Nothing to deal
with
with
there. One message.
So, I haven't run any experiments in a
So, I haven't run any experiments in a
couple of days
couple of days
actually, but we had mazes working very
nicely. Trying to think what type of
nicely. Trying to think what type of
stuff. I think we just want to get meta
stuff. I think we just want to get meta
going. Let me see if I can get the meta
going. Let me see if I can get the meta
out to
train. Oh yeah, they changed a whole
train. Oh yeah, they changed a whole
bunch of stuff, didn't
bunch of stuff, didn't
they?
they?
Forgot about
that. I really want to get a sweep for
that. I really want to get a sweep for
these guys. I'm trying to
these guys. I'm trying to
think the easiest way I can do that is.
Um, I think
This get this to train.
What's their
What's their
setup? Setup
have they just do a pie bind
have they just do a pie bind
extension. No other
extension. No other
depths. No, they have
requirements
requirements
upper. They have
upper. They have
requirements
requirements
pinned. They have they've got way too
pinned. They have they've got way too
much stuff pinned.
much stuff pinned.
Yeah, you don't need all that, guys.
Let me just see if there's anything
Let me just see if there's anything
that's like bad
that's like bad
bad. Think
so. Well, we'll try this and see if it
uh it messes anything up.
That
it. Oh yeah, they built it built
it. Oh yeah, they built it built
something.
This is like this cool like factory grid
This is like this cool like factory grid
based M type
thing
here. Oh my
conf a pip.
Is it hydra
core? Just add this N.
Duck
Duck
DB. You got to be kidding me. Duck
DB. You got to be kidding me. Duck
DB. The hell is that?
Okay, so
apparently there Can you interpret on
this? I don't know what this is.
full
full
key map
builder that room.
Okay, you just ask
him. They have like a new
him. They have like a new
basic config maybe for me.
So is it benchmark
yl the most you're going to get is
yl the most you're going to get is
uh some incline bench sets at the
moment. Move some deprecated features.
moment. Move some deprecated features.
Okay.
Okay.
So
So
like this it
like this it
though. I don't think it's this
though. I don't think it's this
one benchmark
sample. It's sampling
sample. It's sampling
removed. All right.
We need 16 agents in
here.
24.
Why does not match number of agents in
Why does not match number of agents in
map?
Oh, I guess it's because this needs to
Oh, I guess it's because this needs to
be Four.
No. Is that actually a Okay.
Yeah, this really this really annoys
me. I mean, technically I could just
me. I mean, technically I could just
change. Okay, let's say that I use their
change. Okay, let's say that I use their
original config, right? Did it
load? Okay, it does. So, I just need to
load? Okay, it does. So, I just need to
change the batch sizes
then what did this? So, I have 128
then what did this? So, I have 128
times in 4096 total
agents or is it divisible?
agents or is it divisible?
Nope. Uh, so I just have to get
That's actually really
That's actually really
annoying. Uh, I can multiply
by
8.
8.
Maybe that's not
Maybe that's not
terrible. So, we do like 30
72 M.
72 M.
And we have a batch size of 128
And we have a batch size of 128
m. This Yeah, this
works. Would you mind running that trady
works. Would you mind running that trady
val? Yeah, sure
thing.
thing.
Um, where do I get it?
DM you. All right. Let me know.
Okay, that
Okay, that
trans.
trans.
Boom. Doesn't give us any
stats. At least not so
stats. At least not so
far. Go get this repo. Oh, there it is.
far. Go get this repo. Oh, there it is.
Okay, it gives us stats. Let me put this
Okay, it gives us stats. Let me put this
on
on
Neptune so I can do this in the
Neptune so I can do this in the
background while we uh while I do the
background while we uh while I do the
other eval thing.
get my API
token. Okay. Very very secret repo.
Cops are
takes forever to
takes forever to
build. Oh, you know
build. Oh, you know
why? God damn
it. Forget the magic argument that makes
it. Forget the magic argument that makes
it fast and also not
break. Love when they're magic arguments
break. Love when they're magic arguments
that you need to make things not break.
Oh, I didn't train this for enough steps
Oh, I didn't train this for enough steps
at all, did
I? Okay, that after I fix this thing for
you. I don't know why your eval thing
you. I don't know why your eval thing
doesn't
doesn't
work and you see your errors and stuff
work and you see your errors and stuff
at some point.
token.
that terminal is on. I I mean it doesn't
that terminal is on. I I mean it doesn't
matter. It's like
matter. It's like
semi-privateish, right?
I'm just checking if the curve goes up
I'm just checking if the curve goes up
on a environment real quick and then
on a environment real quick and then
we'll go back to the other environment
we'll go back to the other environment
to check if the curve goes
to check if the curve goes
up. Pretty much it. Is it score? Score
up. Pretty much it. Is it score? Score
is the same metric, right?
What are we supposed to get out of
this? Oh, is it are the prams like
this? Oh, is it are the prams like
fiddly?
Oh, is it supposed to be perfect six to
Oh, is it supposed to be perfect six to
70 or what?
70 or what?
This score is 60. Like
what? Like
perf, you
know,
know,
460k.
Okay, that's going up.
6 million steps per
second. I'm going to do a set while I
second. I'm going to do a set while I
wait for this [ __ ]
There you
go. Even better.
We'll
do. We get 80 on this. More than 80.
I don't know. Maybe I just magically
I don't know. Maybe I just magically
improved
things. Hour of
puffer. Five minute training for half a
puffer. Five minute training for half a
billion steps.
Uh, is the thing supposed to render with
Uh, is the thing supposed to render with
the numbers all over the entire
Ran or did you not commit a font
file? Can you check if you committed
file? Can you check if you committed
your font file?
Don't
Don't
file.
What? Confused.
Yeah, just add add your the font file or
Yeah, just add add your the font file or
whatever. Go add that. Pull
whatever. Go add that. Pull
this. I'll give you your eval.
way you do that. I'll
way you do that. I'll
do back to the um at the experiment.
This is like what we're supposed to get,
right? One of these
right? One of these
isish. See how this goes if I run it
for 100 mil.
Not. Oh, Twitter 8 message. Gotcha.
Not. Oh, Twitter 8 message. Gotcha.
Well, when you get me that file, I will
Well, when you get me that file, I will
fix this.
All right. So, let's just go get the
All right. So, let's just go get the
metab box puff box.
for container.
Right. We have to
do Oh, they have a different
do Oh, they have a different
one. Hang on. They have a different
one. Hang on. They have a different
config they want me to use.
See what I can get to work on
See what I can get to work on
this. Heck could I just do
Um,
The heck is wrong with this? Oh, we have
The heck is wrong with this? Oh, we have
the wrong popper installed.
Find what the base image is.
Find this.
23 gigs.
wrong with This
This is running
soon.
soon.
Holy thunderstorm
Oh, it has started completely
Oh, it has started completely
thundering. Hopefully we uh don't get
thundering. Hopefully we uh don't get
knocked out on the internet.
Wait, you need to run register.
Wait, where is this
Wait, where is this
thing? Oh
thing? Oh
jeez. All this stuff.
Then this calls this logger which is
Then this calls this logger which is
[Music]
like Yeah, that's annoying as hell.
and I just like do this.
and just do this.
Okay.
Okay.
So, possibly we can make this
faster if we update to our latest param
sets. I believe this is
Oh, it's really thundering outside.
Oh, it's really thundering outside.
Well, if I'm off if I go offline, you
Well, if I'm off if I go offline, you
know
know
why. It's really
thundering. Okay, we want
thundering. Okay, we want
64 64 of these at MMS, I
believe. And 32 of them
believe. And 32 of them
per per
per per
batch. And then we will keep this 32k
And we'll see. We will see how this
goes. Okay. So, this is actually
goes. Okay. So, this is actually
slower, but we will see if it's slower
slower, but we will see if it's slower
on the other GPU.
Okay, let's go grab here.
Why not
Why not
on this box
here. Oh.
Metagrid
Metagrid
requires. Well, we don't like that.
Huh. So, it's literally ignoring the uh
Huh. So, it's literally ignoring the uh
the Rex in here. Weird.
just grab the newest
one, our fork.
Ah, this is actually pinned. I don't
Ah, this is actually pinned. I don't
know how it is that I've been able to
know how it is that I've been able to
run
it does the speed environment your
it does the speed environment your
policies don't take 99.99% as the
thing like in the majority of RL
thing like in the majority of RL
researcher environment takes 90% of the
researcher environment takes 90% of the
time. The fact that our policies are
time. The fact that our policies are
taking like are taking most of the time
taking like are taking most of the time
now is just because we've optimized our
now is just because we've optimized our
environment so
well. So like that wasn't a thing before
well. So like that wasn't a thing before
us. Like pretty much everyone else in
us. Like pretty much everyone else in
the whole rest of RL is spending like a
the whole rest of RL is spending like a
good chunk of their uh compute if not
good chunk of their uh compute if not
the majority of it on the nth
Okay. So this
Okay. So this
is
370. Grab a new one of
these. Make sure we really give these
these. Make sure we really give these
guys some good policies and good a good
guys some good policies and good a good
sweep setup.
Yeah. Okay. So, that doesn't make a huge
difference. Did I do that
right?
right?
416. I did that totally wrong. It's
416. I did that totally wrong. It's
supposed to be 256*
supposed to be 256*
No.
No.
512.
Yeah. 12
28. I'll see if this is suddenly
fast. Yeah, but not everything is VLMs.
Like we're solving we are solving
Like we're solving we are solving
problems for people with like single
problems for people with like single
digit million
parameters. There we go. 800K.
Make sure that this
Make sure that this
is logging for us.
Okay, there we
Okay, there we
go. I will let this
go. I will let this
run and uh we will see how this does.
run and uh we will see how this does.
Here's a restroom and get a good set in
Here's a restroom and get a good set in
and then we will uh we'll get the sweep
and then we will uh we'll get the sweep
set up. We'll do a few other things. Be
set up. We'll do a few other things. Be
good.
cold
outside.
Borman. Hey, Plasma.
Borman. Hey, Plasma.
What's up, man?
being confused at what policy is doing.
Policying bomb pushed up. Let me get
Policying bomb pushed up. Let me get
that Spence.
Do you have the rewards
Do you have the rewards
negative? It's never that. I've never at
negative? It's never that. I've never at
I've never once had the sign wrong, but
I've never once had the sign wrong, but
it's funny.
Okay. Make sure I'm on the right
Okay. Make sure I'm on the right
machine. I am on the right
machine. Brake system packages.
system. I got to figure out a way to
system. I got to figure out a way to
make it just the default be that because
make it just the default be that because
it's ridiculous.
packaging
packaging
was. It's all
right. All right. Puffer
right. All right. Puffer
[Music]
evalu.
Oops. I forget to
pull. Yes.
how it's supposed to
[ __ ] Font's a little like all over the
[ __ ] Font's a little like all over the
place, but
I will give you that file.
Does it is it resizable?
All right. I will send you
All right. I will send you
this. I can keep running these here if
this. I can keep running these here if
you
want. I can also help you fix your
want. I can also help you fix your
uh render
uh render
setup. All right. Take a quick look. I
setup. All right. Take a quick look. I
will make sure this this thing is good
will make sure this this thing is good
in the
meantime. Not this stuff again. Bound at
meantime. Not this stuff again. Bound at
05.
Yeah.
Yeah.
Okay. Is a
Okay. Is a
run.
run.
Amazing. We will run a sweep though.
E7 like one
E7 like one
[Music]
[Music]
E8
E8
I8
I8
auto and we will see how
auto and we will see how
this
Horizon mini
batch and we'll see how everything else
and I'm still on the
and I'm still on the
wrong uh I'm on the
wrong uh I'm on the
wrong end. Lovely.
All right. So, for real this
time, we'll comment all this.
Um,
Okay, we will see if this discovers
Okay, we will see if this discovers
anything
anything
interesting and we'll keep checking back
interesting and we'll keep checking back
in on this every so often. Goes over
in on this every so often. Goes over
here for
now. What do you want me to do with the
now. What do you want me to do with the
uh render? Spencer want me to render
uh render? Spencer want me to render
more stuff. What do you
want? I guess in the meantime while I'm
want? I guess in the meantime while I'm
look
look
at our
container reading
mode. I could push the Docker for now,
mode. I could push the Docker for now,
right?
Make sure I don't screw anything up
time. Fine.
takes forever even with
takes forever even with
UV Docker
UV Docker
build. I just want to upload it so that
build. I just want to upload it so that
I can see how large the
I can see how large the
um compressed the compressed file is.
press size of 9.8
gigs. We'll see what the new one is.
downloads. For some reason, the depths
downloads. For some reason, the depths
file has 2K
downloads. Somebody must have it in
downloads. Somebody must have it in
their
CI. All right, so
Docker
push. What's the push then? How's the
push. What's the push then? How's the
push
push
work? Username
tag.
tag.
Username
Username
name tag.
Does this not just
Does this not just
work?
work?
Oh,
push. And then there is some uh
push. And then there is some uh
shenanigans.
Uh, I don't remember having a freaking
Uh, I don't remember having a freaking
2FA for Docker of all bloody
things. I don't have one, my
things. I don't have one, my
guy. Oh, dummy. It's
guy. Oh, dummy. It's
right, dummy.
Is it in the wrong browser?
There we
There we
go. So, uh we will see how large the
go. So, uh we will see how large the
docker
docker
is. Seems large.
If it's like 20 gigs, then I'm going to
If it's like 20 gigs, then I'm going to
have to figure out what the heck is
have to figure out what the heck is
wrong with it because it should not be
Uh this might be reream
Uh this might be reream
actually. Uh this might be reream
actually. Uh this might be reream
actually.
We
record. We
record. We
record.
record.
Oh, okay.
note to self, don't push Docker images
note to self, don't push Docker images
on stream
on stream
again. Um, I'll be back after dinner.
again. Um, I'll be back after dinner.
And this is [ __ ]

Kind: captions
Language: en
goal for the rest of today is to get the
goal for the rest of today is to get the
uh two new sample
uh two new sample
environments commented, cleaned up,
environments commented, cleaned up,
documented, and also to put a tutorial
documented, and also to put a tutorial
on our homepage for
on our homepage for
that. Do that and then we're probably
that. Do that and then we're probably
going to see if we can get some sweeps
going to see if we can get some sweeps
going on uh some of the more complicated
going on uh some of the more complicated
MS, try to stress test some things, try
MS, try to stress test some things, try
to fix some user issues like some user
to fix some user issues like some user
submitted bugs, just variety of
submitted bugs, just variety of
different things.
different things.
So, so speaking of which, Captain had
So, so speaking of which, Captain had
some uh some issues. So, let's see if
some uh some issues. So, let's see if
he's
around and helps.
We'll see if he's uh he's here. And in
We'll see if he's uh he's here. And in
the meantime, we
the meantime, we
will that over
there and uh keep on this little
there and uh keep on this little
refactor.
Okay, that gets rid of this function and
Okay, that gets rid of this function and
this function. Done.
and that's 228
and that's 228
lines including
lines including
comments. Let's see if there's anything
comments. Let's see if there's anything
in here that seems redundant.
Got our
Got our
strus. Client has a strct.
Yeah, I think that's
fine. Have the extra client strct like
fine. Have the extra client strct like
that.
I am a little bit knowledge impaired
I am a little bit knowledge impaired
real RL learner. Could you explain if
real RL learner. Could you explain if
not taking too much time what is the
not taking too much time what is the
exact issue that puffer lib is
exact issue that puffer lib is
targeting? My exam understanding the
targeting? My exam understanding the
direct goal is to sit between the
direct goal is to sit between the
environments and learning how them to
environments and learning how them to
deal with vectorization. Therefore am I
deal with vectorization. Therefore am I
misinformed? So that is what we started
misinformed? So that is what we started
off doing in puffer. So like in the
off doing in puffer. So like in the
first few months of dev that was the
first few months of dev that was the
original goal was that multipprocessing
original goal was that multipprocessing
as it is typically used is very janky.
as it is typically used is very janky.
It only supports certain types of m
It only supports certain types of m
doesn't have m native multi- aent
doesn't have m native multi- aent
support doesn't support complex
support doesn't support complex
observation and action spaces whole
observation and action spaces whole
bunch of problems and it's incredibly
bunch of problems and it's incredibly
slow. So the first thing that puffer did
slow. So the first thing that puffer did
was fix that. So we have very fast uh
was fix that. So we have very fast uh
parallel processing. It works with our
parallel processing. It works with our
own environments. It works with
own environments. It works with
gymnasium environments. It works with
gymnasium environments. It works with
petting zoo environments. It uses much
petting zoo environments. It uses much
much more optimized shared memory and a
much more optimized shared memory and a
whole bunch of other tricks uh
whole bunch of other tricks uh
asynchronous simulation, double
asynchronous simulation, double
buffering, lots of different tricks that
buffering, lots of different tricks that
are in the paper uh to make
are in the paper uh to make
vectorization up to like 10x
vectorization up to like 10x
faster. So that was the first thing we
faster. So that was the first thing we
did, but that's like we've had we have
did, but that's like we've had we have
two major releases basically since then.
two major releases basically since then.
So now we also have uh 20 ultra high
So now we also have uh 20 ultra high
performance environments written in C
performance environments written in C
that we use for all our own research
that we use for all our own research
that run at millions of steps per second
that run at millions of steps per second
and we have a trainer that actually can
and we have a trainer that actually can
train at millions of steps per second.
train at millions of steps per second.
So I mean if you compare to like the
So I mean if you compare to like the
libraries that we were trying to build
libraries that we were trying to build
infrastructure to support before like
infrastructure to support before like
SP3 and RLIB and all those we just train
SP3 and RLIB and all those we just train
a 100 to a thousand times faster now. So
a 100 to a thousand times faster now. So
we still provide these compatibility
we still provide these compatibility
tools for use with other libraries, but
tools for use with other libraries, but
really at this point you should consider
really at this point you should consider
using our tools because everything is
using our tools because everything is
just so so dramatically faster. We're
just so so dramatically faster. We're
also doing our own algorithm research
also doing our own algorithm research
now. So we have a better PO like PO with
now. So we have a better PO like PO with
additional things that is just better
additional things that is just better
across the board.
across the board.
Um we have benchmarks on all of our
Um we have benchmarks on all of our
environments. We have we're setting like
environments. We have we're setting like
soda on other people's environments. Uh
soda on other people's environments. Uh
so it's become much much more than what
so it's become much much more than what
we started essentially. It's really it's
we started essentially. It's really it's
a comprehensive effort now to just make
a comprehensive effort now to just make
reinforcement learning ultra fast, ultra
reinforcement learning ultra fast, ultra
simple and very very stable and sane.
simple and very very stable and sane.
That's what Puffer
does. And I stream all of the dev
does. And I stream all of the dev
because why not? It's all open source
because why not? It's all open source
and we're always looking to get more
and we're always looking to get more
people uh interested in contributing
people uh interested in contributing
building environments, helping us run
building environments, helping us run
experiments, helping on science side,
experiments, helping on science side,
all the different things that we do.
trainers departed from torch tracks. No,
trainers departed from torch tracks. No,
they're still implemented in uh our
they're still implemented in uh our
trainer still in torch. But the thing is
trainer still in torch. But the thing is
it's just like it's so much more
it's just like it's so much more
efficient than what was being done
efficient than what was being done
before. Like there were a half dozen
before. Like there were a half dozen
different places where people were just
different places where people were just
losing 99% of their PF in training. Um
losing 99% of their PF in training. Um
to give you an idea, right? Like the
to give you an idea, right? Like the
vast majority of training that's done in
vast majority of training that's done in
reinforcement learning, like you look at
reinforcement learning, like you look at
your average lab, a lot of them are
your average lab, a lot of them are
still on singledigit thousand step per
still on singledigit thousand step per
second trainers. Um, some of the better
second trainers. Um, some of the better
ones are now in like double digits. And
ones are now in like double digits. And
then the ones with full GPUms sometimes
then the ones with full GPUms sometimes
get triple digit like thousand. So like
get triple digit like thousand. So like
100,000 or something, but that's like a
100,000 or something, but that's like a
very very few labs with a very few
very very few labs with a very few
environments that can even do that. So
environments that can even do that. So
our training we run typically between
our training we run typically between
400,000 and 4 million steps per second
400,000 and 4 million steps per second
depending on the architecture and the um
depending on the architecture and the um
the model size. Uh a lot of our simpler
the model size. Uh a lot of our simpler
tests that we use for primary research
tests that we use for primary research
run like 2 to four million steps per
run like 2 to four million steps per
second training on one
second training on one
GPU. And our training code is simpler
GPU. And our training code is simpler
than the slower libraries that it
than the slower libraries that it
replaces. Um it works out of the box
replaces. Um it works out of the box
with multi- aent. It works out of the
with multi- aent. It works out of the
box with a whole bunch of things that
box with a whole bunch of things that
aren't even supported generally
aren't even supported generally
elsewhere. Uh the environment code is
elsewhere. Uh the environment code is
way simpler. It's way way easier to
way simpler. It's way way easier to
build environments in Puffer Lib than it
build environments in Puffer Lib than it
is in Jax, for instance, or like PyTorch
is in Jax, for instance, or like PyTorch
accelerated GPUms. Way way faster to
accelerated GPUms. Way way faster to
build our stuff because you just write
build our stuff because you just write
arbitrary code and it's fast. Um yeah,
arbitrary code and it's fast. Um yeah,
it's like a really comprehensive effort,
it's like a really comprehensive effort,
but we're not like we're not giving you
but we're not like we're not giving you
our own trainer API that's like you have
our own trainer API that's like you have
to go learn. It's just PyTorch. It's
to go learn. It's just PyTorch. It's
very vanilla pietorch. It's just that
very vanilla pietorch. It's just that
our code is way better optimized, right?
our code is way better optimized, right?
Um, we didn't come up with like a crazy
Um, we didn't come up with like a crazy
new interface. It's almost identical to
new interface. It's almost identical to
the gymnasium vector interface, just
the gymnasium vector interface, just
faster, right? So, we really try to make
faster, right? So, we really try to make
stuff just
stuff just
simple. Working through SMB now. Haven't
simple. Working through SMB now. Haven't
finished P1 yet. Notice the proper
finished P1 yet. Notice the proper
vectorization matters a lot in
vectorization matters a lot in
responsiveness. Yes, it does.
responsiveness. Yes, it does.
Multi-agent sounds like a nightmare to
Multi-agent sounds like a nightmare to
Marshall. It's not is the thing. It's
Marshall. It's not is the thing. It's
literally the way that we do it.
literally the way that we do it.
Multi-agent is no more complicated than
Multi-agent is no more complicated than
single agent at all.
single agent at all.
So, this end that I built
So, this end that I built
here actually. Can I can I eval this for
here actually. Can I can I eval this for
you?
Let me see if I can just remember which
Let me see if I can just remember which
of these checkpoints is any good. Okay,
of these checkpoints is any good. Okay,
so here's a multi-agent environment
so here's a multi-agent environment
about Puffer Libs eating stars. I built
about Puffer Libs eating stars. I built
this just as a tutorial as like a really
this just as a tutorial as like a really
simple thing to get people started. This
simple thing to get people started. This
thing trains 100 million steps in 25
thing trains 100 million steps in 25
seconds on my local GPU. Um, it worked
seconds on my local GPU. Um, it worked
instantly with our just our default
instantly with our just our default
training setup and I built this thing
training setup and I built this thing
yesterday on stream. So there was no
yesterday on stream. So there was no
code for this yesterday and now it just
code for this yesterday and now it just
works.
Welcome, sub. What's
up? So, yeah, this thing trains four
up? So, yeah, this thing trains four
million steps a second. And like it's a
million steps a second. And like it's a
very simple end, but this is really
very simple end, but this is really
really clean as far as like multi- aent
really clean as far as like multi- aent
uh multi- aent just like forging type
uh multi- aent just like forging type
behavior
goes. Yeah, the evals are on the website
goes. Yeah, the evals are on the website
as well. So, you can literally watch
as well. So, you can literally watch
agents playing the games in your
agents playing the games in your
browser.
browser.
and they're running locally as
well. I will be honest with you also I
well. I will be honest with you also I
have I've never read this on an embaro
book. I do a lot more engineering side
book. I do a lot more engineering side
stuff that really seems to matter a lot.
stuff that really seems to matter a lot.
I do some algorithm stuff to be fair. I
I do some algorithm stuff to be fair. I
occasionally we have like some algorithm
occasionally we have like some algorithm
advancements in puffer that make a big
advancements in puffer that make a big
difference
difference
but so so so much in RL is just getting
but so so so much in RL is just getting
the engineering right.
the engineering right.
The way I look at it is that there have
The way I look at it is that there have
been a lot of mathematically very smart
been a lot of mathematically very smart
people in RL. Um, that's not why the
people in RL. Um, that's not why the
field is stuck right now. That's not why
field is stuck right now. That's not why
like everything is so difficult. Because
like everything is so difficult. Because
all the code is slow.
Aren't some parts of environments
Aren't some parts of environments
inherently serial? Such as environments
inherently serial? Such as environments
that get more complicated, they will be
that get more complicated, they will be
less easily vectorized. Well, here's the
less easily vectorized. Well, here's the
thing. So, we just write stuff in C, but
thing. So, we just write stuff in C, but
we don't care if things are serial or
we don't care if things are serial or
not. It's not running on GPU. Um, our
not. It's not running on GPU. Um, our
code runs like a million steps a second.
code runs like a million steps a second.
So, we're not just like running one
So, we're not just like running one
environment per core. We're running like
environment per core. We're running like
a thousand environments per core. Some
a thousand environments per core. Some
of those are slower than others. It
of those are slower than others. It
doesn't matter because it gets amortized
doesn't matter because it gets amortized
out anyways. And like we literally end
out anyways. And like we literally end
up saturating the GPU with one or two
up saturating the GPU with one or two
CPU cores with most of our
environments. So like when you get to a
environments. So like when you get to a
certain speed threshold, it really
certain speed threshold, it really
doesn't
doesn't
matter. More headless environments.
matter. More headless environments.
Well, that's one of the things we do,
Well, that's one of the things we do,
right? like it's one of the main ways
right? like it's one of the main ways
that we scale out the open- source
that we scale out the open- source
contributor side because the easiest way
contributor side because the easiest way
to get new people into RL is say hey you
to get new people into RL is say hey you
know if you want to learn RL come up
know if you want to learn RL come up
with a new environment that you would
with a new environment that you would
think would be interesting build it and
think would be interesting build it and
go through the process of building the
go through the process of building the
environment and getting RL working on it
environment and getting RL working on it
so that you see the full end to end loop
so that you see the full end to end loop
so that's how we get a lot of these
so that's how we get a lot of these
environments contributed it's from
environments contributed it's from
people that are trying to learn RL and
people that are trying to learn RL and
are like building cool environments I
are like building cool environments I
build a few of them for myself for
build a few of them for myself for
various research purposes as well those
various research purposes as well those
are also there and and the library grows
are also there and and the library grows
that way. Um, we're also we have our
that way. Um, we're also we have our
work that we do on infrastructure and
work that we do on infrastructure and
good training. We have our own
good training. We have our own
algorithms in this next update. We're
algorithms in this next update. We're
shipping our own uh really high quality
shipping our own uh really high quality
hyperparameter tuning algorithm. We're
hyperparameter tuning algorithm. We're
shipping our new trainer which has a new
shipping our new trainer which has a new
advantage function. It's got a whole
advantage function. It's got a whole
bunch of additional improvements. It's
bunch of additional improvements. It's
very fast. It's got very nice logging
very fast. It's got very nice logging
integration. So, we really do everything
integration. So, we really do everything
um in RL. Like this is a comprehensive
um in RL. Like this is a comprehensive
effort to like fix all the things that
effort to like fix all the things that
are wrong with this
are wrong with this
field. Port working on Flappy Bird.
field. Port working on Flappy Bird.
Awesome. We would like a Flappy Bird M.
Awesome. We would like a Flappy Bird M.
Maybe a Flappy Puffer
Maybe a Flappy Puffer
M. Maybe load the Puffer
M. I have no idea how nobody has
M. I have no idea how nobody has
submitted one of those yet, but um yeah,
submitted one of those yet, but um yeah,
go for
go for
it. What do you do for logging? I notice
it. What do you do for logging? I notice
logging is very underdeveloped in RL
logging is very underdeveloped in RL
space. So we just have a very nice way
space. So we just have a very nice way
where anything that you have in this log
where anything that you have in this log
strct
strct
uh anything that you put in this log
uh anything that you put in this log
strct there is a very clean way to just
strct there is a very clean way to just
get it integrated with well aggregated
get it integrated with well aggregated
across like a thousand environments at
across like a thousand environments at
reasonable intervals and then we just
reasonable intervals and then we just
have backend integrations with WY and
have backend integrations with WY and
Neptune and it's like 20 lines of code
Neptune and it's like 20 lines of code
if you want to add your own of those. So
if you want to add your own of those. So
like when I do my experiments um I'm not
like when I do my experiments um I'm not
just looking at like one plot, right? If
just looking at like one plot, right? If
I go to Neptune
here, I have 30,000
here, I have 30,000
experiments from the last few months and
experiments from the last few months and
I can go to any of these tags and like I
I can go to any of these tags and like I
can go to this maze tag here. This is a
can go to this maze tag here. This is a
hyperparameter sweep and I can see all
hyperparameter sweep and I can see all
these graphs that are getting logged
these graphs that are getting logged
from um from C. Hang on.
from um from C. Hang on.
So I have all these graphs from C but
So I have all these graphs from C but
then I also have these dashboards which
then I also have these dashboards which
I can actually see like hyperparameter
I can actually see like hyperparameter
sensitivity. So I can see like this is
sensitivity. So I can see like this is
the stable regime for learning rate.
the stable regime for learning rate.
This is a stable regime for gamuts very
This is a stable regime for gamuts very
high. I can see like lambda. I have all
high. I can see like lambda. I have all
sorts of different views. I can see
sorts of different views. I can see
sweep progress. So I can see like how
sweep progress. So I can see like how
the parameters were fit over the course
the parameters were fit over the course
of a sweep and sensitivity therein. Um I
of a sweep and sensitivity therein. Um I
just have a ton a ton of insights here
just have a ton a ton of insights here
and it mostly it just comes from the
and it mostly it just comes from the
speed frankly because like these aren't
speed frankly because like these aren't
necessarily things that people couldn't
necessarily things that people couldn't
do before. The difference is that these
do before. The difference is that these
100 experiments or these like 178
100 experiments or these like 178
experiments this is like I don't know 30
experiments this is like I don't know 30
billion steps of training data or
billion steps of training data or
something and uh this ran overnight on
something and uh this ran overnight on
one GPU. So like we just have so so so
one GPU. So like we just have so so so
much more data and so much uh so much
much more data and so much uh so much
faster at such smaller scale that we can
faster at such smaller scale that we can
just do things that would previously
just do things that would previously
have been reserved for Google which is
have been reserved for Google which is
why we've been able to make so much
why we've been able to make so much
progress so
progress so
quickly. Why do you prefer C? Why you
quickly. Why do you prefer C? Why you
prefer using C
prefer using C
inside I I assume that means instead of
inside I I assume that means instead of
C++ it's just simpler and easier like I
C++ it's just simpler and easier like I
don't want to deal with the giant C
don't want to deal with the giant C
standard library. I don't want to deal
standard library. I don't want to deal
with people with boost. I don't want to
with people with boost. I don't want to
deal with like all the crazy things with
deal with like all the crazy things with
PE that things do in C++. C is just
PE that things do in C++. C is just
really easy. That's
really easy. That's
all. I like simple
things. It's just so
easy. Like I any of the environments
easy. Like I any of the environments
that I've worked on, I can just give
that I've worked on, I can just give
people the code file for it. Like I can
people the code file for it. Like I can
give a first year undergrad the code for
give a first year undergrad the code for
neural MMO 3 which is arguably one of
neural MMO 3 which is arguably one of
the most sophisticated RL environments
the most sophisticated RL environments
out there and like a first year
out there and like a first year
undergrad who's taken their first
undergrad who's taken their first
systems course can just read that code
systems course can just read that code
from top to bottom and we'll understand
from top to bottom and we'll understand
everything because it's just that easy.
everything because it's just that easy.
Like why do why do hard thing when
Like why do why do hard thing when
simple things
simple things
suffice and runs at one and a half
suffice and runs at one and a half
million steps per second on one CPU
million steps per second on one CPU
core.
But what I'm currently working on here
But what I'm currently working on here
is this is the this is a multi- aent
is this is the this is a multi- aent
tutorial M. I just built this yesterday
tutorial M. I just built this yesterday
to be like, you know, the sample
to be like, you know, the sample
project where you can like get a sense
project where you can like get a sense
of how a couple different like basic Ms
of how a couple different like basic Ms
are written. So, this is like, you know,
are written. So, this is like, you know,
I tried to like really just pair it down
I tried to like really just pair it down
to a a very basic end, and it's like
to a a very basic end, and it's like
going to be way more commented than uh
going to be way more commented than uh
the other end files are.
I'm mostly interested in
I'm mostly interested in
interrun logging. I was more talk per
interrun logging. I was more talk per
run logging in and viz how learned
run logging in and viz how learned
behaviors interact similar to how ice
behaviors interact similar to how ice
plots work for forest.
plots work for forest.
Um yeah, I mean we kind of just have
Um yeah, I mean we kind of just have
standard metrics for that, right? It's
standard metrics for that, right? It's
just like you log whatever you want from
just like you log whatever you want from
the end and then the key for a lot of
the end and then the key for a lot of
our uh interpretability stuff is just
our uh interpretability stuff is just
trying to make as many of the M's as
trying to make as many of the M's as
possible human playable uh and having
possible human playable uh and having
like a really quick feedback cycle on
like a really quick feedback cycle on
evals, right? Like let me just show you
evals, right? Like let me just show you
how let me show you how we train an
how let me show you how we train an
endbon puffer, right? So puffer
endbon puffer, right? So puffer
train. Hopefully I haven't broken
train. Hopefully I haven't broken
it. Okay, puffer train and puffer
it. Okay, puffer train and puffer
target. This literally just runs our
target. This literally just runs our
main training script. So it's this is
main training script. So it's this is
the same as doing like puffer python-m
the same as doing like puffer python-m
pufferlib like pufferl train. It's like
pufferlib like pufferl train. It's like
the same thing just a convenience. Okay.
the same thing just a convenience. Okay.
So now here we have this thing training
So now here we have this thing training
at
at
3539 million steps per
3539 million steps per
second. Okay. And you get all of this
second. Okay. And you get all of this
data in real time like this. But also if
data in real time like this. But also if
you just pass Neptune or 1B, it'll also
you just pass Neptune or 1B, it'll also
log all this stuff online with full
log all this stuff online with full
graphs.
Okay. And now after 20 to 30 seconds,
Okay. And now after 20 to 30 seconds,
this thing will have trained 100 million
this thing will have trained 100 million
steps, which is typically like an
steps, which is typically like an
overnight run for most of RL
overnight run for most of RL
researchers. Um, and
researchers. Um, and
then experiments just see what it named
then experiments just see what it named
it. And then we just do
eval which runs the exact same script
eval which runs the exact same script
just know not you doing the training
just know not you doing the training
part and it returns rendering arm load
part and it returns rendering arm load
model
model
path
8. And this is the model that we just
8. And this is the model that we just
trained.
And we have like experiments that take
And we have like experiments that take
on the order of seconds to minutes for
on the order of seconds to minutes for
like a pretty surprising range of
like a pretty surprising range of
environment complexity. Like things that
environment complexity. Like things that
you would normally expect to take
you would normally expect to take
multiple days to train, we just do it in
multiple days to train, we just do it in
like five minutes.
like five minutes.
So you kind of just have a lot more
So you kind of just have a lot more
interpretability in the sense that you
interpretability in the sense that you
you just get faster feedback loops of
you just get faster feedback loops of
all. You can just go watch your agent
all. You can just go watch your agent
play the game a
bunch. And I don't have it set up in
bunch. And I don't have it set up in
this end, but for most of them you can
this end, but for most of them you can
actually like hold shift and you can
actually like hold shift and you can
take over for the AI. So you can like
take over for the AI. So you can like
try to like shift around the way it
try to like shift around the way it
plays.
Um yeah, there's a bunch of stuff you
Um yeah, there's a bunch of stuff you
can do.
Python based and Majoko definitely can't
Python based and Majoko definitely can't
keep up. Yeah, I mean it's not even
keep up. Yeah, I mean it's not even
close, right? To be fair, even if you
close, right? To be fair, even if you
give us a Python end, we normally still
give us a Python end, we normally still
make it like
make it like
between I'd say around like 50 times
between I'd say around like 50 times
faster than you'd normally get in like
faster than you'd normally get in like
SP3. Like we've been handed unoptimized
SP3. Like we've been handed unoptimized
Python ends and we still get them to run
Python ends and we still get them to run
like 200,000 steps per second training.
like 200,000 steps per second training.
But like in our C version, it runs 2
But like in our C version, it runs 2
million steps per second,
right? But the thing that's like the
right? But the thing that's like the
emphasis here is just how easy this is.
emphasis here is just how easy this is.
Like this isn't hard. This is that env
Like this isn't hard. This is that env
is 200 lines of C. That's it, including
is 200 lines of C. That's it, including
the
renderer with a very thin binding
renderer with a very thin binding
layer. Do I need to run overwork equals
layer. Do I need to run overwork equals
true to run 96 Python M?
true to run 96 Python M?
recommend keeping both workers and batch
recommend keeping both workers and batch
sets to
sets to
auto. Those are the same as what they
auto. Those are the same as what they
were in 2.0. So link key the way that I
were in 2.0. So link key the way that I
would suggest including the ren well the
would suggest including the ren well the
render is super look at this it's just
render is super look at this it's just
ray this is the whole
ray this is the whole
renderer ray's awesome also if you want
renderer ray's awesome also if you want
to do python uh ray has an identical API
to do python uh ray has an identical API
for python you can basically write the
for python you can basically write the
exact same code and see in python
um yeah lean key so I wouldn't suggest
um yeah lean key so I wouldn't suggest
doing overwork like that parameter is
doing overwork like that parameter is
off by default because you shouldn't do
off by default because you shouldn't do
that um generally what you do is you
that um generally what you do is you
take your cores. All right, you get like
take your cores. All right, you get like
and then you run a number of
and then you run a number of
environments that's going to be
environments that's going to be
divisible by your number of cores, not
divisible by your number of cores, not
threads. So like if you have 16 cores,
threads. So like if you have 16 cores,
then you do 96 ms on on 16 cores. So
then you do 96 ms on on 16 cores. So
what is that? Is that like six each? I
what is that? Is that like six each? I
believe it'll just end up running six
believe it'll just end up running six
per core.
Uh, and the way that the auto things
Uh, and the way that the auto things
work
is
is
well, you might have to you might have
well, you might have to you might have
to set some of those params manually. I
to set some of those params manually. I
can help you with that today, though.
can help you with that today, though.
Like, if you're stuck on this, I can
Like, if you're stuck on this, I can
like look at your config and I can just
like look at your config and I can just
fix it for
fix it for
you. That's just the easiest
thing. I'll start learning Rayb right
thing. I'll start learning Rayb right
now. Yeah, it's really nice. I like it's
now. Yeah, it's really nice. I like it's
rendering is one of those things where
rendering is one of those things where
it's just like you have like pygame and
it's just like you have like pygame and
just these awful libraries. This is so
just these awful libraries. This is so
so much better and it just works
so much better and it just works
effortlessly and see this is all of it.
effortlessly and see this is all of it.
It's just got like really basic stuff
It's just got like really basic stuff
like texture loading and low-level
like texture loading and low-level
things that you would want and then you
things that you would want and then you
just write loops
just write loops
easy. So yeah, I mean this is this
easy. So yeah, I mean this is this
environment here is already committed. I
environment here is already committed. I
will be pushing these comments up today
will be pushing these comments up today
and then this is going to be part of a
and then this is going to be part of a
tutorial on the website for the new
tutorial on the website for the new
version of pufferlib. Uh so it's going
version of pufferlib. Uh so it's going
to be really easy for people to write
to be really easy for people to write
and contribute environments uh to puffer
and contribute environments uh to puffer
lib learn some RL in the process and
lib learn some RL in the process and
then you know from there once you get
then you know from there once you get
comfortable with all the things if you
comfortable with all the things if you
don't have a huge amount of RL
don't have a huge amount of RL
background right like this helps you get
background right like this helps you get
comfortable and then you can actually
comfortable and then you can actually
start helping on our research side right
start helping on our research side right
like there's so many different research
like there's so many different research
problems that we are interested in first
problems that we are interested in first
of all just like new types of
of all just like new types of
environments that pose interesting
environments that pose interesting
problems for reinforcement learning is a
problems for reinforcement learning is a
great way to contribute we have
great way to contribute we have
algorithm side for people that are more
algorithm side for people that are more
mathematically inclined we've that all
mathematically inclined we've that all
this RL infrastructure and optimization
this RL infrastructure and optimization
for people who have like the more
for people who have like the more
engineering side of things, right?
engineering side of things, right?
There's like tons and tons of stuff to
There's like tons and tons of stuff to
do. It's a comprehensive effort to make
do. It's a comprehensive effort to make
reinforcement learning fast and
reinforcement learning fast and
sane. Gives me warning saying numm
sane. Gives me warning saying numm
should be h
uh yeah, link that linky like just paste
uh yeah, link that linky like just paste
that in discord and let me know because
that in discord and let me know because
I'll look at that. It's possible I have
I'll look at that. It's possible I have
the warnings misleading.
and Captain's going to be here to have
and Captain's going to be here to have
uh his stuff fixed in a second as well.
We don't need this. We do not need
We don't need this. We do not need
this. Make this
this. Make this
end. Do
end. Do
init
allocate here.
You don't have to join the discord. It's
You don't have to join the discord. It's
just it's there if you want it. Looking
just it's there if you want it. Looking
to go towards meta RL. If I ever think
to go towards meta RL. If I ever think
of some kinds of M which makes sense,
of some kinds of M which makes sense,
I'll try to contribute. I'm fully
I'll try to contribute. I'm fully
formalized myself yet so it be another
formalized myself yet so it be another
year.
year.
opinionated current neural architecture
opinionated current neural architecture
search is quite naive. It definitely is.
search is quite naive. It definitely is.
The tricky thing that I think is
The tricky thing that I think is
like the thing that's I think difficult
like the thing that's I think difficult
with the neural architecture search is
with the neural architecture search is
like if you want to make this thing
like if you want to make this thing
really work, right? You want to make it
really work, right? You want to make it
fast. And the tough thing is neural
fast. And the tough thing is neural
architecture search tends to generate
architecture search tends to generate
like really
like really
slow uh really slow models because like
slow uh really slow models because like
you'd almost have to generate and then
you'd almost have to generate and then
like run optimization on all the kernels
like run optimization on all the kernels
or something weird like fuse kernels or
or something weird like fuse kernels or
something. There'd be like a lot of
something. There'd be like a lot of
low-level stuff, I think, to make it
low-level stuff, I think, to make it
really
good. I think you'd also be surprised at
good. I think you'd also be surprised at
how quickly you can learn um some of
how quickly you can learn um some of
these things. We've had people come in
these things. We've had people come in
with no reinforcement learning
with no reinforcement learning
background whatsoever, just like a
background whatsoever, just like a
software engine background. Um, who
software engine background. Um, who
haven't even done a ton of low-level dev
haven't even done a ton of low-level dev
and uh they've been like very very
and uh they've been like very very
capable at like helping and doing all
capable at like helping and doing all
sorts of interesting things around
sorts of interesting things around
Puffer within just a few months. We
Puffer within just a few months. We
really really try to make it as low
really really try to make it as low
friction as possible and like you can
friction as possible and like you can
just learn things quicker when you have
just learn things quicker when you have
lower turnaround time for like checking
lower turnaround time for like checking
things that you want to check, right?
things that you want to check, right?
running ideas, experiments, and things.
running ideas, experiments, and things.
It's just it's a way lower turnaround
It's just it's a way lower turnaround
time for
everything. That's the
idea. Definitely naive for a good
idea. Definitely naive for a good
reason. Defining a search space is hard.
reason. Defining a search space is hard.
It's hard. It's also
It's hard. It's also
like I don't know what what flavor of
like I don't know what what flavor of
neural architecture search you're
neural architecture search you're
interested in, right? Like if you're
interested in, right? Like if you're
talking if you're looking at like old
talking if you're looking at like old
like neat or hyper neat or like any of
like neat or hyper neat or like any of
the um like CPPNs or stuff, right? Like
the um like CPPNs or stuff, right? Like
the really small evolved networks that
the really small evolved networks that
do really interesting things just
do really interesting things just
basically via function composition.
basically via function composition.
They're really cool, but I haven't seen
They're really cool, but I haven't seen
them scale very well. And they're also
them scale very well. And they're also
like if you do scale them, it's very
like if you do scale them, it's very
difficult to make them
difficult to make them
fast, which is pretty tough. Generally,
fast, which is pretty tough. Generally,
I think I like the space of EVO in
I think I like the space of EVO in
general. The thing is like you want to
general. The thing is like you want to
make everything like nicely shaped
make everything like nicely shaped
networks that run fast and like run on
networks that run fast and like run on
big
big
batches. Yeah, Captain. I'll hop on
batches. Yeah, Captain. I'll hop on
Discord. We'll do that right now. I've
Discord. We'll do that right now. I've
been waiting for you.
It's not letting me click the
It's not letting me click the
channel. Hang on. Oh, there it is.
Hey, in ENS and darts. Um, yeah, meet
This is like all stuff like Ken Stanley
This is like all stuff like Ken Stanley
adjacent
adjacent
stuff. Yeah, we're always happy to have
stuff. Yeah, we're always happy to have
contributors. It's pretty much if you're
contributors. It's pretty much if you're
pushing uh if you're contributing code
pushing uh if you're contributing code
to Puffer, you'll see that you get lots
to Puffer, you'll see that you get lots
of uh lots of feedback on stuff. All
of uh lots of feedback on stuff. All
right, Captain, you got bugs to
right, Captain, you got bugs to
fix or I have bugs. Let's see here.
Boom. All right. What do we
got? No idea. You might I don't
got? No idea. You might I don't
know, man. This Python packaging is
know, man. This Python packaging is
cursed. Okay, I'm working um best I can.
We're going to have the the demo m done
We're going to have the the demo m done
today. That is the goal. And then I will
today. That is the goal. And then I will
go from
there. Let's
see. Okay. So, I have n there's ninja
see. Okay. So, I have n there's ninja
there. It's built.
Okay, using your correct. It's I see
Okay, using your correct. It's I see
it's using the Python in your PM. So,
it's using the Python in your PM. So,
it's
good. Ah, okay.
good. Ah, okay.
Um, so I see what happened there. Uh,
Um, so I see what happened there. Uh,
can
can
you you run
NVCC? You don't have NVCC.
Boom. So basically what happened there,
Boom. So basically what happened there,
it's a good report because that should
it's a good report because that should
be I should have that working so that
be I should have that working so that
it's an automatic fallback to that.
it's an automatic fallback to that.
Um, but what basically happened is it
Um, but what basically happened is it
compiled the CPU version of that kernel
compiled the CPU version of that kernel
for you because you don't have CUDA dev
tools. So that is still a good error
tools. So that is still a good error
report because it should not fail in
report because it should not fail in
that manner. It should just it should
that manner. It should just it should
just be slower basically. It should just
just be slower basically. It should just
build the CPU back end and just use it
build the CPU back end and just use it
for you.
It's probably well it's it goes in your
It's probably well it's it goes in your
container.
container.
Um, it needs to be installed in the
Um, it needs to be installed in the
container and it's like CUDA dev
container and it's like CUDA dev
tools
something. It doesn't need to be. It's
something. It doesn't need to be. It's
not on the host that you need it
though. Are you not in Oh, you're not in
though. Are you not in Oh, you're not in
a
a
container. Yeah. Then you Well, then you
container. Yeah. Then you Well, then you
just need CUDA dev tools. Yeah.
Uh well, we use the base container,
Uh well, we use the base container,
right? We use the Nvidia
base, but it's yeah, it's just like food
base, but it's yeah, it's just like food
dev tools or whatever if you look it up.
So I the goal is that this should not
So I the goal is that this should not
prevent you from running puffer lib.
prevent you from running puffer lib.
It'll just prevent you from having the
It'll just prevent you from having the
GPU kernel. So I will I will go add that
GPU kernel. So I will I will go add that
fallback um after I fix all the other
fallback um after I fix all the other
stuff
today cuz it actually it correctly
today cuz it actually it correctly
compiled the CPU version. It just didn't
compiled the CPU version. It just didn't
detect that there was no CUDA version
detect that there was no CUDA version
for it. So uh
for it. So uh
yeah, that's all.
You don't have
nvcc. You should you should have the
nvcc. You should you should have the
like you need the you should have a CUDA
like you need the you should have a CUDA
compiler if it's correctly set
up. We do not have a 1-800 puff
number. We should get one of those.
number. We should get one of those.
Can we get 1800 puff?
All
right, you take a second to figure that.
right, you take a second to figure that.
I'm going to go take the quick
I'm going to go take the quick
call back.
You figure it out,
Captain. Let's see.
So I added a warning for that in the
So I added a warning for that in the
latest
version. Is it synced? So there should
version. Is it synced? So there should
be there is an explicit check against
be there is an explicit check against
total mini batches being
zero like yesterday.
If you're on a fork, you would have had
If you're on a fork, you would have had
to fold from upstream or synced
it. I will be right back.
Hey,
Hey,
quicky. We were live earlier. What
quicky. We were live earlier. What
happened? We went for breakfast. I just
happened? We went for breakfast. I just
uh I just do a quick stream in the
uh I just do a quick stream in the
morning because I have breakfast at
morning because I have breakfast at
10:00. So,
10:00. So,
it's some days I run, some days I don't.
it's some days I run, some days I don't.
So, it's like I'm on for half hour to
So, it's like I'm on for half hour to
two hours depending on the day and then
two hours depending on the day and then
I'm back for the rest of the
I'm back for the rest of the
day. Captain, how's it
going? Okay. Is this old Syon or
So run it with serial back
So run it with serial back
end if it if you see CSX
fault.
Yes.
Yes.
Yes. It also works from
Yes. It also works from
CLI like vec.backend serial will do it
CLI like vec.backend serial will do it
as well.
capital S serial. That's this time
though. Um well, I'd have to like
though. Um well, I'd have to like
postprocess the string otherwise. It's
postprocess the string otherwise. It's
because it uses
because it uses
um like it it just like it looks for the
um like it it just like it looks for the
vectorzation thing that matches that
vectorzation thing that matches that
string. Basically, it's a str it's a
string. Basically, it's a str it's a
string match on the classes in vec.
string match on the classes in vec.
Yeah.
uh okay so that is
because you have num m set to
Uh, you shouldn't do that. So, what's
Uh, you shouldn't do that. So, what's
happening there is that you still have
happening there is that you still have
your mini batch size set to
your mini batch size set to
8192. All right. And the batch size is
8192. All right. And the batch size is
BPT Horizon times the number of
ends. There you go. That's
ends. There you go. That's
why. So, the good thing is we've
why. So, the good thing is we've
actually made it so you can debug that
actually made it so you can debug that
now. Um, if you just uh just head uh
setup.py. No, no, no. If you Okay. Yeah,
setup.py. No, no, no. If you Okay. Yeah,
you can do it that way, too. Yeah, but
you can do it that way, too. Yeah, but
there's another command in there you'll
there's another command in there you'll
need. So, debug equals one. You can
need. So, debug equals one. You can
compile that. Yeah, that's fine. But
compile that. Yeah, that's fine. But
then to actually run the thing, you're
then to actually run the thing, you're
going to have to get the command from uh
going to have to get the command from uh
from the setup.py.
from the setup.py.
It's just a comment at the top for
now. Okay. So, CUDA visible device.
now. Okay. So, CUDA visible device.
Yeah. All that stuff. And then you can
Yeah. All that stuff. And then you can
just set that to run your
just set that to run your
end. I would do it with serial back end
end. I would do it with serial back end
and run train.
You get trace out of any of these.
So when I do this, I normally get a with
So when I do this, I normally get a with
that command, I normally get a stack
that command, I normally get a stack
trace that goes into the
C. Well, I'm not I don't have anything
C. Well, I'm not I don't have anything
set. It's so it's however
set. It's so it's however
setup.py like setup
setup.py like setup
tools. Yeah. I don't know.
tools. Yeah. I don't know.
I just have uh the uh the dev container
I just have uh the uh the dev container
is just set up with with
clang. I don't know why there could be a
clang. I don't know why there could be a
stack trace at the top of all that spam
stack trace at the top of all that spam
for all I
know. All right.
So the just thing just pops up is like
So the just thing just pops up is like
an operation not permitted. That's like
an operation not permitted. That's like
some weird permissioning [ __ ] with
some weird permissioning [ __ ] with
my OS has nothing to do with it.
And that only that like that happens
And that only that like that happens
with debugger specifically. Play.
What
doesn't? Well, then that means whatever
doesn't? Well, then that means whatever
compiler I have is working.
Uh, the Python ecosystem is not known
Uh, the Python ecosystem is not known
for being particularly
smart. Yeah, I'm seeing this uh I'm
smart. Yeah, I'm seeing this uh I'm
seeing this linky.
That doesn't exit cleanly.
multi-discrete
outputs. We do multi-disipre outputs.
Action dim is off.
See, I bought
You
argument 13.3.0
which is uh default Abuntu
for I'm pretty sure unless we find that
for I'm pretty sure unless we find that
everything's screwed at the last minute.
Like, yeah, I mean,
Like, yeah, I mean,
it's it just makes more sense to go in
it's it just makes more sense to go in
the major
versions. There's like version inflation
versions. There's like version inflation
because of um all the AI companies just
because of um all the AI companies just
like skipping normal versioning. So,
like skipping normal versioning. So,
it's like fine, screw it.
H. Oh,
yeah. Well, then every update's just
yeah. Well, then every update's just
going to be major
version. Like, oh no, it's a breakage.
version. Like, oh no, it's a breakage.
It's like, "Shut up. You fix our L."
The thing is nobody actually follows the
The thing is nobody actually follows the
versioning and everybody just has it
versioning and everybody just has it
stuck into their config files without
stuck into their config files without
like a pin to a major version anyways.
like a pin to a major version anyways.
So like it just will automatically
So like it just will automatically
upgrade to breaking versions like the
upgrade to breaking versions like the
whole system is wrong for
that. Yes.
The thing is, Python's really not built
The thing is, Python's really not built
for it though because like the correct
for it though because like the correct
way to do this, right, would be that you
way to do this, right, would be that you
would have isolated like let's say you
would have isolated like let's say you
have two different dependencies that
have two different dependencies that
both have numpy they would both build
both have numpy they would both build
their own versions of numpy like
their own versions of numpy like
statically or what however they want to
statically or what however they want to
do it and then you wouldn't get
do it and then you wouldn't get
conflicts but because everything is
conflicts but because everything is
global like you can't pin versions
global like you can't pin versions
otherwise your pin numpy breaks the
otherwise your pin numpy breaks the
minimum pin version of the other library
minimum pin version of the other library
because we don't know what static
because we don't know what static
linking is apparently um and like
linking is apparently um and like
everyone has to share the same bloody
everyone has to share the same bloody
version of everything But it's just a
mess. Like literally, this should not be
mess. Like literally, this should not be
a problem, right? Literally, versioning
a problem, right? Literally, versioning
would not be a problem if it's just
would not be a problem if it's just
like, okay, your library basically takes
like, okay, your library basically takes
all the code. It's like vendored
all the code. It's like vendored
versions. Everything is linked
versions. Everything is linked
reasonably so that you're not sharing
reasonably so that you're not sharing
the name space of your depths with other
the name space of your depths with other
things that need different versions of
things that need different versions of
that depths. And you're done.
I mean, go go go generally seems like
I mean, go go go generally seems like
fine.
I mean, we can literally do all of this
I mean, we can literally do all of this
and
Now, you don't blame Don't blame C for
Now, you don't blame Don't blame C for
this. This is Python being dumb as hell.
this. This is Python being dumb as hell.
All
right. Okay. But like if you just run
right. Okay. But like if you just run
this sanitizer from the C build, it
this sanitizer from the C build, it
works.
works.
So, right.
We use clang as well.
I don't know, man. I'm working as fast
I don't know, man. I'm working as fast
as I can on it. I kind of don't want to
as I can on it. I kind of don't want to
think about it. I just want to like get
think about it. I just want to like get
it
it
done. It's like it's just it
done. It's like it's just it
sucks. I hate releases.
It's really close, but it's like if I
It's really close, but it's like if I
think about like even if I think like,
think about like even if I think like,
oh, there like, you know, 20 more hours
oh, there like, you know, 20 more hours
of like real work to do, it's like I
of like real work to do, it's like I
can't do those 20 hours. It's like I'm
can't do those 20 hours. It's like I'm
just so
bored. I want to get back to doing real
bored. I want to get back to doing real
things, you know?
I haven't ridden one of these like
I haven't ridden one of these like
networks in a while and they're kind of
networks in a while and they're kind of
annoying.
Yeah, this is the
Yeah, this is the
rare the rare watch neural nets in pure
rare the rare watch neural nets in pure
content.
have to make the network a slightly
have to make the network a slightly
different way. I think.
Okay. So, we're going to try this
training split sizes.
I figured it would be good to at least
I figured it would be good to at least
have the tutorial on like how we do our
have the tutorial on like how we do our
um how we do like custom nuts and stuff.
wrong with you.
our model.
So I guess then it will just
be can make this a fair bit
simpler.
Totally make this simpler.
And this will
Forward
linear still not working for
See, make it work suddenly.
Hey,
Hey,
man. That's such a pain in the ass
man. That's such a pain in the ass
because
because
like it's I hate when it's like one
like it's I hate when it's like one
stupid heavy tool makes you use another
stupid heavy tool makes you use another
stupid heavy
stupid heavy
tool because it's it's like this is how
tool because it's it's like this is how
you get this is how C turns into React.
you get this is how C turns into React.
All right, we used to be we used to
All right, we used to be we used to
write real code, let me tell you.
I mean, that's people doing weird
I mean, that's people doing weird
[ __ ] It's pretty damn easy when you
[ __ ] It's pretty damn easy when you
just write loops and conditionals and to
just write loops and conditionals and to
watch your code be a thousandx faster
watch your code be a thousandx faster
and shorter than everything written in
and shorter than everything written in
the modern
era.
era.
Like, you know, I just I like see just
Like, you know, I just I like see just
it's easy, you know?
it's easy, you know?
It's just Easy.
What's it say?
That's where this the debug commands
That's where this the debug commands
were supposed
were supposed
to fix that.
to fix that.
like it's supposed to fix this asand
like it's supposed to fix this asand
linking or whatever.
this the action
space and five
This runs out of weights
Yeah, it's obnoxious. I mean, this is
Yeah, it's obnoxious. I mean, this is
why we have it so that it should be it
why we have it so that it should be it
should be easy. I mean, to be fair, this
should be easy. I mean, to be fair, this
is also kind of like why we ship the dev
is also kind of like why we ship the dev
container. So, it's like there are
container. So, it's like there are
million freaking stupid setup things
million freaking stupid setup things
that can be different. Here's a damn
that can be different. Here's a damn
entire computer that's the same as
mine. Let's use You have access to a
mine. Let's use You have access to a
box, don't you? Do it there.
boxes for you to like fiddle
with. Probably
not. If you need if you get stuff the
not. If you need if you get stuff the
way you need
way you need
it, I can run a thing.
it, I can run a thing.
But I don't know. Does that actually It
But I don't know. Does that actually It
depends what box you have. I will also
depends what box you have. I will also
just have the
just have the
uh the new containers done pretty soon
here. Just finish these darn tutorials.
What? It's input.
What? It's input.
It's state.
137K parameters.
That seems
correct. Something's just slightly off,
correct. Something's just slightly off,
I guess.
Okay,
Okay,
now get
now get
a overflow of some type.
and I'm agents.
Oh, wait. Numb
Oh, wait. Numb
goals. This is it.
We got it
work still now.
Breast sanitizer.
This is you have to
order. Here we are.
These puffers don't seem as smart as the
These puffers don't seem as smart as the
other puffers.
Why don't I get um
Why don't I get um
symbols? I don't get like symbols here.
That's
crazy. Actually version made a
crazy. Actually version made a
difference.
Oh, that sucks.
me retrain. I think I am loading this
me retrain. I think I am loading this
correctly. I think I just trained a bad
correctly. I think I just trained a bad
model.
Rain this Oh,
Right. Uh [ __ ] I broke something in the
Right. Uh [ __ ] I broke something in the
end in the process of refactoring, I
end in the process of refactoring, I
think.
Oh yeah, I kind of forgot about
Oh yeah, I kind of forgot about
that.
Um, that's weird cuz I I can definitely
Um, that's weird cuz I I can definitely
run with device CPU. So
run with device CPU. So
like Huh.
Don't set
Don't set
that. Maybe don't set
that. I That should fix that. Either
way, was the other
That's
That's
bizarre. There's so many of these little
bizarre. There's so many of these little
like stupid things, man.
Oh yeah.
I have a feeling I'm going to have to go
I have a feeling I'm going to have to go
through line by line like the old
through line by line like the old
version and see what I messed up.
Um, I mean, you get that for a ton of
things. Like I just fixed one of those
things. Like I just fixed one of those
not
not
initializing Ray Lib before calling load
initializing Ray Lib before calling load
texture.
Yep. Got the puffers vibing
again. It's like it's cool, right? I
again. It's like it's cool, right? I
honestly I think we should do way more
honestly I think we should do way more
with stuff like this in the future, but
with stuff like this in the future, but
I got to get this update out
I got to get this update out
first and I probably have to
first and I probably have to
go recover from all this stuff I've been
go recover from all this stuff I've been
doing. Get my head on straight and go
doing. Get my head on straight and go
like I don't know, lift some rocks
like I don't know, lift some rocks
outside and watch the birds or
whatever. I You think I'm joking? I have
whatever. I You think I'm joking? I have
uh I've got stones 70 to I don't know
uh I've got stones 70 to I don't know
how much the big one is. It's between
how much the big one is. It's between
150 and
150 and
200. I've got like, you know,
200. I've got like, you know,
uh I've got an overhead pressing stone.
uh I've got an overhead pressing stone.
I've got like a loading
stone. It's just it's fun, you know. I
stone. It's just it's fun, you know. I
got all the barbells behind me. Speaking
got all the barbells behind me. Speaking
of which, thanks for reminding me. I got
of which, thanks for reminding me. I got
to do my first set of squats because I'm
I got a screwy knee, so I got to be
I got a screwy knee, so I got to be
careful. So, this is just going to
careful. So, this is just going to
be that the
cardinal. Going to see if that's the
cardinal. Going to see if that's the
cardinal I've been looking
cardinal I've been looking
for. No, it's
for. No, it's
not.
not.
Um, what do I put
on? I think we'll try
Uh, we'll try like
Uh, we'll try like
two right back.
knees didn't like it. Everything else is
fine. This is the funniest thing. So,
fine. This is the funniest thing. So,
there are I put up a bird house and I've
there are I put up a bird house and I've
got two bluebirds in the uh the
got two bluebirds in the uh the
birdhouse, but every so often some other
birdhouse, but every so often some other
bird comes over to like, "Oh, that looks
bird comes over to like, "Oh, that looks
like a nice box you've got there." And
like a nice box you've got there." And
then there two very angry
then there two very angry
bluebirds chase off whatever it
is. This is my spot.
And I have it like maybe 30 feet outside
And I have it like maybe 30 feet outside
the window. 25 ft. 30 ft outside the
the window. 25 ft. 30 ft outside the
window. That was very
amusing.
Ah, I have
Ah, I have
I've got uh two 27 inch monitors. That's
I've got uh two 27 inch monitors. That's
it. One's verd on the left and then my
it. One's verd on the left and then my
main monitor is in the center. But then
main monitor is in the center. But then
I have I do have to look around this big
I have I do have to look around this big
freaking
camera. The camera's on a tripod where I
camera. The camera's on a tripod where I
have my desk like two feet off the wall
have my desk like two feet off the wall
and the camera tripod has two legs on
and the camera tripod has two legs on
the desk and one leg like extended all
the desk and one leg like extended all
the way down to the ground.
And then the camera is actually mounted
And then the camera is actually mounted
upside down like on the front side. It's
upside down like on the front side. It's
a whole
thing. Uh-huh.
What random
number. What's the data
type? Then that seems like that sounds
type? Then that seems like that sounds
like it's getting overwritten. Not by
like it's getting overwritten. Not by
like that's like some random memory
overwrite. Is it before or
overwrite. Is it before or
after like
after like
um I mean OBS has to be the right size.
um I mean OBS has to be the right size.
There's no check on that, right?
There's no check on that, right?
Like all these things have to be the
Like all these things have to be the
right
size. No, because it's all your
memory. like that's you're overwriting
memory. like that's you're overwriting
within your own
within your own
strct. So that's actually one of the
strct. So that's actually one of the
things where like where people say, "Oh,
things where like where people say, "Oh,
you should use like area allocators."
you should use like area allocators."
That's why that's dumb.
It's not the same
It's not the same
um in uh
um in uh
PyTorch as in
C. Like the model does better in PyTorch
C. Like the model does better in PyTorch
than it does in the C export.
Mhm. Would have to
Mhm. Would have to
be pain in the ass
be pain in the ass
though. I've definitely seen this type
though. I've definitely seen this type
of thing happen before where
of thing happen before where
like behavior degrades somehow.
I mean, but it's like the thing is it
does like these guys do go for
does like these guys do go for
stars. They're just not as good.
Ah, I
see. Well, the log is the log field is
see. Well, the log is the log field is
just for things that are supposed to be
just for things that are supposed to be
logged and they should all be floats.
Mhm.
Well, yeah. No, I think so. I that
Well, yeah. No, I think so. I that
doesn't seem like a pretty a good way to
doesn't seem like a pretty a good way to
scale it anyways. Like if you have a
scale it anyways. Like if you have a
scripted bot that you want stats for
scripted bot that you want stats for
separately versus a trained bot, that's
separately versus a trained bot, that's
one thing. But if they're like all
one thing. But if they're like all
trained bots, then you should just log
trained bots, then you should just log
them all. Like you shouldn't have
them all. Like you shouldn't have
different stats per drone, I wouldn't
think. Yeah, you can have that.
think. Yeah, you can have that.
technically, but then if you really want
technically, but then if you really want
to check that for now, just like put
to check that for now, just like put
four agents worth of logs and call it a
four agents worth of logs and call it a
day. Put four agents worth of log spots
day. Put four agents worth of log spots
and call it a day for
now.
now.
Yeah. Well, then just have two
Yeah. Well, then just have two
till we get your good policy.
There's something screwed up in the net
There's something screwed up in the net
itself, I believe.
Well, I mean, I can always fix this as
Well, I mean, I can always fix this as
part of
part of
um as part of fixing
the the demo.
This mallet.
the ASM.
Yeah, this is installed.
I think there's anything else that
I think there's anything else that
uh allocate Ite.
Do we use
clang? We do, right? Yeah.
playing.
playing.
Oh.
Oh.
Um, why
Um, why
does why is platform not defined?
Oh no, it is
Oh no, it is
never
platform. Trying to fix the debugging
platform. Trying to fix the debugging
experience for this.
Oh, it's GPU driver stuff. Okay.
All right, whatever. I think this is
All right, whatever. I think this is
enough for
now. Just add a couple comments.
Careful. I'm going to find a way to
Careful. I'm going to find a way to
delete setup tools and not include
delete setup tools and not include
CMake. And the whole entire project
CMake. And the whole entire project
setup is going to be a random shell
script. I don't know. I tend to build
script. I don't know. I tend to build
okay stuff.
Hey Aaron, what's up? We're on stream.
Hey Aaron, what's up? We're on stream.
How's it
going? Okay, awesome. I am
going? Okay, awesome. I am
fixing our demos.
So, we will have uh very nice
So, we will have uh very nice
uh M docks. And then I might even send
uh M docks. And then I might even send
these to you to see how bad these look
these to you to see how bad these look
for
you. I don't know. Maybe we'll uh we'll
you. I don't know. Maybe we'll uh we'll
turn you into a C engineer just just
turn you into a C engineer just just
yet.
I mean, it's not that far off from this
I mean, it's not that far off from this
thing that I just built. So, why don't I
thing that I just built. So, why don't I
just give you this? It's like I'll
just give you this? It's like I'll
finish. You can beta test our doc. So, I
finish. You can beta test our doc. So, I
mean, if you can figure this out having
mean, if you can figure this out having
done probably mostly pure Python for the
done probably mostly pure Python for the
last decade, um, then it should be
last decade, um, then it should be
pretty easy.
builds. Yeah. So, this is pretty much
builds. Yeah. So, this is pretty much
ready to go. Um, here, check this out. I
ready to go. Um, here, check this out. I
The one thing is I have to fix our
The one thing is I have to fix our
library, like our neural net library,
library, like our neural net library,
because it's there's a gap with torch
because it's there's a gap with torch
because the networks don't look quite as
because the networks don't look quite as
good in uh like it's loading the
good in uh like it's loading the
network, but I think there's like some
network, but I think there's like some
drift or something. The puffer still get
drift or something. The puffer still get
the stars, just not quite as well as in
the stars, just not quite as well as in
uh Python. This doesn't look that
uh Python. This doesn't look that
bad. So, this is running in pure C right
bad. So, this is running in pure C right
now.
These ones are not as uh as snappy as
These ones are not as uh as snappy as
they so they'll be way snappy when I fix
they so they'll be way snappy when I fix
the uh the torch library.
But yeah, so that's just like drift with
But yeah, so that's just like drift with
our network. But if I just do for
instance, if I do this, that was running
instance, if I do this, that was running
pure C. Now this is running with
pure C. Now this is running with
PyTorch.
see the
puffers. Yeah. So, there's just like
puffers. Yeah. So, there's just like
some numerical stuff going on with our
some numerical stuff going on with our
uh RC library, but it's pretty cool
uh RC library, but it's pretty cool
because let's say I fix this. Okay. This
because let's say I fix this. Okay. This
is a 60line C demo file and then the the
is a 60line C demo file and then the the
source code for this environment this
source code for this environment this
whole with all of the comments included
whole with all of the comments included
and it's like nicely commented up is 230
and it's like nicely commented up is 230
lines and that's literally it. Now the
lines and that's literally it. Now the
one thing we use ray as our renderer and
one thing we use ray as our renderer and
then for that neural net demo
then for that neural net demo
specifically if you want to do that we
specifically if you want to do that we
have extensions puffernet.h
have extensions puffernet.h
page. And I've been thinking of
page. And I've been thinking of
replacing this thing with like the tiny
replacing this thing with like the tiny
red C back end, but uh it's also just
red C back end, but uh it's also just
kind of cool that we have this thing.
kind of cool that we have this thing.
It's like a
It's like a
single it's a single file like no
single it's a single file like no
dependencies C neural net and French
dependencies C neural net and French
library. That's really
library. That's really
easy. So that is the target env done.
easy. So that is the target env done.
I'm going to comment up uh the squared
I'm going to comment up uh the squared
env. I'm going to put them online and
env. I'm going to put them online and
then I can give you docs for that and
then I can give you docs for that and
you can see how that goes because the
you can see how that goes because the
chase thing you can train that at
chase thing you can train that at
probably 4 million steps a
second. Yeah, just insta
solve from the reinforcement learning
solve from the reinforcement learning
conference to actually have
conference to actually have
reinforcement learning.
Okay, there will be plenty of it to go
Okay, there will be plenty of it to go
around with a
around with a
puffer. Let me see the thing that you
puffer. Let me see the thing that you
put in
chat like
chat like
LP. So the are these two different
graphs and then is
graphs and then is
this is this perf this isn't evalf
this is this perf this isn't evalf
though this is this is the training
curve okay so
curve okay so
there's so according to the eval which
there's so according to the eval which
isn't really quite long enough, but
isn't really quite long enough, but
according to that
according to that
eval, it looks like they dip about the
eval, it looks like they dip about the
same amount
same amount
despite Yeah, they dip about the same
despite Yeah, they dip about the same
amount.
amount.
Okay. Yeah. So, I mean, just put all
Okay. Yeah. So, I mean, just put all
this stuff together and we will uh we
this stuff together and we will uh we
will uh you know, send over the final
will uh you know, send over the final
stuff. I mean, you've basically done
stuff. I mean, you've basically done
everything that we could have asked for
everything that we could have asked for
on this,
right? Yeah. It's like it's like it
right? Yeah. It's like it's like it
turns out that the baseline got
turns out that the baseline got
ridiculously good in the meanwhile,
ridiculously good in the meanwhile,
which is kind of what we were trying to
which is kind of what we were trying to
do. Um, but like it's pretty much like
do. Um, but like it's pretty much like
this helps whenever it is
possible. All right, good LP stuff for
possible. All right, good LP stuff for
puffer and hopefully I think that there
puffer and hopefully I think that there
will be there should be a way to get
will be there should be a way to get
some version of this into like more
some version of this into like more
puffer experiments as well. Maybe you'll
puffer experiments as well. Maybe you'll
be interested doing that side of stuff
be interested doing that side of stuff
when uh you have more like M's that are
when uh you have more like M's that are
easy to work with and interpret and
easy to work with and interpret and
randomize over and all
that. Oh yeah, you can of course of
that. Oh yeah, you can of course of
course finish everything. Um I here let
course finish everything. Um I here let
me
just I can commit this anyways.
I can show you this
code. Oh, that's actually way
easier. I mean, this is the code like
easier. I mean, this is the code like
this is the type of code that you write
this is the type of code that you write
for puffer ms, right? So I have
for puffer ms, right? So I have
commented here like which of these
commented here like which of these
fields are required and most of them
fields are required and most of them
aren't. So it's like this is a required
aren't. So it's like this is a required
log structure. You have a couple strrus
log structure. You have a couple strrus
which are literally just data classes,
which are literally just data classes,
right?
right?
Um here's your
Um here's your
env. And then this is the thing setting
env. And then this is the thing setting
new goals for itself. It's literally
new goals for itself. It's literally
just I mean if you just like replace the
just I mean if you just like replace the
parentheses with like white spacing and
parentheses with like white spacing and
stuff and like
stuff and like
other than the fact that there are
other than the fact that there are
pointers for things that in Python like
pointers for things that in Python like
everything that is not a primitive is
everything that is not a primitive is
just passed by reference anyways like
just passed by reference anyways like
this is basically
this is basically
Python like because you can write this
Python like because you can write this
in any
language. It really can be. And like
language. It really can be. And like
this is the observation function right
this is the observation function right
here. This is it.
It's not bad
either. All right. And then this is
either. All right. And then this is
here's how you reset the end. You just
here's how you reset the end. You just
randomize the
positions. All right. Basic stuff like
positions. All right. Basic stuff like
clipping is not built into C. So you
clipping is not built into C. So you
just write your own clipping function,
just write your own clipping function,
but
whatever. And then here's the step. Here
whatever. And then here's the step. Here
are the dynamics.
Yep. Little bit of memory management,
Yep. Little bit of memory management,
right? Little tiny bit of memory
right? Little tiny bit of memory
management right here. But um and then
management right here. But um and then
this is the whole renderer. This is the
this is the whole renderer. This is the
probably the coolest bit. You can't even
probably the coolest bit. You can't even
make the renderer this good in
Python. That's the whole
renderer. And this draws the puffers. It
renderer. And this draws the puffers. It
draws the stars. it would be even
draws the stars. it would be even
shorter except for the fact that I'm
shorter except for the fact that I'm
downscaling the textures as
downscaling the textures as
well. Oh, no. I It's not that I'm
well. Oh, no. I It's not that I'm
downing. It's I'm uh I'm text I have a
downing. It's I'm uh I'm text I have a
uh it's a like texture sheet. So, it's
uh it's a like texture sheet. So, it's
like a texture
atlas. And that's
atlas. And that's
everything that
everything that
is. And then the only thing you need to
is. And then the only thing you need to
do, it's a little bit
do, it's a little bit
annoying. This is the binding file.
This is what you write to bind your end
This is what you write to bind your end
to
Python. So until you start doing crazy
Python. So until you start doing crazy
things that require you to really get
things that require you to really get
into like the Python C API and stuff
into like the Python C API and stuff
like I mean this is pretty simple for
like I mean this is pretty simple for
just binding your end, right?
We will get you riding C
yet. I got to tell you, it's so much
yet. I got to tell you, it's so much
cooler now just to be able to like try
cooler now just to be able to like try
out whatever the heck I want and not
out whatever the heck I want and not
have like some big engineering stack I'm
have like some big engineering stack I'm
dependent
on. But it's but the thing is it's super
on. But it's but the thing is it's super
thin, right? So basically like I've got
thin, right? So basically like I've got
a 500 line script that replaces the need
a 500 line script that replaces the need
for like having some Python thing and it
for like having some Python thing and it
just uses the Python. It's just like a
just uses the Python. It's just like a
small wrapper over the Python C API.
small wrapper over the Python C API.
Like it hardly even does
anything. It's really nice that it's C,
anything. It's really nice that it's C,
not C++.
Um, I mean if we need to, we
Um, I mean if we need to, we
can. I mean that but the thing is it's
can. I mean that but the thing is it's
legitimately it's like whoopde it's not
legitimately it's like whoopde it's not
that
hard and like you know like these M's
hard and like you know like these M's
you can pretty much the way that these
you can pretty much the way that these
M's are written you can almost just copy
M's are written you can almost just copy
paste them into CUDA and like you could
paste them into CUDA and like you could
just run this to
envel you have to set up like the
envel you have to set up like the
parallelism flag but it's basically you
parallelism flag but it's basically you
just parallelize across like one you
just parallelize across like one you
just do one thread per end copy And
just do one thread per end copy And
you're
done. Well, I mean, you can paste this
done. Well, I mean, you can paste this
into CUDA, right? Like apparently nobody
into CUDA, right? Like apparently nobody
else can figure out how to get basic C
else can figure out how to get basic C
to run on a GPU reasonably. Um but look
to run on a GPU reasonably. Um but look
like this is where is it? Yeah, this is
like this is where is it? Yeah, this is
our extension our CUDA. This is our
our extension our CUDA. This is our
whole CUDA extension
whole CUDA extension
file. This is the puffer advantage
file. This is the puffer advantage
function right
function right
here. That's
it. We had
it. We had
to This gets called on This gets called
to This gets called on This gets called
on everything every mini batch.
It's actually really obnoxious that we
It's actually really obnoxious that we
have to ship kernels because it makes
have to ship kernels because it makes
the whole build stuff like way more
the whole build stuff like way more
obnoxious. That's the one thing that
obnoxious. That's the one thing that
really sucks is dealing with setup tools
really sucks is dealing with setup tools
and like Python compilation garb. Like
and like Python compilation garb. Like
that sucks. But like the actual language
that sucks. But like the actual language
itself, like it's it's all easy.
Oh, cool. Yeah, I should chat with
him. Yeah, send
him. Yeah, send
uh and where where were you chatting
uh and where where were you chatting
with him? Send send us a message because
with him? Send send us a message because
I was working on that as
well. Okay. Yeah, if it makes sense,
well. Okay. Yeah, if it makes sense,
send over. Um, I'd be happy to chat with
send over. Um, I'd be happy to chat with
him because I mean, I'd be very happy if
him because I mean, I'd be very happy if
somebody just helps with that and does
somebody just helps with that and does
that.
that.
Um, that's
like doesn't have to go back to the
like doesn't have to go back to the
drawing board. But if he already has
drawing board. But if he already has
stuff like at least like try at least
stuff like at least like try at least
use our ends to eval the thing so it
use our ends to eval the thing so it
actually make so you make sure it
actually make so you make sure it
actually works and it's not like yet
actually works and it's not like yet
another Atari
overfit. I mean at this point I just
overfit. I mean at this point I just
want all the algorithm stuff to just be
want all the algorithm stuff to just be
like done on that and it's like I'm very
like done on that and it's like I'm very
happy to have more people just like
happy to have more people just like
doing things and just providing MS and
doing things and just providing MS and
tools and things. Honestly, at this
tools and things. Honestly, at this
point, like I'm gonna have to spend a
point, like I'm gonna have to spend a
month probably just going through all
month probably just going through all
the algorithm things, but I kind of
the algorithm things, but I kind of
would just like to spend a month instead
would just like to spend a month instead
just building crazy sims and
just building crazy sims and
uh it's but I think it's makes more
uh it's but I think it's makes more
sense to do like split between
sense to do like split between
algorithms and business side
stuff. Yeah, I uh know the priorities
stuff. Yeah, I uh know the priorities
here at Puffer's got to be the
here at Puffer's got to be the
technically the best. that includes the
technically the best. that includes the
algorithms. But then also uh you know
algorithms. But then also uh you know
it's it does it is very important to
it's it does it is very important to
keep everything growing at a reasonable
keep everything growing at a reasonable
uh pace just because it like it opens so
uh pace just because it like it opens so
many more opportunities for us to do
stuff. All
stuff. All
right. Yeah, sure. Send us uh cuz like I
right. Yeah, sure. Send us uh cuz like I
because I got to honestly apologize to
because I got to honestly apologize to
those guys. Like I really wanted to do
those guys. Like I really wanted to do
way more with their lab and then like I
way more with their lab and then like I
I don't even think the pneumonia is an
I don't even think the pneumonia is an
excuse like cuz I could have just
excuse like cuz I could have just
started on it afterwards. I don't know.
started on it afterwards. I don't know.
My whole like mental like my whole like
My whole like mental like my whole like
the whole way I like like framed and
the whole way I like like framed and
like looked at the work I was doing kind
like looked at the work I was doing kind
of just shifted a bit I guess. I don't
know. I don't know. I kind of just like
know. I don't know. I kind of just like
after that instead of like doing a bunch
after that instead of like doing a bunch
of collabs and stuff, it was just like
of collabs and stuff, it was just like
now I'm going to just like focus up and
now I'm going to just like focus up and
like freaking shove RL as much as I can
like freaking shove RL as much as I can
solo with like the smallest amount of
solo with like the smallest amount of
code plus a few awesome open source
contributors. Okay, cool. So plan for
contributors. Okay, cool. So plan for
today is we'll have tutorials done. So
today is we'll have tutorials done. So
if you want to look at these things, you
if you want to look at these things, you
will have like a very nice like onepage
will have like a very nice like onepage
doc plus two super short ends probably
doc plus two super short ends probably
like 500 lines of code total that you
like 500 lines of code total that you
can read. Um one basic mult one basic
can read. Um one basic mult one basic
single agent en one basic multi- aent n.
single agent en one basic multi- aent n.
There will be a demo for the single
There will be a demo for the single
agent one with the um native API uh or
agent one with the um native API uh or
with the python version as well. But
with the python version as well. But
like we could probably train your Python
like we could probably train your Python
end at 100 or 200,000 steps per second
end at 100 or 200,000 steps per second
without even going to C. Um, and then
without even going to C. Um, and then
there's the C to get to like 2
there's the C to get to like 2
million. All right,
million. All right,
cool. Back to
back. Yep, sounds good. You're getting
back. Yep, sounds good. You're getting
good
good
results. All
right. Oh, yeah. I forgot that I wrote
right. Oh, yeah. I forgot that I wrote
the terminal renderer for pi
the terminal renderer for pi
squared. Funny.
So when you do eval um we forcibly set
So when you do eval um we forcibly set
your be back end to serial because
your be back end to serial because
render gets called on the first end.
So you're not
So you're not
rendering is it with serial back end.
rendering is it with serial back end.
So the only thing I can think of is the
So the only thing I can think of is the
first end in serial is the it gets
first end in serial is the it gets
treated as a driver end.
[Music]
Only thing I can think of is that I've
Only thing I can think of is that I've
never seen it. The only thing I can
never seen it. The only thing I can
think of is when we call render, we mess
think of is when we call render, we mess
specifically with the first amp. That's
specifically with the first amp. That's
about it
though.
though.
Yeah, that's weird.
I will be back after
info should be a list of
I actually don't even know why we have
I actually don't even know why we have
it as a list of dictionaries.
All
All
right. This is
right. This is
the your Python end commented up.
refactor and square a little bit here.
Yeah, this code's going to get a lot
Yeah, this code's going to get a lot
easier.
I think we're I'm definitely going to
I think we're I'm definitely going to
have these tutorials done today unless I
have these tutorials done today unless I
like run out of
like run out of
steam. That should be at least done.
can actually just copy this. I think
Perf score return on. Oh,
good. Some of these comments are just
good. Some of these comments are just
going to be the same.
actually rethe this as well. While we're
here, buffer colors.
We have
We have
a we just don't have a computer
a we just don't have a computer
observation. That's
fine. Is just a required phone.
This is your third first time on
stream. Third first time. That's a new
stream. Third first time. That's a new
one.
You're funny,
man. Are you here for any of the
man. Are you here for any of the
technical content and just like trolling
technical content and just like trolling
too? Or you just here to troll? I got to
too? Or you just here to troll? I got to
know.
anything else that I forgot to comment.
All right, this is pretty solid.
PC
PC
specs. Uh, I mean, this one's got a 4090
specs. Uh, I mean, this one's got a 4090
and
and
9950X and 128 gigs DR5.
9950X and 128 gigs DR5.
Uh the one in the back that's on the
Uh the one in the back that's on the
shelf way back there has got a 5090 and
shelf way back there has got a 5090 and
the same
the same
otherwise. And then we've got like eight
otherwise. And then we've got like eight
more that are 4090s with Intel chips
more that are 4090s with Intel chips
that I don't recommend. And
that I don't recommend. And
uh let's see. We're getting two tiny
uh let's see. We're getting two tiny
boxes in like a week or two that are
boxes in like a week or two that are
6490s a piece.
6490s a piece.
Um and then hopefully we'll be getting
Um and then hopefully we'll be getting
more after that.
We got a lot of PCs.
Now, can you send me a
Now, can you send me a
PC? No. PCs are for reinforcement
learning. You can get PC access if you
learning. You can get PC access if you
do reinforcement
learning. All
right, cool. This seems uh this seems
decent. Got to do reinforcement
decent. Got to do reinforcement
learning. I don't know what you mean.
learning. I don't know what you mean.
You need to do the
RL. You got 55090s? No, I only have one
RL. You got 55090s? No, I only have one
5090 at the moment, unfortunately. I
5090 at the moment, unfortunately. I
would like many more 5090s, but
would like many more 5090s, but
unfortunately
tariffs. I want to buy like another 30
tariffs. I want to buy like another 30
of them. 30 or 40.
I think the play might just be to wait
I think the play might just be to wait
for a little bit more revenue and then
for a little bit more revenue and then
just buy them anyways. We'll see. Tiny
just buy them anyways. We'll see. Tiny
boxes are not here yet. Actually, that
boxes are not here yet. Actually, that
is a good point. Let me go check
is a good point. Let me go check
on where are my tiny boxes.
Where the heck is uh I put these on
Where the heck is uh I put these on
my can't find the
email. Oh yeah, there it
email. Oh yeah, there it
is. So this was on
your
your
order. You've received your
order. They actually should be here.
What programming languages do you know?
What programming languages do you know?
All of
them. But I can kind of write code in
them. But I can kind of write code in
any like if you give me like a day, I
any like if you give me like a day, I
can write code in just about
anything. I mostly write C and
Python. So you do see my comments.
I don't know why you do that for free,
I don't know why you do that for free,
man. You can make good money doing that.
All right, this seems like it's good. We
All right, this seems like it's good. We
just got to make sure I didn't break a
just got to make sure I didn't break a
ton of
things. This actually uh cleanly
exits. Target has mem leaks.
Tiny box single board. Now it's um
uh it is
uh George Hotz's like six
uh George Hotz's like six
GPU
box. Let me do let me finish this and
box. Let me do let me finish this and
then there's and then I'll go finish my
then there's and then I'll go finish my
my
my
set. Figure out what else from there.
thought that it's just like unload
texture. Unload texture. Close window.
texture. Unload texture. Close window.
Oh, did I forget to close the window?
I did forget to close the window.
Perfect. No mem links.
What happened
here? End of creator must be a list of
here? End of creator must be a list of
callables. Oh, I know what it is
callables. Oh, I know what it is
actually.
by. So we have from
upper pi squared.
Why is it on size
Why is it on size
squared? Oh, I
squared? Oh, I
know. Oops.
Oh, that's right.
A little bit more work it seems to be
A little bit more work it seems to be
done to get these things to like rain
done to get these things to like rain
correctly.
I'm going to go get my set done and then
I'm going to go get my set done and then
uh Oops. and kill this bug. And then we
uh Oops. and kill this bug. And then we
will be back. I will finish cleaning up
will be back. I will finish cleaning up
all these MS and then we will make the
all these MS and then we will make the
ducks.
Decent enough leg training for today.
See, Linky, this is the type of these
See, Linky, this is the type of these
are the type of improvements we
are the type of improvements we
need. All right, I'll think
need. All right, I'll think
of puffing alternatives.
Why is this thing not train?
How's there no puffer squared?
All the
balls, Python
version and then the
solves takes it 40 million
steps. Hang on.
steps. Hang on.
This is 8192
This is 8192
total. Oh, wait. No, this is because
total. Oh, wait. No, this is because
that's
that's
eval. So, it'll do 20
eval. So, it'll do 20
mil in solved in pretty well. Yeah, get
mil in solved in pretty well. Yeah, get
solved in 20 mil. And then just runs
solved in 20 mil. And then just runs
extra
ebells. Yeah. And this is fine that we
ebells. Yeah. And this is fine that we
have it this way because this is like
have it this way because this is like
our
our
standard standard
configuration. So this should
be should really be
be should really be
8192 on the same two workers would be
Okay. So, this is 400 something K.
Oh, and this does train. Cool. This
Oh, and this does train. Cool. This
totally trains just the same.
So, we can actually just start
So, we can actually just start
um we can start putting these
um we can start putting these
into uh into tutorials and docs, right?
into uh into tutorials and docs, right?
2 p.m. I've got plenty of time. Let's
2 p.m. I've got plenty of time. Let's
commit all this stuff up.
commit all this stuff up.
Uh, we should
Uh, we should
probably There's one other thing we
should do like a speed test thing.
Oh yeah, this is not vectorzed, huh?
All
right, this is pretty good. So, this is
right, this is pretty good. So, this is
a
270K like this.
We'll put this test
on
on
whereh pi Okay.
Oh, that does make it substantially
Oh, that does make it substantially
faster. Cool.
We got to at least be fair to our
We got to at least be fair to our
Python,
right? We can't make the Python like
right? We can't make the Python like
even worse than it already is. That
even worse than it already is. That
would just be
would just be
needlessly
needlessly
silly. Uh what else?
F.
This should give us some crazy number.
Okay. So, I think that um 113 million
Okay. So, I think that um 113 million
steps per second is pretty good. I don't
steps per second is pretty good. I don't
know about
you, I think that's pretty good. That
you, I think that's pretty good. That
put this in
target. Some
M's agents number four.
Python perf
test back
reset. Boom.
could not
broadcast. 16,000
two mains, buddy.
What's
this? I think we just like break point.
That's pretty good.
Low, high, and then
size.
broadcast output
shape. All right, this is cool.
shape. All right, this is cool.
So we
do
do
action environ.
All right. So, this one gives
All right. So, this one gives
you 35 million SPS. Still
you 35 million SPS. Still
decent. Decent 35 million, you
decent. Decent 35 million, you
know. And then you have your pain points
know. And then you have your pain points
listed
listed
here. I'm going to just put these
into We'll put these into here for
into We'll put these into here for
now. I can commit this cleanly.
There we go.
I change
there. That's fine.
And this is where I wanted to cach some
And this is where I wanted to cach some
credentials. And we can push
this command.
Okay, now we have all our nice new
Okay, now we have all our nice new
tutorial items and
uh we should be able to just open
uh we should be able to just open
up the uh hang
up the uh hang
on should be able to just
on should be able to just
open buffer tank buffer
open buffer tank buffer
AI
docs ML
file and look at that actually it has
file and look at that actually it has
um that's me right there doing like I
um that's me right there doing like I
have the stream on
have the stream on
a new site. And then what we get to do
a new site. And then what we get to do
is we get to add this to our docs
page. I'm going to put this
page. I'm going to put this
here. Uh we are pretty well set up with
here. Uh we are pretty well set up with
the code. So I'm going to do a couple
the code. So I'm going to do a couple
quick sets so I get some sort of
quick sets so I get some sort of
exercise in
exercise in
today. and then I will be back in a
today. and then I will be back in a
couple
couple
minutes and we will um write the
minutes and we will um write the
documentation section for these M.
documentation section for these M.
You'll have feedback on what you'd like
You'll have feedback on what you'd like
to see or uh making it easier for other
to see or uh making it easier for other
folks to write cool ultra high perf sims
folks to write cool ultra high perf sims
for RL, you let me know.
I'll be back.
All
right. I do not like writing
right. I do not like writing
docs and I especially don't like writing
docs and I especially don't like writing
docs for the sake of writing docs.
So I'm just going to try to look at this
So I'm just going to try to look at this
and figure out what is the
and figure out what is the
shortest simplest thing we can give
shortest simplest thing we can give
people so that they will be able to
people so that they will be able to
write M's.
The main thing I want to decide on is
The main thing I want to decide on is
whether I want to actually put the code
whether I want to actually put the code
here or just link to it.
honestly the stuff I have here is pretty
honestly the stuff I have here is pretty
well thought out.
well thought out.
Um,
Okay.
I don't think what I even
I don't think what I even
put in here.
I'm just trying to think how much of the
I'm just trying to think how much of the
code I want to inline.
I'm trying to think like what is the
I'm trying to think like what is the
amount of stuff I need to write here to
amount of stuff I need to write here to
make this clear without just writing a
make this clear without just writing a
whole bunch of like stuff people only
whole bunch of like stuff people only
need to read.
It might be better to just inline the
It might be better to just inline the
code. Now
I think it would make sense too.
I kind of know all the things that I
I kind of know all the things that I
want. I just noticed I don't know how I
want. I just noticed I don't know how I
want to structure this at
want to structure this at
all. Also don't want to agonize over it
all. Also don't want to agonize over it
for too long. It's like pretty basic
for too long. It's like pretty basic
ultimately, right? Like I have good
ultimately, right? Like I have good
commented code for how to do this.
It's a good idea to just paste it in
here. Kind of fine
too. It's a lot of code to put like on a
too. It's a lot of code to put like on a
doc page though.
I want to just put no code here and link
I want to just put no code here and link
to it, but like I something's telling me
to it, but like I something's telling me
that's a dumb
idea. If I just say I'm going to paste
idea. If I just say I'm going to paste
the code and do it like then it's a lot
the code and do it like then it's a lot
easier, right?
like I don't think this that was it. No.
Yeah, let's paste. Let's see what this
Yeah, let's paste. Let's see what this
looks like if I um I just like start
looks like if I um I just like start
pasting this in
Got a a whole bunch of these nice
Got a a whole bunch of these nice
bluebirds
bluebirds
outside. Very peaceful.
I think I will go for a good walk after
I think I will go for a good walk after
finishing all these docks.
just makes up random stuff. The model
just makes up random stuff. The model
does
I think what I'll do is I'll put this
I think what I'll do is I'll put this
one online and I'll put I'll leave the
one online and I'll put I'll leave the
other one on GitHub.
really was 400k,
right?
Yeah. Something like that.
Add this
Add this
here. And bugs, man.
Wham!
Oops. here.
the macro. Thank
Okay. So, let's see what this looks like
Okay. So, let's see what this looks like
so far. If I uh open this
so far. If I uh open this
thing. If I open this thing
up. So that Okay, that italics doesn't
up. So that Okay, that italics doesn't
help.
How much code this
Hey Spencer, I'm trying to figure out
Hey Spencer, I'm trying to figure out
what the dock should look like
here. Oh, this was totally fine. Hang
here. Oh, this was totally fine. Hang
on.
There we go.
I think that's decent. And I think what
I think that's decent. And I think what
I'll do is I'll put
I'll do is I'll put
um I'll put this as like the first M.
um I'll put this as like the first M.
I'll give some additional tips with like
I'll give some additional tips with like
debugging and
debugging and
um I will link the uh the other target
um I will link the uh the other target
ends.
ends.
And then that will be
good. That sounds
good. That sounds
good. That's what we will do.
This is
cool. See, the memory obsession though
cool. See, the memory obsession though
is a bit weirder than the perf
obsession.
also true.
Okay, let's see what the uh the docks
Okay, let's see what the uh the docks
look like.
All right, let's see how this does.
So we now
So we now
have a
have a
tutorial writing custom environments
tutorial writing custom environments
that run 1 mil plus steps per
that run 1 mil plus steps per
second. We have the square end which is
second. We have the square end which is
page and a half of code and then we
page and a half of code and then we
have a little explanation. It's farewell
have a little explanation. It's farewell
commented. Then we have the C version.
Uh, I messed up a
Uh, I messed up a
parenthesy was a paragraph or whatever.
Not bad.
probably clean up links and things,
but probably put some like bunch of
but probably put some like bunch of
things. But this
things. But this
is I mean this is like a nicely
is I mean this is like a nicely
commented full code example.
That's decent
progress. Decent
progress. Got the M's done. They're
progress. Got the M's done. They're
clean. Um, there might still be a better
clean. Um, there might still be a better
way I can present it on the site.
way I can present it on the site.
I think it'll be easier once I have like
I think it'll be easier once I have like
the inlinable demos or
whatever. I could also put a GIF, but
whatever. I could also put a GIF, but
probably just inline a demo.
Let me see.
Um, I should probably just add some
Um, I should probably just add some
debug tips actually. Let's do that.
Yeah, I know what we're going to do.
Yeah, I know what we're going to do.
We're not a checklist.
I think it does reset, does
I think it does reset, does
it? I honestly
forget. I don't think it does.
That's
pretty good.
All
right, that ought to be useful,
right? This
Very nice. All right, I'm happy with
that.
Cool. Um, it is 400
Cool. Um, it is 400
p.m. Happy Thursday as well. There's a
p.m. Happy Thursday as well. There's a
restroom real quick and then I think we
restroom real quick and then I think we
um should probably like go check on and
um should probably like go check on and
set up some sweeps instead of just
set up some sweeps instead of just
working on docks all day cuz we got to
working on docks all day cuz we got to
check on our experiments and make sure
check on our experiments and make sure
we're
we're
uh we're making progress towards the
uh we're making progress towards the
final final experiments on here. So, no
final final experiments on here. So, no
more freaking boring docs for today. I
more freaking boring docs for today. I
got the portion of that that I wanted
got the portion of that that I wanted
done, which is we have a
done, which is we have a
decent environment tutorial, which is
decent environment tutorial, which is
mostly just a bunch of nicely commented
mostly just a bunch of nicely commented
code.
code.
Um, but also, you know, some additional
Um, but also, you know, some additional
things on debugging and whatnot.
Yeah, I will be back. Here's the
Yeah, I will be back. Here's the
restroom. Get a couple sets in real
restroom. Get a couple sets in real
quick and then we will uh yeah, we'll
quick and then we will uh yeah, we'll
actually get some uh experiments going.
actually get some uh experiments going.
We'll see how that goes.
All
right. Experiment's going. Unless
right. Experiment's going. Unless
there's
anything. All right. Nothing to deal
anything. All right. Nothing to deal
with
with
there. One message.
So, I haven't run any experiments in a
So, I haven't run any experiments in a
couple of days
couple of days
actually, but we had mazes working very
nicely. Trying to think what type of
nicely. Trying to think what type of
stuff. I think we just want to get meta
stuff. I think we just want to get meta
going. Let me see if I can get the meta
going. Let me see if I can get the meta
out to
train. Oh yeah, they changed a whole
train. Oh yeah, they changed a whole
bunch of stuff, didn't
bunch of stuff, didn't
they?
they?
Forgot about
that. I really want to get a sweep for
that. I really want to get a sweep for
these guys. I'm trying to
these guys. I'm trying to
think the easiest way I can do that is.
Um, I think
This get this to train.
What's their
What's their
setup? Setup
have they just do a pie bind
have they just do a pie bind
extension. No other
extension. No other
depths. No, they have
requirements
requirements
upper. They have
upper. They have
requirements
requirements
pinned. They have they've got way too
pinned. They have they've got way too
much stuff pinned.
much stuff pinned.
Yeah, you don't need all that, guys.
Let me just see if there's anything
Let me just see if there's anything
that's like bad
that's like bad
bad. Think
so. Well, we'll try this and see if it
uh it messes anything up.
That
it. Oh yeah, they built it built
it. Oh yeah, they built it built
something.
This is like this cool like factory grid
This is like this cool like factory grid
based M type
thing
here. Oh my
conf a pip.
Is it hydra
core? Just add this N.
Duck
Duck
DB. You got to be kidding me. Duck
DB. You got to be kidding me. Duck
DB. The hell is that?
Okay, so
apparently there Can you interpret on
this? I don't know what this is.
full
full
key map
builder that room.
Okay, you just ask
him. They have like a new
him. They have like a new
basic config maybe for me.
So is it benchmark
yl the most you're going to get is
yl the most you're going to get is
uh some incline bench sets at the
moment. Move some deprecated features.
moment. Move some deprecated features.
Okay.
Okay.
So
So
like this it
like this it
though. I don't think it's this
though. I don't think it's this
one benchmark
sample. It's sampling
sample. It's sampling
removed. All right.
We need 16 agents in
here.
24.
Why does not match number of agents in
Why does not match number of agents in
map?
Oh, I guess it's because this needs to
Oh, I guess it's because this needs to
be Four.
No. Is that actually a Okay.
Yeah, this really this really annoys
me. I mean, technically I could just
me. I mean, technically I could just
change. Okay, let's say that I use their
change. Okay, let's say that I use their
original config, right? Did it
load? Okay, it does. So, I just need to
load? Okay, it does. So, I just need to
change the batch sizes
then what did this? So, I have 128
then what did this? So, I have 128
times in 4096 total
agents or is it divisible?
agents or is it divisible?
Nope. Uh, so I just have to get
That's actually really
That's actually really
annoying. Uh, I can multiply
by
8.
8.
Maybe that's not
Maybe that's not
terrible. So, we do like 30
72 M.
72 M.
And we have a batch size of 128
And we have a batch size of 128
m. This Yeah, this
works. Would you mind running that trady
works. Would you mind running that trady
val? Yeah, sure
thing.
thing.
Um, where do I get it?
DM you. All right. Let me know.
Okay, that
Okay, that
trans.
trans.
Boom. Doesn't give us any
stats. At least not so
stats. At least not so
far. Go get this repo. Oh, there it is.
far. Go get this repo. Oh, there it is.
Okay, it gives us stats. Let me put this
Okay, it gives us stats. Let me put this
on
on
Neptune so I can do this in the
Neptune so I can do this in the
background while we uh while I do the
background while we uh while I do the
other eval thing.
get my API
token. Okay. Very very secret repo.
Cops are
takes forever to
takes forever to
build. Oh, you know
build. Oh, you know
why? God damn
it. Forget the magic argument that makes
it. Forget the magic argument that makes
it fast and also not
break. Love when they're magic arguments
break. Love when they're magic arguments
that you need to make things not break.
Oh, I didn't train this for enough steps
Oh, I didn't train this for enough steps
at all, did
I? Okay, that after I fix this thing for
you. I don't know why your eval thing
you. I don't know why your eval thing
doesn't
doesn't
work and you see your errors and stuff
work and you see your errors and stuff
at some point.
token.
that terminal is on. I I mean it doesn't
that terminal is on. I I mean it doesn't
matter. It's like
matter. It's like
semi-privateish, right?
I'm just checking if the curve goes up
I'm just checking if the curve goes up
on a environment real quick and then
on a environment real quick and then
we'll go back to the other environment
we'll go back to the other environment
to check if the curve goes
to check if the curve goes
up. Pretty much it. Is it score? Score
up. Pretty much it. Is it score? Score
is the same metric, right?
What are we supposed to get out of
this? Oh, is it are the prams like
this? Oh, is it are the prams like
fiddly?
Oh, is it supposed to be perfect six to
Oh, is it supposed to be perfect six to
70 or what?
70 or what?
This score is 60. Like
what? Like
perf, you
know,
know,
460k.
Okay, that's going up.
6 million steps per
second. I'm going to do a set while I
second. I'm going to do a set while I
wait for this [ __ ]
There you
go. Even better.
We'll
do. We get 80 on this. More than 80.
I don't know. Maybe I just magically
I don't know. Maybe I just magically
improved
things. Hour of
puffer. Five minute training for half a
puffer. Five minute training for half a
billion steps.
Uh, is the thing supposed to render with
Uh, is the thing supposed to render with
the numbers all over the entire
Ran or did you not commit a font
file? Can you check if you committed
file? Can you check if you committed
your font file?
Don't
Don't
file.
What? Confused.
Yeah, just add add your the font file or
Yeah, just add add your the font file or
whatever. Go add that. Pull
whatever. Go add that. Pull
this. I'll give you your eval.
way you do that. I'll
way you do that. I'll
do back to the um at the experiment.
This is like what we're supposed to get,
right? One of these
right? One of these
isish. See how this goes if I run it
for 100 mil.
Not. Oh, Twitter 8 message. Gotcha.
Not. Oh, Twitter 8 message. Gotcha.
Well, when you get me that file, I will
Well, when you get me that file, I will
fix this.
All right. So, let's just go get the
All right. So, let's just go get the
metab box puff box.
for container.
Right. We have to
do Oh, they have a different
do Oh, they have a different
one. Hang on. They have a different
one. Hang on. They have a different
config they want me to use.
See what I can get to work on
See what I can get to work on
this. Heck could I just do
Um,
The heck is wrong with this? Oh, we have
The heck is wrong with this? Oh, we have
the wrong popper installed.
Find what the base image is.
Find this.
23 gigs.
wrong with This
This is running
soon.
soon.
Holy thunderstorm
Oh, it has started completely
Oh, it has started completely
thundering. Hopefully we uh don't get
thundering. Hopefully we uh don't get
knocked out on the internet.
Wait, you need to run register.
Wait, where is this
Wait, where is this
thing? Oh
thing? Oh
jeez. All this stuff.
Then this calls this logger which is
Then this calls this logger which is
[Music]
like Yeah, that's annoying as hell.
and I just like do this.
and just do this.
Okay.
Okay.
So, possibly we can make this
faster if we update to our latest param
sets. I believe this is
Oh, it's really thundering outside.
Oh, it's really thundering outside.
Well, if I'm off if I go offline, you
Well, if I'm off if I go offline, you
know
know
why. It's really
thundering. Okay, we want
thundering. Okay, we want
64 64 of these at MMS, I
believe. And 32 of them
believe. And 32 of them
per per
per per
batch. And then we will keep this 32k
And we'll see. We will see how this
goes. Okay. So, this is actually
goes. Okay. So, this is actually
slower, but we will see if it's slower
slower, but we will see if it's slower
on the other GPU.
Okay, let's go grab here.
Why not
Why not
on this box
here. Oh.
Metagrid
Metagrid
requires. Well, we don't like that.
Huh. So, it's literally ignoring the uh
Huh. So, it's literally ignoring the uh
the Rex in here. Weird.
just grab the newest
one, our fork.
Ah, this is actually pinned. I don't
Ah, this is actually pinned. I don't
know how it is that I've been able to
know how it is that I've been able to
run
it does the speed environment your
it does the speed environment your
policies don't take 99.99% as the
thing like in the majority of RL
thing like in the majority of RL
researcher environment takes 90% of the
researcher environment takes 90% of the
time. The fact that our policies are
time. The fact that our policies are
taking like are taking most of the time
taking like are taking most of the time
now is just because we've optimized our
now is just because we've optimized our
environment so
well. So like that wasn't a thing before
well. So like that wasn't a thing before
us. Like pretty much everyone else in
us. Like pretty much everyone else in
the whole rest of RL is spending like a
the whole rest of RL is spending like a
good chunk of their uh compute if not
good chunk of their uh compute if not
the majority of it on the nth
Okay. So this
Okay. So this
is
370. Grab a new one of
these. Make sure we really give these
these. Make sure we really give these
guys some good policies and good a good
guys some good policies and good a good
sweep setup.
Yeah. Okay. So, that doesn't make a huge
difference. Did I do that
right?
right?
416. I did that totally wrong. It's
416. I did that totally wrong. It's
supposed to be 256*
supposed to be 256*
No.
No.
512.
Yeah. 12
28. I'll see if this is suddenly
fast. Yeah, but not everything is VLMs.
Like we're solving we are solving
Like we're solving we are solving
problems for people with like single
problems for people with like single
digit million
parameters. There we go. 800K.
Make sure that this
Make sure that this
is logging for us.
Okay, there we
Okay, there we
go. I will let this
go. I will let this
run and uh we will see how this does.
run and uh we will see how this does.
Here's a restroom and get a good set in
Here's a restroom and get a good set in
and then we will uh we'll get the sweep
and then we will uh we'll get the sweep
set up. We'll do a few other things. Be
set up. We'll do a few other things. Be
good.
cold
outside.
Borman. Hey, Plasma.
Borman. Hey, Plasma.
What's up, man?
being confused at what policy is doing.
Policying bomb pushed up. Let me get
Policying bomb pushed up. Let me get
that Spence.
Do you have the rewards
Do you have the rewards
negative? It's never that. I've never at
negative? It's never that. I've never at
I've never once had the sign wrong, but
I've never once had the sign wrong, but
it's funny.
Okay. Make sure I'm on the right
Okay. Make sure I'm on the right
machine. I am on the right
machine. Brake system packages.
system. I got to figure out a way to
system. I got to figure out a way to
make it just the default be that because
make it just the default be that because
it's ridiculous.
packaging
packaging
was. It's all
right. All right. Puffer
right. All right. Puffer
[Music]
evalu.
Oops. I forget to
pull. Yes.
how it's supposed to
[ __ ] Font's a little like all over the
[ __ ] Font's a little like all over the
place, but
I will give you that file.
Does it is it resizable?
All right. I will send you
All right. I will send you
this. I can keep running these here if
this. I can keep running these here if
you
want. I can also help you fix your
want. I can also help you fix your
uh render
uh render
setup. All right. Take a quick look. I
setup. All right. Take a quick look. I
will make sure this this thing is good
will make sure this this thing is good
in the
meantime. Not this stuff again. Bound at
meantime. Not this stuff again. Bound at
05.
Yeah.
Yeah.
Okay. Is a
Okay. Is a
run.
run.
Amazing. We will run a sweep though.
E7 like one
E7 like one
[Music]
[Music]
E8
E8
I8
I8
auto and we will see how
auto and we will see how
this
Horizon mini
batch and we'll see how everything else
and I'm still on the
and I'm still on the
wrong uh I'm on the
wrong uh I'm on the
wrong end. Lovely.
All right. So, for real this
time, we'll comment all this.
Um,
Okay, we will see if this discovers
Okay, we will see if this discovers
anything
anything
interesting and we'll keep checking back
interesting and we'll keep checking back
in on this every so often. Goes over
in on this every so often. Goes over
here for
now. What do you want me to do with the
now. What do you want me to do with the
uh render? Spencer want me to render
uh render? Spencer want me to render
more stuff. What do you
want? I guess in the meantime while I'm
want? I guess in the meantime while I'm
look
look
at our
container reading
mode. I could push the Docker for now,
mode. I could push the Docker for now,
right?
Make sure I don't screw anything up
time. Fine.
takes forever even with
takes forever even with
UV Docker
UV Docker
build. I just want to upload it so that
build. I just want to upload it so that
I can see how large the
I can see how large the
um compressed the compressed file is.
press size of 9.8
gigs. We'll see what the new one is.
downloads. For some reason, the depths
downloads. For some reason, the depths
file has 2K
downloads. Somebody must have it in
downloads. Somebody must have it in
their
CI. All right, so
Docker
push. What's the push then? How's the
push. What's the push then? How's the
push
push
work? Username
tag.
tag.
Username
Username
name tag.
Does this not just
Does this not just
work?
work?
Oh,
push. And then there is some uh
push. And then there is some uh
shenanigans.
Uh, I don't remember having a freaking
Uh, I don't remember having a freaking
2FA for Docker of all bloody
things. I don't have one, my
things. I don't have one, my
guy. Oh, dummy. It's
guy. Oh, dummy. It's
right, dummy.
Is it in the wrong browser?
There we
There we
go. So, uh we will see how large the
go. So, uh we will see how large the
docker
docker
is. Seems large.
If it's like 20 gigs, then I'm going to
If it's like 20 gigs, then I'm going to
have to figure out what the heck is
have to figure out what the heck is
wrong with it because it should not be
Uh this might be reream
Uh this might be reream
actually. Uh this might be reream
actually. Uh this might be reream
actually.
We
record. We
record. We
record.
record.
Oh, okay.
note to self, don't push Docker images
note to self, don't push Docker images
on stream
on stream
again. Um, I'll be back after dinner.
again. Um, I'll be back after dinner.
And this is [ __ ]
