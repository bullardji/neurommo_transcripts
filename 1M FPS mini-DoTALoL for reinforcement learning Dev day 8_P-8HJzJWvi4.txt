Kind: captions
Language: en
e
e
e
e
e
e
e
e
e
e e
thank you I forgot to unmute the mic
thank you I forgot to unmute the mic
thank you for
thank you for
that
okay that's what chat's for
okay that's what chat's for
we're good
we're good
now welcome
now welcome
back um it turns out that for whatever
back um it turns out that for whatever
reason uh my
reason uh my
ethernet is really unstable and the
ethernet is really unstable and the
Wi-Fi is stable I don't know how that's
Wi-Fi is stable I don't know how that's
possible because they're both plugged
possible because they're both plugged
they're plugged directly in but um
they're plugged directly in but um
yeah anyways uh to reiterate all the
yeah anyways uh to reiterate all the
stuff that uh that we're doing here so
stuff that uh that we're doing here so
uh I figured out that the rewards were
uh I figured out that the rewards were
getting really weirdly scaled in the
getting really weirdly scaled in the
observations you include the rewards in
observations you include the rewards in
the observations it's a big thing um and
the observations it's a big thing um and
that means that they weren't able to
that means that they weren't able to
actually see the rewards that they were
actually see the rewards that they were
getting but now that I fixed it it looks
getting but now that I fixed it it looks
like they're scaled in the opposite
like they're scaled in the opposite
direction they're over scaled so that
direction they're over scaled so that
leads me to believe that there's
leads me to believe that there's
something wrong with this reward
something wrong with this reward
function and that as soon as we fix it
function and that as soon as we fix it
it should be able to solve this this is
it should be able to solve this this is
a very very easy task um not the whole
a very very easy task um not the whole
environment obviously but just go to the
environment obviously but just go to the
center is given a dense uh given a like
center is given a dense uh given a like
reward every time you get closer that's
reward every time you get closer that's
a very very easy task to solve so this
a very very easy task to solve so this
is the sanity check goal for today right
is the sanity check goal for today right
is we want to have these things just go
is we want to have these things just go
to the center and fight it out learn the
to the center and fight it out learn the
most basic strategy and then we'll
most basic strategy and then we'll
figure it out from
figure it out from
there so how the heck are these things
there so how the heck are these things
getting rewards of
.99 self. rewards of PID
.99 self. rewards of PID
and compute
and compute
observations uh do we zero these in
reset do I zero these in
reset I don't zero the rewards in
reset I don't zero the rewards in
reset
reset
so I could do self. rewards of pit is
so I could do self. rewards of pit is
zero let's try this let's see if it's
zero let's try this let's see if it's
just a reset
bug okay
zero on the reset Time
step 996
step 996
still
so am I just scaling this wrong let me
so am I just scaling this wrong let me
think
think
um it's 0.1
it's 50 times the reward cast to an
it's 50 times the reward cast to an
unsigned
unsigned
sh and then so this is 0 to 255 and then
sh and then so this is 0 to 255 and then
it gets divided by
255 so yeah literally this has to be
255 so yeah literally this has to be
like this has to be 255 in order for
like this has to be 255 in order for
this to happen I would
this to happen I would
think
um you can cast like this
right this doesn't do something
right this doesn't do something
weird no it works perfectly fine for
weird no it works perfectly fine for
player X and player y it looks
player X and player y it looks
like right yeah those are reasonable
like right yeah those are reasonable
values for X and
values for X and
Y very reasonable values
so I've got to assume that the rewards
so I've got to assume that the rewards
are getting weirdly computed at some
are getting weirdly computed at some
point
right we're not muted now right yeah no
right we're not muted now right yeah no
we're good should be good at
least I'm assuming that people are not
least I'm assuming that people are not
watching a muted stream so I'm assuming
watching a muted stream so I'm assuming
assuming we're not muted anymore
um not muted good thank
you we do this difference right
you we do this difference right
here we have previous distance T we have
here we have previous distance T we have
previous distance we have curent
previous distance we have curent
distance this should be at most
distance this should be at most
one and then the reward
one and then the reward
is uh 0.1 times
is uh 0.1 times
this shouldn't be getting any
this shouldn't be getting any
XP so this should be the only reward
XP so this should be the only reward
component I think we should just print
component I think we should just print
this
out we should just print out this reward
out we should just print out this reward
right here right
PD
right let's see if this uh let's see if
right let's see if this uh let's see if
the reward is too large going into the
the reward is too large going into the
observations here
okay
so flat
so flat
features zero rewards right we hit
continue we see a bunch of
continue we see a bunch of
zeros we see a negative value
zeros we see a negative value
here ah
here ah
right negative values are going to roll
right negative values are going to roll
over
yeah negative values are going to roll
yeah negative values are going to roll
over that's
why so because we have a a small
why so because we have a a small
negative this turns into a big positive
negative this turns into a big positive
okay
so it does make sense to
so it does make sense to
have a negative reward here though
maybe I just want to center
maybe I just want to center
it what if I just do like
it what if I just do like
25 25 times rewards plus
25 25 times rewards plus
right I could do
this little awkward but let's see what
this little awkward but let's see what
happens if I do this
okay we should still have some negatives
okay we should still have some negatives
right and
then first
then first
perfect we all these are
perfect we all these are
all 0.5 plus or minus a little
all 0.5 plus or minus a little
bit
um the thing is these are really close
um the thing is these are really close
together
together
right these are really close
right these are really close
together I think what we're going to do
together I think what we're going to do
is we're going to modify this curve so
is we're going to modify this curve so
that we can scale more
that we can scale more
aggressively because those those values
aggressively because those those values
are going to be too close to
are going to be too close to
separate so
oops we're going to do int
reward and then we're going to
reward and then we're going to
do
do
reward equals int XP
reward equals int XP
/ so a th000 good I think a th000 is
/ so a th000 good I think a th000 is
still
still
well
well
250 250 is like a big objective
right that's a nice scale
right that's a nice scale
yeah and then what we're going to do is
yeah and then what we're going to do is
if
if
reward greater than
reward greater than
one we're going to clip
one we're going to clip
it reward
it reward
now we know that we're never getting
now we know that we're never getting
more than one from that
more than one from that
Source
right so now we can scale much more
aggressively and I can say that this is
aggressively and I can say that this is
going to be
I can't multiply by
I can't multiply by
128 I multiply by
128 I multiply by
100 hey Quinn
100 hey Quinn
welcome we're debugging rewards here so
welcome we're debugging rewards here so
that we can train some good
models okay so like the best reward you
models okay so like the best reward you
can get right now is 1.1
can get right now is 1.1
right
right
so if I do like one 1.1
so if I do like one 1.1
time was it plus 120 was it
time was it plus 120 was it
1.1 time 100 +
1.1 time 100 +
128 238 and then if I
do negative 0.1 is the worst you can
do negative 0.1 is the worst you can
do
118 I don't like that
okay this is
fine I'm just trying to find some decent
fine I'm just trying to find some decent
scaling here I think think I found it
scaling here I think think I found it
perfectly pretty much so we're going to
perfectly pretty much so we're going to
do uh this is going to be 200 time this
do uh this is going to be 200 time this
reward plus 32 and then we're going to
reward plus 32 and then we're going to
spot check
spot check
this um we're going to spot check this
this um we're going to spot check this
to make sure this doesn't overflow and
to make sure this doesn't overflow and
obviously I'm not going to leave all
obviously I'm not going to leave all
this stuff hardcoded like this because
this stuff hardcoded like this because
this is like ticking Time Bomb
this is like ticking Time Bomb
um but the goal is to just get some
um but the goal is to just get some
models to work and then make sure that
models to work and then make sure that
this is reasonable
okay so flat featur should be
okay so flat featur should be
empty or
empty or
not right because it's this is the zero
not right because it's this is the zero
because it's is
because it's is
fine this data should now be diverse
fine this data should now be diverse
enough yep so now we at least have some
enough yep so now we at least have some
reasonable level of Separation here I
reasonable level of Separation here I
mean it's not huge it leaves a lot of
mean it's not huge it leaves a lot of
room for high higher Ward spikes but I
room for high higher Ward spikes but I
think this is good
overall I think this is
overall I think this is
good so with this
fixed I think that's actually going to
fixed I think that's actually going to
make it easier to learn as well because
make it easier to learn as well because
they're going to get a big reward for uh
they're going to get a big reward for uh
for fighting creeps
uh rescale
rewards we'll see whether this is enough
rewards we'll see whether this is enough
on its own or not it very well might not
on its own or not it very well might not
be and we're going to set out more
be and we're going to set out more
reasonable 250 mil
reasonable 250 mil
steps these are are still decent default
steps these are are still decent default
hyper pram so we're going to keep
these and then we're going to get models
training track okay
we're now training we're training at
we're now training we're training at
over
over
500k it's not perfect but it's
500k it's not perfect but it's
decent and we'll see how the reward
decent and we'll see how the reward
curves happen I'm going to go use the
curves happen I'm going to go use the
restroom I'll be right back and then we
restroom I'll be right back and then we
will look at the reward curves for
this
e
e e
so we've got this thing training fast
so we've got this thing training fast
very nice of course it slows down as
very nice of course it slows down as
soon as I come back
what's this mooba
what's this mooba
say very weird reward goes up and then
say very weird reward goes up and then
goes back
goes back
down no
down no
levels so this is worse
levels so this is worse
um where's entropy
are we getting entropy
are we getting entropy
crashes doesn't look like we're getting
crashes doesn't look like we're getting
entropy
entropy
crashes I want to check the one from
crashes I want to check the one from
last time because the run from last time
last time because the run from last time
was clearly an entropy crash but I don't
was clearly an entropy crash but I don't
know if it registered as
one no not this one the one
one no not this one the one
before this is another common uh common
before this is another common uh common
debug thing
debug thing
here if entropy is crashing it means
here if entropy is crashing it means
that your policy is
that your policy is
collapsed so interestingly this thing
collapsed so interestingly this thing
which was basically the agents just like
which was basically the agents just like
not doing
not doing
anything uh this actually has reasonably
anything uh this actually has reasonably
High entropy so something is very wonky
High entropy so something is very wonky
here because that shouldn't be possible
here because that shouldn't be possible
do you think having a continuous reward
do you think having a continuous reward
of positive numbers equal to a reward
of positive numbers equal to a reward
centered on zero with positive basically
centered on zero with positive basically
does poo algorithm
does poo algorithm
differentiate it's all Rel it is all
differentiate it's all Rel it is all
relative um po includes a value function
relative um po includes a value function
Baseline so it's actually just going to
Baseline so it's actually just going to
be the difference in the reward and what
be the difference in the reward and what
the model has expected the reward should
be are you training an AI to play DotA
be are you training an AI to play DotA
League uh we're playing we're training
League uh we're playing we're training
an AI to to play a miniature version
an AI to to play a miniature version
this is not the the uh the fine grain
this is not the the uh the fine grain
render this is a high level render uh so
render this is a high level render uh so
we have something that works more or
we have something that works more or
less like do it has five Heroes it's a
less like do it has five Heroes it's a
mirror match for now it has creeps it
mirror match for now it has creeps it
has uh neutral creeps waves Towers all
has uh neutral creeps waves Towers all
the same Towers and stuff um so it's
the same Towers and stuff um so it's
very stripped down but it runs at over a
very stripped down but it runs at over a
million steps per second and we are
million steps per second and we are
training agents to play this very very
training agents to play this very very
fast that is what we're doing at the
fast that is what we're doing at the
moment this is currently training on uh
moment this is currently training on uh
two and a half hours worth of games
two and a half hours worth of games
every single second that we run
it and I'm currently debugging what is
it and I'm currently debugging what is
wrong with these agents and why they're
wrong with these agents and why they're
not learning very well this is the first
not learning very well this is the first
we've actually tried to train anything
we've actually tried to train anything
on this so of course it takes a little
on this so of course it takes a little
bit of time to figure out um you know
bit of time to figure out um you know
how to get stuff working
properly entropy is very weird
properly entropy is very weird
here entrop is very very weird
here entrop is very very weird
here entropy tells you how uh how like
here entropy tells you how uh how like
how different the actions are that the
how different the actions are that the
policy is taking if entropy collapses
policy is taking if entropy collapses
collapses it basically means it's always
collapses it basically means it's always
pressing the same key
that does not seem to be the case
that does not seem to be the case
here though it's hard to tell because
here though it's hard to tell because
before it looked like that was what was
happening it's possible that it's
happening it's possible that it's
learning to just press its like qw and E
learning to just press its like qw and E
Keys randomly and its movement is
Keys randomly and its movement is
collapsed that would also lead to this
collapsed that would also lead to this
result
would a peace wise smooth reward
would a peace wise smooth reward
function peace wise smooth function be
function peace wise smooth function be
good for their distance to ancient
good for their distance to ancient
reward what specific peace wise smooth
reward what specific peace wise smooth
function
function
um the way that the reward is at the
um the way that the reward is at the
moment so first of all there there's
moment so first of all there there's
there's a difference between the reward
there's a difference between the reward
and the observation of the reward
and the observation of the reward
so what we're doing is we are setting
so what we're doing is we are setting
the reward to be a number that is at the
the reward to be a number that is at the
moment it is between negative .1 and
moment it is between negative .1 and
positive 1.1 and the asymmetry comes
positive 1.1 and the asymmetry comes
because you can get an XP reward of up
because you can get an XP reward of up
to one it can get an XP reward of 0 to 1
to one it can get an XP reward of 0 to 1
and then there's a movement reward of
and then there's a movement reward of
negative .1 or positive .1 uh the
negative .1 or positive .1 uh the
observation of the reward is not the
observation of the reward is not the
same the observation of the reward is
same the observation of the reward is
scaled and offset a little bit so it's
scaled and offset a little bit so it's
basically between like 0.1 and 0.9 or
basically between like 0.1 and 0.9 or
something like that uh just scaled and
something like that uh just scaled and
the reason for that is just because we
the reason for that is just because we
have to store this reward in one bite so
have to store this reward in one bite so
we do some scaling and some data munging
we do some scaling and some data munging
around to make that
around to make that
convenient uh that shouldn't even be
convenient uh that shouldn't even be
critical it really should be able to
critical it really should be able to
learn this without that
learn this without that
so yeah I don't think that that's going
so yeah I don't think that that's going
to be the problem either way it's nice
to be the problem either way it's nice
for them to be able to observe their
for them to be able to observe their
reward reasonably and it does help it
reward reasonably and it does help it
can help quite a bit but this is frankly
can help quite a bit but this is frankly
a very very easy task for them to learn
a very very easy task for them to learn
at least like just run down mid and
at least like just run down mid and
fight stuff should be a very easy task
fight stuff should be a very easy task
to learn
and it looks like that is
and it looks like that is
not uh not what is being reflected here
not uh not what is being reflected here
so why is the reward we've already
so why is the reward we've already
checked the observations and they seem
reasonable why is the reward this
reasonable why is the reward this
bad this should basically be free this
bad this should basically be free this
reward
there's got to be a reason why they're
there's got to be a reason why they're
learning this weird
Behavior got to be a
reason let's go look at the policy I bet
reason let's go look at the policy I bet
it's going to just be like collapsed
I haven't tried this with multi-
I haven't tried this with multi-
discreet
lately it could be an error with my
lately it could be an error with my
multi- discreete
processing because they're two
processing because they're two
components to the actions in this game
components to the actions in this game
but we have tests for
that though Poss possibly I haven't run
that though Poss possibly I haven't run
them very uh much
lately oops
nope let's pull the file down and look
nope let's pull the file down and look
at
okay
okay
so it basically just learned to do the
so it basically just learned to do the
exact same thing
exact same thing
right where when went over to
right where when went over to
here why does it do
this why does it do
this possibilities
you know what we should
you know what we should
do why don't we play Let's Make It Let's
do why don't we play Let's Make It Let's
us play the game and then let's see what
us play the game and then let's see what
rewards we get while playing the game
rewards we get while playing the game
that would be a fun way to figure this
that would be a fun way to figure this
out could it be en related not the
out could it be en related not the
rewards yes it could be very many many
rewards yes it could be very many many
things I think that the easiest way to
things I think that the easiest way to
like catch most possibilities is going
like catch most possibilities is going
to be for us to just like get this Back
to be for us to just like get this Back
Being Human playable where we control it
Being Human playable where we control it
and uh figure that out from there
probably the least painful way to go um
okay pretty much
this let's go grab the uh the logic for
this let's go grab the uh the logic for
converting actions
here because we have it discretized
here because we have it discretized
meaning that we play this with
meaning that we play this with
keyboard for the uh the current little
keyboard for the uh the current little
tests
key key or this is key
key key or this is key
W
W
and key
and key
D no key
D no key
a zero
the default action is
four key D is going to
be what's the next one ARL de
be what's the next one ARL de
is ah so obnoxious
this conversion is really
this conversion is really
obnoxious
obnoxious
um yeah I just need to I just need to do
um yeah I just need to I just need to do
like to quickly figure out without
like to quickly figure out without
messing up how to make sure I'm
messing up how to make sure I'm
controlling the character in the way
controlling the character in the way
that I think I am
that I think I am
so the way I have this indexed it's
so the way I have this indexed it's
easier to
easier to
actually split
actually split
on this is should be
on this is should be
a and then this is going to be S right
a and then this is going to be S right
so and then this is a s is no because
so and then this is a s is no because
it's flip so this is
it's flip so this is
W yeah that gives you
W yeah that gives you
zero and
zero and
W LF uh key down is
W LF uh key down is
s sh equals two
s sh equals two
right is it one or
two it's
two a d yeah there we go so
two a d yeah there we go so
W gives you
W gives you
three uh
three uh
right and then this is wrong this
right and then this is wrong this
is five
four and I missed some somehow didn't
I
I
press it's a is down D is down oh right
press it's a is down D is down oh right
because there is also um
key
key
W
and ah I got to fix the indices but it's
and ah I got to fix the indices but it's
it's something like this cuz Ashen it's
it's something like this cuz Ashen it's
1 two 3 four five 6 S 8 n so okay this
1 two 3 four five 6 S 8 n so okay this
is the right logic I just have to get
is the right logic I just have to get
the
the
keys um a and w
keys um a and w
[Music]
[Music]
is -1 -1 this is zero this is correct
is -1 -1 this is zero this is correct
and and S
and and S
is
is
uh one negative one I believe which is
uh one negative one I believe which is
two which is correct and
two which is correct and
then else is just key a which is just
then else is just key a which is just
left which is
zero1 which is what does this is correct
zero1 which is what does this is correct
and then key D D and W
and then key D D and W
is uh this is -11 I
believe which is actually action
believe which is actually action
six right
six right
-11 and then key s
-11 and then key s
is D and S is uh 1 one I believe which
is D and S is uh 1 one I believe which
is actually action
is actually action
8 and then key s or not key s just key D
8 and then key s or not key s just key D
is
01 which is
01 which is
s okay 6 8 and seven and then just key W
s okay 6 8 and seven and then just key W
is just -10 I
is just -10 I
believe which
believe which
[Music]
is where's negative 1
Z
Z
three which is correct I have it correct
three which is correct I have it correct
which leaves only five which is
which leaves only five which is
one uh one Z okay perfect so I have all
one uh one Z okay perfect so I have all
of
this
this
okay and uh we'll comment this for now
don't know what all this crap
don't know what all this crap
is
is
um we don't need any of this
cool see if this works
no is key down what
oh uh it's just apied wrong
well Target pit not
defined uh we don't care
all
all
right so we have
right so we have
uh we have this
uh we have this
thing it doesn't look like we're
thing it doesn't look like we're
controlling it
controlling it
though so let's figure out why we're not
though so let's figure out why we're not
controlling
it because this is commented as why
this
enough oh little awkward that the W key
enough oh little awkward that the W key
overlaps like that
overlaps like that
huh I didn't think about
huh I didn't think about
that I'm going have to replace this with
that I'm going have to replace this with
like up down left right I
think but for now
think but for now
um is still not getting
applied self. Human
Action it's getting returned here
it looks like it is getting
applied hold
on okay we get a break point
this looks like it is getting
this looks like it is getting
applied so oops we should be able to
applied so oops we should be able to
control this agent then
control this agent then
right we should be able to control this
right we should be able to control this
agent and if we can't then that's maybe
agent and if we can't then that's maybe
a clue as to why uh it's not working the
a clue as to why uh it's not working the
way we think it
should yeah I do not have control
should yeah I do not have control
here let's print out the Human
Action so we can see what it thinks that
Action so we can see what it thinks that
we should be doing
we should be doing
here 40
70 this doesn't trigger
70 this doesn't trigger
anything right key trigger
something so down key doesn't trigger
something so down key doesn't trigger
anything everything but down key does
anything everything but down key does
trigger something
though and it's just ignoring all of it
though and it's just ignoring all of it
apparently so that could be a a good
apparently so that could be a a good
indicator
right where's the
action velocity
let's print it out in the
scyon and then this should tell us
scyon and then this should tell us
whoops okay so then this will tell us um
whoops okay so then this will tell us um
why we don't
why we don't
have uh we don't have control over the
have uh we don't have control over the
player and it seems like the neural
player and it seems like the neural
network is still controlling it
and this very possibly will tell us what
and this very possibly will tell us what
we need to do to fix the training as
we need to do to fix the training as
well good debug
well good debug
technique always have your environment
technique always have your environment
be human playable okay
so I'm holding
so I'm holding
keys and nothing is getting transferred
keys and nothing is getting transferred
here
here
whatsoever if I hold W nothing
whatsoever if I hold W nothing
happens no none of these things are
happens no none of these things are
getting transferred I can see my action
getting transferred I can see my action
on the bottom here but it's not making
on the bottom here but it's not making
it into the
it into the
environment okay so that's that's
environment okay so that's that's
important then why is not why is this
important then why is not why is this
not making it into the environment and
not making it into the environment and
why do we have what looks like random
why do we have what looks like random
data here
data here
um it's coming from this actions
um it's coming from this actions
discreet what is actions discreet
discreet what is actions discreet
actions discreet is just self.
actions discreet is just self.
actions okay that's the action that
actions okay that's the action that
should be the actions that we pass to
should be the actions that we pass to
the environment is a buffer
so we're correctly passing the player ID
so we're correctly passing the player ID
the PID
the PID
here which is what we should need in
here which is what we should need in
order to get
order to get
uh these
uh these
velocities and then we
velocities and then we
pass yeah so I'm suspecting that the
pass yeah so I'm suspecting that the
action buffer is just completely not
action buffer is just completely not
getting passed correctly at this point
we're passing self- dead actions of
we're passing self- dead actions of
pointer through
pointer through
end which I would think would be the
end which I would think would be the
correct
correct
thing yeah that seems like the correct
thing yeah that seems like the correct
thing to
me so at this point I kind of just want
me so at this point I kind of just want
to print out the
to print out the
whole actions buffer
maybe let's print out the actions buffer
maybe let's print out the actions buffer
to see if we have any control over that
to see if we have any control over that
and maybe the the parsing is
and maybe the the parsing is
wrong we're getting very close here
all right so the first thing in the
all right so the first thing in the
actions is um the
actions is um the
movement so we're going to basically
movement so we're going to basically
just see if it looks random or if it
just see if it looks random or if it
looks like it's correctly
pared I suspect what's happening is it's
pared I suspect what's happening is it's
not learning anything properly because
not learning anything properly because
it's just having random environment data
it's just having random environment data
okay so this looks pretty random
yeah so the actions tensor is somehow
yeah so the actions tensor is somehow
getting screwed
up self dead actions is here set that to
up self dead actions is here set that to
be an n32
render mode is passed with self actions
render mode is passed with self actions
the first 10 actions which is
correct when we make the environments
correct when we make the environments
themselves we pass a slice of
themselves we pass a slice of
actions from a pointer through the end
actions from a pointer through the end
this pointer just goes over slices of 10
this pointer just goes over slices of 10
which is 10 agents per
which is 10 agents per
environment this should also be
correct right here we load actions in to
correct right here we load actions in to
the
game ah this is right this is self do
actions uh unfortunately I don't think
actions uh unfortunately I don't think
that that's going to tell us too much
that that's going to tell us too much
about why this thing is bugged but
maybe okay
we're still
not still not passing that's weird self
not still not passing that's weird self
dead
dead
actions
zero is equal to the Human
zero is equal to the Human
Action that was
bizarre does this thing get changed at
bizarre does this thing get changed at
runtime
runtime
why did you choose to explore mobas for
why did you choose to explore mobas for
reinforcement
reinforcement
learning well in addition to them being
learning well in addition to them being
a really complex and interesting game
a really complex and interesting game
genre for
genre for
RL there's historical
RL there's historical
precedent I consider this to be the best
precedent I consider this to be the best
result in all of reinforcement
result in all of reinforcement
learning um the only problem is that
learning um the only problem is that
absolutely nobody can build off of any
absolutely nobody can build off of any
of this work
of this work
because they have lots of Hardware not
because they have lots of Hardware not
even lots of hardware by today's
even lots of hardware by today's
standards but but like I don't have 256
standards but but like I don't have 256
gpus so I'm building a smaller scale but
gpus so I'm building a smaller scale but
ludicrously fast version of this so that
ludicrously fast version of this so that
we can actually match the training scale
we can actually match the training scale
if not the model size and like the
if not the model size and like the
actual game itself uh and we can do some
actual game itself uh and we can do some
really awesome research really really
fast we're actually going to be able to
fast we're actually going to be able to
match these batch size numbers as well
match these batch size numbers as well
which is really
which is really
cool we're going to be able to basically
cool we're going to be able to basically
match like most of their scale
parameters it's all open source as
well so researchers are going to be able
well so researchers are going to be able
to use this environment for years to
to use this environment for years to
come
what is wrong provided I can actually
what is wrong provided I can actually
get the thing working
get the thing working
properly that
is these buffers are not playing very
is these buffers are not playing very
nice with me at the
moment like clearly the data is not
moment like clearly the data is not
going to the
going to the
the C version of the environment
right the data should be mirrored from
right the data should be mirrored from
python to C
python to C
though it should be the same data so
though it should be the same data so
unless I accidentally did a copy
unless I accidentally did a copy
somewhere
somewhere
actions what is it actions
actions what is it actions
discreete oh maybe I'm using the wrong
discreete oh maybe I'm using the wrong
one
one
here actions discreet
no we're using actions discreet right
no we're using actions discreet right
actions
continuous that shouldn't be getting
continuous that shouldn't be getting
called
called
somehow actions continuous and actions
somehow actions continuous and actions
SC maybe we load them into the wrong
SC maybe we load them into the wrong
buffer or
something self. actions equals
actions actions
actions actions
is okay we have it as an INT at the
is okay we have it as an INT at the
moment now this all looks fine to me
moment now this all looks fine to me
this all looks totally
fine the heck else would it be
it's getting to be very
weird
so if I press seven
so if I press seven
here I don't see a seven in here and
here I don't see a seven in here and
this agent is still doing whatever the
this agent is still doing whatever the
hell it
wants
yeah I didn't just forget to rebuild it
yeah I didn't just forget to rebuild it
did
I no there's nothing I changed that
I no there's nothing I changed that
should have been
should have been
rebuilt self. actions wait
action is equal to self note
am I somehow like not
am I somehow like not
allocating or it
gets this was working
gets this was working
before this was working a couple of days
before this was working a couple of days
ago so
ago so
somehow but maybe I was using the
somehow but maybe I was using the
continuous version of it
continuous version of it
which was actions continuous
here actions discreet and actions
continuous this looks
right what else could could I have
right what else could could I have
messed
messed
up does self do actions get overwritten
up does self do actions get overwritten
or
something I don't think
something I don't think
so yeah no there's no actions getting
so yeah no there's no actions getting
overwritten here
I thought there was like a weird thing
I thought there was like a weird thing
that I'd done that um would have caused
that I'd done that um would have caused
the actions buffer to get
overwritten it's
n32 that's what it should
be action space should
be action space should
match action space does match
action should be be it should be given a
action should be be it should be given a
slice it is being given a
slice it is being given a
slice it's being given the same exact
slice it's being given the same exact
type of slice as rewards which we know
type of slice as rewards which we know
work pointer through
end what's the size of the actions
end what's the size of the actions
buffer numb agents which is
buffer numb agents which is
correct which would be the same as the
correct which would be the same as the
buffer
buffer
allocated which which it
is wait we have another clue here
is wait we have another clue here
right the additional clue is that new
right the additional clue is that new
data is getting loaded into actions
right the data is always changing here
right the data is always changing here
we're constantly getting new data into
we're constantly getting new data into
actions it's just that our data doesn't
actions it's just that our data doesn't
seem to be getting loaded into
seem to be getting loaded into
actions uh and the reason is that we're
actions uh and the reason is that we're
stupid this goes
here and the bug is just called being
stupid and now it works with the caveat
stupid and now it works with the caveat
that the keys seem to
that the keys seem to
be not all the keys seem to
be not all the keys seem to
work this
work this
works I can't go up I can go diagonally
up diagonals work all the diagonals
work the upkey goes
work the upkey goes
down the right key goes right the left
down the right key goes right the left
key so the up and down Keys the up key
key so the up and down Keys the up key
goes down and the down down key doesn't
goes down and the down down key doesn't
do anything let's go look at
that so the
that so the
upkey
Ws ah this is supposed to be
Ws ah this is supposed to be
W and this is supposed to be s
W and this is supposed to be s
now let's see if this
works
perfect do I
perfect do I
fight yeah you it auto
targets I mean you don't fight
targets I mean you don't fight
particularly
particularly
well but you fight
can I solo one minion
can I solo one minion
maybe oh yeah
look funny enough it doesn't aggro
look funny enough it doesn't aggro
because I'm just out of Agro
range and you will just stand here and
range and you will just stand here and
die
skills don't really do
skills don't really do
anything all right but that's like the
anything all right but that's like the
basic thing that we wanted so now what
basic thing that we wanted so now what
we're going to do
we're going to do
is let's get rid of those other
prints and all we want to know is we
prints and all we want to know is we
want to know the reward
I want to know how much reward we
I want to know how much reward we
get because this will tell us basically
get because this will tell us basically
like if the should be learning something
like if the should be learning something
interesting and they're just not
interesting and they're just not
learning it or if there's a
learning it or if there's a
bug if I go towards the ancient I should
bug if I go towards the ancient I should
be getting
reward Okay so zero
reward Okay so zero
reward
05 looks like I'm getting reward
right you can't stop me okay you get
right you can't stop me okay you get
lots of
lots of
reward and then if you hit a wall you
reward and then if you hit a wall you
get
nothing very reasonable reward
right how much do I get for killing this
guy I do not get a reward for killing
guy I do not get a reward for killing
this guy
really do neutral creeps not have XP set
does not appear that I get any reward
does not appear that I get any reward
for
kills why aren't we getting reward for
kills why aren't we getting reward for
kills
player. reward plus equal
reward let's debug
reward let's debug
here so that's one thing that's going to
here so that's one thing that's going to
hurt a bunch is not not having the XP
hurt a bunch is not not having the XP
reward though I don't think we should
reward though I don't think we should
strictly need the XP reward uh just to
strictly need the XP reward uh just to
get them to go mid and like fight
we going to get a reward here or
we going to get a reward here or
no ah xp35 reward
zero target. XP
onkill it's set for
onkill it's set for
Towers it's supposed to be set for
Towers it's supposed to be set for
neutrals and it's supposed to be set for
creeps oh into XP over
creeps oh into XP over
250 well I don't know why we have
that that would definitely cause this
that that would definitely cause this
right
let's see if we get the reward correctly
let's see if we get the reward correctly
now and if so I think we're going to be
now and if so I think we're going to be
ready to train some models on this and
ready to train some models on this and
uh if we know the controls
uh if we know the controls
work then it should just be a matter of
work then it should just be a matter of
time and like a little bit of
time and like a little bit of
experimentation to get uh something
experimentation to get uh something
training
properly okay we're hitting this
thing reward zero what the
hell well I still have an INT this is
hell well I still have an INT this is
supposed to be float
reward what data type do I have do I
reward what data type do I have do I
have reward as an INT no rewards a float
so this should be like a little bit over
so this should be like a little bit over
0.1
right so actually this is like probably
right so actually this is like probably
too
too
low let's do like over a 100 Maybe
though I don't know if this is going to
though I don't know if this is going to
let them kill enemy Heroes
let them kill enemy Heroes
because they don't get that big of a
because they don't get that big of a
reward for hero
kills okay they're
fighting perfect now we get a reward
fighting perfect now we get a reward
reward of
35 so I think that we can
35 so I think that we can
um we can rebuild this we can commit
um we can rebuild this we can commit
this
and let's see if uh we do any better
what did I forget to
what did I forget to
um what did I
um what did I
forget I forgot I print somewhere
right yep
all right we've got our
all right we've got our
model should be
model should be
training there we
go if we check in the UI here
the
heck there we
heck there we
go it's only given us one data
go it's only given us one data
point should have given us more by now
yeah okay here we go
yeah okay here we go
so reward goes from
so reward goes from
zero which it should start at
zero which it should start at
zero up to some very small
value we're not seeing anything
value we're not seeing anything
great well I left and now it's 800k no
great well I left and now it's 800k no
it's very it's kind of variable and I
it's very it's kind of variable and I
don't know
don't know
why I said we're going to have a million
why I said we're going to have a million
train SPS and we're going to have a
train SPS and we're going to have a
million train SPS it's just a matter of
million train SPS it's just a matter of
when I go do that level of
optimization for context I have this
optimization for context I have this
exact same neural network training on
exact same neural network training on
multiple other environments at a million
multiple other environments at a million
steps per second so it will be a million
steps per second so it will be a million
steps per
second okay so this does improve reward
second okay so this does improve reward
a bit
a bit
but they should really be learning this
but they should really be learning this
very
quickly this is not something that
quickly this is not something that
should be hard to learn
I mean it is going
up cool project what RL algorithm are
up cool project what RL algorithm are
you using it's
po po on a small confet with lstm
we're just trying to get some sanity uh
we're just trying to get some sanity uh
some sanity checks going here so that we
some sanity checks going here so that we
can get the agents actually learning to
can get the agents actually learning to
play the game pretty quick uh like at
play the game pretty quick uh like at
least learning some
Basics do you know what opening I use
Basics do you know what opening I use
for their DOTA bot PO with an lstm same
for their DOTA bot PO with an lstm same
thing they had a substantially fancier
thing they had a substantially fancier
architecture and a uh and obviously much
architecture and a uh and obviously much
larger
larger
networks their networks were oh let's
networks their networks were oh let's
see 10,000 times larger than
see 10,000 times larger than
mine that right
no 50,000 1,000 times larger than
no 50,000 1,000 times larger than
mine but
mine but
hey we're doing pretty good
130 million steps
130 million steps
trained I'm hoping we don't have to run
trained I'm hoping we don't have to run
a hyper pram sweep just to get it to do
a hyper pram sweep just to get it to do
the basic
the basic
thing that we might end up having
to DOTA is also a thousand times more
to DOTA is also a thousand times more
complex than your M as of now well
complex than your M as of now well
yeah I'm one guy I can't build all of
yeah I'm one guy I can't build all of
DOTA in a week but I mean it looks
DOTA in a week but I mean it looks
pretty nice like you've probably seen
pretty nice like you've probably seen
this on this GIF on Twitter like we've
this on this GIF on Twitter like we've
got Lanes we've got creeps we've got
got Lanes we've got creeps we've got
five different hero classes we've got
five different hero classes we've got
neutral camps we've got all the towers
neutral camps we've got all the towers
we've got the same we've got the whole
we've got the same we've got the whole
map
map
right so we have some
right so we have some
stuff it's not
stuff it's not
terrible it runs real fast as well
thank you this is all open source
thank you this is all open source
too so you're free to play with
it it's in oh wow we hit 675
it it's in oh wow we hit 675
awesome
awesome
um it's just in this any config Branch
um it's just in this any config Branch
for now it'll get merged to Dev pretty
for now it'll get merged to Dev pretty
soon
soon
um go ahead start the repo on your way
um go ahead start the repo on your way
and if you wouldn't mind helps us out a
and if you wouldn't mind helps us out a
lot um but yeah it's in here it's in
lot um but yeah it's in here it's in
puffer Li environments
ocean and I'm constantly pushing uh
ocean and I'm constantly pushing uh
little updates and changes to
it what are these agents
it what are these agents
doing they really should be
doing they really should be
learning they should be learning very
learning they should be learning very
quickly to go towards the
quickly to go towards the
the
ancient the fact that they are not
ancient the fact that they are not
leveling means that they're not learning
leveling means that they're not learning
that as
well is it this entropy crashing
is it the way I'm doing multi-
discreet hold
on uh
just run a little sanity and on the
just run a little sanity and on the
other
machine oh
um well
[ __ ] okay I think I broke something in
[ __ ] okay I think I broke something in
Dev that's good to
know I remember somebody asked me for a
know I remember somebody asked me for a
patch and I probably broke something in
patch and I probably broke something in
the
process we're going to push a little
process we're going to push a little
thing here
I'll fix this
separately I bet this will just make it
separately I bet this will just make it
work
by the way what is the purpose of the
by the way what is the purpose of the
lstm in this
lstm in this
setup I'm an ARL Noob never worked po
setup I'm an ARL Noob never worked po
Ina the purpose of the lstm is so that
Ina the purpose of the lstm is so that
the agents can have
the agents can have
memory they can't remember anything from
memory they can't remember anything from
previous frames if you don't give them
previous frames if you don't give them
an lstm
that is the purpose
what
why did that rebuild instantly
I'm
confused and does this improve their pay
confused and does this improve their pay
capabilities well yeah think about it
capabilities well yeah think about it
like if you do not remember anything
like if you do not remember anything
about what's happened in the past in the
about what's happened in the past in the
game you're fundamentally limited on
game you're fundamentally limited on
what you can
do oh there we go why the heck did that
do oh there we go why the heck did that
not
but where is this thing that this is
but where is this thing that this is
being weird
is
this oh I'm on the wrong machine okay
this oh I'm on the wrong machine okay
that's fine no big
deal uh too many indices
deal uh too many indices
for right this is just
for right this is just
compilation right because the past is
compilation right because the past is
not encoded into the current
not encoded into the current
state but we still want the marov the
state but we still want the marov the
marov properties is irrelevant you can
marov properties is irrelevant you can
ignore all of the math here it's it's
ignore all of the math here it's it's
very simple it's just if you don't give
very simple it's just if you don't give
it any information about the past then
it any information about the past then
and it's making its decisions based only
and it's making its decisions based only
off what you give it then it has no
off what you give it then it has no
memory that's all there is to it there's
memory that's all there is to it there's
literally no
literally no
like the markof property is irrelevant
here
for let's just train
this couldn't we embed the past
this couldn't we embed the past
decisions into our state
decisions into our state
space how would you do
space how would you do
that the only other way you could do
that the only other way you could do
that is like you'd have to concatenate
that is like you'd have to concatenate
or
something there's not a good way of
something there's not a good way of
doing that
so this decode needs to just go like
so this decode needs to just go like
this
I see I think yeah like that's what the
I see I think yeah like that's what the
lstm is for right it's for learning the
embedding because otherwise you're just
embedding because otherwise you're just
stacking frames and then you only get
stacking frames and then you only get
like n frames worth of memory so that's
like n frames worth of memory so that's
like yeah you're paying a lot for you're
like yeah you're paying a lot for you're
paying a lot and you're blowing up the
paying a lot and you're blowing up the
complexity of your forward pass it's way
complexity of your forward pass it's way
better to just use a state based model
isn't that what the point is the
isn't that what the point is the
lstm yeah exactly that is exactly the
lstm yeah exactly that is exactly the
point of the lstm
that doesn't mean the lstm is perfect at
that doesn't mean the lstm is perfect at
it though lstms are not amazing
models actions do not match this date
models actions do not match this date
space
jeez why
not I forget to update something
discreet of
nine should match the action space right
I'm what I'm doing now is I'm just
I'm what I'm doing now is I'm just
hacking this to be
hacking this to be
discreet here it is I'm hacking this to
discreet here it is I'm hacking this to
be discreet because I think there's a
be discreet because I think there's a
puffer lib Dev Branch bug with multi-
puffer lib Dev Branch bug with multi-
discreet that I probably introduced
discreet that I probably introduced
while trying to fix something for
while trying to fix something for
somebody
else this isn't even the dev Branch to
else this isn't even the dev Branch to
be fair so this isn't even bad this is
be fair so this isn't even bad this is
like some random hack branch
there we go
cool I think um based on this uh it
cool I think um based on this uh it
definitely Deb boosts your post when you
definitely Deb boosts your post when you
add a link even in a reply cuz my posts
add a link even in a reply cuz my posts
are now doing twice as well as
are now doing twice as well as
before so that's unfortunate I got to
before so that's unfortunate I got to
put puffer AI puffer lib like this
put puffer AI puffer lib like this
but the more you know that'll help now
but the more you know that'll help now
with
with
this getting this project out
there so here's a fun thing these models
there so here's a fun thing these models
are
are
like these models here that we're
like these models here that we're
looking at nowadays are like hundreds of
looking at nowadays are like hundreds of
billions we have now 400 billion
billions we have now 400 billion
parameter models
parameter models
right um the models that we're training
right um the models that we're training
here are like hundreds of thousands
here are like hundreds of thousands
whoops hundreds of thousands to low uh
whoops hundreds of thousands to low uh
to like millions of parameters but they
to like millions of parameters but they
actually use like the same amount of
actually use like the same amount of
they actually use the same or more data
they actually use the same or more data
than these LS these uh language models
than these LS these uh language models
use we can consume like a gigabyte of
use we can consume like a gigabyte of
data per second with these models
data per second with these models
so we're like training on the amount of
so we're like training on the amount of
data that the whole language model is
data that the whole language model is
trained on we do that on one GPU in like
trained on we do that on one GPU in like
a day or
a day or
two that's pretty
cool that's why RL is
awesome well this is now bizarre
reward should
reward should
be this should definitely be good
now unless I broke something in like a
now unless I broke something in like a
recent refactor this should be good
yeah now this is
fine solves the test
fine solves the test
m test m still
works it'd be pretty hard for me to
works it'd be pretty hard for me to
break the whole Library without noticing
it but why would reward go to zero like
it but why would reward go to zero like
this
I think I realized where confusion you
I think I realized where confusion you
don't feed every frame into the
don't feed every frame into the
convenant and then into po you feed X
convenant and then into po you feed X
continuous frames as
one you freed you feed one frame into
one you freed you feed one frame into
the convet
and then that goes into the lstm with
and then that goes into the lstm with
the current state and goes forward you
the current state and goes forward you
don't stack frames you feed one frame in
don't stack frames you feed one frame in
to the
to the
convet and plus the state that has been
convet and plus the state that has been
uh from the last time step from the lstm
did start learning right
here yeah all good
here yeah all good
um I can't tell what the heck is
um I can't tell what the heck is
happening
here this should really not be hard to
here this should really not be hard to
learn so if this is hard to learn that
learn so if this is hard to learn that
means something Jank is
means something Jank is
happening essentially
well now the reward is going up but like
well now the reward is going up but like
why did
it why does it crash in the first place
I'm trying to think of like likely bug
I'm trying to think of like likely bug
sources
here we check the observations they look
here we check the observations they look
reasonable we check the rewards they
reasonable we check the rewards they
look reasonable
we made the action space single
discreet we made sure that it's
discreet we made sure that it's
playable and that the play returns
playable and that the play returns
reasonable Rewards
really expected to have learned
really expected to have learned
something
here but this is not the reward curve
here but this is not the reward curve
that we
that we
want I mean it should basically learn
want I mean it should basically learn
this instantly you would think
[Music]
I mean this really really should be easy
I can go mess with some hypers
I can go mess with some hypers
maybe so it is
maybe so it is
learning but like this is a ridiculous
learning but like this is a ridiculous
pace for something this simple
got this entropy coefficient which is
got this entropy coefficient which is
like reasonable
right Lambda and GMA which are
right Lambda and GMA which are
sufficiently low for something that's
sufficiently low for something that's
easy
a
a
BPT Horizon
yeah I don't freaking know
we're going to YOLO some
we're going to YOLO some
stuff this is usually
stuff this is usually
reasonable reasonable way to like get
reasonable reasonable way to like get
stuff to work
stuff to work
quickly
um we want 400
um we want 400
M's single
M's single
core no
multiprocessing like this should
multiprocessing like this should
basically learn
basically learn
instantly it should be that
easy okay already we see a very
easy okay already we see a very
different entropy value
some of these should have been reported
some of these should have been reported
to w b by
now there we
now there we
go 8 e minus
5 e
have you ever looked into applying
have you ever looked into applying
active
inference do you mean model based
inference do you mean model based
updates World model stuff
it's optimizing now but this is still
it's optimizing now but this is still
ridiculously
ridiculously
slow free energy
minimization you'd have to Define that
minimization you'd have to Define that
in the context of a problem like this
we have an RL we have curiosity based
we have an RL we have curiosity based
learn like we have curiosity stuff we've
learn like we have curiosity stuff we've
got model based
got model based
learning right World modeling all these
learning right World modeling all these
things or you talking about something
things or you talking about something
different
minimize its surprise okay yeah so we
minimize its surprise okay yeah so we
have that in RL we just use different we
have that in RL we just use different we
do use Li different
do use Li different
language surprise minimization we
language surprise minimization we
usually refer to that as uh curiosity
usually refer to that as uh curiosity
based learning or if we're explicitly
based learning or if we're explicitly
minimizing Surprise by making
minimizing Surprise by making
predictions about what we expect the
predictions about what we expect the
future to be then uh we refer to that as
future to be then uh we refer to that as
World modeling typically or model based
World modeling typically or model based
learning
learning
um there have been some results in this
area the research has been sketchy
area the research has been sketchy
though the research has been very
though the research has been very
sketchy I Bur a bunch of hours and
sketchy I Bur a bunch of hours and
wasted a lot of time trying to replicate
wasted a lot of time trying to replicate
a very high-profile paper in this area
a very high-profile paper in this area
recently and
recently and
um yeah
um yeah
it's it's something that's going to
it's it's something that's going to
probably be needed at some point but I
probably be needed at some point but I
don't think that we have a good
don't think that we have a good
understanding of the form in which we
understanding of the form in which we
want to incorporate
want to incorporate
that people are doing like planning and
that people are doing like planning and
hallucinated rollouts or hallucinated
hallucinated rollouts or hallucinated
space um that I don't think we need some
space um that I don't think we need some
form of prediction of the future
form of prediction of the future
probably
probably
needed generative modeling to go through
needed generative modeling to go through
the future outcomes and then pick one
the future outcomes and then pick one
that minimizes so that I don't think we
that minimizes so that I don't think we
necessarily need I think we do need to
necessarily need I think we do need to
model the future but I don't think we
model the future but I don't think we
need to like iteratively hallucinate
need to like iteratively hallucinate
through future outcomes the whole
through future outcomes the whole
strength of reinforcement learning comes
strength of reinforcement learning comes
from the fact that we don't need to like
from the fact that we don't need to like
imagine what date what it would be like
imagine what date what it would be like
if we had good data we have good data we
if we had good data we have good data we
have ground truth from the
have ground truth from the
environment so instead of spending
environment so instead of spending
time um trying to learn based off of you
time um trying to learn based off of you
know predictions of what we think the
know predictions of what we think the
future will be we can just simulate more
future will be we can just simulate more
data and learn directly off of it one of
data and learn directly off of it one of
the big strengths of RL we have infinite
data 0 0
data 0 0
Z
Z
two it's a weird
I mean we literally have infinite data
I mean we literally have infinite data
like look at
this
this
right we have unlimited ground truth
data and yeah you won't always have
data and yeah you won't always have
unlimited ground truth
unlimited ground truth
data will that lead us to
data will that lead us to
Consciousness akin to us I no idea
Consciousness akin to us I no idea
IDE friend of mine who's deep in model
IDE friend of mine who's deep in model
based RL told me that it's ridiculously
based RL told me that it's ridiculously
hard
hard
yeah because you're taking the thing
yeah because you're taking the thing
that RL does really well which is ingest
that RL does really well which is ingest
tons of data very quickly and you're
tons of data very quickly and you're
saying but what if we did RL without
saying but what if we did RL without
that that's why that's
hard like you're really throwing away
hard like you're really throwing away
the major major strength of RL
this thing is learning it's just not
this thing is learning it's just not
learning fast enough to be
satisfying like this is really stupid
satisfying like this is really stupid
that this takes this long this is not
hard should I do like a 10 mil sweep or
something oh come
something oh come
on is this just hypers like what the
heck this should be so unbelievably easy
how we're able to come to conclusions
how we're able to come to conclusions
with such limited well the thing is like
with such limited well the thing is like
you don't necessarily need to model the
you don't necessarily need to model the
future in order to train an agent that
future in order to train an agent that
can model the future like have you seen
can model the future like have you seen
the open ai5 models they're not
the open ai5 models they're not
explicitly trained to predict what the
explicitly trained to predict what the
opponents are doing but they very very
opponents are doing but they very very
clearly understand like they understand
clearly understand like they understand
what you're going to do before you do it
what you're going to do before you do it
so they've developed some sort of
so they've developed some sort of
internal World model with without you
internal World model with without you
training them to do
training them to do
that so it's completely possible for
that so it's completely possible for
that thing to just be an emergent
that thing to just be an emergent
property you don't necessarily have to
property you don't necessarily have to
train it directly
the really high learning
rate 9
2, let's
2, let's
do 160 Maybe
do 160 Maybe
and
and
then no that's really small isn't
it 320 * 4
is we're just doing some YOLO runs here
why does it immediately train to be
why does it immediately train to be
worse than the
default thing immediately trains to be
default thing immediately trains to be
worse than random
I mean there's got to be something wrong
right is there a possibility of you live
right is there a possibility of you live
streaming on a fun environment using
streaming on a fun environment using
hierarchical reinforcement hierarchical
hierarchical reinforcement hierarchical
reinforcement learning does not
work that's just not a thing that works
consistently well this is funny that
consistently well this is funny that
this is actually like doing stuff now
this is actually like doing stuff now
and getting zero
and getting zero
reward
um huh that's really that's really weird
um huh that's really that's really weird
I think that there's something very
I think that there's something very
wrong with with this reward we'll see
wrong with with this reward we'll see
so let me here let me just show you
so let me here let me just show you
something
something
right here's the best result in
right here's the best result in
RL everything is published about it you
RL everything is published about it you
have 150 million parameter model you
have 150 million parameter model you
have an amount of Hardware that's not
have an amount of Hardware that's not
even that big by modern standards with
even that big by modern standards with
uh with language modeling you have a on
uh with language modeling you have a on
layer lstm no hierarchy no algorithmic
layer lstm no hierarchy no algorithmic
Shenanigans very very simple and just
Shenanigans very very simple and just
good engineering in this solves DOTA
good engineering in this solves DOTA
DOTA is a ludicrously complicated game
DOTA is a ludicrously complicated game
environment like I don't think if you
environment like I don't think if you
were describe DOTA to your average adult
were describe DOTA to your average adult
I don't think that they would believe
I don't think that they would believe
you that people have a hobby that is
you that people have a hobby that is
this complicated that like average
this complicated that like average
people have a hobby that is this
people have a hobby that is this
complicated um it is insanely intricate
complicated um it is insanely intricate
this thing is super human it beats the
this thing is super human it beats the
top teams it like stomped basically
top teams it like stomped basically
everybody in pubs
everybody in pubs
it was just
it was just
Godly that's what RL can do without any
Godly that's what RL can do without any
algorithmic
algorithmic
shenanigans that is what RL can
do it is not one character it is a
do it is not one character it is a
pretty sizable pool of
pretty sizable pool of
characters is not one
characters is not one
character they had they didn't do the
character they had they didn't do the
whole pool of characters cuz that's a
whole pool of characters cuz that's a
lot of compute extra cuz you have to
lot of compute extra cuz you have to
train all the characters but they had
train all the characters but they had
like a sizable
pool they originally did a 1V one mid
pool they originally did a 1V one mid
and then they did the full game 5 B5 and
and then they did the full game 5 B5 and
they stomped OG
but is it one model that can do multiple
characters yes it is it's one model that
characters yes it is it's one model that
independently controls each
independently controls each
character so it's one model you run
character so it's one model you run
different copies of the model and it
different copies of the model and it
controls each of the characters on the
team really really really really
team really really really really
freaking
good okay now we're getting
somewhere now we're getting
somewhere this is impressive I'll have
somewhere this is impressive I'll have
to look more it's I mean this paper is I
to look more it's I mean this paper is I
don't know why nobody like I mean I
don't know why nobody like I mean I
guess it is very highly cited but I
guess it is very highly cited but I
swear nobody's read the full paper it's
swear nobody's read the full paper it's
so
good I mean basically you like let me
good I mean basically you like let me
put it this way like if you were to just
put it this way like if you were to just
read this one paper and then go from
read this one paper and then go from
there versus like read the whole rest of
there versus like read the whole rest of
the field for the most part like the
the field for the most part like the
vast majority of Academia minus this
vast majority of Academia minus this
paper you'll learn more from reading
paper you'll learn more from reading
this one
paper they saw everything
paper they saw everything
they saw
everything and then Academia spent the
everything and then Academia spent the
last several years like fiddling around
last several years like fiddling around
with
algorithms and none of them have done
algorithms and none of them have done
any better than
any better than
po not consistently at least maybe sa
po not consistently at least maybe sa
for robotics that's about
for robotics that's about
it and I think that the reasons for that
it and I think that the reasons for that
are even kind of
sketch okay so this is now interesting
sketch okay so this is now interesting
because this reward
because this reward
here
is this reward is low like zero but
is this reward is low like zero but
they're
leveling how are they
leveling are we
back we're back right
back we're back right
stream just
stream just
blipped hopefully it doesn't go
blipped hopefully it doesn't go
down
down
um so the model used local state data
um so the model used local state data
which is the same as the data that you
which is the same as the data that you
would use to render the the map view
would use to render the the map view
that humans are seeing so it used
that humans are seeing so it used
equivalent data to what humans have
equivalent data to what humans have
access
access
to and the reason for that is if you
to and the reason for that is if you
render the game you're 100 times slower
render the game you're 100 times slower
so you need a million cores instead of a
so you need a million cores instead of a
100,000 it's a pure it's a pure uh infr
100,000 it's a pure it's a pure uh infr
limit a of the of
limit a of the of
rendering
rendering
so
yeah how are these things leveling and
yeah how are these things leveling and
not getting
reward CU they're doing
reward CU they're doing
something but they're not getting reward
yeah they've got a level
25 okay
we're doing some very small tests
here we're going to just log some
here we're going to just log some
position
information e
okay so we're just going to log their X
okay so we're just going to log their X
and
y's I want to see if they're like
y's I want to see if they're like
actually moving towards the center or if
actually moving towards the center or if
like some janky weird thing is just
happening this seems to be logging
happening this seems to be logging
successfully so I'm going to go use the
successfully so I'm going to go use the
restroom and I'll be right
back
for
e
e
e e
how are we
how are we
doing are our agents being
stupid let's
stupid let's
see this is going to tell something
see this is going to tell something
either way
radian
X
X
is
is
30 radiant
30 radiant
y oh they're going to the center look
y oh they're going to the center look
they're going to the
they're going to the
center you see
center you see
that dire
that dire
y dire
X they're not doing much yet
what did I miss here uh I remember
what did I miss here uh I remember
watching the games of open ai5 Live I
watching the games of open ai5 Live I
was in the audience as
was in the audience as
well for their finals not for TI agents
well for their finals not for TI agents
were mechanically subpar compared to the
were mechanically subpar compared to the
pros but they yeah they gimped them
pros but they yeah they gimped them
mechanically so that they wouldn't be
mechanically so that they wouldn't be
unfair but their objective taking was
unfair but their objective taking was
really good so they were really
really good so they were really
optimizing for long-term rewards I mean
optimizing for long-term rewards I mean
that's what should tell you that it's
that's what should tell you that it's
good right I mean yeah if they just like
good right I mean yeah if they just like
perfect mechanic stomped the pros that
perfect mechanic stomped the pros that
would be one thing but they
didn't what are useful metrics to plot
didn't what are useful metrics to plot
when implementing
when implementing
RL average reward per batch
um it depends on the
um it depends on the
environment as you can see I'm adding
environment as you can see I'm adding
lots of environment specific stuff here
lots of environment specific stuff here
so like I just I started saying okay
so like I just I started saying okay
let's see what level they're getting
let's see what level they're getting
right like okay they're leveling up but
right like okay they're leveling up but
like I didn't think that they were going
like I didn't think that they were going
to the center of the map so let's start
to the center of the map so let's start
logging their X and Y okay it looks like
logging their X and Y okay it looks like
they're starting to learn to go to the
they're starting to learn to go to the
center of the map that's interesting
center of the map that's interesting
right but they're not getting any reward
right but they're not getting any reward
for doing it so something is wonky there
for doing it so something is wonky there
so now this thing is going to be done in
so now this thing is going to be done in
22 seconds and we're going to look at
22 seconds and we're going to look at
this model and see what it
this model and see what it
does and we're going to see if it
does and we're going to see if it
matches up with what this is telling us
how we doing with
how we doing with
puffer 675 still
nice this is solid
progress this is huge for reinforcement
progress this is huge for reinforcement
learning as
learning as
well for something this new I'm very
well for something this new I'm very
very happy
how do you get live stats in the
how do you get live stats in the
terminal well I built a little thing it
terminal well I built a little thing it
uses Rich for layout and I built that I
uses Rich for layout and I built that I
built that as a dashboard so that people
built that as a dashboard so that people
can have nice RL stats in the
terminal free and open source all free
terminal free and open source all free
and all open
source glad you like
source glad you like
it e
your training model is probably better
your training model is probably better
than the teammates I've had in my last
than the teammates I've had in my last
few
few
games man I played league for a bit I
games man I played league for a bit I
actually I think I like DOTA more
actually I think I like DOTA more
um but
like I always think like oh man I should
like I always think like oh man I should
play that again it's going to be fun and
play that again it's going to be fun and
it's never fun it's like the coolest
it's never fun it's like the coolest
game ever but it's just such rage bait I
game ever but it's just such rage bait I
swear
such a cool game
though it's probably more fun if you're
though it's probably more fun if you're
actually good at it
dota's vastly
Superior I really like
Superior I really like
DOTA I mean I really liked I liked
DOTA I mean I really liked I liked
pretty much everything about DOTA I
pretty much everything about DOTA I
didn't play very much of it the Q times
didn't play very much of it the Q times
were God awful as a new player was the
were God awful as a new player was the
only thing um
yeah I tried playing league and it's
yeah I tried playing league and it's
like it's good
like it's good
but nothing felt as good as playing Lena
but nothing felt as good as playing Lena
on on DOTA that was so freaking
good I don't really have time to play a
good I don't really have time to play a
bunch of games these days I'm kind of
bunch of games these days I'm kind of
just building this all the time but um
just building this all the time but um
my main Jam was uh always
MMOs getting people into do it is almost
MMOs getting people into do it is almost
impossible it took me I spent 40 hours
impossible it took me I spent 40 hours
just like pouring over everything I
just like pouring over everything I
could find about the game the week that
could find about the game the week that
open ai5 launched just trying to
open ai5 launched just trying to
understand this game and I'd never
understand this game and I'd never
played a MOA and I was like holy hell
played a MOA and I was like holy hell
this is complicated like wow people play
this is complicated like wow people play
this as a hobby like are there Geniuses
this as a hobby like are there Geniuses
like yeah you get a lot of respect for
like yeah you get a lot of respect for
it as soon as you actually engage with
it as soon as you actually engage with
it
right RuneScape in Guild Wars
right RuneScape in Guild Wars
1 I have played like three or 4 thousand
1 I have played like three or 4 thousand
hours of old
school I had to stop at the end of last
school I had to stop at the end of last
summer or the middle of last summer so I
summer or the middle of last summer so I
could actually do
work MMOs are just straight heroin for
work MMOs are just straight heroin for
me I swear
I always liked Starcraft but was never
I always liked Starcraft but was never
good at it I don't think I could do
good at it I don't think I could do
Starcraft either maybe I could I never
Starcraft either maybe I could I never
had like the uh the reaction time for uh
had like the uh the reaction time for uh
FPS or anything like that but I had
FPS or anything like that but I had
decent enough muscle memory that I could
decent enough muscle memory that I could
do high APM for MMOs that was fun
yo tree uh does uh Inferno Cape Master's
yo tree uh does uh Inferno Cape Master's
Helm dusted omelette mean anything do
Helm dusted omelette mean anything do
you
[Laughter]
[Laughter]
yeah you
yeah you
know yeah you
know MOA mod Val
know MOA mod Val
render RB
2100 total I think that's where about
2100 total I think that's where about
where my account was was around like
where my account was was around like
2K I didn't skill I just did sweaty
pvm well this doesn't make any sense
right why these things just do this why
right why these things just do this why
why do they why are they like
why do they why are they like
this are they just like clipping the
this are they just like clipping the
Minions on their way past is that what
Minions on their way past is that what
they're doing they're just like clipping
they're doing they're just like clipping
these Minions on their way
these Minions on their way
by but then what are these guys
doing need some RL agents for RuneScape
next don't break my favorite game
why are they like this they came these
why are they like this they came these
two came all the way down
two came all the way down
here are they actually killing anything
here are they actually killing anything
or are they just like clipping I imagine
or are they just like clipping I imagine
they're not killing anything doing
they're not killing anything doing
this right
well we have one additional
well we have one additional
um we do have one additional Technique
um we do have one additional Technique
we can
use y all want the hidden debug
technique
okay last hit count of each agent I have
okay last hit count of each agent I have
access to the XP which is about the same
access to the XP which is about the same
they don't get XP for being an AOE they
they don't get XP for being an AOE they
only get XP for last hits
all right ready for the hidden debug
technique I know if I want to share it
technique I know if I want to share it
it's very powerful
we have S
we have S
Ting Port interval
we do it in
we do it in
snake we didn't do it in
snake one second let me go look one
snake one second let me go look one
thing
up the hidden debug Tech
techque e
where is this thing
let's see if this does it for
us so the goal
us so the goal
here the goal here is to render the
here the goal here is to render the
screen and log it so we can see if over
screen and log it so we can see if over
the course of training there's some
the course of training there's some
weird stupid degenerate situation that
weird stupid degenerate situation that
this thing gets itself into that's
this thing gets itself into that's
crashing
crashing
it that's the most likely thing
that's what that's the debug technique
that's what that's the debug technique
is to just log the damn
render
render
4176 hours on my
4176 hours on my
main H yeah
main H yeah
ah
my favorite content in that game was
my favorite content in that game was
solo challenge mode uh
Chambers is
good no Skilling
where are the
where are the
renders should be some renders by
renders should be some renders by
now unless there's like a specific thing
now unless there's like a specific thing
that I'm
forgetting yeah underscore map is Ty
forgetting yeah underscore map is Ty
prefix it
right yeah info Z mooba map
right yeah info Z mooba map
here I have to log it more
often should have been logged
overview environment
overview environment
losses I don't see
losses I don't see
it I think we got to
it I think we got to
uh we got to increase the frequency of
uh we got to increase the frequency of
this never like
this never like
Tob well the thing is that's because Tob
Tob well the thing is that's because Tob
is designed to make you absolutely
is designed to make you absolutely
despise all of your teammates
so if you like hating your teammates
so if you like hating your teammates
it's really
fun same problem as
League that said I did hard mode on day
League that said I did hard mode on day
of release
it was so
sweaty e
why the hell is this thing not logging
why the hell is this thing not logging
screenshots
screenshots
already should be logging
screenshots is it is it getting cut out
screenshots is it is it getting cut out
somehow it shouldn't be
got infos of MOBA map
right where is the stupid thing
definitely should be rendering by
now oh there we
now oh there we
go so you have MOBA map in Wan
go so you have MOBA map in Wan
B and according to this the dire has
B and according to this the dire has
some
some
levels what's it say is happening
according to this
um here it looks
like
like
five so they're sitting
there how are they getting any XP and
there how are they getting any XP and
stuff if they're sitting there
they look like they're stationary
right how the heck is the dire level
right how the heck is the dire level
mean increasing if they're sitting
there radiant Y is constant this entire
there radiant Y is constant this entire
time rating X is constant this entire
time rating X is constant this entire
time but somehow the level mean goes
up that doesn't make sense right
what gives XP killing stuff getting the
what gives XP killing stuff getting the
last hit on stuff
we have any more
we have any more
maps
no they're just sitting
no they're just sitting
there towers are it looks like the
there towers are it looks like the
minions have
minions have
taken some tier ones
taken some tier ones
maybe minions took some tier
maybe minions took some tier
ones that shouldn't change anything
ones that shouldn't change anything
though right
don't says that the dire y
changed what's it claiming the entropy
changed what's it claiming the entropy
as of this model it's claiming this
as of this model it's claiming this
model actually has some entropy which is
model actually has some entropy which is
just
just
[ __ ] there's no way this model has
[ __ ] there's no way this model has
any entropy
right there's just no
right there's just no
way it's it's completely
way it's it's completely
crashed so I mean something's totally at
crashed so I mean something's totally at
odds here it's saying that this model is
odds here it's saying that this model is
doing stuff it's saying that it's
doing stuff it's saying that it's
leveling up but I'm looking at the model
leveling up but I'm looking at the model
on the map and it's not doing
anything why are we not getting more
anything why are we not getting more
maps
logged radiant level mean shoots way up
Dyer has level 25
somehow doing
what maybe the metrics are broken Maybe
maybe but I need to understand what's
maybe but I need to understand what's
Happening Here I really wish this would
Happening Here I really wish this would
log more frames this is [ __ ] this
log more frames this is [ __ ] this
should be
logging I'm just going to put this in
logging I'm just going to put this in
um I'm just going to put this here
so we're going to just rerun
so we're going to just rerun
this we're going to rerun
this we're going to rerun
this and we're going to at the same time
this and we're going to at the same time
we're going to debug locally and try to
we're going to debug locally and try to
figure out what the hell's happening is
figure out what the hell's happening is
makes absolutely no
makes absolutely no
sense so
we need to go look at Logics as well
let's go check
entropy e
where the
problems this looks like high entropy
problems this looks like high entropy
data
we're going to look at this properly
okay oops no not like this
there pulling out all the stops with the
there pulling out all the stops with the
debugging
debugging
here okay here's our
map got reasonable
map got reasonable
Logics High entropy doesn't mean more
Logics High entropy doesn't mean more
move if it's stuck right well if it's
move if it's stuck right well if it's
stuck then it should be spamming the
stuck then it should be spamming the
same button over and over so it
same button over and over so it
shouldn't be able to have high
entropy if it's stuck it shouldn't be
entropy if it's stuck it shouldn't be
able to have high
able to have high
entropy so let's
entropy so let's
see what these agents
do okay look at this right there they're
do okay look at this right there they're
getting themselves stuck right they
getting themselves stuck right they
haven't
moved doesn't look
possible we'll give it a few more time
possible we'll give it a few more time
steps for them to stabilize maybe it's
steps for them to stabilize maybe it's
funny how one of the agents actually
funny how one of the agents actually
escapes or two of them look like they
escapes or two of them look like they
escape
there are a couple dire agents that are
there are a couple dire agents that are
kind of doing
stuff that's funny they're doing stuff
stuff that's funny they're doing stuff
right not really they're now
right not really they're now
stuck no this one is oh they're fighting
stuck no this one is oh they're fighting
the neutral you see it
now they're going to get merked cuz they
now they're going to get merked cuz they
aggro three
aggro three
neutrals um but let's look at the logits
neutrals um but let's look at the logits
because it says they all have high
because it says they all have high
entropy which shouldn't be possible here
right they're very suspiciously similar
right they're very suspiciously similar
as
as
well this last one has quite different
well this last one has quite different
Logics probably these guys over here
so we've got log
probs do we not have normalized logits
log
prob where's action come
from
51 multinomial logits of probs
see a lot of fives and
see a lot of fives and
[Music]
[Music]
ones are they spamming two keys instead
ones are they spamming two keys instead
of one is that how it's
of one is that how it's
working five and one so 0o one two three
working five and one so 0o one two three
four five yeah Z one
four five yeah Z one
yeah I see an eight in
yeah I see an eight in
here and then zero is only for this guy
here and then zero is only for this guy
this is a weird
really doesn't make any
really doesn't make any
sense they would be doing
this should be an incredibly easy to
this should be an incredibly easy to
learn
learn
task they're not learning
it we're seeing metrics that indicate
it we're seeing metrics that indicate
that they're playing the game at some
that they're playing the game at some
points
goes into a corner where only four moves
goes into a corner where only four moves
are valid and the other four moves don't
are valid and the other four moves don't
work anymore May because it's stuck four
work anymore May because it's stuck four
valid moves it's giving High ENT yeah so
valid moves it's giving High ENT yeah so
that's what I think is happening
potentially but then the question is why
potentially but then the question is why
is it going into a
is it going into a
corner has easily available reward right
has incredibly easily available
has incredibly easily available
reward maybe this run will tell us
something here's our mobile
map so start our
training they're stuck
training they're stuck
there they're still stuck
there they're still stuck
there three are stuck there where' the
there three are stuck there where' the
fourth one
fourth one
go one of them must have
go one of them must have
moved I don't see
it it's still
stuck they all look pretty
stuck they all look pretty
stuck very weird
m
I'm trying to think what I'm missing
here here's one of the
here here's one of the
agents no wait this is a
neutral and this says what is what does
neutral and this says what is what does
it say it's
it say it's
getting it's actually says it's getting
getting it's actually says it's getting
some reward
some reward
now and it has
now and it has
levels so basically this thing says it's
levels so basically this thing says it's
winning and and I look at the map and it
winning and and I look at the map and it
looks like it's not winning
so
so
weird so incredibly
weird so incredibly
weird
weird
well got a few options left available to
well got a few options left available to
us
are you handling situation where an
are you handling situation where an
agent move would cause collision with
agent move would cause collision with
another agent
another agent
yes could that be impacting their valid
yes could that be impacting their valid
move set yes they can't move into each
other I mean we should even be able to
other I mean we should even be able to
here if I just set this to
human right
uh whoops we gotta get out of get rid of
uh whoops we gotta get out of get rid of
this break
point yeah so we'll get let's go play
point yeah so we'll get let's go play
the game with them let's go play our
the game with them let's go play our
game with our stupid teammates here
game with our stupid teammates here
they're really
they're really
stupid o setting an array element with a
stupid o setting an array element with a
sequence right we haven't tested since
sequence right we haven't tested since
this
thing GG
action okay
action okay
so
so
oops here's our
game can you run into a wall and see if
game can you run into a wall and see if
you're getting any reward yeah that's a
you're getting any reward yeah that's a
good
good
idea here let's do
idea here let's do
that so what we're going to do
is okay we're going to print the first
reward I did something like this
reward I did something like this
before okay so we get zero reward
before okay so we get zero reward
negative 05 there for the move away from
negative 05 there for the move away from
the ancient
right that's a
get can't log that
okay I'm like wondering if we're
okay I'm like wondering if we're
maximizing like it looks like they're
maximizing like it looks like they're
training to do the opposite thing but
training to do the opposite thing but
they're
they're
not so here ne0 if I keep running into a
not so here ne0 if I keep running into a
wall Zer if I go this way I get
0.105 literally all you got to do is run
0.105 literally all you got to do is run
down mid for reward
looks reasonable
right just run it down mid see I get all
right just run it down mid see I get all
the way there
hey puffers what's up why you all
hey puffers what's up why you all
chilling over
there I'm in your
there I'm in your
fountain what's
fountain what's
up right
and then if I suicide here I can get
and then if I suicide here I can get
more
reward pretty weird
right looks very
right looks very
simple the observations would have to be
simple the observations would have to be
like completely wrong but we checked the
like completely wrong but we checked the
observations and they looked good
no reward if you move again yeah there's
no reward if you move again yeah there's
no reward there what if you increas the
no reward there what if you increas the
reward for going
reward for going
mid I can do that
let's get rid of the XP
reward okay so we just remove the XP
reward okay so we just remove the XP
reward um and we want to increase the
reward um and we want to increase the
reward for going
mid one times distance yeah
tempted to go back to the larger scale
tempted to go back to the larger scale
version of
it I guess this is fine for now though
the hell
could it be due to the loss of entropy
could it be due to the loss of entropy
is so
high we can try that
high we can try that
next we have a pretty big entropy bonus
next we have a pretty big entropy bonus
on this
well this is broken because if it's
well this is broken because if it's
getting zero reward on an easy task that
getting zero reward on an easy task that
literally requires one step
literally requires one step
competence that's
broken um
I don't know 05 entropy coefficient is
I don't know 05 entropy coefficient is
the highest I've ever even seen used
how's it inst like instantly go to zero
how's it inst like instantly go to zero
reward like
reward like
this I like how the hell is this even
this I like how the hell is this even
it's like are they minimizing reward
me try one more
thing let me try one more
thing let me try one more
thing one other idea
one more thing and then I'll take more
one more thing and then I'll take more
suggestions this will be like a
suggestions this will be like a
two-minute
test it's going to be something stupid
test it's going to be something stupid
I'm sure
I want to make sure I'm not desyncing
I want to make sure I'm not desyncing
the reward values
[Music]
um
well ain't that
funny so these guys think they're
funny so these guys think they're
getting reward for whatever this dumb
getting reward for whatever this dumb
thing they're doing
thing they're doing
is
is well wait they're actually are they
is well wait they're actually are they
doing different stuff
doing different stuff
now really
now really
right give him a
second rewards going up according to to
this are they playing the
this are they playing the
game are they suddenly playing the game
game are they suddenly playing the game
though I didn't change anything I just
though I didn't change anything I just
changed the log parameter
where are
they according to this they think they
they according to this they think they
are doing
well I don't I really really doubt I
well I don't I really really doubt I
have it set to minimize instead of
have it set to minimize instead of
maximize
maximize
I usually don't screw up that
badly right now they only get get a
badly right now they only get get a
reward for walking towards the enemy and
reward for walking towards the enemy and
it looks like they're actually doing
stuff well radiant
stuff well radiant
is I didn't change anything so that's
weird yeah r actually doing
stuff how's it this
stuff how's it this
variable
variable
wait it should not be able to be this
variable unless they're actually winning
variable unless they're actually winning
games it's possible they winning
games games
one I don't know what's making them
one I don't know what's making them
suddenly do
suddenly do
this
like oh yeah they're winning
games but I didn't change anything just
games but I didn't change anything just
then all I changed was a log parameter
doesn't explain why dire is sitting
there well they're not sitting there now
there well they're not sitting there now
according to
this now dire's doing
stuff this says dire doing
stuff this says dire doing
stuff dire level mean is going um
very
very
bizarre we're going to have to look at
bizarre we're going to have to look at
we're going to have to watch
this so the reward spikes up at the end
this so the reward spikes up at the end
above 0.
above 0.
five
five
um assumedly ders learn something by the
um assumedly ders learn something by the
end of this
maybe very
Jank shoot it's already
Jank shoot it's already
7:30 I go get dinner
soon not as productive as the day as I'd
hoped
e e
why is this so
why is this so
weird lant
let's
let's
see does this
work uh
forgot to make that one
change turn
action we will follow the puffers into
battle
what um
why is it super
why is it super
fast
fast
oh that's
why follow the puffers into
battle these puff this puffer goes into
battle uh this puffer goes into battle
battle uh this puffer goes into battle
and gets
and gets
merked this puffer pulls a bunch of a
that puffer is not particularly
smart um that's real
smart um that's real
weird and then we got still we still
weird and then we got still we still
have puffer stuck
here I don't know what they think
here I don't know what they think
they're doing
apparently some of them do eventually go
apparently some of them do eventually go
through like
this and I guess they eventually get XP
this and I guess they eventually get XP
off of the
Creeps it's not exactly
Creeps it's not exactly
good where are dire
good where are dire
Puffs don't kill me I need to go see the
Puffs don't kill me I need to go see the
dire puffs
I have to go very fun to
I have to go very fun to
watch thanks for stopping
by oh look they went over to
here and these ones they go mid dire is
here and these ones they go mid dire is
now going
now going
mid log Pro bug you
mentioned I don't know what the heck
mentioned I don't know what the heck
like look these guys are just chilling
like look these guys are just chilling
over here
over here
now and then like this one is stuck over
here it's very
here it's very
weird I also want to go look at the uh
weird I also want to go look at the uh
the reward bug
because this doesn't look like it's
because this doesn't look like it's
getting good reward right
this model thinks it's doing very well
the self buff rewards
the self buff rewards
right when you reset you give it this
right when you reset you give it this
slice of self. buff.
slice of self. buff.
rewards pointer through
rewards pointer through
end and you reset
it
and sum rewards. append
I was already doing this
correctly so somehow this latest run
correctly so somehow this latest run
just changed
it entropy or something it shouldn't be
it entropy or something it shouldn't be
entropy they real
stupid I really don't know what the hell
stupid I really don't know what the hell
is wrong with
this and I need to go grab dinner in a
this and I need to go grab dinner in a
second let me just think if there's
second let me just think if there's
anything else I can run to think about
anything else I can run to think about
like what the heck this would be
this got to be a
bug
e
e
e e
last thing I'm trying before
dinner e
where are the
stats there we
go still no
stats that's kind of ridiculous
zero reward
low
entropy
bizarre okay well I'm gonna have to
bizarre okay well I'm gonna have to
continue on this tomorrow here it goes
continue on this tomorrow here it goes
um yeah zero reward so I'm going to have
um yeah zero reward so I'm going to have
to continue on this
to continue on this
tomorrow um this is very
tomorrow um this is very
odd I'm sure this is like some dumb data
odd I'm sure this is like some dumb data
problem or something because
problem or something because
uh this is like the easiest possible
uh this is like the easiest possible
setting to
setting to
solve um this is one of the things
solve um this is one of the things
really hard about RL is trying to debug
really hard about RL is trying to debug
like this much deeper stack than the
like this much deeper stack than the
rest of
rest of
AI
AI
so I'm going to be working on this for
so I'm going to be working on this for
as long as it takes to get this set
as long as it takes to get this set
up um but for now I'm kind of burnt so
up um but for now I'm kind of burnt so
I'm going to come back fresh
I'm going to come back fresh
tomorrow uh and I'm going to see if I
tomorrow uh and I'm going to see if I
can figure out how to like at least
can figure out how to like at least
partially automate this
partially automate this
process I might see if I can run a
process I might see if I can run a
little sweep or
little sweep or
something do I want to try to run a
something do I want to try to run a
sweep right
now not really I I think that there's
now not really I I think that there's
something like fundamentally freaking
something like fundamentally freaking
wrong with this
wrong with this
yeah so I'm going to make I'm going to
yeah so I'm going to make I'm going to
grab some like
grab some like
really really robust hyper prams and
really really robust hyper prams and
stuff from some other places um I'm
stuff from some other places um I'm
going to match like the grid environment
going to match like the grid environment
because this basically the same task and
because this basically the same task and
uh I'm going to go from there but we'll
uh I'm going to go from there but we'll
be back
tomorrow for uh folks that are new
tomorrow for uh folks that are new
around
here do feed the puffer the puffer
here do feed the puffer the puffer
consists uh has a you know a very nice
consists uh has a you know a very nice
diet of
diet of
stars so all this stuff is open source
stars so all this stuff is open source
if you want to help me out just start
if you want to help me out just start
the
the
repo I post all this stuff on X here
repo I post all this stuff on X here
I also when I go live it's notified here
I also when I go live it's notified here
as well U but it's all just RL and yeah
as well U but it's all just RL and yeah
you can follow this
you can follow this
here and it helps me out so thank you
here and it helps me out so thank you
very much and I will be back tomorrow
very much and I will be back tomorrow
bye

Kind: captions
Language: en
e
e
e
e
e
e
e
e
e
e e
thank you I forgot to unmute the mic
thank you I forgot to unmute the mic
thank you for
thank you for
that
okay that's what chat's for
okay that's what chat's for
we're good
we're good
now welcome
now welcome
back um it turns out that for whatever
back um it turns out that for whatever
reason uh my
reason uh my
ethernet is really unstable and the
ethernet is really unstable and the
Wi-Fi is stable I don't know how that's
Wi-Fi is stable I don't know how that's
possible because they're both plugged
possible because they're both plugged
they're plugged directly in but um
they're plugged directly in but um
yeah anyways uh to reiterate all the
yeah anyways uh to reiterate all the
stuff that uh that we're doing here so
stuff that uh that we're doing here so
uh I figured out that the rewards were
uh I figured out that the rewards were
getting really weirdly scaled in the
getting really weirdly scaled in the
observations you include the rewards in
observations you include the rewards in
the observations it's a big thing um and
the observations it's a big thing um and
that means that they weren't able to
that means that they weren't able to
actually see the rewards that they were
actually see the rewards that they were
getting but now that I fixed it it looks
getting but now that I fixed it it looks
like they're scaled in the opposite
like they're scaled in the opposite
direction they're over scaled so that
direction they're over scaled so that
leads me to believe that there's
leads me to believe that there's
something wrong with this reward
something wrong with this reward
function and that as soon as we fix it
function and that as soon as we fix it
it should be able to solve this this is
it should be able to solve this this is
a very very easy task um not the whole
a very very easy task um not the whole
environment obviously but just go to the
environment obviously but just go to the
center is given a dense uh given a like
center is given a dense uh given a like
reward every time you get closer that's
reward every time you get closer that's
a very very easy task to solve so this
a very very easy task to solve so this
is the sanity check goal for today right
is the sanity check goal for today right
is we want to have these things just go
is we want to have these things just go
to the center and fight it out learn the
to the center and fight it out learn the
most basic strategy and then we'll
most basic strategy and then we'll
figure it out from
figure it out from
there so how the heck are these things
there so how the heck are these things
getting rewards of
.99 self. rewards of PID
.99 self. rewards of PID
and compute
and compute
observations uh do we zero these in
reset do I zero these in
reset I don't zero the rewards in
reset I don't zero the rewards in
reset
reset
so I could do self. rewards of pit is
so I could do self. rewards of pit is
zero let's try this let's see if it's
zero let's try this let's see if it's
just a reset
bug okay
zero on the reset Time
step 996
step 996
still
so am I just scaling this wrong let me
so am I just scaling this wrong let me
think
think
um it's 0.1
it's 50 times the reward cast to an
it's 50 times the reward cast to an
unsigned
unsigned
sh and then so this is 0 to 255 and then
sh and then so this is 0 to 255 and then
it gets divided by
255 so yeah literally this has to be
255 so yeah literally this has to be
like this has to be 255 in order for
like this has to be 255 in order for
this to happen I would
this to happen I would
think
um you can cast like this
right this doesn't do something
right this doesn't do something
weird no it works perfectly fine for
weird no it works perfectly fine for
player X and player y it looks
player X and player y it looks
like right yeah those are reasonable
like right yeah those are reasonable
values for X and
values for X and
Y very reasonable values
so I've got to assume that the rewards
so I've got to assume that the rewards
are getting weirdly computed at some
are getting weirdly computed at some
point
right we're not muted now right yeah no
right we're not muted now right yeah no
we're good should be good at
least I'm assuming that people are not
least I'm assuming that people are not
watching a muted stream so I'm assuming
watching a muted stream so I'm assuming
assuming we're not muted anymore
um not muted good thank
you we do this difference right
you we do this difference right
here we have previous distance T we have
here we have previous distance T we have
previous distance we have curent
previous distance we have curent
distance this should be at most
distance this should be at most
one and then the reward
one and then the reward
is uh 0.1 times
is uh 0.1 times
this shouldn't be getting any
this shouldn't be getting any
XP so this should be the only reward
XP so this should be the only reward
component I think we should just print
component I think we should just print
this
out we should just print out this reward
out we should just print out this reward
right here right
PD
right let's see if this uh let's see if
right let's see if this uh let's see if
the reward is too large going into the
the reward is too large going into the
observations here
okay
so flat
so flat
features zero rewards right we hit
continue we see a bunch of
continue we see a bunch of
zeros we see a negative value
zeros we see a negative value
here ah
here ah
right negative values are going to roll
right negative values are going to roll
over
yeah negative values are going to roll
yeah negative values are going to roll
over that's
why so because we have a a small
why so because we have a a small
negative this turns into a big positive
negative this turns into a big positive
okay
so it does make sense to
so it does make sense to
have a negative reward here though
maybe I just want to center
maybe I just want to center
it what if I just do like
it what if I just do like
25 25 times rewards plus
25 25 times rewards plus
right I could do
this little awkward but let's see what
this little awkward but let's see what
happens if I do this
okay we should still have some negatives
okay we should still have some negatives
right and
then first
then first
perfect we all these are
perfect we all these are
all 0.5 plus or minus a little
all 0.5 plus or minus a little
bit
um the thing is these are really close
um the thing is these are really close
together
together
right these are really close
right these are really close
together I think what we're going to do
together I think what we're going to do
is we're going to modify this curve so
is we're going to modify this curve so
that we can scale more
that we can scale more
aggressively because those those values
aggressively because those those values
are going to be too close to
are going to be too close to
separate so
oops we're going to do int
reward and then we're going to
reward and then we're going to
do
do
reward equals int XP
reward equals int XP
/ so a th000 good I think a th000 is
/ so a th000 good I think a th000 is
still
still
well
well
250 250 is like a big objective
right that's a nice scale
right that's a nice scale
yeah and then what we're going to do is
yeah and then what we're going to do is
if
if
reward greater than
reward greater than
one we're going to clip
one we're going to clip
it reward
it reward
now we know that we're never getting
now we know that we're never getting
more than one from that
more than one from that
Source
right so now we can scale much more
aggressively and I can say that this is
aggressively and I can say that this is
going to be
I can't multiply by
I can't multiply by
128 I multiply by
128 I multiply by
100 hey Quinn
100 hey Quinn
welcome we're debugging rewards here so
welcome we're debugging rewards here so
that we can train some good
models okay so like the best reward you
models okay so like the best reward you
can get right now is 1.1
can get right now is 1.1
right
right
so if I do like one 1.1
so if I do like one 1.1
time was it plus 120 was it
time was it plus 120 was it
1.1 time 100 +
1.1 time 100 +
128 238 and then if I
do negative 0.1 is the worst you can
do negative 0.1 is the worst you can
do
118 I don't like that
okay this is
fine I'm just trying to find some decent
fine I'm just trying to find some decent
scaling here I think think I found it
scaling here I think think I found it
perfectly pretty much so we're going to
perfectly pretty much so we're going to
do uh this is going to be 200 time this
do uh this is going to be 200 time this
reward plus 32 and then we're going to
reward plus 32 and then we're going to
spot check
spot check
this um we're going to spot check this
this um we're going to spot check this
to make sure this doesn't overflow and
to make sure this doesn't overflow and
obviously I'm not going to leave all
obviously I'm not going to leave all
this stuff hardcoded like this because
this stuff hardcoded like this because
this is like ticking Time Bomb
this is like ticking Time Bomb
um but the goal is to just get some
um but the goal is to just get some
models to work and then make sure that
models to work and then make sure that
this is reasonable
okay so flat featur should be
okay so flat featur should be
empty or
empty or
not right because it's this is the zero
not right because it's this is the zero
because it's is
because it's is
fine this data should now be diverse
fine this data should now be diverse
enough yep so now we at least have some
enough yep so now we at least have some
reasonable level of Separation here I
reasonable level of Separation here I
mean it's not huge it leaves a lot of
mean it's not huge it leaves a lot of
room for high higher Ward spikes but I
room for high higher Ward spikes but I
think this is good
overall I think this is
overall I think this is
good so with this
fixed I think that's actually going to
fixed I think that's actually going to
make it easier to learn as well because
make it easier to learn as well because
they're going to get a big reward for uh
they're going to get a big reward for uh
for fighting creeps
uh rescale
rewards we'll see whether this is enough
rewards we'll see whether this is enough
on its own or not it very well might not
on its own or not it very well might not
be and we're going to set out more
be and we're going to set out more
reasonable 250 mil
reasonable 250 mil
steps these are are still decent default
steps these are are still decent default
hyper pram so we're going to keep
these and then we're going to get models
training track okay
we're now training we're training at
we're now training we're training at
over
over
500k it's not perfect but it's
500k it's not perfect but it's
decent and we'll see how the reward
decent and we'll see how the reward
curves happen I'm going to go use the
curves happen I'm going to go use the
restroom I'll be right back and then we
restroom I'll be right back and then we
will look at the reward curves for
this
e
e e
so we've got this thing training fast
so we've got this thing training fast
very nice of course it slows down as
very nice of course it slows down as
soon as I come back
what's this mooba
what's this mooba
say very weird reward goes up and then
say very weird reward goes up and then
goes back
goes back
down no
down no
levels so this is worse
levels so this is worse
um where's entropy
are we getting entropy
are we getting entropy
crashes doesn't look like we're getting
crashes doesn't look like we're getting
entropy
entropy
crashes I want to check the one from
crashes I want to check the one from
last time because the run from last time
last time because the run from last time
was clearly an entropy crash but I don't
was clearly an entropy crash but I don't
know if it registered as
one no not this one the one
one no not this one the one
before this is another common uh common
before this is another common uh common
debug thing
debug thing
here if entropy is crashing it means
here if entropy is crashing it means
that your policy is
that your policy is
collapsed so interestingly this thing
collapsed so interestingly this thing
which was basically the agents just like
which was basically the agents just like
not doing
not doing
anything uh this actually has reasonably
anything uh this actually has reasonably
High entropy so something is very wonky
High entropy so something is very wonky
here because that shouldn't be possible
here because that shouldn't be possible
do you think having a continuous reward
do you think having a continuous reward
of positive numbers equal to a reward
of positive numbers equal to a reward
centered on zero with positive basically
centered on zero with positive basically
does poo algorithm
does poo algorithm
differentiate it's all Rel it is all
differentiate it's all Rel it is all
relative um po includes a value function
relative um po includes a value function
Baseline so it's actually just going to
Baseline so it's actually just going to
be the difference in the reward and what
be the difference in the reward and what
the model has expected the reward should
be are you training an AI to play DotA
be are you training an AI to play DotA
League uh we're playing we're training
League uh we're playing we're training
an AI to to play a miniature version
an AI to to play a miniature version
this is not the the uh the fine grain
this is not the the uh the fine grain
render this is a high level render uh so
render this is a high level render uh so
we have something that works more or
we have something that works more or
less like do it has five Heroes it's a
less like do it has five Heroes it's a
mirror match for now it has creeps it
mirror match for now it has creeps it
has uh neutral creeps waves Towers all
has uh neutral creeps waves Towers all
the same Towers and stuff um so it's
the same Towers and stuff um so it's
very stripped down but it runs at over a
very stripped down but it runs at over a
million steps per second and we are
million steps per second and we are
training agents to play this very very
training agents to play this very very
fast that is what we're doing at the
fast that is what we're doing at the
moment this is currently training on uh
moment this is currently training on uh
two and a half hours worth of games
two and a half hours worth of games
every single second that we run
it and I'm currently debugging what is
it and I'm currently debugging what is
wrong with these agents and why they're
wrong with these agents and why they're
not learning very well this is the first
not learning very well this is the first
we've actually tried to train anything
we've actually tried to train anything
on this so of course it takes a little
on this so of course it takes a little
bit of time to figure out um you know
bit of time to figure out um you know
how to get stuff working
properly entropy is very weird
properly entropy is very weird
here entrop is very very weird
here entrop is very very weird
here entropy tells you how uh how like
here entropy tells you how uh how like
how different the actions are that the
how different the actions are that the
policy is taking if entropy collapses
policy is taking if entropy collapses
collapses it basically means it's always
collapses it basically means it's always
pressing the same key
that does not seem to be the case
that does not seem to be the case
here though it's hard to tell because
here though it's hard to tell because
before it looked like that was what was
happening it's possible that it's
happening it's possible that it's
learning to just press its like qw and E
learning to just press its like qw and E
Keys randomly and its movement is
Keys randomly and its movement is
collapsed that would also lead to this
collapsed that would also lead to this
result
would a peace wise smooth reward
would a peace wise smooth reward
function peace wise smooth function be
function peace wise smooth function be
good for their distance to ancient
good for their distance to ancient
reward what specific peace wise smooth
reward what specific peace wise smooth
function
function
um the way that the reward is at the
um the way that the reward is at the
moment so first of all there there's
moment so first of all there there's
there's a difference between the reward
there's a difference between the reward
and the observation of the reward
and the observation of the reward
so what we're doing is we are setting
so what we're doing is we are setting
the reward to be a number that is at the
the reward to be a number that is at the
moment it is between negative .1 and
moment it is between negative .1 and
positive 1.1 and the asymmetry comes
positive 1.1 and the asymmetry comes
because you can get an XP reward of up
because you can get an XP reward of up
to one it can get an XP reward of 0 to 1
to one it can get an XP reward of 0 to 1
and then there's a movement reward of
and then there's a movement reward of
negative .1 or positive .1 uh the
negative .1 or positive .1 uh the
observation of the reward is not the
observation of the reward is not the
same the observation of the reward is
same the observation of the reward is
scaled and offset a little bit so it's
scaled and offset a little bit so it's
basically between like 0.1 and 0.9 or
basically between like 0.1 and 0.9 or
something like that uh just scaled and
something like that uh just scaled and
the reason for that is just because we
the reason for that is just because we
have to store this reward in one bite so
have to store this reward in one bite so
we do some scaling and some data munging
we do some scaling and some data munging
around to make that
around to make that
convenient uh that shouldn't even be
convenient uh that shouldn't even be
critical it really should be able to
critical it really should be able to
learn this without that
learn this without that
so yeah I don't think that that's going
so yeah I don't think that that's going
to be the problem either way it's nice
to be the problem either way it's nice
for them to be able to observe their
for them to be able to observe their
reward reasonably and it does help it
reward reasonably and it does help it
can help quite a bit but this is frankly
can help quite a bit but this is frankly
a very very easy task for them to learn
a very very easy task for them to learn
at least like just run down mid and
at least like just run down mid and
fight stuff should be a very easy task
fight stuff should be a very easy task
to learn
and it looks like that is
and it looks like that is
not uh not what is being reflected here
not uh not what is being reflected here
so why is the reward we've already
so why is the reward we've already
checked the observations and they seem
reasonable why is the reward this
reasonable why is the reward this
bad this should basically be free this
bad this should basically be free this
reward
there's got to be a reason why they're
there's got to be a reason why they're
learning this weird
Behavior got to be a
reason let's go look at the policy I bet
reason let's go look at the policy I bet
it's going to just be like collapsed
I haven't tried this with multi-
I haven't tried this with multi-
discreet
lately it could be an error with my
lately it could be an error with my
multi- discreete
processing because they're two
processing because they're two
components to the actions in this game
components to the actions in this game
but we have tests for
that though Poss possibly I haven't run
that though Poss possibly I haven't run
them very uh much
lately oops
nope let's pull the file down and look
nope let's pull the file down and look
at
okay
okay
so it basically just learned to do the
so it basically just learned to do the
exact same thing
exact same thing
right where when went over to
right where when went over to
here why does it do
this why does it do
this possibilities
you know what we should
you know what we should
do why don't we play Let's Make It Let's
do why don't we play Let's Make It Let's
us play the game and then let's see what
us play the game and then let's see what
rewards we get while playing the game
rewards we get while playing the game
that would be a fun way to figure this
that would be a fun way to figure this
out could it be en related not the
out could it be en related not the
rewards yes it could be very many many
rewards yes it could be very many many
things I think that the easiest way to
things I think that the easiest way to
like catch most possibilities is going
like catch most possibilities is going
to be for us to just like get this Back
to be for us to just like get this Back
Being Human playable where we control it
Being Human playable where we control it
and uh figure that out from there
probably the least painful way to go um
okay pretty much
this let's go grab the uh the logic for
this let's go grab the uh the logic for
converting actions
here because we have it discretized
here because we have it discretized
meaning that we play this with
meaning that we play this with
keyboard for the uh the current little
keyboard for the uh the current little
tests
key key or this is key
key key or this is key
W
W
and key
and key
D no key
D no key
a zero
the default action is
four key D is going to
be what's the next one ARL de
be what's the next one ARL de
is ah so obnoxious
this conversion is really
this conversion is really
obnoxious
obnoxious
um yeah I just need to I just need to do
um yeah I just need to I just need to do
like to quickly figure out without
like to quickly figure out without
messing up how to make sure I'm
messing up how to make sure I'm
controlling the character in the way
controlling the character in the way
that I think I am
that I think I am
so the way I have this indexed it's
so the way I have this indexed it's
easier to
easier to
actually split
actually split
on this is should be
on this is should be
a and then this is going to be S right
a and then this is going to be S right
so and then this is a s is no because
so and then this is a s is no because
it's flip so this is
it's flip so this is
W yeah that gives you
W yeah that gives you
zero and
zero and
W LF uh key down is
W LF uh key down is
s sh equals two
s sh equals two
right is it one or
two it's
two a d yeah there we go so
two a d yeah there we go so
W gives you
W gives you
three uh
three uh
right and then this is wrong this
right and then this is wrong this
is five
four and I missed some somehow didn't
I
I
press it's a is down D is down oh right
press it's a is down D is down oh right
because there is also um
key
key
W
and ah I got to fix the indices but it's
and ah I got to fix the indices but it's
it's something like this cuz Ashen it's
it's something like this cuz Ashen it's
1 two 3 four five 6 S 8 n so okay this
1 two 3 four five 6 S 8 n so okay this
is the right logic I just have to get
is the right logic I just have to get
the
the
keys um a and w
keys um a and w
[Music]
[Music]
is -1 -1 this is zero this is correct
is -1 -1 this is zero this is correct
and and S
and and S
is
is
uh one negative one I believe which is
uh one negative one I believe which is
two which is correct and
two which is correct and
then else is just key a which is just
then else is just key a which is just
left which is
zero1 which is what does this is correct
zero1 which is what does this is correct
and then key D D and W
and then key D D and W
is uh this is -11 I
believe which is actually action
believe which is actually action
six right
six right
-11 and then key s
-11 and then key s
is D and S is uh 1 one I believe which
is D and S is uh 1 one I believe which
is actually action
is actually action
8 and then key s or not key s just key D
8 and then key s or not key s just key D
is
01 which is
01 which is
s okay 6 8 and seven and then just key W
s okay 6 8 and seven and then just key W
is just -10 I
is just -10 I
believe which
believe which
[Music]
is where's negative 1
Z
Z
three which is correct I have it correct
three which is correct I have it correct
which leaves only five which is
which leaves only five which is
one uh one Z okay perfect so I have all
one uh one Z okay perfect so I have all
of
this
this
okay and uh we'll comment this for now
don't know what all this crap
don't know what all this crap
is
is
um we don't need any of this
cool see if this works
no is key down what
oh uh it's just apied wrong
well Target pit not
defined uh we don't care
all
all
right so we have
right so we have
uh we have this
uh we have this
thing it doesn't look like we're
thing it doesn't look like we're
controlling it
controlling it
though so let's figure out why we're not
though so let's figure out why we're not
controlling
it because this is commented as why
this
enough oh little awkward that the W key
enough oh little awkward that the W key
overlaps like that
overlaps like that
huh I didn't think about
huh I didn't think about
that I'm going have to replace this with
that I'm going have to replace this with
like up down left right I
think but for now
think but for now
um is still not getting
applied self. Human
Action it's getting returned here
it looks like it is getting
applied hold
on okay we get a break point
this looks like it is getting
this looks like it is getting
applied so oops we should be able to
applied so oops we should be able to
control this agent then
control this agent then
right we should be able to control this
right we should be able to control this
agent and if we can't then that's maybe
agent and if we can't then that's maybe
a clue as to why uh it's not working the
a clue as to why uh it's not working the
way we think it
should yeah I do not have control
should yeah I do not have control
here let's print out the Human
Action so we can see what it thinks that
Action so we can see what it thinks that
we should be doing
we should be doing
here 40
70 this doesn't trigger
70 this doesn't trigger
anything right key trigger
something so down key doesn't trigger
something so down key doesn't trigger
anything everything but down key does
anything everything but down key does
trigger something
though and it's just ignoring all of it
though and it's just ignoring all of it
apparently so that could be a a good
apparently so that could be a a good
indicator
right where's the
action velocity
let's print it out in the
scyon and then this should tell us
scyon and then this should tell us
whoops okay so then this will tell us um
whoops okay so then this will tell us um
why we don't
why we don't
have uh we don't have control over the
have uh we don't have control over the
player and it seems like the neural
player and it seems like the neural
network is still controlling it
and this very possibly will tell us what
and this very possibly will tell us what
we need to do to fix the training as
we need to do to fix the training as
well good debug
well good debug
technique always have your environment
technique always have your environment
be human playable okay
so I'm holding
so I'm holding
keys and nothing is getting transferred
keys and nothing is getting transferred
here
here
whatsoever if I hold W nothing
whatsoever if I hold W nothing
happens no none of these things are
happens no none of these things are
getting transferred I can see my action
getting transferred I can see my action
on the bottom here but it's not making
on the bottom here but it's not making
it into the
it into the
environment okay so that's that's
environment okay so that's that's
important then why is not why is this
important then why is not why is this
not making it into the environment and
not making it into the environment and
why do we have what looks like random
why do we have what looks like random
data here
data here
um it's coming from this actions
um it's coming from this actions
discreet what is actions discreet
discreet what is actions discreet
actions discreet is just self.
actions discreet is just self.
actions okay that's the action that
actions okay that's the action that
should be the actions that we pass to
should be the actions that we pass to
the environment is a buffer
so we're correctly passing the player ID
so we're correctly passing the player ID
the PID
the PID
here which is what we should need in
here which is what we should need in
order to get
order to get
uh these
uh these
velocities and then we
velocities and then we
pass yeah so I'm suspecting that the
pass yeah so I'm suspecting that the
action buffer is just completely not
action buffer is just completely not
getting passed correctly at this point
we're passing self- dead actions of
we're passing self- dead actions of
pointer through
pointer through
end which I would think would be the
end which I would think would be the
correct
correct
thing yeah that seems like the correct
thing yeah that seems like the correct
thing to
me so at this point I kind of just want
me so at this point I kind of just want
to print out the
to print out the
whole actions buffer
maybe let's print out the actions buffer
maybe let's print out the actions buffer
to see if we have any control over that
to see if we have any control over that
and maybe the the parsing is
and maybe the the parsing is
wrong we're getting very close here
all right so the first thing in the
all right so the first thing in the
actions is um the
actions is um the
movement so we're going to basically
movement so we're going to basically
just see if it looks random or if it
just see if it looks random or if it
looks like it's correctly
pared I suspect what's happening is it's
pared I suspect what's happening is it's
not learning anything properly because
not learning anything properly because
it's just having random environment data
it's just having random environment data
okay so this looks pretty random
yeah so the actions tensor is somehow
yeah so the actions tensor is somehow
getting screwed
up self dead actions is here set that to
up self dead actions is here set that to
be an n32
render mode is passed with self actions
render mode is passed with self actions
the first 10 actions which is
correct when we make the environments
correct when we make the environments
themselves we pass a slice of
themselves we pass a slice of
actions from a pointer through the end
actions from a pointer through the end
this pointer just goes over slices of 10
this pointer just goes over slices of 10
which is 10 agents per
which is 10 agents per
environment this should also be
correct right here we load actions in to
correct right here we load actions in to
the
game ah this is right this is self do
actions uh unfortunately I don't think
actions uh unfortunately I don't think
that that's going to tell us too much
that that's going to tell us too much
about why this thing is bugged but
maybe okay
we're still
not still not passing that's weird self
not still not passing that's weird self
dead
dead
actions
zero is equal to the Human
zero is equal to the Human
Action that was
bizarre does this thing get changed at
bizarre does this thing get changed at
runtime
runtime
why did you choose to explore mobas for
why did you choose to explore mobas for
reinforcement
reinforcement
learning well in addition to them being
learning well in addition to them being
a really complex and interesting game
a really complex and interesting game
genre for
genre for
RL there's historical
RL there's historical
precedent I consider this to be the best
precedent I consider this to be the best
result in all of reinforcement
result in all of reinforcement
learning um the only problem is that
learning um the only problem is that
absolutely nobody can build off of any
absolutely nobody can build off of any
of this work
of this work
because they have lots of Hardware not
because they have lots of Hardware not
even lots of hardware by today's
even lots of hardware by today's
standards but but like I don't have 256
standards but but like I don't have 256
gpus so I'm building a smaller scale but
gpus so I'm building a smaller scale but
ludicrously fast version of this so that
ludicrously fast version of this so that
we can actually match the training scale
we can actually match the training scale
if not the model size and like the
if not the model size and like the
actual game itself uh and we can do some
actual game itself uh and we can do some
really awesome research really really
fast we're actually going to be able to
fast we're actually going to be able to
match these batch size numbers as well
match these batch size numbers as well
which is really
which is really
cool we're going to be able to basically
cool we're going to be able to basically
match like most of their scale
parameters it's all open source as
well so researchers are going to be able
well so researchers are going to be able
to use this environment for years to
to use this environment for years to
come
what is wrong provided I can actually
what is wrong provided I can actually
get the thing working
get the thing working
properly that
is these buffers are not playing very
is these buffers are not playing very
nice with me at the
moment like clearly the data is not
moment like clearly the data is not
going to the
going to the
the C version of the environment
right the data should be mirrored from
right the data should be mirrored from
python to C
python to C
though it should be the same data so
though it should be the same data so
unless I accidentally did a copy
unless I accidentally did a copy
somewhere
somewhere
actions what is it actions
actions what is it actions
discreete oh maybe I'm using the wrong
discreete oh maybe I'm using the wrong
one
one
here actions discreet
no we're using actions discreet right
no we're using actions discreet right
actions
continuous that shouldn't be getting
continuous that shouldn't be getting
called
called
somehow actions continuous and actions
somehow actions continuous and actions
SC maybe we load them into the wrong
SC maybe we load them into the wrong
buffer or
something self. actions equals
actions actions
actions actions
is okay we have it as an INT at the
is okay we have it as an INT at the
moment now this all looks fine to me
moment now this all looks fine to me
this all looks totally
fine the heck else would it be
it's getting to be very
weird
so if I press seven
so if I press seven
here I don't see a seven in here and
here I don't see a seven in here and
this agent is still doing whatever the
this agent is still doing whatever the
hell it
wants
yeah I didn't just forget to rebuild it
yeah I didn't just forget to rebuild it
did
I no there's nothing I changed that
I no there's nothing I changed that
should have been
should have been
rebuilt self. actions wait
action is equal to self note
am I somehow like not
am I somehow like not
allocating or it
gets this was working
gets this was working
before this was working a couple of days
before this was working a couple of days
ago so
ago so
somehow but maybe I was using the
somehow but maybe I was using the
continuous version of it
continuous version of it
which was actions continuous
here actions discreet and actions
continuous this looks
right what else could could I have
right what else could could I have
messed
messed
up does self do actions get overwritten
up does self do actions get overwritten
or
something I don't think
something I don't think
so yeah no there's no actions getting
so yeah no there's no actions getting
overwritten here
I thought there was like a weird thing
I thought there was like a weird thing
that I'd done that um would have caused
that I'd done that um would have caused
the actions buffer to get
overwritten it's
n32 that's what it should
be action space should
be action space should
match action space does match
action should be be it should be given a
action should be be it should be given a
slice it is being given a
slice it is being given a
slice it's being given the same exact
slice it's being given the same exact
type of slice as rewards which we know
type of slice as rewards which we know
work pointer through
end what's the size of the actions
end what's the size of the actions
buffer numb agents which is
buffer numb agents which is
correct which would be the same as the
correct which would be the same as the
buffer
buffer
allocated which which it
is wait we have another clue here
is wait we have another clue here
right the additional clue is that new
right the additional clue is that new
data is getting loaded into actions
right the data is always changing here
right the data is always changing here
we're constantly getting new data into
we're constantly getting new data into
actions it's just that our data doesn't
actions it's just that our data doesn't
seem to be getting loaded into
seem to be getting loaded into
actions uh and the reason is that we're
actions uh and the reason is that we're
stupid this goes
here and the bug is just called being
stupid and now it works with the caveat
stupid and now it works with the caveat
that the keys seem to
that the keys seem to
be not all the keys seem to
be not all the keys seem to
work this
work this
works I can't go up I can go diagonally
up diagonals work all the diagonals
work the upkey goes
work the upkey goes
down the right key goes right the left
down the right key goes right the left
key so the up and down Keys the up key
key so the up and down Keys the up key
goes down and the down down key doesn't
goes down and the down down key doesn't
do anything let's go look at
that so the
that so the
upkey
Ws ah this is supposed to be
Ws ah this is supposed to be
W and this is supposed to be s
W and this is supposed to be s
now let's see if this
works
perfect do I
perfect do I
fight yeah you it auto
targets I mean you don't fight
targets I mean you don't fight
particularly
particularly
well but you fight
can I solo one minion
can I solo one minion
maybe oh yeah
look funny enough it doesn't aggro
look funny enough it doesn't aggro
because I'm just out of Agro
range and you will just stand here and
range and you will just stand here and
die
skills don't really do
skills don't really do
anything all right but that's like the
anything all right but that's like the
basic thing that we wanted so now what
basic thing that we wanted so now what
we're going to do
we're going to do
is let's get rid of those other
prints and all we want to know is we
prints and all we want to know is we
want to know the reward
I want to know how much reward we
I want to know how much reward we
get because this will tell us basically
get because this will tell us basically
like if the should be learning something
like if the should be learning something
interesting and they're just not
interesting and they're just not
learning it or if there's a
learning it or if there's a
bug if I go towards the ancient I should
bug if I go towards the ancient I should
be getting
reward Okay so zero
reward Okay so zero
reward
05 looks like I'm getting reward
right you can't stop me okay you get
right you can't stop me okay you get
lots of
lots of
reward and then if you hit a wall you
reward and then if you hit a wall you
get
nothing very reasonable reward
right how much do I get for killing this
guy I do not get a reward for killing
guy I do not get a reward for killing
this guy
really do neutral creeps not have XP set
does not appear that I get any reward
does not appear that I get any reward
for
kills why aren't we getting reward for
kills why aren't we getting reward for
kills
player. reward plus equal
reward let's debug
reward let's debug
here so that's one thing that's going to
here so that's one thing that's going to
hurt a bunch is not not having the XP
hurt a bunch is not not having the XP
reward though I don't think we should
reward though I don't think we should
strictly need the XP reward uh just to
strictly need the XP reward uh just to
get them to go mid and like fight
we going to get a reward here or
we going to get a reward here or
no ah xp35 reward
zero target. XP
onkill it's set for
onkill it's set for
Towers it's supposed to be set for
Towers it's supposed to be set for
neutrals and it's supposed to be set for
creeps oh into XP over
creeps oh into XP over
250 well I don't know why we have
that that would definitely cause this
that that would definitely cause this
right
let's see if we get the reward correctly
let's see if we get the reward correctly
now and if so I think we're going to be
now and if so I think we're going to be
ready to train some models on this and
ready to train some models on this and
uh if we know the controls
uh if we know the controls
work then it should just be a matter of
work then it should just be a matter of
time and like a little bit of
time and like a little bit of
experimentation to get uh something
experimentation to get uh something
training
properly okay we're hitting this
thing reward zero what the
hell well I still have an INT this is
hell well I still have an INT this is
supposed to be float
reward what data type do I have do I
reward what data type do I have do I
have reward as an INT no rewards a float
so this should be like a little bit over
so this should be like a little bit over
0.1
right so actually this is like probably
right so actually this is like probably
too
too
low let's do like over a 100 Maybe
though I don't know if this is going to
though I don't know if this is going to
let them kill enemy Heroes
let them kill enemy Heroes
because they don't get that big of a
because they don't get that big of a
reward for hero
kills okay they're
fighting perfect now we get a reward
fighting perfect now we get a reward
reward of
35 so I think that we can
35 so I think that we can
um we can rebuild this we can commit
um we can rebuild this we can commit
this
and let's see if uh we do any better
what did I forget to
what did I forget to
um what did I
um what did I
forget I forgot I print somewhere
right yep
all right we've got our
all right we've got our
model should be
model should be
training there we
go if we check in the UI here
the
heck there we
heck there we
go it's only given us one data
go it's only given us one data
point should have given us more by now
yeah okay here we go
yeah okay here we go
so reward goes from
so reward goes from
zero which it should start at
zero which it should start at
zero up to some very small
value we're not seeing anything
value we're not seeing anything
great well I left and now it's 800k no
great well I left and now it's 800k no
it's very it's kind of variable and I
it's very it's kind of variable and I
don't know
don't know
why I said we're going to have a million
why I said we're going to have a million
train SPS and we're going to have a
train SPS and we're going to have a
million train SPS it's just a matter of
million train SPS it's just a matter of
when I go do that level of
optimization for context I have this
optimization for context I have this
exact same neural network training on
exact same neural network training on
multiple other environments at a million
multiple other environments at a million
steps per second so it will be a million
steps per second so it will be a million
steps per
second okay so this does improve reward
second okay so this does improve reward
a bit
a bit
but they should really be learning this
but they should really be learning this
very
quickly this is not something that
quickly this is not something that
should be hard to learn
I mean it is going
up cool project what RL algorithm are
up cool project what RL algorithm are
you using it's
po po on a small confet with lstm
we're just trying to get some sanity uh
we're just trying to get some sanity uh
some sanity checks going here so that we
some sanity checks going here so that we
can get the agents actually learning to
can get the agents actually learning to
play the game pretty quick uh like at
play the game pretty quick uh like at
least learning some
Basics do you know what opening I use
Basics do you know what opening I use
for their DOTA bot PO with an lstm same
for their DOTA bot PO with an lstm same
thing they had a substantially fancier
thing they had a substantially fancier
architecture and a uh and obviously much
architecture and a uh and obviously much
larger
larger
networks their networks were oh let's
networks their networks were oh let's
see 10,000 times larger than
see 10,000 times larger than
mine that right
no 50,000 1,000 times larger than
no 50,000 1,000 times larger than
mine but
mine but
hey we're doing pretty good
130 million steps
130 million steps
trained I'm hoping we don't have to run
trained I'm hoping we don't have to run
a hyper pram sweep just to get it to do
a hyper pram sweep just to get it to do
the basic
the basic
thing that we might end up having
to DOTA is also a thousand times more
to DOTA is also a thousand times more
complex than your M as of now well
complex than your M as of now well
yeah I'm one guy I can't build all of
yeah I'm one guy I can't build all of
DOTA in a week but I mean it looks
DOTA in a week but I mean it looks
pretty nice like you've probably seen
pretty nice like you've probably seen
this on this GIF on Twitter like we've
this on this GIF on Twitter like we've
got Lanes we've got creeps we've got
got Lanes we've got creeps we've got
five different hero classes we've got
five different hero classes we've got
neutral camps we've got all the towers
neutral camps we've got all the towers
we've got the same we've got the whole
we've got the same we've got the whole
map
map
right so we have some
right so we have some
stuff it's not
stuff it's not
terrible it runs real fast as well
thank you this is all open source
thank you this is all open source
too so you're free to play with
it it's in oh wow we hit 675
it it's in oh wow we hit 675
awesome
awesome
um it's just in this any config Branch
um it's just in this any config Branch
for now it'll get merged to Dev pretty
for now it'll get merged to Dev pretty
soon
soon
um go ahead start the repo on your way
um go ahead start the repo on your way
and if you wouldn't mind helps us out a
and if you wouldn't mind helps us out a
lot um but yeah it's in here it's in
lot um but yeah it's in here it's in
puffer Li environments
ocean and I'm constantly pushing uh
ocean and I'm constantly pushing uh
little updates and changes to
it what are these agents
it what are these agents
doing they really should be
doing they really should be
learning they should be learning very
learning they should be learning very
quickly to go towards the
quickly to go towards the
the
ancient the fact that they are not
ancient the fact that they are not
leveling means that they're not learning
leveling means that they're not learning
that as
well is it this entropy crashing
is it the way I'm doing multi-
discreet hold
on uh
just run a little sanity and on the
just run a little sanity and on the
other
machine oh
um well
[ __ ] okay I think I broke something in
[ __ ] okay I think I broke something in
Dev that's good to
know I remember somebody asked me for a
know I remember somebody asked me for a
patch and I probably broke something in
patch and I probably broke something in
the
process we're going to push a little
process we're going to push a little
thing here
I'll fix this
separately I bet this will just make it
separately I bet this will just make it
work
by the way what is the purpose of the
by the way what is the purpose of the
lstm in this
lstm in this
setup I'm an ARL Noob never worked po
setup I'm an ARL Noob never worked po
Ina the purpose of the lstm is so that
Ina the purpose of the lstm is so that
the agents can have
the agents can have
memory they can't remember anything from
memory they can't remember anything from
previous frames if you don't give them
previous frames if you don't give them
an lstm
that is the purpose
what
why did that rebuild instantly
I'm
confused and does this improve their pay
confused and does this improve their pay
capabilities well yeah think about it
capabilities well yeah think about it
like if you do not remember anything
like if you do not remember anything
about what's happened in the past in the
about what's happened in the past in the
game you're fundamentally limited on
game you're fundamentally limited on
what you can
do oh there we go why the heck did that
do oh there we go why the heck did that
not
but where is this thing that this is
but where is this thing that this is
being weird
is
this oh I'm on the wrong machine okay
this oh I'm on the wrong machine okay
that's fine no big
deal uh too many indices
deal uh too many indices
for right this is just
for right this is just
compilation right because the past is
compilation right because the past is
not encoded into the current
not encoded into the current
state but we still want the marov the
state but we still want the marov the
marov properties is irrelevant you can
marov properties is irrelevant you can
ignore all of the math here it's it's
ignore all of the math here it's it's
very simple it's just if you don't give
very simple it's just if you don't give
it any information about the past then
it any information about the past then
and it's making its decisions based only
and it's making its decisions based only
off what you give it then it has no
off what you give it then it has no
memory that's all there is to it there's
memory that's all there is to it there's
literally no
literally no
like the markof property is irrelevant
here
for let's just train
this couldn't we embed the past
this couldn't we embed the past
decisions into our state
decisions into our state
space how would you do
space how would you do
that the only other way you could do
that the only other way you could do
that is like you'd have to concatenate
that is like you'd have to concatenate
or
something there's not a good way of
something there's not a good way of
doing that
so this decode needs to just go like
so this decode needs to just go like
this
I see I think yeah like that's what the
I see I think yeah like that's what the
lstm is for right it's for learning the
embedding because otherwise you're just
embedding because otherwise you're just
stacking frames and then you only get
stacking frames and then you only get
like n frames worth of memory so that's
like n frames worth of memory so that's
like yeah you're paying a lot for you're
like yeah you're paying a lot for you're
paying a lot and you're blowing up the
paying a lot and you're blowing up the
complexity of your forward pass it's way
complexity of your forward pass it's way
better to just use a state based model
isn't that what the point is the
isn't that what the point is the
lstm yeah exactly that is exactly the
lstm yeah exactly that is exactly the
point of the lstm
that doesn't mean the lstm is perfect at
that doesn't mean the lstm is perfect at
it though lstms are not amazing
models actions do not match this date
models actions do not match this date
space
jeez why
not I forget to update something
discreet of
nine should match the action space right
I'm what I'm doing now is I'm just
I'm what I'm doing now is I'm just
hacking this to be
hacking this to be
discreet here it is I'm hacking this to
discreet here it is I'm hacking this to
be discreet because I think there's a
be discreet because I think there's a
puffer lib Dev Branch bug with multi-
puffer lib Dev Branch bug with multi-
discreet that I probably introduced
discreet that I probably introduced
while trying to fix something for
while trying to fix something for
somebody
else this isn't even the dev Branch to
else this isn't even the dev Branch to
be fair so this isn't even bad this is
be fair so this isn't even bad this is
like some random hack branch
there we go
cool I think um based on this uh it
cool I think um based on this uh it
definitely Deb boosts your post when you
definitely Deb boosts your post when you
add a link even in a reply cuz my posts
add a link even in a reply cuz my posts
are now doing twice as well as
are now doing twice as well as
before so that's unfortunate I got to
before so that's unfortunate I got to
put puffer AI puffer lib like this
put puffer AI puffer lib like this
but the more you know that'll help now
but the more you know that'll help now
with
with
this getting this project out
there so here's a fun thing these models
there so here's a fun thing these models
are
are
like these models here that we're
like these models here that we're
looking at nowadays are like hundreds of
looking at nowadays are like hundreds of
billions we have now 400 billion
billions we have now 400 billion
parameter models
parameter models
right um the models that we're training
right um the models that we're training
here are like hundreds of thousands
here are like hundreds of thousands
whoops hundreds of thousands to low uh
whoops hundreds of thousands to low uh
to like millions of parameters but they
to like millions of parameters but they
actually use like the same amount of
actually use like the same amount of
they actually use the same or more data
they actually use the same or more data
than these LS these uh language models
than these LS these uh language models
use we can consume like a gigabyte of
use we can consume like a gigabyte of
data per second with these models
data per second with these models
so we're like training on the amount of
so we're like training on the amount of
data that the whole language model is
data that the whole language model is
trained on we do that on one GPU in like
trained on we do that on one GPU in like
a day or
a day or
two that's pretty
cool that's why RL is
awesome well this is now bizarre
reward should
reward should
be this should definitely be good
now unless I broke something in like a
now unless I broke something in like a
recent refactor this should be good
yeah now this is
fine solves the test
fine solves the test
m test m still
works it'd be pretty hard for me to
works it'd be pretty hard for me to
break the whole Library without noticing
it but why would reward go to zero like
it but why would reward go to zero like
this
I think I realized where confusion you
I think I realized where confusion you
don't feed every frame into the
don't feed every frame into the
convenant and then into po you feed X
convenant and then into po you feed X
continuous frames as
one you freed you feed one frame into
one you freed you feed one frame into
the convet
and then that goes into the lstm with
and then that goes into the lstm with
the current state and goes forward you
the current state and goes forward you
don't stack frames you feed one frame in
don't stack frames you feed one frame in
to the
to the
convet and plus the state that has been
convet and plus the state that has been
uh from the last time step from the lstm
did start learning right
here yeah all good
here yeah all good
um I can't tell what the heck is
um I can't tell what the heck is
happening
here this should really not be hard to
here this should really not be hard to
learn so if this is hard to learn that
learn so if this is hard to learn that
means something Jank is
means something Jank is
happening essentially
well now the reward is going up but like
well now the reward is going up but like
why did
it why does it crash in the first place
I'm trying to think of like likely bug
I'm trying to think of like likely bug
sources
here we check the observations they look
here we check the observations they look
reasonable we check the rewards they
reasonable we check the rewards they
look reasonable
we made the action space single
discreet we made sure that it's
discreet we made sure that it's
playable and that the play returns
playable and that the play returns
reasonable Rewards
really expected to have learned
really expected to have learned
something
here but this is not the reward curve
here but this is not the reward curve
that we
that we
want I mean it should basically learn
want I mean it should basically learn
this instantly you would think
[Music]
I mean this really really should be easy
I can go mess with some hypers
I can go mess with some hypers
maybe so it is
maybe so it is
learning but like this is a ridiculous
learning but like this is a ridiculous
pace for something this simple
got this entropy coefficient which is
got this entropy coefficient which is
like reasonable
right Lambda and GMA which are
right Lambda and GMA which are
sufficiently low for something that's
sufficiently low for something that's
easy
a
a
BPT Horizon
yeah I don't freaking know
we're going to YOLO some
we're going to YOLO some
stuff this is usually
stuff this is usually
reasonable reasonable way to like get
reasonable reasonable way to like get
stuff to work
stuff to work
quickly
um we want 400
um we want 400
M's single
M's single
core no
multiprocessing like this should
multiprocessing like this should
basically learn
basically learn
instantly it should be that
easy okay already we see a very
easy okay already we see a very
different entropy value
some of these should have been reported
some of these should have been reported
to w b by
now there we
now there we
go 8 e minus
5 e
have you ever looked into applying
have you ever looked into applying
active
inference do you mean model based
inference do you mean model based
updates World model stuff
it's optimizing now but this is still
it's optimizing now but this is still
ridiculously
ridiculously
slow free energy
minimization you'd have to Define that
minimization you'd have to Define that
in the context of a problem like this
we have an RL we have curiosity based
we have an RL we have curiosity based
learn like we have curiosity stuff we've
learn like we have curiosity stuff we've
got model based
got model based
learning right World modeling all these
learning right World modeling all these
things or you talking about something
things or you talking about something
different
minimize its surprise okay yeah so we
minimize its surprise okay yeah so we
have that in RL we just use different we
have that in RL we just use different we
do use Li different
do use Li different
language surprise minimization we
language surprise minimization we
usually refer to that as uh curiosity
usually refer to that as uh curiosity
based learning or if we're explicitly
based learning or if we're explicitly
minimizing Surprise by making
minimizing Surprise by making
predictions about what we expect the
predictions about what we expect the
future to be then uh we refer to that as
future to be then uh we refer to that as
World modeling typically or model based
World modeling typically or model based
learning
learning
um there have been some results in this
area the research has been sketchy
area the research has been sketchy
though the research has been very
though the research has been very
sketchy I Bur a bunch of hours and
sketchy I Bur a bunch of hours and
wasted a lot of time trying to replicate
wasted a lot of time trying to replicate
a very high-profile paper in this area
a very high-profile paper in this area
recently and
recently and
um yeah
um yeah
it's it's something that's going to
it's it's something that's going to
probably be needed at some point but I
probably be needed at some point but I
don't think that we have a good
don't think that we have a good
understanding of the form in which we
understanding of the form in which we
want to incorporate
want to incorporate
that people are doing like planning and
that people are doing like planning and
hallucinated rollouts or hallucinated
hallucinated rollouts or hallucinated
space um that I don't think we need some
space um that I don't think we need some
form of prediction of the future
form of prediction of the future
probably
probably
needed generative modeling to go through
needed generative modeling to go through
the future outcomes and then pick one
the future outcomes and then pick one
that minimizes so that I don't think we
that minimizes so that I don't think we
necessarily need I think we do need to
necessarily need I think we do need to
model the future but I don't think we
model the future but I don't think we
need to like iteratively hallucinate
need to like iteratively hallucinate
through future outcomes the whole
through future outcomes the whole
strength of reinforcement learning comes
strength of reinforcement learning comes
from the fact that we don't need to like
from the fact that we don't need to like
imagine what date what it would be like
imagine what date what it would be like
if we had good data we have good data we
if we had good data we have good data we
have ground truth from the
have ground truth from the
environment so instead of spending
environment so instead of spending
time um trying to learn based off of you
time um trying to learn based off of you
know predictions of what we think the
know predictions of what we think the
future will be we can just simulate more
future will be we can just simulate more
data and learn directly off of it one of
data and learn directly off of it one of
the big strengths of RL we have infinite
data 0 0
data 0 0
Z
Z
two it's a weird
I mean we literally have infinite data
I mean we literally have infinite data
like look at
this
this
right we have unlimited ground truth
data and yeah you won't always have
data and yeah you won't always have
unlimited ground truth
unlimited ground truth
data will that lead us to
data will that lead us to
Consciousness akin to us I no idea
Consciousness akin to us I no idea
IDE friend of mine who's deep in model
IDE friend of mine who's deep in model
based RL told me that it's ridiculously
based RL told me that it's ridiculously
hard
hard
yeah because you're taking the thing
yeah because you're taking the thing
that RL does really well which is ingest
that RL does really well which is ingest
tons of data very quickly and you're
tons of data very quickly and you're
saying but what if we did RL without
saying but what if we did RL without
that that's why that's
hard like you're really throwing away
hard like you're really throwing away
the major major strength of RL
this thing is learning it's just not
this thing is learning it's just not
learning fast enough to be
satisfying like this is really stupid
satisfying like this is really stupid
that this takes this long this is not
hard should I do like a 10 mil sweep or
something oh come
something oh come
on is this just hypers like what the
heck this should be so unbelievably easy
how we're able to come to conclusions
how we're able to come to conclusions
with such limited well the thing is like
with such limited well the thing is like
you don't necessarily need to model the
you don't necessarily need to model the
future in order to train an agent that
future in order to train an agent that
can model the future like have you seen
can model the future like have you seen
the open ai5 models they're not
the open ai5 models they're not
explicitly trained to predict what the
explicitly trained to predict what the
opponents are doing but they very very
opponents are doing but they very very
clearly understand like they understand
clearly understand like they understand
what you're going to do before you do it
what you're going to do before you do it
so they've developed some sort of
so they've developed some sort of
internal World model with without you
internal World model with without you
training them to do
training them to do
that so it's completely possible for
that so it's completely possible for
that thing to just be an emergent
that thing to just be an emergent
property you don't necessarily have to
property you don't necessarily have to
train it directly
the really high learning
rate 9
2, let's
2, let's
do 160 Maybe
do 160 Maybe
and
and
then no that's really small isn't
it 320 * 4
is we're just doing some YOLO runs here
why does it immediately train to be
why does it immediately train to be
worse than the
default thing immediately trains to be
default thing immediately trains to be
worse than random
I mean there's got to be something wrong
right is there a possibility of you live
right is there a possibility of you live
streaming on a fun environment using
streaming on a fun environment using
hierarchical reinforcement hierarchical
hierarchical reinforcement hierarchical
reinforcement learning does not
work that's just not a thing that works
consistently well this is funny that
consistently well this is funny that
this is actually like doing stuff now
this is actually like doing stuff now
and getting zero
and getting zero
reward
um huh that's really that's really weird
um huh that's really that's really weird
I think that there's something very
I think that there's something very
wrong with with this reward we'll see
wrong with with this reward we'll see
so let me here let me just show you
so let me here let me just show you
something
something
right here's the best result in
right here's the best result in
RL everything is published about it you
RL everything is published about it you
have 150 million parameter model you
have 150 million parameter model you
have an amount of Hardware that's not
have an amount of Hardware that's not
even that big by modern standards with
even that big by modern standards with
uh with language modeling you have a on
uh with language modeling you have a on
layer lstm no hierarchy no algorithmic
layer lstm no hierarchy no algorithmic
Shenanigans very very simple and just
Shenanigans very very simple and just
good engineering in this solves DOTA
good engineering in this solves DOTA
DOTA is a ludicrously complicated game
DOTA is a ludicrously complicated game
environment like I don't think if you
environment like I don't think if you
were describe DOTA to your average adult
were describe DOTA to your average adult
I don't think that they would believe
I don't think that they would believe
you that people have a hobby that is
you that people have a hobby that is
this complicated that like average
this complicated that like average
people have a hobby that is this
people have a hobby that is this
complicated um it is insanely intricate
complicated um it is insanely intricate
this thing is super human it beats the
this thing is super human it beats the
top teams it like stomped basically
top teams it like stomped basically
everybody in pubs
everybody in pubs
it was just
it was just
Godly that's what RL can do without any
Godly that's what RL can do without any
algorithmic
algorithmic
shenanigans that is what RL can
do it is not one character it is a
do it is not one character it is a
pretty sizable pool of
pretty sizable pool of
characters is not one
characters is not one
character they had they didn't do the
character they had they didn't do the
whole pool of characters cuz that's a
whole pool of characters cuz that's a
lot of compute extra cuz you have to
lot of compute extra cuz you have to
train all the characters but they had
train all the characters but they had
like a sizable
pool they originally did a 1V one mid
pool they originally did a 1V one mid
and then they did the full game 5 B5 and
and then they did the full game 5 B5 and
they stomped OG
but is it one model that can do multiple
characters yes it is it's one model that
characters yes it is it's one model that
independently controls each
independently controls each
character so it's one model you run
character so it's one model you run
different copies of the model and it
different copies of the model and it
controls each of the characters on the
team really really really really
team really really really really
freaking
good okay now we're getting
somewhere now we're getting
somewhere this is impressive I'll have
somewhere this is impressive I'll have
to look more it's I mean this paper is I
to look more it's I mean this paper is I
don't know why nobody like I mean I
don't know why nobody like I mean I
guess it is very highly cited but I
guess it is very highly cited but I
swear nobody's read the full paper it's
swear nobody's read the full paper it's
so
good I mean basically you like let me
good I mean basically you like let me
put it this way like if you were to just
put it this way like if you were to just
read this one paper and then go from
read this one paper and then go from
there versus like read the whole rest of
there versus like read the whole rest of
the field for the most part like the
the field for the most part like the
vast majority of Academia minus this
vast majority of Academia minus this
paper you'll learn more from reading
paper you'll learn more from reading
this one
paper they saw everything
paper they saw everything
they saw
everything and then Academia spent the
everything and then Academia spent the
last several years like fiddling around
last several years like fiddling around
with
algorithms and none of them have done
algorithms and none of them have done
any better than
any better than
po not consistently at least maybe sa
po not consistently at least maybe sa
for robotics that's about
for robotics that's about
it and I think that the reasons for that
it and I think that the reasons for that
are even kind of
sketch okay so this is now interesting
sketch okay so this is now interesting
because this reward
because this reward
here
is this reward is low like zero but
is this reward is low like zero but
they're
leveling how are they
leveling are we
back we're back right
back we're back right
stream just
stream just
blipped hopefully it doesn't go
blipped hopefully it doesn't go
down
down
um so the model used local state data
um so the model used local state data
which is the same as the data that you
which is the same as the data that you
would use to render the the map view
would use to render the the map view
that humans are seeing so it used
that humans are seeing so it used
equivalent data to what humans have
equivalent data to what humans have
access
access
to and the reason for that is if you
to and the reason for that is if you
render the game you're 100 times slower
render the game you're 100 times slower
so you need a million cores instead of a
so you need a million cores instead of a
100,000 it's a pure it's a pure uh infr
100,000 it's a pure it's a pure uh infr
limit a of the of
limit a of the of
rendering
rendering
so
yeah how are these things leveling and
yeah how are these things leveling and
not getting
reward CU they're doing
reward CU they're doing
something but they're not getting reward
yeah they've got a level
25 okay
we're doing some very small tests
here we're going to just log some
here we're going to just log some
position
information e
okay so we're just going to log their X
okay so we're just going to log their X
and
y's I want to see if they're like
y's I want to see if they're like
actually moving towards the center or if
actually moving towards the center or if
like some janky weird thing is just
happening this seems to be logging
happening this seems to be logging
successfully so I'm going to go use the
successfully so I'm going to go use the
restroom and I'll be right
back
for
e
e
e e
how are we
how are we
doing are our agents being
stupid let's
stupid let's
see this is going to tell something
see this is going to tell something
either way
radian
X
X
is
is
30 radiant
30 radiant
y oh they're going to the center look
y oh they're going to the center look
they're going to the
they're going to the
center you see
center you see
that dire
that dire
y dire
X they're not doing much yet
what did I miss here uh I remember
what did I miss here uh I remember
watching the games of open ai5 Live I
watching the games of open ai5 Live I
was in the audience as
was in the audience as
well for their finals not for TI agents
well for their finals not for TI agents
were mechanically subpar compared to the
were mechanically subpar compared to the
pros but they yeah they gimped them
pros but they yeah they gimped them
mechanically so that they wouldn't be
mechanically so that they wouldn't be
unfair but their objective taking was
unfair but their objective taking was
really good so they were really
really good so they were really
optimizing for long-term rewards I mean
optimizing for long-term rewards I mean
that's what should tell you that it's
that's what should tell you that it's
good right I mean yeah if they just like
good right I mean yeah if they just like
perfect mechanic stomped the pros that
perfect mechanic stomped the pros that
would be one thing but they
didn't what are useful metrics to plot
didn't what are useful metrics to plot
when implementing
when implementing
RL average reward per batch
um it depends on the
um it depends on the
environment as you can see I'm adding
environment as you can see I'm adding
lots of environment specific stuff here
lots of environment specific stuff here
so like I just I started saying okay
so like I just I started saying okay
let's see what level they're getting
let's see what level they're getting
right like okay they're leveling up but
right like okay they're leveling up but
like I didn't think that they were going
like I didn't think that they were going
to the center of the map so let's start
to the center of the map so let's start
logging their X and Y okay it looks like
logging their X and Y okay it looks like
they're starting to learn to go to the
they're starting to learn to go to the
center of the map that's interesting
center of the map that's interesting
right but they're not getting any reward
right but they're not getting any reward
for doing it so something is wonky there
for doing it so something is wonky there
so now this thing is going to be done in
so now this thing is going to be done in
22 seconds and we're going to look at
22 seconds and we're going to look at
this model and see what it
this model and see what it
does and we're going to see if it
does and we're going to see if it
matches up with what this is telling us
how we doing with
how we doing with
puffer 675 still
nice this is solid
progress this is huge for reinforcement
progress this is huge for reinforcement
learning as
learning as
well for something this new I'm very
well for something this new I'm very
very happy
how do you get live stats in the
how do you get live stats in the
terminal well I built a little thing it
terminal well I built a little thing it
uses Rich for layout and I built that I
uses Rich for layout and I built that I
built that as a dashboard so that people
built that as a dashboard so that people
can have nice RL stats in the
terminal free and open source all free
terminal free and open source all free
and all open
source glad you like
source glad you like
it e
your training model is probably better
your training model is probably better
than the teammates I've had in my last
than the teammates I've had in my last
few
few
games man I played league for a bit I
games man I played league for a bit I
actually I think I like DOTA more
actually I think I like DOTA more
um but
like I always think like oh man I should
like I always think like oh man I should
play that again it's going to be fun and
play that again it's going to be fun and
it's never fun it's like the coolest
it's never fun it's like the coolest
game ever but it's just such rage bait I
game ever but it's just such rage bait I
swear
such a cool game
though it's probably more fun if you're
though it's probably more fun if you're
actually good at it
dota's vastly
Superior I really like
Superior I really like
DOTA I mean I really liked I liked
DOTA I mean I really liked I liked
pretty much everything about DOTA I
pretty much everything about DOTA I
didn't play very much of it the Q times
didn't play very much of it the Q times
were God awful as a new player was the
were God awful as a new player was the
only thing um
yeah I tried playing league and it's
yeah I tried playing league and it's
like it's good
like it's good
but nothing felt as good as playing Lena
but nothing felt as good as playing Lena
on on DOTA that was so freaking
good I don't really have time to play a
good I don't really have time to play a
bunch of games these days I'm kind of
bunch of games these days I'm kind of
just building this all the time but um
just building this all the time but um
my main Jam was uh always
MMOs getting people into do it is almost
MMOs getting people into do it is almost
impossible it took me I spent 40 hours
impossible it took me I spent 40 hours
just like pouring over everything I
just like pouring over everything I
could find about the game the week that
could find about the game the week that
open ai5 launched just trying to
open ai5 launched just trying to
understand this game and I'd never
understand this game and I'd never
played a MOA and I was like holy hell
played a MOA and I was like holy hell
this is complicated like wow people play
this is complicated like wow people play
this as a hobby like are there Geniuses
this as a hobby like are there Geniuses
like yeah you get a lot of respect for
like yeah you get a lot of respect for
it as soon as you actually engage with
it as soon as you actually engage with
it
right RuneScape in Guild Wars
right RuneScape in Guild Wars
1 I have played like three or 4 thousand
1 I have played like three or 4 thousand
hours of old
school I had to stop at the end of last
school I had to stop at the end of last
summer or the middle of last summer so I
summer or the middle of last summer so I
could actually do
work MMOs are just straight heroin for
work MMOs are just straight heroin for
me I swear
I always liked Starcraft but was never
I always liked Starcraft but was never
good at it I don't think I could do
good at it I don't think I could do
Starcraft either maybe I could I never
Starcraft either maybe I could I never
had like the uh the reaction time for uh
had like the uh the reaction time for uh
FPS or anything like that but I had
FPS or anything like that but I had
decent enough muscle memory that I could
decent enough muscle memory that I could
do high APM for MMOs that was fun
yo tree uh does uh Inferno Cape Master's
yo tree uh does uh Inferno Cape Master's
Helm dusted omelette mean anything do
Helm dusted omelette mean anything do
you
[Laughter]
[Laughter]
yeah you
yeah you
know yeah you
know MOA mod Val
know MOA mod Val
render RB
2100 total I think that's where about
2100 total I think that's where about
where my account was was around like
where my account was was around like
2K I didn't skill I just did sweaty
pvm well this doesn't make any sense
right why these things just do this why
right why these things just do this why
why do they why are they like
why do they why are they like
this are they just like clipping the
this are they just like clipping the
Minions on their way past is that what
Minions on their way past is that what
they're doing they're just like clipping
they're doing they're just like clipping
these Minions on their way
these Minions on their way
by but then what are these guys
doing need some RL agents for RuneScape
next don't break my favorite game
why are they like this they came these
why are they like this they came these
two came all the way down
two came all the way down
here are they actually killing anything
here are they actually killing anything
or are they just like clipping I imagine
or are they just like clipping I imagine
they're not killing anything doing
they're not killing anything doing
this right
well we have one additional
well we have one additional
um we do have one additional Technique
um we do have one additional Technique
we can
use y all want the hidden debug
technique
okay last hit count of each agent I have
okay last hit count of each agent I have
access to the XP which is about the same
access to the XP which is about the same
they don't get XP for being an AOE they
they don't get XP for being an AOE they
only get XP for last hits
all right ready for the hidden debug
technique I know if I want to share it
technique I know if I want to share it
it's very powerful
we have S
we have S
Ting Port interval
we do it in
we do it in
snake we didn't do it in
snake one second let me go look one
snake one second let me go look one
thing
up the hidden debug Tech
techque e
where is this thing
let's see if this does it for
us so the goal
us so the goal
here the goal here is to render the
here the goal here is to render the
screen and log it so we can see if over
screen and log it so we can see if over
the course of training there's some
the course of training there's some
weird stupid degenerate situation that
weird stupid degenerate situation that
this thing gets itself into that's
this thing gets itself into that's
crashing
crashing
it that's the most likely thing
that's what that's the debug technique
that's what that's the debug technique
is to just log the damn
render
render
4176 hours on my
4176 hours on my
main H yeah
main H yeah
ah
my favorite content in that game was
my favorite content in that game was
solo challenge mode uh
Chambers is
good no Skilling
where are the
where are the
renders should be some renders by
renders should be some renders by
now unless there's like a specific thing
now unless there's like a specific thing
that I'm
forgetting yeah underscore map is Ty
forgetting yeah underscore map is Ty
prefix it
right yeah info Z mooba map
right yeah info Z mooba map
here I have to log it more
often should have been logged
overview environment
overview environment
losses I don't see
losses I don't see
it I think we got to
it I think we got to
uh we got to increase the frequency of
uh we got to increase the frequency of
this never like
this never like
Tob well the thing is that's because Tob
Tob well the thing is that's because Tob
is designed to make you absolutely
is designed to make you absolutely
despise all of your teammates
so if you like hating your teammates
so if you like hating your teammates
it's really
fun same problem as
League that said I did hard mode on day
League that said I did hard mode on day
of release
it was so
sweaty e
why the hell is this thing not logging
why the hell is this thing not logging
screenshots
screenshots
already should be logging
screenshots is it is it getting cut out
screenshots is it is it getting cut out
somehow it shouldn't be
got infos of MOBA map
right where is the stupid thing
definitely should be rendering by
now oh there we
now oh there we
go so you have MOBA map in Wan
go so you have MOBA map in Wan
B and according to this the dire has
B and according to this the dire has
some
some
levels what's it say is happening
according to this
um here it looks
like
like
five so they're sitting
there how are they getting any XP and
there how are they getting any XP and
stuff if they're sitting there
they look like they're stationary
right how the heck is the dire level
right how the heck is the dire level
mean increasing if they're sitting
there radiant Y is constant this entire
there radiant Y is constant this entire
time rating X is constant this entire
time rating X is constant this entire
time but somehow the level mean goes
up that doesn't make sense right
what gives XP killing stuff getting the
what gives XP killing stuff getting the
last hit on stuff
we have any more
we have any more
maps
no they're just sitting
no they're just sitting
there towers are it looks like the
there towers are it looks like the
minions have
minions have
taken some tier ones
taken some tier ones
maybe minions took some tier
maybe minions took some tier
ones that shouldn't change anything
ones that shouldn't change anything
though right
don't says that the dire y
changed what's it claiming the entropy
changed what's it claiming the entropy
as of this model it's claiming this
as of this model it's claiming this
model actually has some entropy which is
model actually has some entropy which is
just
just
[ __ ] there's no way this model has
[ __ ] there's no way this model has
any entropy
right there's just no
right there's just no
way it's it's completely
way it's it's completely
crashed so I mean something's totally at
crashed so I mean something's totally at
odds here it's saying that this model is
odds here it's saying that this model is
doing stuff it's saying that it's
doing stuff it's saying that it's
leveling up but I'm looking at the model
leveling up but I'm looking at the model
on the map and it's not doing
anything why are we not getting more
anything why are we not getting more
maps
logged radiant level mean shoots way up
Dyer has level 25
somehow doing
what maybe the metrics are broken Maybe
maybe but I need to understand what's
maybe but I need to understand what's
Happening Here I really wish this would
Happening Here I really wish this would
log more frames this is [ __ ] this
log more frames this is [ __ ] this
should be
logging I'm just going to put this in
logging I'm just going to put this in
um I'm just going to put this here
so we're going to just rerun
so we're going to just rerun
this we're going to rerun
this we're going to rerun
this and we're going to at the same time
this and we're going to at the same time
we're going to debug locally and try to
we're going to debug locally and try to
figure out what the hell's happening is
figure out what the hell's happening is
makes absolutely no
makes absolutely no
sense so
we need to go look at Logics as well
let's go check
entropy e
where the
problems this looks like high entropy
problems this looks like high entropy
data
we're going to look at this properly
okay oops no not like this
there pulling out all the stops with the
there pulling out all the stops with the
debugging
debugging
here okay here's our
map got reasonable
map got reasonable
Logics High entropy doesn't mean more
Logics High entropy doesn't mean more
move if it's stuck right well if it's
move if it's stuck right well if it's
stuck then it should be spamming the
stuck then it should be spamming the
same button over and over so it
same button over and over so it
shouldn't be able to have high
entropy if it's stuck it shouldn't be
entropy if it's stuck it shouldn't be
able to have high
able to have high
entropy so let's
entropy so let's
see what these agents
do okay look at this right there they're
do okay look at this right there they're
getting themselves stuck right they
getting themselves stuck right they
haven't
moved doesn't look
possible we'll give it a few more time
possible we'll give it a few more time
steps for them to stabilize maybe it's
steps for them to stabilize maybe it's
funny how one of the agents actually
funny how one of the agents actually
escapes or two of them look like they
escapes or two of them look like they
escape
there are a couple dire agents that are
there are a couple dire agents that are
kind of doing
stuff that's funny they're doing stuff
stuff that's funny they're doing stuff
right not really they're now
right not really they're now
stuck no this one is oh they're fighting
stuck no this one is oh they're fighting
the neutral you see it
now they're going to get merked cuz they
now they're going to get merked cuz they
aggro three
aggro three
neutrals um but let's look at the logits
neutrals um but let's look at the logits
because it says they all have high
because it says they all have high
entropy which shouldn't be possible here
right they're very suspiciously similar
right they're very suspiciously similar
as
as
well this last one has quite different
well this last one has quite different
Logics probably these guys over here
so we've got log
probs do we not have normalized logits
log
prob where's action come
from
51 multinomial logits of probs
see a lot of fives and
see a lot of fives and
[Music]
[Music]
ones are they spamming two keys instead
ones are they spamming two keys instead
of one is that how it's
of one is that how it's
working five and one so 0o one two three
working five and one so 0o one two three
four five yeah Z one
four five yeah Z one
yeah I see an eight in
yeah I see an eight in
here and then zero is only for this guy
here and then zero is only for this guy
this is a weird
really doesn't make any
really doesn't make any
sense they would be doing
this should be an incredibly easy to
this should be an incredibly easy to
learn
learn
task they're not learning
it we're seeing metrics that indicate
it we're seeing metrics that indicate
that they're playing the game at some
that they're playing the game at some
points
goes into a corner where only four moves
goes into a corner where only four moves
are valid and the other four moves don't
are valid and the other four moves don't
work anymore May because it's stuck four
work anymore May because it's stuck four
valid moves it's giving High ENT yeah so
valid moves it's giving High ENT yeah so
that's what I think is happening
potentially but then the question is why
potentially but then the question is why
is it going into a
is it going into a
corner has easily available reward right
has incredibly easily available
has incredibly easily available
reward maybe this run will tell us
something here's our mobile
map so start our
training they're stuck
training they're stuck
there they're still stuck
there they're still stuck
there three are stuck there where' the
there three are stuck there where' the
fourth one
fourth one
go one of them must have
go one of them must have
moved I don't see
it it's still
stuck they all look pretty
stuck they all look pretty
stuck very weird
m
I'm trying to think what I'm missing
here here's one of the
here here's one of the
agents no wait this is a
neutral and this says what is what does
neutral and this says what is what does
it say it's
it say it's
getting it's actually says it's getting
getting it's actually says it's getting
some reward
some reward
now and it has
now and it has
levels so basically this thing says it's
levels so basically this thing says it's
winning and and I look at the map and it
winning and and I look at the map and it
looks like it's not winning
so
so
weird so incredibly
weird so incredibly
weird
weird
well got a few options left available to
well got a few options left available to
us
are you handling situation where an
are you handling situation where an
agent move would cause collision with
agent move would cause collision with
another agent
another agent
yes could that be impacting their valid
yes could that be impacting their valid
move set yes they can't move into each
other I mean we should even be able to
other I mean we should even be able to
here if I just set this to
human right
uh whoops we gotta get out of get rid of
uh whoops we gotta get out of get rid of
this break
point yeah so we'll get let's go play
point yeah so we'll get let's go play
the game with them let's go play our
the game with them let's go play our
game with our stupid teammates here
game with our stupid teammates here
they're really
they're really
stupid o setting an array element with a
stupid o setting an array element with a
sequence right we haven't tested since
sequence right we haven't tested since
this
thing GG
action okay
action okay
so
so
oops here's our
game can you run into a wall and see if
game can you run into a wall and see if
you're getting any reward yeah that's a
you're getting any reward yeah that's a
good
good
idea here let's do
idea here let's do
that so what we're going to do
is okay we're going to print the first
reward I did something like this
reward I did something like this
before okay so we get zero reward
before okay so we get zero reward
negative 05 there for the move away from
negative 05 there for the move away from
the ancient
right that's a
get can't log that
okay I'm like wondering if we're
okay I'm like wondering if we're
maximizing like it looks like they're
maximizing like it looks like they're
training to do the opposite thing but
training to do the opposite thing but
they're
they're
not so here ne0 if I keep running into a
not so here ne0 if I keep running into a
wall Zer if I go this way I get
0.105 literally all you got to do is run
0.105 literally all you got to do is run
down mid for reward
looks reasonable
right just run it down mid see I get all
right just run it down mid see I get all
the way there
hey puffers what's up why you all
hey puffers what's up why you all
chilling over
there I'm in your
there I'm in your
fountain what's
fountain what's
up right
and then if I suicide here I can get
and then if I suicide here I can get
more
reward pretty weird
right looks very
right looks very
simple the observations would have to be
simple the observations would have to be
like completely wrong but we checked the
like completely wrong but we checked the
observations and they looked good
no reward if you move again yeah there's
no reward if you move again yeah there's
no reward there what if you increas the
no reward there what if you increas the
reward for going
reward for going
mid I can do that
let's get rid of the XP
reward okay so we just remove the XP
reward okay so we just remove the XP
reward um and we want to increase the
reward um and we want to increase the
reward for going
mid one times distance yeah
tempted to go back to the larger scale
tempted to go back to the larger scale
version of
it I guess this is fine for now though
the hell
could it be due to the loss of entropy
could it be due to the loss of entropy
is so
high we can try that
high we can try that
next we have a pretty big entropy bonus
next we have a pretty big entropy bonus
on this
well this is broken because if it's
well this is broken because if it's
getting zero reward on an easy task that
getting zero reward on an easy task that
literally requires one step
literally requires one step
competence that's
broken um
I don't know 05 entropy coefficient is
I don't know 05 entropy coefficient is
the highest I've ever even seen used
how's it inst like instantly go to zero
how's it inst like instantly go to zero
reward like
reward like
this I like how the hell is this even
this I like how the hell is this even
it's like are they minimizing reward
me try one more
thing let me try one more
thing let me try one more
thing one other idea
one more thing and then I'll take more
one more thing and then I'll take more
suggestions this will be like a
suggestions this will be like a
two-minute
test it's going to be something stupid
test it's going to be something stupid
I'm sure
I want to make sure I'm not desyncing
I want to make sure I'm not desyncing
the reward values
[Music]
um
well ain't that
funny so these guys think they're
funny so these guys think they're
getting reward for whatever this dumb
getting reward for whatever this dumb
thing they're doing
thing they're doing
is
is well wait they're actually are they
is well wait they're actually are they
doing different stuff
doing different stuff
now really
now really
right give him a
second rewards going up according to to
this are they playing the
this are they playing the
game are they suddenly playing the game
game are they suddenly playing the game
though I didn't change anything I just
though I didn't change anything I just
changed the log parameter
where are
they according to this they think they
they according to this they think they
are doing
well I don't I really really doubt I
well I don't I really really doubt I
have it set to minimize instead of
have it set to minimize instead of
maximize
maximize
I usually don't screw up that
badly right now they only get get a
badly right now they only get get a
reward for walking towards the enemy and
reward for walking towards the enemy and
it looks like they're actually doing
stuff well radiant
stuff well radiant
is I didn't change anything so that's
weird yeah r actually doing
stuff how's it this
stuff how's it this
variable
variable
wait it should not be able to be this
variable unless they're actually winning
variable unless they're actually winning
games it's possible they winning
games games
one I don't know what's making them
one I don't know what's making them
suddenly do
suddenly do
this
like oh yeah they're winning
games but I didn't change anything just
games but I didn't change anything just
then all I changed was a log parameter
doesn't explain why dire is sitting
there well they're not sitting there now
there well they're not sitting there now
according to
this now dire's doing
stuff this says dire doing
stuff this says dire doing
stuff dire level mean is going um
very
very
bizarre we're going to have to look at
bizarre we're going to have to look at
we're going to have to watch
this so the reward spikes up at the end
this so the reward spikes up at the end
above 0.
above 0.
five
five
um assumedly ders learn something by the
um assumedly ders learn something by the
end of this
maybe very
Jank shoot it's already
Jank shoot it's already
7:30 I go get dinner
soon not as productive as the day as I'd
hoped
e e
why is this so
why is this so
weird lant
let's
let's
see does this
work uh
forgot to make that one
change turn
action we will follow the puffers into
battle
what um
why is it super
why is it super
fast
fast
oh that's
why follow the puffers into
battle these puff this puffer goes into
battle uh this puffer goes into battle
battle uh this puffer goes into battle
and gets
and gets
merked this puffer pulls a bunch of a
that puffer is not particularly
smart um that's real
smart um that's real
weird and then we got still we still
weird and then we got still we still
have puffer stuck
here I don't know what they think
here I don't know what they think
they're doing
apparently some of them do eventually go
apparently some of them do eventually go
through like
this and I guess they eventually get XP
this and I guess they eventually get XP
off of the
Creeps it's not exactly
Creeps it's not exactly
good where are dire
good where are dire
Puffs don't kill me I need to go see the
Puffs don't kill me I need to go see the
dire puffs
I have to go very fun to
I have to go very fun to
watch thanks for stopping
by oh look they went over to
here and these ones they go mid dire is
here and these ones they go mid dire is
now going
now going
mid log Pro bug you
mentioned I don't know what the heck
mentioned I don't know what the heck
like look these guys are just chilling
like look these guys are just chilling
over here
over here
now and then like this one is stuck over
here it's very
here it's very
weird I also want to go look at the uh
weird I also want to go look at the uh
the reward bug
because this doesn't look like it's
because this doesn't look like it's
getting good reward right
this model thinks it's doing very well
the self buff rewards
the self buff rewards
right when you reset you give it this
right when you reset you give it this
slice of self. buff.
slice of self. buff.
rewards pointer through
rewards pointer through
end and you reset
it
and sum rewards. append
I was already doing this
correctly so somehow this latest run
correctly so somehow this latest run
just changed
it entropy or something it shouldn't be
it entropy or something it shouldn't be
entropy they real
stupid I really don't know what the hell
stupid I really don't know what the hell
is wrong with
this and I need to go grab dinner in a
this and I need to go grab dinner in a
second let me just think if there's
second let me just think if there's
anything else I can run to think about
anything else I can run to think about
like what the heck this would be
this got to be a
bug
e
e
e e
last thing I'm trying before
dinner e
where are the
stats there we
go still no
stats that's kind of ridiculous
zero reward
low
entropy
bizarre okay well I'm gonna have to
bizarre okay well I'm gonna have to
continue on this tomorrow here it goes
continue on this tomorrow here it goes
um yeah zero reward so I'm going to have
um yeah zero reward so I'm going to have
to continue on this
to continue on this
tomorrow um this is very
tomorrow um this is very
odd I'm sure this is like some dumb data
odd I'm sure this is like some dumb data
problem or something because
problem or something because
uh this is like the easiest possible
uh this is like the easiest possible
setting to
setting to
solve um this is one of the things
solve um this is one of the things
really hard about RL is trying to debug
really hard about RL is trying to debug
like this much deeper stack than the
like this much deeper stack than the
rest of
rest of
AI
AI
so I'm going to be working on this for
so I'm going to be working on this for
as long as it takes to get this set
as long as it takes to get this set
up um but for now I'm kind of burnt so
up um but for now I'm kind of burnt so
I'm going to come back fresh
I'm going to come back fresh
tomorrow uh and I'm going to see if I
tomorrow uh and I'm going to see if I
can figure out how to like at least
can figure out how to like at least
partially automate this
partially automate this
process I might see if I can run a
process I might see if I can run a
little sweep or
little sweep or
something do I want to try to run a
something do I want to try to run a
sweep right
now not really I I think that there's
now not really I I think that there's
something like fundamentally freaking
something like fundamentally freaking
wrong with this
wrong with this
yeah so I'm going to make I'm going to
yeah so I'm going to make I'm going to
grab some like
grab some like
really really robust hyper prams and
really really robust hyper prams and
stuff from some other places um I'm
stuff from some other places um I'm
going to match like the grid environment
going to match like the grid environment
because this basically the same task and
because this basically the same task and
uh I'm going to go from there but we'll
uh I'm going to go from there but we'll
be back
tomorrow for uh folks that are new
tomorrow for uh folks that are new
around
here do feed the puffer the puffer
here do feed the puffer the puffer
consists uh has a you know a very nice
consists uh has a you know a very nice
diet of
diet of
stars so all this stuff is open source
stars so all this stuff is open source
if you want to help me out just start
if you want to help me out just start
the
the
repo I post all this stuff on X here
repo I post all this stuff on X here
I also when I go live it's notified here
I also when I go live it's notified here
as well U but it's all just RL and yeah
as well U but it's all just RL and yeah
you can follow this
you can follow this
here and it helps me out so thank you
here and it helps me out so thank you
very much and I will be back tomorrow
very much and I will be back tomorrow
bye
