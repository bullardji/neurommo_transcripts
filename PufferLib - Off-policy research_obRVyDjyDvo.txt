Kind: captions
Language: en
Okay,
Okay,
we are back live
we are back live
morning.
A whole bunch of work to do today.
Maybe I should do
Maybe I should do
maybe I should do a quick little
maybe I should do a quick little
overview
overview
just to get my thoughts clear because
just to get my thoughts clear because
I've been thinking about this pretty
I've been thinking about this pretty
much non-stop.
much non-stop.
Um,
Um,
so there's all this off policy
so there's all this off policy
literature, right, that's built around
literature, right, that's built around
sample efficiency
sample efficiency
and there's on policy literature that's
and there's on policy literature that's
built around compute speed, compute
built around compute speed, compute
efficiency. Um, and the thing that's
efficiency. Um, and the thing that's
difficult to say is whether you can make
difficult to say is whether you can make
the on the off policy methods actually
the on the off policy methods actually
work in the high data regime well or
work in the high data regime well or
not.
not.
That's the thing that's tricky.
That's the thing that's tricky.
So, I'm trying to bring these two
So, I'm trying to bring these two
together in a way that you can kind of
together in a way that you can kind of
do both.
do both.
Welcome night in Vietnam.
I've never been to Vietnam. I do know
I've never been to Vietnam. I do know
that you guys have good food there.
One off.
I'm going to try one
I'm going to try one
one small change I think
before I
Okay. So, at least in this setup,
Okay. So, at least in this setup,
um, this doesn't make a difference.
um, this doesn't make a difference.
Seems
just makes it slower.
Of
course, it's like now we have a very
course, it's like now we have a very
aggressively tuned baseline.
If I try the uh the on policy baseline,
If I try the uh the on policy baseline,
right, we have this super aggressively
right, we have this super aggressively
tuned baseline
where we literally have the fastest
where we literally have the fastest
training in the world on that.
Kind of ridiculous to see, frankly.
If I do
If I do
I just cut the number of environments in
I just cut the number of environments in
half, right?
What happens?
Welcome to Nash.
Yeah. So, here's the problem, right?
Yeah. So, here's the problem, right?
Because the baseline is so aggressively
Because the baseline is so aggressively
tuned, this actually is going to do
tuned, this actually is going to do
worse.
The whole problem with RL research in a
The whole problem with RL research in a
nutshell, right?
The batch size the batch size gets
The batch size the batch size gets
smaller.
smaller.
The mini batch size doesn't though
The mini batch size doesn't though
actually.
So why do we think that this gets worse?
It should be better, shouldn't it?
Reduce your number of environments.
Got to just be something weird with the
Got to just be something weird with the
hypers. No.
Okay. Like if we think of it that way.
Actually, you know what? I have a
Actually, you know what? I have a
question. I have a question that I want
question. I have a question that I want
answered.
answered.
Let's see how fiddly RL is.
Let's see how fiddly RL is.
Um, when we just don't have
Um, when we just don't have
let's see like basically what happens to
let's see like basically what happens to
reinforcement learning with our current
reinforcement learning with our current
algorithm if we go to unoptimized dummy
algorithm if we go to unoptimized dummy
hyperparameters.
hyperparameters.
I want to get a sense of that problem.
Yes.
where they have their hypers.
I just want to see what happens if we do
I just want to see what happens if we do
this.
Like what happens if we basically So
Like what happens if we basically So
this is how most RL research is done
this is how most RL research is done
these days, right? Is people will just
these days, right? Is people will just
use like defaults
I mean, look at like it actually does do
I mean, look at like it actually does do
reasonably well. It's a huge difference,
reasonably well. It's a huge difference,
right? So, at least we have stuff that's
right? So, at least we have stuff that's
stable enough that we can do that and
stable enough that we can do that and
have it not break. I'm back. Welcome,
have it not break. I'm back. Welcome,
Rishi.
Okay. So, at least we expect
Okay. So, at least we expect
like some form of stability with this.
like some form of stability with this.
Okay. So, this is 500
Okay. So, this is 500
591.
591.
So, what happens if I just This should
So, what happens if I just This should
be more
be more
uh this should be like what smaller
uh this should be like what smaller
batches.
batches.
This should be more fresh data, more
This should be more fresh data, more
games.
games.
Let's see what happens.
like really unstable, huh?
And it's a little bit worse, though
And it's a little bit worse, though
probably not statistically. So,
probably not statistically. So,
that's very weird to me because
Yeah, that's super weird, isn't it?
You're getting more fresh games.
like more stable.
like more stable.
Oh yeah, look at that. Okay,
Oh yeah, look at that. Okay,
so there is something to the um the
so there is something to the um the
batch size.
probably something to do with just the
probably something to do with just the
changing of uh of the LSTM state now.
Of course, you can't really learn
Of course, you can't really learn
anything with this stupid tiny network.
I wonder if there is something
I wonder if there is something
something to do with the LSTM.
Well, yeah, actually you do cuz if you
Well, yeah, actually you do cuz if you
don't have we don't have frame stacking,
don't have we don't have frame stacking,
right? So, we kind of just need the LSTM
right? So, we kind of just need the LSTM
to be a thing.
Okay. So, actually that is better,
Okay. So, actually that is better,
right?
We had to increase the uh the horizon a
We had to increase the uh the horizon a
whole bunch
whole bunch
to make use of fewer ends.
to make use of fewer ends.
I wonder if now,
I wonder if now,
hang on, if I can go to one environment
hang on, if I can go to one environment
and then leave the horizon at 256.
Super long roll outs.
Okay, you kind of can.
So now
So now
now my question is
now my question is
does this let you increase update epochs
does this let you increase update epochs
or we just still in this case where
or we just still in this case where
towel has to be super on policy.
Okay, so
Okay, so
we increase sample efficiency a fair bit
we increase sample efficiency a fair bit
with default hypers
with default hypers
in just a few minutes of playing with
in just a few minutes of playing with
this, right?
a stable solve as well.
Just confirm that this one doesn't work.
But I suppose what is the
But I suppose what is the
what is the question
what is the question
from this?
Yeah. See, this is worse than before by
Yeah. See, this is worse than before by
a lot.
It still actually does like pretty much.
It still actually does like pretty much.
Yeah, that's a solve. But
Yeah, that's a solve. But
I think you're messing with the LSTM
I think you're messing with the LSTM
state too often is the problem
more than anything.
If I really wanted to crank sample
If I really wanted to crank sample
efficiency, how would I do it
this
this
just going to mess it up more. No,
probably better to do
Something like this.
How many M's do?
This would have to be an artifact. Well,
This would have to be an artifact. Well,
no, it's not just an artifact of the
no, it's not just an artifact of the
LSTM.
LSTM.
You also do need the bootstrapping term.
Lucky our implementation is so fast.
Oh, I suppose can we like just crank up
Oh, I suppose can we like just crank up
the number of updates even more
question. Is it possible to place a flag
question. Is it possible to place a flag
on an env
on an env
flag and building two-player turnbased
flag and building two-player turnbased
game
game
against other eval
against other eval
beta
beta
uh just mess with the clean puffer file
uh just mess with the clean puffer file
directly like in the eval loop there's
directly like in the eval loop there's
um you know there's a separate eval
um you know there's a separate eval
thing like it's I think the function's
thing like it's I think the function's
called roll out or whatever
called roll out or whatever
we still have a rollout function
Okay. No, it's this one right here. This
Okay. No, it's this one right here. This
eval function. It just has the loop
eval function. It just has the loop
directly. Just load your model in right
directly. Just load your model in right
here.
here.
Like a few line code Change.
Yeah, this is way worse, right?
Yeah, this is way worse, right?
I guess we'll see if it um
I guess we'll see if it um
it stabilizes,
it stabilizes,
but it's definitely not like
This maybe is just way too many gradient
This maybe is just way too many gradient
steps though.
steps though.
We can't do this many
this. Yeah. 128 m.
Yeah. So this is how you make everything
Yeah. So this is how you make everything
super flow.
Yeah, there's definitely some form of
Yeah, there's definitely some form of
something like this, right? Of course,
something like this, right? Of course,
it's going to nan out.
Got to like there's some form of this.
Got to like there's some form of this.
I'm sure
The problem is it's fundamentally
The problem is it's fundamentally
at least for the on policy case it's a
at least for the on policy case it's a
hardware inefficient setup
honestly
though. like
Where's my advantage?
They're just running like
I'm trying to think where I take this
I'm trying to think where I take this
from here. Right.
the problem with how fundamentally
the problem with how fundamentally
hardware inefficient a lot of these like
hardware inefficient a lot of these like
sample efficient focused methods are.
sample efficient focused methods are.
Like you literally may as well not use a
Like you literally may as well not use a
GPU for most of them. Of course, we nan
GPU for most of them. Of course, we nan
out still on this
The best one was um
this
still working on advantage. I'm trying
still working on advantage. I'm trying
to figure out I mean I'm generally
to figure out I mean I'm generally
trying to figure out how we bridge
trying to figure out how we bridge
computational efficiency and sample
computational efficiency and sample
efficiency.
I mean the behavioral clothing thing so
I mean the behavioral clothing thing so
far
I actually did run a sweep on it
I actually did run a sweep on it
overnight. So, this has mostly solved
overnight. So, this has mostly solved
the task. And this is fewer samples with
the task. And this is fewer samples with
untuned hypers um than my fast method
untuned hypers um than my fast method
with tuned hypers and about twice the
with tuned hypers and about twice the
time, which is not terrible.
Let me show you guys a cool result I got
Let me show you guys a cool result I got
last night.
last night.
So, I just reran the um
So, I just reran the um
I just reran the sweep right on the uh
I just reran the sweep right on the uh
the method we were developing yesterday,
the method we were developing yesterday,
which is this like advantage focus
which is this like advantage focus
behavioral cloning thing.
So, we got up to what is this? 300 and
So, we got up to what is this? 300 and
something. A bit over 300 score.
It's actually more samples than before.
Not great.
I mean it does learn though.
thing is we still have not found
we haven't found any version of this
we haven't found any version of this
that really lets us like crank on
that really lets us like crank on
on the compute.
We could go grab those hypers and see if
We could go grab those hypers and see if
we can get this to solve. That might be
we can get this to solve. That might be
a productive thing to do.
Okay. Well, we understand with this
Okay. Well, we understand with this
experiment, right?
We understand with this experiment that
We understand with this experiment that
our current on Paul's pretty robust and
our current on Paul's pretty robust and
we can make it at least probably like at
we can make it at least probably like at
least a factor of two more sample
least a factor of two more sample
efficient
efficient
already if we want to.
Then we have our onoff paw thing here
Then we have our onoff paw thing here
which is in
which is in
pretty much the same amount of time as
pretty much the same amount of time as
before with the untuned one. It gets 200
before with the untuned one. It gets 200
something score. And then there's a
something score. And then there's a
tuned version that trains for longer
tuned version that trains for longer
that can get over 300.
What happens if I go to two
What happens if I go to two
environments,
environments,
make no other changes? This.
This is supposed to be more data
This is supposed to be more data
scalable, isn't it?
Yeah, but just doing this doesn't make
Yeah, but just doing this doesn't make
it better.
Fusion.
Fusion.
Ow.
Ow.
How does that specifically help the
How does that specifically help the
sample efficiency compute trade-off
sample efficiency compute trade-off
thing?
But interestingly, this doesn't help.
Oh, wait. This is total
Oh, wait. This is total
mini batches.
So,
so then this shouldn't have done better.
so then this shouldn't have done better.
But wait, if I go to this is two
But wait, if I go to this is two
and then I go to this is four. Does this
and then I go to this is four. Does this
do anything?
The correct version of this I think
The correct version of this I think
loads the learning portion of the um the
loads the learning portion of the um the
training like way more heavily, right?
training like way more heavily, right?
That you like you can make the the
That you like you can make the the
sample collection portion will become
sample collection portion will become
less efficient fundamentally,
less efficient fundamentally,
but the hope is that you make up for it
but the hope is that you make up for it
by being compute efficient on learning.
This
else like
Are you right?
That's way worse. Really?
That I would have expected to be way
That I would have expected to be way
better.
was probably just super heavily
was probably just super heavily
overtuned, right?
You're better than this. Really?
I mean, there's no reason to expect like
I mean, there's no reason to expect like
the
the
basic Q-learning version of this to work
basic Q-learning version of this to work
either, right?
Does it make sense to go play with um
Does it make sense to go play with um
the BTR?
implementation of this.
Yeah,
Yeah,
I think it's just really hard to beat
I think it's just really hard to beat
this on policy like a good on policy
this on policy like a good on policy
baseline, right?
I guess this would be
would the other angle that I have here
would the other angle that I have here
be to just try to make my on policy
be to just try to make my on policy
baseline
baseline
um
um
more sample efficient
like bigger architecture and stuff. The
like bigger architecture and stuff. The
the problem here is that I've seen the
the problem here is that I've seen the
fundamental capability
like you need to be able to reuse some
like you need to be able to reuse some
samples
models
models
in behavioral cloning.
in behavioral cloning.
The model would be an independent
The model would be an independent
an independent improvement, wouldn't it?
The thing that I'm stuck on is how do
The thing that I'm stuck on is how do
you reuse samples
you reuse samples
without changing the whole algorithm to
without changing the whole algorithm to
make it slow?
Maybe we do just do the thing that I
Maybe we do just do the thing that I
planned the whole time before
which is we do this whole thing twice.
No, we're not using expert data. That's
No, we're not using expert data. That's
the point.
the point.
It's very easy with expert data.
We're trying to come up with um a
We're trying to come up with um a
formulation of behavioral cloning that
formulation of behavioral cloning that
works online. And the thing is that this
works online. And the thing is that this
actually
actually
the thing that's crazy is that this
the thing that's crazy is that this
actually kind of does work.
actually kind of does work.
I guess the thing we don't know is
No, we do know actually that this works.
No, we do know actually that this works.
This works better because we're reusing
This works better because we're reusing
data. We know that.
I could technically mix them.
Hey, Kvert.
What if instead of doing them
What if instead of doing them
separately, what if I mix them?
separately, what if I mix them?
That make any sense?
I think it does, right?
Let me think about that.
Okay, I have an idea.
This goes
at the bottom.
All right.
Now,
All right.
So this should be similar.
And now we should be able to play with
And now we should be able to play with
um
um
with one additional thing.
I'm basically just going to see what
I'm basically just going to see what
happens if I mix on and off Paul data.
We store it by value. We sample by
We store it by value. We sample by
advantage.
starts off his ones.
Do you see what I'm trying to do here,
Do you see what I'm trying to do here,
though?
And I honestly wouldn't even be
And I honestly wouldn't even be
surprised
surprised
if there's nothing magical about like
if there's nothing magical about like
the Q function formulation with offpaul
the Q function formulation with offpaul
both doing the same freaking advantage
both doing the same freaking advantage
estimate.
Yeah.
Okay. So, somehow this got to be
probably because I didn't do it down
probably because I didn't do it down
here, right?
Okay,
Okay,
so this should be
so this should be
again roughly the same algorithm, right?
Yes, this is roughly the same algorithm
Yes, this is roughly the same algorithm
from before.
And now
And now
let's see about the uh the losses that
let's see about the uh the losses that
we've got.
actually grab our policy loss.
Take this one here. Yeah.
This is the
policy gradient formulation. Right?
This doesn't learn anything.
Make sure I haven't broken something
Make sure I haven't broken something
yet.
I did break something else here, right?
I did break something else here, right?
Because otherwise
Because otherwise
otherwise this would work.
Changing robot from step controls to
Changing robot from step controls to
trajectory.
trajectory.
What do you mean changing to trajectory?
What do you mean changing to trajectory?
You don't want to
as an issue one control sequence for the
as an issue one control sequence for the
whole trajectory. You definitely don't
whole trajectory. You definitely don't
want to do that.
want to do that.
That's like more classic control.
What did I screw up here?
Change the return.
Change the return.
Oh.
Did the clip loss?
Do I go back yet again here?
Oh,
yes.
Hang on.
Hang on.
Something's super screwy here. Now,
This must have copied something wrong.
This must have copied something wrong.
Hang on.
[Music]
[Music]
What did I screw up from before?
This should be the on data, right?
This needs to be abs for
minus vantage.
Oh, wait. This is not
Oh, wait. This is not
Yeah, this is not getting optimized
the ratio that needs
this.
Okay, because this is just our on policy
Okay, because this is just our on policy
RL now here, right?
Yeah, this is just on policy RL.
Yeah, this is just on policy RL.
This is our good baseline. We know it
This is our good baseline. We know it
works. yada yada.
What happens if we I'm curious. What
What happens if we I'm curious. What
happens if we just sub this one little
happens if we just sub this one little
advantage calc?
It's funny.
It's funny.
It's actually um massively worse to only
It's actually um massively worse to only
learn from the positive examples, it
learn from the positive examples, it
seems.
All right. So, we'll do
All right. So, we'll do
we'll do we'll do this one.
Now, what happens if I uncomment
I uncomment this? So, we have offpaul
I uncomment this? So, we have offpaul
data.
Looks like we haven't totally broken
Looks like we haven't totally broken
everything, right? It still trains and
everything, right? It still trains and
it trains better than uh before like
it trains better than uh before like
with our pure IIL thing.
with our pure IIL thing.
Okay.
And we have
And we have
where our clipping coefficients in
where our clipping coefficients in
There.
We do want our clipping in here for
We do want our clipping in here for
sure, right? Betrays.
We have our clipping.
We're computing.
We're sampling by absolute advantage.
We're sampling by absolute advantage.
Oh yeah, this it's totally going to
Oh yeah, this it's totally going to
change the um
change the um
it's going to change the hypers here
it's going to change the hypers here
100%. Right.
Like if we wanted something closer,
Like if we wanted something closer,
we would do let's do like a basic one to
we would do let's do like a basic one to
one.
Or actually, it's got to be it's got to
Or actually, it's got to be it's got to
be double, right?
be double, right?
Oops. Not 120.
Thank you, Weston.
Not uh
Not uh
uh hopefully that doesn't open this on
uh hopefully that doesn't open this on
stream.
stream.
Come on, move the window. All right,
Come on, move the window. All right,
fine.
fine.
There's um there's an exploit going
There's um there's an exploit going
around in Discord where I can't actually
around in Discord where I can't actually
ban all the bots.
ban all the bots.
Like they literally don't have
Why don't I have
Oh, wait. I do have banan on this.
Go.
Is it in general as well
Is it in general as well
or did they just post it in dev?
Looks like just dev, right? I got that
Looks like just dev, right? I got that
one.
one.
Thank you, Weston.
Dummies.
Interesting that this is worse.
Interesting that this is worse.
This might just be the point at which we
This might just be the point at which we
need to optimize uh reoptimize hypers
need to optimize uh reoptimize hypers
and such.
I'm trying to think if this
I'm trying to think if this
what do we all think of this though as
what do we all think of this though as
an algorithm like
you just do on policy like you just do
you just do on policy like you just do
your standard RL
your standard RL
you keep an extra uh you keep the extra
you keep an extra uh you keep the extra
replay buffer
replay buffer
and then you trust uh PO clipping
and then you trust uh PO clipping
important sampling and retrace
important sampling and retrace
to keep stuff from going too far off.
That seems reasonable enough to me.
This would kind of be way better as
This would kind of be way better as
well, wouldn't it?
Like if we get this working.
Okay, it doesn't fix the roll out length
Okay, it doesn't fix the roll out length
thingy,
thingy,
but if we get this working
but if we get this working
then actually we can fix quite a few
then actually we can fix quite a few
other things using this.
other things using this.
Yeah. Yeah. No, we can actually fix
Yeah. Yeah. No, we can actually fix
quite a bit of stuff from That's
I like this. I think this is good.
And actually, we should be fine sampling
And actually, we should be fine sampling
absolute advantage like this as well.
absolute advantage like this as well.
Okay. So, let's see what other things I
Okay. So, let's see what other things I
could have screwed up in here.
Math heavy paper.
Math heavy paper.
Conditional important sampling for off
Conditional important sampling for off
policy. Let me look that up.
What the hell? No.
What the hell? No.
Get out, Google.
Welcome spy.
Do they run experiments on anything?
No, just giant math paper.
I understand that, but this is uh
I understand that, but this is uh
this is a research stream.
The vibe is more like uh what you'd find
The vibe is more like uh what you'd find
in an academic lab, man,
but with a heck of a lot more
but with a heck of a lot more
engineering.
Actually,
this might be kind of useful. I'm going
this might be kind of useful. I'm going
to have to read this like fully at some
to have to read this like fully at some
point, but that's this actually might be
point, but that's this actually might be
kind of useful because basically that
kind of useful because basically that
the offpole correction mechanism
the offpole correction mechanism
is the uh the important thing There.
So we would just decouple batch size,
So we would just decouple batch size,
right? To do this efficiently,
right? To do this efficiently,
we would just like decouple the batch
we would just like decouple the batch
Five.
I think we actually have the core idea
I think we actually have the core idea
in here now, don't we?
I do this
good online course way to get up to
good online course way to get up to
speed.
speed.
Uh, no, but I have a guide. I have a
Uh, no, but I have a guide. I have a
guide that links you to a bunch of uh of
guide that links you to a bunch of uh of
good reading resources and things. It's
good reading resources and things. It's
more peacemail. There's no single like
more peacemail. There's no single like
good course.
good course.
I'll link it to you.
If you go through this, you will
If you go through this, you will
actually be qualified to uh if you
actually be qualified to uh if you
actually do this stuff, you will be
actually do this stuff, you will be
qualified to like advance modern
qualified to like advance modern
research and do a whole bunch of
research and do a whole bunch of
applications. This is the exact thing
applications. This is the exact thing
that like our top contributors have
that like our top contributors have
done.
Okay. So here you see right if you don't
Okay. So here you see right if you don't
store any offpole data
store any offpole data
we get the expected result. We can do
we get the expected result. We can do
like anything. We can even like lo use a
like anything. We can even like lo use a
tiny amount of offpaul data.
Literally, even the tiniest amount of
Literally, even the tiniest amount of
off policy data messes stuff up. Huh?
off policy data messes stuff up. Huh?
Like a little bit
when doing algo research is the uptime
when doing algo research is the uptime
your concern? There are a lot of things
your concern? There are a lot of things
I'm looking at. So there is the high
I'm looking at. So there is the high
data regime
data regime
where you're just trying to solve the
where you're just trying to solve the
task in the like lowest compute, lowest
task in the like lowest compute, lowest
wall clock you possibly can. And now
wall clock you possibly can. And now
what I'm trying to do is I'm trying to
what I'm trying to do is I'm trying to
expand puffer to give it more axes on
expand puffer to give it more axes on
which we can crank up compute increase
which we can crank up compute increase
uptime uh but decrease sample usage.
So it's many things
all these freaking bots.
the fact that you add just the tiniest
the fact that you add just the tiniest
bit of data here.
bit of data here.
I should be able to see it even more
I should be able to see it even more
clearly if I do like this, right?
clearly if I do like this, right?
Whoops.
The 4096 samples,
a tiny fraction of a batch
and we don't even get 700 as a result.
and we don't even get 700 as a result.
Okay, let's see if we can figure out why
why this is
we decide to keep data based on value.
we decide to keep data based on value.
Right?
So you have your whole batch
and we've kept it based on on value.
I didn't discount the sum but still
the sum heristic
the sum heristic
and then it's sampled based on
and then it's sampled based on
advantage.
advantage.
So if this data has
So if this data has
this data had low advantage
there shouldn't be any problem.
This does not make sense, right?
This does not make sense, right?
We have this tiny additional amount of
We have this tiny additional amount of
off policy data that we're keeping.
the buffer at the end. So what we do
the buffer at the end. So what we do
here,
here,
so we always keep all the on policy
so we always keep all the on policy
data, right? And then you're also here,
data, right? And then you're also here,
I'll show you this is like a quick hacky
I'll show you this is like a quick hacky
implementation,
implementation,
but we keep all of the on policy data
but we keep all of the on policy data
and then we concatenate it with
and then we concatenate it with
additional data that we have in a
additional data that we have in a
buffer. So the buffer consists of
buffer. So the buffer consists of
however much off-paul data we want to
however much off-paul data we want to
store plus the onpaul data and then at
store plus the onpaul data and then at
the end
we uh we compute the value
we uh we compute the value
where is it?
Yeah. So here we compute the value of
Yeah. So here we compute the value of
all the samples
all the samples
and then we take the top k
and then we take the top k
and we leave these in the buffer. So the
and we leave these in the buffer. So the
idea is that we're keeping around our
idea is that we're keeping around our
highest value bits of data,
but it's a very small amount of data
but it's a very small amount of data
either way, right? And like up top,
either way, right? And like up top,
we're sampling by advantage.
we're sampling by advantage.
So if the data is not useful for
So if the data is not useful for
training, it should just not get
training, it should just not get
sampled.
That actually make a difference or did I
That actually make a difference or did I
just get lucky? Okay.
do one mini batch worth
do one mini batch worth
data.
Now there's like a fair bit of off pole
Now there's like a fair bit of off pole
data.
Okay.
Starts off as one
Good advantage.
This one wrong.
this like way worse. Is this noise?
Why would you update the value as well?
Yeah, you need to update um
Yeah, you need to update um
Oh, so this is going to screw with the
Oh, so this is going to screw with the
value function a whole bunch.
value function a whole bunch.
I see.
What if we do this?
But you want to sample by a fresh
But you want to sample by a fresh
advantage estimate, don't you?
Oh, I see. I think it's cuz I didn't
Oh, I see. I think it's cuz I didn't
maintain two estimates.
Does it matter what the original value
Does it matter what the original value
of the data is?
Like does the value loss clipping
Like does the value loss clipping
matter?
Maybe we can at least
Huh?
fine both ways.
Welcome, O Wayne.
What is the idea here with the clipping
What is the idea here with the clipping
of the value function?
of the value function?
Value minus.
Don't you explicitly not want to do
Don't you explicitly not want to do
that?
that?
I guess it depends what your motive is
I guess it depends what your motive is
here, right?
We also have the
policy clipping. thing.
I mean the clamped ratio is still um
I mean the clamped ratio is still um
still differentiable, right?
Am I wrong here?
Pretty sure the clamp ratio is still
Pretty sure the clamp ratio is still
differentiable.
more frequent updates of
more frequent updates of
Yeah. Um, this is the main thing, right,
Yeah. Um, this is the main thing, right,
is I haven't been able to get a good
is I haven't been able to get a good
setting where we can crank sample reuse
setting where we can crank sample reuse
and really get much out of it.
and really get much out of it.
It's actually quite difficult to get
It's actually quite difficult to get
that to work correctly.
Let's now increase the IL batch size to
Let's now increase the IL batch size to
something more reasonable.
So, we're going to actually do
That's like a reasonable amount of data
That's like a reasonable amount of data
to keep around.
And it looks like it destabilizes
And it looks like it destabilizes
learning a bunch
like fundamentally why right we should
like fundamentally why right we should
be doing
be doing
I guess it's technically possible I have
I guess it's technically possible I have
vrace done wrong
shouldn't be No.
Well, I mean, that's just like you can
Well, I mean, that's just like you can
just throw that out there. What What
just throw that out there. What What
does that mean here? Right.
does that mean here? Right.
We have several different forms of
We have several different forms of
clipping
clipping
that are supposed to prevent offpaul
that are supposed to prevent offpaul
data from hurting us at the very least.
To be fair, this can totally just be um
To be fair, this can totally just be um
this could totally just be the sampling
this could totally just be the sampling
coefficients, right?
coefficients, right?
Yeah, this could totally just be um
Yeah, this could totally just be um
prioritize replay screwing with this.
prioritize replay screwing with this.
Hang on.
This is a problem with fancy algorithms,
This is a problem with fancy algorithms,
right? A whole bunch of different knobs
right? A whole bunch of different knobs
that mess with you.
Okay. So, let's do
for now because we're not going to um
for now because we're not going to um
we're not going to like re- sweep this.
we're not going to like re- sweep this.
Let's do something that's more moderate.
Like
that's like enough data that we should
that's like enough data that we should
Let's see if we see an interference
Let's see if we see an interference
effect.
Okay. So, I mean it hurts learning, but
Okay. So, I mean it hurts learning, but
it's still it's still training like
it's still it's still training like
reasonably.
Let's try a few different things here.
Let's try a few different things here.
So like first
So like first
just set this to zero.
just set this to zero.
Recover our original
our original form here.
or update box.
or update box.
See what happens.
you kind of get the same result, right?
Now what happens?
If this solves,
you know, it really does feel like we're
you know, it really does feel like we're
throwing off our update somehow, right?
It really does feel like we're just
It really does feel like we're just
throwing off our updates. It's
Cuz otherwise this should be stable,
Cuz otherwise this should be stable,
right?
idea is very powerful on its own. I
does this not work
combined with um
Just curious here.
Just curious here.
Does the uh the same thing as I saw
Does the uh the same thing as I saw
before or not?
Yeah. So, this return combat level,
Yeah. So, this return combat level,
right? The thing is with data reuse, you
right? The thing is with data reuse, you
should be able to actually get something
should be able to actually get something
very quickly.
Is it just like overfitting it super
Is it just like overfitting it super
quick or something?
Is it ever a good idea to have learning
Is it ever a good idea to have learning
rate not decay to zero?
rate not decay to zero?
just increase next steps. I mean, there
just increase next steps. I mean, there
are a bunch of different learning rate
are a bunch of different learning rate
schedulers, right? You could mess with
schedulers, right? You could mess with
it. At least from the testing that I
it. At least from the testing that I
did, it seemed like having a learning
did, it seemed like having a learning
rate scheduler was much more important
rate scheduler was much more important
than the exact details of theuler.
Okay, so 20 million steps. We're already
Okay, so 20 million steps. We're already
up to
up to
uh.13
uh.13
013 return combat. So, it's actually
013 return combat. So, it's actually
already learning a little bit, right?
Okay. Okay. So, what I'm curious about
Okay. Okay. So, what I'm curious about
now, if I set this to zero,
now, if I set this to zero,
how's it compare? There.
Okay. Well, I think this is better. Um,
Okay. Well, I think this is better. Um,
this is your proof here.
this is your proof here.
We'll see if this catches up. I don't
We'll see if this catches up. I don't
think so.
Okay. So, this is this is your uh your
Okay. So, this is this is your uh your
proof of this, right? Um
proof of this, right? Um
we ran I think it was 27 mil before.
Give it 30.
Give it 30.
We'll give this one even a little longer
We'll give this one even a little longer
to see
It is actually doing an okay job on its
It is actually doing an okay job on its
own here.
What?
was actually stable and did help a
was actually stable and did help a
little bit here
little bit here
having um some previous data.
having um some previous data.
You would expect it to as well.
And this also this is one of the ones
And this also this is one of the ones
that worked with literally just um full
that worked with literally just um full
off Paul data.
off Paul data.
And this is with just a few samples of
And this is with just a few samples of
it actually as well. I wonder. So, does
it actually as well. I wonder. So, does
it get better or worse?
If I do,
does this get better or worse?
Yeah, that's definitely better.
And it makes sense that it would be
Yeah, it's massively better. Okay.
So, it could be problem dependent here,
So, it could be problem dependent here,
right?
right?
Can definitely be problem dependent.
Can definitely be problem dependent.
Use
Use
a restroom real quick. I'll be right
a restroom real quick. I'll be right
back.
Holy
40 million steps.
40 million steps.
It's already getting combat levels.
Yeah, that's like ridiculously better.
Yeah, that's like ridiculously better.
Okay, so
what do we do with that information?
with breakout. We have like a one one
here. It messes up. It could totally
here. It messes up. It could totally
just be the coefficients.
I don't even have pryo coefficients
I don't even have pryo coefficients
There.
[Music]
[Music]
like really just not stable or happy.
And this is a problem.
Um, we have it so that it's it appears
Um, we have it so that it's it appears
to be doing substantially better on our
to be doing substantially better on our
hard task of neural MMO,
hard task of neural MMO,
but having a replay buffer, it's very
but having a replay buffer, it's very
very fiddly with even like breakout.
Really shouldn't be either.
How stable is breakout generally?
Like if I go to one here to zero
Does breakout work if you just crank up
Does breakout work if you just crank up
to deox?
It should just not pull from the data
It should just not pull from the data
though if it's not good.
like it's uh it's already being sampled
like it's uh it's already being sampled
reasonably
the breakout with just eight update
the breakout with just eight update
epox.
One of the other wonky things about
One of the other wonky things about
Breakout is the episodes are so long.
Breakout is the episodes are so long.
like basically only getting to play one
like basically only getting to play one
full game in the whole training even
full game in the whole training even
though you're training this long
like it does train stably.
But you like almost get no sample
But you like almost get no sample
efficiency improvement. Almost none
efficiency improvement. Almost none
out of eight update epochs.
It is stable though.
And then it hands out. Well, I say that
And then it hands out. Well, I say that
and then it hands out right at the end.
and then it hands out right at the end.
Cool.
Cool.
Just the brink of being stable, I guess.
Just the brink of being stable, I guess.
Okay. What about eight update epochs on
Okay. What about eight update epochs on
um with all this IIL data in here.
Starts off good.
Then it just decides like actually never
Then it just decides like actually never
mind.
Know why?
I could compute like fraction on and off
I could compute like fraction on and off
Paul I guess.
Yeah, it really does get messed up
Yeah, it really does get messed up
though by this oddly enough.
Why?
I think it gets messed up even worse
I think it gets messed up even worse
than if you just give it um if I just
than if you just give it um if I just
give it one epoch, right?
give it one epoch, right?
Believe this is fast and
literally gets messed up
literally gets messed up
worse with more epochs.
Yes.
average samples from 10 epochs ago. Oh,
Oh, also experience replay has this
Oh, also experience replay has this
weird decay,
weird decay,
right? It has like this weird decay
right? It has like this weird decay
where it goes to it tries to go to
where it goes to it tries to go to
uniform by the end of it, doesn't it?
uniform by the end of it, doesn't it?
Yeah. But we don't want that for this.
It
It could totally just be the sampling,
It could totally just be the sampling,
right?
Something tells me it's not just a
Something tells me it's not just a
simple have to reweep, but we can try
simple have to reweep, but we can try
that.
Where's this clip?
I think this is like decent, right?
Give it a fixed reasonable
reasonable buffer size. Yeah,
I guess we basically see by the end of
I guess we basically see by the end of
this
this
does
I mean, it does seem weird to me that
I mean, it does seem weird to me that
it's like that
it's like that
that screwy.
that screwy.
Um,
on the other hand, it could literally
on the other hand, it could literally
just be the uh the prioritize replay,
just be the uh the prioritize replay,
right?
like breakout is kind of just a a pretty
like breakout is kind of just a a pretty
on policy end because
well is it
well is it
not really right? You're kind of doing
not really right? You're kind of doing
always the same thing in response to the
always the same thing in response to the
ball just trying to hit the ball
ball just trying to hit the ball
but the episodes are really long.
[Music]
Okay. Well, the other
Okay. Well, the other
there are so many big problems with
there are so many big problems with
this, man.
I think what's the next productive thing
I think what's the next productive thing
I can do on this?
I can do on this?
It could also work on the new end test
for For whatever reason, it like
for For whatever reason, it like
just don't believe that it's going to be
just don't believe that it's going to be
better if I just switch from this to um
better if I just switch from this to um
a Q-learning based thing, right?
a Q-learning based thing, right?
Like technically I am doing something at
Like technically I am doing something at
this point that is very much a uh
this point that is very much a uh
like an offpaul type thing.
Am I totally wrong to think that
Am I totally wrong to think that
that like if I just go for um
that like if I just go for um
do you learning based thing it's not
do you learning based thing it's not
going to be better.
We did get like
We did get like
the base DQN actually did kind of work.
Well, it worked on the simplest possible
Well, it worked on the simplest possible
problems.
I guess I could do that next.
I guess I could do that next.
And realistically, I I'm not going to
And realistically, I I'm not going to
sit here and wait for a sweep to run
sit here and wait for a sweep to run
today. I'm going to actually I'm going
today. I'm going to actually I'm going
to end up overnighting it. So, I may as
to end up overnighting it. So, I may as
well take a quick look and see if this
well take a quick look and see if this
happen to do. Yeah. So, like you do
happen to do. Yeah. So, like you do
something, but so far you don't really
something, but so far you don't really
do anything amazing.
I guess the next thing to look at would
I guess the next thing to look at would
be
be
just
just
forms of Q-learning.
What does SACE do that's different?
I'm just going to look at the clean RL.
This has SACE, right?
the triad paper.
I looked at this. Yeah.
Yeah. So this is this I understand this
Yeah. So this is this I understand this
is the key thing here right the problem
is the key thing here right the problem
is that the off policy methods the
is that the off policy methods the
theoretical guarantees are there um
theoretical guarantees are there um
without the multi-step bootstrapping
without the multi-step bootstrapping
bootstrapping but the algorithms just
bootstrapping but the algorithms just
suck
suck
and then when you start bootstrapping
and then when you start bootstrapping
you have to do all this off policy
you have to do all this off policy
correction crap that's not actually
correction crap that's not actually
making use of the data.
making use of the data.
So essentially you just you end up with
So essentially you just you end up with
a algorithm that can theoretically reuse
a algorithm that can theoretically reuse
data but is just worse than your on
data but is just worse than your on
policy algorithm.
That's the big problem as far as I've
That's the big problem as far as I've
seen it.
Right.
Right.
That's the big problem I've seen.
That's the big problem I've seen.
Does SACE have a smart way of dealing
Does SACE have a smart way of dealing
with this? I've never used SACE at all.
Okay. So, this actually does give you a
Okay. So, this actually does give you a
nice sample base thing, right?
Or freaking networks is obscene.
Is this literally a one step
on one step bootstrap?
It's literally a onestep bootstrap.
Maybe your problem would be
wrong prioritization.
wrong prioritization.
Yeah, it can as well.
I guess it's kind of crazy that SACE
I guess it's kind of crazy that SACE
even works, right?
even works, right?
I didn't realize SACE, they literally
I didn't realize SACE, they literally
have a onestep bootstrap in this thing,
have a onestep bootstrap in this thing,
right?
Could I not make a version of this like
Could I not make a version of this like
really fast?
If it's literally a one-step bootstrap,
If it's literally a one-step bootstrap,
right?
Why don't we just go grab this thing,
Why don't we just go grab this thing,
right?
I'm kind of curious.
I'm kind of curious.
is to like
I'm kind of curious.
Can you not mess with my torch version
EQN plus prioritize play
EQN plus prioritize play
single experience level.
I don't think that does work
I don't think that does work
phenomenally good. I think that's like a
phenomenally good. I think that's like a
super like algorithm, isn't it?
super like algorithm, isn't it?
Literally, you have baselines for that
Literally, you have baselines for that
stuff.
Like if rainbow and po are supposed to
Like if rainbow and po are supposed to
be competitive,
be competitive,
prioritize replay is like one of the
prioritize replay is like one of the
tricks that like is it's way worse
tricks that like is it's way worse
without. There are other tricks that
without. There are other tricks that
it's way worse without as well. This is
it's way worse without as well. This is
the problem is you actually need like
the problem is you actually need like
two or three major improvements before
two or three major improvements before
you even get close to um like pretty
you even get close to um like pretty
basic on policy methods.
Nothing close to PE but for off policy
Nothing close to PE but for off policy
it's good. Well, that's a useless
it's good. Well, that's a useless
algorithm though, right?
Like if you're underperforming base PO,
Like if you're underperforming base PO,
that's a useless algorithm.
And the problem is that there are a lot
And the problem is that there are a lot
of useless algorithms in off policy. And
of useless algorithms in off policy. And
you actually have to bolt a bunch of
you actually have to bolt a bunch of
crap onto the base things before you get
crap onto the base things before you get
something useful. Like here,
something useful. Like here,
this thing actually looks kind of good.
Okay, so this takes
Okay, so this takes
This takes rainbow, which is already a
This takes rainbow, which is already a
kitchen sink of random techniques, and
kitchen sink of random techniques, and
bolts on six more techniques. It removes
bolts on six more techniques. It removes
two. So, it brings you to 10 additions.
two. So, it brings you to 10 additions.
You have 10 additions to base DQN before
You have 10 additions to base DQN before
it's a usably good algorithm.
Wait, is this a single N
Is this literally intended for one
Is this literally intended for one
environment?
environment?
You've got to be kidding me.
You've got to be kidding me.
SACE, it's literally set up for one
SACE, it's literally set up for one
environment. That is the dumbest thing
environment. That is the dumbest thing
I've seen in a long time.
That's like crazy.
I wonder if their implementation even
I wonder if their implementation even
works for batched um
works for batched um
batched environments.
It looks like they're supposed to
Why does it always insert a random ass
Why does it always insert a random ass
character when I open a file? Drives me
character when I open a file? Drives me
insane.
All right.
doesn't print reward or anything.
It gets slower and slower and doesn't
It gets slower and slower and doesn't
print reward or anything.
Oh, I see.
Hang on.
Yeah, they also they assume a specific
Yeah, they also they assume a specific
format of stuff.
I honestly don't think this is going to
I honestly don't think this is going to
do anything. Like it's just
do anything. Like it's just
it's such a screwy regime of algorithms.
it's such a screwy regime of algorithms.
Like the majority of RL is just in such
Like the majority of RL is just in such
a screwy regime. It's like I forget that
a screwy regime. It's like I forget that
it's I always say, you know, puffer is a
it's I always say, you know, puffer is a
thousand times faster. Puffer thousand
thousand times faster. Puffer thousand
is a thousand times faster. It sounds
is a thousand times faster. It sounds
like a marketing ploy, but like you
like a marketing ploy, but like you
literally just run defaults on stuff and
literally just run defaults on stuff and
it's actually a thousand times faster.
it's actually a thousand times faster.
So, I don't
That's next. Yeah, exactly. That's the
That's next. Yeah, exactly. That's the
point. You can't It killed the entire
point. You can't It killed the entire
field and everybody went over to work on
field and everybody went over to work on
language models because you can't get
language models because you can't get
anything done at that speed.
anything done at that speed.
Now try to make it a thousand times
Now try to make it a thousand times
slower and have 10 times more code
slower and have 10 times more code
because no researchers can write code.
because no researchers can write code.
Like clean is the best thing out there
Like clean is the best thing out there
by a mile. It's actually simple. It's
by a mile. It's actually simple. It's
just slow.
Like honestly, the whole field owes a
Like honestly, the whole field owes a
blood debt to Costa for actually making
blood debt to Costa for actually making
this exist in the first place.
Probably without Clean R, I would have
Probably without Clean R, I would have
given up and just said this field is
given up and just said this field is
stupid by now. At least clean gave me a
stupid by now. At least clean gave me a
starting point for like, okay, I can do
starting point for like, okay, I can do
this but fast.
Don't use a lib torch like the C++
Don't use a lib torch like the C++
because literally nobody will use puffer
because literally nobody will use puffer
lib if I do that.
Why am I not getting any infos?
Huh?
H funny.
H funny.
Yeah, I the thing is it's C++ but I
Yeah, I the thing is it's C++ but I
don't even think it's like good C++,
don't even think it's like good C++,
right?
like
and
upper size. Where's batch size?
It's not like this magically works,
It's not like this magically works,
right? It's just super slow
to steer towards and reach your goal.
to steer towards and reach your goal.
Very easy with large step size.
Very easy with large step size.
Lower step size, better rewards. Uh
Lower step size, better rewards. Uh
yeah, there is a thing. So, you're
yeah, there is a thing. So, you're
increasing the credit assignment
increasing the credit assignment
horizon. So, the smaller the steps are,
horizon. So, the smaller the steps are,
the harder the credit assignment is. Um,
the harder the credit assignment is. Um,
if you need it to be small step size to
if you need it to be small step size to
be accurate, you do some sort of
be accurate, you do some sort of
curriculum learning thing, just start it
curriculum learning thing, just start it
near the goal and then, you know, move
near the goal and then, you know, move
it farther away over time or like
it farther away over time or like
randomize distance from goal, something
randomize distance from goal, something
like that.
like that.
It'll solve
Okay, so this is just getting slower and
Okay, so this is just getting slower and
slower and is not even solving anything.
It's really not even worth messing with
It's really not even worth messing with
anything in the hardware inefficient
anything in the hardware inefficient
regime, right?
Like you're just back to square one with
Like you're just back to square one with
like RL being a super cursed field where
like RL being a super cursed field where
nothing makes sense.
nothing makes sense.
I mean, this is literally running on
I mean, this is literally running on
like what 1% GPU utilization.
is the thing that killed the field in
is the thing that killed the field in
the first place.
This is kind of cool. At least this is
This is kind of cool. At least this is
up to like 500 or whatever under this
up to like 500 or whatever under this
circumstance.
Subst.
Yeah.
Okay, I don't know what the point of
Okay, I don't know what the point of
this exercise was other than to remind
this exercise was other than to remind
me how ridiculous everything is.
200. Yeah, exactly.
But you see what I'm trying to do,
But you see what I'm trying to do,
right, Tim? Like
right, Tim? Like
we do need to bridge the compute and
we do need to bridge the compute and
sample efficiency gap somehow.
And like realistically the only way to
And like realistically the only way to
do that was with massive sample reuse.
Because here's the problem, right?
Because here's the problem, right?
If I run a ton of parallel environments,
um, I don't really get to update my
um, I don't really get to update my
policy very much before like to get
policy very much before like to get
fresh data.
fresh data.
Like you can't really do sample f with a
Like you can't really do sample f with a
ton of parallel ends.
So the forward pass is just going to be
So the forward pass is just going to be
slow.
I guess I can try
maybe we just go over to some of the
maybe we just go over to some of the
offpaul stuff in puffer
offpaul stuff in puffer
robots and imitation. Imitation on its
robots and imitation. Imitation on its
own is easy.
own is easy.
It's like the mix data collection
It's like the mix data collection
setting that's hard.
What the heck?
here's your basic Q-learning thing.
He
Even with puffer, I got 10 FPS.
Even with puffer, I got 10 FPS.
Yeah, you definitely definitely 10 FPS
Yeah, you definitely definitely 10 FPS
is uh something's wrong.
is uh something's wrong.
Should never be 10 FPS.
Does anybody know um Soft Actor Critic
Does anybody know um Soft Actor Critic
doesn't do a distributional thing,
doesn't do a distributional thing,
right? Does anybody know how the hell
right? Does anybody know how the hell
soft actor critic works without uh
soft actor critic works without uh
distributional RL in
Okay. So this is the basic
Okay. So this is the basic
uh cart pole thing.
this give you QA Ready.
Got my RL exam on Monday. I forget that
Got my RL exam on Monday. I forget that
there are actually RL courses out there.
Do tend to forget that.
Is that any good?
Why is it like this?
Ah,
does get flattened.
you end up so it's like kind of Kevin
you end up so it's like kind of Kevin
uh you can actually get something to
uh you can actually get something to
kind of work literally just doing
kind of work literally just doing
behavioral cloning on like best data. I
behavioral cloning on like best data. I
also this morning here what I did is I
also this morning here what I did is I
took that and I integrated it
took that and I integrated it
um well I took that and I basically
um well I took that and I basically
added a a buffer to the existing puffer
added a a buffer to the existing puffer
online RL which seems to be doing better
online RL which seems to be doing better
and I'm relying on um a couple different
and I'm relying on um a couple different
off correction mechanisms to keep it
off correction mechanisms to keep it
stable and it's done like
stable and it's done like
it's done Okay, the thing that's weird
it's done Okay, the thing that's weird
is I actually have it making neural MMO
is I actually have it making neural MMO
learn way faster, which is like the
learn way faster, which is like the
hardest environment,
hardest environment,
but just having the data around at all
but just having the data around at all
seems to make it harder for stuff like
seems to make it harder for stuff like
Breakout. I can't figure out why.
Breakout. I can't figure out why.
like it should just not sample um the
like it should just not sample um the
offball data.
I mean, I I got to give this an honest
I mean, I I got to give this an honest
try. It's so far-fetched in my mind that
try. It's so far-fetched in my mind that
like you would be able to get anything
like you would be able to get anything
out of a one-step bootstrap.
out of a one-step bootstrap.
I didn't actually realize that the soft
I didn't actually realize that the soft
actor critic doesn't. It's literally
actor critic doesn't. It's literally
doing a onestep bootstrap, but it's so
doing a onestep bootstrap, but it's so
hard to compare because it's like the
hard to compare because it's like the
most inefficient thing ever conceived.
most inefficient thing ever conceived.
Saved.
endstep TD.
Yeah, that's like
probably Sutton and Bardo, right?
The problem is the endstep bootstrap
The problem is the endstep bootstrap
actually doesn't really work.
Behavioral cloning filtering
Behavioral cloning filtering
filter with advantages. Yeah. So, uh,
filter with advantages. Yeah. So, uh,
the thing I did yesterday was just store
the thing I did yesterday was just store
all the data in a buffer. Um, and
all the data in a buffer. Um, and
like keep the highest value data, not
like keep the highest value data, not
advantage. I believe I did it by uh
advantage. I believe I did it by uh
return like trajectory reward sum plus
return like trajectory reward sum plus
uh final value.
uh final value.
And then you sample it by advantage and
And then you sample it by advantage and
you just do behavior weighted behavioral
you just do behavior weighted behavioral
cloning which looks very similar to
cloning which looks very similar to
onpaul RL. And then today what I did is
onpaul RL. And then today what I did is
I just literally did the on policy RL
I just literally did the on policy RL
but I just do it with an additional data
but I just do it with an additional data
buffer that you can sample from.
buffer that you can sample from.
And then I'm hoping that between
And then I'm hoping that between
clipping and well the fact that between
clipping and well the fact that between
clipping and uh the vtrace component
clipping and uh the vtrace component
that's already embedded in puffer
that's already embedded in puffer
advantage that it keeps the data
advantage that it keeps the data
generally useful
generally useful
which it should to be honest because
which it should to be honest because
like the advantage computation should
like the advantage computation should
just go to zero or whatever if it gets
just go to zero or whatever if it gets
too off policy because it stops trusting
too off policy because it stops trusting
those samples I would think.
those samples I would think.
I'd have to double check. I'm pretty
I'd have to double check. I'm pretty
sure that's what should happen though.
sure that's what should happen though.
Was that advantage weighted regression?
Was that advantage weighted regression?
It was similar.
It was similar.
It was similar. Let's say
Think
I'm just going to have to go to the
I'm just going to have to go to the
other environment build stuff soon.
Drive me nuts.
direction with one policy with clipping.
direction with one policy with clipping.
Does that mean you also sort old pi
Does that mean you also sort old pi
waiting for all the data in the buffer?
waiting for all the data in the buffer?
You just have to store the log prob of
You just have to store the log prob of
the action you selected, right? That's
the action you selected, right? That's
all you need.
I mean, I guess you could technically do
I mean, I guess you could technically do
full KL div, but I'm pretty sure it's
full KL div, but I'm pretty sure it's
just you take the log prop of the action
just you take the log prop of the action
that the old policy selected, you
that the old policy selected, you
construct a ratio with the uh the new
construct a ratio with the uh the new
probability, right?
You could model the full distribution,
You could model the full distribution,
but I don't think that's the normal
but I don't think that's the normal
formulation.
Why is this so confusing?
Oh, I just have this set up in like a
Oh, I just have this set up in like a
totally weird way, I think, for um
totally weird way, I think, for um
I just have this set up in like a
I just have this set up in like a
totally
totally
weird way for this
It almost makes more sense to go off of
It almost makes more sense to go off of
the clean RLSC initially.
So freaking slow though.
And just tack it for now a little bit.
And just tack it for now a little bit.
Keep pushing on it just a little bit.
doing it this way.
Doing stupid things here.
Why is the mini batch like this as well?
Oh, cuz I have 8192.
Uh, I forgot about that.
I'm not really trying SACE. I'm just
I'm not really trying SACE. I'm just
trying to get something with a onestep
trying to get something with a onestep
bootstrap to do anything.
bootstrap to do anything.
I don't know. I'm messing around with a
I don't know. I'm messing around with a
bunch of different things at the moment,
bunch of different things at the moment,
Kevin.
Kevin.
Like, honestly, the whole literature is
Like, honestly, the whole literature is
just screwed.
just screwed.
Like, it's completely screwed.
be 16k.
be 16k.
Should be 16.
Okay. So this is now batched.
Okay. So this is now batched.
This is like batch Q-learning. Yeah.
Okay. Not amazingly stable, but it's
Okay. Not amazingly stable, but it's
it's a thing. It's there.
So, how is this supposed to do?
So, how is this supposed to do?
What does SACE do on top of this that's
What does SACE do on top of this that's
so special?
that like everyone always shows it
that like everyone always shows it
matching roughly po
the right on.
get this thing instead.
Next log.
Next log.
Get action on obs
of X. Okay.
of X. Okay.
So, this actually
Oh, okay. So, there's a separate actor
Oh, okay. So, there's a separate actor
in a soft Q network.
Okay,
trying to derive the entirety of RL.
trying to derive the entirety of RL.
It's not that I'm trying to derive the
It's not that I'm trying to derive the
rederive the entirety of RL RL
rederive the entirety of RL RL
literature from scratch. is that I'm
literature from scratch. is that I'm
trying to figure out how to make the
trying to figure out how to make the
things actually work because I mean if
things actually work because I mean if
you try any of them, none of them
you try any of them, none of them
actually work.
actually work.
I'm trying to fill in a bunch of holes
I'm trying to fill in a bunch of holes
basically.
I mean, we could make a version of this
I mean, we could make a version of this
fast, right?
Yeah, we could make a version of this
Yeah, we could make a version of this
fast, right?
You got like Well, you can get rid of
You got like Well, you can get rid of
some of them.
some of them.
This is like the double
This is like the double
the double DQN thing right here.
the double DQN thing right here.
So, there are other like there are other
So, there are other like there are other
offpaul variants where you get rid of
offpaul variants where you get rid of
two of these. Um,
I'm more confident that if we make it
I'm more confident that if we make it
work, I can come up with workarounds to
work, I can come up with workarounds to
strip things out and make it fast. Uh,
strip things out and make it fast. Uh,
the question is whether this actually
the question is whether this actually
does much on its own.
Literally best thing is going to do it
Literally best thing is going to do it
would be though for me to um like start
would be though for me to um like start
from 300 fresh again. Annoyingly enough,
the thing that I didn't realize. Okay.
the thing that I didn't realize. Okay.
So, the the key piece of missing
So, the the key piece of missing
information that I didn't realize here
information that I didn't realize here
was that Soft Actor Critic doesn't have
was that Soft Actor Critic doesn't have
a um a multi-step bootstrap and somehow
a um a multi-step bootstrap and somehow
does Okay.
does Okay.
Right.
That was the key thing that I didn't
That was the key thing that I didn't
understand.
which should make a difference.
Okay, let me use a restroom real quick
Okay, let me use a restroom real quick
and then we'll try we'll see whether uh
and then we'll try we'll see whether uh
this does anything.
I should be able to get this to match PO
I should be able to get this to match PO
but fast. We'll see. I'll be right back.
I have some ideas of how I'm going to do
I have some ideas of how I'm going to do
this.
So screwy though.
Okay. So, we have all these little
Okay. So, we have all these little
networks.
not normalize my observations. Do I not
not normalize my observations. Do I not
have that in the checklist?
have that in the checklist?
I'm pretty sure I have that in the
I'm pretty sure I have that in the
checklist.
checklist.
Normalize your observations.
Yep. I'll do that. Never did.
Well,
only so much I can do for that, eh?
Are you using reparameterize?
Are you using reparameterize?
Are you using reparameterization trick
Are you using reparameterization trick
to back propagate through actor
to back propagate through actor
through the actor update?
through the actor update?
Um,
Um,
no. What I was doing here is I have I
no. What I was doing here is I have I
have the four fully separate uh Q
have the four fully separate uh Q
functions and then the actor just has
functions and then the actor just has
its own LSTM
its own LSTM
and the Q functions don't.
What reparameterization trick
What reparameterization trick
through the actor update? I haven't even
through the actor update? I haven't even
gotten there yet at all. I'm literally
gotten there yet at all. I'm literally
just setting up optimizers.
get action.
I think we still get logits here.
Yeah, you just need to store actions.
Yeah, you just need to store actions.
No values to store.
And I think we're already at the
And I think we're already at the
training loop.
Yeah. So, we are at the training loop
Yeah. So, we are at the training loop
and
and
we're going to just ignore
we're going to just ignore
all this advantage stuff.
objects.
We do actions like this.
We do actions like this.
Then we need
Q
X
this right and then this doesn't take
this right and then this doesn't take
this is fine
this is fine
use action probabilities
min Q function Next target
and then
and then
next state action prop
you want full action props I guess.
you want full action props I guess.
Yeah. Does that just log softmax?
That just soft max actually
probs.
So is this not just um
So is this not just um
this is just soft max of log jets right
wait next state.
wait next state.
Next state.
Next state.
Oh, this is on next observations, I
Oh, this is on next observations, I
suppose.
suppose.
Okay,
we can do it either way.
We're just going to have to index
because it's going to be action robs
because it's going to be action robs
times
to domin and
Okay, we'll see if I've done this
Okay, we'll see if I've done this
correctly. Probably this is the uh the
correctly. Probably this is the uh the
crucial don't screw this up.
And then
Great.
Okay. So, we have to now be careful with
Okay. So, we have to now be careful with
this, right? Because this is
this, right? Because this is
the reward.
X
MB rewards.
This is the one that I have to be
This is the one that I have to be
careful with, right? Because it needs to
careful with, right? Because it needs to
be the next target. So, it's
be the next target. So, it's
this I believe.
this I believe.
And then this will have to be like
what the heck is this?
The torch. No grad.
The torch. No grad.
So this whole thing is under a nood. Red
one.
That's so inefficient the way that's
That's so inefficient the way that's
implemented.
implemented.
Okay, we'll have to change that. We can
Okay, we'll have to change that. We can
at least share some of this.
Oh, actually, no, we can't. Lovely.
I think it is something it's something
I think it is something it's something
like that.
And this one's got to be like
And this one's got to be like
this, right?
Why are we calling get action again?
Oh, cuz now we step
and now we do it again some for some
and now we do it again some for some
reason.
reason.
I mean, this is still
I mean, this is still
this is actually going to end up being
this is actually going to end up being
less complicated than the current
less complicated than the current
implementation of the algorithm because
implementation of the algorithm because
of the lack of prioritized replay. Of
of the lack of prioritized replay. Of
course, as soon as you go stick that
course, as soon as you go stick that
back in, it gets worse again.
Need action props as well.
And now this thing here, here's your
And now this thing here, here's your
actor loss.
Is there anything else?
Is there anything else?
No, just the target nets.
Wait, alpha loss.
Are there three optimizers here?
Oh, it's just um that's just for
Oh, it's just um that's just for
retuning. Fine. We can ignore this for
retuning. Fine. We can ignore this for
now.
All right. So, now we get rid of all
All right. So, now we get rid of all
this stuff.
this stuff.
like kind of okay.
No gradient clipping, nothing.
Tons of stuff we can do here.
Tons of stuff we can do here.
It's going to look something like this.
It's going to look something like this.
We could very easily clean this up.
Okay, we're actually getting like
Okay, we're actually getting like
reasonably far into the algorithm Now,
Yeah. So that gives you a min, right?
Action props are screwy.
Why is action props like this? Yeah.
Why is action props like this? Yeah.
Oh, cuz logits
Wait, what? This looks good to me,
Wait, what? This looks good to me,
doesn't it? Now
they not do this correctly.
Just do
Just do
go up one target.
Not next.
Want to get something to be useful
Want to get something to be useful
today.
Uhhuh.
Oh, you know why I see it?
That's dumb.
Oops.
And then we can sum this, right?
Yeah.
Then
Then
of course this chunk here.
of course this chunk here.
This this
This this
crazy they use four separate Q functions
crazy they use four separate Q functions
in this thing
and
and
this is probably the exact same thing as
this is probably the exact same thing as
before. Yeah,
we'll actually be able to reuse a couple
we'll actually be able to reuse a couple
of these computations though.
So, it'll get faster, but even like
So, it'll get faster, but even like
this, it we should still be able to get
this, it we should still be able to get
like a million steps a second or
like a million steps a second or
whatever.
Okay. Well, that is the initial
Okay. Well, that is the initial
implementation.
The initial implementation right there
The initial implementation right there
doesn't seem to do anything.
doesn't seem to do anything.
But we do have we have it running and
But we do have we have it running and
it's actually running pretty fast.
Now we just go bug hunting right.
Now we just go bug hunting right.
Oh well, I didn't sync the damn
Oh well, I didn't sync the damn
networks. That will do it.
target network frequency.
target network frequency.
Yeah, this whole like Q function
Yeah, this whole like Q function
synchronization garbage.
8,000
8,000
target network frequency.
Maybe
I actually think I just want to
and towel.
and towel.
They have this thing set not to one.
Set to one.
There
quite collapsed.
Go look at the uh the clean RL1.
Go look at the uh the clean RL1.
See what I've missed.
Many things.
Why do they do a categorical followed by
Why do they do a categorical followed by
a sample here?
Oh, because they this is Yeah, we have
Oh, because they this is Yeah, we have
this as well. This is fine.
But this takes policy parameters
But this takes policy parameters
with two parameters.
with two parameters.
Then this takes
Then this takes
what the heck? Where' the target
what the heck? Where' the target
optimizer go?
Wait, wait, wait. What? Huh?
Wait, wait, wait. What? Huh?
Oh, no you don't. you just get
Oh, no you don't. you just get
right the target just gets copied. It
right the target just gets copied. It
doesn't get optimized.
doesn't get optimized.
Um
and then the actor optimizer needs the
and then the actor optimizer needs the
encoder, the decoder and the LSTM,
encoder, the decoder and the LSTM,
right?
Okay,
Okay,
have all this stuff
have all this stuff
and we get to training.
function.
Okay. So, we have to make sure we did
Okay. So, we have to make sure we did
next observations correct, right?
next observations correct, right?
So, I do on MB ops, we do a sample
So, I do on MB ops, we do a sample
do view logs.
Okay.
Get action.
Get action.
So then this can go
So then this can go
wait
wait
this actually can go in this whole thing
this actually can go in this whole thing
can go into this nrad loop right
can go into this nrad loop right
this whole thing goes into the nrad
now budget action
do men
do men
alpha And then this is next state log I
Yeah. So this is the full distribution.
Now we do
Now we do
Q function
Q function
one. The target Q function on next obs.
one. The target Q function on next obs.
Okay. So I've done this on current obs,
Okay. So I've done this on current obs,
but I have them in a buffer that's going
but I have them in a buffer that's going
to get shifted by one. Anyways, we do
to get shifted by one. Anyways, we do
this min alpha next state log pi. Okay.
this min alpha next state log pi. Okay.
So I think next state action probs
So I think next state action probs
action probs and state log pi log log
action probs and state log pi log log
action probs.
I think that's fine.
And then you sum along this dim
And then you sum along this dim
and then you take
and then you take
you offset it right. So it's plus one
you offset it right. So it's plus one
minus terminals.
gamma
gamma
+ 1 - m times yeah this looks good to me
+ 1 - m times yeah this looks good to me
and then
and then
you do this q function I don't know if
you do this q function I don't know if
this gather is correct we'll double
this gather is correct we'll double
check this
okay but you you do this and then you
okay but you you do this and then you
have a q function loss which is mean
have a q function loss which is mean
squared error between
squared error between
next Q value,
which is already offset by one. Okay, so
which is already offset by one. Okay, so
I think this looks good to me.
I think this looks good to me.
And then this we don't care about.
Yeah, this looks good to me. Let's go
Yeah, this looks good to me. Let's go
make sure we're doing this this sampling
make sure we're doing this this sampling
operation. I know I've gotten wrong
operation. I know I've gotten wrong
before.
before.
Let me see if this looks Good.
So we do one
So we do one
zero
zero
one.
one.
Wait, what? Hang on. One 0 one one. So
Wait, what? Hang on. One 0 one one. So
one 0 one
one 0 one
one. Yeah, this actually looks correct
one. Yeah, this actually looks correct
to me.
to me.
Yeah. No, this is correct. So, huh, the
Yeah. No, this is correct. So, huh, the
gather is actually correct here.
What do I have? Uh, what do I have wrong
What do I have? Uh, what do I have wrong
then?
Let's log some losses.
Did I delete all of them already? Hang
Did I delete all of them already? Hang
on.
Where's the Q function loss?
Well, it doesn't blow up or anything.
Well, it doesn't blow up or anything.
There's another loss though, right? The
There's another loss though, right? The
actor loss.
Those
are both reasonable
are both reasonable
numbers.
numbers.
Let's check if the update frequency is
Let's check if the update frequency is
something totally obscene.
How often do they update the target
How often do they update the target
network?
Wrong one.
They update the target network frequency
They update the target network frequency
of updates for the target network.
So every 8,000 samples
which is in their case
which is in their case
that is
that is
quite uncommon because
quite uncommon because
like a hundred something updates, right?
And then what is this target entropy
And then what is this target entropy
scale?
Oh, auto tune entry. We're good.
Well, we're missing something. Yeah.
Oh, yeah.
Just making sure it's not a terminal
Just making sure it's not a terminal
issue. Way easier to catch on breakout.
issue. Way easier to catch on breakout.
But yeah, no, this doesn't learn
But yeah, no, this doesn't learn
anything.
anything.
Welcome tree form.
I'm currently messing around trying to
I'm currently messing around trying to
get um SACE into Puffer LIIB so I can
get um SACE into Puffer LIIB so I can
run some comparisons.
This is definitely not learning.
This is definitely not learning.
Well,
Well,
the rewards kind of goes up, but it's
the rewards kind of goes up, but it's
like it's below the noise threshold
like it's below the noise threshold
really.
Yeah. Soft actor critic.
The thing that I hadn't realized about
The thing that I hadn't realized about
soft actor critic, I figured that it had
soft actor critic, I figured that it had
um like an endstep bootstrap
um like an endstep bootstrap
like all of the other off policy
like all of the other off policy
methods. Uh but it doesn't. It actually
methods. Uh but it doesn't. It actually
does
does
well. I guess it depends what papers you
well. I guess it depends what papers you
believe, but comparably to PO with
believe, but comparably to PO with
uh only a one-step bootstrap.
Of course, I need to actually get it to
Of course, I need to actually get it to
do something for that to be uh evident.
We do get action.
Hang on.
Left max log action prom.
Yeah, this is right because we do
it.
Maybe there is a mismatch here, right?
Maybe you just want to return
problems.
problems.
Maybe you want to do like this.
So when you collect uh where you step up
So when you collect uh where you step up
the end.
the end.
Yeah. You just want to collect the
Yeah. You just want to collect the
actions, right?
actions, right?
So we actually
We don't really care too much.
Yeah, we actually do not care very much
Yeah, we actually do not care very much
about any of this. Let's get actions
man. So we can just get over here,
man. So we can just get over here,
right?
We don't need MB log props.
We don't need this. We don't need this.
We don't need this. We don't need this.
Right.
Right.
Some ways, if this works, it will
Some ways, if this works, it will
actually simplify things a little bit.
We have action probs.
So this should just be probs log prop
So this should just be probs log prop
like this. Yeah.
like this. Yeah.
Prob log prob yada yada
Prob log prob yada yada
prob.
Um,
I'll just
Bob,
Bob.
Find a bug.
Uh, this needs to be
Uh, this needs to be
Oh, and log prop also needs to Okay.
Watch.
Okay,
now this does need to get reshaped,
now this does need to get reshaped,
right?
You
You
Hub.
Seriously?
Oh,
Oh,
hang on.
Okay. So, this is now running.
Okay. So, this is now running.
Still doesn't train.
Still doesn't train.
Really?
Really?
You do all that, it still doesn't train.
Hey, we go hunting for more bugs.
just fighting you. Yep.
Compile false the whole time.
programming would not be fun if it
programming would not be fun if it
worked the first time all the time. I
worked the first time all the time. I
disagree. It would be dramatically more
disagree. It would be dramatically more
fun
fun
because I would just solve one thing,
because I would just solve one thing,
move on, solve the next thing, and then
move on, solve the next thing, and then
all the problems would be solved.
I think if there's like anywhere obvious
I think if there's like anywhere obvious
I would have messed
Hang on. This this is on obs
Hang on. This this is on obs
actions, right?
actions, right?
Next Q value.
Next Q value.
You sum these. Yeah.
What the point of that is, but sure.
And here you have your targets.
And the next one is going to be acquired
And the next one is going to be acquired
by
Yeah, I think this is fine.
Yeah, I think this is fine.
Rewards
Rewards
of
one
This seems fine.
This seems fine.
That seems completely fine.
That's all there is. Like there's really
That's all there is. Like there's really
not that much to this algorithm.
Add backwards up.
the actor loss is going to be probs
alpha
and this can be log
doesn't make a difference So
me in on this
me in on this
min q function values.
This one doesn't need to be offset.
This one doesn't need to be offset.
Right.
Right.
This one's good.
We use action props for temperature
We use action props for temperature
loss.
loss.
No, this is not needed. It's just a
No, this is not needed. It's just a
fixed alpha.
And where's alpha?
Alpha times next.
Whole thing looks reasonable to me.
Oh, okay. It's
Oh, okay. It's
the fact it's jumping around.
the fact it's jumping around.
It's probably doing something.
It's probably doing something.
Might need to just fix a bunch of other
Might need to just fix a bunch of other
things.
Just to be sure that it's not just this
Just to be sure that it's not just this
jumping around the scores.
Oh, it's not it. H
Oh, it's not it. H
I still have bugs in here then. Yeah,
I still have bugs in here then. Yeah,
it should pretty much any reasonable
it should pretty much any reasonable
algorithm should jump above uh above
algorithm should jump above uh above
seven
seven
almost instantly.
It is weird that this is slowly ever so
It is weird that this is slowly ever so
slightly going up, I'll say, but
it's not like a significant
result
result
like even directionally.
You can probably just maximize entropy
You can probably just maximize entropy
and do better than this.
Yeah, but like it should
Yeah, but like it should
any reasonable algorithm should have
any reasonable algorithm should have
solved uh should have gotten it way way
solved uh should have gotten it way way
before then. Like you can literally run
before then. Like you can literally run
vanilla DQN and get that immediately.
Far more likely I have something bugged.
There we go.
Let's see if it's consistent or just
Let's see if it's consistent or just
jumpy.
Okay. So, let's get this to be stable
Okay. So, let's get this to be stable
and see if we can run it on breakout.
I don't know how frequently we should
I don't know how frequently we should
sync.
Like do you want it to be synced very
Like do you want it to be synced very
frequently or is that screw it up?
That screws it up, Head.
tow equals
tow equals
05. Does that make a big difference?
This update rate seems to matter a
Oh well, learning rate.
Way worse.
Way worse.
believe so. Okay, we'll try that. It's
believe so. Okay, we'll try that. It's
weird because the cleaner defaults don't
weird because the cleaner defaults don't
even have um they just have it set to
even have um they just have it set to
one.
I mean, it's possible I still have major
I mean, it's possible I still have major
problems in the implementation as well
problems in the implementation as well
here, right?
Yeah, cartpole is like so incredibly
Yeah, cartpole is like so incredibly
easy.
You can have stuff be completely wrong
You can have stuff be completely wrong
and just still kind of do stuff on that.
Are you just supposed to run way more
Are you just supposed to run way more
updates?
I don't think so. Right.
Not like magically. Oh yeah. Samples.
Four neurons.
Four neurons.
Yeah, you can solve it with um you can
Yeah, you can solve it with um you can
solve it with four neurons. Sure. I
solve it with four neurons. Sure. I
think it's maybe Pong is four. a lot of
think it's maybe Pong is four. a lot of
the Atari is six,
the Atari is six,
but that's not really the point of Yes.
See?
See?
Figure out what it is.
You have your actor optimizer.
Did I miss anything crucial here?
So,
same learning rates.
Now factor.
This kind of works. Carpole, right?
And just mess with the hypers a little
And just mess with the hypers a little
bit.
really.
I mean, it should just like
I mean, it should just like
literally Q-learning works on this. It
literally Q-learning works on this. It
should totally work.
Is it like too much entropy?
No, that actually breaks it instantly as
No, that actually breaks it instantly as
well.
thing. Nine.
speed.
Make sure I did that in the right
props. Log props, right?
Prob
target
and
that's definitely fine.
This stuff is like pretty basic though.
This stuff is like pretty basic though.
Now
the value
the value
the loss
supposed to match up.
So this is
take your OBS
N
alpha time log prop
minus
minus
in
actor.
Yeah,
as far as I can see, I have this set up
as far as I can see, I have this set up
correctly and it's like really
correctly and it's like really
underperforming.
really really underperforming
epochs.
What else would I have to mess with?
Yeah, doesn't work on pong at all.
somehow works on just cartpole.
somehow works on just cartpole.
Guess the easiest problem
and not even super consistently.
and not even super consistently.
Yeah. See, like that's a mess.
What?
This should definitely
This should definitely
definitely like work way better.
Oh, wait. Hang on.
We do use MB actions here. Yeah,
We do use MB actions here. Yeah,
that was the one thing I wasn't sure is
that was the one thing I wasn't sure is
if I had grabbed the uh the actions
if I had grabbed the uh the actions
correctly.
Seems like we do, though.
Wait, why is this a sum?
That makes sense.
Next Q value.
Next Q value.
This is a gather.
Weird that this is a sum.
How does this make sense as a sum?
H.
Ah.
Nothing else in here.
Nothing else in here.
This is literally just a sample.
Target loads the state deck of the uh
Target loads the state deck of the uh
target loads the state deck
of
of
fine.
I like not optimizing the main network
I like not optimizing the main network
somehow.
somehow.
Encoder decoder
where I should be
the self policy. Got your encoder. You
the self policy. Got your encoder. You
got your decoder. Oh,
got your decoder. Oh,
that's bad. Yeah. Okay, that'll do it.
that's bad. Yeah. Okay, that'll do it.
Hang on.
Hang on.
Yeah, hang on. Hang on.
Oh, wait. Shouldn't they be linked?
They should be linked. Don't know.
This might be doubling up.
Like they actually they should be the
Like they actually they should be the
same, right?
same, right?
Hang on. Yeah, I thought I'd seen
Hang on. Yeah, I thought I'd seen
something, but it's that's not it.
I mean, then it literally wouldn't be
I mean, then it literally wouldn't be
able to learn
Yeah, it's the LSTM weight that you
Yeah, it's the LSTM weight that you
want.
want.
Anyways,
that's it.
that's it.
player norm.
Uh I don't have a learning rate schedule
Uh I don't have a learning rate schedule
on this actually, right? That's kind of
on this actually, right? That's kind of
a big deal.
Okay.
And it's still not amazing. thing.
How do you make soft actor critic
How do you make soft actor critic
actually learn something
does this but like come
Oh,
this one doesn't take
this one doesn't take
I don't know if that changes anything.
actually don't think that does change
actually don't think that does change
anything.
Heck,
Heck,
we just are we have to search over
we just are we have to search over
random seeds now. Is this where we're
random seeds now. Is this where we're
at?
Is there no um
where's the
where's the
this gather here?
this gather here?
Yeah. So, this Q function doesn't use
how can you tell her the score?
how can you tell her the score?
Yeah, it's the score. So, cartpole, you
Yeah, it's the score. So, cartpole, you
just get a point for every step until
just get a point for every step until
you fall over. And if you get 200
you fall over. And if you get 200
points, then you're balancing it
points, then you're balancing it
perfectly.
Something is just wrong. I don't know
Something is just wrong. I don't know
what.
what.
trim both of these against the same
trim both of these against the same
target here.
target here.
Do this gather which I double checked
Do this gather which I double checked
this gather and this but gather is fine.
do policy
do policy
Q function one Q function two
boards plus 1 - done times gamma
boards plus 1 - done times gamma
in Q function on the next steps do next
in Q function next target probs
Min of the two targets minus alpha
Min of the two targets minus alpha
prop
have
have
log prop pro log prop here from ours
loget state
empty and this one which is fine. Man,
Yeah, these things are not helping.
is so incredibly difficult to get right.
is so incredibly difficult to get right.
Is it a bug? Is it the hyper suck? Does
Is it a bug? Is it the hyper suck? Does
the algorithm suck?
all over the place. I mean, it's like at
all over the place. I mean, it's like at
least there's something correct here.
least there's something correct here.
Otherwise, you wouldn't get um you
Otherwise, you wouldn't get um you
wouldn't see any learning, right?
adjustment of weight. No, if you
adjustment of weight. No, if you
randomly are adjusting weights, you
randomly are adjusting weights, you
won't get a you won't get a score like
won't get a you won't get a score like
this.
It's like directionally correct at the
It's like directionally correct at the
very least
and I don't but I don't know like what
and I don't but I don't know like what
is
is
I have no idea what the problem is,
I have no idea what the problem is,
right? If it's just hypers, if I have a
right? If it's just hypers, if I have a
bug Hug.
on the original code.
on the original code.
The problem is these are like these
The problem is these are like these
algorithms are fancy enough that you're
algorithms are fancy enough that you're
not going to ever really be able to do
not going to ever really be able to do
that. Like if you're never going to be
that. Like if you're never going to be
able to really get like one to one
able to really get like one to one
comparison like this. What's the
comparison like this. What's the
baseline score with PO instantly solved?
Like by the time it's printing out logs,
Like by the time it's printing out logs,
it's already solved.
And this thing should just have
And this thing should just have
instantly solved everything.
Especially like I'm giving it so much
Especially like I'm giving it so much
more fresh data as well.
And this is random perf here.
13 seconds.
13 seconds.
Yeah, exactly. I mean, on a faster
Yeah, exactly. I mean, on a faster
machine, it'll like be basically
machine, it'll like be basically
instant.
Okay. I mean, this does do something
Okay. I mean, this does do something
now, right?
like it's not random,
but it's just bad. And I don't know why.
Algorithm
The hell could I have possibly done
The hell could I have possibly done
wrong here?
are the Q I mean no it shouldn't matter
are the Q I mean no it shouldn't matter
that the QETs I have kind of suck
And nothing.
And nothing.
Nothing at all.
Uh, why the heck did Reream just sign me
Uh, why the heck did Reream just sign me
out?
There we go.
Why the heck is my reream being dumb?
Okay.
Okay.
Well, whatever. I got it now.
Okay.
You don't use any of this stuff Now,
You'll get gradient clipping
when you were at Stamford or I don't
when you were at Stamford or I don't
know MIT for your undergrad. Stanford
know MIT for your undergrad. Stanford
was undergrad. MIT was PhD. Do you have
was undergrad. MIT was PhD. Do you have
like people who are admitted to school
like people who are admitted to school
with bad grades
with bad grades
accepted due to impressive
accepted due to impressive
extracurricular?
extracurricular?
Uh, I don't know about impressive
Uh, I don't know about impressive
extracurriculars. I definitely know a
extracurriculars. I definitely know a
bunch of people who I have no idea how
bunch of people who I have no idea how
they got in.
It's tough to get in with bad grades
It's tough to get in with bad grades
because it's like
having bad grades. It's like it's such
having bad grades. It's like it's such
an it's such like a a basic checkbox.
an it's such like a a basic checkbox.
It's like failing to get up and tie your
It's like failing to get up and tie your
shoes in the morning, you know, on the
shoes in the morning, you know, on the
way out.
how they entered.
I mean, there's like a huge gap. Like,
I mean, there's like a huge gap. Like,
you meet people who are absolutely
you meet people who are absolutely
brilliant, right? And then you meet
brilliant, right? And then you meet
people who are like, "All right, this
people who are like, "All right, this
guy's a I have absolutely no idea
guy's a I have absolutely no idea
why he's here, but okay, I guess.
Well, yeah, let's just go get grad.
Well, yeah, let's just go get grad.
Norm.
me just one thing real quick.
me just one thing real quick.
Tab zero grad. I don't think it makes a
Tab zero grad. I don't think it makes a
difference
difference
unless I you know it's possible I have
unless I you know it's possible I have
my loop set up differently
because that wouldn't make any sense
because that wouldn't make any sense
though, right?
Yeah, cuz it has to be optimized
Yeah, cuz it has to be optimized
obviously, right?
Otherwise, you wouldn't see if I had
Otherwise, you wouldn't see if I had
that as a mistake, then you would
that as a mistake, then you would
literally wouldn't see anything work.
I will say the one thing I think is kind
I will say the one thing I think is kind
of ridiculous is um
of ridiculous is um
that people get rejected from PhD
that people get rejected from PhD
programs over not having great grades.
programs over not having great grades.
It's like
It's like
I almost like question if if you do have
I almost like question if if you do have
perfect grades coming out of undergrad,
perfect grades coming out of undergrad,
I almost question what the hell you've
I almost question what the hell you've
been doing with your time.
Okay,
Okay,
this replicable
Not precisely.
Not precisely.
A heck of a lot more stable though,
A heck of a lot more stable though,
right?
frequency matters a lot here, huh?
Every 64 steps maybe.
Okay. So, that is
and that's closer.
interesting. It breaks with four update
interesting. It breaks with four update
epochs. Really?
This noise
This noise
didn't break.
didn't break.
The whole point of these
And okay, it's not terrible.
Yeah, literally no signal in there. None
Yeah, literally no signal in there. None
at all.
The heck?
The heck?
That's like bizarre.
That's like bizarre.
You also shouldn't need um
You also shouldn't need um
need that number of update epochs for
need that number of update epochs for
freaking cartpole. I mean cartpole
freaking cartpole. I mean cartpole
should be solved in like a tiny number
should be solved in like a tiny number
of steps. This shouldn't be remotely a
of steps. This shouldn't be remotely a
challenge.
I only have it kind of training.
where it's like one of the only damn
where it's like one of the only damn
fields where you can have worked in the
fields where you can have worked in the
space for 10 years. You can have like
space for 10 years. You can have like
very deep knowledge on a ton of
very deep knowledge on a ton of
different things in this space and like
different things in this space and like
literally basic algorithm implementation
literally basic algorithm implementation
from scratch. Good luck. Not going to
from scratch. Good luck. Not going to
work.
Is there like do I have like an extra
Is there like do I have like an extra
log or something? I've checked this so
log or something? I've checked this so
many times.
many times.
I don't know what else it would be.
this puffer lab. Where's the clean arm?
This shouldn't be important, right?
This shouldn't be important, right?
It literally is random when you start
It literally is random when you start
it. Anyways,
get this
This actually does tell me sample reuse,
This actually does tell me sample reuse,
right?
right?
Update frequency. Is it four?
Yeah. So they have 8x sample reuse.
I guess it is totally possible, but it's
I guess it is totally possible, but it's
just like a crappy algorithm that you
just like a crappy algorithm that you
have to crank the uh
have to crank the uh
compute on, right?
But 8x that's only like um
But 8x that's only like um
x more than on paul
for these same tasks.
Oh,
you're los
Robs.
You know, it's really not that much
You know, it's really not that much
code.
code.
There's really not that much space for
There's really not that much space for
stuff to be wrong. Um,
particularly for it to be doing worse
particularly for it to be doing worse
more samples.
I mean, still like, come on. It's more
I mean, still like, come on. It's more
stable than it was, but it still it
stable than it was, but it still it
doesn't freaking solve cart pole. Take
what else is in SACE doesn't even have
what else is in SACE doesn't even have
like prioritize replay or anything by
like prioritize replay or anything by
default, right?
default, right?
It just has um
it just has like a circular buffer
it just has like a circular buffer
thing.
There anything else I haven't messed
There anything else I haven't messed
with?
with?
Feel like I've messed with everything.
Why does it take so long for it to start
Why does it take so long for it to start
learning like anything?
Weird how it does that.
the QET like the architecture is pretty
the QET like the architecture is pretty
dumb but like
dumb but like
freaking cartpole
Yeah, it's freaking cartpole. It
Yeah, it's freaking cartpole. It
shouldn't matter.
I understand this
policy.
fun.
fun.
Got to check one small thing.
I'm kind of at a loss here.
Like I expected this to do something.
Do I just have like crazy hyper prams or
Do I just have like crazy hyper prams or
something? But there really aren't even
something? But there really aren't even
that many hyper prams with this, right?
Like we cut out most of them.
Literally, we've cut out almost all the
Literally, we've cut out almost all the
hypers.
This massively like No. Right. There's
This massively like No. Right. There's
no way,
right?
It shouldn't be this brittle at this
It shouldn't be this brittle at this
scale.
scale.
Like basically, you literally can solve
Like basically, you literally can solve
this problem for reference just by
this problem for reference just by
behavioral cloning the best lucky
behavioral cloning the best lucky
segments that you get.
I'm giving it on policy data pretty
I'm giving it on policy data pretty
much, which should be better than
much, which should be better than
normal.
normal.
There should be like a best case
There should be like a best case
scenario for it pretty much. It should
scenario for it pretty much. It should
be super easy
be super easy
to have this solve.
Like look, if I got this to solve long
Like look, if I got this to solve long
and like decent enough on breakout and
and like decent enough on breakout and
then I have to tune it, that's one
then I have to tune it, that's one
thing. This is just not doing
thing. This is just not doing
remotely close to how well I would
remotely close to how well I would
expect it to.
expect it to.
I mean, do we have to think about epochs
I mean, do we have to think about epochs
here?
64.
I mean, we know that this is sensitive,
I mean, we know that this is sensitive,
right?
right?
Oops.
Oops.
We know that this is sensitive.
Maybe it's every 16 epochs we do this.
Okay,
Okay,
there you go. So, it's really really
there you go. So, it's really really
sensitive to um
sensitive to um
the uh the update
interval here. It's still not stable, by
interval here. It's still not stable, by
the way.
Why doesn't this give you a stable
Why doesn't this give you a stable
solve?
Let me go double check. make sure that
Let me go double check. make sure that
the uh the reward thing is what I think
the uh the reward thing is what I think
it is.
I want to just be sure that I haven't
I want to just be sure that I haven't
screwed this up.
Okay, I think that this is confirmation,
Okay, I think that this is confirmation,
right?
right?
Just can't learn
Just can't learn
That's confirmation I had it right this
That's confirmation I had it right this
way
way
is the one I would expect anyways
plus the next Q target.
Let's see. Can we get away with updating
Let's see. Can we get away with updating
this every eight?
this every eight?
That the play
see this completely crashes. It
see this completely crashes. It
doesn't learn a single thing.
So sensitive.
Okay, at least this is like reasonable,
Okay, at least this is like reasonable,
right?
I wonder if the alpha is like
four m
of eight.
Can we get this to do anything on pong?
best paper at RLC besides my own work
best paper at RLC besides my own work
and my friend's work, I assume.
Honestly, there really wasn't anything
Honestly, there really wasn't anything
stand out
stand out
that I was like super excited to go look
that I was like super excited to go look
at.
I mean, it it started optimizing for a
I mean, it it started optimizing for a
little second,
little second,
but it's still like
like so freaking bad, right?
Do I not have like some major bug in
Do I not have like some major bug in
Yeah.
and talk RL
and talk RL
get the shilling down. Oh, that was like
get the shilling down. Oh, that was like
a just a super clumsy impromptu
a just a super clumsy impromptu
interview. If you want to watch my
interview. If you want to watch my
actually properly prepared talk, uh it's
actually properly prepared talk, uh it's
on the neural MMO YouTube.
like that. I'm actually proud of.
like that. I'm actually proud of.
That was a good talk.
Algorithm just feels super weak to me.
Is there any other major details
Is there any other major details
I'm missing? I don't
I'm missing? I don't
hang on. I don't think like I really
hang on. I don't think like I really
don't think so.
I mean, I guess the uh
I mean, I guess the uh
we're using really crappy nets, but
we're using really crappy nets, but
shouldn't matter that much. Maybe for
shouldn't matter that much. Maybe for
pong and stuff it does.
Let me at least get us like one.
Let's just at least get it like one
Let's just at least get it like one
extra hidden layer.
All right, Mr. Off Paul, do we work?
Well, didn't break it for um ppole at
Well, didn't break it for um ppole at
least initially.
Weirdly stuck.
It's ridiculously slow.
I mean, what's the deal with this? Is
I mean, what's the deal with this? Is
this just a stupid algorithm that you
this just a stupid algorithm that you
just like have to crank a ton of compute
just like have to crank a ton of compute
into before it solves even simple
into before it solves even simple
problems but scales better?
problems but scales better?
Like
kind of ridiculous.
It's annoying because like this is like
It's annoying because like this is like
I've pushed it to a point here where I
I've pushed it to a point here where I
can't be positive it's correct. So I
can't be positive it's correct. So I
can't say oh yeah the algorithm just
can't say oh yeah the algorithm just
sucks because I don't know if I have a
sucks because I don't know if I have a
bug.
I don't think so at this point.
I don't think so at this point.
Solves the simple end.
I've like roughly calibrated it as well.
I've like roughly calibrated it as well.
The update frequency
like I've roughly calibrated it.
It only solves freaking cart pole and it
It only solves freaking cart pole and it
doesn't even fully solve that.
Well, no, this one wasn't doing well
Well, no, this one wasn't doing well
with neural MMO.
with neural MMO.
It was a different algorithm.
We have this.
This got up to like 550 whatever.
This got up to like 550 whatever.
Doesn't sell breakout
You know, I really thought that I was
You know, I really thought that I was
going to be able to get something
going to be able to get something
quickly that at least did like okay, and
quickly that at least did like okay, and
I thought, you know, maybe I won't be
I thought, you know, maybe I won't be
able to tune it as good as my algorithm
able to tune it as good as my algorithm
is.
Like the puffer baseline is just
Like the puffer baseline is just
incredibly
incredibly
It's like an incredibly tough one to
It's like an incredibly tough one to
beat
beat
cuz everything out there is just like so
cuz everything out there is just like so
incredibly ludicrously laughably slow by
incredibly ludicrously laughably slow by
comparison, All right.
Does anybody see a bug? I don't see a
Does anybody see a bug? I don't see a
bug.
bug.
Like it looks to me like I have this
Like it looks to me like I have this
pretty reasonable
value.
value.
basically identical code.
I'm actually here.
I'm actually here.
Let me commit this.
Guess I ris I uh I I named it wrong a
Guess I ris I uh I I named it wrong a
little bit.
little bit.
Fine.
That's square.
Um,
Um,
doesn't look like it's working to me.
I figured we'd check the reference
I figured we'd check the reference
implementation, right?
Maybe it's just like stupidly stupidly
Maybe it's just like stupidly stupidly
slow. Maybe that's the deal.
I think I might have undersold puffer li
I think I might have undersold puffer li
a little bit when I say a thousand times
a little bit when I say a thousand times
faster
faster
because now this is like 5,000x faster,
because now this is like 5,000x faster,
right?
This is literally running under 600
This is literally running under 600
steps per second with like tiny networks
steps per second with like tiny networks
on the fastest possible environment on a
on the fastest possible environment on a
5090.
It's also not even solving it.
This is the reference implementation as
This is the reference implementation as
well.
So, um,
So, um,
this thing just not
Have I been here like trying to match
Have I been here like trying to match
an algorithm that just doesn't freaking
an algorithm that just doesn't freaking
work?
This is actually now at um close to the
This is actually now at um close to the
steps that I did mine for.
steps that I did mine for.
And my version's actually better than
And my version's actually better than
this.
I mean, I feel somewhat better about
I mean, I feel somewhat better about
this,
but like what the heck, right?
Oh, hang on.
Supposed to be a batch size of 32.
Supposed to be a batch size of 32.
Maybe it's Maybe it needs the tiny
Maybe it's Maybe it needs the tiny
batch. I thought it's going to be even
batch. I thought it's going to be even
slower.
Try it with the 32 batch size.
It's going to be kind of funny to me if
It's going to be kind of funny to me if
this only works with tiny little batches
this only works with tiny little batches
of data.
Be super cursed.
I don't know if it's doing any better.
Supposed to be 5 million steps.
This norm is not good for um
This norm is not good for um
and I caught a bad norm.
and I caught a bad norm.
Maybe it does work.
have also has the 255 norm.
This is such an easy problem like
This is such an easy problem like
it should insta solve.
it should insta solve.
Let's
make Man,
honestly, hard to see if it's learning.
I'm trying to make sure I don't have
I'm trying to make sure I don't have
like some screw up from um puffer. I
like some screw up from um puffer. I
don't think I do though.
Oh, you know what? Hang on.
I just want to be sure.
I think this might have been it.
See,
it's actually hard to say.
The trajectories are super mixed.
Yeah, this is learning.
It's weird. It's like It is actually
It's weird. It's like It is actually
noisy though.
noisy though.
The segments are really noisy,
The segments are really noisy,
but it is learning.
It's funny though. It does actually take
It's funny though. It does actually take
it like several minutes
it like several minutes
to solve freaking uh cartpole.
So I mean if this is supposed to be the
So I mean if this is supposed to be the
sample efficient algorithm
sample efficient algorithm
and this is what it does.
So noisy that it's like
So noisy that it's like
it's still getting bad
it's still getting bad
bad segments at this point.
Nothing else I'm missing, right?
Nothing else I'm missing, right?
I think so.
I mean, okay, we see that this just
I mean, okay, we see that this just
continues to bounce around a ton,
continues to bounce around a ton,
but it does do something. Let's just
but it does do something. Let's just
make sure there's not
make sure there's not
do pong real quick. Cuz
if this solves faster then it's like all
if this solves faster then it's like all
right there's something screw Okay.
And if this doesn't work,
then what? Just a crappy algorithm.
I mean that's probably fair right
if I tried to implement myself and I
if I tried to implement myself and I
used like one of the best reference
used like one of the best reference
impleations
impleations
and then what would that imply
and then what would that imply
basically that a lot of these control
basically that a lot of these control
tasks they're used on are just like
tasks they're used on are just like
really stupid really data constrained
really stupid really data constrained
problems
not a single episode
not a single episode
the reward.
Well, that's kind of surprising.
This is a random paper, right?
Yeah, this is bad. This is a It fails
Yeah, this is bad. This is a It fails
pong.
0.7 breaker. So, this is just bad.
They get results.
across Atari 100K.
Okay,
they have results
here.
And why is it
efficient zero is the best one on here.
The fact that they actually got this to
The fact that they actually got this to
do anything though.
I don't understand why there's this much
I don't understand why there's this much
of a
of a
a discrepancy. Like I'm literally seeing
a discrepancy. Like I'm literally seeing
this do nothing across both my own
this do nothing across both my own
implementation
implementation
and uh the clean RL1.
Oh, you know, I wonder who implemented I
Oh, you know, I wonder who implemented I
wonder if they did it off of this one
wonder if they did it off of this one
and they got a bad uh a bad port of it.
I'm down to do other stuff. Like I'm
I'm down to do other stuff. Like I'm
down to try this
The MCTS is probably a bit of a pain.
No code. Lovely.
Uh yeah, this is stuck at -21.
Uh yeah, this is stuck at -21.
Okay.
Did you figure it out? Try four. No. Um
Did you figure it out? Try four. No. Um
I think it just sucks.
Yeah, the problem is sucks.
All right, we'll do the from this, I
All right, we'll do the from this, I
guess.
Okay. So we do self.h
Then the dynamics function
predict state and reward
predict state and reward
given state and action.
What in the hell did this thing try to
What in the hell did this thing try to
Welcome.
Oh, wait. This is actually easier cuz
Oh, wait. This is actually easier cuz
this actually takes num actions at
this actually takes num actions at
hidden size, right? And it's
size plus action size.
Then we'll just do a self dot 8 o
like this. Exactly.
Okay. Okay. And then there's a policy
Okay. Okay. And then there's a policy
function,
a value function.
I think this is all you add.
Let me make sure I actually understand
Let me make sure I actually understand
what they're doing here.
replaces MCTS with sampling based
replaces MCTS with sampling based
gumball search.
Okay.
jointly.
No code for this is there.
Ah, it is
Please be good.
Worse.
Oh no, it couldn't. Ray Ray and freaking
Oh no, it couldn't. Ray Ray and freaking
Hydra.
their code.
Yeah, this is their code.
I think this is pretty much all of it
except the network maybe.
Um,
read Search.
Imagine
episode.
Do they use the MCTS though in the uh
Do they use the MCTS though in the uh
the eval loop or no?
or do they only use MCTS
or do they only use MCTS
in training?
Dynamic
function supervised learning method of
function supervised learning method of
using the true reward.
very complicated method.
Okay. But you do actually
You implemented this thing
over here
over here
monitor.
monitor.
I'm going to see what I can do about
I'm going to see what I can do about
Yes.
is dynamics, right?
is dynamics, right?
Is dynamics
H takes Then
take state action.
state action and outputs. Next state
state action and outputs. Next state
action.
Yes, it does.
Yes, it does.
Right. So, you have to give it the
Right. So, you have to give it the
action from the previous time step, I
action from the previous time step, I
suppose.
Oh, it outputs
Oh, it outputs
reward, not action.
reward, not action.
Duh.
So, you do reward decoder.
Let's just see if I can get a dynamics
Let's just see if I can get a dynamics
model uh to be trained jointly with
model uh to be trained jointly with
the environment for now uh with the
the environment for now uh with the
policy for now. So for this you need to
policy for now. So for this you need to
do
Well, actually you just train the policy
Well, actually you just train the policy
as normal, right? You use the policy as
as normal, right? You use the policy as
normal
and then you just add in an extra
and then you just add in an extra
dynamics, right?
You have state and action
all
I was
and do.
Uh you need the encoder.
Yeah, you need the uh the encoder.
So
So
8s
actions.
is going to be.
Yes.
Okay. And do like this.
Okay. And do like this.
And then we do
And then we do
dynamics.
Okay.
And then we just add these in.
Obviously there's no planning or
Obviously there's no planning or
anything.
Dynamics takes
You do not need num ops.
Uh, I see you have to embed the action.
Do action embed.
Okay. So, you just have this auxiliary
Okay. So, you just have this auxiliary
thing, right?
And
these losses should not be zero.
H. This one has to be rewards.
Why? That's why it doesn't matter.
Okay,
there's your reward loss.
And this does not get in the way at all,
And this does not get in the way at all,
obviously.
Now, how do we do this via search?
So they take H
This
and then they
and then they
sample action.
sample action.
Wait, I thought that they don't they do
Wait, I thought that they don't they do
this. Do they do this during training or
this. Do they do this during training or
just during um the collection of
just during um the collection of
actions?
Is this just done during the
templing based gumball search?
this. No, wait. Is this actually just um
it does
revised learning method?
Is this
Is this
is this totally different from dreamer?
forms planned planning. Yes.
forms planned planning. Yes.
But then it does this to pick better
But then it does this to pick better
actions, right?
That's just the
I can just figure out how this planning
I can just figure out how this planning
thing works. So you take H.
thing works. So you take H.
H gives you state.
You sample actions. It says from state
presumably with
what
the policy function right.
Okay. So we change it so that policy
Okay. So we change it so that policy
policy takes state right
is H.
So instead of encoder,
well h is the encoder, right?
not having good observation. Okay.
Now we're sharing an embedding layer.
Wrong.
Uh, it's not designed to be Um,
Okay,
Okay,
let's do uh encode observations.
as forward.
I returned everything correctly. That's
I returned everything correctly. That's
weird.
Well, we haven't made a huge ton of
Well, we haven't made a huge ton of
research progress today,
research progress today,
but you can't say it was for lack of
but you can't say it was for lack of
trying.
trying.
I've been pretty well focused up today.
Just how research freaking is
I guess I've been somewhat biased
I guess I've been somewhat biased
against how world modeling works since
against how world modeling works since
we spent a whole bunch of time trying to
we spent a whole bunch of time trying to
replicate the dreamer paper and it was
replicate the dreamer paper and it was
kind of crap.
kind of crap.
See about this instead.
search has always made more sense to me
search has always made more sense to me
than um
than um
streamer style stuff. Anyways,
Oh, okay. I see. I see it.
I guess I would feel kind of stupid
I guess I would feel kind of stupid
anyways if there were just a bunch of
anyways if there were just a bunch of
free winds lying around
free winds lying around
being this crap.
You need stupid
Actually,
what you want to do is this embed.
Okay. So, you have these separate
Okay,
Okay,
still good. Still trains.
We have a world model as part of this
We have a world model as part of this
now
now
with an LSTM.
with an LSTM.
Yeah. So, the the thing that we don't
Yeah. So, the the thing that we don't
know yet
know yet
is how we're going to do all the search
is how we're going to do all the search
crap.
So I can't tell are they are they
So I can't tell are they are they
basically using the model to like the
basically using the model to like the
planning model to get better actions
planning model to get better actions
fast
fast
or are they training on it as well?
They do have J and everything.
Is this how it works though?
They say that they have there's a Q
They say that they have there's a Q
function though as well, isn't There.
So they have this tree search thing.
So they have this tree search thing.
I saw the function in here.
I saw the function in here.
This is the hard part.
Most likely
I thought they said they don't use MCTS.
Use
gumball noise.
Yeah, they don't use this thing.
Jeez. Please.
policy.
Yeah, I don't understand how this uh
Yeah, I don't understand how this uh
this only has the policy.
how their search works.
gives you values,
policies, and best action.
They're call they're like calling
They're call they're like calling
something weird a policy, I think. Are
something weird a policy, I think. Are
they just calling it the probability
they just calling it the probability
vector over uh over actions?
I think they are.
give you the value, the policy
supposed to be Search.
Okay. Search.
This takes in the model batch size.
This takes in the model batch size.
dates, root values, root policy logic.
They have LSTM as well.
It's a pretty messy looking
It's a pretty messy looking
implementation.
and I should be able to figure out how
and I should be able to figure out how
this thing works
It doesn't seem like they have um
It doesn't seem like they have um
they must have a
they must have a
model
freaking lines agent.
Not the agent, dude. You have a train
Not the agent, dude. You have a train
function. That's not the agent.
Oh yeah, they have it.
Oh yeah, they have it.
This code I guarantee is super slow
but faster.
Main thing I need to understand I think
Main thing I need to understand I think
is the um
is the um
if I understand the MCTS
that up.
description is just not helping me.
description is just not helping me.
Nor is their code.
Oh, I think they don't actually have a
Oh, I think they don't actually have a
policy. I think they have um
policy. I think they have um
Okay,
I don't think they have a policy
I don't think they have a policy
network.
Is
You actually maintain a Q function here.
Random rollups for patient.
Okay, this I understand.
Yeah, we're kind of our bet.
I mean, I could technically try this.
I mean, I could technically try this.
The thing is, I know for a fact this
The thing is, I know for a fact this
paper is
paper is
At least like the reasoning in most of
At least like the reasoning in most of
it is
I mean, I could very easily.
I mean, I could very easily.
Do you know exactly why? I mean, we ran
Do you know exactly why? I mean, we ran
20,800 hours worth of ablations on that
20,800 hours worth of ablations on that
thing. We've got a pretty good idea, I'd
thing. We've got a pretty good idea, I'd
say.
Mhm.
I could add in
to add in like the auxiliary planning
to add in like the auxiliary planning
here.
thing is like
thing is like
I mean I could Right.
It's tough cuz I've been bitten so badly
It's tough cuz I've been bitten so badly
by this paper,
right?
Like the result is very good, but this
Like the result is very good, but this
absolutely should not have been
absolutely should not have been
published with um
published with um
the way that it is cuz it's like half
the way that it is cuz it's like half
the paper is long.
It is actually a decent baseline though
It is actually a decent baseline though
because it's way simpler to implement
because it's way simpler to implement
specifically what part. Uh, I know that
specifically what part. Uh, I know that
the all the little tricks that they add,
the all the little tricks that they add,
like the little tricks that they
like the little tricks that they
proposed are essentially just like
proposed are essentially just like
random clever sounding things applied to
random clever sounding things applied to
reinforce because they didn't have a
reinforce because they didn't have a
strong baseline. What's the advantage of
strong baseline. What's the advantage of
off policy besides experience replay?
off policy besides experience replay?
That's the advantage and that's a very
That's the advantage and that's a very
important one.
In my mind, it makes sense that search
In my mind, it makes sense that search
works. It actually it doesn't. In my
works. It actually it doesn't. In my
mind, it doesn't even make sense that
mind, it doesn't even make sense that
this necessarily works.
Yeah. Like all this crap is wrong.
Yeah. Like all this crap is wrong.
Um,
yeah, that problem bet is search is like
yeah, that problem bet is search is like
monstrously complicated to implement.
replay same end port for different
replay same end port for different
agents. So I don't know what you mean by
agents. So I don't know what you mean by
that.
Yes, it does work for num ms greater
Yes, it does work for num ms greater
than one,
than one,
but he asked for different agents, not
but he asked for different agents, not
different ends.
So presumably it's something different
So presumably it's something different
that he wanted.
Dreamer V3.
Dreamer V3.
Well, that and Efficient Zero.
Well, that and Efficient Zero.
I actually like the formulation in
I actually like the formulation in
Efficient Zero a lot more. It makes
Efficient Zero a lot more. It makes
sense to me that it works. Um, I think
sense to me that it works. Um, I think
it's backed by a lot more as well. It's
it's backed by a lot more as well. It's
just horrendously complicated
just horrendously complicated
because making search efficient with
because making search efficient with
like batch to reinforcement learning is
like batch to reinforcement learning is
not easy.
not easy.
I mean, search on its own is like
I mean, search on its own is like
already kind of non-trivial to implement
already kind of non-trivial to implement
like in like a good MCTS.
It wouldn't be that bad honestly, but
It wouldn't be that bad honestly, but
doing it in a context of neural nets is
doing it in a context of neural nets is
freaking hard.
849.
849.
I'm going to pick this up tomorrow. I
I'm going to pick this up tomorrow. I
think
it's Monday.
Yeah.
Okay.
Okay.
I've tried so many different things
I've tried so many different things
today. Um,
this was the
this was the
result of the off paw
result of the off paw
or the buffer.
Yeah, that's tough.
I know their code is slowed.
I know their code is slowed.
This does quite a bit better.
Well, we shall see, right?
At the very least.
At the very least.
We found out that there really aren't
We found out that there really aren't
any easy wins in offpaul.
any easy wins in offpaul.
Uh the whole kind of the vast majority
Uh the whole kind of the vast majority
of the offpaul work is just super
of the offpaul work is just super
freaking cursed. Honestly,
breakout is supposed to be the easy task
breakout is supposed to be the easy task
bet, but
it's kind of a mess that it's this
it's kind of a mess that it's this
screwy.
Also, literally, I under I undersshot
Also, literally, I under I undersshot
when I said puffer is a thousandx
when I said puffer is a thousandx
faster.
faster.
Like literally some of these offpaul
Like literally some of these offpaul
methods, they're not even set up by
methods, they're not even set up by
default for batched learning. It's crazy
or at least batched in uh inference.
You'd literally be faster on CPU under
You'd literally be faster on CPU under
that setup. for like small networks.
that setup. for like small networks.
Crazy.
Crazy.
Okay, though. Um,
Okay, though. Um,
it's almost 6. I am hungry. I got to get
it's almost 6. I am hungry. I got to get
food.
food.
I'm going running tomorrow
I'm going running tomorrow
and get myself
and get myself
situated
situated
and then uh I will be back on Monday.
and then uh I will be back on Monday.
I'm going to try to allocate as much
I'm going to try to allocate as much
week as much of next week as possible
week as much of next week as possible
to working on all this stuff.
to working on all this stuff.
We shall see.
Now this really freaking bothers me
Now this really freaking bothers me
though.
The fact that this
The fact that this
is even like possible that this works is
is even like possible that this works is
just that really bothers me.
Expert recording.
Expert recording.
Yeah, that works. That's super easy. Bet
it. Yeah, but it shouldn't be worse than
it. Yeah, but it shouldn't be worse than
um on policy is the thing.
Wow. Okay, folks. For those of you who
Wow. Okay, folks. For those of you who
have spent Saturday watching me uh bang
have spent Saturday watching me uh bang
my head against all this research, I did
my head against all this research, I did
a lot of different things today.
a lot of different things today.
Thank you for tuning in.
Thank you for tuning in.
Everything I do is right here at
Everything I do is right here at
puffer.ai
puffer.ai
free. It's open source part of the repo.
free. It's open source part of the repo.
Helps me out a ton. You want to get
Helps me out a ton. You want to get
involved Discord right here. Other than
involved Discord right here. Other than
that, you can follow me on X.
that, you can follow me on X.
Uh for more reinforcement learning
Uh for more reinforcement learning
content,
content,
whole bunch of articles here for
whole bunch of articles here for
beginners. Click the uh follow button
beginners. Click the uh follow button
wherever that is. All right.

Kind: captions
Language: en
Okay,
Okay,
we are back live
we are back live
morning.
A whole bunch of work to do today.
Maybe I should do
Maybe I should do
maybe I should do a quick little
maybe I should do a quick little
overview
overview
just to get my thoughts clear because
just to get my thoughts clear because
I've been thinking about this pretty
I've been thinking about this pretty
much non-stop.
much non-stop.
Um,
Um,
so there's all this off policy
so there's all this off policy
literature, right, that's built around
literature, right, that's built around
sample efficiency
sample efficiency
and there's on policy literature that's
and there's on policy literature that's
built around compute speed, compute
built around compute speed, compute
efficiency. Um, and the thing that's
efficiency. Um, and the thing that's
difficult to say is whether you can make
difficult to say is whether you can make
the on the off policy methods actually
the on the off policy methods actually
work in the high data regime well or
work in the high data regime well or
not.
not.
That's the thing that's tricky.
That's the thing that's tricky.
So, I'm trying to bring these two
So, I'm trying to bring these two
together in a way that you can kind of
together in a way that you can kind of
do both.
do both.
Welcome night in Vietnam.
I've never been to Vietnam. I do know
I've never been to Vietnam. I do know
that you guys have good food there.
One off.
I'm going to try one
I'm going to try one
one small change I think
before I
Okay. So, at least in this setup,
Okay. So, at least in this setup,
um, this doesn't make a difference.
um, this doesn't make a difference.
Seems
just makes it slower.
Of
course, it's like now we have a very
course, it's like now we have a very
aggressively tuned baseline.
If I try the uh the on policy baseline,
If I try the uh the on policy baseline,
right, we have this super aggressively
right, we have this super aggressively
tuned baseline
where we literally have the fastest
where we literally have the fastest
training in the world on that.
Kind of ridiculous to see, frankly.
If I do
If I do
I just cut the number of environments in
I just cut the number of environments in
half, right?
What happens?
Welcome to Nash.
Yeah. So, here's the problem, right?
Yeah. So, here's the problem, right?
Because the baseline is so aggressively
Because the baseline is so aggressively
tuned, this actually is going to do
tuned, this actually is going to do
worse.
The whole problem with RL research in a
The whole problem with RL research in a
nutshell, right?
The batch size the batch size gets
The batch size the batch size gets
smaller.
smaller.
The mini batch size doesn't though
The mini batch size doesn't though
actually.
So why do we think that this gets worse?
It should be better, shouldn't it?
Reduce your number of environments.
Got to just be something weird with the
Got to just be something weird with the
hypers. No.
Okay. Like if we think of it that way.
Actually, you know what? I have a
Actually, you know what? I have a
question. I have a question that I want
question. I have a question that I want
answered.
answered.
Let's see how fiddly RL is.
Let's see how fiddly RL is.
Um, when we just don't have
Um, when we just don't have
let's see like basically what happens to
let's see like basically what happens to
reinforcement learning with our current
reinforcement learning with our current
algorithm if we go to unoptimized dummy
algorithm if we go to unoptimized dummy
hyperparameters.
hyperparameters.
I want to get a sense of that problem.
Yes.
where they have their hypers.
I just want to see what happens if we do
I just want to see what happens if we do
this.
Like what happens if we basically So
Like what happens if we basically So
this is how most RL research is done
this is how most RL research is done
these days, right? Is people will just
these days, right? Is people will just
use like defaults
I mean, look at like it actually does do
I mean, look at like it actually does do
reasonably well. It's a huge difference,
reasonably well. It's a huge difference,
right? So, at least we have stuff that's
right? So, at least we have stuff that's
stable enough that we can do that and
stable enough that we can do that and
have it not break. I'm back. Welcome,
have it not break. I'm back. Welcome,
Rishi.
Okay. So, at least we expect
Okay. So, at least we expect
like some form of stability with this.
like some form of stability with this.
Okay. So, this is 500
Okay. So, this is 500
591.
591.
So, what happens if I just This should
So, what happens if I just This should
be more
be more
uh this should be like what smaller
uh this should be like what smaller
batches.
batches.
This should be more fresh data, more
This should be more fresh data, more
games.
games.
Let's see what happens.
like really unstable, huh?
And it's a little bit worse, though
And it's a little bit worse, though
probably not statistically. So,
probably not statistically. So,
that's very weird to me because
Yeah, that's super weird, isn't it?
You're getting more fresh games.
like more stable.
like more stable.
Oh yeah, look at that. Okay,
Oh yeah, look at that. Okay,
so there is something to the um the
so there is something to the um the
batch size.
probably something to do with just the
probably something to do with just the
changing of uh of the LSTM state now.
Of course, you can't really learn
Of course, you can't really learn
anything with this stupid tiny network.
I wonder if there is something
I wonder if there is something
something to do with the LSTM.
Well, yeah, actually you do cuz if you
Well, yeah, actually you do cuz if you
don't have we don't have frame stacking,
don't have we don't have frame stacking,
right? So, we kind of just need the LSTM
right? So, we kind of just need the LSTM
to be a thing.
Okay. So, actually that is better,
Okay. So, actually that is better,
right?
We had to increase the uh the horizon a
We had to increase the uh the horizon a
whole bunch
whole bunch
to make use of fewer ends.
to make use of fewer ends.
I wonder if now,
I wonder if now,
hang on, if I can go to one environment
hang on, if I can go to one environment
and then leave the horizon at 256.
Super long roll outs.
Okay, you kind of can.
So now
So now
now my question is
now my question is
does this let you increase update epochs
does this let you increase update epochs
or we just still in this case where
or we just still in this case where
towel has to be super on policy.
Okay, so
Okay, so
we increase sample efficiency a fair bit
we increase sample efficiency a fair bit
with default hypers
with default hypers
in just a few minutes of playing with
in just a few minutes of playing with
this, right?
a stable solve as well.
Just confirm that this one doesn't work.
But I suppose what is the
But I suppose what is the
what is the question
what is the question
from this?
Yeah. See, this is worse than before by
Yeah. See, this is worse than before by
a lot.
It still actually does like pretty much.
It still actually does like pretty much.
Yeah, that's a solve. But
Yeah, that's a solve. But
I think you're messing with the LSTM
I think you're messing with the LSTM
state too often is the problem
more than anything.
If I really wanted to crank sample
If I really wanted to crank sample
efficiency, how would I do it
this
this
just going to mess it up more. No,
probably better to do
Something like this.
How many M's do?
This would have to be an artifact. Well,
This would have to be an artifact. Well,
no, it's not just an artifact of the
no, it's not just an artifact of the
LSTM.
LSTM.
You also do need the bootstrapping term.
Lucky our implementation is so fast.
Oh, I suppose can we like just crank up
Oh, I suppose can we like just crank up
the number of updates even more
question. Is it possible to place a flag
question. Is it possible to place a flag
on an env
on an env
flag and building two-player turnbased
flag and building two-player turnbased
game
game
against other eval
against other eval
beta
beta
uh just mess with the clean puffer file
uh just mess with the clean puffer file
directly like in the eval loop there's
directly like in the eval loop there's
um you know there's a separate eval
um you know there's a separate eval
thing like it's I think the function's
thing like it's I think the function's
called roll out or whatever
called roll out or whatever
we still have a rollout function
Okay. No, it's this one right here. This
Okay. No, it's this one right here. This
eval function. It just has the loop
eval function. It just has the loop
directly. Just load your model in right
directly. Just load your model in right
here.
here.
Like a few line code Change.
Yeah, this is way worse, right?
Yeah, this is way worse, right?
I guess we'll see if it um
I guess we'll see if it um
it stabilizes,
it stabilizes,
but it's definitely not like
This maybe is just way too many gradient
This maybe is just way too many gradient
steps though.
steps though.
We can't do this many
this. Yeah. 128 m.
Yeah. So this is how you make everything
Yeah. So this is how you make everything
super flow.
Yeah, there's definitely some form of
Yeah, there's definitely some form of
something like this, right? Of course,
something like this, right? Of course,
it's going to nan out.
Got to like there's some form of this.
Got to like there's some form of this.
I'm sure
The problem is it's fundamentally
The problem is it's fundamentally
at least for the on policy case it's a
at least for the on policy case it's a
hardware inefficient setup
honestly
though. like
Where's my advantage?
They're just running like
I'm trying to think where I take this
I'm trying to think where I take this
from here. Right.
the problem with how fundamentally
the problem with how fundamentally
hardware inefficient a lot of these like
hardware inefficient a lot of these like
sample efficient focused methods are.
sample efficient focused methods are.
Like you literally may as well not use a
Like you literally may as well not use a
GPU for most of them. Of course, we nan
GPU for most of them. Of course, we nan
out still on this
The best one was um
this
still working on advantage. I'm trying
still working on advantage. I'm trying
to figure out I mean I'm generally
to figure out I mean I'm generally
trying to figure out how we bridge
trying to figure out how we bridge
computational efficiency and sample
computational efficiency and sample
efficiency.
I mean the behavioral clothing thing so
I mean the behavioral clothing thing so
far
I actually did run a sweep on it
I actually did run a sweep on it
overnight. So, this has mostly solved
overnight. So, this has mostly solved
the task. And this is fewer samples with
the task. And this is fewer samples with
untuned hypers um than my fast method
untuned hypers um than my fast method
with tuned hypers and about twice the
with tuned hypers and about twice the
time, which is not terrible.
Let me show you guys a cool result I got
Let me show you guys a cool result I got
last night.
last night.
So, I just reran the um
So, I just reran the um
I just reran the sweep right on the uh
I just reran the sweep right on the uh
the method we were developing yesterday,
the method we were developing yesterday,
which is this like advantage focus
which is this like advantage focus
behavioral cloning thing.
So, we got up to what is this? 300 and
So, we got up to what is this? 300 and
something. A bit over 300 score.
It's actually more samples than before.
Not great.
I mean it does learn though.
thing is we still have not found
we haven't found any version of this
we haven't found any version of this
that really lets us like crank on
that really lets us like crank on
on the compute.
We could go grab those hypers and see if
We could go grab those hypers and see if
we can get this to solve. That might be
we can get this to solve. That might be
a productive thing to do.
Okay. Well, we understand with this
Okay. Well, we understand with this
experiment, right?
We understand with this experiment that
We understand with this experiment that
our current on Paul's pretty robust and
our current on Paul's pretty robust and
we can make it at least probably like at
we can make it at least probably like at
least a factor of two more sample
least a factor of two more sample
efficient
efficient
already if we want to.
Then we have our onoff paw thing here
Then we have our onoff paw thing here
which is in
which is in
pretty much the same amount of time as
pretty much the same amount of time as
before with the untuned one. It gets 200
before with the untuned one. It gets 200
something score. And then there's a
something score. And then there's a
tuned version that trains for longer
tuned version that trains for longer
that can get over 300.
What happens if I go to two
What happens if I go to two
environments,
environments,
make no other changes? This.
This is supposed to be more data
This is supposed to be more data
scalable, isn't it?
Yeah, but just doing this doesn't make
Yeah, but just doing this doesn't make
it better.
Fusion.
Fusion.
Ow.
Ow.
How does that specifically help the
How does that specifically help the
sample efficiency compute trade-off
sample efficiency compute trade-off
thing?
But interestingly, this doesn't help.
Oh, wait. This is total
Oh, wait. This is total
mini batches.
So,
so then this shouldn't have done better.
so then this shouldn't have done better.
But wait, if I go to this is two
But wait, if I go to this is two
and then I go to this is four. Does this
and then I go to this is four. Does this
do anything?
The correct version of this I think
The correct version of this I think
loads the learning portion of the um the
loads the learning portion of the um the
training like way more heavily, right?
training like way more heavily, right?
That you like you can make the the
That you like you can make the the
sample collection portion will become
sample collection portion will become
less efficient fundamentally,
less efficient fundamentally,
but the hope is that you make up for it
but the hope is that you make up for it
by being compute efficient on learning.
This
else like
Are you right?
That's way worse. Really?
That I would have expected to be way
That I would have expected to be way
better.
was probably just super heavily
was probably just super heavily
overtuned, right?
You're better than this. Really?
I mean, there's no reason to expect like
I mean, there's no reason to expect like
the
the
basic Q-learning version of this to work
basic Q-learning version of this to work
either, right?
Does it make sense to go play with um
Does it make sense to go play with um
the BTR?
implementation of this.
Yeah,
Yeah,
I think it's just really hard to beat
I think it's just really hard to beat
this on policy like a good on policy
this on policy like a good on policy
baseline, right?
I guess this would be
would the other angle that I have here
would the other angle that I have here
be to just try to make my on policy
be to just try to make my on policy
baseline
baseline
um
um
more sample efficient
like bigger architecture and stuff. The
like bigger architecture and stuff. The
the problem here is that I've seen the
the problem here is that I've seen the
fundamental capability
like you need to be able to reuse some
like you need to be able to reuse some
samples
models
models
in behavioral cloning.
in behavioral cloning.
The model would be an independent
The model would be an independent
an independent improvement, wouldn't it?
The thing that I'm stuck on is how do
The thing that I'm stuck on is how do
you reuse samples
you reuse samples
without changing the whole algorithm to
without changing the whole algorithm to
make it slow?
Maybe we do just do the thing that I
Maybe we do just do the thing that I
planned the whole time before
which is we do this whole thing twice.
No, we're not using expert data. That's
No, we're not using expert data. That's
the point.
the point.
It's very easy with expert data.
We're trying to come up with um a
We're trying to come up with um a
formulation of behavioral cloning that
formulation of behavioral cloning that
works online. And the thing is that this
works online. And the thing is that this
actually
actually
the thing that's crazy is that this
the thing that's crazy is that this
actually kind of does work.
actually kind of does work.
I guess the thing we don't know is
No, we do know actually that this works.
No, we do know actually that this works.
This works better because we're reusing
This works better because we're reusing
data. We know that.
I could technically mix them.
Hey, Kvert.
What if instead of doing them
What if instead of doing them
separately, what if I mix them?
separately, what if I mix them?
That make any sense?
I think it does, right?
Let me think about that.
Okay, I have an idea.
This goes
at the bottom.
All right.
Now,
All right.
So this should be similar.
And now we should be able to play with
And now we should be able to play with
um
um
with one additional thing.
I'm basically just going to see what
I'm basically just going to see what
happens if I mix on and off Paul data.
We store it by value. We sample by
We store it by value. We sample by
advantage.
starts off his ones.
Do you see what I'm trying to do here,
Do you see what I'm trying to do here,
though?
And I honestly wouldn't even be
And I honestly wouldn't even be
surprised
surprised
if there's nothing magical about like
if there's nothing magical about like
the Q function formulation with offpaul
the Q function formulation with offpaul
both doing the same freaking advantage
both doing the same freaking advantage
estimate.
Yeah.
Okay. So, somehow this got to be
probably because I didn't do it down
probably because I didn't do it down
here, right?
Okay,
Okay,
so this should be
so this should be
again roughly the same algorithm, right?
Yes, this is roughly the same algorithm
Yes, this is roughly the same algorithm
from before.
And now
And now
let's see about the uh the losses that
let's see about the uh the losses that
we've got.
actually grab our policy loss.
Take this one here. Yeah.
This is the
policy gradient formulation. Right?
This doesn't learn anything.
Make sure I haven't broken something
Make sure I haven't broken something
yet.
I did break something else here, right?
I did break something else here, right?
Because otherwise
Because otherwise
otherwise this would work.
Changing robot from step controls to
Changing robot from step controls to
trajectory.
trajectory.
What do you mean changing to trajectory?
What do you mean changing to trajectory?
You don't want to
as an issue one control sequence for the
as an issue one control sequence for the
whole trajectory. You definitely don't
whole trajectory. You definitely don't
want to do that.
want to do that.
That's like more classic control.
What did I screw up here?
Change the return.
Change the return.
Oh.
Did the clip loss?
Do I go back yet again here?
Oh,
yes.
Hang on.
Hang on.
Something's super screwy here. Now,
This must have copied something wrong.
This must have copied something wrong.
Hang on.
[Music]
[Music]
What did I screw up from before?
This should be the on data, right?
This needs to be abs for
minus vantage.
Oh, wait. This is not
Oh, wait. This is not
Yeah, this is not getting optimized
the ratio that needs
this.
Okay, because this is just our on policy
Okay, because this is just our on policy
RL now here, right?
Yeah, this is just on policy RL.
Yeah, this is just on policy RL.
This is our good baseline. We know it
This is our good baseline. We know it
works. yada yada.
What happens if we I'm curious. What
What happens if we I'm curious. What
happens if we just sub this one little
happens if we just sub this one little
advantage calc?
It's funny.
It's funny.
It's actually um massively worse to only
It's actually um massively worse to only
learn from the positive examples, it
learn from the positive examples, it
seems.
All right. So, we'll do
All right. So, we'll do
we'll do we'll do this one.
Now, what happens if I uncomment
I uncomment this? So, we have offpaul
I uncomment this? So, we have offpaul
data.
Looks like we haven't totally broken
Looks like we haven't totally broken
everything, right? It still trains and
everything, right? It still trains and
it trains better than uh before like
it trains better than uh before like
with our pure IIL thing.
with our pure IIL thing.
Okay.
And we have
And we have
where our clipping coefficients in
where our clipping coefficients in
There.
We do want our clipping in here for
We do want our clipping in here for
sure, right? Betrays.
We have our clipping.
We're computing.
We're sampling by absolute advantage.
We're sampling by absolute advantage.
Oh yeah, this it's totally going to
Oh yeah, this it's totally going to
change the um
change the um
it's going to change the hypers here
it's going to change the hypers here
100%. Right.
Like if we wanted something closer,
Like if we wanted something closer,
we would do let's do like a basic one to
we would do let's do like a basic one to
one.
Or actually, it's got to be it's got to
Or actually, it's got to be it's got to
be double, right?
be double, right?
Oops. Not 120.
Thank you, Weston.
Not uh
Not uh
uh hopefully that doesn't open this on
uh hopefully that doesn't open this on
stream.
stream.
Come on, move the window. All right,
Come on, move the window. All right,
fine.
fine.
There's um there's an exploit going
There's um there's an exploit going
around in Discord where I can't actually
around in Discord where I can't actually
ban all the bots.
ban all the bots.
Like they literally don't have
Why don't I have
Oh, wait. I do have banan on this.
Go.
Is it in general as well
Is it in general as well
or did they just post it in dev?
Looks like just dev, right? I got that
Looks like just dev, right? I got that
one.
one.
Thank you, Weston.
Dummies.
Interesting that this is worse.
Interesting that this is worse.
This might just be the point at which we
This might just be the point at which we
need to optimize uh reoptimize hypers
need to optimize uh reoptimize hypers
and such.
I'm trying to think if this
I'm trying to think if this
what do we all think of this though as
what do we all think of this though as
an algorithm like
you just do on policy like you just do
you just do on policy like you just do
your standard RL
your standard RL
you keep an extra uh you keep the extra
you keep an extra uh you keep the extra
replay buffer
replay buffer
and then you trust uh PO clipping
and then you trust uh PO clipping
important sampling and retrace
important sampling and retrace
to keep stuff from going too far off.
That seems reasonable enough to me.
This would kind of be way better as
This would kind of be way better as
well, wouldn't it?
Like if we get this working.
Okay, it doesn't fix the roll out length
Okay, it doesn't fix the roll out length
thingy,
thingy,
but if we get this working
but if we get this working
then actually we can fix quite a few
then actually we can fix quite a few
other things using this.
other things using this.
Yeah. Yeah. No, we can actually fix
Yeah. Yeah. No, we can actually fix
quite a bit of stuff from That's
I like this. I think this is good.
And actually, we should be fine sampling
And actually, we should be fine sampling
absolute advantage like this as well.
absolute advantage like this as well.
Okay. So, let's see what other things I
Okay. So, let's see what other things I
could have screwed up in here.
Math heavy paper.
Math heavy paper.
Conditional important sampling for off
Conditional important sampling for off
policy. Let me look that up.
What the hell? No.
What the hell? No.
Get out, Google.
Welcome spy.
Do they run experiments on anything?
No, just giant math paper.
I understand that, but this is uh
I understand that, but this is uh
this is a research stream.
The vibe is more like uh what you'd find
The vibe is more like uh what you'd find
in an academic lab, man,
but with a heck of a lot more
but with a heck of a lot more
engineering.
Actually,
this might be kind of useful. I'm going
this might be kind of useful. I'm going
to have to read this like fully at some
to have to read this like fully at some
point, but that's this actually might be
point, but that's this actually might be
kind of useful because basically that
kind of useful because basically that
the offpole correction mechanism
the offpole correction mechanism
is the uh the important thing There.
So we would just decouple batch size,
So we would just decouple batch size,
right? To do this efficiently,
right? To do this efficiently,
we would just like decouple the batch
we would just like decouple the batch
Five.
I think we actually have the core idea
I think we actually have the core idea
in here now, don't we?
I do this
good online course way to get up to
good online course way to get up to
speed.
speed.
Uh, no, but I have a guide. I have a
Uh, no, but I have a guide. I have a
guide that links you to a bunch of uh of
guide that links you to a bunch of uh of
good reading resources and things. It's
good reading resources and things. It's
more peacemail. There's no single like
more peacemail. There's no single like
good course.
good course.
I'll link it to you.
If you go through this, you will
If you go through this, you will
actually be qualified to uh if you
actually be qualified to uh if you
actually do this stuff, you will be
actually do this stuff, you will be
qualified to like advance modern
qualified to like advance modern
research and do a whole bunch of
research and do a whole bunch of
applications. This is the exact thing
applications. This is the exact thing
that like our top contributors have
that like our top contributors have
done.
Okay. So here you see right if you don't
Okay. So here you see right if you don't
store any offpole data
store any offpole data
we get the expected result. We can do
we get the expected result. We can do
like anything. We can even like lo use a
like anything. We can even like lo use a
tiny amount of offpaul data.
Literally, even the tiniest amount of
Literally, even the tiniest amount of
off policy data messes stuff up. Huh?
off policy data messes stuff up. Huh?
Like a little bit
when doing algo research is the uptime
when doing algo research is the uptime
your concern? There are a lot of things
your concern? There are a lot of things
I'm looking at. So there is the high
I'm looking at. So there is the high
data regime
data regime
where you're just trying to solve the
where you're just trying to solve the
task in the like lowest compute, lowest
task in the like lowest compute, lowest
wall clock you possibly can. And now
wall clock you possibly can. And now
what I'm trying to do is I'm trying to
what I'm trying to do is I'm trying to
expand puffer to give it more axes on
expand puffer to give it more axes on
which we can crank up compute increase
which we can crank up compute increase
uptime uh but decrease sample usage.
So it's many things
all these freaking bots.
the fact that you add just the tiniest
the fact that you add just the tiniest
bit of data here.
bit of data here.
I should be able to see it even more
I should be able to see it even more
clearly if I do like this, right?
clearly if I do like this, right?
Whoops.
The 4096 samples,
a tiny fraction of a batch
and we don't even get 700 as a result.
and we don't even get 700 as a result.
Okay, let's see if we can figure out why
why this is
we decide to keep data based on value.
we decide to keep data based on value.
Right?
So you have your whole batch
and we've kept it based on on value.
I didn't discount the sum but still
the sum heristic
the sum heristic
and then it's sampled based on
and then it's sampled based on
advantage.
advantage.
So if this data has
So if this data has
this data had low advantage
there shouldn't be any problem.
This does not make sense, right?
This does not make sense, right?
We have this tiny additional amount of
We have this tiny additional amount of
off policy data that we're keeping.
the buffer at the end. So what we do
the buffer at the end. So what we do
here,
here,
so we always keep all the on policy
so we always keep all the on policy
data, right? And then you're also here,
data, right? And then you're also here,
I'll show you this is like a quick hacky
I'll show you this is like a quick hacky
implementation,
implementation,
but we keep all of the on policy data
but we keep all of the on policy data
and then we concatenate it with
and then we concatenate it with
additional data that we have in a
additional data that we have in a
buffer. So the buffer consists of
buffer. So the buffer consists of
however much off-paul data we want to
however much off-paul data we want to
store plus the onpaul data and then at
store plus the onpaul data and then at
the end
we uh we compute the value
we uh we compute the value
where is it?
Yeah. So here we compute the value of
Yeah. So here we compute the value of
all the samples
all the samples
and then we take the top k
and then we take the top k
and we leave these in the buffer. So the
and we leave these in the buffer. So the
idea is that we're keeping around our
idea is that we're keeping around our
highest value bits of data,
but it's a very small amount of data
but it's a very small amount of data
either way, right? And like up top,
either way, right? And like up top,
we're sampling by advantage.
we're sampling by advantage.
So if the data is not useful for
So if the data is not useful for
training, it should just not get
training, it should just not get
sampled.
That actually make a difference or did I
That actually make a difference or did I
just get lucky? Okay.
do one mini batch worth
do one mini batch worth
data.
Now there's like a fair bit of off pole
Now there's like a fair bit of off pole
data.
Okay.
Starts off as one
Good advantage.
This one wrong.
this like way worse. Is this noise?
Why would you update the value as well?
Yeah, you need to update um
Yeah, you need to update um
Oh, so this is going to screw with the
Oh, so this is going to screw with the
value function a whole bunch.
value function a whole bunch.
I see.
What if we do this?
But you want to sample by a fresh
But you want to sample by a fresh
advantage estimate, don't you?
Oh, I see. I think it's cuz I didn't
Oh, I see. I think it's cuz I didn't
maintain two estimates.
Does it matter what the original value
Does it matter what the original value
of the data is?
Like does the value loss clipping
Like does the value loss clipping
matter?
Maybe we can at least
Huh?
fine both ways.
Welcome, O Wayne.
What is the idea here with the clipping
What is the idea here with the clipping
of the value function?
of the value function?
Value minus.
Don't you explicitly not want to do
Don't you explicitly not want to do
that?
that?
I guess it depends what your motive is
I guess it depends what your motive is
here, right?
We also have the
policy clipping. thing.
I mean the clamped ratio is still um
I mean the clamped ratio is still um
still differentiable, right?
Am I wrong here?
Pretty sure the clamp ratio is still
Pretty sure the clamp ratio is still
differentiable.
more frequent updates of
more frequent updates of
Yeah. Um, this is the main thing, right,
Yeah. Um, this is the main thing, right,
is I haven't been able to get a good
is I haven't been able to get a good
setting where we can crank sample reuse
setting where we can crank sample reuse
and really get much out of it.
and really get much out of it.
It's actually quite difficult to get
It's actually quite difficult to get
that to work correctly.
Let's now increase the IL batch size to
Let's now increase the IL batch size to
something more reasonable.
So, we're going to actually do
That's like a reasonable amount of data
That's like a reasonable amount of data
to keep around.
And it looks like it destabilizes
And it looks like it destabilizes
learning a bunch
like fundamentally why right we should
like fundamentally why right we should
be doing
be doing
I guess it's technically possible I have
I guess it's technically possible I have
vrace done wrong
shouldn't be No.
Well, I mean, that's just like you can
Well, I mean, that's just like you can
just throw that out there. What What
just throw that out there. What What
does that mean here? Right.
does that mean here? Right.
We have several different forms of
We have several different forms of
clipping
clipping
that are supposed to prevent offpaul
that are supposed to prevent offpaul
data from hurting us at the very least.
To be fair, this can totally just be um
To be fair, this can totally just be um
this could totally just be the sampling
this could totally just be the sampling
coefficients, right?
coefficients, right?
Yeah, this could totally just be um
Yeah, this could totally just be um
prioritize replay screwing with this.
prioritize replay screwing with this.
Hang on.
This is a problem with fancy algorithms,
This is a problem with fancy algorithms,
right? A whole bunch of different knobs
right? A whole bunch of different knobs
that mess with you.
Okay. So, let's do
for now because we're not going to um
for now because we're not going to um
we're not going to like re- sweep this.
we're not going to like re- sweep this.
Let's do something that's more moderate.
Like
that's like enough data that we should
that's like enough data that we should
Let's see if we see an interference
Let's see if we see an interference
effect.
Okay. So, I mean it hurts learning, but
Okay. So, I mean it hurts learning, but
it's still it's still training like
it's still it's still training like
reasonably.
Let's try a few different things here.
Let's try a few different things here.
So like first
So like first
just set this to zero.
just set this to zero.
Recover our original
our original form here.
or update box.
or update box.
See what happens.
you kind of get the same result, right?
Now what happens?
If this solves,
you know, it really does feel like we're
you know, it really does feel like we're
throwing off our update somehow, right?
It really does feel like we're just
It really does feel like we're just
throwing off our updates. It's
Cuz otherwise this should be stable,
Cuz otherwise this should be stable,
right?
idea is very powerful on its own. I
does this not work
combined with um
Just curious here.
Just curious here.
Does the uh the same thing as I saw
Does the uh the same thing as I saw
before or not?
Yeah. So, this return combat level,
Yeah. So, this return combat level,
right? The thing is with data reuse, you
right? The thing is with data reuse, you
should be able to actually get something
should be able to actually get something
very quickly.
Is it just like overfitting it super
Is it just like overfitting it super
quick or something?
Is it ever a good idea to have learning
Is it ever a good idea to have learning
rate not decay to zero?
rate not decay to zero?
just increase next steps. I mean, there
just increase next steps. I mean, there
are a bunch of different learning rate
are a bunch of different learning rate
schedulers, right? You could mess with
schedulers, right? You could mess with
it. At least from the testing that I
it. At least from the testing that I
did, it seemed like having a learning
did, it seemed like having a learning
rate scheduler was much more important
rate scheduler was much more important
than the exact details of theuler.
Okay, so 20 million steps. We're already
Okay, so 20 million steps. We're already
up to
up to
uh.13
uh.13
013 return combat. So, it's actually
013 return combat. So, it's actually
already learning a little bit, right?
Okay. Okay. So, what I'm curious about
Okay. Okay. So, what I'm curious about
now, if I set this to zero,
now, if I set this to zero,
how's it compare? There.
Okay. Well, I think this is better. Um,
Okay. Well, I think this is better. Um,
this is your proof here.
this is your proof here.
We'll see if this catches up. I don't
We'll see if this catches up. I don't
think so.
Okay. So, this is this is your uh your
Okay. So, this is this is your uh your
proof of this, right? Um
proof of this, right? Um
we ran I think it was 27 mil before.
Give it 30.
Give it 30.
We'll give this one even a little longer
We'll give this one even a little longer
to see
It is actually doing an okay job on its
It is actually doing an okay job on its
own here.
What?
was actually stable and did help a
was actually stable and did help a
little bit here
little bit here
having um some previous data.
having um some previous data.
You would expect it to as well.
And this also this is one of the ones
And this also this is one of the ones
that worked with literally just um full
that worked with literally just um full
off Paul data.
off Paul data.
And this is with just a few samples of
And this is with just a few samples of
it actually as well. I wonder. So, does
it actually as well. I wonder. So, does
it get better or worse?
If I do,
does this get better or worse?
Yeah, that's definitely better.
And it makes sense that it would be
Yeah, it's massively better. Okay.
So, it could be problem dependent here,
So, it could be problem dependent here,
right?
right?
Can definitely be problem dependent.
Can definitely be problem dependent.
Use
Use
a restroom real quick. I'll be right
a restroom real quick. I'll be right
back.
Holy
40 million steps.
40 million steps.
It's already getting combat levels.
Yeah, that's like ridiculously better.
Yeah, that's like ridiculously better.
Okay, so
what do we do with that information?
with breakout. We have like a one one
here. It messes up. It could totally
here. It messes up. It could totally
just be the coefficients.
I don't even have pryo coefficients
I don't even have pryo coefficients
There.
[Music]
[Music]
like really just not stable or happy.
And this is a problem.
Um, we have it so that it's it appears
Um, we have it so that it's it appears
to be doing substantially better on our
to be doing substantially better on our
hard task of neural MMO,
hard task of neural MMO,
but having a replay buffer, it's very
but having a replay buffer, it's very
very fiddly with even like breakout.
Really shouldn't be either.
How stable is breakout generally?
Like if I go to one here to zero
Does breakout work if you just crank up
Does breakout work if you just crank up
to deox?
It should just not pull from the data
It should just not pull from the data
though if it's not good.
like it's uh it's already being sampled
like it's uh it's already being sampled
reasonably
the breakout with just eight update
the breakout with just eight update
epox.
One of the other wonky things about
One of the other wonky things about
Breakout is the episodes are so long.
Breakout is the episodes are so long.
like basically only getting to play one
like basically only getting to play one
full game in the whole training even
full game in the whole training even
though you're training this long
like it does train stably.
But you like almost get no sample
But you like almost get no sample
efficiency improvement. Almost none
efficiency improvement. Almost none
out of eight update epochs.
It is stable though.
And then it hands out. Well, I say that
And then it hands out. Well, I say that
and then it hands out right at the end.
and then it hands out right at the end.
Cool.
Cool.
Just the brink of being stable, I guess.
Just the brink of being stable, I guess.
Okay. What about eight update epochs on
Okay. What about eight update epochs on
um with all this IIL data in here.
Starts off good.
Then it just decides like actually never
Then it just decides like actually never
mind.
Know why?
I could compute like fraction on and off
I could compute like fraction on and off
Paul I guess.
Yeah, it really does get messed up
Yeah, it really does get messed up
though by this oddly enough.
Why?
I think it gets messed up even worse
I think it gets messed up even worse
than if you just give it um if I just
than if you just give it um if I just
give it one epoch, right?
give it one epoch, right?
Believe this is fast and
literally gets messed up
literally gets messed up
worse with more epochs.
Yes.
average samples from 10 epochs ago. Oh,
Oh, also experience replay has this
Oh, also experience replay has this
weird decay,
weird decay,
right? It has like this weird decay
right? It has like this weird decay
where it goes to it tries to go to
where it goes to it tries to go to
uniform by the end of it, doesn't it?
uniform by the end of it, doesn't it?
Yeah. But we don't want that for this.
It
It could totally just be the sampling,
It could totally just be the sampling,
right?
Something tells me it's not just a
Something tells me it's not just a
simple have to reweep, but we can try
simple have to reweep, but we can try
that.
Where's this clip?
I think this is like decent, right?
Give it a fixed reasonable
reasonable buffer size. Yeah,
I guess we basically see by the end of
I guess we basically see by the end of
this
this
does
I mean, it does seem weird to me that
I mean, it does seem weird to me that
it's like that
it's like that
that screwy.
that screwy.
Um,
on the other hand, it could literally
on the other hand, it could literally
just be the uh the prioritize replay,
just be the uh the prioritize replay,
right?
like breakout is kind of just a a pretty
like breakout is kind of just a a pretty
on policy end because
well is it
well is it
not really right? You're kind of doing
not really right? You're kind of doing
always the same thing in response to the
always the same thing in response to the
ball just trying to hit the ball
ball just trying to hit the ball
but the episodes are really long.
[Music]
Okay. Well, the other
Okay. Well, the other
there are so many big problems with
there are so many big problems with
this, man.
I think what's the next productive thing
I think what's the next productive thing
I can do on this?
I can do on this?
It could also work on the new end test
for For whatever reason, it like
for For whatever reason, it like
just don't believe that it's going to be
just don't believe that it's going to be
better if I just switch from this to um
better if I just switch from this to um
a Q-learning based thing, right?
a Q-learning based thing, right?
Like technically I am doing something at
Like technically I am doing something at
this point that is very much a uh
this point that is very much a uh
like an offpaul type thing.
Am I totally wrong to think that
Am I totally wrong to think that
that like if I just go for um
that like if I just go for um
do you learning based thing it's not
do you learning based thing it's not
going to be better.
We did get like
We did get like
the base DQN actually did kind of work.
Well, it worked on the simplest possible
Well, it worked on the simplest possible
problems.
I guess I could do that next.
I guess I could do that next.
And realistically, I I'm not going to
And realistically, I I'm not going to
sit here and wait for a sweep to run
sit here and wait for a sweep to run
today. I'm going to actually I'm going
today. I'm going to actually I'm going
to end up overnighting it. So, I may as
to end up overnighting it. So, I may as
well take a quick look and see if this
well take a quick look and see if this
happen to do. Yeah. So, like you do
happen to do. Yeah. So, like you do
something, but so far you don't really
something, but so far you don't really
do anything amazing.
I guess the next thing to look at would
I guess the next thing to look at would
be
be
just
just
forms of Q-learning.
What does SACE do that's different?
I'm just going to look at the clean RL.
This has SACE, right?
the triad paper.
I looked at this. Yeah.
Yeah. So this is this I understand this
Yeah. So this is this I understand this
is the key thing here right the problem
is the key thing here right the problem
is that the off policy methods the
is that the off policy methods the
theoretical guarantees are there um
theoretical guarantees are there um
without the multi-step bootstrapping
without the multi-step bootstrapping
bootstrapping but the algorithms just
bootstrapping but the algorithms just
suck
suck
and then when you start bootstrapping
and then when you start bootstrapping
you have to do all this off policy
you have to do all this off policy
correction crap that's not actually
correction crap that's not actually
making use of the data.
making use of the data.
So essentially you just you end up with
So essentially you just you end up with
a algorithm that can theoretically reuse
a algorithm that can theoretically reuse
data but is just worse than your on
data but is just worse than your on
policy algorithm.
That's the big problem as far as I've
That's the big problem as far as I've
seen it.
Right.
Right.
That's the big problem I've seen.
That's the big problem I've seen.
Does SACE have a smart way of dealing
Does SACE have a smart way of dealing
with this? I've never used SACE at all.
Okay. So, this actually does give you a
Okay. So, this actually does give you a
nice sample base thing, right?
Or freaking networks is obscene.
Is this literally a one step
on one step bootstrap?
It's literally a onestep bootstrap.
Maybe your problem would be
wrong prioritization.
wrong prioritization.
Yeah, it can as well.
I guess it's kind of crazy that SACE
I guess it's kind of crazy that SACE
even works, right?
even works, right?
I didn't realize SACE, they literally
I didn't realize SACE, they literally
have a onestep bootstrap in this thing,
have a onestep bootstrap in this thing,
right?
Could I not make a version of this like
Could I not make a version of this like
really fast?
If it's literally a one-step bootstrap,
If it's literally a one-step bootstrap,
right?
Why don't we just go grab this thing,
Why don't we just go grab this thing,
right?
I'm kind of curious.
I'm kind of curious.
is to like
I'm kind of curious.
Can you not mess with my torch version
EQN plus prioritize play
EQN plus prioritize play
single experience level.
I don't think that does work
I don't think that does work
phenomenally good. I think that's like a
phenomenally good. I think that's like a
super like algorithm, isn't it?
super like algorithm, isn't it?
Literally, you have baselines for that
Literally, you have baselines for that
stuff.
Like if rainbow and po are supposed to
Like if rainbow and po are supposed to
be competitive,
be competitive,
prioritize replay is like one of the
prioritize replay is like one of the
tricks that like is it's way worse
tricks that like is it's way worse
without. There are other tricks that
without. There are other tricks that
it's way worse without as well. This is
it's way worse without as well. This is
the problem is you actually need like
the problem is you actually need like
two or three major improvements before
two or three major improvements before
you even get close to um like pretty
you even get close to um like pretty
basic on policy methods.
Nothing close to PE but for off policy
Nothing close to PE but for off policy
it's good. Well, that's a useless
it's good. Well, that's a useless
algorithm though, right?
Like if you're underperforming base PO,
Like if you're underperforming base PO,
that's a useless algorithm.
And the problem is that there are a lot
And the problem is that there are a lot
of useless algorithms in off policy. And
of useless algorithms in off policy. And
you actually have to bolt a bunch of
you actually have to bolt a bunch of
crap onto the base things before you get
crap onto the base things before you get
something useful. Like here,
something useful. Like here,
this thing actually looks kind of good.
Okay, so this takes
Okay, so this takes
This takes rainbow, which is already a
This takes rainbow, which is already a
kitchen sink of random techniques, and
kitchen sink of random techniques, and
bolts on six more techniques. It removes
bolts on six more techniques. It removes
two. So, it brings you to 10 additions.
two. So, it brings you to 10 additions.
You have 10 additions to base DQN before
You have 10 additions to base DQN before
it's a usably good algorithm.
Wait, is this a single N
Is this literally intended for one
Is this literally intended for one
environment?
environment?
You've got to be kidding me.
You've got to be kidding me.
SACE, it's literally set up for one
SACE, it's literally set up for one
environment. That is the dumbest thing
environment. That is the dumbest thing
I've seen in a long time.
That's like crazy.
I wonder if their implementation even
I wonder if their implementation even
works for batched um
works for batched um
batched environments.
It looks like they're supposed to
Why does it always insert a random ass
Why does it always insert a random ass
character when I open a file? Drives me
character when I open a file? Drives me
insane.
All right.
doesn't print reward or anything.
It gets slower and slower and doesn't
It gets slower and slower and doesn't
print reward or anything.
Oh, I see.
Hang on.
Yeah, they also they assume a specific
Yeah, they also they assume a specific
format of stuff.
I honestly don't think this is going to
I honestly don't think this is going to
do anything. Like it's just
do anything. Like it's just
it's such a screwy regime of algorithms.
it's such a screwy regime of algorithms.
Like the majority of RL is just in such
Like the majority of RL is just in such
a screwy regime. It's like I forget that
a screwy regime. It's like I forget that
it's I always say, you know, puffer is a
it's I always say, you know, puffer is a
thousand times faster. Puffer thousand
thousand times faster. Puffer thousand
is a thousand times faster. It sounds
is a thousand times faster. It sounds
like a marketing ploy, but like you
like a marketing ploy, but like you
literally just run defaults on stuff and
literally just run defaults on stuff and
it's actually a thousand times faster.
it's actually a thousand times faster.
So, I don't
That's next. Yeah, exactly. That's the
That's next. Yeah, exactly. That's the
point. You can't It killed the entire
point. You can't It killed the entire
field and everybody went over to work on
field and everybody went over to work on
language models because you can't get
language models because you can't get
anything done at that speed.
anything done at that speed.
Now try to make it a thousand times
Now try to make it a thousand times
slower and have 10 times more code
slower and have 10 times more code
because no researchers can write code.
because no researchers can write code.
Like clean is the best thing out there
Like clean is the best thing out there
by a mile. It's actually simple. It's
by a mile. It's actually simple. It's
just slow.
Like honestly, the whole field owes a
Like honestly, the whole field owes a
blood debt to Costa for actually making
blood debt to Costa for actually making
this exist in the first place.
Probably without Clean R, I would have
Probably without Clean R, I would have
given up and just said this field is
given up and just said this field is
stupid by now. At least clean gave me a
stupid by now. At least clean gave me a
starting point for like, okay, I can do
starting point for like, okay, I can do
this but fast.
Don't use a lib torch like the C++
Don't use a lib torch like the C++
because literally nobody will use puffer
because literally nobody will use puffer
lib if I do that.
Why am I not getting any infos?
Huh?
H funny.
H funny.
Yeah, I the thing is it's C++ but I
Yeah, I the thing is it's C++ but I
don't even think it's like good C++,
don't even think it's like good C++,
right?
like
and
upper size. Where's batch size?
It's not like this magically works,
It's not like this magically works,
right? It's just super slow
to steer towards and reach your goal.
to steer towards and reach your goal.
Very easy with large step size.
Very easy with large step size.
Lower step size, better rewards. Uh
Lower step size, better rewards. Uh
yeah, there is a thing. So, you're
yeah, there is a thing. So, you're
increasing the credit assignment
increasing the credit assignment
horizon. So, the smaller the steps are,
horizon. So, the smaller the steps are,
the harder the credit assignment is. Um,
the harder the credit assignment is. Um,
if you need it to be small step size to
if you need it to be small step size to
be accurate, you do some sort of
be accurate, you do some sort of
curriculum learning thing, just start it
curriculum learning thing, just start it
near the goal and then, you know, move
near the goal and then, you know, move
it farther away over time or like
it farther away over time or like
randomize distance from goal, something
randomize distance from goal, something
like that.
like that.
It'll solve
Okay, so this is just getting slower and
Okay, so this is just getting slower and
slower and is not even solving anything.
It's really not even worth messing with
It's really not even worth messing with
anything in the hardware inefficient
anything in the hardware inefficient
regime, right?
Like you're just back to square one with
Like you're just back to square one with
like RL being a super cursed field where
like RL being a super cursed field where
nothing makes sense.
nothing makes sense.
I mean, this is literally running on
I mean, this is literally running on
like what 1% GPU utilization.
is the thing that killed the field in
is the thing that killed the field in
the first place.
This is kind of cool. At least this is
This is kind of cool. At least this is
up to like 500 or whatever under this
up to like 500 or whatever under this
circumstance.
Subst.
Yeah.
Okay, I don't know what the point of
Okay, I don't know what the point of
this exercise was other than to remind
this exercise was other than to remind
me how ridiculous everything is.
200. Yeah, exactly.
But you see what I'm trying to do,
But you see what I'm trying to do,
right, Tim? Like
right, Tim? Like
we do need to bridge the compute and
we do need to bridge the compute and
sample efficiency gap somehow.
And like realistically the only way to
And like realistically the only way to
do that was with massive sample reuse.
Because here's the problem, right?
Because here's the problem, right?
If I run a ton of parallel environments,
um, I don't really get to update my
um, I don't really get to update my
policy very much before like to get
policy very much before like to get
fresh data.
fresh data.
Like you can't really do sample f with a
Like you can't really do sample f with a
ton of parallel ends.
So the forward pass is just going to be
So the forward pass is just going to be
slow.
I guess I can try
maybe we just go over to some of the
maybe we just go over to some of the
offpaul stuff in puffer
offpaul stuff in puffer
robots and imitation. Imitation on its
robots and imitation. Imitation on its
own is easy.
own is easy.
It's like the mix data collection
It's like the mix data collection
setting that's hard.
What the heck?
here's your basic Q-learning thing.
He
Even with puffer, I got 10 FPS.
Even with puffer, I got 10 FPS.
Yeah, you definitely definitely 10 FPS
Yeah, you definitely definitely 10 FPS
is uh something's wrong.
is uh something's wrong.
Should never be 10 FPS.
Does anybody know um Soft Actor Critic
Does anybody know um Soft Actor Critic
doesn't do a distributional thing,
doesn't do a distributional thing,
right? Does anybody know how the hell
right? Does anybody know how the hell
soft actor critic works without uh
soft actor critic works without uh
distributional RL in
Okay. So this is the basic
Okay. So this is the basic
uh cart pole thing.
this give you QA Ready.
Got my RL exam on Monday. I forget that
Got my RL exam on Monday. I forget that
there are actually RL courses out there.
Do tend to forget that.
Is that any good?
Why is it like this?
Ah,
does get flattened.
you end up so it's like kind of Kevin
you end up so it's like kind of Kevin
uh you can actually get something to
uh you can actually get something to
kind of work literally just doing
kind of work literally just doing
behavioral cloning on like best data. I
behavioral cloning on like best data. I
also this morning here what I did is I
also this morning here what I did is I
took that and I integrated it
took that and I integrated it
um well I took that and I basically
um well I took that and I basically
added a a buffer to the existing puffer
added a a buffer to the existing puffer
online RL which seems to be doing better
online RL which seems to be doing better
and I'm relying on um a couple different
and I'm relying on um a couple different
off correction mechanisms to keep it
off correction mechanisms to keep it
stable and it's done like
stable and it's done like
it's done Okay, the thing that's weird
it's done Okay, the thing that's weird
is I actually have it making neural MMO
is I actually have it making neural MMO
learn way faster, which is like the
learn way faster, which is like the
hardest environment,
hardest environment,
but just having the data around at all
but just having the data around at all
seems to make it harder for stuff like
seems to make it harder for stuff like
Breakout. I can't figure out why.
Breakout. I can't figure out why.
like it should just not sample um the
like it should just not sample um the
offball data.
I mean, I I got to give this an honest
I mean, I I got to give this an honest
try. It's so far-fetched in my mind that
try. It's so far-fetched in my mind that
like you would be able to get anything
like you would be able to get anything
out of a one-step bootstrap.
out of a one-step bootstrap.
I didn't actually realize that the soft
I didn't actually realize that the soft
actor critic doesn't. It's literally
actor critic doesn't. It's literally
doing a onestep bootstrap, but it's so
doing a onestep bootstrap, but it's so
hard to compare because it's like the
hard to compare because it's like the
most inefficient thing ever conceived.
most inefficient thing ever conceived.
Saved.
endstep TD.
Yeah, that's like
probably Sutton and Bardo, right?
The problem is the endstep bootstrap
The problem is the endstep bootstrap
actually doesn't really work.
Behavioral cloning filtering
Behavioral cloning filtering
filter with advantages. Yeah. So, uh,
filter with advantages. Yeah. So, uh,
the thing I did yesterday was just store
the thing I did yesterday was just store
all the data in a buffer. Um, and
all the data in a buffer. Um, and
like keep the highest value data, not
like keep the highest value data, not
advantage. I believe I did it by uh
advantage. I believe I did it by uh
return like trajectory reward sum plus
return like trajectory reward sum plus
uh final value.
uh final value.
And then you sample it by advantage and
And then you sample it by advantage and
you just do behavior weighted behavioral
you just do behavior weighted behavioral
cloning which looks very similar to
cloning which looks very similar to
onpaul RL. And then today what I did is
onpaul RL. And then today what I did is
I just literally did the on policy RL
I just literally did the on policy RL
but I just do it with an additional data
but I just do it with an additional data
buffer that you can sample from.
buffer that you can sample from.
And then I'm hoping that between
And then I'm hoping that between
clipping and well the fact that between
clipping and well the fact that between
clipping and uh the vtrace component
clipping and uh the vtrace component
that's already embedded in puffer
that's already embedded in puffer
advantage that it keeps the data
advantage that it keeps the data
generally useful
generally useful
which it should to be honest because
which it should to be honest because
like the advantage computation should
like the advantage computation should
just go to zero or whatever if it gets
just go to zero or whatever if it gets
too off policy because it stops trusting
too off policy because it stops trusting
those samples I would think.
those samples I would think.
I'd have to double check. I'm pretty
I'd have to double check. I'm pretty
sure that's what should happen though.
sure that's what should happen though.
Was that advantage weighted regression?
Was that advantage weighted regression?
It was similar.
It was similar.
It was similar. Let's say
Think
I'm just going to have to go to the
I'm just going to have to go to the
other environment build stuff soon.
Drive me nuts.
direction with one policy with clipping.
direction with one policy with clipping.
Does that mean you also sort old pi
Does that mean you also sort old pi
waiting for all the data in the buffer?
waiting for all the data in the buffer?
You just have to store the log prob of
You just have to store the log prob of
the action you selected, right? That's
the action you selected, right? That's
all you need.
I mean, I guess you could technically do
I mean, I guess you could technically do
full KL div, but I'm pretty sure it's
full KL div, but I'm pretty sure it's
just you take the log prop of the action
just you take the log prop of the action
that the old policy selected, you
that the old policy selected, you
construct a ratio with the uh the new
construct a ratio with the uh the new
probability, right?
You could model the full distribution,
You could model the full distribution,
but I don't think that's the normal
but I don't think that's the normal
formulation.
Why is this so confusing?
Oh, I just have this set up in like a
Oh, I just have this set up in like a
totally weird way, I think, for um
totally weird way, I think, for um
I just have this set up in like a
I just have this set up in like a
totally
totally
weird way for this
It almost makes more sense to go off of
It almost makes more sense to go off of
the clean RLSC initially.
So freaking slow though.
And just tack it for now a little bit.
And just tack it for now a little bit.
Keep pushing on it just a little bit.
doing it this way.
Doing stupid things here.
Why is the mini batch like this as well?
Oh, cuz I have 8192.
Uh, I forgot about that.
I'm not really trying SACE. I'm just
I'm not really trying SACE. I'm just
trying to get something with a onestep
trying to get something with a onestep
bootstrap to do anything.
bootstrap to do anything.
I don't know. I'm messing around with a
I don't know. I'm messing around with a
bunch of different things at the moment,
bunch of different things at the moment,
Kevin.
Kevin.
Like, honestly, the whole literature is
Like, honestly, the whole literature is
just screwed.
just screwed.
Like, it's completely screwed.
be 16k.
be 16k.
Should be 16.
Okay. So this is now batched.
Okay. So this is now batched.
This is like batch Q-learning. Yeah.
Okay. Not amazingly stable, but it's
Okay. Not amazingly stable, but it's
it's a thing. It's there.
So, how is this supposed to do?
So, how is this supposed to do?
What does SACE do on top of this that's
What does SACE do on top of this that's
so special?
that like everyone always shows it
that like everyone always shows it
matching roughly po
the right on.
get this thing instead.
Next log.
Next log.
Get action on obs
of X. Okay.
of X. Okay.
So, this actually
Oh, okay. So, there's a separate actor
Oh, okay. So, there's a separate actor
in a soft Q network.
Okay,
trying to derive the entirety of RL.
trying to derive the entirety of RL.
It's not that I'm trying to derive the
It's not that I'm trying to derive the
rederive the entirety of RL RL
rederive the entirety of RL RL
literature from scratch. is that I'm
literature from scratch. is that I'm
trying to figure out how to make the
trying to figure out how to make the
things actually work because I mean if
things actually work because I mean if
you try any of them, none of them
you try any of them, none of them
actually work.
actually work.
I'm trying to fill in a bunch of holes
I'm trying to fill in a bunch of holes
basically.
I mean, we could make a version of this
I mean, we could make a version of this
fast, right?
Yeah, we could make a version of this
Yeah, we could make a version of this
fast, right?
You got like Well, you can get rid of
You got like Well, you can get rid of
some of them.
some of them.
This is like the double
This is like the double
the double DQN thing right here.
the double DQN thing right here.
So, there are other like there are other
So, there are other like there are other
offpaul variants where you get rid of
offpaul variants where you get rid of
two of these. Um,
I'm more confident that if we make it
I'm more confident that if we make it
work, I can come up with workarounds to
work, I can come up with workarounds to
strip things out and make it fast. Uh,
strip things out and make it fast. Uh,
the question is whether this actually
the question is whether this actually
does much on its own.
Literally best thing is going to do it
Literally best thing is going to do it
would be though for me to um like start
would be though for me to um like start
from 300 fresh again. Annoyingly enough,
the thing that I didn't realize. Okay.
the thing that I didn't realize. Okay.
So, the the key piece of missing
So, the the key piece of missing
information that I didn't realize here
information that I didn't realize here
was that Soft Actor Critic doesn't have
was that Soft Actor Critic doesn't have
a um a multi-step bootstrap and somehow
a um a multi-step bootstrap and somehow
does Okay.
does Okay.
Right.
That was the key thing that I didn't
That was the key thing that I didn't
understand.
which should make a difference.
Okay, let me use a restroom real quick
Okay, let me use a restroom real quick
and then we'll try we'll see whether uh
and then we'll try we'll see whether uh
this does anything.
I should be able to get this to match PO
I should be able to get this to match PO
but fast. We'll see. I'll be right back.
I have some ideas of how I'm going to do
I have some ideas of how I'm going to do
this.
So screwy though.
Okay. So, we have all these little
Okay. So, we have all these little
networks.
not normalize my observations. Do I not
not normalize my observations. Do I not
have that in the checklist?
have that in the checklist?
I'm pretty sure I have that in the
I'm pretty sure I have that in the
checklist.
checklist.
Normalize your observations.
Yep. I'll do that. Never did.
Well,
only so much I can do for that, eh?
Are you using reparameterize?
Are you using reparameterize?
Are you using reparameterization trick
Are you using reparameterization trick
to back propagate through actor
to back propagate through actor
through the actor update?
through the actor update?
Um,
Um,
no. What I was doing here is I have I
no. What I was doing here is I have I
have the four fully separate uh Q
have the four fully separate uh Q
functions and then the actor just has
functions and then the actor just has
its own LSTM
its own LSTM
and the Q functions don't.
What reparameterization trick
What reparameterization trick
through the actor update? I haven't even
through the actor update? I haven't even
gotten there yet at all. I'm literally
gotten there yet at all. I'm literally
just setting up optimizers.
get action.
I think we still get logits here.
Yeah, you just need to store actions.
Yeah, you just need to store actions.
No values to store.
And I think we're already at the
And I think we're already at the
training loop.
Yeah. So, we are at the training loop
Yeah. So, we are at the training loop
and
and
we're going to just ignore
we're going to just ignore
all this advantage stuff.
objects.
We do actions like this.
We do actions like this.
Then we need
Q
X
this right and then this doesn't take
this right and then this doesn't take
this is fine
this is fine
use action probabilities
min Q function Next target
and then
and then
next state action prop
you want full action props I guess.
you want full action props I guess.
Yeah. Does that just log softmax?
That just soft max actually
probs.
So is this not just um
So is this not just um
this is just soft max of log jets right
wait next state.
wait next state.
Next state.
Next state.
Oh, this is on next observations, I
Oh, this is on next observations, I
suppose.
suppose.
Okay,
we can do it either way.
We're just going to have to index
because it's going to be action robs
because it's going to be action robs
times
to domin and
Okay, we'll see if I've done this
Okay, we'll see if I've done this
correctly. Probably this is the uh the
correctly. Probably this is the uh the
crucial don't screw this up.
And then
Great.
Okay. So, we have to now be careful with
Okay. So, we have to now be careful with
this, right? Because this is
this, right? Because this is
the reward.
X
MB rewards.
This is the one that I have to be
This is the one that I have to be
careful with, right? Because it needs to
careful with, right? Because it needs to
be the next target. So, it's
be the next target. So, it's
this I believe.
this I believe.
And then this will have to be like
what the heck is this?
The torch. No grad.
The torch. No grad.
So this whole thing is under a nood. Red
one.
That's so inefficient the way that's
That's so inefficient the way that's
implemented.
implemented.
Okay, we'll have to change that. We can
Okay, we'll have to change that. We can
at least share some of this.
Oh, actually, no, we can't. Lovely.
I think it is something it's something
I think it is something it's something
like that.
And this one's got to be like
And this one's got to be like
this, right?
Why are we calling get action again?
Oh, cuz now we step
and now we do it again some for some
and now we do it again some for some
reason.
reason.
I mean, this is still
I mean, this is still
this is actually going to end up being
this is actually going to end up being
less complicated than the current
less complicated than the current
implementation of the algorithm because
implementation of the algorithm because
of the lack of prioritized replay. Of
of the lack of prioritized replay. Of
course, as soon as you go stick that
course, as soon as you go stick that
back in, it gets worse again.
Need action props as well.
And now this thing here, here's your
And now this thing here, here's your
actor loss.
Is there anything else?
Is there anything else?
No, just the target nets.
Wait, alpha loss.
Are there three optimizers here?
Oh, it's just um that's just for
Oh, it's just um that's just for
retuning. Fine. We can ignore this for
retuning. Fine. We can ignore this for
now.
All right. So, now we get rid of all
All right. So, now we get rid of all
this stuff.
this stuff.
like kind of okay.
No gradient clipping, nothing.
Tons of stuff we can do here.
Tons of stuff we can do here.
It's going to look something like this.
It's going to look something like this.
We could very easily clean this up.
Okay, we're actually getting like
Okay, we're actually getting like
reasonably far into the algorithm Now,
Yeah. So that gives you a min, right?
Action props are screwy.
Why is action props like this? Yeah.
Why is action props like this? Yeah.
Oh, cuz logits
Wait, what? This looks good to me,
Wait, what? This looks good to me,
doesn't it? Now
they not do this correctly.
Just do
Just do
go up one target.
Not next.
Want to get something to be useful
Want to get something to be useful
today.
Uhhuh.
Oh, you know why I see it?
That's dumb.
Oops.
And then we can sum this, right?
Yeah.
Then
Then
of course this chunk here.
of course this chunk here.
This this
This this
crazy they use four separate Q functions
crazy they use four separate Q functions
in this thing
and
and
this is probably the exact same thing as
this is probably the exact same thing as
before. Yeah,
we'll actually be able to reuse a couple
we'll actually be able to reuse a couple
of these computations though.
So, it'll get faster, but even like
So, it'll get faster, but even like
this, it we should still be able to get
this, it we should still be able to get
like a million steps a second or
like a million steps a second or
whatever.
Okay. Well, that is the initial
Okay. Well, that is the initial
implementation.
The initial implementation right there
The initial implementation right there
doesn't seem to do anything.
doesn't seem to do anything.
But we do have we have it running and
But we do have we have it running and
it's actually running pretty fast.
Now we just go bug hunting right.
Now we just go bug hunting right.
Oh well, I didn't sync the damn
Oh well, I didn't sync the damn
networks. That will do it.
target network frequency.
target network frequency.
Yeah, this whole like Q function
Yeah, this whole like Q function
synchronization garbage.
8,000
8,000
target network frequency.
Maybe
I actually think I just want to
and towel.
and towel.
They have this thing set not to one.
Set to one.
There
quite collapsed.
Go look at the uh the clean RL1.
Go look at the uh the clean RL1.
See what I've missed.
Many things.
Why do they do a categorical followed by
Why do they do a categorical followed by
a sample here?
Oh, because they this is Yeah, we have
Oh, because they this is Yeah, we have
this as well. This is fine.
But this takes policy parameters
But this takes policy parameters
with two parameters.
with two parameters.
Then this takes
Then this takes
what the heck? Where' the target
what the heck? Where' the target
optimizer go?
Wait, wait, wait. What? Huh?
Wait, wait, wait. What? Huh?
Oh, no you don't. you just get
Oh, no you don't. you just get
right the target just gets copied. It
right the target just gets copied. It
doesn't get optimized.
doesn't get optimized.
Um
and then the actor optimizer needs the
and then the actor optimizer needs the
encoder, the decoder and the LSTM,
encoder, the decoder and the LSTM,
right?
Okay,
Okay,
have all this stuff
have all this stuff
and we get to training.
function.
Okay. So, we have to make sure we did
Okay. So, we have to make sure we did
next observations correct, right?
next observations correct, right?
So, I do on MB ops, we do a sample
So, I do on MB ops, we do a sample
do view logs.
Okay.
Get action.
Get action.
So then this can go
So then this can go
wait
wait
this actually can go in this whole thing
this actually can go in this whole thing
can go into this nrad loop right
can go into this nrad loop right
this whole thing goes into the nrad
now budget action
do men
do men
alpha And then this is next state log I
Yeah. So this is the full distribution.
Now we do
Now we do
Q function
Q function
one. The target Q function on next obs.
one. The target Q function on next obs.
Okay. So I've done this on current obs,
Okay. So I've done this on current obs,
but I have them in a buffer that's going
but I have them in a buffer that's going
to get shifted by one. Anyways, we do
to get shifted by one. Anyways, we do
this min alpha next state log pi. Okay.
this min alpha next state log pi. Okay.
So I think next state action probs
So I think next state action probs
action probs and state log pi log log
action probs and state log pi log log
action probs.
I think that's fine.
And then you sum along this dim
And then you sum along this dim
and then you take
and then you take
you offset it right. So it's plus one
you offset it right. So it's plus one
minus terminals.
gamma
gamma
+ 1 - m times yeah this looks good to me
+ 1 - m times yeah this looks good to me
and then
and then
you do this q function I don't know if
you do this q function I don't know if
this gather is correct we'll double
this gather is correct we'll double
check this
okay but you you do this and then you
okay but you you do this and then you
have a q function loss which is mean
have a q function loss which is mean
squared error between
squared error between
next Q value,
which is already offset by one. Okay, so
which is already offset by one. Okay, so
I think this looks good to me.
I think this looks good to me.
And then this we don't care about.
Yeah, this looks good to me. Let's go
Yeah, this looks good to me. Let's go
make sure we're doing this this sampling
make sure we're doing this this sampling
operation. I know I've gotten wrong
operation. I know I've gotten wrong
before.
before.
Let me see if this looks Good.
So we do one
So we do one
zero
zero
one.
one.
Wait, what? Hang on. One 0 one one. So
Wait, what? Hang on. One 0 one one. So
one 0 one
one 0 one
one. Yeah, this actually looks correct
one. Yeah, this actually looks correct
to me.
to me.
Yeah. No, this is correct. So, huh, the
Yeah. No, this is correct. So, huh, the
gather is actually correct here.
What do I have? Uh, what do I have wrong
What do I have? Uh, what do I have wrong
then?
Let's log some losses.
Did I delete all of them already? Hang
Did I delete all of them already? Hang
on.
Where's the Q function loss?
Well, it doesn't blow up or anything.
Well, it doesn't blow up or anything.
There's another loss though, right? The
There's another loss though, right? The
actor loss.
Those
are both reasonable
are both reasonable
numbers.
numbers.
Let's check if the update frequency is
Let's check if the update frequency is
something totally obscene.
How often do they update the target
How often do they update the target
network?
Wrong one.
They update the target network frequency
They update the target network frequency
of updates for the target network.
So every 8,000 samples
which is in their case
which is in their case
that is
that is
quite uncommon because
quite uncommon because
like a hundred something updates, right?
And then what is this target entropy
And then what is this target entropy
scale?
Oh, auto tune entry. We're good.
Well, we're missing something. Yeah.
Oh, yeah.
Just making sure it's not a terminal
Just making sure it's not a terminal
issue. Way easier to catch on breakout.
issue. Way easier to catch on breakout.
But yeah, no, this doesn't learn
But yeah, no, this doesn't learn
anything.
anything.
Welcome tree form.
I'm currently messing around trying to
I'm currently messing around trying to
get um SACE into Puffer LIIB so I can
get um SACE into Puffer LIIB so I can
run some comparisons.
This is definitely not learning.
This is definitely not learning.
Well,
Well,
the rewards kind of goes up, but it's
the rewards kind of goes up, but it's
like it's below the noise threshold
like it's below the noise threshold
really.
Yeah. Soft actor critic.
The thing that I hadn't realized about
The thing that I hadn't realized about
soft actor critic, I figured that it had
soft actor critic, I figured that it had
um like an endstep bootstrap
um like an endstep bootstrap
like all of the other off policy
like all of the other off policy
methods. Uh but it doesn't. It actually
methods. Uh but it doesn't. It actually
does
does
well. I guess it depends what papers you
well. I guess it depends what papers you
believe, but comparably to PO with
believe, but comparably to PO with
uh only a one-step bootstrap.
Of course, I need to actually get it to
Of course, I need to actually get it to
do something for that to be uh evident.
We do get action.
Hang on.
Left max log action prom.
Yeah, this is right because we do
it.
Maybe there is a mismatch here, right?
Maybe you just want to return
problems.
problems.
Maybe you want to do like this.
So when you collect uh where you step up
So when you collect uh where you step up
the end.
the end.
Yeah. You just want to collect the
Yeah. You just want to collect the
actions, right?
actions, right?
So we actually
We don't really care too much.
Yeah, we actually do not care very much
Yeah, we actually do not care very much
about any of this. Let's get actions
man. So we can just get over here,
man. So we can just get over here,
right?
We don't need MB log props.
We don't need this. We don't need this.
We don't need this. We don't need this.
Right.
Right.
Some ways, if this works, it will
Some ways, if this works, it will
actually simplify things a little bit.
We have action probs.
So this should just be probs log prop
So this should just be probs log prop
like this. Yeah.
like this. Yeah.
Prob log prob yada yada
Prob log prob yada yada
prob.
Um,
I'll just
Bob,
Bob.
Find a bug.
Uh, this needs to be
Uh, this needs to be
Oh, and log prop also needs to Okay.
Watch.
Okay,
now this does need to get reshaped,
now this does need to get reshaped,
right?
You
You
Hub.
Seriously?
Oh,
Oh,
hang on.
Okay. So, this is now running.
Okay. So, this is now running.
Still doesn't train.
Still doesn't train.
Really?
Really?
You do all that, it still doesn't train.
Hey, we go hunting for more bugs.
just fighting you. Yep.
Compile false the whole time.
programming would not be fun if it
programming would not be fun if it
worked the first time all the time. I
worked the first time all the time. I
disagree. It would be dramatically more
disagree. It would be dramatically more
fun
fun
because I would just solve one thing,
because I would just solve one thing,
move on, solve the next thing, and then
move on, solve the next thing, and then
all the problems would be solved.
I think if there's like anywhere obvious
I think if there's like anywhere obvious
I would have messed
Hang on. This this is on obs
Hang on. This this is on obs
actions, right?
actions, right?
Next Q value.
Next Q value.
You sum these. Yeah.
What the point of that is, but sure.
And here you have your targets.
And the next one is going to be acquired
And the next one is going to be acquired
by
Yeah, I think this is fine.
Yeah, I think this is fine.
Rewards
Rewards
of
one
This seems fine.
This seems fine.
That seems completely fine.
That's all there is. Like there's really
That's all there is. Like there's really
not that much to this algorithm.
Add backwards up.
the actor loss is going to be probs
alpha
and this can be log
doesn't make a difference So
me in on this
me in on this
min q function values.
This one doesn't need to be offset.
This one doesn't need to be offset.
Right.
Right.
This one's good.
We use action props for temperature
We use action props for temperature
loss.
loss.
No, this is not needed. It's just a
No, this is not needed. It's just a
fixed alpha.
And where's alpha?
Alpha times next.
Whole thing looks reasonable to me.
Oh, okay. It's
Oh, okay. It's
the fact it's jumping around.
the fact it's jumping around.
It's probably doing something.
It's probably doing something.
Might need to just fix a bunch of other
Might need to just fix a bunch of other
things.
Just to be sure that it's not just this
Just to be sure that it's not just this
jumping around the scores.
Oh, it's not it. H
Oh, it's not it. H
I still have bugs in here then. Yeah,
I still have bugs in here then. Yeah,
it should pretty much any reasonable
it should pretty much any reasonable
algorithm should jump above uh above
algorithm should jump above uh above
seven
seven
almost instantly.
It is weird that this is slowly ever so
It is weird that this is slowly ever so
slightly going up, I'll say, but
it's not like a significant
result
result
like even directionally.
You can probably just maximize entropy
You can probably just maximize entropy
and do better than this.
Yeah, but like it should
Yeah, but like it should
any reasonable algorithm should have
any reasonable algorithm should have
solved uh should have gotten it way way
solved uh should have gotten it way way
before then. Like you can literally run
before then. Like you can literally run
vanilla DQN and get that immediately.
Far more likely I have something bugged.
There we go.
Let's see if it's consistent or just
Let's see if it's consistent or just
jumpy.
Okay. So, let's get this to be stable
Okay. So, let's get this to be stable
and see if we can run it on breakout.
I don't know how frequently we should
I don't know how frequently we should
sync.
Like do you want it to be synced very
Like do you want it to be synced very
frequently or is that screw it up?
That screws it up, Head.
tow equals
tow equals
05. Does that make a big difference?
This update rate seems to matter a
Oh well, learning rate.
Way worse.
Way worse.
believe so. Okay, we'll try that. It's
believe so. Okay, we'll try that. It's
weird because the cleaner defaults don't
weird because the cleaner defaults don't
even have um they just have it set to
even have um they just have it set to
one.
I mean, it's possible I still have major
I mean, it's possible I still have major
problems in the implementation as well
problems in the implementation as well
here, right?
Yeah, cartpole is like so incredibly
Yeah, cartpole is like so incredibly
easy.
You can have stuff be completely wrong
You can have stuff be completely wrong
and just still kind of do stuff on that.
Are you just supposed to run way more
Are you just supposed to run way more
updates?
I don't think so. Right.
Not like magically. Oh yeah. Samples.
Four neurons.
Four neurons.
Yeah, you can solve it with um you can
Yeah, you can solve it with um you can
solve it with four neurons. Sure. I
solve it with four neurons. Sure. I
think it's maybe Pong is four. a lot of
think it's maybe Pong is four. a lot of
the Atari is six,
the Atari is six,
but that's not really the point of Yes.
See?
See?
Figure out what it is.
You have your actor optimizer.
Did I miss anything crucial here?
So,
same learning rates.
Now factor.
This kind of works. Carpole, right?
And just mess with the hypers a little
And just mess with the hypers a little
bit.
really.
I mean, it should just like
I mean, it should just like
literally Q-learning works on this. It
literally Q-learning works on this. It
should totally work.
Is it like too much entropy?
No, that actually breaks it instantly as
No, that actually breaks it instantly as
well.
thing. Nine.
speed.
Make sure I did that in the right
props. Log props, right?
Prob
target
and
that's definitely fine.
This stuff is like pretty basic though.
This stuff is like pretty basic though.
Now
the value
the value
the loss
supposed to match up.
So this is
take your OBS
N
alpha time log prop
minus
minus
in
actor.
Yeah,
as far as I can see, I have this set up
as far as I can see, I have this set up
correctly and it's like really
correctly and it's like really
underperforming.
really really underperforming
epochs.
What else would I have to mess with?
Yeah, doesn't work on pong at all.
somehow works on just cartpole.
somehow works on just cartpole.
Guess the easiest problem
and not even super consistently.
and not even super consistently.
Yeah. See, like that's a mess.
What?
This should definitely
This should definitely
definitely like work way better.
Oh, wait. Hang on.
We do use MB actions here. Yeah,
We do use MB actions here. Yeah,
that was the one thing I wasn't sure is
that was the one thing I wasn't sure is
if I had grabbed the uh the actions
if I had grabbed the uh the actions
correctly.
Seems like we do, though.
Wait, why is this a sum?
That makes sense.
Next Q value.
Next Q value.
This is a gather.
Weird that this is a sum.
How does this make sense as a sum?
H.
Ah.
Nothing else in here.
Nothing else in here.
This is literally just a sample.
Target loads the state deck of the uh
Target loads the state deck of the uh
target loads the state deck
of
of
fine.
I like not optimizing the main network
I like not optimizing the main network
somehow.
somehow.
Encoder decoder
where I should be
the self policy. Got your encoder. You
the self policy. Got your encoder. You
got your decoder. Oh,
got your decoder. Oh,
that's bad. Yeah. Okay, that'll do it.
that's bad. Yeah. Okay, that'll do it.
Hang on.
Hang on.
Yeah, hang on. Hang on.
Oh, wait. Shouldn't they be linked?
They should be linked. Don't know.
This might be doubling up.
Like they actually they should be the
Like they actually they should be the
same, right?
same, right?
Hang on. Yeah, I thought I'd seen
Hang on. Yeah, I thought I'd seen
something, but it's that's not it.
I mean, then it literally wouldn't be
I mean, then it literally wouldn't be
able to learn
Yeah, it's the LSTM weight that you
Yeah, it's the LSTM weight that you
want.
want.
Anyways,
that's it.
that's it.
player norm.
Uh I don't have a learning rate schedule
Uh I don't have a learning rate schedule
on this actually, right? That's kind of
on this actually, right? That's kind of
a big deal.
Okay.
And it's still not amazing. thing.
How do you make soft actor critic
How do you make soft actor critic
actually learn something
does this but like come
Oh,
this one doesn't take
this one doesn't take
I don't know if that changes anything.
actually don't think that does change
actually don't think that does change
anything.
Heck,
Heck,
we just are we have to search over
we just are we have to search over
random seeds now. Is this where we're
random seeds now. Is this where we're
at?
Is there no um
where's the
where's the
this gather here?
this gather here?
Yeah. So, this Q function doesn't use
how can you tell her the score?
how can you tell her the score?
Yeah, it's the score. So, cartpole, you
Yeah, it's the score. So, cartpole, you
just get a point for every step until
just get a point for every step until
you fall over. And if you get 200
you fall over. And if you get 200
points, then you're balancing it
points, then you're balancing it
perfectly.
Something is just wrong. I don't know
Something is just wrong. I don't know
what.
what.
trim both of these against the same
trim both of these against the same
target here.
target here.
Do this gather which I double checked
Do this gather which I double checked
this gather and this but gather is fine.
do policy
do policy
Q function one Q function two
boards plus 1 - done times gamma
boards plus 1 - done times gamma
in Q function on the next steps do next
in Q function next target probs
Min of the two targets minus alpha
Min of the two targets minus alpha
prop
have
have
log prop pro log prop here from ours
loget state
empty and this one which is fine. Man,
Yeah, these things are not helping.
is so incredibly difficult to get right.
is so incredibly difficult to get right.
Is it a bug? Is it the hyper suck? Does
Is it a bug? Is it the hyper suck? Does
the algorithm suck?
all over the place. I mean, it's like at
all over the place. I mean, it's like at
least there's something correct here.
least there's something correct here.
Otherwise, you wouldn't get um you
Otherwise, you wouldn't get um you
wouldn't see any learning, right?
adjustment of weight. No, if you
adjustment of weight. No, if you
randomly are adjusting weights, you
randomly are adjusting weights, you
won't get a you won't get a score like
won't get a you won't get a score like
this.
It's like directionally correct at the
It's like directionally correct at the
very least
and I don't but I don't know like what
and I don't but I don't know like what
is
is
I have no idea what the problem is,
I have no idea what the problem is,
right? If it's just hypers, if I have a
right? If it's just hypers, if I have a
bug Hug.
on the original code.
on the original code.
The problem is these are like these
The problem is these are like these
algorithms are fancy enough that you're
algorithms are fancy enough that you're
not going to ever really be able to do
not going to ever really be able to do
that. Like if you're never going to be
that. Like if you're never going to be
able to really get like one to one
able to really get like one to one
comparison like this. What's the
comparison like this. What's the
baseline score with PO instantly solved?
Like by the time it's printing out logs,
Like by the time it's printing out logs,
it's already solved.
And this thing should just have
And this thing should just have
instantly solved everything.
Especially like I'm giving it so much
Especially like I'm giving it so much
more fresh data as well.
And this is random perf here.
13 seconds.
13 seconds.
Yeah, exactly. I mean, on a faster
Yeah, exactly. I mean, on a faster
machine, it'll like be basically
machine, it'll like be basically
instant.
Okay. I mean, this does do something
Okay. I mean, this does do something
now, right?
like it's not random,
but it's just bad. And I don't know why.
Algorithm
The hell could I have possibly done
The hell could I have possibly done
wrong here?
are the Q I mean no it shouldn't matter
are the Q I mean no it shouldn't matter
that the QETs I have kind of suck
And nothing.
And nothing.
Nothing at all.
Uh, why the heck did Reream just sign me
Uh, why the heck did Reream just sign me
out?
There we go.
Why the heck is my reream being dumb?
Okay.
Okay.
Well, whatever. I got it now.
Okay.
You don't use any of this stuff Now,
You'll get gradient clipping
when you were at Stamford or I don't
when you were at Stamford or I don't
know MIT for your undergrad. Stanford
know MIT for your undergrad. Stanford
was undergrad. MIT was PhD. Do you have
was undergrad. MIT was PhD. Do you have
like people who are admitted to school
like people who are admitted to school
with bad grades
with bad grades
accepted due to impressive
accepted due to impressive
extracurricular?
extracurricular?
Uh, I don't know about impressive
Uh, I don't know about impressive
extracurriculars. I definitely know a
extracurriculars. I definitely know a
bunch of people who I have no idea how
bunch of people who I have no idea how
they got in.
It's tough to get in with bad grades
It's tough to get in with bad grades
because it's like
having bad grades. It's like it's such
having bad grades. It's like it's such
an it's such like a a basic checkbox.
an it's such like a a basic checkbox.
It's like failing to get up and tie your
It's like failing to get up and tie your
shoes in the morning, you know, on the
shoes in the morning, you know, on the
way out.
how they entered.
I mean, there's like a huge gap. Like,
I mean, there's like a huge gap. Like,
you meet people who are absolutely
you meet people who are absolutely
brilliant, right? And then you meet
brilliant, right? And then you meet
people who are like, "All right, this
people who are like, "All right, this
guy's a I have absolutely no idea
guy's a I have absolutely no idea
why he's here, but okay, I guess.
Well, yeah, let's just go get grad.
Well, yeah, let's just go get grad.
Norm.
me just one thing real quick.
me just one thing real quick.
Tab zero grad. I don't think it makes a
Tab zero grad. I don't think it makes a
difference
difference
unless I you know it's possible I have
unless I you know it's possible I have
my loop set up differently
because that wouldn't make any sense
because that wouldn't make any sense
though, right?
Yeah, cuz it has to be optimized
Yeah, cuz it has to be optimized
obviously, right?
Otherwise, you wouldn't see if I had
Otherwise, you wouldn't see if I had
that as a mistake, then you would
that as a mistake, then you would
literally wouldn't see anything work.
I will say the one thing I think is kind
I will say the one thing I think is kind
of ridiculous is um
of ridiculous is um
that people get rejected from PhD
that people get rejected from PhD
programs over not having great grades.
programs over not having great grades.
It's like
It's like
I almost like question if if you do have
I almost like question if if you do have
perfect grades coming out of undergrad,
perfect grades coming out of undergrad,
I almost question what the hell you've
I almost question what the hell you've
been doing with your time.
Okay,
Okay,
this replicable
Not precisely.
Not precisely.
A heck of a lot more stable though,
A heck of a lot more stable though,
right?
frequency matters a lot here, huh?
Every 64 steps maybe.
Okay. So, that is
and that's closer.
interesting. It breaks with four update
interesting. It breaks with four update
epochs. Really?
This noise
This noise
didn't break.
didn't break.
The whole point of these
And okay, it's not terrible.
Yeah, literally no signal in there. None
Yeah, literally no signal in there. None
at all.
The heck?
The heck?
That's like bizarre.
That's like bizarre.
You also shouldn't need um
You also shouldn't need um
need that number of update epochs for
need that number of update epochs for
freaking cartpole. I mean cartpole
freaking cartpole. I mean cartpole
should be solved in like a tiny number
should be solved in like a tiny number
of steps. This shouldn't be remotely a
of steps. This shouldn't be remotely a
challenge.
I only have it kind of training.
where it's like one of the only damn
where it's like one of the only damn
fields where you can have worked in the
fields where you can have worked in the
space for 10 years. You can have like
space for 10 years. You can have like
very deep knowledge on a ton of
very deep knowledge on a ton of
different things in this space and like
different things in this space and like
literally basic algorithm implementation
literally basic algorithm implementation
from scratch. Good luck. Not going to
from scratch. Good luck. Not going to
work.
Is there like do I have like an extra
Is there like do I have like an extra
log or something? I've checked this so
log or something? I've checked this so
many times.
many times.
I don't know what else it would be.
this puffer lab. Where's the clean arm?
This shouldn't be important, right?
This shouldn't be important, right?
It literally is random when you start
It literally is random when you start
it. Anyways,
get this
This actually does tell me sample reuse,
This actually does tell me sample reuse,
right?
right?
Update frequency. Is it four?
Yeah. So they have 8x sample reuse.
I guess it is totally possible, but it's
I guess it is totally possible, but it's
just like a crappy algorithm that you
just like a crappy algorithm that you
have to crank the uh
have to crank the uh
compute on, right?
But 8x that's only like um
But 8x that's only like um
x more than on paul
for these same tasks.
Oh,
you're los
Robs.
You know, it's really not that much
You know, it's really not that much
code.
code.
There's really not that much space for
There's really not that much space for
stuff to be wrong. Um,
particularly for it to be doing worse
particularly for it to be doing worse
more samples.
I mean, still like, come on. It's more
I mean, still like, come on. It's more
stable than it was, but it still it
stable than it was, but it still it
doesn't freaking solve cart pole. Take
what else is in SACE doesn't even have
what else is in SACE doesn't even have
like prioritize replay or anything by
like prioritize replay or anything by
default, right?
default, right?
It just has um
it just has like a circular buffer
it just has like a circular buffer
thing.
There anything else I haven't messed
There anything else I haven't messed
with?
with?
Feel like I've messed with everything.
Why does it take so long for it to start
Why does it take so long for it to start
learning like anything?
Weird how it does that.
the QET like the architecture is pretty
the QET like the architecture is pretty
dumb but like
dumb but like
freaking cartpole
Yeah, it's freaking cartpole. It
Yeah, it's freaking cartpole. It
shouldn't matter.
I understand this
policy.
fun.
fun.
Got to check one small thing.
I'm kind of at a loss here.
Like I expected this to do something.
Do I just have like crazy hyper prams or
Do I just have like crazy hyper prams or
something? But there really aren't even
something? But there really aren't even
that many hyper prams with this, right?
Like we cut out most of them.
Literally, we've cut out almost all the
Literally, we've cut out almost all the
hypers.
This massively like No. Right. There's
This massively like No. Right. There's
no way,
right?
It shouldn't be this brittle at this
It shouldn't be this brittle at this
scale.
scale.
Like basically, you literally can solve
Like basically, you literally can solve
this problem for reference just by
this problem for reference just by
behavioral cloning the best lucky
behavioral cloning the best lucky
segments that you get.
I'm giving it on policy data pretty
I'm giving it on policy data pretty
much, which should be better than
much, which should be better than
normal.
normal.
There should be like a best case
There should be like a best case
scenario for it pretty much. It should
scenario for it pretty much. It should
be super easy
be super easy
to have this solve.
Like look, if I got this to solve long
Like look, if I got this to solve long
and like decent enough on breakout and
and like decent enough on breakout and
then I have to tune it, that's one
then I have to tune it, that's one
thing. This is just not doing
thing. This is just not doing
remotely close to how well I would
remotely close to how well I would
expect it to.
expect it to.
I mean, do we have to think about epochs
I mean, do we have to think about epochs
here?
64.
I mean, we know that this is sensitive,
I mean, we know that this is sensitive,
right?
right?
Oops.
Oops.
We know that this is sensitive.
Maybe it's every 16 epochs we do this.
Okay,
Okay,
there you go. So, it's really really
there you go. So, it's really really
sensitive to um
sensitive to um
the uh the update
interval here. It's still not stable, by
interval here. It's still not stable, by
the way.
Why doesn't this give you a stable
Why doesn't this give you a stable
solve?
Let me go double check. make sure that
Let me go double check. make sure that
the uh the reward thing is what I think
the uh the reward thing is what I think
it is.
I want to just be sure that I haven't
I want to just be sure that I haven't
screwed this up.
Okay, I think that this is confirmation,
Okay, I think that this is confirmation,
right?
right?
Just can't learn
Just can't learn
That's confirmation I had it right this
That's confirmation I had it right this
way
way
is the one I would expect anyways
plus the next Q target.
Let's see. Can we get away with updating
Let's see. Can we get away with updating
this every eight?
this every eight?
That the play
see this completely crashes. It
see this completely crashes. It
doesn't learn a single thing.
So sensitive.
Okay, at least this is like reasonable,
Okay, at least this is like reasonable,
right?
I wonder if the alpha is like
four m
of eight.
Can we get this to do anything on pong?
best paper at RLC besides my own work
best paper at RLC besides my own work
and my friend's work, I assume.
Honestly, there really wasn't anything
Honestly, there really wasn't anything
stand out
stand out
that I was like super excited to go look
that I was like super excited to go look
at.
I mean, it it started optimizing for a
I mean, it it started optimizing for a
little second,
little second,
but it's still like
like so freaking bad, right?
Do I not have like some major bug in
Do I not have like some major bug in
Yeah.
and talk RL
and talk RL
get the shilling down. Oh, that was like
get the shilling down. Oh, that was like
a just a super clumsy impromptu
a just a super clumsy impromptu
interview. If you want to watch my
interview. If you want to watch my
actually properly prepared talk, uh it's
actually properly prepared talk, uh it's
on the neural MMO YouTube.
like that. I'm actually proud of.
like that. I'm actually proud of.
That was a good talk.
Algorithm just feels super weak to me.
Is there any other major details
Is there any other major details
I'm missing? I don't
I'm missing? I don't
hang on. I don't think like I really
hang on. I don't think like I really
don't think so.
I mean, I guess the uh
I mean, I guess the uh
we're using really crappy nets, but
we're using really crappy nets, but
shouldn't matter that much. Maybe for
shouldn't matter that much. Maybe for
pong and stuff it does.
Let me at least get us like one.
Let's just at least get it like one
Let's just at least get it like one
extra hidden layer.
All right, Mr. Off Paul, do we work?
Well, didn't break it for um ppole at
Well, didn't break it for um ppole at
least initially.
Weirdly stuck.
It's ridiculously slow.
I mean, what's the deal with this? Is
I mean, what's the deal with this? Is
this just a stupid algorithm that you
this just a stupid algorithm that you
just like have to crank a ton of compute
just like have to crank a ton of compute
into before it solves even simple
into before it solves even simple
problems but scales better?
problems but scales better?
Like
kind of ridiculous.
It's annoying because like this is like
It's annoying because like this is like
I've pushed it to a point here where I
I've pushed it to a point here where I
can't be positive it's correct. So I
can't be positive it's correct. So I
can't say oh yeah the algorithm just
can't say oh yeah the algorithm just
sucks because I don't know if I have a
sucks because I don't know if I have a
bug.
I don't think so at this point.
I don't think so at this point.
Solves the simple end.
I've like roughly calibrated it as well.
I've like roughly calibrated it as well.
The update frequency
like I've roughly calibrated it.
It only solves freaking cart pole and it
It only solves freaking cart pole and it
doesn't even fully solve that.
Well, no, this one wasn't doing well
Well, no, this one wasn't doing well
with neural MMO.
with neural MMO.
It was a different algorithm.
We have this.
This got up to like 550 whatever.
This got up to like 550 whatever.
Doesn't sell breakout
You know, I really thought that I was
You know, I really thought that I was
going to be able to get something
going to be able to get something
quickly that at least did like okay, and
quickly that at least did like okay, and
I thought, you know, maybe I won't be
I thought, you know, maybe I won't be
able to tune it as good as my algorithm
able to tune it as good as my algorithm
is.
Like the puffer baseline is just
Like the puffer baseline is just
incredibly
incredibly
It's like an incredibly tough one to
It's like an incredibly tough one to
beat
beat
cuz everything out there is just like so
cuz everything out there is just like so
incredibly ludicrously laughably slow by
incredibly ludicrously laughably slow by
comparison, All right.
Does anybody see a bug? I don't see a
Does anybody see a bug? I don't see a
bug.
bug.
Like it looks to me like I have this
Like it looks to me like I have this
pretty reasonable
value.
value.
basically identical code.
I'm actually here.
I'm actually here.
Let me commit this.
Guess I ris I uh I I named it wrong a
Guess I ris I uh I I named it wrong a
little bit.
little bit.
Fine.
That's square.
Um,
Um,
doesn't look like it's working to me.
I figured we'd check the reference
I figured we'd check the reference
implementation, right?
Maybe it's just like stupidly stupidly
Maybe it's just like stupidly stupidly
slow. Maybe that's the deal.
I think I might have undersold puffer li
I think I might have undersold puffer li
a little bit when I say a thousand times
a little bit when I say a thousand times
faster
faster
because now this is like 5,000x faster,
because now this is like 5,000x faster,
right?
This is literally running under 600
This is literally running under 600
steps per second with like tiny networks
steps per second with like tiny networks
on the fastest possible environment on a
on the fastest possible environment on a
5090.
It's also not even solving it.
This is the reference implementation as
This is the reference implementation as
well.
So, um,
So, um,
this thing just not
Have I been here like trying to match
Have I been here like trying to match
an algorithm that just doesn't freaking
an algorithm that just doesn't freaking
work?
This is actually now at um close to the
This is actually now at um close to the
steps that I did mine for.
steps that I did mine for.
And my version's actually better than
And my version's actually better than
this.
I mean, I feel somewhat better about
I mean, I feel somewhat better about
this,
but like what the heck, right?
Oh, hang on.
Supposed to be a batch size of 32.
Supposed to be a batch size of 32.
Maybe it's Maybe it needs the tiny
Maybe it's Maybe it needs the tiny
batch. I thought it's going to be even
batch. I thought it's going to be even
slower.
Try it with the 32 batch size.
It's going to be kind of funny to me if
It's going to be kind of funny to me if
this only works with tiny little batches
this only works with tiny little batches
of data.
Be super cursed.
I don't know if it's doing any better.
Supposed to be 5 million steps.
This norm is not good for um
This norm is not good for um
and I caught a bad norm.
and I caught a bad norm.
Maybe it does work.
have also has the 255 norm.
This is such an easy problem like
This is such an easy problem like
it should insta solve.
it should insta solve.
Let's
make Man,
honestly, hard to see if it's learning.
I'm trying to make sure I don't have
I'm trying to make sure I don't have
like some screw up from um puffer. I
like some screw up from um puffer. I
don't think I do though.
Oh, you know what? Hang on.
I just want to be sure.
I think this might have been it.
See,
it's actually hard to say.
The trajectories are super mixed.
Yeah, this is learning.
It's weird. It's like It is actually
It's weird. It's like It is actually
noisy though.
noisy though.
The segments are really noisy,
The segments are really noisy,
but it is learning.
It's funny though. It does actually take
It's funny though. It does actually take
it like several minutes
it like several minutes
to solve freaking uh cartpole.
So I mean if this is supposed to be the
So I mean if this is supposed to be the
sample efficient algorithm
sample efficient algorithm
and this is what it does.
So noisy that it's like
So noisy that it's like
it's still getting bad
it's still getting bad
bad segments at this point.
Nothing else I'm missing, right?
Nothing else I'm missing, right?
I think so.
I mean, okay, we see that this just
I mean, okay, we see that this just
continues to bounce around a ton,
continues to bounce around a ton,
but it does do something. Let's just
but it does do something. Let's just
make sure there's not
make sure there's not
do pong real quick. Cuz
if this solves faster then it's like all
if this solves faster then it's like all
right there's something screw Okay.
And if this doesn't work,
then what? Just a crappy algorithm.
I mean that's probably fair right
if I tried to implement myself and I
if I tried to implement myself and I
used like one of the best reference
used like one of the best reference
impleations
impleations
and then what would that imply
and then what would that imply
basically that a lot of these control
basically that a lot of these control
tasks they're used on are just like
tasks they're used on are just like
really stupid really data constrained
really stupid really data constrained
problems
not a single episode
not a single episode
the reward.
Well, that's kind of surprising.
This is a random paper, right?
Yeah, this is bad. This is a It fails
Yeah, this is bad. This is a It fails
pong.
0.7 breaker. So, this is just bad.
They get results.
across Atari 100K.
Okay,
they have results
here.
And why is it
efficient zero is the best one on here.
The fact that they actually got this to
The fact that they actually got this to
do anything though.
I don't understand why there's this much
I don't understand why there's this much
of a
of a
a discrepancy. Like I'm literally seeing
a discrepancy. Like I'm literally seeing
this do nothing across both my own
this do nothing across both my own
implementation
implementation
and uh the clean RL1.
Oh, you know, I wonder who implemented I
Oh, you know, I wonder who implemented I
wonder if they did it off of this one
wonder if they did it off of this one
and they got a bad uh a bad port of it.
I'm down to do other stuff. Like I'm
I'm down to do other stuff. Like I'm
down to try this
The MCTS is probably a bit of a pain.
No code. Lovely.
Uh yeah, this is stuck at -21.
Uh yeah, this is stuck at -21.
Okay.
Did you figure it out? Try four. No. Um
Did you figure it out? Try four. No. Um
I think it just sucks.
Yeah, the problem is sucks.
All right, we'll do the from this, I
All right, we'll do the from this, I
guess.
Okay. So we do self.h
Then the dynamics function
predict state and reward
predict state and reward
given state and action.
What in the hell did this thing try to
What in the hell did this thing try to
Welcome.
Oh, wait. This is actually easier cuz
Oh, wait. This is actually easier cuz
this actually takes num actions at
this actually takes num actions at
hidden size, right? And it's
size plus action size.
Then we'll just do a self dot 8 o
like this. Exactly.
Okay. Okay. And then there's a policy
Okay. Okay. And then there's a policy
function,
a value function.
I think this is all you add.
Let me make sure I actually understand
Let me make sure I actually understand
what they're doing here.
replaces MCTS with sampling based
replaces MCTS with sampling based
gumball search.
Okay.
jointly.
No code for this is there.
Ah, it is
Please be good.
Worse.
Oh no, it couldn't. Ray Ray and freaking
Oh no, it couldn't. Ray Ray and freaking
Hydra.
their code.
Yeah, this is their code.
I think this is pretty much all of it
except the network maybe.
Um,
read Search.
Imagine
episode.
Do they use the MCTS though in the uh
Do they use the MCTS though in the uh
the eval loop or no?
or do they only use MCTS
or do they only use MCTS
in training?
Dynamic
function supervised learning method of
function supervised learning method of
using the true reward.
very complicated method.
Okay. But you do actually
You implemented this thing
over here
over here
monitor.
monitor.
I'm going to see what I can do about
I'm going to see what I can do about
Yes.
is dynamics, right?
is dynamics, right?
Is dynamics
H takes Then
take state action.
state action and outputs. Next state
state action and outputs. Next state
action.
Yes, it does.
Yes, it does.
Right. So, you have to give it the
Right. So, you have to give it the
action from the previous time step, I
action from the previous time step, I
suppose.
Oh, it outputs
Oh, it outputs
reward, not action.
reward, not action.
Duh.
So, you do reward decoder.
Let's just see if I can get a dynamics
Let's just see if I can get a dynamics
model uh to be trained jointly with
model uh to be trained jointly with
the environment for now uh with the
the environment for now uh with the
policy for now. So for this you need to
policy for now. So for this you need to
do
Well, actually you just train the policy
Well, actually you just train the policy
as normal, right? You use the policy as
as normal, right? You use the policy as
normal
and then you just add in an extra
and then you just add in an extra
dynamics, right?
You have state and action
all
I was
and do.
Uh you need the encoder.
Yeah, you need the uh the encoder.
So
So
8s
actions.
is going to be.
Yes.
Okay. And do like this.
Okay. And do like this.
And then we do
And then we do
dynamics.
Okay.
And then we just add these in.
Obviously there's no planning or
Obviously there's no planning or
anything.
Dynamics takes
You do not need num ops.
Uh, I see you have to embed the action.
Do action embed.
Okay. So, you just have this auxiliary
Okay. So, you just have this auxiliary
thing, right?
And
these losses should not be zero.
H. This one has to be rewards.
Why? That's why it doesn't matter.
Okay,
there's your reward loss.
And this does not get in the way at all,
And this does not get in the way at all,
obviously.
Now, how do we do this via search?
So they take H
This
and then they
and then they
sample action.
sample action.
Wait, I thought that they don't they do
Wait, I thought that they don't they do
this. Do they do this during training or
this. Do they do this during training or
just during um the collection of
just during um the collection of
actions?
Is this just done during the
templing based gumball search?
this. No, wait. Is this actually just um
it does
revised learning method?
Is this
Is this
is this totally different from dreamer?
forms planned planning. Yes.
forms planned planning. Yes.
But then it does this to pick better
But then it does this to pick better
actions, right?
That's just the
I can just figure out how this planning
I can just figure out how this planning
thing works. So you take H.
thing works. So you take H.
H gives you state.
You sample actions. It says from state
presumably with
what
the policy function right.
Okay. So we change it so that policy
Okay. So we change it so that policy
policy takes state right
is H.
So instead of encoder,
well h is the encoder, right?
not having good observation. Okay.
Now we're sharing an embedding layer.
Wrong.
Uh, it's not designed to be Um,
Okay,
Okay,
let's do uh encode observations.
as forward.
I returned everything correctly. That's
I returned everything correctly. That's
weird.
Well, we haven't made a huge ton of
Well, we haven't made a huge ton of
research progress today,
research progress today,
but you can't say it was for lack of
but you can't say it was for lack of
trying.
trying.
I've been pretty well focused up today.
Just how research freaking is
I guess I've been somewhat biased
I guess I've been somewhat biased
against how world modeling works since
against how world modeling works since
we spent a whole bunch of time trying to
we spent a whole bunch of time trying to
replicate the dreamer paper and it was
replicate the dreamer paper and it was
kind of crap.
kind of crap.
See about this instead.
search has always made more sense to me
search has always made more sense to me
than um
than um
streamer style stuff. Anyways,
Oh, okay. I see. I see it.
I guess I would feel kind of stupid
I guess I would feel kind of stupid
anyways if there were just a bunch of
anyways if there were just a bunch of
free winds lying around
free winds lying around
being this crap.
You need stupid
Actually,
what you want to do is this embed.
Okay. So, you have these separate
Okay,
Okay,
still good. Still trains.
We have a world model as part of this
We have a world model as part of this
now
now
with an LSTM.
with an LSTM.
Yeah. So, the the thing that we don't
Yeah. So, the the thing that we don't
know yet
know yet
is how we're going to do all the search
is how we're going to do all the search
crap.
So I can't tell are they are they
So I can't tell are they are they
basically using the model to like the
basically using the model to like the
planning model to get better actions
planning model to get better actions
fast
fast
or are they training on it as well?
They do have J and everything.
Is this how it works though?
They say that they have there's a Q
They say that they have there's a Q
function though as well, isn't There.
So they have this tree search thing.
So they have this tree search thing.
I saw the function in here.
I saw the function in here.
This is the hard part.
Most likely
I thought they said they don't use MCTS.
Use
gumball noise.
Yeah, they don't use this thing.
Jeez. Please.
policy.
Yeah, I don't understand how this uh
Yeah, I don't understand how this uh
this only has the policy.
how their search works.
gives you values,
policies, and best action.
They're call they're like calling
They're call they're like calling
something weird a policy, I think. Are
something weird a policy, I think. Are
they just calling it the probability
they just calling it the probability
vector over uh over actions?
I think they are.
give you the value, the policy
supposed to be Search.
Okay. Search.
This takes in the model batch size.
This takes in the model batch size.
dates, root values, root policy logic.
They have LSTM as well.
It's a pretty messy looking
It's a pretty messy looking
implementation.
and I should be able to figure out how
and I should be able to figure out how
this thing works
It doesn't seem like they have um
It doesn't seem like they have um
they must have a
they must have a
model
freaking lines agent.
Not the agent, dude. You have a train
Not the agent, dude. You have a train
function. That's not the agent.
Oh yeah, they have it.
Oh yeah, they have it.
This code I guarantee is super slow
but faster.
Main thing I need to understand I think
Main thing I need to understand I think
is the um
is the um
if I understand the MCTS
that up.
description is just not helping me.
description is just not helping me.
Nor is their code.
Oh, I think they don't actually have a
Oh, I think they don't actually have a
policy. I think they have um
policy. I think they have um
Okay,
I don't think they have a policy
I don't think they have a policy
network.
Is
You actually maintain a Q function here.
Random rollups for patient.
Okay, this I understand.
Yeah, we're kind of our bet.
I mean, I could technically try this.
I mean, I could technically try this.
The thing is, I know for a fact this
The thing is, I know for a fact this
paper is
paper is
At least like the reasoning in most of
At least like the reasoning in most of
it is
I mean, I could very easily.
I mean, I could very easily.
Do you know exactly why? I mean, we ran
Do you know exactly why? I mean, we ran
20,800 hours worth of ablations on that
20,800 hours worth of ablations on that
thing. We've got a pretty good idea, I'd
thing. We've got a pretty good idea, I'd
say.
Mhm.
I could add in
to add in like the auxiliary planning
to add in like the auxiliary planning
here.
thing is like
thing is like
I mean I could Right.
It's tough cuz I've been bitten so badly
It's tough cuz I've been bitten so badly
by this paper,
right?
Like the result is very good, but this
Like the result is very good, but this
absolutely should not have been
absolutely should not have been
published with um
published with um
the way that it is cuz it's like half
the way that it is cuz it's like half
the paper is long.
It is actually a decent baseline though
It is actually a decent baseline though
because it's way simpler to implement
because it's way simpler to implement
specifically what part. Uh, I know that
specifically what part. Uh, I know that
the all the little tricks that they add,
the all the little tricks that they add,
like the little tricks that they
like the little tricks that they
proposed are essentially just like
proposed are essentially just like
random clever sounding things applied to
random clever sounding things applied to
reinforce because they didn't have a
reinforce because they didn't have a
strong baseline. What's the advantage of
strong baseline. What's the advantage of
off policy besides experience replay?
off policy besides experience replay?
That's the advantage and that's a very
That's the advantage and that's a very
important one.
In my mind, it makes sense that search
In my mind, it makes sense that search
works. It actually it doesn't. In my
works. It actually it doesn't. In my
mind, it doesn't even make sense that
mind, it doesn't even make sense that
this necessarily works.
Yeah. Like all this crap is wrong.
Yeah. Like all this crap is wrong.
Um,
yeah, that problem bet is search is like
yeah, that problem bet is search is like
monstrously complicated to implement.
replay same end port for different
replay same end port for different
agents. So I don't know what you mean by
agents. So I don't know what you mean by
that.
Yes, it does work for num ms greater
Yes, it does work for num ms greater
than one,
than one,
but he asked for different agents, not
but he asked for different agents, not
different ends.
So presumably it's something different
So presumably it's something different
that he wanted.
Dreamer V3.
Dreamer V3.
Well, that and Efficient Zero.
Well, that and Efficient Zero.
I actually like the formulation in
I actually like the formulation in
Efficient Zero a lot more. It makes
Efficient Zero a lot more. It makes
sense to me that it works. Um, I think
sense to me that it works. Um, I think
it's backed by a lot more as well. It's
it's backed by a lot more as well. It's
just horrendously complicated
just horrendously complicated
because making search efficient with
because making search efficient with
like batch to reinforcement learning is
like batch to reinforcement learning is
not easy.
not easy.
I mean, search on its own is like
I mean, search on its own is like
already kind of non-trivial to implement
already kind of non-trivial to implement
like in like a good MCTS.
It wouldn't be that bad honestly, but
It wouldn't be that bad honestly, but
doing it in a context of neural nets is
doing it in a context of neural nets is
freaking hard.
849.
849.
I'm going to pick this up tomorrow. I
I'm going to pick this up tomorrow. I
think
it's Monday.
Yeah.
Okay.
Okay.
I've tried so many different things
I've tried so many different things
today. Um,
this was the
this was the
result of the off paw
result of the off paw
or the buffer.
Yeah, that's tough.
I know their code is slowed.
I know their code is slowed.
This does quite a bit better.
Well, we shall see, right?
At the very least.
At the very least.
We found out that there really aren't
We found out that there really aren't
any easy wins in offpaul.
any easy wins in offpaul.
Uh the whole kind of the vast majority
Uh the whole kind of the vast majority
of the offpaul work is just super
of the offpaul work is just super
freaking cursed. Honestly,
breakout is supposed to be the easy task
breakout is supposed to be the easy task
bet, but
it's kind of a mess that it's this
it's kind of a mess that it's this
screwy.
Also, literally, I under I undersshot
Also, literally, I under I undersshot
when I said puffer is a thousandx
when I said puffer is a thousandx
faster.
faster.
Like literally some of these offpaul
Like literally some of these offpaul
methods, they're not even set up by
methods, they're not even set up by
default for batched learning. It's crazy
or at least batched in uh inference.
You'd literally be faster on CPU under
You'd literally be faster on CPU under
that setup. for like small networks.
that setup. for like small networks.
Crazy.
Crazy.
Okay, though. Um,
Okay, though. Um,
it's almost 6. I am hungry. I got to get
it's almost 6. I am hungry. I got to get
food.
food.
I'm going running tomorrow
I'm going running tomorrow
and get myself
and get myself
situated
situated
and then uh I will be back on Monday.
and then uh I will be back on Monday.
I'm going to try to allocate as much
I'm going to try to allocate as much
week as much of next week as possible
week as much of next week as possible
to working on all this stuff.
to working on all this stuff.
We shall see.
Now this really freaking bothers me
Now this really freaking bothers me
though.
The fact that this
The fact that this
is even like possible that this works is
is even like possible that this works is
just that really bothers me.
Expert recording.
Expert recording.
Yeah, that works. That's super easy. Bet
it. Yeah, but it shouldn't be worse than
it. Yeah, but it shouldn't be worse than
um on policy is the thing.
Wow. Okay, folks. For those of you who
Wow. Okay, folks. For those of you who
have spent Saturday watching me uh bang
have spent Saturday watching me uh bang
my head against all this research, I did
my head against all this research, I did
a lot of different things today.
a lot of different things today.
Thank you for tuning in.
Thank you for tuning in.
Everything I do is right here at
Everything I do is right here at
puffer.ai
puffer.ai
free. It's open source part of the repo.
free. It's open source part of the repo.
Helps me out a ton. You want to get
Helps me out a ton. You want to get
involved Discord right here. Other than
involved Discord right here. Other than
that, you can follow me on X.
that, you can follow me on X.
Uh for more reinforcement learning
Uh for more reinforcement learning
content,
content,
whole bunch of articles here for
whole bunch of articles here for
beginners. Click the uh follow button
beginners. Click the uh follow button
wherever that is. All right.
