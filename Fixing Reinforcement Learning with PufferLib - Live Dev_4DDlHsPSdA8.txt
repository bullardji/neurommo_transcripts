Kind: captions
Language: en
here's our sweet progress it's pretty
good and here is our Pito curve
this is pretty solid
there are a few things to figure out
there are a few things to figure out
here there are a ton of points grouped
here there are a ton of points grouped
up over
here there's also too much encouragement
here there's also too much encouragement
to
to
uh to you do this in the first
place where's the first like solved
Point H 20.9 over
Point H 20.9 over
here it's kind of crazy that it even
here it's kind of crazy that it even
found something this expensive let me
found something this expensive let me
say what did it find
we're missing mini bat size I think here
okay so you can see here it's
okay so you can see here it's
actually it's intentionally pushed
actually it's intentionally pushed
everything down to the bottom
B now this will be the graph that we
B now this will be the graph that we
want to see here so
okay cool this I think this
okay cool this I think this
has mostly what we need
mini
mini
batch what's missing here I think just
batch what's missing here I think just
mini batch
so here's the cost here's the
front so learning rate
we actually I think we get the best
we actually I think we get the best
results with the lowest learning
rate just by a little bit
right I would call this more of a stable
right I would call this more of a stable
region I guess but this I think this is
region I guess but this I think this is
the best point right here is the one
the best point right here is the one
with the absolute lowest learning rate
with the absolute lowest learning rate
over here
there's an outlier ignore
this something happened with these
this something happened with these
points that we'll have to figure out I
points that we'll have to figure out I
think these are probably like the
think these are probably like the
resamples or
something and then gamma
yeah anything from0 n what is
yeah anything from0 n what is
this 98
something yeah nine pretty much anything
something yeah nine pretty much anything
from like
from like
Point really it should be 0 N9 it looks
Point really it should be 0 N9 it looks
like based on
like based on
this we might even want to increase this
range Lambda a little bit
more I really actually all the ones at
more I really actually all the ones at
the top are pretty
the top are pretty
high 98
99 probably this one needs to be logic
spaced update EPO one gets you to
19 to 20
9 then the value coefficient
here yeah the normal5 is
here yeah the normal5 is
fine I I don't know why it did all of
fine I I don't know why it did all of
this mess but um it does seem like
this mess but um it does seem like
anything over here is fine it's
cool maximum gradient
cool maximum gradient
Norm .5 CS to make it substantially
Norm .5 CS to make it substantially
worse over here right
Norm of
Norm of
one more
one more
stable
stable
entropy everything from virtually no
entropy everything from virtually no
entropy out
to 01 is like yeah so pretty much
to 01 is like yeah so pretty much
there's good stuff anywhere in the
there's good stuff anywhere in the
entropy range for this n
is this actually better it looks like
is this actually better it looks like
there are better points
there are better points
here
20.9 maybe
number of
M's okay and batch
size any of the back sizes are good
uh this plot gets
removed mini batch size
here we
go up to 4096 it seems is good there's
go up to 4096 it seems is good there's
degradation after 4096
I mean this is kind of
nutty pull restream up we'll continue on
nutty pull restream up we'll continue on
this analysis I mean this is this is a
this analysis I mean this is this is a
good result this is not a perfect
good result this is not a perfect
result um the reason this is not a
result um the reason this is not a
perfect result
perfect result
is made some very expensive runs
the simple
end e
I don't understand their filter thing it
I don't understand their filter thing it
just doesn't work
well that did it wait no it didn't
why do I now only
have I have 99
items oh there we
go for
it's funny there is still a little Gap
it's funny there is still a little Gap
here
all the best results are with a small
all the best results are with a small
number of ends
so there's a huge amount of uh different
so there's a huge amount of uh different
good learning rates learning rates
good learning rates learning rates
actually one of the most stable
actually one of the most stable
parameters for this
end gamma's optimal at 995 which means I
end gamma's optimal at 995 which means I
should extend that
range probably on both of them
and four update
EO also I have like 140 runs and over a
EO also I have like 140 runs and over a
100 of them
100 of them
are 97 uh
are 97 uh
19.75 and
up and actually they are almost all 20
up and actually they are almost all 20
and up
I still have 86 items
so still learning rating here is very
so still learning rating here is very
stable
stable
right
right
o5 so this
o5 so this
is 5 eus 4
1 E minus
4 looks like 2 e minus 4 is a good
4 looks like 2 e minus 4 is a good
starting
point the original was way too high
okay this is definitely now when you
okay this is definitely now when you
plot it this way you can see you
plot it this way you can see you
definitely have to
definitely have to
expand gamma outward Lambda actually
expand gamma outward Lambda actually
doesn't matter so much right Lambda
doesn't matter so much right Lambda
they've done all like all these trials
they've done all like all these trials
here but actually the best ones are a
here but actually the best ones are a
little bit out this
way so gamma needs to go up a bit
I'm going to start on that
actually no reason not to
so
I I forget if there's a reason I did
I I forget if there's a reason I did
this but I doubt
this but I doubt
it do that
GMA to be fair I did kind of fix the
GMA to be fair I did kind of fix the
search ranges so it doesn't matter so
search ranges so it doesn't matter so
much
anymore this definitely goes to 0.
anymore this definitely goes to 0.
N9 um nine as a
N9 um nine as a
Max then
gamma I'm going to leave it at 098
gamma I'm going to leave it at 098
because I know pong is uh you know it's
because I know pong is uh you know it's
for other ends it can be
for other ends it can be
different five this is fine DX we're not
different five this is fine DX we're not
going to let you do more than four it's
going to let you do more than four it's
ridiculous
value
coefficient yeah looks like
coefficient yeah looks like
0.75 or
0.75 or
08 is way better
the best result is at minimal entropy
the best result is at minimal entropy
actually
here this is fine
here this is fine
back prop Horizon is
reasonable and at some point the number
reasonable and at some point the number
of Ms just gets to be too bloody
small bad size prams look good
we're not going smaller than this for
we're not going smaller than this for
Hardware
Hardware
reasons plus their points
reasons plus their points
here
here
okay I'm going to try to look at the
charts that's actually surprisingly it's
charts that's actually surprisingly it's
very stable
and actually it's stable in uh not that
and actually it's stable in uh not that
many steps isn't
it that's 10 million
steps with a bunch of high scores
right it'ss all the way back
here so it just pushed everything to the
here so it just pushed everything to the
maximum
maximum
time it really didn't try very
time it really didn't try very
hard at the lower end of the Pito
hard at the lower end of the Pito
Frontier because they're all these
Frontier because they're all these
stable
curves and like the shape is a little
curves and like the shape is a little
inconsistent at the start right like
inconsistent at the start right like
maybe it takes you an extra few million
maybe it takes you an extra few million
steps depending on the
seed was very stable
overall see cost
like okay it says that the best runs are
like okay it says that the best runs are
at the highest cost but there are only a
at the highest cost but there are only a
few of them that are the best the best
few of them that are the best the best
and uh it's CU it ran so many
and uh it's CU it ran so many
experiments
right it says we have some as low
as it's 130
as it's 130
cost so a little over 2
minutes and probably this point
minutes and probably this point
here could have been run for fewer
steps okay
steps okay
these are very good results
then we have to the only thing we really
then we have to the only thing we really
have to think about is how this thing is
have to think about is how this thing is
budgeting its compute cuz the result
budgeting its compute cuz the result
itself is very
itself is very
good now we're happy with the Sweep
result can see it there's still some
result can see it there's still some
diversity in here as
diversity in here as
well with the
well with the
cost so it's still doing different
stuff let's take the filter off
yeah so you can see here in
yeah so you can see here in
cost that there is a span here where it
cost that there is a span here where it
does a bunch of these very high cost
does a bunch of these very high cost
runs that are probably pretty
runs that are probably pretty
similar but then it does spread
similar but then it does spread
out so I imagine this is it filling in
out so I imagine this is it filling in
the frontier at the
end I mean these early runs like
so many of these
runs
yeah interesting that it still me misses
yeah interesting that it still me misses
a few of
a few of
these like hard misses I guess trying to
these like hard misses I guess trying to
fill in lower end of Fredo
front I mean this is the main
right the Pito chart
I have exactly the curve I
I have exactly the curve I
want but too much out at the high cost
want but too much out at the high cost
and and actually I think I can
scale 350
yeah I can just see what how many of
yeah I can just see what how many of
these things are above
these things are above
three uh above like 300 cost
okay so we have 60 runs so almost half
okay so we have 60 runs so almost half
of the
of the
runs right between a third and a half of
runs right between a third and a half of
the runs are at the really really
the runs are at the really really
highend
cost we need this to not happen
okay so there are a couple things going
okay so there are a couple things going
on here
I think the first is we're telling it
I think the first is we're telling it
that um
that um
29.9 is way or 20 uh 20.9 is way better
29.9 is way or 20 uh 20.9 is way better
than like
20.7 so we're overemphasizing the high
20.7 so we're overemphasizing the high
end so
end so
much that it's scaling to Max
cost we're also not penalizing higher
cost we're also not penalizing higher
cost at all
okay so the other thing that I haven't
okay so the other thing that I haven't
thought about
here I have this binary search aspect to
here I have this binary search aspect to
my algorithm
where it's supposed to fill in the gaps
where it's supposed to fill in the gaps
in the uh the cost
in the uh the cost
curve but if you do that in linear space
curve but if you do that in linear space
right if you sum up if you just did like
right if you sum up if you just did like
the cost is 1 to 10 and then you do that
the cost is 1 to 10 and then you do that
you're going to get like 1 to
you're going to get like 1 to
well the sum of 1 to 5 is
well the sum of 1 to 5 is
15 I believe 6 to 10 is 40
15 I believe 6 to 10 is 40
right cuz it's
55 so you're spending all the compute at
55 so you're spending all the compute at
the high
end e
one thing to look at
here there are good
here there are good
runs at the low end
take a look at the algorithm
burito C
Norm a 01 Norm
I see so it is just the cost in a linear
scale if I do the cost and log distance
scale if I do the cost and log distance
does that fix it
we can kill the Palm
we can kill the Palm
sweep I
sweep I
think cuz it's like it's complete
right I want to pollute the last point
right I want to pollute the last point
so I'll let that finish
so I'll let that finish
H this is
H this is
fine okay so we have our sweep locked in
fine okay so we have our sweep locked in
this is the first good sweep as well
this is the first good sweep as well
that we've gotten in a while so it is
that we've gotten in a while so it is
encouraging to get such a good sweep as
encouraging to get such a good sweep as
the first one with such heavy
the first one with such heavy
modification of the algorithm
and test this then going to test log
and test this then going to test log
cost
oh I'm kind of dumb I guess
huh yeah I'm kind of dumb
here this literally shows it to you
here this literally shows it to you
doesn't
it so this is what we got on the
it so this is what we got on the
percentile task
percentile task
right and uh the top
this little chunk of data here is
this little chunk of data here is
using half the compute
okay so this run it looks like we did
okay so this run it looks like we did
more data points but this is the same
more data points but this is the same
thing
thing
right this curves the same
let me just check one thing with
visualize yeah it goes to one not zero
visualize yeah it goes to one not zero
here this is is
fine
fine
so yeah this is going to be an
issue
duh let me try the next
duh let me try the next
thing we'll do it in log space
ay
Norm e
I had to be careful to get that right
see if this evens the curve
out e
funny most common question I get is
funny most common question I get is
probably about RL and trading it's
crazy important here oh
really I'm confused that that didn't do
really I'm confused that that didn't do
much
it didn't seem to do very
much no it's it's slightly less but it's
much no it's it's slightly less but it's
not linear still
okay the next thing is going to be the
okay the next thing is going to be the
score function
the next thing is going to be the score
the next thing is going to be the score
function I think
so I don't know if I like this in in log
so I don't know if I like this in in log
space either
well we will see
let's go back we'll do this one at a
let's go back we'll do this one at a
time
time
here the other one is the score
function I think I need to understand
function I think I need to understand
this score function for
this is what you get for a score of
this is what you get for a score of
10 what you get for a score of
20 8.5
goes up to
6.9 okay so the Epsilon
Factor the Epsilon factor is actually
Factor the Epsilon factor is actually
pretty nice
pretty nice
so if we do like minus one here right
so if we do like minus one here right
20 this goes to
20.3 versus
20 this is almost linear right
so I think that this Epsilon factor is
so I think that this Epsilon factor is
very
important this should essentially give
important this should essentially give
you the aggression Factor
I want to see what happens with 1 E
I want to see what happens with 1 E
minus one first to see if this is
actually going to change the data the
actually going to change the data the
way I
expect this score transformation is very
expect this score transformation is very
important um if you make it very
important um if you make it very
aggressive then the algorithm is going
aggressive then the algorithm is going
to bias very heavily towards trying to
to bias very heavily towards trying to
find runs that really really Max the
find runs that really really Max the
score at any cost um but if you make it
score at any cost um but if you make it
two passive it won't find
two passive it won't find
the best
results this is hilarious
up for
okay so here is our oops
okay so here is our oops
this is
before well this is before to be fair
before well this is before to be fair
the other one was with the different
change okay
interesting that also was not enough
do I put them both together
both of these and if this doesn't work
both of these and if this doesn't work
then we'll have to reconsider some
then we'll have to reconsider some
stuff things won't make
sense to reconsider some scoring
functions
e
e e
does the variability of the samples tell
does the variability of the samples tell
you
much probably doesn't tell you a ton
right let me make sure that I actually
right let me make sure that I actually
hit visualized before I did so these two
hit visualized before I did so these two
together fixes
together fixes
it this is
perfect yeah this is a perfect
perfect yeah this is a perfect
curve this is exactly what we
curve this is exactly what we
want now the question is going to be
the Epsilon
term Epsilon is the aggression factor I
think man if this works if this just
think man if this works if this just
works this way this is going to be
works this way this is going to be
awesome this is going to be so cool
e
you just publish
he just published a
thing
thing
oh
maybe he
maybe he
pinned that's weird I thought he had a
pinned that's weird I thought he had a
paper maybe
not
for
e e
this is it right
yeah is it really this much caused by 1
yeah is it really this much caused by 1
eus2 to 1 eus 3 that jump hang
eus2 to 1 eus 3 that jump hang
on we'll rerun this again and then we'll
on we'll rerun this again and then we'll
test one minus 1 2 and 3
and then we'll T us longer yo Spencer
and then we'll T us longer yo Spencer
yes uh
yes uh
absolutely I am going to do this morning
absolutely I am going to do this morning
session get some breakfast and then
session get some breakfast and then
afternoon is perfectly good and uh we
afternoon is perfectly good and uh we
will I can look at your code we will
will I can look at your code we will
chat about some research stuff and I may
chat about some research stuff and I may
as well even like go uh install the CPUs
as well even like go uh install the CPUs
while we're chatting about
that also I think we have soda hyper
that also I think we have soda hyper
parameter
tuning uh there are a few things I want
tuning uh there are a few things I want
to
to
tweak
tweak
but I mean I think we're going to
but I mean I think we're going to
have and we have the the result on pong
have and we have the the result on pong
as well already
where's the where's the thing yeah know
where's the where's the thing yeah know
this is like really
this is like really
[Music]
[Music]
good yeah this parito front here this is
good yeah this parito front here this is
perfect now the only thing wrong with it
perfect now the only thing wrong with it
is that I think it spends too much time
is that I think it spends too much time
on expensive runs uh it needs to be so
on expensive runs uh it needs to be so
it spends too much time on these
it spends too much time on these
expensive runs it needs to spend more
expensive runs it needs to spend more
time over here so we're going to figure
time over here so we're going to figure
out some configuration stuff with it but
out some configuration stuff with it but
I think we're just going to full solve
I think we're just going to full solve
everything with this
that's the
hope it's been quite a bit of work it
hope it's been quite a bit of work it
really has been quite a bit of work I
really has been quite a bit of work I
think there's still some
left all right so this is the original
left all right so this is the original
that we
that we
had yeah this is the
had yeah this is the
original you get this sort of a curve
complex M we have a bunch of complex M's
complex M we have a bunch of complex M's
actually I think that we need what we
actually I think that we need what we
really need is we need more stuff on the
really need is we need more stuff on the
research side um there's too many things
research side um there's too many things
I want to test and I don't have people
I want to test and I don't have people
to do
it I mean I think that you've
it I mean I think that you've
demonstrated with the m that you've
demonstrated with the m that you've
built that you should be able to handle
built that you should be able to handle
some of the research side stuff pretty
well we'll see maybe I'm overestimating
well we'll see maybe I'm overestimating
but I think in my mind the science is
but I think in my mind the science is
dramatically easier than than the
dramatically easier than than the
engineering
so maybe that's just because I've been
so maybe that's just because I've been
doing it forever but I don't think it's
doing it forever but I don't think it's
that hard
okay so the log cost
has quite some
impact I think I need to run this for
longer yeah we need to get the
original yeah there's your original
okay we're going to need to do four runs
okay we're going to need to do four runs
that are going to take a few minutes but
that are going to take a few minutes but
whatever you know we we really need to
whatever you know we we really need to
do it just to make absolutely certain
do it just to make absolutely certain
that we have this correct
new run going on medium difficulty steps
new run going on medium difficulty steps
per second back to 900k that's what we
per second back to 900k that's what we
like that very
good it ironically I so this
good it ironically I so this
hyperparameter stuff I'm doing it
hyperparameter stuff I'm doing it
doesn't like solve all the problems in
doesn't like solve all the problems in
RL but I think that this gets our foot
RL but I think that this gets our foot
in the door like my hope is that what's
in the door like my hope is that what's
going to happen is right now RL
going to happen is right now RL
experimentation just feels very clunky
experimentation just feels very clunky
and inconsistent I think that if you
and inconsistent I think that if you
have a really good sweep algorithm that
have a really good sweep algorithm that
you know that you can trust um then at
you know that you can trust um then at
the very least you're going to know okay
the very least you're going to know okay
I can run a sweep and if it's just
I can run a sweep and if it's just
hyperparameters it's going to find them
hyperparameters it's going to find them
that's the
that's the
goal so then basically everything else
goal so then basically everything else
that we do just becomes much faster
that we do just becomes much faster
because it's way higher confidence right
because it's way higher confidence right
you know how you just like everything is
you know how you just like everything is
just slower when you don't really know
just slower when you don't really know
for sure whether your experiments or
for sure whether your experiments or
whether the stuff you're doing is
whether the stuff you're doing is
correct so like you kept always second
correct so like you kept always second
gu everything I want to eliminate that
gu everything I want to eliminate that
from
RL so I took a quick look at that paper
RL so I took a quick look at that paper
uh it looks good that's nice they have
uh it looks good that's nice they have
two variations of two
environments so um
environments so um
yeah ironically the number of
yeah ironically the number of
experiments that they ran would be like
experiments that they ran would be like
we would do those in a few days on our
we would do those in a few days on our
few
few
gpus with like a puffer
end I think I've done more more
end I think I've done more more
experiments than they have
experiments than they have
done for that paper just testing this
done for that paper just testing this
algorithm just testing this hyper
algorithm just testing this hyper
parameter sweep
it looks like very good work I think
it looks like very good work I think
Eugene's on it as
well
e e
okay this is the previous
okay this is the previous
one this will
one this will
be the new
Baseline okay these match very nice
good and then we got to
do we got to do log
cost be the next
one
e
e e
box one went
down I think I meant to go outside this
down I think I meant to go outside this
morning to reboot it and I
morning to reboot it and I
forgot so let me finish this and then I
forgot so let me finish this and then I
will go reboot it and have breakfast and
will go reboot it and have breakfast and
we will go from there
it's going to be much easier in the new
it's going to be much easier in the new
facility where I just have to walk
facility where I just have to walk
across the room to reboot stuff so
across the room to reboot stuff so
hopefully they won't be
crashing we're definitely going to do
crashing we're definitely going to do
some good
tests yeah add me on Discord whenever
tests yeah add me on Discord whenever
I'm pretty much I'm going to finish this
I'm pretty much I'm going to finish this
morning session another half hour
morning session another half hour
whatever I do here and then um I'm going
whatever I do here and then um I'm going
to get breakfast and then I'll be around
to get breakfast and then I'll be around
the whole rest of the
day see you
Spencer e
all
right two
that's already dramatically
better still a whole bunch of points
better still a whole bunch of points
right at the
end e
e
the goal with this
the goal with this
here the uh the goal with this change is
here the uh the goal with this change is
to make this
to make this
algorithm here maybe I should just while
algorithm here maybe I should just while
this is running I'll go through real
this is running I'll go through real
quick so this curve is the total amount
quick so this curve is the total amount
of time spent in the experiments that
of time spent in the experiments that
cost up to this amount
cost up to this amount
so we spend a total
so we spend a total
of 15% or whatever of the time in
of 15% or whatever of the time in
experiments that cost 50 or less so all
experiments that cost 50 or less so all
these
these
experiments cost very
experiments cost very
little uh and then here you can see we
little uh and then here you can see we
spend another
20% in experiments from here to
20% in experiments from here to
here and then you can
here and then you can
see that the vast majority of the
see that the vast majority of the
compute 23 3/4 of the compute is spent
compute 23 3/4 of the compute is spent
on these experiments
on these experiments
here so the idea
here so the idea
here is you're spending too much time on
here is you're spending too much time on
these expensive
these expensive
experiments you'd like this to be more
experiments you'd like this to be more
even
oh yeah this is the first one this is
oh yeah this is the first one this is
our rerun of the same thing so this is
our rerun of the same thing so this is
the same and then this is with the
the same and then this is with the
modification so
modification so
now you can see that you're spending
now you can see that you're spending
little less than 2/3 it looks like of
little less than 2/3 it looks like of
the compute on these
experiments and then we'll see if this
experiments and then we'll see if this
changes
changes
material uh with the different Epsilon
I think the main thing that you can see
I think the main thing that you can see
here is this sort of cluster of
here is this sort of cluster of
points right that you spend a lot of the
points right that you spend a lot of the
compute right at the end here there's
compute right at the end here there's
this
this
Spike and this is not just noise this
Spike and this is not just noise this
the algorithm does
this to be fair it does it way worse
this to be fair it does it way worse
right here
right here
so we've already improved this thing
dramatically the spike is
gone I think this is going to be the
gone I think this is going to be the
better one
here I think this is it so
the thing is that we could try let me
the thing is that we could try let me
let me see
something th this Epsilon term is very
important it's not just like a noise
important it's not just like a noise
term or whatever so I should probably
term or whatever so I should probably
rename
it so this is if we just do actually let
it so this is if we just do actually let
me
do uh 101
15 okay so these points
this is with Epsilon let's do actually
this is with Epsilon let's do actually
one 2 three so you can see how this
changes so with Epsilon of 1 one minus
changes so with Epsilon of 1 one minus
one we do
one we do
20.5 this changes by about where is
it yeah from here 20
it yeah from here 20
825 uh
825 uh
285 3
285 3
4ish so this gets about.
4ish so this gets about.
5.55
is oh and this still gains
one okay this I'm looking at the wrong
one okay this I'm looking at the wrong
one my bad okay let's look down here so
from
from
20 to
20 to
20.5 this goes
up little less than 0 2 and a little
up little less than 0 2 and a little
less than 02 so this thing is no I
less than 02 so this thing is no I
should just plot these is what I should
do I should just plot these that would
do I should just plot these that would
be easier
this will be the quickest
okay
where's the green
one e
this okay it doesn't actually seem to
this okay it doesn't actually seem to
matter very much after
uh maybe it does it's just hard to
uh maybe it does it's just hard to
see oh no it definitely
see oh no it definitely
does yeah know it definitely definitely
does yeah let's get rid of this one
does yeah let's get rid of this one
though because after that it goes
though because after that it goes
crazy
crazy
okay this will be
important so right
important so right
here this
here this
is this is the reward graph
put this where you can see
it
it
so this goes
so this goes
negative is this pretty linear
negative is this pretty linear
looking that's pretty linear looking to
me it's just a shallow yeah line
so at -20
here3 or
whatever6 let me make sure I'm doing
whatever6 let me make sure I'm doing
this right as well
do I not Norm
I thought I had a normalization
I thought I had a normalization
operation on
this yeah this is screwed up
this should be above this shouldn't
it well okay let's say that this is 0 to
it well okay let's say that this is 0 to
one
scaled e
okay so the point1 is actually here this
okay so the point1 is actually here this
is the one I think that we would want
is the one I think that we would want
right this gives you
eight okay I didn't realize I just
eight okay I didn't realize I just
messed up the
messed up the
scaling this is going to take a little
scaling this is going to take a little
more work I'll be back in a few got to
more work I'll be back in a few got to
do a couple things and then we will
do a couple things and then we will
we'll take a hard look at the scaling
we'll take a hard look at the scaling
and uh we will finish the algorithm
and uh we will finish the algorithm
hopefully be right
back
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e
hello Tyler if you're still
hello Tyler if you're still
here sometimes uh X likes to eat
here sometimes uh X likes to eat
messages what do you graphing and why is
messages what do you graphing and why is
it important okay so this
it important okay so this
is this is the score
is this is the score
function that we are using for the
function that we are using for the
hyperparameter sweep this transformation
hyperparameter sweep this transformation
is applied
is applied
to uh score data that comes in from the
to uh score data that comes in from the
environment and this is the transform
environment and this is the transform
I've come up with and there are a few
I've come up with and there are a few
different well there's a parameter that
different well there's a parameter that
can change the shape of it
here okay so
interestingly if we're going to say that
interestingly if we're going to say that
we scale score data from 0 to
one doesn't matter that it always be
one doesn't matter that it always be
positive or anything like
positive or anything like
that doesn't
that doesn't
matter should not matter
so I think the thing to look at
so I think the thing to look at
here how's it defined by which policy uh
here how's it defined by which policy uh
so the way that this works is you get a
so the way that this works is you get a
score from the environment after
score from the environment after
training right this could be your win
training right this could be your win
rate this could be like how many points
rate this could be like how many points
you got in pong or whatever and uh we
you got in pong or whatever and uh we
need some way that is we need some way
need some way that is we need some way
to turn that data into a nice uniform
to turn that data into a nice uniform
space for the hyper parameter sweep to
space for the hyper parameter sweep to
use
use
uh and
uh and
typically you want to accentuate the
typically you want to accentuate the
upper end of it so like if you have a
upper end of it so like if you have a
score from uh that's like a win rate so
score from uh that's like a win rate so
like 099 is you win 99% of the time and
like 099 is you win 99% of the time and
09 is you win 90% of the time you want
09 is you win 90% of the time you want
there to be a big difference between
there to be a big difference between
those
those
two um usually this is going to work off
two um usually this is going to work off
of some sort of difference between
of some sort of difference between
scores so you're going to look at the
scores so you're going to look at the
difference of the score of one run
difference of the score of one run
versus another and you need the
versus another and you need the
algorithm to actually care about the
algorithm to actually care about the
upper end of that
upper end of that
range but not too much so there's you
range but not too much so there's you
know there's a function that needs to be
know there's a function that needs to be
fit to
fit to
that so what I'm looking at here is
that so what I'm looking at here is
essentially the slope at the upper end
essentially the slope at the upper end
of the range so for
instance here if we go from 0.9 this
instance here if we go from 0.9 this
goes from .25
goes from .25
is to3 so you only get
0.05
3D uh this is a 2d
3D uh this is a 2d
function I don't think that this this
function I don't think that this this
one is would make
one is would make
sense to add more Dimensions to
this okay this is good here so this one
this okay this is good here so this one
you get
you get
Point hold on you
get it's a little tricky I need
to I need to take into account the the
to I need to take into account the the
whole range of this so here's like your
whole range of this so here's like your
midpoint is
Right loss this isn't a loss though this
Right loss this isn't a loss though this
is
is
uh this is something that's fit to a
uh this is something that's fit to a
hyper parameter sweep not to an
hyper parameter sweep not to an
individual
individual
experiment yeah loss space can be 3D or
experiment yeah loss space can be 3D or
I mean loss space is very high
dimensional okay so if you set this this
dimensional okay so if you set this this
one is I think 0. five
one is I think 0. five
so I mean in the limit this is just
so I mean in the limit this is just
going to become a line right so this is
going to become a line right so this is
essentially linear
this one
is25 it's going to be something
is25 it's going to be something
something in here I think one of these
something in here I think one of these
two so this one is virtually 0 to one a
two so this one is virtually 0 to one a
little off but we'll call it 0 to one
little off but we'll call it 0 to one
and we allocate
it's like 75 and it
doubles so if you go if this is
0.9 we might just have to test both of
0.9 we might just have to test both of
these
I think it's something between these
I think it's something between these
two is going to be the
best I'm trying to think how environment
best I'm trying to think how environment
dependent this would be as well
this is five squares and then this is
this is five squares and then this is
what four
squares this is so this is reinforcement
squares this is so this is reinforcement
learning hyper parameter sweep right
learning hyper parameter sweep right
this stream is all about reinforcement
this stream is all about reinforcement
learning
learning
Dev
Dev
um I mean this is a sweep where every
um I mean this is a sweep where every
single point here is an experiment that
single point here is an experiment that
achieves a certain score and cost a
achieves a certain score and cost a
certain amount to run
certain amount to run
I mean and then we have data for a bunch
I mean and then we have data for a bunch
of real experiments like this and what
of real experiments like this and what
we're trying to do at the end of the day
we're trying to do at the end of the day
is we're trying to come up with a method
is we're trying to come up with a method
for running a ton of experiments and
for running a ton of experiments and
setting the hyperparameters of those
setting the hyperparameters of those
experiments such that we are able to
experiments such that we are able to
discover uh the Optimal
discover uh the Optimal
Performance at any specific experimental
Performance at any specific experimental
cost and ideally we want to be able to
cost and ideally we want to be able to
find uh hyper parameters that make the
find uh hyper parameters that make the
environment train quickly and also
environment train quickly and also
perform
well and this is just one small part of
well and this is just one small part of
the algorithm for that though a very
the algorithm for that though a very
important
part I'm just trying to get a sense of
part I'm just trying to get a sense of
how the scaling works
how the scaling works
here if I just count
squares so five for2 and then this is
squares so five for2 and then this is
like
like
three two and
three two and
half and
half and
[Music]
two um a move 37 is the problem with
two um a move 37 is the problem with
that is that it's dependent you can't
that is that it's dependent you can't
have a new like you can't really get
have a new like you can't really get
that on an environment that isn't
that on an environment that isn't
excessively played that's really more of
excessively played that's really more of
like a human thing than an AI thing um
like a human thing than an AI thing um
you need that only can exist in an
you need that only can exist in an
environment where people play it a ton
environment where people play it a ton
and have like an established way where
and have like an established way where
they're really confident that this is
they're really confident that this is
the way that you do it and then they're
the way that you do it and then they're
just
just
wrong so AI can come up with awesome
wrong so AI can come up with awesome
stuff all the time we're just not going
stuff all the time we're just not going
to know about it if we don't have a
to know about it if we don't have a
setting like that if we're not using a
setting like that if we're not using a
setting like that
this is this one is the one that's I
this is this one is the one that's I
think the
closest the thing is this is devoting so
closest the thing is this is devoting so
this devotes half of the
this devotes half of the
space to the last
20% I think I'm fine with
that and then it devotes about to the
that and then it devotes about to the
last
10% 30% of this Spas to the last 10%
18% to the last
5% last 5% is one this is
fine and then a few perent to the last
fine and then a few perent to the last
so this is I think that this one is
so this is I think that this one is
probably this one is probably the best
probably this one is probably the best
one here this is
one here this is
0.1 uh the 0.01 is insane I think yeah
0.1 uh the 0.01 is insane I think yeah
this one is insane because this one here
if we try to get to
if we try to get to
one yeah one
two yeah this one devotes half of its
two yeah this one devotes half of its
space to the last
10% it could actually
that could actually be what we
that could actually be what we
want let me think about
that so the margin of error is going to
be 40 is going to be like a percent
be 40 is going to be like a percent
right how much space does this devote to
right how much space does this devote to
the 9 to 99% is my
the 9 to 99% is my
question so 99 is
Right 15% of the space is devoted to the
Right 15% of the space is devoted to the
last 1% of score
okay we have some options this makes
okay we have some options this makes
this uh this at least makes some
sense let's see if we can fix up the
sense let's see if we can fix up the
algorithm now with that so the first
algorithm now with that so the first
thing is I don't know why I didn't
thing is I don't know why I didn't
normalize the score coming in here
oneus
one H thanks for the offer but no I'm
one H thanks for the offer but no I'm
not going to accept that thank you
not going to accept that thank you
though that's
though that's
funny that's cool you do laser engraving
funny that's cool you do laser engraving
stuff
stuff
that very
cool puffer doesn't get any
cool puffer doesn't get any
uh puffer doesn't get any any swag until
uh puffer doesn't get any any swag until
we
we
uh once we cross a million in Revenue
uh once we cross a million in Revenue
then we can get some swag
maybe but early
maybe but early
stages very very early on
you know those startups that like they
you know those startups that like they
just like it's brand new they don't even
just like it's brand new they don't even
have anything and then they've got like
have anything and then they've got like
all sorts of
t-shirts I did uh I don't know I took a
t-shirts I did uh I don't know I took a
course at Stamford I did like I did a
course at Stamford I did like I did a
little bit of laser engraving stuff uh I
little bit of laser engraving stuff uh I
mean it was just on
mean it was just on
wood yeah it's
fun back then it was just like an Adobe
fun back then it was just like an Adobe
Illustrator file I think that you could
Illustrator file I think that you could
do yeah you could set the strength as
do yeah you could set the strength as
well so you could kind of etch it was
cool that was fun work I do sometimes
cool that was fun work I do sometimes
Miss like doing work in a shop being at
Miss like doing work in a shop being at
the keyboard all day gets boring even if
the keyboard all day gets boring even if
you are working on cool stuff a lot of
you are working on cool stuff a lot of
the
the
time but hey we're making good
time but hey we're making good
algorithms here
well I have this set to want minus
well I have this set to want minus
two okay then y
two okay then y
norm and you rorm like
this
form for
YT and then what score space do I change
YT and then what score space do I change
this
in
in
GPT
Norm yeah this is fine I think the rest
Norm yeah this is fine I think the rest
of this is fine we'll see if I have any
of this is fine we'll see if I have any
bugs with this but we will run this
we might now have to rescale some stuff
we might now have to rescale some stuff
I don't know how I missed this
I don't know how I missed this
before I thought I'd normalized the
input I don't know if it'll make a huge
input I don't know if it'll make a huge
difference here but it could definitely
difference here but it could definitely
make a big difference on
Palm for
this is pretty
funny everybody's trying to sell deep
funny everybody's trying to sell deep
research it's like yeah okay you give us
research it's like yeah okay you give us
a problem we'll deeply research it for
a problem we'll deeply research it for
you
now the question is going to be did we
now the question is going to be did we
completely Break
Stuff uh either we completely broke
Stuff uh either we completely broke
stuff or I'm just reporting
oh I'm reporting wrong let me
see what do we report we
report
report
score gpy
we'll see in a second I guess
yeah we're not taking B
yeah we're not taking B
see we just bootstrapped some
see we just bootstrapped some
stuff do contract work around RL
okay so this is very
okay so this is very
close almost identical I
close almost identical I
think so I don't think we broke anything
think so I don't think we broke anything
massive
predictions are definitely wrong here so
predictions are definitely wrong here so
we probably just didn't report this
correctly this
gpy okay so you take gpy T
Norm you linearize
oh other way
oh other way
around
nor should be
this
GPT
form for
then we'll try this with 1 eus 2 and
then we'll try this with 1 eus 2 and
we'll try this with 1 E minus
one hopefully this should be some
one hopefully this should be some
substantially
better
e
e
e
e e
yeah the score uh and predicted are
yeah the score uh and predicted are
still completely different so I still
still completely different so I still
have something
broken gpy
wait hang on Max
wait hang on Max
4 yeah the inverse is wrong hold on
GP get rid of thisiz
I think that this should be like
I think that this should be like
virtually identical
virtually identical
right
right
yeah there'll be a little bit of
yeah there'll be a little bit of
variation run to run
variation run to run
but this is pretty
good and now I think we fixed the score
good and now I think we fixed the score
prediction do we use this FR gpy
Norm no because you use gpy
gpy T Norm for the score
CL e
what are you
what are you
doing fine tuning this hyperparameter to
doing fine tuning this hyperparameter to
tuning algorithm I'm tuning the hyper
tuning algorithm I'm tuning the hyper
parameter tuning
parameter tuning
algorithm uh there were we ran a bunch
algorithm uh there were we ran a bunch
of experiments
of experiments
overnight uh this is probably
overnight uh this is probably
state-of-the-art this is very very good
state-of-the-art this is very very good
but there are a couple quirks with this
but there are a couple quirks with this
tuning
tuning
algorithm uh the big one at the moment
is hold on
so there're just a ton of points right
so there're just a ton of points right
here there's this huge just cluster of
here there's this huge just cluster of
points all the way over here so it
points all the way over here so it
should spread this data out a little bit
more and that will make the uh The Sweep
more and that will make the uh The Sweep
substantially faster to run
currently the thing that needs to be
currently the thing that needs to be
fixed
fixed
is actually I have a nice graph of
is actually I have a nice graph of
it there's a transformation that is
it there's a transformation that is
applied
applied
to um to the scores that the
to um to the scores that the
environments get and the one that I've
environments get and the one that I've
come up with can
come up with can
be it's kind of a family of
be it's kind of a family of
transforms parameterized by this Epsilon
transforms parameterized by this Epsilon
which is this number here at the
which is this number here at the
end and depending on how you set it
end and depending on how you set it
up it either like really encourages you
up it either like really encourages you
to push everything to the right hand
to push everything to the right hand
side to push everything to very high
side to push everything to very high
cost or it doesn't encourage you at all
cost or it doesn't encourage you at all
so we have to tune this parameter a
so we have to tune this parameter a
little bit and just get a feel for how
little bit and just get a feel for how
sensitive it is and stuff like that
sensitive it is and stuff like that
the unfortunate thing with Hyper
the unfortunate thing with Hyper
parameter algorithms that have hyper
parameter algorithms that have hyper
parameters is who Tunes the tuner
that's a funny little
qup who Tunes the tuner
okay this
ran deep learning exam and what course
ran deep learning exam and what course
is this
my favorite
my favorite
uh ML Class was Stanford
cs231n with Justin's lectures or Andre's
cs231n with Justin's lectures or Andre's
lectures those are both good okay it's
lectures those are both good okay it's
in Germany cool they have courses in
in Germany cool they have courses in
this stuff
now okay so this looks pretty decent
now okay so this looks pretty decent
here this is spoton with what we want
yeah this is pretty spoton with what we
yeah this is pretty spoton with what we
want let me make sure that this is oh
want let me make sure that this is oh
yeah no this is perfect and then the
yeah no this is perfect and then the
ratings
here are pretty consistent
we need to be a little careful with the
we need to be a little careful with the
suggestions per Paro
suggestions per Paro
right can we o
it we shouldn't be able to O it
right no it should be able to handle a
right no it should be able to handle a
very large batch
very large batch
actually I kind of want to do
this I think it'll make it just a little
this I think it'll make it just a little
bit more
bit more
consistent and
consistent and
then well that can't hurt it so I think
then well that can't hurt it so I think
we'll be fine leaving
we'll be fine leaving
that uh can it hurt it it might actually
that uh can it hurt it it might actually
be able to hurt it let me
be able to hurt it let me
think more samples technically means you
think more samples technically means you
have more chances to go off the rail so
have more chances to go off the rail so
we do have to rerun it for this so we'll
we do have to rerun it for this so we'll
give it a second
give it a second
and uh I think after this we just test
and uh I think after this we just test
out uh one or two of the other synthetic
out uh one or two of the other synthetic
problems and then we run the pong sweep
again we get C cumulative cost
will
will
tune yeah good
luck all right this looks
fine it's a little funny how it finds oh
fine it's a little funny how it finds oh
no cuz it's going to find higher rating
no cuz it's going to find higher rating
stuff for a little bit these aren't
stuff for a little bit these aren't
going to be consistent until it EXP the
going to be consistent until it EXP the
whole front so now it should start
whole front so now it should start
reducing
right yeah so this rating term should
decrease and we'll be able to compute
decrease and we'll be able to compute
all sorts of metrics over that
you could even have this thing
you could even have this thing
automatically terminated below a certain
automatically terminated below a certain
rating threshold because this is just an
rating threshold because this is just an
estimate of how much you how much value
estimate of how much you how much value
you expect to get from the experiment
right that's pretty
solid so I mean we'll see it's possible
solid so I mean we'll see it's possible
that this totally messes up the pong
sweep but hopefully not
and I think we can just run
this you can see it's like it's not
this you can see it's like it's not
expecting to get a ton from these these
expecting to get a ton from these these
runs now it's just like
runs now it's just like
a I mean that's a very nice estimate
a I mean that's a very nice estimate
that's a really really good metric to
that's a really really good metric to
have
have
holy a hyperparameter algorithm that
holy a hyperparameter algorithm that
tells you
tells you
how much value it has to keep running
it and actually you could complete
it and actually you could complete
compete this uh you could compute this
metric H
ideas I do have ideas
but that's a nice consistent
metric oh well okay if you're going to
metric oh well okay if you're going to
give me a perfect curve I'll take
give me a perfect curve I'll take
that thank you very
that thank you very
much and look at that actually this is
much and look at that actually this is
good it gives you in the elbow of the
good it gives you in the elbow of the
curve do you see how much coverage you
curve do you see how much coverage you
have
have
here you'd want coverage here in the
elbow uh I'm going to run this on one
elbow uh I'm going to run this on one
more synthetic task I think and then I'm
more synthetic task I think and then I'm
going
going
to H I think we can just call it and run
to H I think we can just call it and run
it on pong it's pretty solid as
is
is
on mod
no Cuda gpus are available
no Cuda gpus are available
really well that's
weird did my drivers crash something
somehow reboot the container
random driver crash is
random driver crash is
weird I'm going to have to go get the
weird I'm going to have to go get the
uh
token and we shall see how this does
there we
go
okay um there are still a few things
okay um there are still a few things
that we want to experiment with but this
that we want to experiment with but this
this should be enough to start running a
this should be enough to start running a
bunch of experiments I mean a bunch of
bunch of experiments I mean a bunch of
uh a bunch of runs cuz I think anything
uh a bunch of runs cuz I think anything
else we're going to do with this we'll
else we're going to do with this we'll
probably want to run far more
experiments so we will let this go and
experiments so we will let this go and
um we will check back in in a little bit
um we will check back in in a little bit
to see how this
to see how this
goes you see we have a few folks on
goes you see we have a few folks on
Twitch at the moment So for anybody
Twitch at the moment So for anybody
interested in following my work this is
interested in following my work this is
live tons of R
Lev buffer. it's all free and open
Lev buffer. it's all free and open
source you can check it out right
source you can check it out right
here St the repo to help us out can join
here St the repo to help us out can join
the Discord if you want to get involved
the Discord if you want to get involved
can follow me on X for a lot more
can follow me on X for a lot more
information on reinforcement learning
information on reinforcement learning
and what we are
doing we also have a Blog with lots of
doing we also have a Blog with lots of
information some of the latest
information some of the latest
developments in RL but also quick start
developments in RL but also quick start
guides for beginners so thank you and I
guides for beginners so thank you and I
will probably be back later in the
will probably be back later in the
afternoon working on this some more

Kind: captions
Language: en
here's our sweet progress it's pretty
good and here is our Pito curve
this is pretty solid
there are a few things to figure out
there are a few things to figure out
here there are a ton of points grouped
here there are a ton of points grouped
up over
here there's also too much encouragement
here there's also too much encouragement
to
to
uh to you do this in the first
place where's the first like solved
Point H 20.9 over
Point H 20.9 over
here it's kind of crazy that it even
here it's kind of crazy that it even
found something this expensive let me
found something this expensive let me
say what did it find
we're missing mini bat size I think here
okay so you can see here it's
okay so you can see here it's
actually it's intentionally pushed
actually it's intentionally pushed
everything down to the bottom
B now this will be the graph that we
B now this will be the graph that we
want to see here so
okay cool this I think this
okay cool this I think this
has mostly what we need
mini
mini
batch what's missing here I think just
batch what's missing here I think just
mini batch
so here's the cost here's the
front so learning rate
we actually I think we get the best
we actually I think we get the best
results with the lowest learning
rate just by a little bit
right I would call this more of a stable
right I would call this more of a stable
region I guess but this I think this is
region I guess but this I think this is
the best point right here is the one
the best point right here is the one
with the absolute lowest learning rate
with the absolute lowest learning rate
over here
there's an outlier ignore
this something happened with these
this something happened with these
points that we'll have to figure out I
points that we'll have to figure out I
think these are probably like the
think these are probably like the
resamples or
something and then gamma
yeah anything from0 n what is
yeah anything from0 n what is
this 98
something yeah nine pretty much anything
something yeah nine pretty much anything
from like
from like
Point really it should be 0 N9 it looks
Point really it should be 0 N9 it looks
like based on
like based on
this we might even want to increase this
range Lambda a little bit
more I really actually all the ones at
more I really actually all the ones at
the top are pretty
the top are pretty
high 98
99 probably this one needs to be logic
spaced update EPO one gets you to
19 to 20
9 then the value coefficient
here yeah the normal5 is
here yeah the normal5 is
fine I I don't know why it did all of
fine I I don't know why it did all of
this mess but um it does seem like
this mess but um it does seem like
anything over here is fine it's
cool maximum gradient
cool maximum gradient
Norm .5 CS to make it substantially
Norm .5 CS to make it substantially
worse over here right
Norm of
Norm of
one more
one more
stable
stable
entropy everything from virtually no
entropy everything from virtually no
entropy out
to 01 is like yeah so pretty much
to 01 is like yeah so pretty much
there's good stuff anywhere in the
there's good stuff anywhere in the
entropy range for this n
is this actually better it looks like
is this actually better it looks like
there are better points
there are better points
here
20.9 maybe
number of
M's okay and batch
size any of the back sizes are good
uh this plot gets
removed mini batch size
here we
go up to 4096 it seems is good there's
go up to 4096 it seems is good there's
degradation after 4096
I mean this is kind of
nutty pull restream up we'll continue on
nutty pull restream up we'll continue on
this analysis I mean this is this is a
this analysis I mean this is this is a
good result this is not a perfect
good result this is not a perfect
result um the reason this is not a
result um the reason this is not a
perfect result
perfect result
is made some very expensive runs
the simple
end e
I don't understand their filter thing it
I don't understand their filter thing it
just doesn't work
well that did it wait no it didn't
why do I now only
have I have 99
items oh there we
go for
it's funny there is still a little Gap
it's funny there is still a little Gap
here
all the best results are with a small
all the best results are with a small
number of ends
so there's a huge amount of uh different
so there's a huge amount of uh different
good learning rates learning rates
good learning rates learning rates
actually one of the most stable
actually one of the most stable
parameters for this
end gamma's optimal at 995 which means I
end gamma's optimal at 995 which means I
should extend that
range probably on both of them
and four update
EO also I have like 140 runs and over a
EO also I have like 140 runs and over a
100 of them
100 of them
are 97 uh
are 97 uh
19.75 and
up and actually they are almost all 20
up and actually they are almost all 20
and up
I still have 86 items
so still learning rating here is very
so still learning rating here is very
stable
stable
right
right
o5 so this
o5 so this
is 5 eus 4
1 E minus
4 looks like 2 e minus 4 is a good
4 looks like 2 e minus 4 is a good
starting
point the original was way too high
okay this is definitely now when you
okay this is definitely now when you
plot it this way you can see you
plot it this way you can see you
definitely have to
definitely have to
expand gamma outward Lambda actually
expand gamma outward Lambda actually
doesn't matter so much right Lambda
doesn't matter so much right Lambda
they've done all like all these trials
they've done all like all these trials
here but actually the best ones are a
here but actually the best ones are a
little bit out this
way so gamma needs to go up a bit
I'm going to start on that
actually no reason not to
so
I I forget if there's a reason I did
I I forget if there's a reason I did
this but I doubt
this but I doubt
it do that
GMA to be fair I did kind of fix the
GMA to be fair I did kind of fix the
search ranges so it doesn't matter so
search ranges so it doesn't matter so
much
anymore this definitely goes to 0.
anymore this definitely goes to 0.
N9 um nine as a
N9 um nine as a
Max then
gamma I'm going to leave it at 098
gamma I'm going to leave it at 098
because I know pong is uh you know it's
because I know pong is uh you know it's
for other ends it can be
for other ends it can be
different five this is fine DX we're not
different five this is fine DX we're not
going to let you do more than four it's
going to let you do more than four it's
ridiculous
value
coefficient yeah looks like
coefficient yeah looks like
0.75 or
0.75 or
08 is way better
the best result is at minimal entropy
the best result is at minimal entropy
actually
here this is fine
here this is fine
back prop Horizon is
reasonable and at some point the number
reasonable and at some point the number
of Ms just gets to be too bloody
small bad size prams look good
we're not going smaller than this for
we're not going smaller than this for
Hardware
Hardware
reasons plus their points
reasons plus their points
here
here
okay I'm going to try to look at the
charts that's actually surprisingly it's
charts that's actually surprisingly it's
very stable
and actually it's stable in uh not that
and actually it's stable in uh not that
many steps isn't
it that's 10 million
steps with a bunch of high scores
right it'ss all the way back
here so it just pushed everything to the
here so it just pushed everything to the
maximum
maximum
time it really didn't try very
time it really didn't try very
hard at the lower end of the Pito
hard at the lower end of the Pito
Frontier because they're all these
Frontier because they're all these
stable
curves and like the shape is a little
curves and like the shape is a little
inconsistent at the start right like
inconsistent at the start right like
maybe it takes you an extra few million
maybe it takes you an extra few million
steps depending on the
seed was very stable
overall see cost
like okay it says that the best runs are
like okay it says that the best runs are
at the highest cost but there are only a
at the highest cost but there are only a
few of them that are the best the best
few of them that are the best the best
and uh it's CU it ran so many
and uh it's CU it ran so many
experiments
right it says we have some as low
as it's 130
as it's 130
cost so a little over 2
minutes and probably this point
minutes and probably this point
here could have been run for fewer
steps okay
steps okay
these are very good results
then we have to the only thing we really
then we have to the only thing we really
have to think about is how this thing is
have to think about is how this thing is
budgeting its compute cuz the result
budgeting its compute cuz the result
itself is very
itself is very
good now we're happy with the Sweep
result can see it there's still some
result can see it there's still some
diversity in here as
diversity in here as
well with the
well with the
cost so it's still doing different
stuff let's take the filter off
yeah so you can see here in
yeah so you can see here in
cost that there is a span here where it
cost that there is a span here where it
does a bunch of these very high cost
does a bunch of these very high cost
runs that are probably pretty
runs that are probably pretty
similar but then it does spread
similar but then it does spread
out so I imagine this is it filling in
out so I imagine this is it filling in
the frontier at the
end I mean these early runs like
so many of these
runs
yeah interesting that it still me misses
yeah interesting that it still me misses
a few of
a few of
these like hard misses I guess trying to
these like hard misses I guess trying to
fill in lower end of Fredo
front I mean this is the main
right the Pito chart
I have exactly the curve I
I have exactly the curve I
want but too much out at the high cost
want but too much out at the high cost
and and actually I think I can
scale 350
yeah I can just see what how many of
yeah I can just see what how many of
these things are above
these things are above
three uh above like 300 cost
okay so we have 60 runs so almost half
okay so we have 60 runs so almost half
of the
of the
runs right between a third and a half of
runs right between a third and a half of
the runs are at the really really
the runs are at the really really
highend
cost we need this to not happen
okay so there are a couple things going
okay so there are a couple things going
on here
I think the first is we're telling it
I think the first is we're telling it
that um
that um
29.9 is way or 20 uh 20.9 is way better
29.9 is way or 20 uh 20.9 is way better
than like
20.7 so we're overemphasizing the high
20.7 so we're overemphasizing the high
end so
end so
much that it's scaling to Max
cost we're also not penalizing higher
cost we're also not penalizing higher
cost at all
okay so the other thing that I haven't
okay so the other thing that I haven't
thought about
here I have this binary search aspect to
here I have this binary search aspect to
my algorithm
where it's supposed to fill in the gaps
where it's supposed to fill in the gaps
in the uh the cost
in the uh the cost
curve but if you do that in linear space
curve but if you do that in linear space
right if you sum up if you just did like
right if you sum up if you just did like
the cost is 1 to 10 and then you do that
the cost is 1 to 10 and then you do that
you're going to get like 1 to
you're going to get like 1 to
well the sum of 1 to 5 is
well the sum of 1 to 5 is
15 I believe 6 to 10 is 40
15 I believe 6 to 10 is 40
right cuz it's
55 so you're spending all the compute at
55 so you're spending all the compute at
the high
end e
one thing to look at
here there are good
here there are good
runs at the low end
take a look at the algorithm
burito C
Norm a 01 Norm
I see so it is just the cost in a linear
scale if I do the cost and log distance
scale if I do the cost and log distance
does that fix it
we can kill the Palm
we can kill the Palm
sweep I
sweep I
think cuz it's like it's complete
right I want to pollute the last point
right I want to pollute the last point
so I'll let that finish
so I'll let that finish
H this is
H this is
fine okay so we have our sweep locked in
fine okay so we have our sweep locked in
this is the first good sweep as well
this is the first good sweep as well
that we've gotten in a while so it is
that we've gotten in a while so it is
encouraging to get such a good sweep as
encouraging to get such a good sweep as
the first one with such heavy
the first one with such heavy
modification of the algorithm
and test this then going to test log
and test this then going to test log
cost
oh I'm kind of dumb I guess
huh yeah I'm kind of dumb
here this literally shows it to you
here this literally shows it to you
doesn't
it so this is what we got on the
it so this is what we got on the
percentile task
percentile task
right and uh the top
this little chunk of data here is
this little chunk of data here is
using half the compute
okay so this run it looks like we did
okay so this run it looks like we did
more data points but this is the same
more data points but this is the same
thing
thing
right this curves the same
let me just check one thing with
visualize yeah it goes to one not zero
visualize yeah it goes to one not zero
here this is is
fine
fine
so yeah this is going to be an
issue
duh let me try the next
duh let me try the next
thing we'll do it in log space
ay
Norm e
I had to be careful to get that right
see if this evens the curve
out e
funny most common question I get is
funny most common question I get is
probably about RL and trading it's
crazy important here oh
really I'm confused that that didn't do
really I'm confused that that didn't do
much
it didn't seem to do very
much no it's it's slightly less but it's
much no it's it's slightly less but it's
not linear still
okay the next thing is going to be the
okay the next thing is going to be the
score function
the next thing is going to be the score
the next thing is going to be the score
function I think
so I don't know if I like this in in log
so I don't know if I like this in in log
space either
well we will see
let's go back we'll do this one at a
let's go back we'll do this one at a
time
time
here the other one is the score
function I think I need to understand
function I think I need to understand
this score function for
this is what you get for a score of
this is what you get for a score of
10 what you get for a score of
20 8.5
goes up to
6.9 okay so the Epsilon
Factor the Epsilon factor is actually
Factor the Epsilon factor is actually
pretty nice
pretty nice
so if we do like minus one here right
so if we do like minus one here right
20 this goes to
20.3 versus
20 this is almost linear right
so I think that this Epsilon factor is
so I think that this Epsilon factor is
very
important this should essentially give
important this should essentially give
you the aggression Factor
I want to see what happens with 1 E
I want to see what happens with 1 E
minus one first to see if this is
actually going to change the data the
actually going to change the data the
way I
expect this score transformation is very
expect this score transformation is very
important um if you make it very
important um if you make it very
aggressive then the algorithm is going
aggressive then the algorithm is going
to bias very heavily towards trying to
to bias very heavily towards trying to
find runs that really really Max the
find runs that really really Max the
score at any cost um but if you make it
score at any cost um but if you make it
two passive it won't find
two passive it won't find
the best
results this is hilarious
up for
okay so here is our oops
okay so here is our oops
this is
before well this is before to be fair
before well this is before to be fair
the other one was with the different
change okay
interesting that also was not enough
do I put them both together
both of these and if this doesn't work
both of these and if this doesn't work
then we'll have to reconsider some
then we'll have to reconsider some
stuff things won't make
sense to reconsider some scoring
functions
e
e e
does the variability of the samples tell
does the variability of the samples tell
you
much probably doesn't tell you a ton
right let me make sure that I actually
right let me make sure that I actually
hit visualized before I did so these two
hit visualized before I did so these two
together fixes
together fixes
it this is
perfect yeah this is a perfect
perfect yeah this is a perfect
curve this is exactly what we
curve this is exactly what we
want now the question is going to be
the Epsilon
term Epsilon is the aggression factor I
think man if this works if this just
think man if this works if this just
works this way this is going to be
works this way this is going to be
awesome this is going to be so cool
e
you just publish
he just published a
thing
thing
oh
maybe he
maybe he
pinned that's weird I thought he had a
pinned that's weird I thought he had a
paper maybe
not
for
e e
this is it right
yeah is it really this much caused by 1
yeah is it really this much caused by 1
eus2 to 1 eus 3 that jump hang
eus2 to 1 eus 3 that jump hang
on we'll rerun this again and then we'll
on we'll rerun this again and then we'll
test one minus 1 2 and 3
and then we'll T us longer yo Spencer
and then we'll T us longer yo Spencer
yes uh
yes uh
absolutely I am going to do this morning
absolutely I am going to do this morning
session get some breakfast and then
session get some breakfast and then
afternoon is perfectly good and uh we
afternoon is perfectly good and uh we
will I can look at your code we will
will I can look at your code we will
chat about some research stuff and I may
chat about some research stuff and I may
as well even like go uh install the CPUs
as well even like go uh install the CPUs
while we're chatting about
that also I think we have soda hyper
that also I think we have soda hyper
parameter
tuning uh there are a few things I want
tuning uh there are a few things I want
to
to
tweak
tweak
but I mean I think we're going to
but I mean I think we're going to
have and we have the the result on pong
have and we have the the result on pong
as well already
where's the where's the thing yeah know
where's the where's the thing yeah know
this is like really
this is like really
[Music]
[Music]
good yeah this parito front here this is
good yeah this parito front here this is
perfect now the only thing wrong with it
perfect now the only thing wrong with it
is that I think it spends too much time
is that I think it spends too much time
on expensive runs uh it needs to be so
on expensive runs uh it needs to be so
it spends too much time on these
it spends too much time on these
expensive runs it needs to spend more
expensive runs it needs to spend more
time over here so we're going to figure
time over here so we're going to figure
out some configuration stuff with it but
out some configuration stuff with it but
I think we're just going to full solve
I think we're just going to full solve
everything with this
that's the
hope it's been quite a bit of work it
hope it's been quite a bit of work it
really has been quite a bit of work I
really has been quite a bit of work I
think there's still some
left all right so this is the original
left all right so this is the original
that we
that we
had yeah this is the
had yeah this is the
original you get this sort of a curve
complex M we have a bunch of complex M's
complex M we have a bunch of complex M's
actually I think that we need what we
actually I think that we need what we
really need is we need more stuff on the
really need is we need more stuff on the
research side um there's too many things
research side um there's too many things
I want to test and I don't have people
I want to test and I don't have people
to do
it I mean I think that you've
it I mean I think that you've
demonstrated with the m that you've
demonstrated with the m that you've
built that you should be able to handle
built that you should be able to handle
some of the research side stuff pretty
well we'll see maybe I'm overestimating
well we'll see maybe I'm overestimating
but I think in my mind the science is
but I think in my mind the science is
dramatically easier than than the
dramatically easier than than the
engineering
so maybe that's just because I've been
so maybe that's just because I've been
doing it forever but I don't think it's
doing it forever but I don't think it's
that hard
okay so the log cost
has quite some
impact I think I need to run this for
longer yeah we need to get the
original yeah there's your original
okay we're going to need to do four runs
okay we're going to need to do four runs
that are going to take a few minutes but
that are going to take a few minutes but
whatever you know we we really need to
whatever you know we we really need to
do it just to make absolutely certain
do it just to make absolutely certain
that we have this correct
new run going on medium difficulty steps
new run going on medium difficulty steps
per second back to 900k that's what we
per second back to 900k that's what we
like that very
good it ironically I so this
good it ironically I so this
hyperparameter stuff I'm doing it
hyperparameter stuff I'm doing it
doesn't like solve all the problems in
doesn't like solve all the problems in
RL but I think that this gets our foot
RL but I think that this gets our foot
in the door like my hope is that what's
in the door like my hope is that what's
going to happen is right now RL
going to happen is right now RL
experimentation just feels very clunky
experimentation just feels very clunky
and inconsistent I think that if you
and inconsistent I think that if you
have a really good sweep algorithm that
have a really good sweep algorithm that
you know that you can trust um then at
you know that you can trust um then at
the very least you're going to know okay
the very least you're going to know okay
I can run a sweep and if it's just
I can run a sweep and if it's just
hyperparameters it's going to find them
hyperparameters it's going to find them
that's the
that's the
goal so then basically everything else
goal so then basically everything else
that we do just becomes much faster
that we do just becomes much faster
because it's way higher confidence right
because it's way higher confidence right
you know how you just like everything is
you know how you just like everything is
just slower when you don't really know
just slower when you don't really know
for sure whether your experiments or
for sure whether your experiments or
whether the stuff you're doing is
whether the stuff you're doing is
correct so like you kept always second
correct so like you kept always second
gu everything I want to eliminate that
gu everything I want to eliminate that
from
RL so I took a quick look at that paper
RL so I took a quick look at that paper
uh it looks good that's nice they have
uh it looks good that's nice they have
two variations of two
environments so um
environments so um
yeah ironically the number of
yeah ironically the number of
experiments that they ran would be like
experiments that they ran would be like
we would do those in a few days on our
we would do those in a few days on our
few
few
gpus with like a puffer
end I think I've done more more
end I think I've done more more
experiments than they have
experiments than they have
done for that paper just testing this
done for that paper just testing this
algorithm just testing this hyper
algorithm just testing this hyper
parameter sweep
it looks like very good work I think
it looks like very good work I think
Eugene's on it as
well
e e
okay this is the previous
okay this is the previous
one this will
one this will
be the new
Baseline okay these match very nice
good and then we got to
do we got to do log
cost be the next
one
e
e e
box one went
down I think I meant to go outside this
down I think I meant to go outside this
morning to reboot it and I
morning to reboot it and I
forgot so let me finish this and then I
forgot so let me finish this and then I
will go reboot it and have breakfast and
will go reboot it and have breakfast and
we will go from there
it's going to be much easier in the new
it's going to be much easier in the new
facility where I just have to walk
facility where I just have to walk
across the room to reboot stuff so
across the room to reboot stuff so
hopefully they won't be
crashing we're definitely going to do
crashing we're definitely going to do
some good
tests yeah add me on Discord whenever
tests yeah add me on Discord whenever
I'm pretty much I'm going to finish this
I'm pretty much I'm going to finish this
morning session another half hour
morning session another half hour
whatever I do here and then um I'm going
whatever I do here and then um I'm going
to get breakfast and then I'll be around
to get breakfast and then I'll be around
the whole rest of the
day see you
Spencer e
all
right two
that's already dramatically
better still a whole bunch of points
better still a whole bunch of points
right at the
end e
e
the goal with this
the goal with this
here the uh the goal with this change is
here the uh the goal with this change is
to make this
to make this
algorithm here maybe I should just while
algorithm here maybe I should just while
this is running I'll go through real
this is running I'll go through real
quick so this curve is the total amount
quick so this curve is the total amount
of time spent in the experiments that
of time spent in the experiments that
cost up to this amount
cost up to this amount
so we spend a total
so we spend a total
of 15% or whatever of the time in
of 15% or whatever of the time in
experiments that cost 50 or less so all
experiments that cost 50 or less so all
these
these
experiments cost very
experiments cost very
little uh and then here you can see we
little uh and then here you can see we
spend another
20% in experiments from here to
20% in experiments from here to
here and then you can
here and then you can
see that the vast majority of the
see that the vast majority of the
compute 23 3/4 of the compute is spent
compute 23 3/4 of the compute is spent
on these experiments
on these experiments
here so the idea
here so the idea
here is you're spending too much time on
here is you're spending too much time on
these expensive
these expensive
experiments you'd like this to be more
experiments you'd like this to be more
even
oh yeah this is the first one this is
oh yeah this is the first one this is
our rerun of the same thing so this is
our rerun of the same thing so this is
the same and then this is with the
the same and then this is with the
modification so
modification so
now you can see that you're spending
now you can see that you're spending
little less than 2/3 it looks like of
little less than 2/3 it looks like of
the compute on these
experiments and then we'll see if this
experiments and then we'll see if this
changes
changes
material uh with the different Epsilon
I think the main thing that you can see
I think the main thing that you can see
here is this sort of cluster of
here is this sort of cluster of
points right that you spend a lot of the
points right that you spend a lot of the
compute right at the end here there's
compute right at the end here there's
this
this
Spike and this is not just noise this
Spike and this is not just noise this
the algorithm does
this to be fair it does it way worse
this to be fair it does it way worse
right here
right here
so we've already improved this thing
dramatically the spike is
gone I think this is going to be the
gone I think this is going to be the
better one
here I think this is it so
the thing is that we could try let me
the thing is that we could try let me
let me see
something th this Epsilon term is very
important it's not just like a noise
important it's not just like a noise
term or whatever so I should probably
term or whatever so I should probably
rename
it so this is if we just do actually let
it so this is if we just do actually let
me
do uh 101
15 okay so these points
this is with Epsilon let's do actually
this is with Epsilon let's do actually
one 2 three so you can see how this
changes so with Epsilon of 1 one minus
changes so with Epsilon of 1 one minus
one we do
one we do
20.5 this changes by about where is
it yeah from here 20
it yeah from here 20
825 uh
825 uh
285 3
285 3
4ish so this gets about.
4ish so this gets about.
5.55
is oh and this still gains
one okay this I'm looking at the wrong
one okay this I'm looking at the wrong
one my bad okay let's look down here so
from
from
20 to
20 to
20.5 this goes
up little less than 0 2 and a little
up little less than 0 2 and a little
less than 02 so this thing is no I
less than 02 so this thing is no I
should just plot these is what I should
do I should just plot these that would
do I should just plot these that would
be easier
this will be the quickest
okay
where's the green
one e
this okay it doesn't actually seem to
this okay it doesn't actually seem to
matter very much after
uh maybe it does it's just hard to
uh maybe it does it's just hard to
see oh no it definitely
see oh no it definitely
does yeah know it definitely definitely
does yeah let's get rid of this one
does yeah let's get rid of this one
though because after that it goes
though because after that it goes
crazy
crazy
okay this will be
important so right
important so right
here this
here this
is this is the reward graph
put this where you can see
it
it
so this goes
so this goes
negative is this pretty linear
negative is this pretty linear
looking that's pretty linear looking to
me it's just a shallow yeah line
so at -20
here3 or
whatever6 let me make sure I'm doing
whatever6 let me make sure I'm doing
this right as well
do I not Norm
I thought I had a normalization
I thought I had a normalization
operation on
this yeah this is screwed up
this should be above this shouldn't
it well okay let's say that this is 0 to
it well okay let's say that this is 0 to
one
scaled e
okay so the point1 is actually here this
okay so the point1 is actually here this
is the one I think that we would want
is the one I think that we would want
right this gives you
eight okay I didn't realize I just
eight okay I didn't realize I just
messed up the
messed up the
scaling this is going to take a little
scaling this is going to take a little
more work I'll be back in a few got to
more work I'll be back in a few got to
do a couple things and then we will
do a couple things and then we will
we'll take a hard look at the scaling
we'll take a hard look at the scaling
and uh we will finish the algorithm
and uh we will finish the algorithm
hopefully be right
back
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e
hello Tyler if you're still
hello Tyler if you're still
here sometimes uh X likes to eat
here sometimes uh X likes to eat
messages what do you graphing and why is
messages what do you graphing and why is
it important okay so this
it important okay so this
is this is the score
is this is the score
function that we are using for the
function that we are using for the
hyperparameter sweep this transformation
hyperparameter sweep this transformation
is applied
is applied
to uh score data that comes in from the
to uh score data that comes in from the
environment and this is the transform
environment and this is the transform
I've come up with and there are a few
I've come up with and there are a few
different well there's a parameter that
different well there's a parameter that
can change the shape of it
here okay so
interestingly if we're going to say that
interestingly if we're going to say that
we scale score data from 0 to
one doesn't matter that it always be
one doesn't matter that it always be
positive or anything like
positive or anything like
that doesn't
that doesn't
matter should not matter
so I think the thing to look at
so I think the thing to look at
here how's it defined by which policy uh
here how's it defined by which policy uh
so the way that this works is you get a
so the way that this works is you get a
score from the environment after
score from the environment after
training right this could be your win
training right this could be your win
rate this could be like how many points
rate this could be like how many points
you got in pong or whatever and uh we
you got in pong or whatever and uh we
need some way that is we need some way
need some way that is we need some way
to turn that data into a nice uniform
to turn that data into a nice uniform
space for the hyper parameter sweep to
space for the hyper parameter sweep to
use
use
uh and
uh and
typically you want to accentuate the
typically you want to accentuate the
upper end of it so like if you have a
upper end of it so like if you have a
score from uh that's like a win rate so
score from uh that's like a win rate so
like 099 is you win 99% of the time and
like 099 is you win 99% of the time and
09 is you win 90% of the time you want
09 is you win 90% of the time you want
there to be a big difference between
there to be a big difference between
those
those
two um usually this is going to work off
two um usually this is going to work off
of some sort of difference between
of some sort of difference between
scores so you're going to look at the
scores so you're going to look at the
difference of the score of one run
difference of the score of one run
versus another and you need the
versus another and you need the
algorithm to actually care about the
algorithm to actually care about the
upper end of that
upper end of that
range but not too much so there's you
range but not too much so there's you
know there's a function that needs to be
know there's a function that needs to be
fit to
fit to
that so what I'm looking at here is
that so what I'm looking at here is
essentially the slope at the upper end
essentially the slope at the upper end
of the range so for
instance here if we go from 0.9 this
instance here if we go from 0.9 this
goes from .25
goes from .25
is to3 so you only get
0.05
3D uh this is a 2d
3D uh this is a 2d
function I don't think that this this
function I don't think that this this
one is would make
one is would make
sense to add more Dimensions to
this okay this is good here so this one
this okay this is good here so this one
you get
you get
Point hold on you
get it's a little tricky I need
to I need to take into account the the
to I need to take into account the the
whole range of this so here's like your
whole range of this so here's like your
midpoint is
Right loss this isn't a loss though this
Right loss this isn't a loss though this
is
is
uh this is something that's fit to a
uh this is something that's fit to a
hyper parameter sweep not to an
hyper parameter sweep not to an
individual
individual
experiment yeah loss space can be 3D or
experiment yeah loss space can be 3D or
I mean loss space is very high
dimensional okay so if you set this this
dimensional okay so if you set this this
one is I think 0. five
one is I think 0. five
so I mean in the limit this is just
so I mean in the limit this is just
going to become a line right so this is
going to become a line right so this is
essentially linear
this one
is25 it's going to be something
is25 it's going to be something
something in here I think one of these
something in here I think one of these
two so this one is virtually 0 to one a
two so this one is virtually 0 to one a
little off but we'll call it 0 to one
little off but we'll call it 0 to one
and we allocate
it's like 75 and it
doubles so if you go if this is
0.9 we might just have to test both of
0.9 we might just have to test both of
these
I think it's something between these
I think it's something between these
two is going to be the
best I'm trying to think how environment
best I'm trying to think how environment
dependent this would be as well
this is five squares and then this is
this is five squares and then this is
what four
squares this is so this is reinforcement
squares this is so this is reinforcement
learning hyper parameter sweep right
learning hyper parameter sweep right
this stream is all about reinforcement
this stream is all about reinforcement
learning
learning
Dev
Dev
um I mean this is a sweep where every
um I mean this is a sweep where every
single point here is an experiment that
single point here is an experiment that
achieves a certain score and cost a
achieves a certain score and cost a
certain amount to run
certain amount to run
I mean and then we have data for a bunch
I mean and then we have data for a bunch
of real experiments like this and what
of real experiments like this and what
we're trying to do at the end of the day
we're trying to do at the end of the day
is we're trying to come up with a method
is we're trying to come up with a method
for running a ton of experiments and
for running a ton of experiments and
setting the hyperparameters of those
setting the hyperparameters of those
experiments such that we are able to
experiments such that we are able to
discover uh the Optimal
discover uh the Optimal
Performance at any specific experimental
Performance at any specific experimental
cost and ideally we want to be able to
cost and ideally we want to be able to
find uh hyper parameters that make the
find uh hyper parameters that make the
environment train quickly and also
environment train quickly and also
perform
well and this is just one small part of
well and this is just one small part of
the algorithm for that though a very
the algorithm for that though a very
important
part I'm just trying to get a sense of
part I'm just trying to get a sense of
how the scaling works
how the scaling works
here if I just count
squares so five for2 and then this is
squares so five for2 and then this is
like
like
three two and
three two and
half and
half and
[Music]
two um a move 37 is the problem with
two um a move 37 is the problem with
that is that it's dependent you can't
that is that it's dependent you can't
have a new like you can't really get
have a new like you can't really get
that on an environment that isn't
that on an environment that isn't
excessively played that's really more of
excessively played that's really more of
like a human thing than an AI thing um
like a human thing than an AI thing um
you need that only can exist in an
you need that only can exist in an
environment where people play it a ton
environment where people play it a ton
and have like an established way where
and have like an established way where
they're really confident that this is
they're really confident that this is
the way that you do it and then they're
the way that you do it and then they're
just
just
wrong so AI can come up with awesome
wrong so AI can come up with awesome
stuff all the time we're just not going
stuff all the time we're just not going
to know about it if we don't have a
to know about it if we don't have a
setting like that if we're not using a
setting like that if we're not using a
setting like that
this is this one is the one that's I
this is this one is the one that's I
think the
closest the thing is this is devoting so
closest the thing is this is devoting so
this devotes half of the
this devotes half of the
space to the last
20% I think I'm fine with
that and then it devotes about to the
that and then it devotes about to the
last
10% 30% of this Spas to the last 10%
18% to the last
5% last 5% is one this is
fine and then a few perent to the last
fine and then a few perent to the last
so this is I think that this one is
so this is I think that this one is
probably this one is probably the best
probably this one is probably the best
one here this is
one here this is
0.1 uh the 0.01 is insane I think yeah
0.1 uh the 0.01 is insane I think yeah
this one is insane because this one here
if we try to get to
if we try to get to
one yeah one
two yeah this one devotes half of its
two yeah this one devotes half of its
space to the last
10% it could actually
that could actually be what we
that could actually be what we
want let me think about
that so the margin of error is going to
be 40 is going to be like a percent
be 40 is going to be like a percent
right how much space does this devote to
right how much space does this devote to
the 9 to 99% is my
the 9 to 99% is my
question so 99 is
Right 15% of the space is devoted to the
Right 15% of the space is devoted to the
last 1% of score
okay we have some options this makes
okay we have some options this makes
this uh this at least makes some
sense let's see if we can fix up the
sense let's see if we can fix up the
algorithm now with that so the first
algorithm now with that so the first
thing is I don't know why I didn't
thing is I don't know why I didn't
normalize the score coming in here
oneus
one H thanks for the offer but no I'm
one H thanks for the offer but no I'm
not going to accept that thank you
not going to accept that thank you
though that's
though that's
funny that's cool you do laser engraving
funny that's cool you do laser engraving
stuff
stuff
that very
cool puffer doesn't get any
cool puffer doesn't get any
uh puffer doesn't get any any swag until
uh puffer doesn't get any any swag until
we
we
uh once we cross a million in Revenue
uh once we cross a million in Revenue
then we can get some swag
maybe but early
maybe but early
stages very very early on
you know those startups that like they
you know those startups that like they
just like it's brand new they don't even
just like it's brand new they don't even
have anything and then they've got like
have anything and then they've got like
all sorts of
t-shirts I did uh I don't know I took a
t-shirts I did uh I don't know I took a
course at Stamford I did like I did a
course at Stamford I did like I did a
little bit of laser engraving stuff uh I
little bit of laser engraving stuff uh I
mean it was just on
mean it was just on
wood yeah it's
fun back then it was just like an Adobe
fun back then it was just like an Adobe
Illustrator file I think that you could
Illustrator file I think that you could
do yeah you could set the strength as
do yeah you could set the strength as
well so you could kind of etch it was
cool that was fun work I do sometimes
cool that was fun work I do sometimes
Miss like doing work in a shop being at
Miss like doing work in a shop being at
the keyboard all day gets boring even if
the keyboard all day gets boring even if
you are working on cool stuff a lot of
you are working on cool stuff a lot of
the
the
time but hey we're making good
time but hey we're making good
algorithms here
well I have this set to want minus
well I have this set to want minus
two okay then y
two okay then y
norm and you rorm like
this
form for
YT and then what score space do I change
YT and then what score space do I change
this
in
in
GPT
Norm yeah this is fine I think the rest
Norm yeah this is fine I think the rest
of this is fine we'll see if I have any
of this is fine we'll see if I have any
bugs with this but we will run this
we might now have to rescale some stuff
we might now have to rescale some stuff
I don't know how I missed this
I don't know how I missed this
before I thought I'd normalized the
input I don't know if it'll make a huge
input I don't know if it'll make a huge
difference here but it could definitely
difference here but it could definitely
make a big difference on
Palm for
this is pretty
funny everybody's trying to sell deep
funny everybody's trying to sell deep
research it's like yeah okay you give us
research it's like yeah okay you give us
a problem we'll deeply research it for
a problem we'll deeply research it for
you
now the question is going to be did we
now the question is going to be did we
completely Break
Stuff uh either we completely broke
Stuff uh either we completely broke
stuff or I'm just reporting
oh I'm reporting wrong let me
see what do we report we
report
report
score gpy
we'll see in a second I guess
yeah we're not taking B
yeah we're not taking B
see we just bootstrapped some
see we just bootstrapped some
stuff do contract work around RL
okay so this is very
okay so this is very
close almost identical I
close almost identical I
think so I don't think we broke anything
think so I don't think we broke anything
massive
predictions are definitely wrong here so
predictions are definitely wrong here so
we probably just didn't report this
correctly this
gpy okay so you take gpy T
Norm you linearize
oh other way
oh other way
around
nor should be
this
GPT
form for
then we'll try this with 1 eus 2 and
then we'll try this with 1 eus 2 and
we'll try this with 1 E minus
one hopefully this should be some
one hopefully this should be some
substantially
better
e
e
e
e e
yeah the score uh and predicted are
yeah the score uh and predicted are
still completely different so I still
still completely different so I still
have something
broken gpy
wait hang on Max
wait hang on Max
4 yeah the inverse is wrong hold on
GP get rid of thisiz
I think that this should be like
I think that this should be like
virtually identical
virtually identical
right
right
yeah there'll be a little bit of
yeah there'll be a little bit of
variation run to run
variation run to run
but this is pretty
good and now I think we fixed the score
good and now I think we fixed the score
prediction do we use this FR gpy
Norm no because you use gpy
gpy T Norm for the score
CL e
what are you
what are you
doing fine tuning this hyperparameter to
doing fine tuning this hyperparameter to
tuning algorithm I'm tuning the hyper
tuning algorithm I'm tuning the hyper
parameter tuning
parameter tuning
algorithm uh there were we ran a bunch
algorithm uh there were we ran a bunch
of experiments
of experiments
overnight uh this is probably
overnight uh this is probably
state-of-the-art this is very very good
state-of-the-art this is very very good
but there are a couple quirks with this
but there are a couple quirks with this
tuning
tuning
algorithm uh the big one at the moment
is hold on
so there're just a ton of points right
so there're just a ton of points right
here there's this huge just cluster of
here there's this huge just cluster of
points all the way over here so it
points all the way over here so it
should spread this data out a little bit
more and that will make the uh The Sweep
more and that will make the uh The Sweep
substantially faster to run
currently the thing that needs to be
currently the thing that needs to be
fixed
fixed
is actually I have a nice graph of
is actually I have a nice graph of
it there's a transformation that is
it there's a transformation that is
applied
applied
to um to the scores that the
to um to the scores that the
environments get and the one that I've
environments get and the one that I've
come up with can
come up with can
be it's kind of a family of
be it's kind of a family of
transforms parameterized by this Epsilon
transforms parameterized by this Epsilon
which is this number here at the
which is this number here at the
end and depending on how you set it
end and depending on how you set it
up it either like really encourages you
up it either like really encourages you
to push everything to the right hand
to push everything to the right hand
side to push everything to very high
side to push everything to very high
cost or it doesn't encourage you at all
cost or it doesn't encourage you at all
so we have to tune this parameter a
so we have to tune this parameter a
little bit and just get a feel for how
little bit and just get a feel for how
sensitive it is and stuff like that
sensitive it is and stuff like that
the unfortunate thing with Hyper
the unfortunate thing with Hyper
parameter algorithms that have hyper
parameter algorithms that have hyper
parameters is who Tunes the tuner
that's a funny little
qup who Tunes the tuner
okay this
ran deep learning exam and what course
ran deep learning exam and what course
is this
my favorite
my favorite
uh ML Class was Stanford
cs231n with Justin's lectures or Andre's
cs231n with Justin's lectures or Andre's
lectures those are both good okay it's
lectures those are both good okay it's
in Germany cool they have courses in
in Germany cool they have courses in
this stuff
now okay so this looks pretty decent
now okay so this looks pretty decent
here this is spoton with what we want
yeah this is pretty spoton with what we
yeah this is pretty spoton with what we
want let me make sure that this is oh
want let me make sure that this is oh
yeah no this is perfect and then the
yeah no this is perfect and then the
ratings
here are pretty consistent
we need to be a little careful with the
we need to be a little careful with the
suggestions per Paro
suggestions per Paro
right can we o
it we shouldn't be able to O it
right no it should be able to handle a
right no it should be able to handle a
very large batch
very large batch
actually I kind of want to do
this I think it'll make it just a little
this I think it'll make it just a little
bit more
bit more
consistent and
consistent and
then well that can't hurt it so I think
then well that can't hurt it so I think
we'll be fine leaving
we'll be fine leaving
that uh can it hurt it it might actually
that uh can it hurt it it might actually
be able to hurt it let me
be able to hurt it let me
think more samples technically means you
think more samples technically means you
have more chances to go off the rail so
have more chances to go off the rail so
we do have to rerun it for this so we'll
we do have to rerun it for this so we'll
give it a second
give it a second
and uh I think after this we just test
and uh I think after this we just test
out uh one or two of the other synthetic
out uh one or two of the other synthetic
problems and then we run the pong sweep
again we get C cumulative cost
will
will
tune yeah good
luck all right this looks
fine it's a little funny how it finds oh
fine it's a little funny how it finds oh
no cuz it's going to find higher rating
no cuz it's going to find higher rating
stuff for a little bit these aren't
stuff for a little bit these aren't
going to be consistent until it EXP the
going to be consistent until it EXP the
whole front so now it should start
whole front so now it should start
reducing
right yeah so this rating term should
decrease and we'll be able to compute
decrease and we'll be able to compute
all sorts of metrics over that
you could even have this thing
you could even have this thing
automatically terminated below a certain
automatically terminated below a certain
rating threshold because this is just an
rating threshold because this is just an
estimate of how much you how much value
estimate of how much you how much value
you expect to get from the experiment
right that's pretty
solid so I mean we'll see it's possible
solid so I mean we'll see it's possible
that this totally messes up the pong
sweep but hopefully not
and I think we can just run
this you can see it's like it's not
this you can see it's like it's not
expecting to get a ton from these these
expecting to get a ton from these these
runs now it's just like
runs now it's just like
a I mean that's a very nice estimate
a I mean that's a very nice estimate
that's a really really good metric to
that's a really really good metric to
have
have
holy a hyperparameter algorithm that
holy a hyperparameter algorithm that
tells you
tells you
how much value it has to keep running
it and actually you could complete
it and actually you could complete
compete this uh you could compute this
metric H
ideas I do have ideas
but that's a nice consistent
metric oh well okay if you're going to
metric oh well okay if you're going to
give me a perfect curve I'll take
give me a perfect curve I'll take
that thank you very
that thank you very
much and look at that actually this is
much and look at that actually this is
good it gives you in the elbow of the
good it gives you in the elbow of the
curve do you see how much coverage you
curve do you see how much coverage you
have
have
here you'd want coverage here in the
elbow uh I'm going to run this on one
elbow uh I'm going to run this on one
more synthetic task I think and then I'm
more synthetic task I think and then I'm
going
going
to H I think we can just call it and run
to H I think we can just call it and run
it on pong it's pretty solid as
is
is
on mod
no Cuda gpus are available
no Cuda gpus are available
really well that's
weird did my drivers crash something
somehow reboot the container
random driver crash is
random driver crash is
weird I'm going to have to go get the
weird I'm going to have to go get the
uh
token and we shall see how this does
there we
go
okay um there are still a few things
okay um there are still a few things
that we want to experiment with but this
that we want to experiment with but this
this should be enough to start running a
this should be enough to start running a
bunch of experiments I mean a bunch of
bunch of experiments I mean a bunch of
uh a bunch of runs cuz I think anything
uh a bunch of runs cuz I think anything
else we're going to do with this we'll
else we're going to do with this we'll
probably want to run far more
experiments so we will let this go and
experiments so we will let this go and
um we will check back in in a little bit
um we will check back in in a little bit
to see how this
to see how this
goes you see we have a few folks on
goes you see we have a few folks on
Twitch at the moment So for anybody
Twitch at the moment So for anybody
interested in following my work this is
interested in following my work this is
live tons of R
Lev buffer. it's all free and open
Lev buffer. it's all free and open
source you can check it out right
source you can check it out right
here St the repo to help us out can join
here St the repo to help us out can join
the Discord if you want to get involved
the Discord if you want to get involved
can follow me on X for a lot more
can follow me on X for a lot more
information on reinforcement learning
information on reinforcement learning
and what we are
doing we also have a Blog with lots of
doing we also have a Blog with lots of
information some of the latest
information some of the latest
developments in RL but also quick start
developments in RL but also quick start
guides for beginners so thank you and I
guides for beginners so thank you and I
will probably be back later in the
will probably be back later in the
afternoon working on this some more
