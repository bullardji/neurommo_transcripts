Kind: captions
Language: en
hi how's it
hi how's it
going it's
going it's
Sunday so why are we
Sunday so why are we
streaming why are we
streaming why are we
live
well is
well is
wife we got some cool results
and it looks like it looks like this one
and it looks like it looks like this one
might have crashed after 88 experiments
might have crashed after 88 experiments
or something we'll go look at
or something we'll go look at
that net size
though oh
though oh
man 38 seconds best solve that is
man 38 seconds best solve that is
crazy and this is before
crazy and this is before
muan I don't even know how good this
muan I don't even know how good this
could possibly
could possibly
be when we
be when we
uh when you really get the whole thing
working I think this is the really good
working I think this is the really good
run here yeah 38 second solve with
Adam 256 M * 2 it's 512 total
Adam 256 M * 2 it's 512 total
M where's the step for
M where's the step for
second must be fast cuz it did 100K
second must be fast cuz it did 100K
steps wait
steps wait
what uh something might be weird
what uh something might be weird
here
here
K steps per
K steps per
second did I read this wrong
hang on how is the cost wait the cost is
hang on how is the cost wait the cost is
way higher wa
26 oh 2 24750 okay
26 oh 2 24750 okay
good
24750 I was getting concerned there that
24750 I was getting concerned there that
I had something
wrong okay this
one 38 seconds so this is 64 million
steps 1.3 million steps per
steps 1.3 million steps per
second that's
second that's
cool oh and 20 48 total Ms 1024 two full
cool oh and 20 48 total Ms 1024 two full
day
day
sync okay so there were a few things I
sync okay so there were a few things I
was testing with this yo how big a deal
was testing with this yo how big a deal
is normalizing OBS enough to prevent
is normalizing OBS enough to prevent
full solving M's uh enough to prevent
full solving M's uh enough to prevent
training from working at all if they're
training from working at all if they're
very very badly normalized yes they
very very badly normalized yes they
don't need to be like mean zero standard
don't need to be like mean zero standard
deviation one or anything but generally
deviation one or anything but generally
you need to not have gigantic numbers in
you need to not have gigantic numbers in
there man these are we have some amazing
there man these are we have some amazing
results right now by the way we have
results right now by the way we have
some amazing results okay and
some amazing results okay and
interestingly the hidden size didn't
interestingly the hidden size didn't
change I actually want to plot that real
change I actually want to plot that real
quick
quick
what's new this is a 38 second breakout
what's new this is a 38 second breakout
solve right here and this one that
solve right here and this one that
didn't even run a full sweep is a 34
didn't even run a full sweep is a 34
second breakout
solve I was hard at work
yesterday very very hard at work
yesterday very very hard at work
yesterday I'm kind of happy that this
yesterday I'm kind of happy that this
worked out really well because yesterday
worked out really well because yesterday
kind of sucked I mean I was just full on
kind of sucked I mean I was just full on
grinding out all this these new features
we're
we're
modernizing the whole RL stack a little
modernizing the whole RL stack a little
bit
bit
here and I don't mean that in terms of
here and I don't mean that in terms of
just adding a bunch of dumb layers I
just adding a bunch of dumb layers I
mean like basic nuts and bolts
mean like basic nuts and bolts
algorithmic things that were used in
algorithmic things that were used in
llms we now
llms we now
have so the thing I want to do with this
have so the thing I want to do with this
one
here parito
sample scatter
plot this is going to be
plot this is going to be
model hidden size
so this is now sweeping model
so this is now sweeping model
size excluding normalizing OBS I think I
size excluding normalizing OBS I think I
got GPU Drive fully training on One
got GPU Drive fully training on One
Core 500ks SPS 750k with
Core 500ks SPS 750k with
multiprocessing uh okay so I completely
multiprocessing uh okay so I completely
broke in Dev I completely broke
broke in Dev I completely broke
multiprocessing indexing I fixed that
multiprocessing indexing I fixed that
yesterday lot of progress last couple
yesterday lot of progress last couple
weeks
weeks
absolutely I've been working I've been
absolutely I've been working I've been
hard at work here
hard at work here
and it looks like it's paying
off so I think we need to do cost here
off so I think we need to do cost here
so cost less than let's do like 45
seconds it's going to be 30 seconds flat
seconds it's going to be 30 seconds flat
by the time I get done with it I'm sure
hang on this is doing the dumb thing
hang on this is doing the dumb thing
where it's like not letting me apply a
where it's like not letting me apply a
filter I want to see what the uh the net
filter I want to see what the uh the net
size has to do with
size has to do with
it reconnecting with old
it reconnecting with old
friends very
friends very
good
good
H so this is just going to be a quick
H so this is just going to be a quick
stream today looking at these results
stream today looking at these results
and you know fixing anything that we
and you know fixing anything that we
need uh that needs fixing
and then after that I'm going to the
gym I'm trying I'm going to be buying
gym I'm trying I'm going to be buying
some equipment soon and I want to figure
some equipment soon and I want to figure
out what to
out what to
buy ah so here you have it right
buy ah so here you have it right
the the fast runs
the the fast runs
here don't do as well with the smaller
here don't do as well with the smaller
hidden size and it didn't really go to
hidden size and it didn't really go to
the to the larger ones either so
the to the larger ones either so
actually we kind of just got lucky with
actually we kind of just got lucky with
128 where 128 ended up being the
best so we are sweeping hidden size now
best so we are sweeping hidden size now
but it doesn't really
but it doesn't really
matter so then it's kind of cool then to
matter so then it's kind of cool then to
know the
know the
um that all of these results they didn't
um that all of these results they didn't
even come from sweeping Network size
even come from sweeping Network size
they just came from the other changes uh
they just came from the other changes uh
that we made
here I want to know what's wrong with
here I want to know what's wrong with
this
this
muon setup like why is it why is it
muon setup like why is it why is it
doing stuff way out
here this must just be like some drift
here this must just be like some drift
or
something I think what we should
something I think what we should
do 2750 so this is our
do 2750 so this is our
best uh atom result right
there this is Adam
there this is Adam
now let's get our best muon
result where do we
result where do we
go these are actually pretty well the
same 24605 will do
but yeah this is kind of why I tend to
but yeah this is kind of why I tend to
just like disappear for uh chunks of
just like disappear for uh chunks of
time you know when I'm like I'll be
time you know when I'm like I'll be
working on a project and then you won't
working on a project and then you won't
hear anything on it why it's cuz I'm
hear anything on it why it's cuz I'm
doing stuff like this Pion when I get a
doing stuff like this Pion when I get a
run that solves with full OBS and
run that solves with full OBS and
collision on awesome code got a little
collision on awesome code got a little
big but at least it's fast yeah I was
big but at least it's fast yeah I was
afraid of that I will review when you
afraid of that I will review when you
have it hopefully I review before it
have it hopefully I review before it
gets to be gigantic
gets to be gigantic
right at least there's no box 2D
right at least there's no box 2D
dependency or anything at the moment so
dependency or anything at the moment so
self-contained the impulse Wars one was
self-contained the impulse Wars one was
like really hard to review cuz all of
like really hard to review cuz all of
Captain's code was good it was like
Captain's code was good it was like
pretty well good I didn't have any
pretty well good I didn't have any
complaints but I had tons of complaints
complaints but I had tons of complaints
about box
about box
2D 1,200 with render okay it's not bad
2D 1,200 with render okay it's not bad
that's not bad I can deal with
that's not bad I can deal with
that especially if that includes map
stuff on the right we have
stuff on the right we have
muon and on the left we have
muon and on the left we have
Adam we'll start
with with the
curves on one
40 seconds was 46 seconds 30 oh yeah so
40 seconds was 46 seconds 30 oh yeah so
this is 37
this is 37
seconds for the
seconds for the
solve and then this one is 37 seconds
solve and then this one is 37 seconds
for the solve so these are very very
similar check metad data
similar check metad data
here this one is with muon is actually
here this one is with muon is actually
more sample efficient this one's 45
more sample efficient this one's 45
million
steps they both use 1024
steps they both use 1024
M so that seems nice and
stable what on Earth happened to the
stable what on Earth happened to the
learning
learning
rates oh never mind that's after a
rates oh never mind that's after a
kneeling yeah that's that's fine that's
kneeling yeah that's that's fine that's
after
after
ailing policy hidden size 128
ailing policy hidden size 128
policy hidden size
policy hidden size
128 uh I guess it's just the train arcs
128 uh I guess it's just the train arcs
then and I want to see if there any
then and I want to see if there any
interesting differences oh wait uh the
interesting differences oh wait uh the
steps per second so this
steps per second so this
is
is
uh 8 860,000 SPS the other one is 1.3
uh 8 860,000 SPS the other one is 1.3
mil so probably there's going to be like
mil so probably there's going to be like
an update Epoch difference or
something okay so
something okay so
Adam
Adam
betas
betas
9894 96
9894 96
99 quite
99 quite
different atom epsilon's also different
different atom epsilon's also different
so these are two different algorithms
so these are two different algorithms
this is not entirely
this is not entirely
unexpected we get the exact same back
size none of this stuff is
size none of this stuff is
on Entry entropy very different this is
on Entry entropy very different this is
actually max out
entropy Lambda and
entropy Lambda and
Gamma also quite
Gamma also quite
different learning rate these have both
different learning rate these have both
maxed their learning
maxed their learning
rate so uh we can potentially do better
rate so uh we can potentially do better
by increasing the max learning rate
here Max grad Norm of two
here Max grad Norm of two
1.7 so this one maxed out its Max grad
1.7 so this one maxed out its Max grad
Norm as
Norm as
well uh so here's going to be the perf
well uh so here's going to be the perf
difference this one has 8192 mini
difference this one has 8192 mini
batches this one has
batches this one has
4096 update epox is
4096 update epox is
One update epox is one so they just have
One update epox is one so they just have
different mini bat size and that's where
different mini bat size and that's where
all the perf difference is going to come
from cosine schedulers that's
from cosine schedulers that's
hardcoded value function coefficient
hardcoded value function coefficient
one and
one and
0.8 but coefficient is not
0.8 but coefficient is not
swept
swept
cool yeah so this is
cool yeah so this is
insane
insane
um the only thing I can think to do here
um the only thing I can think to do here
is to now
is to now
rerun with the bigger ranges on
rerun with the bigger ranges on
stuff I think that's what we're going to
stuff I think that's what we're going to
do we
rerun do we rerun both of them with the
rerun do we rerun both of them with the
bigger
ranges probably we should rerun both of
ranges probably we should rerun both of
them with bigger ranges we don't really
them with bigger ranges we don't really
need to sweep net size since there's
need to sweep net size since there's
nothing there loose
binding let's just set that up I think
binding let's just set that up I think
that's all I wanted to do today is just
that's all I wanted to do today is just
to set this up and get these running and
to set this up and get these running and
uh then we will be able to have you know
uh then we will be able to have you know
some uh some cool results
okay so we're not sweeping
not so we PID size cuz it screws up mu
not so we PID size cuz it screws up mu
on it screws up the timing of Mo
on actually I wouldn't be surprised that
on actually I wouldn't be surprised that
that's what the really long run was was
that's what the really long run was was
building
building
kernels probably got stuck doing
kernels probably got stuck doing
that PPT
that PPT
[Music]
[Music]
Horizon I think we should just go down
Horizon I think we should just go down
the list and see if uh any of these are
the list and see if uh any of these are
suspiciously close to their Maxes now Ms
suspiciously close to their Maxes now Ms
is
is
good Min is 5 E7 here wait a second 5
E7 50 mil
right how
um this is 45 mil
45 how does this
happen total
happen total
[Music]
[Music]
train E7 7 is 10 ms yeah 50
train E7 7 is 10 ms yeah 50
mil 100 Mil default
45 uh I'm a little
confused sweep. train
well I don't know what's going on
here because it shouldn't be able to do
this oh I'm dumb
this oh I'm dumb
yeah there we go it's uh the breakout
yeah there we go it's uh the breakout
config it overrides this okay we're good
config it overrides this okay we're good
27 is uh the max 20 mil nothing's come
27 is uh the max 20 mil nothing's come
close so solving in that time there's no
close so solving in that time there's no
limitation
there this pram is good batch size Min
there this pram is good batch size Min
is 32k Max is
262k that is the power of two right I'll
262k that is the power of two right I'll
make sure it didn't give me a wrong
make sure it didn't give me a wrong
Power of Two
19 yeah okay that got it right
19 yeah okay that got it right
cool and uh I think we actually should
cool and uh I think we actually should
increase the default now because this
increase the default now because this
one doesn't seem to be used we'll do
one doesn't seem to be used we'll do
that
one
one
Auto mini batch size Min is this Max is
Auto mini batch size Min is this Max is
this it always falls in the middle
this it always falls in the middle
that's fine
um I mean we're getting higher learning
um I mean we're getting higher learning
rates
now so we'll try this even though this
now so we'll try this even though this
seems ridiculous to
me I think we had Max entropy on this
me I think we had Max entropy on this
didn't
we it Max the entropy out
yeah
yeah
01 good
01 good
default and then
default and then
gamma 994
L does
73 just tweak that a little
bit update
EPO the value function coefficient up a
EPO the value function coefficient up a
little
little
bit we do five Max
BPT
BPT
Horizon uh this is hitting the max now
right so we'll do like this
and then we check out the Adam
and then we check out the Adam
pams
0.94 to
0.94 to
0.99999 is it 999
0.99999 is it 999
yeah this is good and then this one is a
yeah this is good and then this one is a
little
little
higher so good and then Epsilon
higher so good and then Epsilon
o O2
o O2
is fine I think we're happy with all of
this we're happy with all of this
now
this is the Adam
mission for
okay so this is just a rerun with um
okay so this is just a rerun with um
expanded Max hyper pram
ranges because it's pressing up against
ranges because it's pressing up against
the max learning rate which is kind of
the max learning rate which is kind of
cool that it's able to optimize with the
cool that it's able to optimize with the
learning rate that high I've never seen
that uh which box am I on
that uh which box am I on
for I think this is the Box
yeah this is
it so just commenting
it so just commenting
this no big deal
good
down so what happened to
muan just random core
dump weird
not so happy with
that it's kind of encouraging to see
that it's kind of encouraging to see
um these sweeps just like that's better
um these sweeps just like that's better
performance than like the original
performance than like the original
optimized perams
holy well not expanded perfect
holy well not expanded perfect
hello
welcome I mean that's the experiment
welcome I mean that's the experiment
setup I guess the one other thing uh we
setup I guess the one other thing uh we
could do now
could do now
is just the atom one or the muan one
is just the atom one or the muan one
on muan okay let's just grab these muan
on muan okay let's just grab these muan
param
param
and uh yeah let's try these out on
breakout is it
breakout is it
crazy crazy results
262 BPT Horizon
32 entropy is 05
and a
Nutty
Nutty
Lambda
Lambda
Gamma then learning rates
maxed Max grad Norm is
maxed Max grad Norm is
two mini batch size
4096 oh this is 32 million
4096 oh this is 32 million
holy that's like
holy that's like
nothing so we'll just
nothing so we'll just
do
do
three
three
million give it a little bit of play say
million give it a little bit of play say
35 for the Repro cuz usually there's a
35 for the Repro cuz usually there's a
little bit of bias
and then value function coefficient is
1.0 anything
1.0 anything
else think that's
else think that's
it that's all the
parameters yeah let's try this
right and I have to run with
3.12
what ah
driver for
okay so this should give us a new run
I'm pretty sure this the uh the time
I'm pretty sure this the uh the time
includes like startup as well right now
820 it needs like another second or two
820 it needs like another second or two
doesn't
doesn't
it so let's uh let's assume that we had
it so let's uh let's assume that we had
just like a slightly slightly lucky run
just like a slightly slightly lucky run
because usually you know um there is a
because usually you know um there is a
bit of
bias so usually you add a little bit of
bias so usually you add a little bit of
time to
time to
training because if you think about it
training because if you think about it
right if there's any sort of play in the
right if there's any sort of play in the
amount of time that a run
amount of time that a run
takes uh the run that you're going to
takes uh the run that you're going to
see as the highest on your dashboard is
see as the highest on your dashboard is
going to be the one that got a little
going to be the one that got a little
bit
bit
lucky but assuming that your runs are
lucky but assuming that your runs are
stable you should just be able to add a
stable you should just be able to add a
little bit of extra time allocation and
little bit of extra time allocation and
then you should be able to reproduce the
then you should be able to reproduce the
result consistently just you know add a
result consistently just you know add a
fudge factor maybe 15% extra time
20% extra time something like
that that's like really fast and really
that that's like really fast and really
consistent though
holy look at that curve
holy look at that curve
and presumably the wobble in it is from
and presumably the wobble in it is from
the cosine
anding
38 so if I wanted to like show this off
38 so if I wanted to like show this off
I'd probably give it more of an
I'd probably give it more of an
allocation
allocation
budget so maybe you do have to add you
budget so maybe you do have to add you
know a Fair bit cuz it's just it's
know a Fair bit cuz it's just it's
working well it's just a little bit
working well it's just a little bit
below the perf level of
below the perf level of
um you know a little bit below the perf
um you know a little bit below the perf
level of the original
level of the original
run you can see it's going up at a very
run you can see it's going up at a very
nice Pace though it's it's stable it's
nice Pace though it's it's stable it's
just
just
um there a bit of
play that's kind of I'm not going to lie
play that's kind of I'm not going to lie
that's kind of just like magic looking
that's kind of just like magic looking
at how fast this thing
at how fast this thing
trains like ah this is this is
trains like ah this is this is
something to see RL just like work this
well I mean that feels that feels
well I mean that feels that feels
different from what we've seen
different from what we've seen
before Oh no are we going to get stuck
before Oh no are we going to get stuck
at the very end
though that's a bit unfortunate
and run it a few times just to see the
variance might be that the
uh cosine anding needs to be configured
uh cosine anding needs to be configured
a little bit differently
I for
I for
coign
kneeling Min is zero
there might be a little bit of a
there might be a little bit of a
difficulty of converging to the uh the
difficulty of converging to the uh the
very like the last optimal
very like the last optimal
point I mean that's up to
point I mean that's up to
840 864 is the max
these are just such Better Learning
these are just such Better Learning
curves than before as well that could
curves than before as well that could
just be a backat size artifact but still
just be a backat size artifact but still
even if it is I that's an advantage yeah
even if it is I that's an advantage yeah
there you go that's what you
there you go that's what you
want that's what you want so same
want that's what you want so same
thing just runs a little
thing just runs a little
differently 48 seconds or whatever if
differently 48 seconds or whatever if
you want to make sure it's solved 34 was
you want to make sure it's solved 34 was
the best
the best
solve those are
solve those are
like that's like not fair very bad as
like that's like not fair very bad as
far as variance
far as variance
goes
goes
perfect let's just make sure my uh my
perfect let's just make sure my uh my
new sweeps are working out
I
I
tag we've got
Adam we already we have some pretty good
Adam we already we have some pretty good
runs in
runs in
here Mo
here Mo
on let's just pick a better
on let's just pick a better
dashboard we got
this we got the atom
this we got the atom
one which is nowhere near as
one which is nowhere near as
good I guess we'll be able to really
good I guess we'll be able to really
compare these
compare these
two uh properly this new eval all right
two uh properly this new eval all right
um
um
I think that's all for the time being
I think that's all for the time being
I'm going to go get some other stuff
I'm going to go get some other stuff
done we've got these up and running uh
done we've got these up and running uh
my hope is that this is going to let us
my hope is that this is going to let us
start moving a lot of research onto more
start moving a lot of research onto more
complex Ms so you know next week maybe
complex Ms so you know next week maybe
by the week after I want to start doing
by the week after I want to start doing
runs on like MOBA on neural MMO right on
runs on like MOBA on neural MMO right on
the more complex M that we have on
the more complex M that we have on
impulse Force once that's merged because
impulse Force once that's merged because
right now we're kind of just using a lot
right now we're kind of just using a lot
of pong and breakout for quick tests but
of pong and breakout for quick tests but
I think we're going to be able to move
I think we're going to be able to move
past that with this snake's a quick one
past that with this snake's a quick one
it's like a little bit more interesting
it's like a little bit more interesting
for
for
multi-agent uh but really like these up
multi-agent uh but really like these up
here probably will have to finish puffer
here probably will have to finish puffer
tactics as well cuz this would be a nice
tactics as well cuz this would be a nice
test Induro is fun it's very easy to
test Induro is fun it's very easy to
solve quickly um it's a cool one to
solve quickly um it's a cool one to
Showcase and we'll see about some of
Showcase and we'll see about some of
these as well all right um for folks
these as well all right um for folks
watching if you're interested in RL if
watching if you're interested in RL if
you're interested in following this
you're interested in following this
stuffff . a start the GitHub to help us
stuffff . a start the GitHub to help us
out it's free really helps us out can
out it's free really helps us out can
join the Discord to get involved
join the Discord to get involved
discord.gg puffer you can follow me on X
discord.gg puffer you can follow me on X
for more reinforcement learning content
for more reinforcement learning content
I will be back streaming

Kind: captions
Language: en
hi how's it
hi how's it
going it's
going it's
Sunday so why are we
Sunday so why are we
streaming why are we
streaming why are we
live
well is
well is
wife we got some cool results
and it looks like it looks like this one
and it looks like it looks like this one
might have crashed after 88 experiments
might have crashed after 88 experiments
or something we'll go look at
or something we'll go look at
that net size
though oh
though oh
man 38 seconds best solve that is
man 38 seconds best solve that is
crazy and this is before
crazy and this is before
muan I don't even know how good this
muan I don't even know how good this
could possibly
could possibly
be when we
be when we
uh when you really get the whole thing
working I think this is the really good
working I think this is the really good
run here yeah 38 second solve with
Adam 256 M * 2 it's 512 total
Adam 256 M * 2 it's 512 total
M where's the step for
M where's the step for
second must be fast cuz it did 100K
second must be fast cuz it did 100K
steps wait
steps wait
what uh something might be weird
what uh something might be weird
here
here
K steps per
K steps per
second did I read this wrong
hang on how is the cost wait the cost is
hang on how is the cost wait the cost is
way higher wa
26 oh 2 24750 okay
26 oh 2 24750 okay
good
24750 I was getting concerned there that
24750 I was getting concerned there that
I had something
wrong okay this
one 38 seconds so this is 64 million
steps 1.3 million steps per
steps 1.3 million steps per
second that's
second that's
cool oh and 20 48 total Ms 1024 two full
cool oh and 20 48 total Ms 1024 two full
day
day
sync okay so there were a few things I
sync okay so there were a few things I
was testing with this yo how big a deal
was testing with this yo how big a deal
is normalizing OBS enough to prevent
is normalizing OBS enough to prevent
full solving M's uh enough to prevent
full solving M's uh enough to prevent
training from working at all if they're
training from working at all if they're
very very badly normalized yes they
very very badly normalized yes they
don't need to be like mean zero standard
don't need to be like mean zero standard
deviation one or anything but generally
deviation one or anything but generally
you need to not have gigantic numbers in
you need to not have gigantic numbers in
there man these are we have some amazing
there man these are we have some amazing
results right now by the way we have
results right now by the way we have
some amazing results okay and
some amazing results okay and
interestingly the hidden size didn't
interestingly the hidden size didn't
change I actually want to plot that real
change I actually want to plot that real
quick
quick
what's new this is a 38 second breakout
what's new this is a 38 second breakout
solve right here and this one that
solve right here and this one that
didn't even run a full sweep is a 34
didn't even run a full sweep is a 34
second breakout
solve I was hard at work
yesterday very very hard at work
yesterday very very hard at work
yesterday I'm kind of happy that this
yesterday I'm kind of happy that this
worked out really well because yesterday
worked out really well because yesterday
kind of sucked I mean I was just full on
kind of sucked I mean I was just full on
grinding out all this these new features
we're
we're
modernizing the whole RL stack a little
modernizing the whole RL stack a little
bit
bit
here and I don't mean that in terms of
here and I don't mean that in terms of
just adding a bunch of dumb layers I
just adding a bunch of dumb layers I
mean like basic nuts and bolts
mean like basic nuts and bolts
algorithmic things that were used in
algorithmic things that were used in
llms we now
llms we now
have so the thing I want to do with this
have so the thing I want to do with this
one
here parito
sample scatter
plot this is going to be
plot this is going to be
model hidden size
so this is now sweeping model
so this is now sweeping model
size excluding normalizing OBS I think I
size excluding normalizing OBS I think I
got GPU Drive fully training on One
got GPU Drive fully training on One
Core 500ks SPS 750k with
Core 500ks SPS 750k with
multiprocessing uh okay so I completely
multiprocessing uh okay so I completely
broke in Dev I completely broke
broke in Dev I completely broke
multiprocessing indexing I fixed that
multiprocessing indexing I fixed that
yesterday lot of progress last couple
yesterday lot of progress last couple
weeks
weeks
absolutely I've been working I've been
absolutely I've been working I've been
hard at work here
hard at work here
and it looks like it's paying
off so I think we need to do cost here
off so I think we need to do cost here
so cost less than let's do like 45
seconds it's going to be 30 seconds flat
seconds it's going to be 30 seconds flat
by the time I get done with it I'm sure
hang on this is doing the dumb thing
hang on this is doing the dumb thing
where it's like not letting me apply a
where it's like not letting me apply a
filter I want to see what the uh the net
filter I want to see what the uh the net
size has to do with
size has to do with
it reconnecting with old
it reconnecting with old
friends very
friends very
good
good
H so this is just going to be a quick
H so this is just going to be a quick
stream today looking at these results
stream today looking at these results
and you know fixing anything that we
and you know fixing anything that we
need uh that needs fixing
and then after that I'm going to the
gym I'm trying I'm going to be buying
gym I'm trying I'm going to be buying
some equipment soon and I want to figure
some equipment soon and I want to figure
out what to
out what to
buy ah so here you have it right
buy ah so here you have it right
the the fast runs
the the fast runs
here don't do as well with the smaller
here don't do as well with the smaller
hidden size and it didn't really go to
hidden size and it didn't really go to
the to the larger ones either so
the to the larger ones either so
actually we kind of just got lucky with
actually we kind of just got lucky with
128 where 128 ended up being the
best so we are sweeping hidden size now
best so we are sweeping hidden size now
but it doesn't really
but it doesn't really
matter so then it's kind of cool then to
matter so then it's kind of cool then to
know the
know the
um that all of these results they didn't
um that all of these results they didn't
even come from sweeping Network size
even come from sweeping Network size
they just came from the other changes uh
they just came from the other changes uh
that we made
here I want to know what's wrong with
here I want to know what's wrong with
this
this
muon setup like why is it why is it
muon setup like why is it why is it
doing stuff way out
here this must just be like some drift
here this must just be like some drift
or
something I think what we should
something I think what we should
do 2750 so this is our
do 2750 so this is our
best uh atom result right
there this is Adam
there this is Adam
now let's get our best muon
result where do we
result where do we
go these are actually pretty well the
same 24605 will do
but yeah this is kind of why I tend to
but yeah this is kind of why I tend to
just like disappear for uh chunks of
just like disappear for uh chunks of
time you know when I'm like I'll be
time you know when I'm like I'll be
working on a project and then you won't
working on a project and then you won't
hear anything on it why it's cuz I'm
hear anything on it why it's cuz I'm
doing stuff like this Pion when I get a
doing stuff like this Pion when I get a
run that solves with full OBS and
run that solves with full OBS and
collision on awesome code got a little
collision on awesome code got a little
big but at least it's fast yeah I was
big but at least it's fast yeah I was
afraid of that I will review when you
afraid of that I will review when you
have it hopefully I review before it
have it hopefully I review before it
gets to be gigantic
gets to be gigantic
right at least there's no box 2D
right at least there's no box 2D
dependency or anything at the moment so
dependency or anything at the moment so
self-contained the impulse Wars one was
self-contained the impulse Wars one was
like really hard to review cuz all of
like really hard to review cuz all of
Captain's code was good it was like
Captain's code was good it was like
pretty well good I didn't have any
pretty well good I didn't have any
complaints but I had tons of complaints
complaints but I had tons of complaints
about box
about box
2D 1,200 with render okay it's not bad
2D 1,200 with render okay it's not bad
that's not bad I can deal with
that's not bad I can deal with
that especially if that includes map
stuff on the right we have
stuff on the right we have
muon and on the left we have
muon and on the left we have
Adam we'll start
with with the
curves on one
40 seconds was 46 seconds 30 oh yeah so
40 seconds was 46 seconds 30 oh yeah so
this is 37
this is 37
seconds for the
seconds for the
solve and then this one is 37 seconds
solve and then this one is 37 seconds
for the solve so these are very very
similar check metad data
similar check metad data
here this one is with muon is actually
here this one is with muon is actually
more sample efficient this one's 45
more sample efficient this one's 45
million
steps they both use 1024
steps they both use 1024
M so that seems nice and
stable what on Earth happened to the
stable what on Earth happened to the
learning
learning
rates oh never mind that's after a
rates oh never mind that's after a
kneeling yeah that's that's fine that's
kneeling yeah that's that's fine that's
after
after
ailing policy hidden size 128
ailing policy hidden size 128
policy hidden size
policy hidden size
128 uh I guess it's just the train arcs
128 uh I guess it's just the train arcs
then and I want to see if there any
then and I want to see if there any
interesting differences oh wait uh the
interesting differences oh wait uh the
steps per second so this
steps per second so this
is
is
uh 8 860,000 SPS the other one is 1.3
uh 8 860,000 SPS the other one is 1.3
mil so probably there's going to be like
mil so probably there's going to be like
an update Epoch difference or
something okay so
something okay so
Adam
Adam
betas
betas
9894 96
9894 96
99 quite
99 quite
different atom epsilon's also different
different atom epsilon's also different
so these are two different algorithms
so these are two different algorithms
this is not entirely
this is not entirely
unexpected we get the exact same back
size none of this stuff is
size none of this stuff is
on Entry entropy very different this is
on Entry entropy very different this is
actually max out
entropy Lambda and
entropy Lambda and
Gamma also quite
Gamma also quite
different learning rate these have both
different learning rate these have both
maxed their learning
maxed their learning
rate so uh we can potentially do better
rate so uh we can potentially do better
by increasing the max learning rate
here Max grad Norm of two
here Max grad Norm of two
1.7 so this one maxed out its Max grad
1.7 so this one maxed out its Max grad
Norm as
Norm as
well uh so here's going to be the perf
well uh so here's going to be the perf
difference this one has 8192 mini
difference this one has 8192 mini
batches this one has
batches this one has
4096 update epox is
4096 update epox is
One update epox is one so they just have
One update epox is one so they just have
different mini bat size and that's where
different mini bat size and that's where
all the perf difference is going to come
from cosine schedulers that's
from cosine schedulers that's
hardcoded value function coefficient
hardcoded value function coefficient
one and
one and
0.8 but coefficient is not
0.8 but coefficient is not
swept
swept
cool yeah so this is
cool yeah so this is
insane
insane
um the only thing I can think to do here
um the only thing I can think to do here
is to now
is to now
rerun with the bigger ranges on
rerun with the bigger ranges on
stuff I think that's what we're going to
stuff I think that's what we're going to
do we
rerun do we rerun both of them with the
rerun do we rerun both of them with the
bigger
ranges probably we should rerun both of
ranges probably we should rerun both of
them with bigger ranges we don't really
them with bigger ranges we don't really
need to sweep net size since there's
need to sweep net size since there's
nothing there loose
binding let's just set that up I think
binding let's just set that up I think
that's all I wanted to do today is just
that's all I wanted to do today is just
to set this up and get these running and
to set this up and get these running and
uh then we will be able to have you know
uh then we will be able to have you know
some uh some cool results
okay so we're not sweeping
not so we PID size cuz it screws up mu
not so we PID size cuz it screws up mu
on it screws up the timing of Mo
on actually I wouldn't be surprised that
on actually I wouldn't be surprised that
that's what the really long run was was
that's what the really long run was was
building
building
kernels probably got stuck doing
kernels probably got stuck doing
that PPT
that PPT
[Music]
[Music]
Horizon I think we should just go down
Horizon I think we should just go down
the list and see if uh any of these are
the list and see if uh any of these are
suspiciously close to their Maxes now Ms
suspiciously close to their Maxes now Ms
is
is
good Min is 5 E7 here wait a second 5
E7 50 mil
right how
um this is 45 mil
45 how does this
happen total
happen total
[Music]
[Music]
train E7 7 is 10 ms yeah 50
train E7 7 is 10 ms yeah 50
mil 100 Mil default
45 uh I'm a little
confused sweep. train
well I don't know what's going on
here because it shouldn't be able to do
this oh I'm dumb
this oh I'm dumb
yeah there we go it's uh the breakout
yeah there we go it's uh the breakout
config it overrides this okay we're good
config it overrides this okay we're good
27 is uh the max 20 mil nothing's come
27 is uh the max 20 mil nothing's come
close so solving in that time there's no
close so solving in that time there's no
limitation
there this pram is good batch size Min
there this pram is good batch size Min
is 32k Max is
262k that is the power of two right I'll
262k that is the power of two right I'll
make sure it didn't give me a wrong
make sure it didn't give me a wrong
Power of Two
19 yeah okay that got it right
19 yeah okay that got it right
cool and uh I think we actually should
cool and uh I think we actually should
increase the default now because this
increase the default now because this
one doesn't seem to be used we'll do
one doesn't seem to be used we'll do
that
one
one
Auto mini batch size Min is this Max is
Auto mini batch size Min is this Max is
this it always falls in the middle
this it always falls in the middle
that's fine
um I mean we're getting higher learning
um I mean we're getting higher learning
rates
now so we'll try this even though this
now so we'll try this even though this
seems ridiculous to
me I think we had Max entropy on this
me I think we had Max entropy on this
didn't
we it Max the entropy out
yeah
yeah
01 good
01 good
default and then
default and then
gamma 994
L does
73 just tweak that a little
bit update
EPO the value function coefficient up a
EPO the value function coefficient up a
little
little
bit we do five Max
BPT
BPT
Horizon uh this is hitting the max now
right so we'll do like this
and then we check out the Adam
and then we check out the Adam
pams
0.94 to
0.94 to
0.99999 is it 999
0.99999 is it 999
yeah this is good and then this one is a
yeah this is good and then this one is a
little
little
higher so good and then Epsilon
higher so good and then Epsilon
o O2
o O2
is fine I think we're happy with all of
this we're happy with all of this
now
this is the Adam
mission for
okay so this is just a rerun with um
okay so this is just a rerun with um
expanded Max hyper pram
ranges because it's pressing up against
ranges because it's pressing up against
the max learning rate which is kind of
the max learning rate which is kind of
cool that it's able to optimize with the
cool that it's able to optimize with the
learning rate that high I've never seen
that uh which box am I on
that uh which box am I on
for I think this is the Box
yeah this is
it so just commenting
it so just commenting
this no big deal
good
down so what happened to
muan just random core
dump weird
not so happy with
that it's kind of encouraging to see
that it's kind of encouraging to see
um these sweeps just like that's better
um these sweeps just like that's better
performance than like the original
performance than like the original
optimized perams
holy well not expanded perfect
holy well not expanded perfect
hello
welcome I mean that's the experiment
welcome I mean that's the experiment
setup I guess the one other thing uh we
setup I guess the one other thing uh we
could do now
could do now
is just the atom one or the muan one
is just the atom one or the muan one
on muan okay let's just grab these muan
on muan okay let's just grab these muan
param
param
and uh yeah let's try these out on
breakout is it
breakout is it
crazy crazy results
262 BPT Horizon
32 entropy is 05
and a
Nutty
Nutty
Lambda
Lambda
Gamma then learning rates
maxed Max grad Norm is
maxed Max grad Norm is
two mini batch size
4096 oh this is 32 million
4096 oh this is 32 million
holy that's like
holy that's like
nothing so we'll just
nothing so we'll just
do
do
three
three
million give it a little bit of play say
million give it a little bit of play say
35 for the Repro cuz usually there's a
35 for the Repro cuz usually there's a
little bit of bias
and then value function coefficient is
1.0 anything
1.0 anything
else think that's
else think that's
it that's all the
parameters yeah let's try this
right and I have to run with
3.12
what ah
driver for
okay so this should give us a new run
I'm pretty sure this the uh the time
I'm pretty sure this the uh the time
includes like startup as well right now
820 it needs like another second or two
820 it needs like another second or two
doesn't
doesn't
it so let's uh let's assume that we had
it so let's uh let's assume that we had
just like a slightly slightly lucky run
just like a slightly slightly lucky run
because usually you know um there is a
because usually you know um there is a
bit of
bias so usually you add a little bit of
bias so usually you add a little bit of
time to
time to
training because if you think about it
training because if you think about it
right if there's any sort of play in the
right if there's any sort of play in the
amount of time that a run
amount of time that a run
takes uh the run that you're going to
takes uh the run that you're going to
see as the highest on your dashboard is
see as the highest on your dashboard is
going to be the one that got a little
going to be the one that got a little
bit
bit
lucky but assuming that your runs are
lucky but assuming that your runs are
stable you should just be able to add a
stable you should just be able to add a
little bit of extra time allocation and
little bit of extra time allocation and
then you should be able to reproduce the
then you should be able to reproduce the
result consistently just you know add a
result consistently just you know add a
fudge factor maybe 15% extra time
20% extra time something like
that that's like really fast and really
that that's like really fast and really
consistent though
holy look at that curve
holy look at that curve
and presumably the wobble in it is from
and presumably the wobble in it is from
the cosine
anding
38 so if I wanted to like show this off
38 so if I wanted to like show this off
I'd probably give it more of an
I'd probably give it more of an
allocation
allocation
budget so maybe you do have to add you
budget so maybe you do have to add you
know a Fair bit cuz it's just it's
know a Fair bit cuz it's just it's
working well it's just a little bit
working well it's just a little bit
below the perf level of
below the perf level of
um you know a little bit below the perf
um you know a little bit below the perf
level of the original
level of the original
run you can see it's going up at a very
run you can see it's going up at a very
nice Pace though it's it's stable it's
nice Pace though it's it's stable it's
just
just
um there a bit of
play that's kind of I'm not going to lie
play that's kind of I'm not going to lie
that's kind of just like magic looking
that's kind of just like magic looking
at how fast this thing
at how fast this thing
trains like ah this is this is
trains like ah this is this is
something to see RL just like work this
well I mean that feels that feels
well I mean that feels that feels
different from what we've seen
different from what we've seen
before Oh no are we going to get stuck
before Oh no are we going to get stuck
at the very end
though that's a bit unfortunate
and run it a few times just to see the
variance might be that the
uh cosine anding needs to be configured
uh cosine anding needs to be configured
a little bit differently
I for
I for
coign
kneeling Min is zero
there might be a little bit of a
there might be a little bit of a
difficulty of converging to the uh the
difficulty of converging to the uh the
very like the last optimal
very like the last optimal
point I mean that's up to
point I mean that's up to
840 864 is the max
these are just such Better Learning
these are just such Better Learning
curves than before as well that could
curves than before as well that could
just be a backat size artifact but still
just be a backat size artifact but still
even if it is I that's an advantage yeah
even if it is I that's an advantage yeah
there you go that's what you
there you go that's what you
want that's what you want so same
want that's what you want so same
thing just runs a little
thing just runs a little
differently 48 seconds or whatever if
differently 48 seconds or whatever if
you want to make sure it's solved 34 was
you want to make sure it's solved 34 was
the best
the best
solve those are
solve those are
like that's like not fair very bad as
like that's like not fair very bad as
far as variance
far as variance
goes
goes
perfect let's just make sure my uh my
perfect let's just make sure my uh my
new sweeps are working out
I
I
tag we've got
Adam we already we have some pretty good
Adam we already we have some pretty good
runs in
runs in
here Mo
here Mo
on let's just pick a better
on let's just pick a better
dashboard we got
this we got the atom
this we got the atom
one which is nowhere near as
one which is nowhere near as
good I guess we'll be able to really
good I guess we'll be able to really
compare these
compare these
two uh properly this new eval all right
two uh properly this new eval all right
um
um
I think that's all for the time being
I think that's all for the time being
I'm going to go get some other stuff
I'm going to go get some other stuff
done we've got these up and running uh
done we've got these up and running uh
my hope is that this is going to let us
my hope is that this is going to let us
start moving a lot of research onto more
start moving a lot of research onto more
complex Ms so you know next week maybe
complex Ms so you know next week maybe
by the week after I want to start doing
by the week after I want to start doing
runs on like MOBA on neural MMO right on
runs on like MOBA on neural MMO right on
the more complex M that we have on
the more complex M that we have on
impulse Force once that's merged because
impulse Force once that's merged because
right now we're kind of just using a lot
right now we're kind of just using a lot
of pong and breakout for quick tests but
of pong and breakout for quick tests but
I think we're going to be able to move
I think we're going to be able to move
past that with this snake's a quick one
past that with this snake's a quick one
it's like a little bit more interesting
it's like a little bit more interesting
for
for
multi-agent uh but really like these up
multi-agent uh but really like these up
here probably will have to finish puffer
here probably will have to finish puffer
tactics as well cuz this would be a nice
tactics as well cuz this would be a nice
test Induro is fun it's very easy to
test Induro is fun it's very easy to
solve quickly um it's a cool one to
solve quickly um it's a cool one to
Showcase and we'll see about some of
Showcase and we'll see about some of
these as well all right um for folks
these as well all right um for folks
watching if you're interested in RL if
watching if you're interested in RL if
you're interested in following this
you're interested in following this
stuffff . a start the GitHub to help us
stuffff . a start the GitHub to help us
out it's free really helps us out can
out it's free really helps us out can
join the Discord to get involved
join the Discord to get involved
discord.gg puffer you can follow me on X
discord.gg puffer you can follow me on X
for more reinforcement learning content
for more reinforcement learning content
I will be back streaming
