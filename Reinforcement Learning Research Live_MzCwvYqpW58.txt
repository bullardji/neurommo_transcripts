Kind: captions
Language: en
Okay,
Okay,
you're back live. Hello.
Greetings take a lot out of me for some
Greetings take a lot out of me for some
reason. I don't know.
reason. I don't know.
Internal engineers problem, but at least
Internal engineers problem, but at least
I'm just writing today.
All right, let's get uh let's get
All right, let's get uh let's get
started on this.
We got all this done. Let me just go
We got all this done. Let me just go
warming up. Let's just go get all the
warming up. Let's just go get all the
links.
Okay.
Where's their paper?
Crazy
that they um
that they um
that like you still these publications
that like you still these publications
still have like this restricted access
It's a very well named paper.
They not have the PDF.
Oh, here it is.
Hey Barath, welcome.
I am currently working on the uh the
I am currently working on the uh the
reinforcement learning
reinforcement learning
compendium of sorts.
This is all material designed to help
This is all material designed to help
new people to the field.
new people to the field.
I don't do this type of stuff super
I don't do this type of stuff super
often, but it's been heavily requested.
often, but it's been heavily requested.
So,
Did I get 10? One, two, three, four,
10.
Welcome, potato.
Welcome, potato.
2.3k words is crazy. This is half of
2.3k words is crazy. This is half of
what I've written so far for you guys.
what I've written so far for you guys.
Uh there's also
Uh there's also
for my articles
for my articles
there's the general intro guide which is
there's the general intro guide which is
another 2k words.
I don't want it to be excessively long.
I don't want it to be excessively long.
I want it to mostly be a reference
I want it to mostly be a reference
to other things
but it's going to be quite complete.
People have kind of like
the way I tell people to learn things.
the way I tell people to learn things.
The thing is like
The thing is like
I'm pretty dang independent with the way
I'm pretty dang independent with the way
that I approach new problems. And like
that I approach new problems. And like
if you tell me exactly how to do
if you tell me exactly how to do
something, I'm probably not going to do
something, I'm probably not going to do
it that way anyways.
it that way anyways.
Um, and that works really well for some
Um, and that works really well for some
people, but like a lot of people have
people, but like a lot of people have
just been asking me for basically a step
just been asking me for basically a step
by step, like what do I do to learn RL?
by step, like what do I do to learn RL?
So that's what I'm trying to do here.
So that's what I'm trying to do here.
Thanks for the effort. You got it, man.
Thanks for the effort. You got it, man.
The support has been great lately.
The support has been great lately.
Today was awesome. Like,
Today was awesome. Like,
look at this. We got plus 600 followers
look at this. We got plus 600 followers
today. We're going to hit 10K soon.
10K is a huge account in reinforcement
10K is a huge account in reinforcement
learning. That's like
learning. That's like
been a longtime goal of mine.
like most of the people that um have
like most of the people that um have
accounts that size in RL, they also post
accounts that size in RL, they also post
politics or memes or some other things.
politics or memes or some other things.
It's actually like pretty hard to get to
It's actually like pretty hard to get to
a sizable account when you're just
a sizable account when you're just
posting deeply technical content.
If you notice, right,
like pretty much everyone else I can
like pretty much everyone else I can
think of in RL who has a larger account
think of in RL who has a larger account
um posts other crap.
I allow myself like one fitness post
I allow myself like one fitness post
once in a while because I've actually
once in a while because I've actually
gotten a couple of people to like start
gotten a couple of people to like start
actually getting in shape, which is very
actually getting in shape, which is very
important, but it's very rare.
Okay, this is a good reference.
started streaming too after seeing
started streaming too after seeing
yours.
yours.
Welcome.
I've had fun with it.
Hi, Joseph. Watch your live stream every
Hi, Joseph. Watch your live stream every
day when you were working with Syon
day when you were working with Syon
every day a few months ago and then I
every day a few months ago and then I
got busy with stuff. That look like it
got busy with stuff. That look like it
would be good at least to have a
would be good at least to have a
summarization each of your live videos
summarization each of your live videos
maybe in the description.
maybe in the description.
Oh man, it's like it's kind of tough to
Oh man, it's like it's kind of tough to
do that is the thing because you see
do that is the thing because you see
this on YouTube, right? But it also goes
this on YouTube, right? But it also goes
to Twitch and X. I guess technically the
to Twitch and X. I guess technically the
main VODs are on just YouTube. Yeah,
main VODs are on just YouTube. Yeah,
there isn't really a uh
there isn't really a uh
a topic is the thing. Like most of the
a topic is the thing. Like most of the
streams don't have a coherent topic for
streams don't have a coherent topic for
the most part, the most spot. Um,
the most part, the most spot. Um,
trying to think if there's a way that I
trying to think if there's a way that I
can do that easily.
I'm actually kind of surprised that uh
I'm actually kind of surprised that uh
YouTube hasn't added a thing for that
YouTube hasn't added a thing for that
cuz they already get transcriptions,
cuz they already get transcriptions,
don't they?
Trying to think what to do with that.
I mean, do you have any idea on how to
I mean, do you have any idea on how to
actually do that other than like
it feels like something that you should
it feels like something that you should
be able to do automatically?
Not your videos.
treasure.
treasure.
Oh, wait. Was about to build a tool with
Oh, wait. Was about to build a tool with
whisper. I missed one message. Make use
whisper. I missed one message. Make use
of an element summarized. Put
of an element summarized. Put
descriptions.
Is there a way to do that through
Is there a way to do that through
YouTube?
We have over 1k subs.
Is there a way to do that?
This is not it.
This is not it.
I missed several messages. Yeah, hang
I missed several messages. Yeah, hang
on. Let me scroll back through.
It's kind of hard to set up like an LLM
It's kind of hard to set up like an LLM
to do that thing locally because it's a
to do that thing locally because it's a
lot of data
API to do to get the captions and do
API to do to get the captions and do
something with them.
Do they have like a summary thing?
No, it's all third party crap
because like if I have to like go
because like if I have to like go
download a six-hour VOD and then like
download a six-hour VOD and then like
feed a six-hour VOD through a local
feed a six-hour VOD through a local
model, right?
I mean, if there is a third party thing
I mean, if there is a third party thing
that I can just like add that will do it
that I can just like add that will do it
and not make a mess, I'll do it.
We've made a lot of progress since the
We've made a lot of progress since the
days of uh MOA and Syon.
days of uh MOA and Syon.
It's uh
It's uh
it's literally a different field. Like
it's literally a different field. Like
RL is a different field from that which
RL is a different field from that which
was I think I was doing that last
was I think I was doing that last
summer.
Don't feed the video.
Don't feed the video.
Just the audio.
Just the audio.
Oh yeah, obviously.
Is there an API that you can download?
Is there an API that you can download?
Can you just get the audio like that? Is
Can you just get the audio like that? Is
that a thing you can do?
Cuz if you have a thing that I can set
Cuz if you have a thing that I can set
up that I can do that easily. Sure.
Right. If I can just like have a model,
Right. If I can just like have a model,
if even if I have to set up a container
if even if I have to set up a container
somewhere, if I just set it up on one of
somewhere, if I just set it up on one of
these machines and I just like, you
these machines and I just like, you
know, paste a link to the video or
know, paste a link to the video or
whatever
whatever
and it'll like just generate me a
and it'll like just generate me a
summary, that would be
Is that a question for me or Barath?
That'd be a nice thing to have. I'll
That'd be a nice thing to have. I'll
build one just for buffer.
build one just for buffer.
Can turn on a live caption button. Let
Can turn on a live caption button. Let
me see.
me see.
Let me see.
Uh, this is the wrong tab. Hang on.
So
Yeah.
I actually have blocked links off
and it still blocks them. Anyways,
there's no chat moderation.
Oh, this is their like that's not it.
Oh, this is their like that's not it.
Yeah. I don't know if you have any idea
Yeah. I don't know if you have any idea
where this thing is or if there is
where this thing is or if there is
actually a feature. I'll gladly like set
actually a feature. I'll gladly like set
it up.
I'd also love to know how to set YouTube
I'd also love to know how to set YouTube
to low latency by default. There doesn't
to low latency by default. There doesn't
seem to be a way to do it in their UI.
It's like you have to do it when you
It's like you have to do it when you
start the stream, but if you start the
start the stream, but if you start the
stream through reream, it doesn't work.
I feel like I was forgetting a
Oh, are there papers I can suggest
Oh, are there papers I can suggest
people on applications?
People always ask me about this anyways.
People always ask me about this anyways.
So, it would be good to just have these
So, it would be good to just have these
links.
Let me just go grab all of the um
all of the ones that I know.
I think of other ones. Yes.
Borax
later in Jax.
later in Jax.
Yeah, that's cool. I have to I haven't
Yeah, that's cool. I have to I haven't
looked at like the details of those
looked at like the details of those
specific apps. See if that makes sense
specific apps. See if that makes sense
as an approach or not.
There are a bunch of other ones as well.
There are a bunch of other ones as well.
I'm trying to think if there are any
I'm trying to think if there are any
that are worth highlighting.
They don't have like an article though.
Yeah, it's YouTube.
Yeah, it's YouTube.
Okay.
Okay.
We'll do Yes.
What did Alpha Star use to train?
Wasn't it some crazy thing? It's the
Wasn't it some crazy thing? It's the
league. It's the league play thing. But
league. It's the league play thing. But
didn't they also have um
Oh yeah, they also bootstrapped it with
Oh yeah, they also bootstrapped it with
imitation learning.
imitation learning.
I forgot about that.
What was the actual algorithm? Does
What was the actual algorithm? Does
anybody remember?
anybody remember?
Learning algorithm.
replayed experience. So, it's off
replayed experience. So, it's off
policy. Yeah.
Hello him.
Welcome. Oh jeez, it's already 400 p.m.
Welcome. Oh jeez, it's already 400 p.m.
How the heck did that happen?
Like I've been lazy.
messages, meetings, and writing.
They really have got to fix XDMs. Like,
They really have got to fix XDMs. Like,
they you should not be able to be
they you should not be able to be
getting added by this many spam bots.
India. You're hardworking. It's Yeah,
India. You're hardworking. It's Yeah,
this is pretty lazy, I'd say, for me.
this is pretty lazy, I'd say, for me.
We're not putting in crazy hours at the
We're not putting in crazy hours at the
moment.
welcome Rolo.
Oh, Mark Towers is on here.
What other off policy papers do I give?
What other off policy papers do I give?
Not what's the other one?
Not what's the other one?
Atari 200 times faster.
What other off policy papers?
What other off policy papers?
Impala.
Impala.
Maybe I impala
flies. This
fly.
There's the original world model paper.
Yeah, this one.
He
What are some good starting projects?
What are some good starting projects?
What would be an impressive project?
What would be an impressive project?
Those are two different things and it
Those are two different things and it
depends on your background. A good
depends on your background. A good
starter project
starter project
uh if you have a decent programming
uh if you have a decent programming
background is something like a simple
background is something like a simple
arcade game, something that you can do
arcade game, something that you can do
within like a few days.
within like a few days.
You can see all the projects on
You can see all the projects on
puffer.ai. Many of these are made by
puffer.ai. Many of these are made by
contributors.
contributors.
Uh some of these end up being very
Uh some of these end up being very
useful in research where you would not
useful in research where you would not
expect like 2048 actually ends up being
expect like 2048 actually ends up being
pretty good for research.
pretty good for research.
Tetris questionable whether it's as
Tetris questionable whether it's as
useful, but it's just freaking awesome
useful, but it's just freaking awesome
to just watch. It's like really
to just watch. It's like really
mesmerizing and cool to see.
mesmerizing and cool to see.
We've got like application stuff.
We've got like application stuff.
Applications are always awesome things
Applications are always awesome things
on uh different problems in industry
on uh different problems in industry
like this drone sim.
So generally newcomers to RL pick
So generally newcomers to RL pick
something you can do in a few days. It
something you can do in a few days. It
could be applied. It could just be a
could be applied. It could just be a
game or whatever. And then what is an
game or whatever. And then what is an
impressive project in RL? It's something
impressive project in RL? It's something
that is going to pose a new type of
that is going to pose a new type of
challenge that allows us to test our
challenge that allows us to test our
algorithms in a different way or
algorithms in a different way or
something that unlocks a new application
something that unlocks a new application
area for us. Those are the two things
area for us. Those are the two things
that are most impressive.
that are most impressive.
BQN and PO work on a lot of domains.
BQN and PO work on a lot of domains.
What would the difference between my
What would the difference between my
work be other than the end? So, first of
work be other than the end? So, first of
all, DQN doesn't really work on anything
all, DQN doesn't really work on anything
uh unless you mean like the follow-up
uh unless you mean like the follow-up
variance of it. PO does work generally.
variance of it. PO does work generally.
It's just very fiddly. Uh we're not
It's just very fiddly. Uh we're not
actually doing either of those
actually doing either of those
technically. We have a follow-up to PO
technically. We have a follow-up to PO
uh that is substantially substantially
uh that is substantially substantially
more stable, a heck of a lot faster as
more stable, a heck of a lot faster as
well. So, that's what we use for our
well. So, that's what we use for our
training. We also do our own
training. We also do our own
hyperparameter sweep work as well. What
hyperparameter sweep work as well. What
would my works be other than the M? So
would my works be other than the M? So
an environment can be a research
an environment can be a research
contribute, right? If an environment
contribute, right? If an environment
allows us to study an algorithm in a way
allows us to study an algorithm in a way
that we have not been able to before uh
that we have not been able to before uh
or if it allow unlocks a new applied
or if it allow unlocks a new applied
area that is a major contribution to
area that is a major contribution to
give you a very quick idea of like one
give you a very quick idea of like one
of the environments we used super
of the environments we used super
heavily in development of uh 3.0. I
heavily in development of uh 3.0. I
know.
Okay. So, we have this grid environment
Okay. So, we have this grid environment
here and this can generate grids of like
here and this can generate grids of like
whatever size from this up to you know
whatever size from this up to you know
the size of this full container.
the size of this full container.
So they can be big, they can be small.
So they can be big, they can be small.
And if you look at our release docs, we
And if you look at our release docs, we
have agents that solve this thing
have agents that solve this thing
relatively well just using the 01 signal
relatively well just using the 01 signal
of getting to the goal. So it's a very
of getting to the goal. So it's a very
very absurdly hard exploration task. Uh
very absurdly hard exploration task. Uh
and this served as a very reliable
and this served as a very reliable
benchmark for us. And uh pretty much
benchmark for us. And uh pretty much
training on this environment was a very
training on this environment was a very
good indicator like doing better on this
good indicator like doing better on this
environment was a very very strong
environment was a very very strong
indicator of doing better on everything.
indicator of doing better on everything.
So this is like one example of a targetd
So this is like one example of a targetd
designed environment for research
designed environment for research
applications are also useful like um you
applications are also useful like um you
know the drone environment was one of
know the drone environment was one of
the first super fast continuous
the first super fast continuous
environments that we had added. So
environments that we had added. So
continuous action spaces plus super fast
continuous action spaces plus super fast
and allowed us to make uh to get some of
and allowed us to make uh to get some of
the kinks out of continuous training.
the kinks out of continuous training.
2048's kind of interesting because it
2048's kind of interesting because it
just it takes a pretty dang long time to
just it takes a pretty dang long time to
train more than you would expect for
train more than you would expect for
something this simple. looks like a
something this simple. looks like a
non-trivial benchmark.
non-trivial benchmark.
There are lots of different things here.
If you want to contribute to the
If you want to contribute to the
research side, you can also do that.
Do people create environments to test
Do people create environments to test
agents against each other? Sort of like
agents against each other? Sort of like
Kaggle. Uh yes, they do. There have been
Kaggle. Uh yes, they do. There have been
competitions like this. Uh I have ran I
competitions like this. Uh I have ran I
have run several of these at
have run several of these at
conferences. I have colleagues who have
conferences. I have colleagues who have
run uh run these at conferences. Uh it
run uh run these at conferences. Uh it
is an incredible mindnumbing around
is an incredible mindnumbing around
amount of work and it is not fun at all.
amount of work and it is not fun at all.
So I don't do those anymore.
So I don't do those anymore.
Uh it is absolutely something that can
Uh it is absolutely something that can
be done in principle and knowing what I
be done in principle and knowing what I
know now, I could probably do it a lot
know now, I could probably do it a lot
easier than I did it back then.
But let's just say it was a traumatic
But let's just say it was a traumatic
amount of work and was kind of just
amount of work and was kind of just
awful for everyone involved.
Why 28 2048 took a long time. No idea.
Why 28 2048 took a long time. No idea.
To be fair, I got the train time from I
To be fair, I got the train time from I
got it from six uh six billion steps
got it from six uh six billion steps
down to 600 million steps with a very
down to 600 million steps with a very
good hyperprem sweep, but 600 million is
good hyperprem sweep, but 600 million is
still quite a lot of experience for
still quite a lot of experience for
something like that environment.
It's going to be a useful benchmark
It's going to be a useful benchmark
environment for us though because of
environment for us though because of
this, right?
this, right?
Like that's actually going to make it a
Like that's actually going to make it a
useful benchmark.
Even expect works there
in max with probabilities.
in max with probabilities.
Well, yeah, you can do that, right? Like
Well, yeah, you can do that, right? Like
if you write down any environment as an
if you write down any environment as an
actual tabular game, right? versus
actual tabular game, right? versus
having to learn
having to learn
everything from representations. It's
everything from representations. It's
going to be a lot easier.
The tabular algorithms break immediately
The tabular algorithms break immediately
when you can't do that though, right?
Like sure technically you can do you can
Like sure technically you can do you can
say the same thing about mazes, right?
say the same thing about mazes, right?
Oh, I can just implement breath for
Oh, I can just implement breath for
search or depth for search.
M AlphaGo things work better than their
M AlphaGo things work better than their
MCTS with learned
MCTS with learned
MCTS with learned functions.
The Alph Go is
The Alph Go is
you make the problem easier when you
you make the problem easier when you
have the ability to set specific states,
have the ability to set specific states,
right?
So yes, that should make it easier.
So yes, that should make it easier.
Um whether it's easier with something
Um whether it's easier with something
like mu0 where you have to learn the
like mu0 where you have to learn the
dynamics as well, that is less certain.
dynamics as well, that is less certain.
I think in general if we just like if
I think in general if we just like if
you look at the topics I'm writing down
you look at the topics I'm writing down
here
here
it's pretty much just a matter of like
it's pretty much just a matter of like
going through everything that DeepMind
going through everything that DeepMind
has done over the last seven years like
has done over the last seven years like
ripping out all the fancy math that is
ripping out all the fancy math that is
there for the sake of them wanting to do
there for the sake of them wanting to do
fancy math and like figuring out what
fancy math and like figuring out what
are the generalizable components that uh
are the generalizable components that uh
can be used to improve not just like
can be used to improve not just like
this crazy all we care about is sample
this crazy all we care about is sample
efficiency thing but like balance sample
efficiency thing but like balance sample
efficiency and compute in a reasonable
efficiency and compute in a reasonable
sane manner. Ideally with like a tunable
sane manner. Ideally with like a tunable
trade-off, then we'd be good.
nice teaching. Thank you. I guess you
nice teaching. Thank you. I guess you
could always bootstrap your RL learner
could always bootstrap your RL learner
with the search approach. Just speed up
with the search approach. Just speed up
the initial few steps.
the initial few steps.
It depends on what you're trying to do.
It depends on what you're trying to do.
Are you trying to make it train faster
Are you trying to make it train faster
in wall clock or in steps?
in wall clock or in steps?
Because our thing is ridiculously fast
Because our thing is ridiculously fast
in wall clock right now. It's possible
in wall clock right now. It's possible
we can still make it faster.
we can still make it faster.
Heck, I think we can probably double the
Heck, I think we can probably double the
speed just with even more infrastructure
speed just with even more infrastructure
optimizations.
optimizations.
Um,
Um,
yeah, wall clock we kind of steamroll
yeah, wall clock we kind of steamroll
everything already.
Like we can train at 3 million steps per
Like we can train at 3 million steps per
second. So
second. So
everything's just fast and puffer.
everything's just fast and puffer.
We did um we did 640 billion steps of
We did um we did 640 billion steps of
neural MMO 3, which is a pabyte of
neural MMO 3, which is a pabyte of
training data in 3 days and 8 hours on
training data in 3 days and 8 hours on
six GPUs.
So our wall clock is very very good.
So our wall clock is very very good.
We're basically what makes it so much
We're basically what makes it so much
faster CUDA kernels. No, we really only
faster CUDA kernels. No, we really only
have one custom CUDA kernel which is the
have one custom CUDA kernel which is the
advantage function which is only a we
advantage function which is only a we
only had to do that because our thing is
only had to do that because our thing is
so fast. It's that we have environments
so fast. It's that we have environments
that are fast themselves. So the ends
that are fast themselves. So the ends
are all in C and they have our own
are all in C and they have our own
infrastructure that doesn't slow them
infrastructure that doesn't slow them
down. So we can do tens of gigabytes a
down. So we can do tens of gigabytes a
second no problem uh from the
second no problem uh from the
environment side. And then we got like a
environment side. And then we got like a
really big batch training to work and we
really big batch training to work and we
cut out a lot of the overhead that
cut out a lot of the overhead that
people don't typically optimize because
people don't typically optimize because
their trainers are too slow for it to
their trainers are too slow for it to
matter anyways.
CPUbound uh everything in RL is
CPUbound uh everything in RL is
typically CPUbound on the simulator side
typically CPUbound on the simulator side
and a lot of the GPU sims where they're
and a lot of the GPU sims where they're
like, "Oh, it's super fast." They're not
like, "Oh, it's super fast." They're not
actually that fast.
actually that fast.
It's usually environment bound. A lot of
It's usually environment bound. A lot of
stuff in reinforcement learning is
stuff in reinforcement learning is
environment bound
or like it may as well be environment
or like it may as well be environment
bound because it'll be like GPU bound
bound because it'll be like GPU bound
but only because you're using batch size
but only because you're using batch size
eight with tiny models you're like you
eight with tiny models you're like you
know if you're using the GPU at 2% util
know if you're using the GPU at 2% util
like 2% efficiency right it may as well
like 2% efficiency right it may as well
be environment bound because you're
be environment bound because you're
doing dumb things
doing dumb things
is RL easier to distribute because all
is RL easier to distribute because all
you care about is rollouts
Uh, I would think that RL is kind of
Uh, I would think that RL is kind of
harder to distribute than everything
harder to distribute than everything
else in general until you get into like
else in general until you get into like
I guess the only thing that's harder to
I guess the only thing that's harder to
distribute than RL is the really big
distribute than RL is the really big
models where you have to do like several
models where you have to do like several
dimension parallelism.
But no, RL's really hard to distribute
But no, RL's really hard to distribute
because you have a lot of data and you
because you have a lot of data and you
have to synchronize very frequently. We
have to synchronize very frequently. We
have it working though.
You don't do the rollouts on CPU. You
You don't do the rollouts on CPU. You
don't do the policy inference pass on
don't do the policy inference pass on
CPU. That's the Impala architecture and
CPU. That's the Impala architecture and
it's a shitty architecture.
What if you share
What if you share
some of the states? You train several
some of the states? You train several
similar rollouts together like we
similar rollouts together like we
branch. Nobody does epsilon greedy
branch. Nobody does epsilon greedy
things anymore and that wouldn't matter
things anymore and that wouldn't matter
anyways
anyways
here. Okay. So our default training
here. Okay. So our default training
setup works like this. This is like the
setup works like this. This is like the
average training setup that we have. We
average training setup that we have. We
take eight CPU cores.
take eight CPU cores.
On each CPU core, we simulate a thousand
On each CPU core, we simulate a thousand
copies of the environment or if it's a
copies of the environment or if it's a
multi- aent environment, however many
multi- aent environment, however many
agents you need to get to a total of a
agents you need to get to a total of a
thousand agents. So they're like 8192
thousand agents. So they're like 8192
total agents. And then in every single
total agents. And then in every single
batch of data, every single rollout
batch of data, every single rollout
batch, we get 4096 agents worth of data.
batch, we get 4096 agents worth of data.
And while half of the CPUs are
And while half of the CPUs are
simulating data, you're stepping the
simulating data, you're stepping the
other half of them. So the CPUs are
other half of them. So the CPUs are
always going in the background uh during
always going in the background uh during
the rollout process. Then you collect
the rollout process. Then you collect
segments from all of those environments.
segments from all of those environments.
So if you're going to collect 64 steps
So if you're going to collect 64 steps
from each environment, that is a total
from each environment, that is a total
batch size of uh 500,000ish.
batch size of uh 500,000ish.
And then we go over that 500,000 steps
And then we go over that 500,000 steps
of experience uh with PO plus our
of experience uh with PO plus our
modifications using a mini batch size of
modifications using a mini batch size of
somewhere between 16 and 32,000.
somewhere between 16 and 32,000.
And we usually just do one pass of it.
And we usually just do one pass of it.
That whole process uh collects us and
That whole process uh collects us and
trains on well for breakout it's like
trains on well for breakout it's like
three uh it's like three million steps
three uh it's like three million steps
per second. Most of the fast
per second. Most of the fast
environments are three million steps per
environments are three million steps per
second. We have some larger policies,
second. We have some larger policies,
harder environments that we can get over
harder environments that we can get over
1 million. And then the uh the really
1 million. And then the uh the really
tough environments with like bigger
tough environments with like bigger
policies are going to be between 300 and
policies are going to be between 300 and
700,000
700,000
per GPU. And then multiGPU like neural
per GPU. And then multiGPU like neural
MMO 3, which is probably the hardest, we
MMO 3, which is probably the hardest, we
get like 400,000 steps per second on one
get like 400,000 steps per second on one
GPU. We get up to like 2.2 million steps
GPU. We get up to like 2.2 million steps
per second on six GPU.
Have people tried mixture of experts
Have people tried mixture of experts
type things? Why would you do that?
The policies are tiny. There's not
The policies are tiny. There's not
really any reason to make like a larger
really any reason to make like a larger
number of tiny networks just
number of tiny networks just
computationally inefficient.
computationally inefficient.
Like the main problem with uh efficiency
Like the main problem with uh efficiency
right in RL is that the like really
right in RL is that the like really
small policies are hard to saturate
small policies are hard to saturate
because you get data transfer bound
bound. No, it's not that it's CPU bound.
bound. No, it's not that it's CPU bound.
Okay. It's that so really small networks
Okay. It's that so really small networks
are hard to saturate because
are hard to saturate because
you're like you're spending more time
you're like you're spending more time
moving data around than you are actually
moving data around than you are actually
multiplying matrices because the
multiplying matrices because the
matrices are not that big.
like it's actually it's pretty hard to
like it's actually it's pretty hard to
get near optimal utilization
get near optimal utilization
um or linear scaling let's say below
um or linear scaling let's say below
like
like
mid uh mid singledigit million
mid uh mid singledigit million
parameters.
parameters.
No, not because the batch size is tiny.
No, not because the batch size is tiny.
Even if you give it a massive batch
Even if you give it a massive batch
size, you still get bound. You still get
size, you still get bound. You still get
like data transfer bound.
Like you can do the math on what we
Like you can do the math on what we
should be able to be pushing here,
should be able to be pushing here,
right? Because like a 128 hidden dim
right? Because like a 128 hidden dim
LSTM should be 64 times faster than a uh
LSTM should be 64 times faster than a uh
I believe it should be 64 times faster
I believe it should be 64 times faster
than a,024 hidden dim LSTM. We can train
than a,024 hidden dim LSTM. We can train
a,024 hidden dim at a million steps per
a,024 hidden dim at a million steps per
second. So based on that, we should be
second. So based on that, we should be
able to hit like 60s something million
able to hit like 60s something million
steps per second uh with a tiny LSTM.
steps per second uh with a tiny LSTM.
You just never get that. I think that
You just never get that. I think that
the most we've ever seen uh we got
the most we've ever seen uh we got
something to hit like 15 million steps
something to hit like 15 million steps
per second with gigantic batch sizes.
per second with gigantic batch sizes.
You just get data transfer bound
or not even necessarily data transfer
or not even necessarily data transfer
bound like it's not even just CPU to
bound like it's not even just CPU to
GPU. It can just be moving stuff into
GPU. It can just be moving stuff into
them like caches and spinning up
them like caches and spinning up
kernels. You get overhead bound by all
kernels. You get overhead bound by all
sorts of types of overhead.
Well, we got this section done.
Well, we got this section done.
I got to write the infrastructure
I got to write the infrastructure
section and probably one more. Let me
section and probably one more. Let me
use a restroom real quick and I will be
use a restroom real quick and I will be
right back. Also, can I just say it's
right back. Also, can I just say it's
pretty dang nice the uh the growth
pretty dang nice the uh the growth
today. Like
today. Like
this is a ridiculous.
this is a ridiculous.
plus like 700 followers is awesome. All
plus like 700 followers is awesome. All
right, I'll be right back.
Did you see the mega kernel for llama?
Did you see the mega kernel for llama?
They only have one kernel for the entire
They only have one kernel for the entire
llama. So less overhead. Yeah. So that
llama. So less overhead. Yeah. So that
I've been thinking of that for
I've been thinking of that for
reinforcement learning and uh it would
reinforcement learning and uh it would
be massively more important for RL than
be massively more important for RL than
for everything else. It's you'd have to
for everything else. It's you'd have to
build something to gen the kernel though
build something to gen the kernel though
because like you need different types of
because like you need different types of
networks
networks
that would actually probably let you hit
that would actually probably let you hit
like if somebody wanted to do that in
like if somebody wanted to do that in
puffer lib I would be all about it
puffer lib I would be all about it
because we could probably hit 50 million
because we could probably hit 50 million
steps per second training. It would be
steps per second training. It would be
insane.
It actually wouldn't be anywhere near as
It actually wouldn't be anywhere near as
hard, I don't think, either because like
hard, I don't think, either because like
the kernels are pretty simple.
the kernels are pretty simple.
We actually we have stuff that would
We actually we have stuff that would
basically work as kernels. They're just
basically work as kernels. They're just
they're kind of shitty. You'd have to do
they're kind of shitty. You'd have to do
like you'd have to actually get better
like you'd have to actually get better
maples. Probably just matt moles.
maples. Probably just matt moles.
Everything else is pretty easy. But like
Everything else is pretty easy. But like
I wrote this. Let me show you.
We actually used this on the website. So
We actually used this on the website. So
if you wondered how we got neural nets
if you wondered how we got neural nets
to run on your in your browser, it's
to run on your in your browser, it's
this file. It's a tiny little file.
this file. It's a tiny little file.
Twothirds of it is just wrappers. So
Twothirds of it is just wrappers. So
like it's easier for a UI, but like we
like it's easier for a UI, but like we
have a linear layer. We've got a conf 3D
have a linear layer. We've got a conf 3D
con. We've got an LSTM. got layer norm
con. We've got an LSTM. got layer norm
embeddings one hot we've got like all
embeddings one hot we've got like all
the common operations in just brain
the common operations in just brain
deadad simple C
like you could technically like what we
like you could technically like what we
do now is we wire these all together and
do now is we wire these all together and
that's your network right for your
that's your network right for your
forward pass you could do this add a
forward pass you could do this add a
backwards convert it to CUDA make a less
backwards convert it to CUDA make a less
stupid matmo uh matmo that's actually
stupid matmo uh matmo that's actually
tiled correctly and all that and Like
tiled correctly and all that and Like
maybe we get 50 million steps per
maybe we get 50 million steps per
second. I don't know.
I don't know what that is.
If you have an idea on how to do this,
If you have an idea on how to do this,
by all means, like 50 million step per
by all means, like 50 million step per
second training would be crazy cool.
second training would be crazy cool.
thoughtless.
If you have an idea on how to do this,
If you have an idea on how to do this,
like awesome.
Why specifically in RL is important
Why specifically in RL is important
because the models are smaller and when
because the models are smaller and when
the models are smaller the overhead
the models are smaller the overhead
matters more because they run faster. So
matters more because they run faster. So
smaller chunks of overhead matter more.
That's why
Most of the stuff is bandwidthbound.
It's not specifically device bandwidth
It's not specifically device bandwidth
though that is sometimes a factor.
though that is sometimes a factor.
I think it's more so that you're just
I think it's more so that you're just
like
I to be fair I don't really know. It
I to be fair I don't really know. It
could be kernel launch overhead. It
could be kernel launch overhead. It
could technically be just like it it
could technically be just like it it
shouldn't be GPU GPU bandwidth. Like it
shouldn't be GPU GPU bandwidth. Like it
shouldn't be memory moving around in the
shouldn't be memory moving around in the
GPU because that's really fast. But for
GPU because that's really fast. But for
all I know, it's just done incredibly
all I know, it's just done incredibly
inefficiently right now.
Uh replay doesn't matter because so all
Uh replay doesn't matter because so all
of our data gets like we don't take the
of our data gets like we don't take the
data off the GPU anyways. Once it goes
data off the GPU anyways. Once it goes
on the GPU, it gets it stays on the GPU.
Like the entire PO batch just fits on
Like the entire PO batch just fits on
the GPU and we don't come close to
the GPU and we don't come close to
running out of memory ever. It's just
running out of memory ever. It's just
fine.
Why do you think LM people are using
Why do you think LM people are using
chain of thought
chain of thought
and not any of the classic deep RL
and not any of the classic deep RL
approaches? They are doing RL now.
Oh, LLM people are straight up doing RL
Oh, LLM people are straight up doing RL
now.
now.
Have you seen Ken Stanley's newest paper
Have you seen Ken Stanley's newest paper
on fractured entanglement entangled
on fractured entanglement entangled
representations? So I have a friend and
representations? So I have a friend and
a colleague at MIT who worked on that
a colleague at MIT who worked on that
paper with him. Uh he does good work. I
paper with him. Uh he does good work. I
haven't had a chance to read the
haven't had a chance to read the
fractured entangled representations
fractured entangled representations
paper yet though so I cannot comment
paper yet though so I cannot comment
yet.
URL but it's completely different policy
URL but it's completely different policy
network.
network.
They use
They use
a lot of to
actually they use no data at all
actually they use no data at all
compared to uh compared to what we do in
compared to uh compared to what we do in
RL. Like we do training runs 20 times
RL. Like we do training runs 20 times
the pre 20 times the data size of the
the pre 20 times the data size of the
pre-training of GPT4 and we do it on one
pre-training of GPT4 and we do it on one
server.
server.
So by our by RL standards they have no
So by our by RL standards they have no
data.
references. Oh, that's the test time the
references. Oh, that's the test time the
test time compute thing. Yeah,
that's separate. I that is separate.
that's separate. I that is separate.
You can do uh you can do reinforcement
You can do uh you can do reinforcement
learning with or without test time
learning with or without test time
compute.
They're kind of separate.
Where am I from? Originally,
Where am I from? Originally,
Connecticut.
Maybe RL environments are more type one
Maybe RL environments are more type one
thing. Type two. Nope. Depends
thing. Type two. Nope. Depends
completely on the environment.
Depends completely on the environment.
Depends completely on the environment.
Some are one, some are the other, some
Some are one, some are the other, some
are both.
What are some cool applied domains that
What are some cool applied domains that
haven't been explored enough? So, the
haven't been explored enough? So, the
one that I really want to look into,
one that I really want to look into,
uh, there are a few. So, I I think that
uh, there are a few. So, I I think that
the most practical one on the business
the most practical one on the business
side for us at the moment and the one
side for us at the moment and the one
I'm spending the most time on is uh,
I'm spending the most time on is uh,
drones.
drones.
lots of applications there.
lots of applications there.
Uh the one I'm more interested in longer
Uh the one I'm more interested in longer
term is like hardcore material
term is like hardcore material
simulation.
simulation.
I want to see if there's some way that
I want to see if there's some way that
we can do like like molecule level or
we can do like like molecule level or
atomic level simulation
atomic level simulation
um while cutting some corners and
um while cutting some corners and
keeping it fast.
Are you creating a company? Pufferai is
Are you creating a company? Pufferai is
a company. So, all of my stuff is free
a company. So, all of my stuff is free
and open source, but um we do have
and open source, but um we do have
contracts with companies that want help
contracts with companies that want help
with their reinforcement learning needs.
with their reinforcement learning needs.
We do like both basic service on uh
We do like both basic service on uh
puffer liib and surrounding features
and uh we also do bigger things than
and uh we also do bigger things than
that.
What do you mean by material science?
What do you mean by material science?
Where is RL involved?
Where is RL involved?
That's what we're trying to figure out,
That's what we're trying to figure out,
right? Most RL problems look like fiddly
right? Most RL problems look like fiddly
optimization problems.
optimization problems.
I want to see if we can cast like
I want to see if we can cast like
materials discovery or testing or
materials discovery or testing or
something uh as an interactive basically
something uh as an interactive basically
as an interactive process with a sim.
No cuspi. Nope.
Oh, cool.
Oh, cool.
That's awesome.
You have Yan Lun on the board here.
You have Yan Lun on the board here.
Funny.
Is this a new thing?
Oh, it's like pretty new.
Oh, climate change.
Oh, climate change.
What are they doing? Climate change.
Carbon capture and storage. Yeah,
there's so much more cool stuff you can
there's so much more cool stuff you can
do than that.
uh superconductors
uh superconductors
like actually viable space like space uh
like actually viable space like space uh
space age materials.
space age materials.
They're like so many freaking things.
They're like so many freaking things.
Yeah, superconductors would be the
Yeah, superconductors would be the
really obvious awesome one.
RL for discovering a room temperature
RL for discovering a room temperature
superconductor would be awesome.
superconductor would be awesome.
I don't know how that we could do that,
I don't know how that we could do that,
but
I'm trying to think if that would be the
I'm trying to think if that would be the
highest impact thing you could do or
highest impact thing you could do or
not. Would it be
like if you had to pick like one problem
like if you had to pick like one problem
to solve to make the most sci-fi
to solve to make the most sci-fi
possible thing work?
possible thing work?
Would it be that? I don't know.
Generally, I like things that make I
Generally, I like things that make I
like things that make sci-fi tech real.
That is what I like.
Oh. Oh, perfect.
be a trillion plus dollar company. Yeah,
be a trillion plus dollar company. Yeah,
but like so is humanoid robotics and
but like so is humanoid robotics and
that's like I don't find humanoid
that's like I don't find humanoid
robotics particularly exciting at all.
You know what actually in the robotics
You know what actually in the robotics
space I find way cooler than humanoid
space I find way cooler than humanoid
robotics? like those industrial arms
robotics? like those industrial arms
that are used in assembling stuff. I
that are used in assembling stuff. I
want to see those operating faster than
want to see those operating faster than
you can visually track with your eyes.
That would be cool.
Just put a warning sign that's like do
Just put a warning sign that's like do
not approach the robot with a icon of a
not approach the robot with a icon of a
blender on it. Robot at work.
Do not approach.
How many hours in Factorio though? Zero.
How many hours in Factorio though? Zero.
That game does not appeal to me at all.
That game does not appeal to me at all.
I played Satisfactory for a little bit
I played Satisfactory for a little bit
and then got bored.
and then got bored.
Yeah, that kind of nerd snipe does not
Yeah, that kind of nerd snipe does not
work on me.
It just doesn't work. The thing that
It just doesn't work. The thing that
really got me good for like a long time
really got me good for like a long time
was optimizing MMO builds. That got me
was optimizing MMO builds. That got me
for a long time.
for a long time.
But not none of the manufacturing like
But not none of the manufacturing like
logistics sims have
logistics sims have
Welcome, Finn. Oh, yeah. You were asking
Welcome, Finn. Oh, yeah. You were asking
about applications.
about applications.
Finn is working on the uh the drone
Finn is working on the uh the drone
stuff with us. Ride Path of Exile.
stuff with us. Ride Path of Exile.
Yeah, I you know, I never got into it. I
Yeah, I you know, I never got into it. I
don't really like that it's
don't really like that it's
I don't really want a live service game
I don't really want a live service game
of that style that's seasonal. I
of that style that's seasonal. I
actually the last few days I've been
actually the last few days I've been
playing Grim Dawn. I've been having more
playing Grim Dawn. I've been having more
fun with that.
I played Diablo I for a little bit and
I played Diablo I for a little bit and
just didn't like it at all.
Do you recommend doing a PhD? I only
Do you recommend doing a PhD? I only
recommend doing a PhD to people who know
recommend doing a PhD to people who know
that they want to do a PhD.
ultimate game for optimizing builds.
ultimate game for optimizing builds.
Probably better if you don't get into
Probably better if you don't get into
it. The thing that I don't like about
it. The thing that I don't like about
the build optimization stuff in uh in
the build optimization stuff in uh in
those is that they
those is that they
like they kind of are all boring to
like they kind of are all boring to
play. It doesn't like you're kind of
play. It doesn't like you're kind of
just optimizing a DPS number and like
just optimizing a DPS number and like
resistances and stuff. You're not really
resistances and stuff. You're not really
The stuff I was into.
The stuff I was into.
The MMO I played was basically it was
The MMO I played was basically it was
fantasy chess on steroids. So like the
fantasy chess on steroids. So like the
number of interactions and the number of
number of interactions and the number of
things you had to consider like you
things you had to consider like you
didn't have something as crazy as the
didn't have something as crazy as the
passive tree and like socketing gems and
passive tree and like socketing gems and
all that stuff, but like there was a lot
all that stuff, but like there was a lot
of complexity because the gameplay
of complexity because the gameplay
itself had a lot more complexity
based on my experience.
based on my experience.
So, uh,
So, uh,
I had a very supportive adviser who let
I had a very supportive adviser who let
me do whatever. I was at what I would
me do whatever. I was at what I would
say is the best institution to do a PhD.
say is the best institution to do a PhD.
And there's still just like lots of
And there's still just like lots of
things that are not great. Um, academia
things that are not great. Um, academia
is not the place to have fancy new
is not the place to have fancy new
ideas. As it turns out, like the place
ideas. As it turns out, like the place
to do fancy new things is
to do fancy new things is
in like a financially comfortable,
in like a financially comfortable,
independent position,
independent position,
pretty much doing your own thing, like
pretty much doing your own thing, like
without any other strings attached.
without any other strings attached.
Academia does not like new ideas that do
Academia does not like new ideas that do
not fit the mold of how academia likes
not fit the mold of how academia likes
to see their new ideas play out. They do
to see their new ideas play out. They do
not like it when the solution to their
not like it when the solution to their
problem is to rewrite all their garbage
problem is to rewrite all their garbage
code and not do any like fancy new
code and not do any like fancy new
methods or write down any math. Um, they
methods or write down any math. Um, they
don't like it when you force them to
don't like it when you force them to
think about new problems that don't
think about new problems that don't
cleanly fit their math. Like there are a
cleanly fit their math. Like there are a
lot of just bad annoying things. Oh, and
lot of just bad annoying things. Oh, and
the paper writing process sucks.
the paper writing process sucks.
So there is a fair bit of overhead and a
So there is a fair bit of overhead and a
fair bit of constraint that you probably
fair bit of constraint that you probably
would not see from the outside.
So that is the negative. Positive is
So that is the negative. Positive is
generally speaking I still did have 5
generally speaking I still did have 5
years to explore the area of the problem
years to explore the area of the problem
that I wanted to explore and I developed
that I wanted to explore and I developed
a lot of the core stuff that now makes
a lot of the core stuff that now makes
it possible to build puffer lip. So, I
it possible to build puffer lip. So, I
would not have been able to build Puffer
would not have been able to build Puffer
Lib without doing my PhD. That is for
Lib without doing my PhD. That is for
sure.
Who's Yaken and what's he been up to?
Who's Yaken and what's he been up to?
Uh, he is a tech guy who posts a lot a
Uh, he is a tech guy who posts a lot a
lot a lot a lot of things on X.
lot a lot a lot of things on X.
Like he kind of just posts constantly
Like he kind of just posts constantly
and he has a large following and he does
and he has a large following and he does
uh some quite cool tech stuff actually,
uh some quite cool tech stuff actually,
but he's mostly just memes. So, he was
but he's mostly just memes. So, he was
doing some RL and saw the drone stuff we
doing some RL and saw the drone stuff we
were doing. I've been chatting with him.
were doing. I've been chatting with him.
Been cool.
Good VFS, VFX, and aspects and the
Good VFS, VFX, and aspects and the
random loot. It becomes satisfying.
Yeah, nothing beats gameplay loop
Yeah, nothing beats gameplay loop
though. The most important thing for
though. The most important thing for
like the most important thing for games
like the most important thing for games
that you put a lot of time into is the
that you put a lot of time into is the
gameplay loop. So, for me, like playing
gameplay loop. So, for me, like playing
a like an A RPG casually, like when I'm
a like an A RPG casually, like when I'm
fried and I just don't want to think
fried and I just don't want to think
about anything, that's cool. But like
about anything, that's cool. But like
the games that I played be like the
the games that I played be like the
games I sunk a ton of hours into like
games I sunk a ton of hours into like
one of them was uh old school Runescape
one of them was uh old school Runescape
and like you wouldn't look at it from
and like you wouldn't look at it from
the outside but it has probably the most
the outside but it has probably the most
satisfying mechanics of any game I've
satisfying mechanics of any game I've
ever played at the high level.
It's just like really good.
There's got to at least be something I
There's got to at least be something I
think there.
Actually, the mechanic system of
Actually, the mechanic system of
Runescape was so good that it uh it
Runescape was so good that it uh it
actually was the basis of a lot of what
actually was the basis of a lot of what
I did in neural MMO because it turns out
I did in neural MMO because it turns out
it's also a perfect match for
it's also a perfect match for
reinforcement learning.
How does doing a PhD compare to working
How does doing a PhD compare to working
in a research position at big tech? Uh,
in a research position at big tech? Uh,
if you can get a good research position
if you can get a good research position
at a top lab or company, you should do
at a top lab or company, you should do
that. If you can, the thing is usually
that. If you can, the thing is usually
you cannot get one of those without
you cannot get one of those without
doing a PhD.
It kind of depends because like
It kind of depends because like
a lot of the labs are not releasing
a lot of the labs are not releasing
enough stuff about their work these
enough stuff about their work these
days.
days.
At least in like 2019 though um when
At least in like 2019 though um when
they were actually writing blogs and
they were actually writing blogs and
stuff I I would say it was better.
stuff I I would say it was better.
Welcome Magader.
What are the biggest advances in RL
What are the biggest advances in RL
since PO? Are the algorithms improving
since PO? Are the algorithms improving
uh outside of Puffer? No. With Puffer?
uh outside of Puffer? No. With Puffer?
Yes.
We have the article right here on X
We have the article right here on X
puffing up PO.
This is our research and this is what we
This is our research and this is what we
released with 300 that makes everything
released with 300 that makes everything
work well.
work well.
It is a generational leap in
It is a generational leap in
improvement.
Well,
Well,
missed a lot of a lot of things today.
missed a lot of a lot of things today.
Yeah, working on the article. It's
Yeah, working on the article. It's
almost done.
almost done.
Probably be 3,500 words.
make puffer lip standardized in
make puffer lip standardized in
academia. Uh we do work with a couple
academia. Uh we do work with a couple
labs.
labs.
Academia is very stubborn about
Academia is very stubborn about
everything.
I basically,
I basically,
if you want to think of it this way, I
if you want to think of it this way, I
spent five years in my PhD trying to
spent five years in my PhD trying to
like sell my work to academics.
like sell my work to academics.
And uh let's say I just don't do that
And uh let's say I just don't do that
anymore. I'm sick of it.
Like
you're trying to help people who don't
you're trying to help people who don't
want to be helped for free.
Make wall clock time competitions.
Make wall clock time competitions.
They'll say it's invalid because it
They'll say it's invalid because it
doesn't fit their holy grail of steps.
Like nano GPT is not an academic thing.
Like you want to know what actually
Like you want to know what actually
happened outside of robotics? Almost all
happened outside of robotics? Almost all
the RL labs just died. There are like
the RL labs just died. There are like
two or three that actually kind of still
two or three that actually kind of still
do some things and all the students now
do some things and all the students now
work on LLMs so that they can get jobs.
work on LLMs so that they can get jobs.
So
So
academia succeeded in killing their
academia succeeded in killing their
field completely.
field completely.
You said you are a doctor and not a
You said you are a doctor and not a
professor. Yes, a doctor is somebody who
professor. Yes, a doctor is somebody who
is who has a PhD. Uh a professor works
is who has a PhD. Uh a professor works
in a teaching position at a university.
in a teaching position at a university.
So
So
yeah,
what is the content of the second part
what is the content of the second part
blog post? You're looking at it here.
blog post? You're looking at it here.
Tons of stuff.
What made you give up?
What made you give up?
What do you mean? What made me give? Oh,
What do you mean? What made me give? Oh,
on that academic not suited
just a lot of the stuff that ACMIA cares
just a lot of the stuff that ACMIA cares
about and like
about and like
they've kind of dug themselves into a
they've kind of dug themselves into a
hole in reinforcement learning where
hole in reinforcement learning where
like the benchmarks that they've chosen
like the benchmarks that they've chosen
have gotten slower and slower over the
have gotten slower and slower over the
years. So like they basically they have
years. So like they basically they have
no compute because they don't have
no compute because they don't have
budget for GPUs and when they do have
budget for GPUs and when they do have
budget they buy the wrong stuff and then
budget they buy the wrong stuff and then
they manage it poorly so it's even
they manage it poorly so it's even
slower than it should be and then they
slower than it should be and then they
write like slow code so it's a hundred
write like slow code so it's a hundred
times slower than it should be and then
times slower than it should be and then
they take this like awful 100 to
they take this like awful 100 to
thousand times too slow code and then
thousand times too slow code and then
they run it on a benchmark that's also a
they run it on a benchmark that's also a
thousand times too slow and they just
thousand times too slow and they just
don't get anything done.
don't get anything done.
like it's a pure it's a pure engineering
like it's a pure it's a pure engineering
failure. And the thing is because they
failure. And the thing is because they
don't value engineering, even though
don't value engineering, even though
it's completely preventing them from
it's completely preventing them from
doing science, they don't care.
How did you even get into MIT, my guy?
How did you even get into MIT, my guy?
I'm 17 and I want to apply there. Uh,
I'm 17 and I want to apply there. Uh,
best of luck at that age. All I can tell
best of luck at that age. All I can tell
you is to work your ass off.
you is to work your ass off.
There is not a single portion of my life
There is not a single portion of my life
where I have worked harder than in high
where I have worked harder than in high
school trying to get into undergrad.
school trying to get into undergrad.
Initially, I did Stanford for undergrad
Initially, I did Stanford for undergrad
MIT for PhD. Uh I worked
MIT for PhD. Uh I worked
probably except for freshman year, I
probably except for freshman year, I
probably averaged 110 hours of work a
probably averaged 110 hours of work a
week throughout high school. It was
week throughout high school. It was
brutal.
The most I've done after that for like
The most I've done after that for like
even like a month or two
even like a month or two
uh is probably 90. And I don't work 90
uh is probably 90. And I don't work 90
now. Occasionally I'll work like an 80
now. Occasionally I'll work like an 80
90 hour week, but I don't work like 90
90 hour week, but I don't work like 90
hour a week months even.
Like when I'm really focused and working
Like when I'm really focused and working
on a new project, typically I will work
on a new project, typically I will work
a few consecutive 70 to 80 hour weeks
a few consecutive 70 to 80 hour weeks
building the thing and then I burn
building the thing and then I burn
myself out and I work a few weeks of
myself out and I work a few weeks of
like 40 to 50 hours.
Have you published first part of this
Have you published first part of this
blog? No, it's they'll probably go out
blog? No, it's they'll probably go out
at the same time.
at the same time.
How much overhead in coms is there?
How much overhead in coms is there?
So, it depends what layer you're talking
So, it depends what layer you're talking
about. hosted device can be anywhere
about. hosted device can be anywhere
from like 2% up to 50%.
from like 2% up to 50%.
It's usually
It's usually
somewhere between five and 20%
somewhere between five and 20%
overhead.
overhead.
Um
Um
the scaling is not perfect on multiGPU.
the scaling is not perfect on multiGPU.
We don't even need to do multiGPU for
We don't even need to do multiGPU for
most stuff because we solve it so fast.
most stuff because we solve it so fast.
So we don't get perfect linear scaling
So we don't get perfect linear scaling
there either. I think we get like 400
there either. I think we get like 400
something thousand steps per second on a
something thousand steps per second on a
single 4090 and then we get like 2.2
single 4090 and then we get like 2.2
million on six 4090s. So we actually do
million on six 4090s. So we actually do
have a little bit of additional CPU
have a little bit of additional CPU
overhead in that configuration. So it's
overhead in that configuration. So it's
probably a little bit better than that.
probably a little bit better than that.
And then I we haven't tried multiGPU for
And then I we haven't tried multiGPU for
uh environments that are like already 3
uh environments that are like already 3
million steps per second. I'd imagine
million steps per second. I'd imagine
that wouldn't scale linearly.
that wouldn't scale linearly.
110 hours a week is madness.
110 hours a week is madness.
like flexing so fast.
like flexing so fast.
Yeah. So, basically what you're seeing
Yeah. So, basically what you're seeing
here, right, is like when you work a
here, right, is like when you work a
level of intensity for a while, like
level of intensity for a while, like
even when I'm working at a very small
even when I'm working at a very small
fraction of that, it will look like, oh
fraction of that, it will look like, oh
yeah, this guy's very focused. The thing
yeah, this guy's very focused. The thing
is, it's just like I've done I've done
is, it's just like I've done I've done
way more than that, right?
It's like the same thing where like, you
It's like the same thing where like, you
know, I went and I ran my 50k and I hit
know, I went and I ran my 50k and I hit
my thousand total powerlifting. So, it's
my thousand total powerlifting. So, it's
like people are always going to be like,
like people are always going to be like,
"Oh yeah, that guy's relatively fit even
"Oh yeah, that guy's relatively fit even
when I've been totally slacking for like
when I've been totally slacking for like
months, right?
months, right?
That's honestly the best way. Like, if
That's honestly the best way. Like, if
you want to get like if you want to be
you want to get like if you want to be
able to be that like a certain level of
able to be that like a certain level of
good, you have to just get way better
good, you have to just get way better
than that for at least a short period of
than that for at least a short period of
time that you can't even maintain.
You can apparently get super linear
You can apparently get super linear
scaling with multiGPU.
scaling with multiGPU.
I don't think you can in reinforcement
I don't think you can in reinforcement
learning.
learning.
I think that makes sense for um large
I think that makes sense for um large
models where Yeah. You know what that
models where Yeah. You know what that
is? That's got to be for large models
is? That's got to be for large models
where that all the weights don't fit on
where that all the weights don't fit on
one GPU, right? So if you have to keep
one GPU, right? So if you have to keep
loading the weights on and off device
loading the weights on and off device
for each part of the model, right? It's
for each part of the model, right? It's
going to be slower than if every GPU has
going to be slower than if every GPU has
like the same part of the model. That's
like the same part of the model. That's
just pipeline parallelism. It's not
just pipeline parallelism. It's not
applicable to reinforcement learning.
applicable to reinforcement learning.
Our models are too small for that to
Our models are too small for that to
matter.
in general scaling with multiGPU.
in general scaling with multiGPU.
Yeah. So I mean the thing I just
Yeah. So I mean the thing I just
described would be how you would do it,
described would be how you would do it,
right?
right?
It wouldn't really work in RL.
I think it's kind of trivial even like
I think it's kind of trivial even like
pretty much you get you should get super
pretty much you get you should get super
linear scaling as soon as your weights
linear scaling as soon as your weights
don't fit on one GPU maybe even earlier
don't fit on one GPU maybe even earlier
because you know you want to have um
because you know you want to have um
ideally you don't want to have batch
ideally you don't want to have batch
size on.
Need more examples from importable use
Need more examples from importable use
case.
case.
Can do that.
What was the hardest thing when you
What was the hardest thing when you
built Puffer Lib?
built Puffer Lib?
Um,
there's a lot of stuff.
there's a lot of stuff.
Like in retrospect, each individual
Like in retrospect, each individual
thing seems pretty simple, but like it
thing seems pretty simple, but like it
simple doesn't mean easy.
Honestly, a lot of it was just a
Honestly, a lot of it was just a
perspective shift. Like Puffer Lib kind
perspective shift. Like Puffer Lib kind
of takes every single thing in the field
of takes every single thing in the field
and flips it on its head. So
and flips it on its head. So
even if the final solution here is
even if the final solution here is
simple, it's like it's such a radical
simple, it's like it's such a radical
departure from everything that's out
departure from everything that's out
there that it like figuring out how to
there that it like figuring out how to
do everything this way was just very
do everything this way was just very
difficult. Like it went through many
difficult. Like it went through many
many iterations. Let's say we went
many iterations. Let's say we went
through many iterations. We started off
through many iterations. We started off
just having these emulation wrappers
just having these emulation wrappers
that made it easier to work with um
that made it easier to work with um
complicated environments and improved a
complicated environments and improved a
little bit of performance there. And
little bit of performance there. And
then we did the vectorzation thing that
then we did the vectorzation thing that
worked on top of that to get like up to
worked on top of that to get like up to
10x speed up, right? And then I started
10x speed up, right? And then I started
doing Syon environments and that very
doing Syon environments and that very
quickly got us to like, you know, 100x
quickly got us to like, you know, 100x
speed up, right? And then we did we
speed up, right? And then we did we
improved our infrastructure. We went to
improved our infrastructure. We went to
C. We did like large batch stuff. We did
C. We did like large batch stuff. We did
algorithmic optimizations, PyTorch
algorithmic optimizations, PyTorch
optimizations, and that got us to a
optimizations, and that got us to a
thousandx. And then now where we're
thousandx. And then now where we're
sitting here, like it's a,000x plus for
sitting here, like it's a,000x plus for
a lot of stuff.
How do you debug RL algorithms? Well,
How do you debug RL algorithms? Well,
with Puffer Lib, the fact is we have so
with Puffer Lib, the fact is we have so
many different environments,
many different environments,
right, that we like we can get
right, that we like we can get
performance on a ton of different tasks.
performance on a ton of different tasks.
So when we introduce something that
So when we introduce something that
breaks something else,
breaks something else,
we can see pretty quickly. And when we
we can see pretty quickly. And when we
introduce a change, we can see very
introduce a change, we can see very
quickly whether it just helps on one
quickly whether it just helps on one
specific environment or whether it helps
specific environment or whether it helps
on like most of them. We also have
on like most of them. We also have
environments that we trust more than
environments that we trust more than
others where like if something does well
others where like if something does well
on this, it tends to do well on
on this, it tends to do well on
everything else versus there are other
everything else versus there are other
ones where, you know, if you just train
ones where, you know, if you just train
something on Breakout, it's usually
something on Breakout, it's usually
better on Breakout is better everywhere,
better on Breakout is better everywhere,
but not always. It's not super
but not always. It's not super
generalizable. If you get it to work on
generalizable. If you get it to work on
mazes, it's usually good everywhere. If
mazes, it's usually good everywhere. If
you get it to work on neural MMO 3, it's
you get it to work on neural MMO 3, it's
usually good everywhere.
How do you debug why an agent learned
How do you debug why an agent learned
well or didn't learn well? You look at
well or didn't learn well? You look at
your data, observations, rewards,
your data, observations, rewards,
and then you watch the policy.
and then you watch the policy.
It's almost always an error in
It's almost always an error in
observations and rewards, though.
I have a debugging checklist on the docs
I have a debugging checklist on the docs
page.
most difficult part stage of solving RL
most difficult part stage of solving RL
problems for beginner. Lack of
problems for beginner. Lack of
programming maturity
programming maturity
just generally not being comfortable
just generally not being comfortable
enough writing clean, simple, low-level
enough writing clean, simple, low-level
code without breaking things.
code without breaking things.
What are your favorite RL labs? What are
What are your favorite RL labs? What are
some good ones to keep up with? There
some good ones to keep up with? There
really aren't that many right now. Um,
really aren't that many right now. Um,
Flair is Jacob Forers's lab. They're
Flair is Jacob Forers's lab. They're
doing good stuff. Uh, and then Eugene's
doing good stuff. Uh, and then Eugene's
lab has a few things at NYU.
lab has a few things at NYU.
I think Julian I don't know if Julian's
I think Julian I don't know if Julian's
lab is still really doing much RL. They
lab is still really doing much RL. They
have a little bit.
Let me think who else. I think there are
Let me think who else. I think there are
still a few people at like Stanford and
still a few people at like Stanford and
Berkeley who are doing RL stuff that's
Berkeley who are doing RL stuff that's
not robotics, but you don't hear about
not robotics, but you don't hear about
stuff very often.
And I guess we'll see if there are any
And I guess we'll see if there are any
new ones like any new good things at
new ones like any new good things at
RLC.
Benjamin for Princeton
Benjamin for Princeton
Eisenbach.
Eisenbach.
Uh I'm trying to remember what what are
Uh I'm trying to remember what what are
the latest things that he's done.
Yeah.
Oh, contrastive.
Oh, contrastive.
I haven't really looked into contrastive
I haven't really looked into contrastive
at all
at all
for RL. I don't know if there's
for RL. I don't know if there's
something there. Maybe
But not super practical. Yeah.
You know George Hotz? Yeah. I uh I
You know George Hotz? Yeah. I uh I
bought the tiny boxes from him, right?
ended up chatting with them a whole
ended up chatting with them a whole
bunch because we had a bunch of problems
bunch because we had a bunch of problems
getting them to work initially, but uh
getting them to work initially, but uh
they work quite well now.
You think it is a good idea to make
You think it is a good idea to make
general purpose observation space
general purpose observation space
converter?
converter?
It's hard to do that fast.
It's hard to do that fast.
Very hard to do that fast. That is the
Very hard to do that fast. That is the
big issue.
You know, one of the things I was
You know, one of the things I was
thinking that we could do, I don't know
thinking that we could do, I don't know
if it would work, but we could kind of
if it would work, but we could kind of
just define the observation space to be
just define the observation space to be
like, I don't know, some upper bound. It
like, I don't know, some upper bound. It
could be like 256 or something. And then
could be like 256 or something. And then
we could just put all the flat data from
we could just put all the flat data from
many of the different arcade ends into
many of the different arcade ends into
that and just see if we can train on
that and just see if we can train on
with padded observations and actions.
with padded observations and actions.
See if we can just train on a bunch of
See if we can just train on a bunch of
different environments with the same
different environments with the same
policy.
policy.
I think we could force it to work now. I
I think we could force it to work now. I
honestly do.
honestly do.
Super cool and doing amazing work.
Super cool and doing amazing work.
Thanks for this agreement and open
Thanks for this agreement and open
sourcing this. Thank you very much.
I've learned a lot from RL from your
I've learned a lot from RL from your
articles and live stream faster than the
articles and live stream faster than the
knowledge I gain from books out content
knowledge I gain from books out content
and then I search and come across it.
and then I search and come across it.
I'm working on it. I this here is going
I'm working on it. I this here is going
to be like a 3,500 word article plus the
to be like a 3,500 word article plus the
other one. So 5,500 words of uh intro
other one. So 5,500 words of uh intro
material with a ton of links.
Let's try to this fixed bound fixed
Let's try to this fixed bound fixed
length
length
general observator
online.
So the thing with that, right, is I
So the thing with that, right, is I
don't know if
you'd have to do some experiments.
you'd have to do some experiments.
Basically, I don't know if you can learn
Basically, I don't know if you can learn
a useful shared representation.
a useful shared representation.
The data is formatted that way.
It's technically possible that you could
It's technically possible that you could
learn it in like you can't learn it in
learn it in like you can't learn it in
the first layer. That's for sure. It's
the first layer. That's for sure. It's
possible you could learn it in later
possible you could learn it in later
layers, but we use really shallow
layers, but we use really shallow
policies and reinforcement learning most
policies and reinforcement learning most
of the
2.3K.
2.3K.
Heck yeah.
Okay.
5:44.
5:44.
I need to go in a couple of minutes here
I need to go in a couple of minutes here
for dinner.
for dinner.
We have a full draft of this.
And
where's the other one?
where's the other one?
Oh, yeah. A full draft for this.
Oh, yeah. A full draft for this.
So, what is it usually? Is it 250 words
So, what is it usually? Is it 250 words
a page? Usually,
I was aiming for about 20 pages of
I was aiming for about 20 pages of
content.
I ended up with like 21 22 pages
I ended up with like 21 22 pages
with the big heading. So, it's probably
with the big heading. So, it's probably
20 pages just about sped on.
How long was my quick start guide?
It doesn't show you. That's annoying.
It doesn't show you. That's annoying.
Substantially shorter though.
Substantially shorter though.
Maybe like 2K.
I don't even have time to read through
I don't even have time to read through
this once, I don't think, before uh
this once, I don't think, before uh
before dinner. So, I think I will just
before dinner. So, I think I will just
call it Anybody have any questions?
call it Anybody have any questions?
Obviously, I also have to fix the doc
Obviously, I also have to fix the doc
section or the section of the docs on
section or the section of the docs on
the web page that I'm linking people to
the web page that I'm linking people to
or like add a little bit to that. I will
or like add a little bit to that. I will
do that.
do that.
But otherwise, I think this is like a
But otherwise, I think this is like a
full full draft. I'll edit this
full full draft. I'll edit this
tomorrow. I actually have a flight to
tomorrow. I actually have a flight to
edit this on soon, so I'll probably just
edit this on soon, so I'll probably just
do that later this week.
do that later this week.
I don't want to spend too many days in a
I don't want to spend too many days in a
row writing. So, we'll probably be back
row writing. So, we'll probably be back
running experiments tomorrow. We will
running experiments tomorrow. We will
see. Um, but other than that, yeah,
see. Um, but other than that, yeah,
thank you for tuning in, folks.
thank you for tuning in, folks.
Hopefully, you all enjoy this once it's
Hopefully, you all enjoy this once it's
ready.
ready.
Puffer.ai for all the things. If you
Puffer.ai for all the things. If you
want to help me out for free, star the
want to help me out for free, star the
repo. Just star. It really helps
repo. Just star. It really helps
the puffer star.
the puffer star.
And other than that
And other than that
can join the discord to get involved
can join the discord to get involved
with development and follow me on extra
with development and follow me on extra
more. We do the importable example
more. We do the importable example
today.
today.
Not really cuz I got to run for dinner.
Not really cuz I got to run for dinner.
It is about to be 6:00 p.m. here.
It is about to be 6:00 p.m. here.
If you have um
wait generalized general observation
wait generalized general observation
space is different from importable
space is different from importable
stuff.
I don't know if I'm going to be working
I don't know if I'm going to be working
this evening.
I'm not working crazy hours this uh for
I'm not working crazy hours this uh for
the next little bit here here
on night live stream. Yeah, I don't I
on night live stream. Yeah, I don't I
don't really know if I want to do
don't really know if I want to do
another session tonight to be honest
another session tonight to be honest
with you.
If you actually message me though, I'll
If you actually message me though, I'll
tell you what though. If you actually
tell you what though. If you actually
message me in the Discord because I
message me in the Discord because I
wasn't going to do docs tomorrow. If you
wasn't going to do docs tomorrow. If you
actually message me like specifically
actually message me like specifically
what needs to be done, I can do it on
what needs to be done, I can do it on
the stream tomorrow
because you haven't told me what to do
because you haven't told me what to do
with the importable like what more to do
with the importable like what more to do
with the importable example or like what
with the importable example or like what
you're still confused about that it
you're still confused about that it
doesn't cover.
I'm very much I'd like to improve it,
I'm very much I'd like to improve it,
but I need to know how. Okay, I got to
but I need to know how. Okay, I got to
run though.

Kind: captions
Language: en
Okay,
Okay,
you're back live. Hello.
Greetings take a lot out of me for some
Greetings take a lot out of me for some
reason. I don't know.
reason. I don't know.
Internal engineers problem, but at least
Internal engineers problem, but at least
I'm just writing today.
All right, let's get uh let's get
All right, let's get uh let's get
started on this.
We got all this done. Let me just go
We got all this done. Let me just go
warming up. Let's just go get all the
warming up. Let's just go get all the
links.
Okay.
Where's their paper?
Crazy
that they um
that they um
that like you still these publications
that like you still these publications
still have like this restricted access
It's a very well named paper.
They not have the PDF.
Oh, here it is.
Hey Barath, welcome.
I am currently working on the uh the
I am currently working on the uh the
reinforcement learning
reinforcement learning
compendium of sorts.
This is all material designed to help
This is all material designed to help
new people to the field.
new people to the field.
I don't do this type of stuff super
I don't do this type of stuff super
often, but it's been heavily requested.
often, but it's been heavily requested.
So,
Did I get 10? One, two, three, four,
10.
Welcome, potato.
Welcome, potato.
2.3k words is crazy. This is half of
2.3k words is crazy. This is half of
what I've written so far for you guys.
what I've written so far for you guys.
Uh there's also
Uh there's also
for my articles
for my articles
there's the general intro guide which is
there's the general intro guide which is
another 2k words.
I don't want it to be excessively long.
I don't want it to be excessively long.
I want it to mostly be a reference
I want it to mostly be a reference
to other things
but it's going to be quite complete.
People have kind of like
the way I tell people to learn things.
the way I tell people to learn things.
The thing is like
The thing is like
I'm pretty dang independent with the way
I'm pretty dang independent with the way
that I approach new problems. And like
that I approach new problems. And like
if you tell me exactly how to do
if you tell me exactly how to do
something, I'm probably not going to do
something, I'm probably not going to do
it that way anyways.
it that way anyways.
Um, and that works really well for some
Um, and that works really well for some
people, but like a lot of people have
people, but like a lot of people have
just been asking me for basically a step
just been asking me for basically a step
by step, like what do I do to learn RL?
by step, like what do I do to learn RL?
So that's what I'm trying to do here.
So that's what I'm trying to do here.
Thanks for the effort. You got it, man.
Thanks for the effort. You got it, man.
The support has been great lately.
The support has been great lately.
Today was awesome. Like,
Today was awesome. Like,
look at this. We got plus 600 followers
look at this. We got plus 600 followers
today. We're going to hit 10K soon.
10K is a huge account in reinforcement
10K is a huge account in reinforcement
learning. That's like
learning. That's like
been a longtime goal of mine.
like most of the people that um have
like most of the people that um have
accounts that size in RL, they also post
accounts that size in RL, they also post
politics or memes or some other things.
politics or memes or some other things.
It's actually like pretty hard to get to
It's actually like pretty hard to get to
a sizable account when you're just
a sizable account when you're just
posting deeply technical content.
If you notice, right,
like pretty much everyone else I can
like pretty much everyone else I can
think of in RL who has a larger account
think of in RL who has a larger account
um posts other crap.
I allow myself like one fitness post
I allow myself like one fitness post
once in a while because I've actually
once in a while because I've actually
gotten a couple of people to like start
gotten a couple of people to like start
actually getting in shape, which is very
actually getting in shape, which is very
important, but it's very rare.
Okay, this is a good reference.
started streaming too after seeing
started streaming too after seeing
yours.
yours.
Welcome.
I've had fun with it.
Hi, Joseph. Watch your live stream every
Hi, Joseph. Watch your live stream every
day when you were working with Syon
day when you were working with Syon
every day a few months ago and then I
every day a few months ago and then I
got busy with stuff. That look like it
got busy with stuff. That look like it
would be good at least to have a
would be good at least to have a
summarization each of your live videos
summarization each of your live videos
maybe in the description.
maybe in the description.
Oh man, it's like it's kind of tough to
Oh man, it's like it's kind of tough to
do that is the thing because you see
do that is the thing because you see
this on YouTube, right? But it also goes
this on YouTube, right? But it also goes
to Twitch and X. I guess technically the
to Twitch and X. I guess technically the
main VODs are on just YouTube. Yeah,
main VODs are on just YouTube. Yeah,
there isn't really a uh
there isn't really a uh
a topic is the thing. Like most of the
a topic is the thing. Like most of the
streams don't have a coherent topic for
streams don't have a coherent topic for
the most part, the most spot. Um,
the most part, the most spot. Um,
trying to think if there's a way that I
trying to think if there's a way that I
can do that easily.
I'm actually kind of surprised that uh
I'm actually kind of surprised that uh
YouTube hasn't added a thing for that
YouTube hasn't added a thing for that
cuz they already get transcriptions,
cuz they already get transcriptions,
don't they?
Trying to think what to do with that.
I mean, do you have any idea on how to
I mean, do you have any idea on how to
actually do that other than like
it feels like something that you should
it feels like something that you should
be able to do automatically?
Not your videos.
treasure.
treasure.
Oh, wait. Was about to build a tool with
Oh, wait. Was about to build a tool with
whisper. I missed one message. Make use
whisper. I missed one message. Make use
of an element summarized. Put
of an element summarized. Put
descriptions.
Is there a way to do that through
Is there a way to do that through
YouTube?
We have over 1k subs.
Is there a way to do that?
This is not it.
This is not it.
I missed several messages. Yeah, hang
I missed several messages. Yeah, hang
on. Let me scroll back through.
It's kind of hard to set up like an LLM
It's kind of hard to set up like an LLM
to do that thing locally because it's a
to do that thing locally because it's a
lot of data
API to do to get the captions and do
API to do to get the captions and do
something with them.
Do they have like a summary thing?
No, it's all third party crap
because like if I have to like go
because like if I have to like go
download a six-hour VOD and then like
download a six-hour VOD and then like
feed a six-hour VOD through a local
feed a six-hour VOD through a local
model, right?
I mean, if there is a third party thing
I mean, if there is a third party thing
that I can just like add that will do it
that I can just like add that will do it
and not make a mess, I'll do it.
We've made a lot of progress since the
We've made a lot of progress since the
days of uh MOA and Syon.
days of uh MOA and Syon.
It's uh
It's uh
it's literally a different field. Like
it's literally a different field. Like
RL is a different field from that which
RL is a different field from that which
was I think I was doing that last
was I think I was doing that last
summer.
Don't feed the video.
Don't feed the video.
Just the audio.
Just the audio.
Oh yeah, obviously.
Is there an API that you can download?
Is there an API that you can download?
Can you just get the audio like that? Is
Can you just get the audio like that? Is
that a thing you can do?
Cuz if you have a thing that I can set
Cuz if you have a thing that I can set
up that I can do that easily. Sure.
Right. If I can just like have a model,
Right. If I can just like have a model,
if even if I have to set up a container
if even if I have to set up a container
somewhere, if I just set it up on one of
somewhere, if I just set it up on one of
these machines and I just like, you
these machines and I just like, you
know, paste a link to the video or
know, paste a link to the video or
whatever
whatever
and it'll like just generate me a
and it'll like just generate me a
summary, that would be
Is that a question for me or Barath?
That'd be a nice thing to have. I'll
That'd be a nice thing to have. I'll
build one just for buffer.
build one just for buffer.
Can turn on a live caption button. Let
Can turn on a live caption button. Let
me see.
me see.
Let me see.
Uh, this is the wrong tab. Hang on.
So
Yeah.
I actually have blocked links off
and it still blocks them. Anyways,
there's no chat moderation.
Oh, this is their like that's not it.
Oh, this is their like that's not it.
Yeah. I don't know if you have any idea
Yeah. I don't know if you have any idea
where this thing is or if there is
where this thing is or if there is
actually a feature. I'll gladly like set
actually a feature. I'll gladly like set
it up.
I'd also love to know how to set YouTube
I'd also love to know how to set YouTube
to low latency by default. There doesn't
to low latency by default. There doesn't
seem to be a way to do it in their UI.
It's like you have to do it when you
It's like you have to do it when you
start the stream, but if you start the
start the stream, but if you start the
stream through reream, it doesn't work.
I feel like I was forgetting a
Oh, are there papers I can suggest
Oh, are there papers I can suggest
people on applications?
People always ask me about this anyways.
People always ask me about this anyways.
So, it would be good to just have these
So, it would be good to just have these
links.
Let me just go grab all of the um
all of the ones that I know.
I think of other ones. Yes.
Borax
later in Jax.
later in Jax.
Yeah, that's cool. I have to I haven't
Yeah, that's cool. I have to I haven't
looked at like the details of those
looked at like the details of those
specific apps. See if that makes sense
specific apps. See if that makes sense
as an approach or not.
There are a bunch of other ones as well.
There are a bunch of other ones as well.
I'm trying to think if there are any
I'm trying to think if there are any
that are worth highlighting.
They don't have like an article though.
Yeah, it's YouTube.
Yeah, it's YouTube.
Okay.
Okay.
We'll do Yes.
What did Alpha Star use to train?
Wasn't it some crazy thing? It's the
Wasn't it some crazy thing? It's the
league. It's the league play thing. But
league. It's the league play thing. But
didn't they also have um
Oh yeah, they also bootstrapped it with
Oh yeah, they also bootstrapped it with
imitation learning.
imitation learning.
I forgot about that.
What was the actual algorithm? Does
What was the actual algorithm? Does
anybody remember?
anybody remember?
Learning algorithm.
replayed experience. So, it's off
replayed experience. So, it's off
policy. Yeah.
Hello him.
Welcome. Oh jeez, it's already 400 p.m.
Welcome. Oh jeez, it's already 400 p.m.
How the heck did that happen?
Like I've been lazy.
messages, meetings, and writing.
They really have got to fix XDMs. Like,
They really have got to fix XDMs. Like,
they you should not be able to be
they you should not be able to be
getting added by this many spam bots.
India. You're hardworking. It's Yeah,
India. You're hardworking. It's Yeah,
this is pretty lazy, I'd say, for me.
this is pretty lazy, I'd say, for me.
We're not putting in crazy hours at the
We're not putting in crazy hours at the
moment.
welcome Rolo.
Oh, Mark Towers is on here.
What other off policy papers do I give?
What other off policy papers do I give?
Not what's the other one?
Not what's the other one?
Atari 200 times faster.
What other off policy papers?
What other off policy papers?
Impala.
Impala.
Maybe I impala
flies. This
fly.
There's the original world model paper.
Yeah, this one.
He
What are some good starting projects?
What are some good starting projects?
What would be an impressive project?
What would be an impressive project?
Those are two different things and it
Those are two different things and it
depends on your background. A good
depends on your background. A good
starter project
starter project
uh if you have a decent programming
uh if you have a decent programming
background is something like a simple
background is something like a simple
arcade game, something that you can do
arcade game, something that you can do
within like a few days.
within like a few days.
You can see all the projects on
You can see all the projects on
puffer.ai. Many of these are made by
puffer.ai. Many of these are made by
contributors.
contributors.
Uh some of these end up being very
Uh some of these end up being very
useful in research where you would not
useful in research where you would not
expect like 2048 actually ends up being
expect like 2048 actually ends up being
pretty good for research.
pretty good for research.
Tetris questionable whether it's as
Tetris questionable whether it's as
useful, but it's just freaking awesome
useful, but it's just freaking awesome
to just watch. It's like really
to just watch. It's like really
mesmerizing and cool to see.
mesmerizing and cool to see.
We've got like application stuff.
We've got like application stuff.
Applications are always awesome things
Applications are always awesome things
on uh different problems in industry
on uh different problems in industry
like this drone sim.
So generally newcomers to RL pick
So generally newcomers to RL pick
something you can do in a few days. It
something you can do in a few days. It
could be applied. It could just be a
could be applied. It could just be a
game or whatever. And then what is an
game or whatever. And then what is an
impressive project in RL? It's something
impressive project in RL? It's something
that is going to pose a new type of
that is going to pose a new type of
challenge that allows us to test our
challenge that allows us to test our
algorithms in a different way or
algorithms in a different way or
something that unlocks a new application
something that unlocks a new application
area for us. Those are the two things
area for us. Those are the two things
that are most impressive.
that are most impressive.
BQN and PO work on a lot of domains.
BQN and PO work on a lot of domains.
What would the difference between my
What would the difference between my
work be other than the end? So, first of
work be other than the end? So, first of
all, DQN doesn't really work on anything
all, DQN doesn't really work on anything
uh unless you mean like the follow-up
uh unless you mean like the follow-up
variance of it. PO does work generally.
variance of it. PO does work generally.
It's just very fiddly. Uh we're not
It's just very fiddly. Uh we're not
actually doing either of those
actually doing either of those
technically. We have a follow-up to PO
technically. We have a follow-up to PO
uh that is substantially substantially
uh that is substantially substantially
more stable, a heck of a lot faster as
more stable, a heck of a lot faster as
well. So, that's what we use for our
well. So, that's what we use for our
training. We also do our own
training. We also do our own
hyperparameter sweep work as well. What
hyperparameter sweep work as well. What
would my works be other than the M? So
would my works be other than the M? So
an environment can be a research
an environment can be a research
contribute, right? If an environment
contribute, right? If an environment
allows us to study an algorithm in a way
allows us to study an algorithm in a way
that we have not been able to before uh
that we have not been able to before uh
or if it allow unlocks a new applied
or if it allow unlocks a new applied
area that is a major contribution to
area that is a major contribution to
give you a very quick idea of like one
give you a very quick idea of like one
of the environments we used super
of the environments we used super
heavily in development of uh 3.0. I
heavily in development of uh 3.0. I
know.
Okay. So, we have this grid environment
Okay. So, we have this grid environment
here and this can generate grids of like
here and this can generate grids of like
whatever size from this up to you know
whatever size from this up to you know
the size of this full container.
the size of this full container.
So they can be big, they can be small.
So they can be big, they can be small.
And if you look at our release docs, we
And if you look at our release docs, we
have agents that solve this thing
have agents that solve this thing
relatively well just using the 01 signal
relatively well just using the 01 signal
of getting to the goal. So it's a very
of getting to the goal. So it's a very
very absurdly hard exploration task. Uh
very absurdly hard exploration task. Uh
and this served as a very reliable
and this served as a very reliable
benchmark for us. And uh pretty much
benchmark for us. And uh pretty much
training on this environment was a very
training on this environment was a very
good indicator like doing better on this
good indicator like doing better on this
environment was a very very strong
environment was a very very strong
indicator of doing better on everything.
indicator of doing better on everything.
So this is like one example of a targetd
So this is like one example of a targetd
designed environment for research
designed environment for research
applications are also useful like um you
applications are also useful like um you
know the drone environment was one of
know the drone environment was one of
the first super fast continuous
the first super fast continuous
environments that we had added. So
environments that we had added. So
continuous action spaces plus super fast
continuous action spaces plus super fast
and allowed us to make uh to get some of
and allowed us to make uh to get some of
the kinks out of continuous training.
the kinks out of continuous training.
2048's kind of interesting because it
2048's kind of interesting because it
just it takes a pretty dang long time to
just it takes a pretty dang long time to
train more than you would expect for
train more than you would expect for
something this simple. looks like a
something this simple. looks like a
non-trivial benchmark.
non-trivial benchmark.
There are lots of different things here.
If you want to contribute to the
If you want to contribute to the
research side, you can also do that.
Do people create environments to test
Do people create environments to test
agents against each other? Sort of like
agents against each other? Sort of like
Kaggle. Uh yes, they do. There have been
Kaggle. Uh yes, they do. There have been
competitions like this. Uh I have ran I
competitions like this. Uh I have ran I
have run several of these at
have run several of these at
conferences. I have colleagues who have
conferences. I have colleagues who have
run uh run these at conferences. Uh it
run uh run these at conferences. Uh it
is an incredible mindnumbing around
is an incredible mindnumbing around
amount of work and it is not fun at all.
amount of work and it is not fun at all.
So I don't do those anymore.
So I don't do those anymore.
Uh it is absolutely something that can
Uh it is absolutely something that can
be done in principle and knowing what I
be done in principle and knowing what I
know now, I could probably do it a lot
know now, I could probably do it a lot
easier than I did it back then.
But let's just say it was a traumatic
But let's just say it was a traumatic
amount of work and was kind of just
amount of work and was kind of just
awful for everyone involved.
Why 28 2048 took a long time. No idea.
Why 28 2048 took a long time. No idea.
To be fair, I got the train time from I
To be fair, I got the train time from I
got it from six uh six billion steps
got it from six uh six billion steps
down to 600 million steps with a very
down to 600 million steps with a very
good hyperprem sweep, but 600 million is
good hyperprem sweep, but 600 million is
still quite a lot of experience for
still quite a lot of experience for
something like that environment.
It's going to be a useful benchmark
It's going to be a useful benchmark
environment for us though because of
environment for us though because of
this, right?
this, right?
Like that's actually going to make it a
Like that's actually going to make it a
useful benchmark.
Even expect works there
in max with probabilities.
in max with probabilities.
Well, yeah, you can do that, right? Like
Well, yeah, you can do that, right? Like
if you write down any environment as an
if you write down any environment as an
actual tabular game, right? versus
actual tabular game, right? versus
having to learn
having to learn
everything from representations. It's
everything from representations. It's
going to be a lot easier.
The tabular algorithms break immediately
The tabular algorithms break immediately
when you can't do that though, right?
Like sure technically you can do you can
Like sure technically you can do you can
say the same thing about mazes, right?
say the same thing about mazes, right?
Oh, I can just implement breath for
Oh, I can just implement breath for
search or depth for search.
M AlphaGo things work better than their
M AlphaGo things work better than their
MCTS with learned
MCTS with learned
MCTS with learned functions.
The Alph Go is
The Alph Go is
you make the problem easier when you
you make the problem easier when you
have the ability to set specific states,
have the ability to set specific states,
right?
So yes, that should make it easier.
So yes, that should make it easier.
Um whether it's easier with something
Um whether it's easier with something
like mu0 where you have to learn the
like mu0 where you have to learn the
dynamics as well, that is less certain.
dynamics as well, that is less certain.
I think in general if we just like if
I think in general if we just like if
you look at the topics I'm writing down
you look at the topics I'm writing down
here
here
it's pretty much just a matter of like
it's pretty much just a matter of like
going through everything that DeepMind
going through everything that DeepMind
has done over the last seven years like
has done over the last seven years like
ripping out all the fancy math that is
ripping out all the fancy math that is
there for the sake of them wanting to do
there for the sake of them wanting to do
fancy math and like figuring out what
fancy math and like figuring out what
are the generalizable components that uh
are the generalizable components that uh
can be used to improve not just like
can be used to improve not just like
this crazy all we care about is sample
this crazy all we care about is sample
efficiency thing but like balance sample
efficiency thing but like balance sample
efficiency and compute in a reasonable
efficiency and compute in a reasonable
sane manner. Ideally with like a tunable
sane manner. Ideally with like a tunable
trade-off, then we'd be good.
nice teaching. Thank you. I guess you
nice teaching. Thank you. I guess you
could always bootstrap your RL learner
could always bootstrap your RL learner
with the search approach. Just speed up
with the search approach. Just speed up
the initial few steps.
the initial few steps.
It depends on what you're trying to do.
It depends on what you're trying to do.
Are you trying to make it train faster
Are you trying to make it train faster
in wall clock or in steps?
in wall clock or in steps?
Because our thing is ridiculously fast
Because our thing is ridiculously fast
in wall clock right now. It's possible
in wall clock right now. It's possible
we can still make it faster.
we can still make it faster.
Heck, I think we can probably double the
Heck, I think we can probably double the
speed just with even more infrastructure
speed just with even more infrastructure
optimizations.
optimizations.
Um,
Um,
yeah, wall clock we kind of steamroll
yeah, wall clock we kind of steamroll
everything already.
Like we can train at 3 million steps per
Like we can train at 3 million steps per
second. So
second. So
everything's just fast and puffer.
everything's just fast and puffer.
We did um we did 640 billion steps of
We did um we did 640 billion steps of
neural MMO 3, which is a pabyte of
neural MMO 3, which is a pabyte of
training data in 3 days and 8 hours on
training data in 3 days and 8 hours on
six GPUs.
So our wall clock is very very good.
So our wall clock is very very good.
We're basically what makes it so much
We're basically what makes it so much
faster CUDA kernels. No, we really only
faster CUDA kernels. No, we really only
have one custom CUDA kernel which is the
have one custom CUDA kernel which is the
advantage function which is only a we
advantage function which is only a we
only had to do that because our thing is
only had to do that because our thing is
so fast. It's that we have environments
so fast. It's that we have environments
that are fast themselves. So the ends
that are fast themselves. So the ends
are all in C and they have our own
are all in C and they have our own
infrastructure that doesn't slow them
infrastructure that doesn't slow them
down. So we can do tens of gigabytes a
down. So we can do tens of gigabytes a
second no problem uh from the
second no problem uh from the
environment side. And then we got like a
environment side. And then we got like a
really big batch training to work and we
really big batch training to work and we
cut out a lot of the overhead that
cut out a lot of the overhead that
people don't typically optimize because
people don't typically optimize because
their trainers are too slow for it to
their trainers are too slow for it to
matter anyways.
CPUbound uh everything in RL is
CPUbound uh everything in RL is
typically CPUbound on the simulator side
typically CPUbound on the simulator side
and a lot of the GPU sims where they're
and a lot of the GPU sims where they're
like, "Oh, it's super fast." They're not
like, "Oh, it's super fast." They're not
actually that fast.
actually that fast.
It's usually environment bound. A lot of
It's usually environment bound. A lot of
stuff in reinforcement learning is
stuff in reinforcement learning is
environment bound
or like it may as well be environment
or like it may as well be environment
bound because it'll be like GPU bound
bound because it'll be like GPU bound
but only because you're using batch size
but only because you're using batch size
eight with tiny models you're like you
eight with tiny models you're like you
know if you're using the GPU at 2% util
know if you're using the GPU at 2% util
like 2% efficiency right it may as well
like 2% efficiency right it may as well
be environment bound because you're
be environment bound because you're
doing dumb things
doing dumb things
is RL easier to distribute because all
is RL easier to distribute because all
you care about is rollouts
Uh, I would think that RL is kind of
Uh, I would think that RL is kind of
harder to distribute than everything
harder to distribute than everything
else in general until you get into like
else in general until you get into like
I guess the only thing that's harder to
I guess the only thing that's harder to
distribute than RL is the really big
distribute than RL is the really big
models where you have to do like several
models where you have to do like several
dimension parallelism.
But no, RL's really hard to distribute
But no, RL's really hard to distribute
because you have a lot of data and you
because you have a lot of data and you
have to synchronize very frequently. We
have to synchronize very frequently. We
have it working though.
You don't do the rollouts on CPU. You
You don't do the rollouts on CPU. You
don't do the policy inference pass on
don't do the policy inference pass on
CPU. That's the Impala architecture and
CPU. That's the Impala architecture and
it's a shitty architecture.
What if you share
What if you share
some of the states? You train several
some of the states? You train several
similar rollouts together like we
similar rollouts together like we
branch. Nobody does epsilon greedy
branch. Nobody does epsilon greedy
things anymore and that wouldn't matter
things anymore and that wouldn't matter
anyways
anyways
here. Okay. So our default training
here. Okay. So our default training
setup works like this. This is like the
setup works like this. This is like the
average training setup that we have. We
average training setup that we have. We
take eight CPU cores.
take eight CPU cores.
On each CPU core, we simulate a thousand
On each CPU core, we simulate a thousand
copies of the environment or if it's a
copies of the environment or if it's a
multi- aent environment, however many
multi- aent environment, however many
agents you need to get to a total of a
agents you need to get to a total of a
thousand agents. So they're like 8192
thousand agents. So they're like 8192
total agents. And then in every single
total agents. And then in every single
batch of data, every single rollout
batch of data, every single rollout
batch, we get 4096 agents worth of data.
batch, we get 4096 agents worth of data.
And while half of the CPUs are
And while half of the CPUs are
simulating data, you're stepping the
simulating data, you're stepping the
other half of them. So the CPUs are
other half of them. So the CPUs are
always going in the background uh during
always going in the background uh during
the rollout process. Then you collect
the rollout process. Then you collect
segments from all of those environments.
segments from all of those environments.
So if you're going to collect 64 steps
So if you're going to collect 64 steps
from each environment, that is a total
from each environment, that is a total
batch size of uh 500,000ish.
batch size of uh 500,000ish.
And then we go over that 500,000 steps
And then we go over that 500,000 steps
of experience uh with PO plus our
of experience uh with PO plus our
modifications using a mini batch size of
modifications using a mini batch size of
somewhere between 16 and 32,000.
somewhere between 16 and 32,000.
And we usually just do one pass of it.
And we usually just do one pass of it.
That whole process uh collects us and
That whole process uh collects us and
trains on well for breakout it's like
trains on well for breakout it's like
three uh it's like three million steps
three uh it's like three million steps
per second. Most of the fast
per second. Most of the fast
environments are three million steps per
environments are three million steps per
second. We have some larger policies,
second. We have some larger policies,
harder environments that we can get over
harder environments that we can get over
1 million. And then the uh the really
1 million. And then the uh the really
tough environments with like bigger
tough environments with like bigger
policies are going to be between 300 and
policies are going to be between 300 and
700,000
700,000
per GPU. And then multiGPU like neural
per GPU. And then multiGPU like neural
MMO 3, which is probably the hardest, we
MMO 3, which is probably the hardest, we
get like 400,000 steps per second on one
get like 400,000 steps per second on one
GPU. We get up to like 2.2 million steps
GPU. We get up to like 2.2 million steps
per second on six GPU.
Have people tried mixture of experts
Have people tried mixture of experts
type things? Why would you do that?
The policies are tiny. There's not
The policies are tiny. There's not
really any reason to make like a larger
really any reason to make like a larger
number of tiny networks just
number of tiny networks just
computationally inefficient.
computationally inefficient.
Like the main problem with uh efficiency
Like the main problem with uh efficiency
right in RL is that the like really
right in RL is that the like really
small policies are hard to saturate
small policies are hard to saturate
because you get data transfer bound
bound. No, it's not that it's CPU bound.
bound. No, it's not that it's CPU bound.
Okay. It's that so really small networks
Okay. It's that so really small networks
are hard to saturate because
are hard to saturate because
you're like you're spending more time
you're like you're spending more time
moving data around than you are actually
moving data around than you are actually
multiplying matrices because the
multiplying matrices because the
matrices are not that big.
like it's actually it's pretty hard to
like it's actually it's pretty hard to
get near optimal utilization
get near optimal utilization
um or linear scaling let's say below
um or linear scaling let's say below
like
like
mid uh mid singledigit million
mid uh mid singledigit million
parameters.
parameters.
No, not because the batch size is tiny.
No, not because the batch size is tiny.
Even if you give it a massive batch
Even if you give it a massive batch
size, you still get bound. You still get
size, you still get bound. You still get
like data transfer bound.
Like you can do the math on what we
Like you can do the math on what we
should be able to be pushing here,
should be able to be pushing here,
right? Because like a 128 hidden dim
right? Because like a 128 hidden dim
LSTM should be 64 times faster than a uh
LSTM should be 64 times faster than a uh
I believe it should be 64 times faster
I believe it should be 64 times faster
than a,024 hidden dim LSTM. We can train
than a,024 hidden dim LSTM. We can train
a,024 hidden dim at a million steps per
a,024 hidden dim at a million steps per
second. So based on that, we should be
second. So based on that, we should be
able to hit like 60s something million
able to hit like 60s something million
steps per second uh with a tiny LSTM.
steps per second uh with a tiny LSTM.
You just never get that. I think that
You just never get that. I think that
the most we've ever seen uh we got
the most we've ever seen uh we got
something to hit like 15 million steps
something to hit like 15 million steps
per second with gigantic batch sizes.
per second with gigantic batch sizes.
You just get data transfer bound
or not even necessarily data transfer
or not even necessarily data transfer
bound like it's not even just CPU to
bound like it's not even just CPU to
GPU. It can just be moving stuff into
GPU. It can just be moving stuff into
them like caches and spinning up
them like caches and spinning up
kernels. You get overhead bound by all
kernels. You get overhead bound by all
sorts of types of overhead.
Well, we got this section done.
Well, we got this section done.
I got to write the infrastructure
I got to write the infrastructure
section and probably one more. Let me
section and probably one more. Let me
use a restroom real quick and I will be
use a restroom real quick and I will be
right back. Also, can I just say it's
right back. Also, can I just say it's
pretty dang nice the uh the growth
pretty dang nice the uh the growth
today. Like
today. Like
this is a ridiculous.
this is a ridiculous.
plus like 700 followers is awesome. All
plus like 700 followers is awesome. All
right, I'll be right back.
Did you see the mega kernel for llama?
Did you see the mega kernel for llama?
They only have one kernel for the entire
They only have one kernel for the entire
llama. So less overhead. Yeah. So that
llama. So less overhead. Yeah. So that
I've been thinking of that for
I've been thinking of that for
reinforcement learning and uh it would
reinforcement learning and uh it would
be massively more important for RL than
be massively more important for RL than
for everything else. It's you'd have to
for everything else. It's you'd have to
build something to gen the kernel though
build something to gen the kernel though
because like you need different types of
because like you need different types of
networks
networks
that would actually probably let you hit
that would actually probably let you hit
like if somebody wanted to do that in
like if somebody wanted to do that in
puffer lib I would be all about it
puffer lib I would be all about it
because we could probably hit 50 million
because we could probably hit 50 million
steps per second training. It would be
steps per second training. It would be
insane.
It actually wouldn't be anywhere near as
It actually wouldn't be anywhere near as
hard, I don't think, either because like
hard, I don't think, either because like
the kernels are pretty simple.
the kernels are pretty simple.
We actually we have stuff that would
We actually we have stuff that would
basically work as kernels. They're just
basically work as kernels. They're just
they're kind of shitty. You'd have to do
they're kind of shitty. You'd have to do
like you'd have to actually get better
like you'd have to actually get better
maples. Probably just matt moles.
maples. Probably just matt moles.
Everything else is pretty easy. But like
Everything else is pretty easy. But like
I wrote this. Let me show you.
We actually used this on the website. So
We actually used this on the website. So
if you wondered how we got neural nets
if you wondered how we got neural nets
to run on your in your browser, it's
to run on your in your browser, it's
this file. It's a tiny little file.
this file. It's a tiny little file.
Twothirds of it is just wrappers. So
Twothirds of it is just wrappers. So
like it's easier for a UI, but like we
like it's easier for a UI, but like we
have a linear layer. We've got a conf 3D
have a linear layer. We've got a conf 3D
con. We've got an LSTM. got layer norm
con. We've got an LSTM. got layer norm
embeddings one hot we've got like all
embeddings one hot we've got like all
the common operations in just brain
the common operations in just brain
deadad simple C
like you could technically like what we
like you could technically like what we
do now is we wire these all together and
do now is we wire these all together and
that's your network right for your
that's your network right for your
forward pass you could do this add a
forward pass you could do this add a
backwards convert it to CUDA make a less
backwards convert it to CUDA make a less
stupid matmo uh matmo that's actually
stupid matmo uh matmo that's actually
tiled correctly and all that and Like
tiled correctly and all that and Like
maybe we get 50 million steps per
maybe we get 50 million steps per
second. I don't know.
I don't know what that is.
If you have an idea on how to do this,
If you have an idea on how to do this,
by all means, like 50 million step per
by all means, like 50 million step per
second training would be crazy cool.
second training would be crazy cool.
thoughtless.
If you have an idea on how to do this,
If you have an idea on how to do this,
like awesome.
Why specifically in RL is important
Why specifically in RL is important
because the models are smaller and when
because the models are smaller and when
the models are smaller the overhead
the models are smaller the overhead
matters more because they run faster. So
matters more because they run faster. So
smaller chunks of overhead matter more.
That's why
Most of the stuff is bandwidthbound.
It's not specifically device bandwidth
It's not specifically device bandwidth
though that is sometimes a factor.
though that is sometimes a factor.
I think it's more so that you're just
I think it's more so that you're just
like
I to be fair I don't really know. It
I to be fair I don't really know. It
could be kernel launch overhead. It
could be kernel launch overhead. It
could technically be just like it it
could technically be just like it it
shouldn't be GPU GPU bandwidth. Like it
shouldn't be GPU GPU bandwidth. Like it
shouldn't be memory moving around in the
shouldn't be memory moving around in the
GPU because that's really fast. But for
GPU because that's really fast. But for
all I know, it's just done incredibly
all I know, it's just done incredibly
inefficiently right now.
Uh replay doesn't matter because so all
Uh replay doesn't matter because so all
of our data gets like we don't take the
of our data gets like we don't take the
data off the GPU anyways. Once it goes
data off the GPU anyways. Once it goes
on the GPU, it gets it stays on the GPU.
Like the entire PO batch just fits on
Like the entire PO batch just fits on
the GPU and we don't come close to
the GPU and we don't come close to
running out of memory ever. It's just
running out of memory ever. It's just
fine.
Why do you think LM people are using
Why do you think LM people are using
chain of thought
chain of thought
and not any of the classic deep RL
and not any of the classic deep RL
approaches? They are doing RL now.
Oh, LLM people are straight up doing RL
Oh, LLM people are straight up doing RL
now.
now.
Have you seen Ken Stanley's newest paper
Have you seen Ken Stanley's newest paper
on fractured entanglement entangled
on fractured entanglement entangled
representations? So I have a friend and
representations? So I have a friend and
a colleague at MIT who worked on that
a colleague at MIT who worked on that
paper with him. Uh he does good work. I
paper with him. Uh he does good work. I
haven't had a chance to read the
haven't had a chance to read the
fractured entangled representations
fractured entangled representations
paper yet though so I cannot comment
paper yet though so I cannot comment
yet.
URL but it's completely different policy
URL but it's completely different policy
network.
network.
They use
They use
a lot of to
actually they use no data at all
actually they use no data at all
compared to uh compared to what we do in
compared to uh compared to what we do in
RL. Like we do training runs 20 times
RL. Like we do training runs 20 times
the pre 20 times the data size of the
the pre 20 times the data size of the
pre-training of GPT4 and we do it on one
pre-training of GPT4 and we do it on one
server.
server.
So by our by RL standards they have no
So by our by RL standards they have no
data.
references. Oh, that's the test time the
references. Oh, that's the test time the
test time compute thing. Yeah,
that's separate. I that is separate.
that's separate. I that is separate.
You can do uh you can do reinforcement
You can do uh you can do reinforcement
learning with or without test time
learning with or without test time
compute.
They're kind of separate.
Where am I from? Originally,
Where am I from? Originally,
Connecticut.
Maybe RL environments are more type one
Maybe RL environments are more type one
thing. Type two. Nope. Depends
thing. Type two. Nope. Depends
completely on the environment.
Depends completely on the environment.
Depends completely on the environment.
Some are one, some are the other, some
Some are one, some are the other, some
are both.
What are some cool applied domains that
What are some cool applied domains that
haven't been explored enough? So, the
haven't been explored enough? So, the
one that I really want to look into,
one that I really want to look into,
uh, there are a few. So, I I think that
uh, there are a few. So, I I think that
the most practical one on the business
the most practical one on the business
side for us at the moment and the one
side for us at the moment and the one
I'm spending the most time on is uh,
I'm spending the most time on is uh,
drones.
drones.
lots of applications there.
lots of applications there.
Uh the one I'm more interested in longer
Uh the one I'm more interested in longer
term is like hardcore material
term is like hardcore material
simulation.
simulation.
I want to see if there's some way that
I want to see if there's some way that
we can do like like molecule level or
we can do like like molecule level or
atomic level simulation
atomic level simulation
um while cutting some corners and
um while cutting some corners and
keeping it fast.
Are you creating a company? Pufferai is
Are you creating a company? Pufferai is
a company. So, all of my stuff is free
a company. So, all of my stuff is free
and open source, but um we do have
and open source, but um we do have
contracts with companies that want help
contracts with companies that want help
with their reinforcement learning needs.
with their reinforcement learning needs.
We do like both basic service on uh
We do like both basic service on uh
puffer liib and surrounding features
and uh we also do bigger things than
and uh we also do bigger things than
that.
What do you mean by material science?
What do you mean by material science?
Where is RL involved?
Where is RL involved?
That's what we're trying to figure out,
That's what we're trying to figure out,
right? Most RL problems look like fiddly
right? Most RL problems look like fiddly
optimization problems.
optimization problems.
I want to see if we can cast like
I want to see if we can cast like
materials discovery or testing or
materials discovery or testing or
something uh as an interactive basically
something uh as an interactive basically
as an interactive process with a sim.
No cuspi. Nope.
Oh, cool.
Oh, cool.
That's awesome.
You have Yan Lun on the board here.
You have Yan Lun on the board here.
Funny.
Is this a new thing?
Oh, it's like pretty new.
Oh, climate change.
Oh, climate change.
What are they doing? Climate change.
Carbon capture and storage. Yeah,
there's so much more cool stuff you can
there's so much more cool stuff you can
do than that.
uh superconductors
uh superconductors
like actually viable space like space uh
like actually viable space like space uh
space age materials.
space age materials.
They're like so many freaking things.
They're like so many freaking things.
Yeah, superconductors would be the
Yeah, superconductors would be the
really obvious awesome one.
RL for discovering a room temperature
RL for discovering a room temperature
superconductor would be awesome.
superconductor would be awesome.
I don't know how that we could do that,
I don't know how that we could do that,
but
I'm trying to think if that would be the
I'm trying to think if that would be the
highest impact thing you could do or
highest impact thing you could do or
not. Would it be
like if you had to pick like one problem
like if you had to pick like one problem
to solve to make the most sci-fi
to solve to make the most sci-fi
possible thing work?
possible thing work?
Would it be that? I don't know.
Generally, I like things that make I
Generally, I like things that make I
like things that make sci-fi tech real.
That is what I like.
Oh. Oh, perfect.
be a trillion plus dollar company. Yeah,
be a trillion plus dollar company. Yeah,
but like so is humanoid robotics and
but like so is humanoid robotics and
that's like I don't find humanoid
that's like I don't find humanoid
robotics particularly exciting at all.
You know what actually in the robotics
You know what actually in the robotics
space I find way cooler than humanoid
space I find way cooler than humanoid
robotics? like those industrial arms
robotics? like those industrial arms
that are used in assembling stuff. I
that are used in assembling stuff. I
want to see those operating faster than
want to see those operating faster than
you can visually track with your eyes.
That would be cool.
Just put a warning sign that's like do
Just put a warning sign that's like do
not approach the robot with a icon of a
not approach the robot with a icon of a
blender on it. Robot at work.
Do not approach.
How many hours in Factorio though? Zero.
How many hours in Factorio though? Zero.
That game does not appeal to me at all.
That game does not appeal to me at all.
I played Satisfactory for a little bit
I played Satisfactory for a little bit
and then got bored.
and then got bored.
Yeah, that kind of nerd snipe does not
Yeah, that kind of nerd snipe does not
work on me.
It just doesn't work. The thing that
It just doesn't work. The thing that
really got me good for like a long time
really got me good for like a long time
was optimizing MMO builds. That got me
was optimizing MMO builds. That got me
for a long time.
for a long time.
But not none of the manufacturing like
But not none of the manufacturing like
logistics sims have
logistics sims have
Welcome, Finn. Oh, yeah. You were asking
Welcome, Finn. Oh, yeah. You were asking
about applications.
about applications.
Finn is working on the uh the drone
Finn is working on the uh the drone
stuff with us. Ride Path of Exile.
stuff with us. Ride Path of Exile.
Yeah, I you know, I never got into it. I
Yeah, I you know, I never got into it. I
don't really like that it's
don't really like that it's
I don't really want a live service game
I don't really want a live service game
of that style that's seasonal. I
of that style that's seasonal. I
actually the last few days I've been
actually the last few days I've been
playing Grim Dawn. I've been having more
playing Grim Dawn. I've been having more
fun with that.
I played Diablo I for a little bit and
I played Diablo I for a little bit and
just didn't like it at all.
Do you recommend doing a PhD? I only
Do you recommend doing a PhD? I only
recommend doing a PhD to people who know
recommend doing a PhD to people who know
that they want to do a PhD.
ultimate game for optimizing builds.
ultimate game for optimizing builds.
Probably better if you don't get into
Probably better if you don't get into
it. The thing that I don't like about
it. The thing that I don't like about
the build optimization stuff in uh in
the build optimization stuff in uh in
those is that they
those is that they
like they kind of are all boring to
like they kind of are all boring to
play. It doesn't like you're kind of
play. It doesn't like you're kind of
just optimizing a DPS number and like
just optimizing a DPS number and like
resistances and stuff. You're not really
resistances and stuff. You're not really
The stuff I was into.
The stuff I was into.
The MMO I played was basically it was
The MMO I played was basically it was
fantasy chess on steroids. So like the
fantasy chess on steroids. So like the
number of interactions and the number of
number of interactions and the number of
things you had to consider like you
things you had to consider like you
didn't have something as crazy as the
didn't have something as crazy as the
passive tree and like socketing gems and
passive tree and like socketing gems and
all that stuff, but like there was a lot
all that stuff, but like there was a lot
of complexity because the gameplay
of complexity because the gameplay
itself had a lot more complexity
based on my experience.
based on my experience.
So, uh,
So, uh,
I had a very supportive adviser who let
I had a very supportive adviser who let
me do whatever. I was at what I would
me do whatever. I was at what I would
say is the best institution to do a PhD.
say is the best institution to do a PhD.
And there's still just like lots of
And there's still just like lots of
things that are not great. Um, academia
things that are not great. Um, academia
is not the place to have fancy new
is not the place to have fancy new
ideas. As it turns out, like the place
ideas. As it turns out, like the place
to do fancy new things is
to do fancy new things is
in like a financially comfortable,
in like a financially comfortable,
independent position,
independent position,
pretty much doing your own thing, like
pretty much doing your own thing, like
without any other strings attached.
without any other strings attached.
Academia does not like new ideas that do
Academia does not like new ideas that do
not fit the mold of how academia likes
not fit the mold of how academia likes
to see their new ideas play out. They do
to see their new ideas play out. They do
not like it when the solution to their
not like it when the solution to their
problem is to rewrite all their garbage
problem is to rewrite all their garbage
code and not do any like fancy new
code and not do any like fancy new
methods or write down any math. Um, they
methods or write down any math. Um, they
don't like it when you force them to
don't like it when you force them to
think about new problems that don't
think about new problems that don't
cleanly fit their math. Like there are a
cleanly fit their math. Like there are a
lot of just bad annoying things. Oh, and
lot of just bad annoying things. Oh, and
the paper writing process sucks.
the paper writing process sucks.
So there is a fair bit of overhead and a
So there is a fair bit of overhead and a
fair bit of constraint that you probably
fair bit of constraint that you probably
would not see from the outside.
So that is the negative. Positive is
So that is the negative. Positive is
generally speaking I still did have 5
generally speaking I still did have 5
years to explore the area of the problem
years to explore the area of the problem
that I wanted to explore and I developed
that I wanted to explore and I developed
a lot of the core stuff that now makes
a lot of the core stuff that now makes
it possible to build puffer lip. So, I
it possible to build puffer lip. So, I
would not have been able to build Puffer
would not have been able to build Puffer
Lib without doing my PhD. That is for
Lib without doing my PhD. That is for
sure.
Who's Yaken and what's he been up to?
Who's Yaken and what's he been up to?
Uh, he is a tech guy who posts a lot a
Uh, he is a tech guy who posts a lot a
lot a lot a lot of things on X.
lot a lot a lot of things on X.
Like he kind of just posts constantly
Like he kind of just posts constantly
and he has a large following and he does
and he has a large following and he does
uh some quite cool tech stuff actually,
uh some quite cool tech stuff actually,
but he's mostly just memes. So, he was
but he's mostly just memes. So, he was
doing some RL and saw the drone stuff we
doing some RL and saw the drone stuff we
were doing. I've been chatting with him.
were doing. I've been chatting with him.
Been cool.
Good VFS, VFX, and aspects and the
Good VFS, VFX, and aspects and the
random loot. It becomes satisfying.
Yeah, nothing beats gameplay loop
Yeah, nothing beats gameplay loop
though. The most important thing for
though. The most important thing for
like the most important thing for games
like the most important thing for games
that you put a lot of time into is the
that you put a lot of time into is the
gameplay loop. So, for me, like playing
gameplay loop. So, for me, like playing
a like an A RPG casually, like when I'm
a like an A RPG casually, like when I'm
fried and I just don't want to think
fried and I just don't want to think
about anything, that's cool. But like
about anything, that's cool. But like
the games that I played be like the
the games that I played be like the
games I sunk a ton of hours into like
games I sunk a ton of hours into like
one of them was uh old school Runescape
one of them was uh old school Runescape
and like you wouldn't look at it from
and like you wouldn't look at it from
the outside but it has probably the most
the outside but it has probably the most
satisfying mechanics of any game I've
satisfying mechanics of any game I've
ever played at the high level.
It's just like really good.
There's got to at least be something I
There's got to at least be something I
think there.
Actually, the mechanic system of
Actually, the mechanic system of
Runescape was so good that it uh it
Runescape was so good that it uh it
actually was the basis of a lot of what
actually was the basis of a lot of what
I did in neural MMO because it turns out
I did in neural MMO because it turns out
it's also a perfect match for
it's also a perfect match for
reinforcement learning.
How does doing a PhD compare to working
How does doing a PhD compare to working
in a research position at big tech? Uh,
in a research position at big tech? Uh,
if you can get a good research position
if you can get a good research position
at a top lab or company, you should do
at a top lab or company, you should do
that. If you can, the thing is usually
that. If you can, the thing is usually
you cannot get one of those without
you cannot get one of those without
doing a PhD.
It kind of depends because like
It kind of depends because like
a lot of the labs are not releasing
a lot of the labs are not releasing
enough stuff about their work these
enough stuff about their work these
days.
days.
At least in like 2019 though um when
At least in like 2019 though um when
they were actually writing blogs and
they were actually writing blogs and
stuff I I would say it was better.
stuff I I would say it was better.
Welcome Magader.
What are the biggest advances in RL
What are the biggest advances in RL
since PO? Are the algorithms improving
since PO? Are the algorithms improving
uh outside of Puffer? No. With Puffer?
uh outside of Puffer? No. With Puffer?
Yes.
We have the article right here on X
We have the article right here on X
puffing up PO.
This is our research and this is what we
This is our research and this is what we
released with 300 that makes everything
released with 300 that makes everything
work well.
work well.
It is a generational leap in
It is a generational leap in
improvement.
Well,
Well,
missed a lot of a lot of things today.
missed a lot of a lot of things today.
Yeah, working on the article. It's
Yeah, working on the article. It's
almost done.
almost done.
Probably be 3,500 words.
make puffer lip standardized in
make puffer lip standardized in
academia. Uh we do work with a couple
academia. Uh we do work with a couple
labs.
labs.
Academia is very stubborn about
Academia is very stubborn about
everything.
I basically,
I basically,
if you want to think of it this way, I
if you want to think of it this way, I
spent five years in my PhD trying to
spent five years in my PhD trying to
like sell my work to academics.
like sell my work to academics.
And uh let's say I just don't do that
And uh let's say I just don't do that
anymore. I'm sick of it.
Like
you're trying to help people who don't
you're trying to help people who don't
want to be helped for free.
Make wall clock time competitions.
Make wall clock time competitions.
They'll say it's invalid because it
They'll say it's invalid because it
doesn't fit their holy grail of steps.
Like nano GPT is not an academic thing.
Like you want to know what actually
Like you want to know what actually
happened outside of robotics? Almost all
happened outside of robotics? Almost all
the RL labs just died. There are like
the RL labs just died. There are like
two or three that actually kind of still
two or three that actually kind of still
do some things and all the students now
do some things and all the students now
work on LLMs so that they can get jobs.
work on LLMs so that they can get jobs.
So
So
academia succeeded in killing their
academia succeeded in killing their
field completely.
field completely.
You said you are a doctor and not a
You said you are a doctor and not a
professor. Yes, a doctor is somebody who
professor. Yes, a doctor is somebody who
is who has a PhD. Uh a professor works
is who has a PhD. Uh a professor works
in a teaching position at a university.
in a teaching position at a university.
So
So
yeah,
what is the content of the second part
what is the content of the second part
blog post? You're looking at it here.
blog post? You're looking at it here.
Tons of stuff.
What made you give up?
What made you give up?
What do you mean? What made me give? Oh,
What do you mean? What made me give? Oh,
on that academic not suited
just a lot of the stuff that ACMIA cares
just a lot of the stuff that ACMIA cares
about and like
about and like
they've kind of dug themselves into a
they've kind of dug themselves into a
hole in reinforcement learning where
hole in reinforcement learning where
like the benchmarks that they've chosen
like the benchmarks that they've chosen
have gotten slower and slower over the
have gotten slower and slower over the
years. So like they basically they have
years. So like they basically they have
no compute because they don't have
no compute because they don't have
budget for GPUs and when they do have
budget for GPUs and when they do have
budget they buy the wrong stuff and then
budget they buy the wrong stuff and then
they manage it poorly so it's even
they manage it poorly so it's even
slower than it should be and then they
slower than it should be and then they
write like slow code so it's a hundred
write like slow code so it's a hundred
times slower than it should be and then
times slower than it should be and then
they take this like awful 100 to
they take this like awful 100 to
thousand times too slow code and then
thousand times too slow code and then
they run it on a benchmark that's also a
they run it on a benchmark that's also a
thousand times too slow and they just
thousand times too slow and they just
don't get anything done.
don't get anything done.
like it's a pure it's a pure engineering
like it's a pure it's a pure engineering
failure. And the thing is because they
failure. And the thing is because they
don't value engineering, even though
don't value engineering, even though
it's completely preventing them from
it's completely preventing them from
doing science, they don't care.
How did you even get into MIT, my guy?
How did you even get into MIT, my guy?
I'm 17 and I want to apply there. Uh,
I'm 17 and I want to apply there. Uh,
best of luck at that age. All I can tell
best of luck at that age. All I can tell
you is to work your ass off.
you is to work your ass off.
There is not a single portion of my life
There is not a single portion of my life
where I have worked harder than in high
where I have worked harder than in high
school trying to get into undergrad.
school trying to get into undergrad.
Initially, I did Stanford for undergrad
Initially, I did Stanford for undergrad
MIT for PhD. Uh I worked
MIT for PhD. Uh I worked
probably except for freshman year, I
probably except for freshman year, I
probably averaged 110 hours of work a
probably averaged 110 hours of work a
week throughout high school. It was
week throughout high school. It was
brutal.
The most I've done after that for like
The most I've done after that for like
even like a month or two
even like a month or two
uh is probably 90. And I don't work 90
uh is probably 90. And I don't work 90
now. Occasionally I'll work like an 80
now. Occasionally I'll work like an 80
90 hour week, but I don't work like 90
90 hour week, but I don't work like 90
hour a week months even.
Like when I'm really focused and working
Like when I'm really focused and working
on a new project, typically I will work
on a new project, typically I will work
a few consecutive 70 to 80 hour weeks
a few consecutive 70 to 80 hour weeks
building the thing and then I burn
building the thing and then I burn
myself out and I work a few weeks of
myself out and I work a few weeks of
like 40 to 50 hours.
Have you published first part of this
Have you published first part of this
blog? No, it's they'll probably go out
blog? No, it's they'll probably go out
at the same time.
at the same time.
How much overhead in coms is there?
How much overhead in coms is there?
So, it depends what layer you're talking
So, it depends what layer you're talking
about. hosted device can be anywhere
about. hosted device can be anywhere
from like 2% up to 50%.
from like 2% up to 50%.
It's usually
It's usually
somewhere between five and 20%
somewhere between five and 20%
overhead.
overhead.
Um
Um
the scaling is not perfect on multiGPU.
the scaling is not perfect on multiGPU.
We don't even need to do multiGPU for
We don't even need to do multiGPU for
most stuff because we solve it so fast.
most stuff because we solve it so fast.
So we don't get perfect linear scaling
So we don't get perfect linear scaling
there either. I think we get like 400
there either. I think we get like 400
something thousand steps per second on a
something thousand steps per second on a
single 4090 and then we get like 2.2
single 4090 and then we get like 2.2
million on six 4090s. So we actually do
million on six 4090s. So we actually do
have a little bit of additional CPU
have a little bit of additional CPU
overhead in that configuration. So it's
overhead in that configuration. So it's
probably a little bit better than that.
probably a little bit better than that.
And then I we haven't tried multiGPU for
And then I we haven't tried multiGPU for
uh environments that are like already 3
uh environments that are like already 3
million steps per second. I'd imagine
million steps per second. I'd imagine
that wouldn't scale linearly.
that wouldn't scale linearly.
110 hours a week is madness.
110 hours a week is madness.
like flexing so fast.
like flexing so fast.
Yeah. So, basically what you're seeing
Yeah. So, basically what you're seeing
here, right, is like when you work a
here, right, is like when you work a
level of intensity for a while, like
level of intensity for a while, like
even when I'm working at a very small
even when I'm working at a very small
fraction of that, it will look like, oh
fraction of that, it will look like, oh
yeah, this guy's very focused. The thing
yeah, this guy's very focused. The thing
is, it's just like I've done I've done
is, it's just like I've done I've done
way more than that, right?
It's like the same thing where like, you
It's like the same thing where like, you
know, I went and I ran my 50k and I hit
know, I went and I ran my 50k and I hit
my thousand total powerlifting. So, it's
my thousand total powerlifting. So, it's
like people are always going to be like,
like people are always going to be like,
"Oh yeah, that guy's relatively fit even
"Oh yeah, that guy's relatively fit even
when I've been totally slacking for like
when I've been totally slacking for like
months, right?
months, right?
That's honestly the best way. Like, if
That's honestly the best way. Like, if
you want to get like if you want to be
you want to get like if you want to be
able to be that like a certain level of
able to be that like a certain level of
good, you have to just get way better
good, you have to just get way better
than that for at least a short period of
than that for at least a short period of
time that you can't even maintain.
You can apparently get super linear
You can apparently get super linear
scaling with multiGPU.
scaling with multiGPU.
I don't think you can in reinforcement
I don't think you can in reinforcement
learning.
learning.
I think that makes sense for um large
I think that makes sense for um large
models where Yeah. You know what that
models where Yeah. You know what that
is? That's got to be for large models
is? That's got to be for large models
where that all the weights don't fit on
where that all the weights don't fit on
one GPU, right? So if you have to keep
one GPU, right? So if you have to keep
loading the weights on and off device
loading the weights on and off device
for each part of the model, right? It's
for each part of the model, right? It's
going to be slower than if every GPU has
going to be slower than if every GPU has
like the same part of the model. That's
like the same part of the model. That's
just pipeline parallelism. It's not
just pipeline parallelism. It's not
applicable to reinforcement learning.
applicable to reinforcement learning.
Our models are too small for that to
Our models are too small for that to
matter.
in general scaling with multiGPU.
in general scaling with multiGPU.
Yeah. So I mean the thing I just
Yeah. So I mean the thing I just
described would be how you would do it,
described would be how you would do it,
right?
right?
It wouldn't really work in RL.
I think it's kind of trivial even like
I think it's kind of trivial even like
pretty much you get you should get super
pretty much you get you should get super
linear scaling as soon as your weights
linear scaling as soon as your weights
don't fit on one GPU maybe even earlier
don't fit on one GPU maybe even earlier
because you know you want to have um
because you know you want to have um
ideally you don't want to have batch
ideally you don't want to have batch
size on.
Need more examples from importable use
Need more examples from importable use
case.
case.
Can do that.
What was the hardest thing when you
What was the hardest thing when you
built Puffer Lib?
built Puffer Lib?
Um,
there's a lot of stuff.
there's a lot of stuff.
Like in retrospect, each individual
Like in retrospect, each individual
thing seems pretty simple, but like it
thing seems pretty simple, but like it
simple doesn't mean easy.
Honestly, a lot of it was just a
Honestly, a lot of it was just a
perspective shift. Like Puffer Lib kind
perspective shift. Like Puffer Lib kind
of takes every single thing in the field
of takes every single thing in the field
and flips it on its head. So
and flips it on its head. So
even if the final solution here is
even if the final solution here is
simple, it's like it's such a radical
simple, it's like it's such a radical
departure from everything that's out
departure from everything that's out
there that it like figuring out how to
there that it like figuring out how to
do everything this way was just very
do everything this way was just very
difficult. Like it went through many
difficult. Like it went through many
many iterations. Let's say we went
many iterations. Let's say we went
through many iterations. We started off
through many iterations. We started off
just having these emulation wrappers
just having these emulation wrappers
that made it easier to work with um
that made it easier to work with um
complicated environments and improved a
complicated environments and improved a
little bit of performance there. And
little bit of performance there. And
then we did the vectorzation thing that
then we did the vectorzation thing that
worked on top of that to get like up to
worked on top of that to get like up to
10x speed up, right? And then I started
10x speed up, right? And then I started
doing Syon environments and that very
doing Syon environments and that very
quickly got us to like, you know, 100x
quickly got us to like, you know, 100x
speed up, right? And then we did we
speed up, right? And then we did we
improved our infrastructure. We went to
improved our infrastructure. We went to
C. We did like large batch stuff. We did
C. We did like large batch stuff. We did
algorithmic optimizations, PyTorch
algorithmic optimizations, PyTorch
optimizations, and that got us to a
optimizations, and that got us to a
thousandx. And then now where we're
thousandx. And then now where we're
sitting here, like it's a,000x plus for
sitting here, like it's a,000x plus for
a lot of stuff.
How do you debug RL algorithms? Well,
How do you debug RL algorithms? Well,
with Puffer Lib, the fact is we have so
with Puffer Lib, the fact is we have so
many different environments,
many different environments,
right, that we like we can get
right, that we like we can get
performance on a ton of different tasks.
performance on a ton of different tasks.
So when we introduce something that
So when we introduce something that
breaks something else,
breaks something else,
we can see pretty quickly. And when we
we can see pretty quickly. And when we
introduce a change, we can see very
introduce a change, we can see very
quickly whether it just helps on one
quickly whether it just helps on one
specific environment or whether it helps
specific environment or whether it helps
on like most of them. We also have
on like most of them. We also have
environments that we trust more than
environments that we trust more than
others where like if something does well
others where like if something does well
on this, it tends to do well on
on this, it tends to do well on
everything else versus there are other
everything else versus there are other
ones where, you know, if you just train
ones where, you know, if you just train
something on Breakout, it's usually
something on Breakout, it's usually
better on Breakout is better everywhere,
better on Breakout is better everywhere,
but not always. It's not super
but not always. It's not super
generalizable. If you get it to work on
generalizable. If you get it to work on
mazes, it's usually good everywhere. If
mazes, it's usually good everywhere. If
you get it to work on neural MMO 3, it's
you get it to work on neural MMO 3, it's
usually good everywhere.
How do you debug why an agent learned
How do you debug why an agent learned
well or didn't learn well? You look at
well or didn't learn well? You look at
your data, observations, rewards,
your data, observations, rewards,
and then you watch the policy.
and then you watch the policy.
It's almost always an error in
It's almost always an error in
observations and rewards, though.
I have a debugging checklist on the docs
I have a debugging checklist on the docs
page.
most difficult part stage of solving RL
most difficult part stage of solving RL
problems for beginner. Lack of
problems for beginner. Lack of
programming maturity
programming maturity
just generally not being comfortable
just generally not being comfortable
enough writing clean, simple, low-level
enough writing clean, simple, low-level
code without breaking things.
code without breaking things.
What are your favorite RL labs? What are
What are your favorite RL labs? What are
some good ones to keep up with? There
some good ones to keep up with? There
really aren't that many right now. Um,
really aren't that many right now. Um,
Flair is Jacob Forers's lab. They're
Flair is Jacob Forers's lab. They're
doing good stuff. Uh, and then Eugene's
doing good stuff. Uh, and then Eugene's
lab has a few things at NYU.
lab has a few things at NYU.
I think Julian I don't know if Julian's
I think Julian I don't know if Julian's
lab is still really doing much RL. They
lab is still really doing much RL. They
have a little bit.
Let me think who else. I think there are
Let me think who else. I think there are
still a few people at like Stanford and
still a few people at like Stanford and
Berkeley who are doing RL stuff that's
Berkeley who are doing RL stuff that's
not robotics, but you don't hear about
not robotics, but you don't hear about
stuff very often.
And I guess we'll see if there are any
And I guess we'll see if there are any
new ones like any new good things at
new ones like any new good things at
RLC.
Benjamin for Princeton
Benjamin for Princeton
Eisenbach.
Eisenbach.
Uh I'm trying to remember what what are
Uh I'm trying to remember what what are
the latest things that he's done.
Yeah.
Oh, contrastive.
Oh, contrastive.
I haven't really looked into contrastive
I haven't really looked into contrastive
at all
at all
for RL. I don't know if there's
for RL. I don't know if there's
something there. Maybe
But not super practical. Yeah.
You know George Hotz? Yeah. I uh I
You know George Hotz? Yeah. I uh I
bought the tiny boxes from him, right?
ended up chatting with them a whole
ended up chatting with them a whole
bunch because we had a bunch of problems
bunch because we had a bunch of problems
getting them to work initially, but uh
getting them to work initially, but uh
they work quite well now.
You think it is a good idea to make
You think it is a good idea to make
general purpose observation space
general purpose observation space
converter?
converter?
It's hard to do that fast.
It's hard to do that fast.
Very hard to do that fast. That is the
Very hard to do that fast. That is the
big issue.
You know, one of the things I was
You know, one of the things I was
thinking that we could do, I don't know
thinking that we could do, I don't know
if it would work, but we could kind of
if it would work, but we could kind of
just define the observation space to be
just define the observation space to be
like, I don't know, some upper bound. It
like, I don't know, some upper bound. It
could be like 256 or something. And then
could be like 256 or something. And then
we could just put all the flat data from
we could just put all the flat data from
many of the different arcade ends into
many of the different arcade ends into
that and just see if we can train on
that and just see if we can train on
with padded observations and actions.
with padded observations and actions.
See if we can just train on a bunch of
See if we can just train on a bunch of
different environments with the same
different environments with the same
policy.
policy.
I think we could force it to work now. I
I think we could force it to work now. I
honestly do.
honestly do.
Super cool and doing amazing work.
Super cool and doing amazing work.
Thanks for this agreement and open
Thanks for this agreement and open
sourcing this. Thank you very much.
I've learned a lot from RL from your
I've learned a lot from RL from your
articles and live stream faster than the
articles and live stream faster than the
knowledge I gain from books out content
knowledge I gain from books out content
and then I search and come across it.
and then I search and come across it.
I'm working on it. I this here is going
I'm working on it. I this here is going
to be like a 3,500 word article plus the
to be like a 3,500 word article plus the
other one. So 5,500 words of uh intro
other one. So 5,500 words of uh intro
material with a ton of links.
Let's try to this fixed bound fixed
Let's try to this fixed bound fixed
length
length
general observator
online.
So the thing with that, right, is I
So the thing with that, right, is I
don't know if
you'd have to do some experiments.
you'd have to do some experiments.
Basically, I don't know if you can learn
Basically, I don't know if you can learn
a useful shared representation.
a useful shared representation.
The data is formatted that way.
It's technically possible that you could
It's technically possible that you could
learn it in like you can't learn it in
learn it in like you can't learn it in
the first layer. That's for sure. It's
the first layer. That's for sure. It's
possible you could learn it in later
possible you could learn it in later
layers, but we use really shallow
layers, but we use really shallow
policies and reinforcement learning most
policies and reinforcement learning most
of the
2.3K.
2.3K.
Heck yeah.
Okay.
5:44.
5:44.
I need to go in a couple of minutes here
I need to go in a couple of minutes here
for dinner.
for dinner.
We have a full draft of this.
And
where's the other one?
where's the other one?
Oh, yeah. A full draft for this.
Oh, yeah. A full draft for this.
So, what is it usually? Is it 250 words
So, what is it usually? Is it 250 words
a page? Usually,
I was aiming for about 20 pages of
I was aiming for about 20 pages of
content.
I ended up with like 21 22 pages
I ended up with like 21 22 pages
with the big heading. So, it's probably
with the big heading. So, it's probably
20 pages just about sped on.
How long was my quick start guide?
It doesn't show you. That's annoying.
It doesn't show you. That's annoying.
Substantially shorter though.
Substantially shorter though.
Maybe like 2K.
I don't even have time to read through
I don't even have time to read through
this once, I don't think, before uh
this once, I don't think, before uh
before dinner. So, I think I will just
before dinner. So, I think I will just
call it Anybody have any questions?
call it Anybody have any questions?
Obviously, I also have to fix the doc
Obviously, I also have to fix the doc
section or the section of the docs on
section or the section of the docs on
the web page that I'm linking people to
the web page that I'm linking people to
or like add a little bit to that. I will
or like add a little bit to that. I will
do that.
do that.
But otherwise, I think this is like a
But otherwise, I think this is like a
full full draft. I'll edit this
full full draft. I'll edit this
tomorrow. I actually have a flight to
tomorrow. I actually have a flight to
edit this on soon, so I'll probably just
edit this on soon, so I'll probably just
do that later this week.
do that later this week.
I don't want to spend too many days in a
I don't want to spend too many days in a
row writing. So, we'll probably be back
row writing. So, we'll probably be back
running experiments tomorrow. We will
running experiments tomorrow. We will
see. Um, but other than that, yeah,
see. Um, but other than that, yeah,
thank you for tuning in, folks.
thank you for tuning in, folks.
Hopefully, you all enjoy this once it's
Hopefully, you all enjoy this once it's
ready.
ready.
Puffer.ai for all the things. If you
Puffer.ai for all the things. If you
want to help me out for free, star the
want to help me out for free, star the
repo. Just star. It really helps
repo. Just star. It really helps
the puffer star.
the puffer star.
And other than that
And other than that
can join the discord to get involved
can join the discord to get involved
with development and follow me on extra
with development and follow me on extra
more. We do the importable example
more. We do the importable example
today.
today.
Not really cuz I got to run for dinner.
Not really cuz I got to run for dinner.
It is about to be 6:00 p.m. here.
It is about to be 6:00 p.m. here.
If you have um
wait generalized general observation
wait generalized general observation
space is different from importable
space is different from importable
stuff.
I don't know if I'm going to be working
I don't know if I'm going to be working
this evening.
I'm not working crazy hours this uh for
I'm not working crazy hours this uh for
the next little bit here here
on night live stream. Yeah, I don't I
on night live stream. Yeah, I don't I
don't really know if I want to do
don't really know if I want to do
another session tonight to be honest
another session tonight to be honest
with you.
If you actually message me though, I'll
If you actually message me though, I'll
tell you what though. If you actually
tell you what though. If you actually
message me in the Discord because I
message me in the Discord because I
wasn't going to do docs tomorrow. If you
wasn't going to do docs tomorrow. If you
actually message me like specifically
actually message me like specifically
what needs to be done, I can do it on
what needs to be done, I can do it on
the stream tomorrow
because you haven't told me what to do
because you haven't told me what to do
with the importable like what more to do
with the importable like what more to do
with the importable example or like what
with the importable example or like what
you're still confused about that it
you're still confused about that it
doesn't cover.
I'm very much I'd like to improve it,
I'm very much I'd like to improve it,
but I need to know how. Okay, I got to
but I need to know how. Okay, I got to
run though.
