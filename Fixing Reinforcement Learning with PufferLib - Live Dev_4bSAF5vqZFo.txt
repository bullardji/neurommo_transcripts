Kind: captions
Language: en
hello how's it
hello how's it
going has been busy lately with some
going has been busy lately with some
stuff crazy
stuff crazy
week I really only have one goal for
today um the plan for today make one
today um the plan for today make one
quick tweet on the Pokemon stuff so we
quick tweet on the Pokemon stuff so we
uh keep go keep that going
uh keep go keep that going
and then all I want to do
and then all I want to do
today
today
is
is
fix long Horizons in the new algorithm
fix long Horizons in the new algorithm
that's all I want to do today it should
that's all I want to do today it should
be a relatively simple normalization
be a relatively simple normalization
problem I think I just was too tired
problem I think I just was too tired
yesterday to think about it correctly
yesterday to think about it correctly
that's all I want to do if we get that
that's all I want to do if we get that
done today I will be happy because that
done today I will be happy because that
will push us to where we can move on to
will push us to where we can move on to
a lot of new stuff with puffer I'll be
a lot of new stuff with puffer I'll be
able to clear my mind of it and we'll be
able to clear my mind of it and we'll be
able to work on what I have planned for
able to work on what I have planned for
tomorrow all day and it'll be
tomorrow all day and it'll be
good we got some cool new people in uh
good we got some cool new people in uh
in the Discord as
well
interesting
e
e e
very nice let's get this tweet out uh
very nice let's get this tweet out uh
this has been doing pretty well this
this has been doing pretty well this
Pokemon stuff has been doing pretty well
Pokemon stuff has been doing pretty well
where's the blog
here I just want to pick
here I just want to pick
something something cool from
here for
what can you not
what can you not
uh oh my
uh oh my
gosh he included this as a
gosh he included this as a
graph this is so funny man why would you
graph this is so funny man why would you
do that that's so much
effort e
I'm trying to think which one would do
I'm trying to think which one would do
better on X what do we
better on X what do we
think this is a cool
visual maybe this one
visual maybe this one
did Claude make that mermaid try I don't
did Claude make that mermaid try I don't
think so I think that guy did
it well I guess he no longer Anonymous
it well I guess he no longer Anonymous
David but
I think this one will do
I think this one will do
[Music]
[Music]
maybe we do this one today and then
maybe we do this one today and then
we'll do the next one on
Monday yeah cuz then people will see
Monday yeah cuz then people will see
it's a different tweet clearly
it's important to get this
right
e e
I'm trying to think what else
I don't think anybody's even done this
oh this is a cool little
oh this is a cool little
uh there's a cool post but it doesn't
uh there's a cool post but it doesn't
have any
have any
results it's like hey this would be a
results it's like hey this would be a
cool thing to do it's a proposal
what's wrong with Twitter
spacing
e
e
e
e
e
e e
all right I think that's a nice po
all right I think that's a nice po
we'll see whether this CH style thing
we'll see whether this CH style thing
does decently or not but it's
cool cool some is actually in hyper pram
cool cool some is actually in hyper pram
optimization that's
cool
e
e
e e
all right cool I think that's uh
all right cool that
works and uh we will
works and uh we will
see we will see with this this not get
see we will see with this this not get
posted oh yeah it did
posted oh yeah it did
okay now let's talk about the research
okay now let's talk about the research
side stuff I want to do today I want to
side stuff I want to do today I want to
at least outline what I'm trying to do
at least outline what I'm trying to do
once cuz it's a little fiddly and
once cuz it's a little fiddly and
requires a lot of thinking so it's a lot
requires a lot of thinking so it's a lot
of me just staring at the screen um so I
of me just staring at the screen um so I
want to make sure that I do this at
want to make sure that I do this at
least
once and maybe this will help me get set
once and maybe this will help me get set
up for the day as
well so what I'm going to work on today
well so what I'm going to work on today
is part of a new algorithm I've been
is part of a new algorithm I've been
working on uh to improve over
working on uh to improve over
P that has been attempted like hundreds
P that has been attempted like hundreds
of times over the
of times over the
last seven years at this point six years
last seven years at this point six years
seven years and nobody's done it PPO is
seven years and nobody's done it PPO is
still King so I think we're actually
still King so I think we're actually
going to be able to improve on PO
going to be able to improve on PO
specifically we are replacing
specifically we are replacing
generalized Advantage estimation the
generalized Advantage estimation the
reason for that is generalized advantage
reason for that is generalized advantage
estimation requires two tunable hyper
estimation requires two tunable hyper
parameters and it locks you to learning
parameters and it locks you to learning
rewards over specific
rewards over specific
Horizons so and you can tune these
Horizons so and you can tune these
parameters to essentially say hey I care
parameters to essentially say hey I care
about rewards 10 steps out but then if
about rewards 10 steps out but then if
your environment is complex and as you
your environment is complex and as you
learn more it turns out that there are
learn more it turns out that there are
rewards that are 100 steps out you're
rewards that are 100 steps out you're
not really going to learn that so I want
not really going to learn that so I want
to replace J with something that doesn't
to replace J with something that doesn't
require Lambda and Gamma and has
require Lambda and Gamma and has
flexible adaptive Horizons what I've
flexible adaptive Horizons what I've
done for this is I have
done for this is I have
replaced the value function right which
replaced the value function right which
is typically just output from the neural
is typically just output from the neural
net right like you have your hidden
net right like you have your hidden
layers okay and then you output your
layers okay and then you output your
value like
value like
this and so you have hidden and then you
this and so you have hidden and then you
have value well what I've actually done
have value well what I've actually done
here is I make
oops I call this
oops I call this
VT VT +
VT VT +
1 and uh v t + 2 and then we do this
1 and uh v t + 2 and then we do this
going out and then we actually we also
going out and then we actually we also
output uh we also have
output uh we also have
BT standard
BT standard
deviation it's variational and it is
deviation it's variational and it is
predicted over the next many many many
predicted over the next many many many
time steps this is the new
time steps this is the new
algorithm uh well part of it so what
algorithm uh well part of it so what
what are we going to train this first of
what are we going to train this first of
all to well at least training this value
all to well at least training this value
function is uh relatively easy because
function is uh relatively easy because
we have standard deviation and because
we have standard deviation and because
we have t this is just a gaussian
we have t this is just a gaussian
nll on
nll on
b v b s d
b v b s d
and then
and then
rewards and you have to take in the
rewards and you have to take in the
whole reward Vector to do this but this
whole reward Vector to do this but this
is how you train your value function uh
is how you train your value function uh
and this is kind of cool because now
and this is kind of cool because now
you're using not just a single float
you're using not just a single float
you're using as many value functions
you're using as many value functions
forward as you predict at every time
forward as you predict at every time
step for training so this is quite
step for training so this is quite
powerful already but you still need to
powerful already but you still need to
write down the advantage function okay
write down the advantage function okay
and the advantage function is a little
and the advantage function is a little
bit trickier this is I've been working
bit trickier this is I've been working
on at the
on at the
moment because what I want to do uh is I
moment because what I want to do uh is I
want to say that the advantage is going
want to say that the advantage is going
to be something
like let me make sure so this is like
like let me make sure so this is like
Yus
Yus
V
V
over
over
Sigma squared which is just this thing
Sigma squared which is just this thing
now this is Sigma squared so I just even
now this is Sigma squared so I just even
write it that way so it's easier B
write it that way so it's easier B
SD squ I want to write something like uh
SD squ I want to write something like uh
like
this but there's an issue with the
this but there's an issue with the
scaling of this thing like this is a
scaling of this thing like this is a
perfectly good Advantage formula but
perfectly good Advantage formula but
there's an issue with the
there's an issue with the
scaling so you're going to learn VT
scaling so you're going to learn VT
standard deviation right this is a
standard deviation right this is a
learnable parameter but the environment
learnable parameter but the environment
has
has
some intrinsic frequency of reward to it
some intrinsic frequency of reward to it
right uh where you can kind of just
right uh where you can kind of just
predict the mean
predict the mean
reward and you're going to get something
reward and you're going to get something
that looks like this uh if we draw a
graph okay so this is T and this is t
graph okay so this is T and this is t
plus t plus
plus t plus
n so you're going to get
n so you're going to get
something that maybe starts up here
something that maybe starts up here
and it's going to end not where is it
and it's going to end not where is it
hang on other way around so you're going
hang on other way around so you're going
to get standard deviation is going to
to get standard deviation is going to
look something like
this okay so you're confident at time T
this okay so you're confident at time T
because this one's easy to
because this one's easy to
predict and then up here this is
predict and then up here this is
essentially random but it doesn't like
essentially random but it doesn't like
go off to Infinity it converges to a
go off to Infinity it converges to a
value and there's a little bit of noise
value and there's a little bit of noise
in here as well so so we want these
in here as well so so we want these
predictions to contribute
predictions to contribute
zero okay these have to contribute zero
zero okay these have to contribute zero
to our advantage function so we can't
to our advantage function so we can't
just put over V standard deviation squar
just put over V standard deviation squar
this has to essentially go to infinity
this has to essentially go to infinity
or something and then here uh we would
or something and then here uh we would
like to also normalize this so that
like to also normalize this so that
these sum to one and the reason we'd
these sum to one and the reason we'd
like to normalize these so that they sum
like to normalize these so that they sum
to one is that this VT is kind of a
to one is that this VT is kind of a
parameter right you can set the maximum
parameter right you can set the maximum
number of steps that you can predict out
number of steps that you can predict out
like this could be 32 this could be 128
like this could be 32 this could be 128
whatever and uh the algorithm should
whatever and uh the algorithm should
basically work the same no matter what
basically work the same no matter what
you said it to so you should not have to
you said it to so you should not have to
retune your hyper parameters if you go
retune your hyper parameters if you go
from 32 to 128 and if you don't
from 32 to 128 and if you don't
normalize you have to retune because
normalize you have to retune because
you're summing these or taking the mean
you're summing these or taking the mean
of these and that's going to change the
of these and that's going to change the
magnitude if you don't fix
magnitude if you don't fix
that so this is the new algorithm uh I
that so this is the new algorithm uh I
think that right now based on what I've
think that right now based on what I've
seen I think think that we have a 60 to
seen I think think that we have a 60 to
70% chance of H of having this thing
70% chance of H of having this thing
work as a general purpose improvement
work as a general purpose improvement
over po that you're basically always
over po that you're basically always
going to want to use like everywhere if
going to want to use like everywhere if
we do this thing correctly
we do this thing correctly
so if I can do that that would mean that
so if I can do that that would mean that
puffer uh puffer would have the first
puffer uh puffer would have the first
core algorithm Improvement to
core algorithm Improvement to
reinforcement learning since 2017 that
reinforcement learning since 2017 that
would be huge
would be huge
and it would enable a lot of our
and it would enable a lot of our
research because there's very very good
research because there's very very good
reason to believe that this algorithm is
reason to believe that this algorithm is
going to specifically work better for
going to specifically work better for
hard environments as well what is why in
hard environments as well what is why in
this case uh Y is going to be the labels
this case uh Y is going to be the labels
so I probably I wrote this like a gan
so I probably I wrote this like a gan
process this should probably be
R so this thing was a pain in the ass to
R so this thing was a pain in the ass to
implement as well because uh the way
implement as well because uh the way
that I've been doing this I had to write
that I've been doing this I had to write
a Cuda kernel for this
a Cuda kernel for this
um because there's like masking and
um because there's like masking and
stuff involved uh it's it's not quite as
stuff involved uh it's it's not quite as
easy as just writing two lines of pie
easy as just writing two lines of pie
torch
torch
but I think it's very close if I can
but I think it's very close if I can
figure out this normalization I think it
figure out this normalization I think it
should work and if it doesn't work then
should work and if it doesn't work then
we'll have to figure out why there
we'll have to figure out why there
should be a reason for that but I will
should be a reason for that but I will
leave this somewhere I'll leave this
leave this somewhere I'll leave this
here in case anybody asks and then we
here in case anybody asks and then we
will attempt to figure this out from
will attempt to figure this out from
here okay a couple people are seeing
here okay a couple people are seeing
this new post
if anybody wants to go boost the Pokemon
if anybody wants to go boost the Pokemon
stuff feel free to go follow uh that guy
stuff feel free to go follow uh that guy
as well here why do you think it will
as well here why do you think it will
help more for complex uh ends yes very
help more for complex uh ends yes very
simple
so let's
so let's
say let's say that you have uh let me
say let's say that you have uh let me
think what's a good example of this
I mean I could even do like neural MMO
right all right you have your agent
right all right you have your agent
here and you have an enemy here okay and
here and you have an enemy here okay and
uh you need to kill this enemy that's
uh you need to kill this enemy that's
like one way that you're going to get
like one way that you're going to get
reward so GAE you tune your hyper
reward so GAE you tune your hyper
parameters and J tells you yeah you just
parameters and J tells you yeah you just
go ahead and set this to you know uh
go ahead and set this to you know uh
gamma how do you even write
gamma
gamma
0.8 set gamma equal to 0.8 and you'll be
0.8 set gamma equal to 0.8 and you'll be
good okay because it's a very short
good okay because it's a very short
Horizon
Horizon
problem but then you know that's all
problem but then you know that's all
well and good that teaches you to kill
well and good that teaches you to kill
enemies but then over here you know you
enemies but then over here you know you
have a resource that you need to get
have a resource that you need to get
okay and this is is way further away you
okay and this is is way further away you
have to actually have a longer Horizon
have to actually have a longer Horizon
so if you want to get this
resource 0.95 or something so gamma
resource 0.95 or something so gamma
changes uh oh like that's a problem
changes uh oh like that's a problem
because now you're not going to learn
because now you're not going to learn
this problem because you're looking too
this problem because you're looking too
far out and you're not going to learn
far out and you're not going to learn
this one as
this one as
easily so in Dota they actually this
easily so in Dota they actually this
didn't work like Po broke for them in
didn't work like Po broke for them in
Dota because of this and they actually
Dota because of this and they actually
had to anneal gamma over the course of
had to anneal gamma over the course of
training so that gamma would get uh
training so that gamma would get uh
would get larger over the course of
would get larger over the course of
training but they're kind of just
training but they're kind of just
applying some like stupid scheduler to
applying some like stupid scheduler to
this thing right this is not like a good
this thing right this is not like a good
way of doing this and you still have to
way of doing this and you still have to
tune these parameters in all
tune these parameters in all
cases but what I can do with this new
cases but what I can do with this new
algorithm is I can just delete this
algorithm is I can just delete this
right and I can say whatever you know
right and I can say whatever you know
you're going to have VT
you're going to have VT
uh through like v n was it t plus
uh through like v n was it t plus
n t plus I can't write okay and uh then
n t plus I can't write okay and uh then
all I do is I just say hey
all I do is I just say hey
look this one Sigma 1 = 1 Sigma 2
look this one Sigma 1 = 1 Sigma 2
equals like
0.15 it'll just learn this right it'll
0.15 it'll just learn this right it'll
just learn learn this like this
just learn learn this like this
distribution and then when it needs to
distribution and then when it needs to
learn the longer one it'll just adjust
learn the longer one it'll just adjust
this so it's like Sigma
this so it's like Sigma
1.1 Sigma 2 = one1 and it'll just learn
1.1 Sigma 2 = one1 and it'll just learn
essentially to adapt the effect of gamma
essentially to adapt the effect of gamma
the effect of Horizon on its
own so it's not the episode length it is
own so it's not the episode length it is
the frequency of sub goals that are
the frequency of sub goals that are
defined by the
rewards that is why I think this
rewards that is why I think this
algorithm is
cool okay so now with that taken with
cool okay so now with that taken with
that said let's open this up and see if
that said let's open this up and see if
we can actually get this thing
we can actually get this thing
normalized correctly so that we can
normalized correctly so that we can
figure this out
watch
right now the way I have this thing this
right now the way I have this thing this
is the cernal board at the
is the cernal board at the
moment I am passing
moment I am passing
in the
in the
global maximum standard
deviation does that make sense
potential now and p3o curves I've seen
potential now and p3o curves I've seen
make more sense takes a bit for the
make more sense takes a bit for the
neuronet to learn the to learn than
neuronet to learn the to learn than
episode yes it
episode yes it
should also note that even on breakout
should also note that even on breakout
the version of P30 that I have with 32
the version of P30 that I have with 32
Horizon uh that almost matches po
Horizon uh that almost matches po
performance like Po sample efficiency it
performance like Po sample efficiency it
takes just a little bit longer like 10
takes just a little bit longer like 10
15%
15%
longer samples in wall
longer samples in wall
clock so like we're very very close even
clock so like we're very very close even
on an end where it shouldn't really help
on an end where it shouldn't really help
that
much like breakout is an environment you
much like breakout is an environment you
would like you know the gamma and Lambda
would like you know the gamma and Lambda
are very very accurate because the
are very very accurate because the
Horizon of the task is basically fixed
Horizon of the task is basically fixed
it's just break a
block so even Ming there is very very
good and then actually what I should
good and then actually what I should
probably do is I should probably compare
probably do is I should probably compare
to uh like Atari default parameters
to uh like Atari default parameters
right 099 gamma 095 Lambda right or
right 099 gamma 095 Lambda right or
whatever it is that they use as defaults
whatever it is that they use as defaults
maybe it's the other way around I'd have
maybe it's the other way around I'd have
to check um yeah that would be the the
to check um yeah that would be the the
real test because if you can beat the
real test because if you can beat the
parameters everyone uses out of the box
parameters everyone uses out of the box
then like
then like
that's pretty darn good as well
so I currently I passed in the maximum
so I currently I passed in the maximum
standard deviation
what do you think would be a good
what do you think would be a good
end yeah it takes billions of steps but
end yeah it takes billions of steps but
it runs at it runs a billion and a half
it runs at it runs a billion and a half
steps an hour so we'll just do
that
right that's the cool thing about
right that's the cool thing about
puffers we can do
that e
so apparently I broke something
here for
I don't know what I'm doing there we go
22k runs is
22k runs is
crazy yeah
crazy yeah
it'll be way more than that I got to do
it'll be way more than that I got to do
the full impulse Wars review for you as
the full impulse Wars review for you as
well how uh how close do you think you
well how uh how close do you think you
are on that
as I'm looking forward to that end being
as I'm looking forward to that end being
merged
in that to see build stuff yep that made
in that to see build stuff yep that made
that's about right that sounds about
right I've actually got like plenty of
right I've actually got like plenty of
the core
the core
core science side tasks um that you
core science side tasks um that you
would be pretty much directly able to
would be pretty much directly able to
help with that uh you know will not like
help with that uh you know will not like
drive you nuts but you'll still be able
drive you nuts but you'll still be able
to learn some stuff and definitely be
to learn some stuff and definitely be
able to contribute well to the science
able to contribute well to the science
side
side
here
here
um like we really want to get a lot of
um like we really want to get a lot of
these Ms in a spot where we've got
these Ms in a spot where we've got
benchmarks on them they're nice and
benchmarks on them they're nice and
stable for benchmarking and then I have
stable for benchmarking and then I have
like there's a whole list of techniques
like there's a whole list of techniques
that need ablations on them to see like
that need ablations on them to see like
does this technique help does that
does this technique help does that
technique help
right okay so this is broken
somehow e
also considering finishing lunar lander
also considering finishing lunar lander
anov Maybe by Walker that'd be
anov Maybe by Walker that'd be
cool it would be it would be good to
cool it would be it would be good to
have more continuous physics based stuff
have more continuous physics based stuff
yeah it definitely
would I tried to do a rocket Lander
would I tried to do a rocket Lander
which is kind of the same
thing and I just got stuck with box 2D
thing and I just got stuck with box 2D
being a pain
we'll see how this one does today I
we'll see how this one does today I
don't expect this one to do super crazy
don't expect this one to do super crazy
because it doesn't have the flashy
because it doesn't have the flashy
visuals but I wanted to save the next
visuals but I wanted to save the next
one for
one for
[Music]
Monday post something
Monday post something
today oh yeah here he are
autonomous
autonomous
coding
interesting autonomous coding does kind
interesting autonomous coding does kind
of solve effort
of solve effort
thing but
um
interesting problem is it's also the
interesting problem is it's also the
area that's been like the most heavily
area that's been like the most heavily
investigated and doesn't work
okay so that was the that normalization
okay so that was the that normalization
factor was what was breaking it
factor was what was breaking it
cool
um now if I do this normalization
um now if I do this normalization
again just have to wait for experiments
again just have to wait for experiments
but only two minute experiments not
bad e
oh hey that's funny this is finally the
oh hey that's funny this is finally the
uh we got Rune following that's
uh we got Rune following that's
cool oh his his run escape. is
cool oh his his run escape. is
hilarious one request is
hilarious one request is
you you get some kind of
you you get some kind of
controller to be able to
controller to be able to
understand wait wait wait my my one
understand wait wait wait my my one
request when you do review impulse Wars
request when you do review impulse Wars
is that you get some kind of
is that you get some kind of
controller oh oh yeah a
controller oh oh yeah a
controller okay I can do
that is it that big of a difference just
that is it that big of a difference just
the controls or is it like I'm just
the controls or is it like I'm just
going to be just as bad with a
going to be just as bad with a
controller at it
night and day okay because I do play
night and day okay because I do play
some games and I'm stuck in like bronze
some games and I'm stuck in like bronze
and silver in every game I play to be
and silver in every game I play to be
fair I don't play that much
anymore the only game I was ever good at
anymore the only game I was ever good at
was RuneScape ironically like the
was RuneScape ironically like the
highend
PVE the mechanics were like way way way
PVE the mechanics were like way way way
more muscle
memory so this crop totally screws up
memory so this crop totally screws up
learning as
well is this the
case this totally screws up learning
case this totally screws up learning
about
this never
this never
played no competitive
games I mean I have by far by far the
games I mean I have by far by far the
most number of hours in MMOs and then as
most number of hours in MMOs and then as
a distant distant seconds will be like
a distant distant seconds will be like
you know spread across just the various
you know spread across just the various
larger competitive games
FPS is fair
FPS is fair
enough I get bored of every single
enough I get bored of every single
player game like very very
quickly never really played many
quickly never really played many
fighting games
why does this clip mess everything up so
why does this clip mess everything up so
badly or does it just delay a little bit
badly or does it just delay a little bit
let's
say that really breaks everything thing
huh I have to see what the advantage
huh I have to see what the advantage
values are then that should not break
everything e
Green Run good P30 run nope this is
Green Run good P30 run nope this is
uh oh this is just I I just did this two
uh oh this is just I I just did this two
minutes ago or 10 minutes ago man this
minutes ago or 10 minutes ago man this
is just like if you don't do I have P30
is just like if you don't do I have P30
working with short Horizons I'm just
working with short Horizons I'm just
trying to fix the thing that I mentioned
trying to fix the thing that I mentioned
before with the advantage normalization
before with the advantage normalization
for long
for long
Horizons that's the only issue the only
Horizons that's the only issue the only
issue we have now is normalization
yeah p3o works and it works at like a
yeah p3o works and it works at like a
million steps a
million steps a
second it's just the Horizon thing being
second it's just the Horizon thing being
an issue and that's not like the Horizon
an issue and that's not like the Horizon
thing is also not a
thing is also not a
um it's not like a oh no you have to
um it's not like a oh no you have to
tune Horizon it's just it's a bug with
tune Horizon it's just it's a bug with
the way that we do our normalization
the way that we do our normalization
it's just not designed
correctly okay so this doesn't work
correctly okay so this doesn't work
either
either
so we'll have to figure out when from
so we'll have to figure out when from
here so after you find a good Norm
here so after you find a good Norm
function tons of testing find well it
function tons of testing find well it
depends what the results are
depends what the results are
right I mean if we get the normalization
right I mean if we get the normalization
working well I think it's like very very
working well I think it's like very very
light
light
that will just throw this on everything
that will just throw this on everything
and when you tune this and you tune po
and when you tune this and you tune po
this will do about as well or
better
e e
here your values
going to drive me
nuts hang on you don't clear the buffer
nuts hang on you don't clear the buffer
though right
though right
yeah yeah hold on you don't clear your
buffer let's just add this as a
um let's just add this for now just so
um let's just add this for now just so
it's easier to
it's easier to
debug CU it keeps confusing me
okay is
perfect
perfect
so this gives us our
waiting and you're telling me that
waiting and you're telling me that
cutting these
cutting these
short screws up learning perf really
that's when that's like interesting
right for
oh okay so this is actually giving you a
oh okay so this is actually giving you a
uh a reasonable distribution over values
see this on Neptune or did I forget to
see this on Neptune or did I forget to
add
add
it foret to add it
let's see if this clipping messes
let's see if this clipping messes
anything up it really shouldn't though
wait what's
this there was a gap
that doesn't make sense
right man it's still
right man it's still
like there's still a big difference in
like there's still a big difference in
perf just by clipping the last one
perf just by clipping the last one
really
why do you want to clip
it
it
um I mean fair
um I mean fair
point I guess I haven't waited long
point I guess I haven't waited long
enough in training to see what the
enough in training to see what the
values converge
values converge
to but the idea is that if you have a
to but the idea is that if you have a
little bit of noise down here and you
little bit of noise down here and you
sum these all up um
sum these all up um
you're still incorporating
you're still incorporating
predictions well here if I have like 12
predictions well here if I have like 12
128 samples and there's a little bit of
128 samples and there's a little bit of
noise then like you can actually end up
noise then like you can actually end up
just training on the noise instead of
just training on the noise instead of
training on the sample you actually care
training on the sample you actually care
about it seems like there needs to be
about it seems like there needs to be
some sort of clipper
some sort of clipper
similar this is
360 so yeah this still does
360 so yeah this still does
terribly that's really weird though
terribly that's really weird though
hang
hang
on how's that even make
sense is there something I'm missing
sense is there something I'm missing
here where like
you don't have to compile the Cuda code
you don't have to compile the Cuda code
uh it's loaded from PI bind it it'll
uh it's loaded from PI bind it it'll
recompile
automatically that'd be a funny mistake
automatically that'd be a funny mistake
though if it doesn't
but I mean no I can see it clipping at
but I mean no I can see it clipping at
the right values
so here 50 mil into
so here 50 mil into
training right we're at about 0.5 or
training right we're at about 0.5 or
0.05 32 steps
in e
I don't see how it's possible that this
I don't see how it's possible that this
clip term it's almost never even been
clip term it's almost never even been
applied because it's so
small screws up training that badly
small screws up training that badly
right if I comment this then it works
right if I comment this then it works
doesn't it or am I
doesn't it or am I
wrong for
okay so it gets slightly better when you
okay so it gets slightly better when you
reduce
reduce
clipping and then magically when you
clipping and then magically when you
when you completely delete clipping it
when you completely delete clipping it
just works is that what we're dealing
with oh this thing goes
negative that's that's interesting
okay
okay
so we can see how low these values get
so we can see how low these values get
towards the end of training here right
uh something is different though because
uh something is different though because
this no longer reproduces the original
this no longer reproduces the original
curve let's see what this could possibly
curve let's see what this could possibly
have
been well now I'm confused because this
been well now I'm confused because this
is the same as before where it was
is the same as before where it was
solving
solving
right going run this
again make go check on something
okay
what I like to
do stage files working
do stage files working
on yeah that's
on yeah that's
fair
e e
me see if this fixes it
oops wrong
oops wrong
way
05 that shouldn't be
needed
e
e e
Gotta Love
Cuda breaking on
you literally have to remove the build
you literally have to remove the build
build files
okay is this the thing that was
okay is this the thing that was
required to make it work
seem like
seem like
it oh
it oh
maybe yeah maybe this is
maybe yeah maybe this is
it oh
it oh
yeah that's interesting though
yeah that's interesting though
so it want you to keep some probability
so it want you to keep some probability
Mass on this
huh e
you get the idea just your own intuition
you get the idea just your own intuition
just my own
intuition I mean I've kind of been
intuition I mean I've kind of been
thinking
thinking
like I've kind of been thinking for a
like I've kind of been thinking for a
long time right like I'm always thinking
long time right like I'm always thinking
in RL about like what are the levers
in RL about like what are the levers
that move things
that move things
and uh J seems like one of like the big
and uh J seems like one of like the big
bottlenecks where if you could get rid
bottlenecks where if you could get rid
of it your learning would be a lot more
of it your learning would be a lot more
flexible yeah so it's this that's
flexible yeah so it's this that's
annoying so then I was thinking how do
annoying so then I was thinking how do
we get rid of J okay well what does J do
we get rid of J okay well what does J do
J gives you this like essentially fancy
J gives you this like essentially fancy
discounting
discounting
scheme so okay how do we replace that
scheme so okay how do we replace that
well we have to learn just discounting
well we have to learn just discounting
how do we learn discounting well we
how do we learn discounting well we
can't just
can't just
like I you could just learn the one
like I you could just learn the one
parameter of G to be fair and have that
parameter of G to be fair and have that
adapt over training um that would be an
adapt over training um that would be an
alternative but I want I went this way
alternative but I want I went this way
with
it I don't know actually know how you
it I don't know actually know how you
learn that one
learn that one
parameter it seems more
sensible okay so let's do this now let's
sensible okay so let's do this now let's
go I want to see what happens when you
go I want to see what happens when you
do 128
uhoh this working
okay
.1 okay there we go that's
something e
see the problem is all these like these
see the problem is all these like these
low values
here it kind of converges to a uniform
here it kind of converges to a uniform
as
oh
oh
no it actually goes down
more
782 kind of fast how feed the feedback
782 kind of fast how feed the feedback
loop is yeah what's the fastest that ran
loop is yeah what's the fastest that ran
at SPS wise 2.5
million 2.5 million with larger mini
million 2.5 million with larger mini
batches and
such it's fast
I'm seeing now the limitation of
I'm seeing now the limitation of
subtracting the
subtracting the
in oh the original Atari
in oh the original Atari
Ms uh yeah it's like 6,000 A4 or
Ms uh yeah it's like 6,000 A4 or
something with the original architecture
something with the original architecture
I think the fastest we got training to
I think the fastest we got training to
was
was
30,000 so you know at least like 30X
30,000 so you know at least like 30X
around like 30X lower than this and mind
around like 30X lower than this and mind
you the original clean RL version is
you the original clean RL version is
like 1,000 so this is 800 times faster
like 1,000 so this is 800 times faster
currently and up to 2500 times
faster yeah really people just did not
faster yeah really people just did not
optimize anything at all in RL for years
optimize anything at all in RL for years
it was kind of sad
it's
e
e e
it's just not learning a reasonable
it's just not learning a reasonable
distribution here is it
that's kind of nice how this
that's kind of nice how this
works where you don't have to subtract
works where you don't have to subtract
anything but the magnitude difference is
anything but the magnitude difference is
not big
enough I mean it does reasonably well
enough I mean it does reasonably well
actually though for a first
actually though for a first
iteration though I
iteration though I
don't I don't think that this thing
don't I don't think that this thing
works
works
generally if this had a much shorter
generally if this had a much shorter
effect of horizon it wouldn't
work we'll be right
work we'll be right
back going to think about how we can do
back going to think about how we can do
this I think that we want something
this I think that we want something
closer this is way closer to like nll
closer this is way closer to like nll
loss yeah let me think about this I'll
loss yeah let me think about this I'll
be right
back
e e
so see
let's see what this
let's see what this
does what do what happens to this if you
does what do what happens to this if you
add log
add log
of
variance doesn't really work
that rewards higher variance doesn't
it is there some sort of smooth I can
it is there some sort of smooth I can
add
like if I subtract them in from this
like if I subtract them in from this
what happens
I would like to have that one tracked
I would like to have that one tracked
actually
so this actually did kind of close to
so this actually did kind of close to
the original
one this might even be too aggressive
see why is there like this six here
okay but up until that it looked kind of
okay but up until that it looked kind of
reasonable
okay there's something so
okay there's something so
10 and then this Decay is down
to maybe you have to just subtract the
to maybe you have to just subtract the
mean
it's kind of reasonable
okay we're getting somewhere with
okay we're getting somewhere with
this like that was a reasonable run
right what else could we do
we could relativize
it hang on can you you can't
it hang on can you you can't
right this disadvantage
right this disadvantage
scale as we have it defined
now e
we overthinking this massively
we overthinking this massively
overthinking
this I can't subtract the mean value for
divide by the
scale it's still going to be positive or
scale it's still going to be positive or
zero right
most
likely yeah this is still going to be
likely yeah this is still going to be
positive or zero
standard deviation standard
deviation I you subtract the Mach
then you divide by the
sum and that gets you zero to one
just do this so I can see a little
better e
well this number should also get more
well this number should also get more
stable over training
right e
and this just gave me completely
and this just gave me completely
different
perf right
same
seed completely different
perf It's tricky because you can't
perf It's tricky because you can't
really just subtract the mean can you
you know what maybe I can find a better
you know what maybe I can find a better
value to subtract than the uh V standard
deviation maybe I've been looking at it
deviation maybe I've been looking at it
wrong hang on we do have a giant off of
wrong hang on we do have a giant off of
rewards right here don't we
I'm just train this for a bit
but you might have to
but you might have to
subtract the standard deviation of the
reward
08 six
that could be something
we might have something here
this does look
good continue
here do it continue need to
decrease it
shouldn't BB back up now
I think we have our winner
I think we have our winner
here we'll see if this works
here we'll see if this works
empirically what I'm thinking
empirically what I'm thinking
right I think this is actually kind of
right I think this is actually kind of
clever I have my
clever I have my
moments so here's the cool thing with
moments so here's the cool thing with
what we can do
what we can do
whoops we'll implement this right
now so if you
have what's going to happen is it's
have what's going to happen is it's
going to go like it's going to look
going to go like it's going to look
something like
this this is you're going to be your
this this is you're going to be your
prediction over time of the standard
prediction over time of the standard
deviation and the standard
deviation and the standard
deviation in reality is going to be
deviation in reality is going to be
something like
this
this
right
right
so if the standard
so if the standard
deviation of your prediction is the same
deviation of your prediction is the same
as the standard deviation of the
as the standard deviation of the
underlying distribution and I believe
underlying distribution and I believe
that means that you have no signal here
but the thing that could happen also
but the thing that could happen also
which I think is what was screwing us up
which I think is what was screwing us up
on 32 steps so like if you are at a 32
on 32 steps so like if you are at a 32
step
prediction so that say like right here I
prediction so that say like right here I
was subtracting this value from the
was subtracting this value from the
whole distribution so I was saying nah
whole distribution so I was saying nah
there's no signal here but there is
there's no signal here but there is
signal here there's only not signal when
signal here there's only not signal when
these two things
these two things
meet so I need to subtract the standard
meet so I need to subtract the standard
deviation of the actual distribution
deviation of the actual distribution
which I have an estimate for because I
which I have an estimate for because I
have all the
have all the
samples so let's just give it
that yeah let's just give it that so
that yeah let's just give it that so
instead of giving it V SPD Max or
whatever we'll do
D reward standard
deviation
okay
e e
error building Advantage
colonel e
this should never be able to be zero
this should never be able to be zero
anyways
anyways
hang
on it's going to be pretty cool
NS lovely
what happened
here e
heck is where do we have
heck is where do we have
NS they
ridiculous
e
e e
what the heck is wrong with this
oh I know I'm
dumb my
bad we'll have to just clean this carel
bad we'll have to just clean this carel
up
there we
go e
one six and then it goes down
one six and then it goes down
to negative
values we'll try a clip term next
okay try with clipping next next
there we
go e
oh yeah it's
oh yeah it's
flipping and the ones that are not
flipping and the ones that are not
getting clipped are e minus 4 e minus5
getting clipped are e minus 4 e minus5
that looks good
now it's weird that
now it's weird that
um this does so much
um this does so much
worse than the unclipped
version for
this is probably just the warmup stuff
this is probably just the warmup stuff
though
right
e
e
e
e
e e
there we
go
e e
doesn't seem to be learning very
e e
I didn't do anything right
that doesn't do anything
either really doesn't like this clip
either really doesn't like this clip
Factor
huh
e e
oh no maybe this isn't
it
e
e e
okay so this is not this whole thing
okay so this is not this whole thing
that's just breaking it good that
that's just breaking it good that
wouldn't have made much sense now would
wouldn't have made much sense now would
it
have e
okay so this like little bit of
okay so this like little bit of
uniformity apparently helped at the
uniformity apparently helped at the
start before you know the value
function the value function here is very
function the value function here is very
important
what happens if we do a 32
Horizon e
I mean this to me looks
I mean this to me looks
like mostly stable
like mostly stable
learning but it just takes forever to
learning but it just takes forever to
get started
what time
and this has learned a very nice smooth
and this has learned a very nice smooth
distribution very very nice and smooth
distribution very very nice and smooth
distribution
distribution
we have this gets
104 actually we can log both of these
nicely e
this is now possible like we're in the
this is now possible like we're in the
territory where we've changed it enough
territory where we've changed it enough
that we might just need to retune
hypers and we might just straight up
hypers and we might just straight up
need to retune the hyper
prams
e e
this makes sense right
yeah yeah this totally makes
sense
sense
well maybe not maybe
not what if I don't do
not what if I don't do
this what happens
if you get zero Advantage for a bit you
if you get zero Advantage for a bit you
just you know you just learn the value
just you know you just learn the value
function that's
fine e
so this latest run
here we're going to just just have to
here we're going to just just have to
rerun uh The Sweep to see you know it's
rerun uh The Sweep to see you know it's
hard to say I think in principle this
hard to say I think in principle this
algorithm now makes way more sense uh it
algorithm now makes way more sense uh it
has some properties that were clearly
has some properties that were clearly
missing
before but ultimately we will have to
before but ultimately we will have to
see
let me start working on committing this
let me start working on committing this
stuff up
106 all right so it's on
par
e e
sweep get this token
I think in principle at the very least
I think in principle at the very least
the new waiting is a it makes a lot of
sense it makes a lot of
sense it makes a lot of
sense we'll see whether I'm right cu the
sense we'll see whether I'm right cu the
current experiment just with the same
current experiment just with the same
hypers does not do very well at all but
hypers does not do very well at all but
you never know right we changed it a lot
you never know right we changed it a lot
it could require completely different
it could require completely different
tuning this is why you need to be able
tuning this is why you need to be able
to run massive hyper parameter sweeps
to run massive hyper parameter sweeps
otherwise I'd be sitting here for the
otherwise I'd be sitting here for the
next week like wondering if I'm stupid
next week like wondering if I'm stupid
or not I just get to know if I'm stupid
or not I just get to know if I'm stupid
in a couple
in a couple
hours let's go check
hours let's go check
out
Neptune make sure this works
okay great we'll let this go and we'll
okay great we'll let this go and we'll
uh we'll see how it
uh we'll see how it
does I will be probably back later today
does I will be probably back later today
working on other stuff depends we'll see
working on other stuff depends we'll see
how it goes I'm going to have a long uh
how it goes I'm going to have a long uh
workday plan tomorrow so might
workday plan tomorrow so might
not um but I will be at least checking
not um but I will be at least checking
back on these
back on these
things exciting possibilities as soon as
things exciting possibilities as soon as
we get the scaling right I I think we're
we get the scaling right I I think we're
going to have something really special
going to have something really special
here so if you're interested in
here so if you're interested in
following this and other work of mine
following this and other work of mine
buffer. it's all free an open source
buffer. it's all free an open source
star the GitHub to help us out uh join
star the GitHub to help us out uh join
the Discord if you want to get involved
the Discord if you want to get involved
and follow on X for more RL
and follow on X for more RL
announcements and stuff like that

Kind: captions
Language: en
hello how's it
hello how's it
going has been busy lately with some
going has been busy lately with some
stuff crazy
stuff crazy
week I really only have one goal for
today um the plan for today make one
today um the plan for today make one
quick tweet on the Pokemon stuff so we
quick tweet on the Pokemon stuff so we
uh keep go keep that going
uh keep go keep that going
and then all I want to do
and then all I want to do
today
today
is
is
fix long Horizons in the new algorithm
fix long Horizons in the new algorithm
that's all I want to do today it should
that's all I want to do today it should
be a relatively simple normalization
be a relatively simple normalization
problem I think I just was too tired
problem I think I just was too tired
yesterday to think about it correctly
yesterday to think about it correctly
that's all I want to do if we get that
that's all I want to do if we get that
done today I will be happy because that
done today I will be happy because that
will push us to where we can move on to
will push us to where we can move on to
a lot of new stuff with puffer I'll be
a lot of new stuff with puffer I'll be
able to clear my mind of it and we'll be
able to clear my mind of it and we'll be
able to work on what I have planned for
able to work on what I have planned for
tomorrow all day and it'll be
tomorrow all day and it'll be
good we got some cool new people in uh
good we got some cool new people in uh
in the Discord as
well
interesting
e
e e
very nice let's get this tweet out uh
very nice let's get this tweet out uh
this has been doing pretty well this
this has been doing pretty well this
Pokemon stuff has been doing pretty well
Pokemon stuff has been doing pretty well
where's the blog
here I just want to pick
here I just want to pick
something something cool from
here for
what can you not
what can you not
uh oh my
uh oh my
gosh he included this as a
gosh he included this as a
graph this is so funny man why would you
graph this is so funny man why would you
do that that's so much
effort e
I'm trying to think which one would do
I'm trying to think which one would do
better on X what do we
better on X what do we
think this is a cool
visual maybe this one
visual maybe this one
did Claude make that mermaid try I don't
did Claude make that mermaid try I don't
think so I think that guy did
it well I guess he no longer Anonymous
it well I guess he no longer Anonymous
David but
I think this one will do
I think this one will do
[Music]
[Music]
maybe we do this one today and then
maybe we do this one today and then
we'll do the next one on
Monday yeah cuz then people will see
Monday yeah cuz then people will see
it's a different tweet clearly
it's important to get this
right
e e
I'm trying to think what else
I don't think anybody's even done this
oh this is a cool little
oh this is a cool little
uh there's a cool post but it doesn't
uh there's a cool post but it doesn't
have any
have any
results it's like hey this would be a
results it's like hey this would be a
cool thing to do it's a proposal
what's wrong with Twitter
spacing
e
e
e
e
e
e e
all right I think that's a nice po
all right I think that's a nice po
we'll see whether this CH style thing
we'll see whether this CH style thing
does decently or not but it's
cool cool some is actually in hyper pram
cool cool some is actually in hyper pram
optimization that's
cool
e
e
e e
all right cool I think that's uh
all right cool that
works and uh we will
works and uh we will
see we will see with this this not get
see we will see with this this not get
posted oh yeah it did
posted oh yeah it did
okay now let's talk about the research
okay now let's talk about the research
side stuff I want to do today I want to
side stuff I want to do today I want to
at least outline what I'm trying to do
at least outline what I'm trying to do
once cuz it's a little fiddly and
once cuz it's a little fiddly and
requires a lot of thinking so it's a lot
requires a lot of thinking so it's a lot
of me just staring at the screen um so I
of me just staring at the screen um so I
want to make sure that I do this at
want to make sure that I do this at
least
once and maybe this will help me get set
once and maybe this will help me get set
up for the day as
well so what I'm going to work on today
well so what I'm going to work on today
is part of a new algorithm I've been
is part of a new algorithm I've been
working on uh to improve over
working on uh to improve over
P that has been attempted like hundreds
P that has been attempted like hundreds
of times over the
of times over the
last seven years at this point six years
last seven years at this point six years
seven years and nobody's done it PPO is
seven years and nobody's done it PPO is
still King so I think we're actually
still King so I think we're actually
going to be able to improve on PO
going to be able to improve on PO
specifically we are replacing
specifically we are replacing
generalized Advantage estimation the
generalized Advantage estimation the
reason for that is generalized advantage
reason for that is generalized advantage
estimation requires two tunable hyper
estimation requires two tunable hyper
parameters and it locks you to learning
parameters and it locks you to learning
rewards over specific
rewards over specific
Horizons so and you can tune these
Horizons so and you can tune these
parameters to essentially say hey I care
parameters to essentially say hey I care
about rewards 10 steps out but then if
about rewards 10 steps out but then if
your environment is complex and as you
your environment is complex and as you
learn more it turns out that there are
learn more it turns out that there are
rewards that are 100 steps out you're
rewards that are 100 steps out you're
not really going to learn that so I want
not really going to learn that so I want
to replace J with something that doesn't
to replace J with something that doesn't
require Lambda and Gamma and has
require Lambda and Gamma and has
flexible adaptive Horizons what I've
flexible adaptive Horizons what I've
done for this is I have
done for this is I have
replaced the value function right which
replaced the value function right which
is typically just output from the neural
is typically just output from the neural
net right like you have your hidden
net right like you have your hidden
layers okay and then you output your
layers okay and then you output your
value like
value like
this and so you have hidden and then you
this and so you have hidden and then you
have value well what I've actually done
have value well what I've actually done
here is I make
oops I call this
oops I call this
VT VT +
VT VT +
1 and uh v t + 2 and then we do this
1 and uh v t + 2 and then we do this
going out and then we actually we also
going out and then we actually we also
output uh we also have
output uh we also have
BT standard
BT standard
deviation it's variational and it is
deviation it's variational and it is
predicted over the next many many many
predicted over the next many many many
time steps this is the new
time steps this is the new
algorithm uh well part of it so what
algorithm uh well part of it so what
what are we going to train this first of
what are we going to train this first of
all to well at least training this value
all to well at least training this value
function is uh relatively easy because
function is uh relatively easy because
we have standard deviation and because
we have standard deviation and because
we have t this is just a gaussian
we have t this is just a gaussian
nll on
nll on
b v b s d
b v b s d
and then
and then
rewards and you have to take in the
rewards and you have to take in the
whole reward Vector to do this but this
whole reward Vector to do this but this
is how you train your value function uh
is how you train your value function uh
and this is kind of cool because now
and this is kind of cool because now
you're using not just a single float
you're using not just a single float
you're using as many value functions
you're using as many value functions
forward as you predict at every time
forward as you predict at every time
step for training so this is quite
step for training so this is quite
powerful already but you still need to
powerful already but you still need to
write down the advantage function okay
write down the advantage function okay
and the advantage function is a little
and the advantage function is a little
bit trickier this is I've been working
bit trickier this is I've been working
on at the
on at the
moment because what I want to do uh is I
moment because what I want to do uh is I
want to say that the advantage is going
want to say that the advantage is going
to be something
like let me make sure so this is like
like let me make sure so this is like
Yus
Yus
V
V
over
over
Sigma squared which is just this thing
Sigma squared which is just this thing
now this is Sigma squared so I just even
now this is Sigma squared so I just even
write it that way so it's easier B
write it that way so it's easier B
SD squ I want to write something like uh
SD squ I want to write something like uh
like
this but there's an issue with the
this but there's an issue with the
scaling of this thing like this is a
scaling of this thing like this is a
perfectly good Advantage formula but
perfectly good Advantage formula but
there's an issue with the
there's an issue with the
scaling so you're going to learn VT
scaling so you're going to learn VT
standard deviation right this is a
standard deviation right this is a
learnable parameter but the environment
learnable parameter but the environment
has
has
some intrinsic frequency of reward to it
some intrinsic frequency of reward to it
right uh where you can kind of just
right uh where you can kind of just
predict the mean
predict the mean
reward and you're going to get something
reward and you're going to get something
that looks like this uh if we draw a
graph okay so this is T and this is t
graph okay so this is T and this is t
plus t plus
plus t plus
n so you're going to get
n so you're going to get
something that maybe starts up here
something that maybe starts up here
and it's going to end not where is it
and it's going to end not where is it
hang on other way around so you're going
hang on other way around so you're going
to get standard deviation is going to
to get standard deviation is going to
look something like
this okay so you're confident at time T
this okay so you're confident at time T
because this one's easy to
because this one's easy to
predict and then up here this is
predict and then up here this is
essentially random but it doesn't like
essentially random but it doesn't like
go off to Infinity it converges to a
go off to Infinity it converges to a
value and there's a little bit of noise
value and there's a little bit of noise
in here as well so so we want these
in here as well so so we want these
predictions to contribute
predictions to contribute
zero okay these have to contribute zero
zero okay these have to contribute zero
to our advantage function so we can't
to our advantage function so we can't
just put over V standard deviation squar
just put over V standard deviation squar
this has to essentially go to infinity
this has to essentially go to infinity
or something and then here uh we would
or something and then here uh we would
like to also normalize this so that
like to also normalize this so that
these sum to one and the reason we'd
these sum to one and the reason we'd
like to normalize these so that they sum
like to normalize these so that they sum
to one is that this VT is kind of a
to one is that this VT is kind of a
parameter right you can set the maximum
parameter right you can set the maximum
number of steps that you can predict out
number of steps that you can predict out
like this could be 32 this could be 128
like this could be 32 this could be 128
whatever and uh the algorithm should
whatever and uh the algorithm should
basically work the same no matter what
basically work the same no matter what
you said it to so you should not have to
you said it to so you should not have to
retune your hyper parameters if you go
retune your hyper parameters if you go
from 32 to 128 and if you don't
from 32 to 128 and if you don't
normalize you have to retune because
normalize you have to retune because
you're summing these or taking the mean
you're summing these or taking the mean
of these and that's going to change the
of these and that's going to change the
magnitude if you don't fix
magnitude if you don't fix
that so this is the new algorithm uh I
that so this is the new algorithm uh I
think that right now based on what I've
think that right now based on what I've
seen I think think that we have a 60 to
seen I think think that we have a 60 to
70% chance of H of having this thing
70% chance of H of having this thing
work as a general purpose improvement
work as a general purpose improvement
over po that you're basically always
over po that you're basically always
going to want to use like everywhere if
going to want to use like everywhere if
we do this thing correctly
we do this thing correctly
so if I can do that that would mean that
so if I can do that that would mean that
puffer uh puffer would have the first
puffer uh puffer would have the first
core algorithm Improvement to
core algorithm Improvement to
reinforcement learning since 2017 that
reinforcement learning since 2017 that
would be huge
would be huge
and it would enable a lot of our
and it would enable a lot of our
research because there's very very good
research because there's very very good
reason to believe that this algorithm is
reason to believe that this algorithm is
going to specifically work better for
going to specifically work better for
hard environments as well what is why in
hard environments as well what is why in
this case uh Y is going to be the labels
this case uh Y is going to be the labels
so I probably I wrote this like a gan
so I probably I wrote this like a gan
process this should probably be
R so this thing was a pain in the ass to
R so this thing was a pain in the ass to
implement as well because uh the way
implement as well because uh the way
that I've been doing this I had to write
that I've been doing this I had to write
a Cuda kernel for this
a Cuda kernel for this
um because there's like masking and
um because there's like masking and
stuff involved uh it's it's not quite as
stuff involved uh it's it's not quite as
easy as just writing two lines of pie
easy as just writing two lines of pie
torch
torch
but I think it's very close if I can
but I think it's very close if I can
figure out this normalization I think it
figure out this normalization I think it
should work and if it doesn't work then
should work and if it doesn't work then
we'll have to figure out why there
we'll have to figure out why there
should be a reason for that but I will
should be a reason for that but I will
leave this somewhere I'll leave this
leave this somewhere I'll leave this
here in case anybody asks and then we
here in case anybody asks and then we
will attempt to figure this out from
will attempt to figure this out from
here okay a couple people are seeing
here okay a couple people are seeing
this new post
if anybody wants to go boost the Pokemon
if anybody wants to go boost the Pokemon
stuff feel free to go follow uh that guy
stuff feel free to go follow uh that guy
as well here why do you think it will
as well here why do you think it will
help more for complex uh ends yes very
help more for complex uh ends yes very
simple
so let's
so let's
say let's say that you have uh let me
say let's say that you have uh let me
think what's a good example of this
I mean I could even do like neural MMO
right all right you have your agent
right all right you have your agent
here and you have an enemy here okay and
here and you have an enemy here okay and
uh you need to kill this enemy that's
uh you need to kill this enemy that's
like one way that you're going to get
like one way that you're going to get
reward so GAE you tune your hyper
reward so GAE you tune your hyper
parameters and J tells you yeah you just
parameters and J tells you yeah you just
go ahead and set this to you know uh
go ahead and set this to you know uh
gamma how do you even write
gamma
gamma
0.8 set gamma equal to 0.8 and you'll be
0.8 set gamma equal to 0.8 and you'll be
good okay because it's a very short
good okay because it's a very short
Horizon
Horizon
problem but then you know that's all
problem but then you know that's all
well and good that teaches you to kill
well and good that teaches you to kill
enemies but then over here you know you
enemies but then over here you know you
have a resource that you need to get
have a resource that you need to get
okay and this is is way further away you
okay and this is is way further away you
have to actually have a longer Horizon
have to actually have a longer Horizon
so if you want to get this
resource 0.95 or something so gamma
resource 0.95 or something so gamma
changes uh oh like that's a problem
changes uh oh like that's a problem
because now you're not going to learn
because now you're not going to learn
this problem because you're looking too
this problem because you're looking too
far out and you're not going to learn
far out and you're not going to learn
this one as
this one as
easily so in Dota they actually this
easily so in Dota they actually this
didn't work like Po broke for them in
didn't work like Po broke for them in
Dota because of this and they actually
Dota because of this and they actually
had to anneal gamma over the course of
had to anneal gamma over the course of
training so that gamma would get uh
training so that gamma would get uh
would get larger over the course of
would get larger over the course of
training but they're kind of just
training but they're kind of just
applying some like stupid scheduler to
applying some like stupid scheduler to
this thing right this is not like a good
this thing right this is not like a good
way of doing this and you still have to
way of doing this and you still have to
tune these parameters in all
tune these parameters in all
cases but what I can do with this new
cases but what I can do with this new
algorithm is I can just delete this
algorithm is I can just delete this
right and I can say whatever you know
right and I can say whatever you know
you're going to have VT
you're going to have VT
uh through like v n was it t plus
uh through like v n was it t plus
n t plus I can't write okay and uh then
n t plus I can't write okay and uh then
all I do is I just say hey
all I do is I just say hey
look this one Sigma 1 = 1 Sigma 2
look this one Sigma 1 = 1 Sigma 2
equals like
0.15 it'll just learn this right it'll
0.15 it'll just learn this right it'll
just learn learn this like this
just learn learn this like this
distribution and then when it needs to
distribution and then when it needs to
learn the longer one it'll just adjust
learn the longer one it'll just adjust
this so it's like Sigma
this so it's like Sigma
1.1 Sigma 2 = one1 and it'll just learn
1.1 Sigma 2 = one1 and it'll just learn
essentially to adapt the effect of gamma
essentially to adapt the effect of gamma
the effect of Horizon on its
own so it's not the episode length it is
own so it's not the episode length it is
the frequency of sub goals that are
the frequency of sub goals that are
defined by the
rewards that is why I think this
rewards that is why I think this
algorithm is
cool okay so now with that taken with
cool okay so now with that taken with
that said let's open this up and see if
that said let's open this up and see if
we can actually get this thing
we can actually get this thing
normalized correctly so that we can
normalized correctly so that we can
figure this out
watch
right now the way I have this thing this
right now the way I have this thing this
is the cernal board at the
is the cernal board at the
moment I am passing
moment I am passing
in the
in the
global maximum standard
deviation does that make sense
potential now and p3o curves I've seen
potential now and p3o curves I've seen
make more sense takes a bit for the
make more sense takes a bit for the
neuronet to learn the to learn than
neuronet to learn the to learn than
episode yes it
episode yes it
should also note that even on breakout
should also note that even on breakout
the version of P30 that I have with 32
the version of P30 that I have with 32
Horizon uh that almost matches po
Horizon uh that almost matches po
performance like Po sample efficiency it
performance like Po sample efficiency it
takes just a little bit longer like 10
takes just a little bit longer like 10
15%
15%
longer samples in wall
longer samples in wall
clock so like we're very very close even
clock so like we're very very close even
on an end where it shouldn't really help
on an end where it shouldn't really help
that
much like breakout is an environment you
much like breakout is an environment you
would like you know the gamma and Lambda
would like you know the gamma and Lambda
are very very accurate because the
are very very accurate because the
Horizon of the task is basically fixed
Horizon of the task is basically fixed
it's just break a
block so even Ming there is very very
good and then actually what I should
good and then actually what I should
probably do is I should probably compare
probably do is I should probably compare
to uh like Atari default parameters
to uh like Atari default parameters
right 099 gamma 095 Lambda right or
right 099 gamma 095 Lambda right or
whatever it is that they use as defaults
whatever it is that they use as defaults
maybe it's the other way around I'd have
maybe it's the other way around I'd have
to check um yeah that would be the the
to check um yeah that would be the the
real test because if you can beat the
real test because if you can beat the
parameters everyone uses out of the box
parameters everyone uses out of the box
then like
then like
that's pretty darn good as well
so I currently I passed in the maximum
so I currently I passed in the maximum
standard deviation
what do you think would be a good
what do you think would be a good
end yeah it takes billions of steps but
end yeah it takes billions of steps but
it runs at it runs a billion and a half
it runs at it runs a billion and a half
steps an hour so we'll just do
that
right that's the cool thing about
right that's the cool thing about
puffers we can do
that e
so apparently I broke something
here for
I don't know what I'm doing there we go
22k runs is
22k runs is
crazy yeah
crazy yeah
it'll be way more than that I got to do
it'll be way more than that I got to do
the full impulse Wars review for you as
the full impulse Wars review for you as
well how uh how close do you think you
well how uh how close do you think you
are on that
as I'm looking forward to that end being
as I'm looking forward to that end being
merged
in that to see build stuff yep that made
in that to see build stuff yep that made
that's about right that sounds about
right I've actually got like plenty of
right I've actually got like plenty of
the core
the core
core science side tasks um that you
core science side tasks um that you
would be pretty much directly able to
would be pretty much directly able to
help with that uh you know will not like
help with that uh you know will not like
drive you nuts but you'll still be able
drive you nuts but you'll still be able
to learn some stuff and definitely be
to learn some stuff and definitely be
able to contribute well to the science
able to contribute well to the science
side
side
here
here
um like we really want to get a lot of
um like we really want to get a lot of
these Ms in a spot where we've got
these Ms in a spot where we've got
benchmarks on them they're nice and
benchmarks on them they're nice and
stable for benchmarking and then I have
stable for benchmarking and then I have
like there's a whole list of techniques
like there's a whole list of techniques
that need ablations on them to see like
that need ablations on them to see like
does this technique help does that
does this technique help does that
technique help
right okay so this is broken
somehow e
also considering finishing lunar lander
also considering finishing lunar lander
anov Maybe by Walker that'd be
anov Maybe by Walker that'd be
cool it would be it would be good to
cool it would be it would be good to
have more continuous physics based stuff
have more continuous physics based stuff
yeah it definitely
would I tried to do a rocket Lander
would I tried to do a rocket Lander
which is kind of the same
thing and I just got stuck with box 2D
thing and I just got stuck with box 2D
being a pain
we'll see how this one does today I
we'll see how this one does today I
don't expect this one to do super crazy
don't expect this one to do super crazy
because it doesn't have the flashy
because it doesn't have the flashy
visuals but I wanted to save the next
visuals but I wanted to save the next
one for
one for
[Music]
Monday post something
Monday post something
today oh yeah here he are
autonomous
autonomous
coding
interesting autonomous coding does kind
interesting autonomous coding does kind
of solve effort
of solve effort
thing but
um
interesting problem is it's also the
interesting problem is it's also the
area that's been like the most heavily
area that's been like the most heavily
investigated and doesn't work
okay so that was the that normalization
okay so that was the that normalization
factor was what was breaking it
factor was what was breaking it
cool
um now if I do this normalization
um now if I do this normalization
again just have to wait for experiments
again just have to wait for experiments
but only two minute experiments not
bad e
oh hey that's funny this is finally the
oh hey that's funny this is finally the
uh we got Rune following that's
uh we got Rune following that's
cool oh his his run escape. is
cool oh his his run escape. is
hilarious one request is
hilarious one request is
you you get some kind of
you you get some kind of
controller to be able to
controller to be able to
understand wait wait wait my my one
understand wait wait wait my my one
request when you do review impulse Wars
request when you do review impulse Wars
is that you get some kind of
is that you get some kind of
controller oh oh yeah a
controller oh oh yeah a
controller okay I can do
that is it that big of a difference just
that is it that big of a difference just
the controls or is it like I'm just
the controls or is it like I'm just
going to be just as bad with a
going to be just as bad with a
controller at it
night and day okay because I do play
night and day okay because I do play
some games and I'm stuck in like bronze
some games and I'm stuck in like bronze
and silver in every game I play to be
and silver in every game I play to be
fair I don't play that much
anymore the only game I was ever good at
anymore the only game I was ever good at
was RuneScape ironically like the
was RuneScape ironically like the
highend
PVE the mechanics were like way way way
PVE the mechanics were like way way way
more muscle
memory so this crop totally screws up
memory so this crop totally screws up
learning as
well is this the
case this totally screws up learning
case this totally screws up learning
about
this never
this never
played no competitive
games I mean I have by far by far the
games I mean I have by far by far the
most number of hours in MMOs and then as
most number of hours in MMOs and then as
a distant distant seconds will be like
a distant distant seconds will be like
you know spread across just the various
you know spread across just the various
larger competitive games
FPS is fair
FPS is fair
enough I get bored of every single
enough I get bored of every single
player game like very very
quickly never really played many
quickly never really played many
fighting games
why does this clip mess everything up so
why does this clip mess everything up so
badly or does it just delay a little bit
badly or does it just delay a little bit
let's
say that really breaks everything thing
huh I have to see what the advantage
huh I have to see what the advantage
values are then that should not break
everything e
Green Run good P30 run nope this is
Green Run good P30 run nope this is
uh oh this is just I I just did this two
uh oh this is just I I just did this two
minutes ago or 10 minutes ago man this
minutes ago or 10 minutes ago man this
is just like if you don't do I have P30
is just like if you don't do I have P30
working with short Horizons I'm just
working with short Horizons I'm just
trying to fix the thing that I mentioned
trying to fix the thing that I mentioned
before with the advantage normalization
before with the advantage normalization
for long
for long
Horizons that's the only issue the only
Horizons that's the only issue the only
issue we have now is normalization
yeah p3o works and it works at like a
yeah p3o works and it works at like a
million steps a
million steps a
second it's just the Horizon thing being
second it's just the Horizon thing being
an issue and that's not like the Horizon
an issue and that's not like the Horizon
thing is also not a
thing is also not a
um it's not like a oh no you have to
um it's not like a oh no you have to
tune Horizon it's just it's a bug with
tune Horizon it's just it's a bug with
the way that we do our normalization
the way that we do our normalization
it's just not designed
correctly okay so this doesn't work
correctly okay so this doesn't work
either
either
so we'll have to figure out when from
so we'll have to figure out when from
here so after you find a good Norm
here so after you find a good Norm
function tons of testing find well it
function tons of testing find well it
depends what the results are
depends what the results are
right I mean if we get the normalization
right I mean if we get the normalization
working well I think it's like very very
working well I think it's like very very
light
light
that will just throw this on everything
that will just throw this on everything
and when you tune this and you tune po
and when you tune this and you tune po
this will do about as well or
better
e e
here your values
going to drive me
nuts hang on you don't clear the buffer
nuts hang on you don't clear the buffer
though right
though right
yeah yeah hold on you don't clear your
buffer let's just add this as a
um let's just add this for now just so
um let's just add this for now just so
it's easier to
it's easier to
debug CU it keeps confusing me
okay is
perfect
perfect
so this gives us our
waiting and you're telling me that
waiting and you're telling me that
cutting these
cutting these
short screws up learning perf really
that's when that's like interesting
right for
oh okay so this is actually giving you a
oh okay so this is actually giving you a
uh a reasonable distribution over values
see this on Neptune or did I forget to
see this on Neptune or did I forget to
add
add
it foret to add it
let's see if this clipping messes
let's see if this clipping messes
anything up it really shouldn't though
wait what's
this there was a gap
that doesn't make sense
right man it's still
right man it's still
like there's still a big difference in
like there's still a big difference in
perf just by clipping the last one
perf just by clipping the last one
really
why do you want to clip
it
it
um I mean fair
um I mean fair
point I guess I haven't waited long
point I guess I haven't waited long
enough in training to see what the
enough in training to see what the
values converge
values converge
to but the idea is that if you have a
to but the idea is that if you have a
little bit of noise down here and you
little bit of noise down here and you
sum these all up um
sum these all up um
you're still incorporating
you're still incorporating
predictions well here if I have like 12
predictions well here if I have like 12
128 samples and there's a little bit of
128 samples and there's a little bit of
noise then like you can actually end up
noise then like you can actually end up
just training on the noise instead of
just training on the noise instead of
training on the sample you actually care
training on the sample you actually care
about it seems like there needs to be
about it seems like there needs to be
some sort of clipper
some sort of clipper
similar this is
360 so yeah this still does
360 so yeah this still does
terribly that's really weird though
terribly that's really weird though
hang
hang
on how's that even make
sense is there something I'm missing
sense is there something I'm missing
here where like
you don't have to compile the Cuda code
you don't have to compile the Cuda code
uh it's loaded from PI bind it it'll
uh it's loaded from PI bind it it'll
recompile
automatically that'd be a funny mistake
automatically that'd be a funny mistake
though if it doesn't
but I mean no I can see it clipping at
but I mean no I can see it clipping at
the right values
so here 50 mil into
so here 50 mil into
training right we're at about 0.5 or
training right we're at about 0.5 or
0.05 32 steps
in e
I don't see how it's possible that this
I don't see how it's possible that this
clip term it's almost never even been
clip term it's almost never even been
applied because it's so
small screws up training that badly
small screws up training that badly
right if I comment this then it works
right if I comment this then it works
doesn't it or am I
doesn't it or am I
wrong for
okay so it gets slightly better when you
okay so it gets slightly better when you
reduce
reduce
clipping and then magically when you
clipping and then magically when you
when you completely delete clipping it
when you completely delete clipping it
just works is that what we're dealing
with oh this thing goes
negative that's that's interesting
okay
okay
so we can see how low these values get
so we can see how low these values get
towards the end of training here right
uh something is different though because
uh something is different though because
this no longer reproduces the original
this no longer reproduces the original
curve let's see what this could possibly
curve let's see what this could possibly
have
been well now I'm confused because this
been well now I'm confused because this
is the same as before where it was
is the same as before where it was
solving
solving
right going run this
again make go check on something
okay
what I like to
do stage files working
do stage files working
on yeah that's
on yeah that's
fair
e e
me see if this fixes it
oops wrong
oops wrong
way
05 that shouldn't be
needed
e
e e
Gotta Love
Cuda breaking on
you literally have to remove the build
you literally have to remove the build
build files
okay is this the thing that was
okay is this the thing that was
required to make it work
seem like
seem like
it oh
it oh
maybe yeah maybe this is
maybe yeah maybe this is
it oh
it oh
yeah that's interesting though
yeah that's interesting though
so it want you to keep some probability
so it want you to keep some probability
Mass on this
huh e
you get the idea just your own intuition
you get the idea just your own intuition
just my own
intuition I mean I've kind of been
intuition I mean I've kind of been
thinking
thinking
like I've kind of been thinking for a
like I've kind of been thinking for a
long time right like I'm always thinking
long time right like I'm always thinking
in RL about like what are the levers
in RL about like what are the levers
that move things
that move things
and uh J seems like one of like the big
and uh J seems like one of like the big
bottlenecks where if you could get rid
bottlenecks where if you could get rid
of it your learning would be a lot more
of it your learning would be a lot more
flexible yeah so it's this that's
flexible yeah so it's this that's
annoying so then I was thinking how do
annoying so then I was thinking how do
we get rid of J okay well what does J do
we get rid of J okay well what does J do
J gives you this like essentially fancy
J gives you this like essentially fancy
discounting
discounting
scheme so okay how do we replace that
scheme so okay how do we replace that
well we have to learn just discounting
well we have to learn just discounting
how do we learn discounting well we
how do we learn discounting well we
can't just
can't just
like I you could just learn the one
like I you could just learn the one
parameter of G to be fair and have that
parameter of G to be fair and have that
adapt over training um that would be an
adapt over training um that would be an
alternative but I want I went this way
alternative but I want I went this way
with
it I don't know actually know how you
it I don't know actually know how you
learn that one
learn that one
parameter it seems more
sensible okay so let's do this now let's
sensible okay so let's do this now let's
go I want to see what happens when you
go I want to see what happens when you
do 128
uhoh this working
okay
.1 okay there we go that's
something e
see the problem is all these like these
see the problem is all these like these
low values
here it kind of converges to a uniform
here it kind of converges to a uniform
as
oh
oh
no it actually goes down
more
782 kind of fast how feed the feedback
782 kind of fast how feed the feedback
loop is yeah what's the fastest that ran
loop is yeah what's the fastest that ran
at SPS wise 2.5
million 2.5 million with larger mini
million 2.5 million with larger mini
batches and
such it's fast
I'm seeing now the limitation of
I'm seeing now the limitation of
subtracting the
subtracting the
in oh the original Atari
in oh the original Atari
Ms uh yeah it's like 6,000 A4 or
Ms uh yeah it's like 6,000 A4 or
something with the original architecture
something with the original architecture
I think the fastest we got training to
I think the fastest we got training to
was
was
30,000 so you know at least like 30X
30,000 so you know at least like 30X
around like 30X lower than this and mind
around like 30X lower than this and mind
you the original clean RL version is
you the original clean RL version is
like 1,000 so this is 800 times faster
like 1,000 so this is 800 times faster
currently and up to 2500 times
faster yeah really people just did not
faster yeah really people just did not
optimize anything at all in RL for years
optimize anything at all in RL for years
it was kind of sad
it's
e
e e
it's just not learning a reasonable
it's just not learning a reasonable
distribution here is it
that's kind of nice how this
that's kind of nice how this
works where you don't have to subtract
works where you don't have to subtract
anything but the magnitude difference is
anything but the magnitude difference is
not big
enough I mean it does reasonably well
enough I mean it does reasonably well
actually though for a first
actually though for a first
iteration though I
iteration though I
don't I don't think that this thing
don't I don't think that this thing
works
works
generally if this had a much shorter
generally if this had a much shorter
effect of horizon it wouldn't
work we'll be right
work we'll be right
back going to think about how we can do
back going to think about how we can do
this I think that we want something
this I think that we want something
closer this is way closer to like nll
closer this is way closer to like nll
loss yeah let me think about this I'll
loss yeah let me think about this I'll
be right
back
e e
so see
let's see what this
let's see what this
does what do what happens to this if you
does what do what happens to this if you
add log
add log
of
variance doesn't really work
that rewards higher variance doesn't
it is there some sort of smooth I can
it is there some sort of smooth I can
add
like if I subtract them in from this
like if I subtract them in from this
what happens
I would like to have that one tracked
I would like to have that one tracked
actually
so this actually did kind of close to
so this actually did kind of close to
the original
one this might even be too aggressive
see why is there like this six here
okay but up until that it looked kind of
okay but up until that it looked kind of
reasonable
okay there's something so
okay there's something so
10 and then this Decay is down
to maybe you have to just subtract the
to maybe you have to just subtract the
mean
it's kind of reasonable
okay we're getting somewhere with
okay we're getting somewhere with
this like that was a reasonable run
right what else could we do
we could relativize
it hang on can you you can't
it hang on can you you can't
right this disadvantage
right this disadvantage
scale as we have it defined
now e
we overthinking this massively
we overthinking this massively
overthinking
this I can't subtract the mean value for
divide by the
scale it's still going to be positive or
scale it's still going to be positive or
zero right
most
likely yeah this is still going to be
likely yeah this is still going to be
positive or zero
standard deviation standard
deviation I you subtract the Mach
then you divide by the
sum and that gets you zero to one
just do this so I can see a little
better e
well this number should also get more
well this number should also get more
stable over training
right e
and this just gave me completely
and this just gave me completely
different
perf right
same
seed completely different
perf It's tricky because you can't
perf It's tricky because you can't
really just subtract the mean can you
you know what maybe I can find a better
you know what maybe I can find a better
value to subtract than the uh V standard
deviation maybe I've been looking at it
deviation maybe I've been looking at it
wrong hang on we do have a giant off of
wrong hang on we do have a giant off of
rewards right here don't we
I'm just train this for a bit
but you might have to
but you might have to
subtract the standard deviation of the
reward
08 six
that could be something
we might have something here
this does look
good continue
here do it continue need to
decrease it
shouldn't BB back up now
I think we have our winner
I think we have our winner
here we'll see if this works
here we'll see if this works
empirically what I'm thinking
empirically what I'm thinking
right I think this is actually kind of
right I think this is actually kind of
clever I have my
clever I have my
moments so here's the cool thing with
moments so here's the cool thing with
what we can do
what we can do
whoops we'll implement this right
now so if you
have what's going to happen is it's
have what's going to happen is it's
going to go like it's going to look
going to go like it's going to look
something like
this this is you're going to be your
this this is you're going to be your
prediction over time of the standard
prediction over time of the standard
deviation and the standard
deviation and the standard
deviation in reality is going to be
deviation in reality is going to be
something like
this
this
right
right
so if the standard
so if the standard
deviation of your prediction is the same
deviation of your prediction is the same
as the standard deviation of the
as the standard deviation of the
underlying distribution and I believe
underlying distribution and I believe
that means that you have no signal here
but the thing that could happen also
but the thing that could happen also
which I think is what was screwing us up
which I think is what was screwing us up
on 32 steps so like if you are at a 32
on 32 steps so like if you are at a 32
step
prediction so that say like right here I
prediction so that say like right here I
was subtracting this value from the
was subtracting this value from the
whole distribution so I was saying nah
whole distribution so I was saying nah
there's no signal here but there is
there's no signal here but there is
signal here there's only not signal when
signal here there's only not signal when
these two things
these two things
meet so I need to subtract the standard
meet so I need to subtract the standard
deviation of the actual distribution
deviation of the actual distribution
which I have an estimate for because I
which I have an estimate for because I
have all the
have all the
samples so let's just give it
that yeah let's just give it that so
that yeah let's just give it that so
instead of giving it V SPD Max or
whatever we'll do
D reward standard
deviation
okay
e e
error building Advantage
colonel e
this should never be able to be zero
this should never be able to be zero
anyways
anyways
hang
on it's going to be pretty cool
NS lovely
what happened
here e
heck is where do we have
heck is where do we have
NS they
ridiculous
e
e e
what the heck is wrong with this
oh I know I'm
dumb my
bad we'll have to just clean this carel
bad we'll have to just clean this carel
up
there we
go e
one six and then it goes down
one six and then it goes down
to negative
values we'll try a clip term next
okay try with clipping next next
there we
go e
oh yeah it's
oh yeah it's
flipping and the ones that are not
flipping and the ones that are not
getting clipped are e minus 4 e minus5
getting clipped are e minus 4 e minus5
that looks good
now it's weird that
now it's weird that
um this does so much
um this does so much
worse than the unclipped
version for
this is probably just the warmup stuff
this is probably just the warmup stuff
though
right
e
e
e
e
e e
there we
go
e e
doesn't seem to be learning very
e e
I didn't do anything right
that doesn't do anything
either really doesn't like this clip
either really doesn't like this clip
Factor
huh
e e
oh no maybe this isn't
it
e
e e
okay so this is not this whole thing
okay so this is not this whole thing
that's just breaking it good that
that's just breaking it good that
wouldn't have made much sense now would
wouldn't have made much sense now would
it
have e
okay so this like little bit of
okay so this like little bit of
uniformity apparently helped at the
uniformity apparently helped at the
start before you know the value
function the value function here is very
function the value function here is very
important
what happens if we do a 32
Horizon e
I mean this to me looks
I mean this to me looks
like mostly stable
like mostly stable
learning but it just takes forever to
learning but it just takes forever to
get started
what time
and this has learned a very nice smooth
and this has learned a very nice smooth
distribution very very nice and smooth
distribution very very nice and smooth
distribution
distribution
we have this gets
104 actually we can log both of these
nicely e
this is now possible like we're in the
this is now possible like we're in the
territory where we've changed it enough
territory where we've changed it enough
that we might just need to retune
hypers and we might just straight up
hypers and we might just straight up
need to retune the hyper
prams
e e
this makes sense right
yeah yeah this totally makes
sense
sense
well maybe not maybe
not what if I don't do
not what if I don't do
this what happens
if you get zero Advantage for a bit you
if you get zero Advantage for a bit you
just you know you just learn the value
just you know you just learn the value
function that's
fine e
so this latest run
here we're going to just just have to
here we're going to just just have to
rerun uh The Sweep to see you know it's
rerun uh The Sweep to see you know it's
hard to say I think in principle this
hard to say I think in principle this
algorithm now makes way more sense uh it
algorithm now makes way more sense uh it
has some properties that were clearly
has some properties that were clearly
missing
before but ultimately we will have to
before but ultimately we will have to
see
let me start working on committing this
let me start working on committing this
stuff up
106 all right so it's on
par
e e
sweep get this token
I think in principle at the very least
I think in principle at the very least
the new waiting is a it makes a lot of
sense it makes a lot of
sense it makes a lot of
sense we'll see whether I'm right cu the
sense we'll see whether I'm right cu the
current experiment just with the same
current experiment just with the same
hypers does not do very well at all but
hypers does not do very well at all but
you never know right we changed it a lot
you never know right we changed it a lot
it could require completely different
it could require completely different
tuning this is why you need to be able
tuning this is why you need to be able
to run massive hyper parameter sweeps
to run massive hyper parameter sweeps
otherwise I'd be sitting here for the
otherwise I'd be sitting here for the
next week like wondering if I'm stupid
next week like wondering if I'm stupid
or not I just get to know if I'm stupid
or not I just get to know if I'm stupid
in a couple
in a couple
hours let's go check
hours let's go check
out
Neptune make sure this works
okay great we'll let this go and we'll
okay great we'll let this go and we'll
uh we'll see how it
uh we'll see how it
does I will be probably back later today
does I will be probably back later today
working on other stuff depends we'll see
working on other stuff depends we'll see
how it goes I'm going to have a long uh
how it goes I'm going to have a long uh
workday plan tomorrow so might
workday plan tomorrow so might
not um but I will be at least checking
not um but I will be at least checking
back on these
back on these
things exciting possibilities as soon as
things exciting possibilities as soon as
we get the scaling right I I think we're
we get the scaling right I I think we're
going to have something really special
going to have something really special
here so if you're interested in
here so if you're interested in
following this and other work of mine
following this and other work of mine
buffer. it's all free an open source
buffer. it's all free an open source
star the GitHub to help us out uh join
star the GitHub to help us out uh join
the Discord if you want to get involved
the Discord if you want to get involved
and follow on X for more RL
and follow on X for more RL
announcements and stuff like that
