Kind: captions
Language: en
good
good
morning we are
back I remember um something about what
back I remember um something about what
we were doing yesterday
so oops
what we're looking
what we're looking
at we were looking at whoops move this
at we were looking at whoops move this
over here
over here
uh the
uh the
variance estimate of Gan processes as
variance estimate of Gan processes as
well as the overall fit to determine how
well as the overall fit to determine how
we want to make use of these in our
we want to make use of these in our
hyper parameters we bgo because the
hyper parameters we bgo because the
defaults really don't seem very
defaults really don't seem very
good um so St stuff like this is kind of
good um so St stuff like this is kind of
interesting I welcome morning
man it's tough because
here the solution is not necessarily to
here the solution is not necessarily to
fit the curv
how can I get something that's more
how can I get something that's more
representative I wonder
and then how can I figure out whether
and then how can I figure out whether
the thing that I come up with is going
the thing that I come up with is going
to be useful for actual real problems or
to be useful for actual real problems or
if I'm just overfitting to the synthetic
task hi morning
task hi morning
welcome
well see the issue here that I have is
well see the issue here that I have is
that I don't think the linear colel
that I don't think the linear colel
linear kernel is really not doing what I
linear kernel is really not doing what I
want
right like if I were to put a linear
kernel it's not like piecewise linear or
kernel it's not like piecewise linear or
or
anything e
yeah so if I just do
yeah so if I just do
kernel linear
kernel it just kind of does
kernel it just kind of does
this it fits a line to this part of the
this it fits a line to this part of the
data over
data over
here which is where uh the support is
here which is where uh the support is
this is where your real data points are
this is where your real data points are
and then it's trying to
extrapolate and then if I do like a
extrapolate and then if I do like a
really soft matern kernel
instead what if I do a really soft
instead what if I do a really soft
matern kernel
see this is so awkward
like like how's people stupid as that
not this one
this the back to just being a my turn
this the back to just being a my turn
kernel
right yeah so that doesn't help at
all for
see what I
see what I
want I want this and then I want to just
want I want this and then I want to just
drop
here
e e
that do
something ah that does do
something that does do
something that does do
something okay so we just have to come
something okay so we just have to come
up
up
with with a good mean function then
right
e
e e
it's very silly
because yeah the contribution of the Gan
because yeah the contribution of the Gan
processes just screwed
up it's reverting down to zero
h
let me see something so if we
take
for for
so this is this and then we do
see do this
and so here's your
distance let me see if I can do this
start
for e
AR
sort for
oh wait I don't need this they're
oh wait I don't need this they're
already tensors
something like
this for
see for
okay
um and then if we expand k
you see it dips down
here it should dip down more than this
here it should dip down more than this
though
because without it you get this
right uh so it doesn't dip down very
right uh so it doesn't dip down very
much at the moment because of the kernel
much at the moment because of the kernel
that's fine hold
that's fine hold
on hold on a
second return
do this
okay so
okay so
here this is just the matern kernel
here this is just the matern kernel
right now we do mean
function how many points do we
function how many points do we
have 100 points
I doing this right let me see
e
xarray messed this up hold on I think I
xarray messed this up hold on I think I
messed this
messed this
up
for
e e
this should be very easy I'm just half
asleep no I did have it right didn't
wait now this is bogus what am I doing
here for each sample you're
here for each sample you're
comparing you're going to find the
comparing you're going to find the
nearest
X well hang on we can test this return
X well hang on we can test this return
x.
x.
mean right we can just do that because
mean right we can just do that because
it knows y do
mean we can just do
that okay same exact thing
so I thought that's a little
so I thought that's a little
weird Point um
mean is
one guess the mean is one okay that is
correct just being
correct just being
silly uh we'll leave that
silly uh we'll leave that
alone so this is what we have at the
alone so this is what we have at the
moment we have this with the nearest
moment we have this with the nearest
neighbor metric uh this this is over all
neighbor metric uh this this is over all
the neighbors and if we change y to be
the neighbors and if we change y to be
like or k to be like
10 then this is something that's a fair
10 then this is something that's a fair
bit better
here taking this neighborhood if we did
here taking this neighborhood if we did
k equal 1 this is just like a nearest
k equal 1 this is just like a nearest
sample right so this will give us like a
sample right so this will give us like a
flat flat
flat flat
line yeah this gives us perfectly flat
line yeah this gives us perfectly flat
line just from the nearest Point
cool and then let's see what happens
cool and then let's see what happens
when we add in like one extra point like
this okay interesting so we get this
this okay interesting so we get this
sort of
sort of
behavior I actually like this
and then if we change uh K to two
and then if we change uh K to two
here does this substantially change the
shape little bit not
shape little bit not
really and then if we use because I
really and then if we use because I
think it is more reasonable to use like
think it is more reasonable to use like
a a k of like five at
least or maybe like I don't know square
least or maybe like I don't know square
root numb
points okay so how's this
happen that's weird is that
consistent yeah so there's a point here
there's got to just be some weird
discontinuity but this is I think what
discontinuity but this is I think what
we uh what we want let's see without
we uh what we want let's see without
this what does it
do so without
this yeah you see it drops all the way
this yeah you see it drops all the way
back down to zero
so this is kind of
silly I don't know how to be honest
I think we need to do this on the real
I think we need to do this on the real
data don't we well synthetic data still
data don't we well synthetic data still
but on the real problem
this is still
not not
amazing it's cool that we found a way to
amazing it's cool that we found a way to
do this
one he set up the
repo
welcome let me know if you run into any
difficulties
for
e
e e
did I now directly start making an
N yeah um
N yeah um
here remind me because I've had like a
here remind me because I've had like a
ton of different people with a ton of
ton of different people with a ton of
different user did my camera just
different user did my camera just
freeze hang
on okay um yeah remind me what
on okay um yeah remind me what
conversation we had because I've had
conversation we had because I've had
like five or six people in the last few
like five or six people in the last few
days cycle through here on a bunch of
days cycle through here on a bunch of
different accounts with twitch YouTube
different accounts with twitch YouTube
and uh
X jug my memory real quick would
you and I can show you where the M stuff
you and I can show you where the M stuff
is
so we have the docs here but also and
so we have the docs here but also and
this is in the
docs this is the simplest possible
docs this is the simplest possible
environment
um it's a tiny little python file this
um it's a tiny little python file this
is all that's in
is all that's in
here there is a tiny little test file
here there is a tiny little test file
and see this is all that's in here
and then here is the whole code here is
and then here is the whole code here is
lines for just a very simple test
lines for just a very simple test
environment in renderer so I suggest
environment in renderer so I suggest
using this as a template just like copy
using this as a template just like copy
squared over and then go from there
squared over and then go from there
because you can see all the logic is in
because you can see all the logic is in
squar Doh
squar Doh
here how to set up on Mac yeah okay
here how to set up on Mac yeah okay
gotcha yeah it'll work on Mac just fine
gotcha yeah it'll work on Mac just fine
for you just get cling 18 and you'll be
for you just get cling 18 and you'll be
good um well the thing that I didn't get
good um well the thing that I didn't get
from you right was what you're trying to
from you right was what you're trying to
do with puffer lib I don't think um did
do with puffer lib I don't think um did
you say what you were trying to do with
you say what you were trying to do with
puffer
puffer
lib do you have a specific problem are
lib do you have a specific problem are
you new to RL are you experienced an RL
you new to RL are you experienced an RL
and like trying to use it for something
and like trying to use it for something
like that I don't know if I got that
like that I don't know if I got that
background out of
background out of
you because you know what I suggest will
you because you know what I suggest will
differ depending on
circumstance
for
e e
you also want to figure it out good
you also want to figure it out good
experience in Python like your glasses
experience in Python like your glasses
thank you they are the cheapest possible
thank you they are the cheapest possible
glasses uh because I just end up getting
glasses uh because I just end up getting
new pairs of them
constantly and I typically just wear
constantly and I typically just wear
them when I'm being lazy too lazy to put
them when I'm being lazy too lazy to put
contacts in
contacts in
anyways um good experience with python
anyways um good experience with python
new to RL so yeah uh in that case then
new to RL so yeah uh in that case then
for newcomers to
for newcomers to
RL I I wrote a guide specifically for
you this is my RL quickart guide I
you this is my RL quickart guide I
suggest spending uh 80% of your time
suggest spending uh 80% of your time
that you're going to allocate to RL like
that you're going to allocate to RL like
building new stuff and like learning by
building new stuff and like learning by
building and then 20% of time going
building and then 20% of time going
through the formal resources here this
through the formal resources here this
gives you a just a very quick outline of
gives you a just a very quick outline of
stuff in reinforcement learning and a
stuff in reinforcement learning and a
little bit of perspective it links a
little bit of perspective it links a
bunch of papers that again you know 20%
bunch of papers that again you know 20%
of your time that you're going to
of your time that you're going to
allocate this on
allocate this on
papers um and then uh it suggests that
papers um and then uh it suggests that
you
you
essentially build just build a very
essentially build just build a very
simple test M and get like get an agent
simple test M and get like get an agent
doing something on a new task right and
doing something on a new task right and
I have a template for you squared as a
I have a template for you squared as a
template all the other environments that
template all the other environments that
are in here can be used as references
are in here can be used as references
and then myself and others are around on
and then myself and others are around on
Discord if you get stuck
[Music]
you're not here to ask me about crypto
you're not here to ask me about crypto
are you I've had
are you I've had
uh a bunch of accounts getting very
uh a bunch of accounts getting very
creative with that in the last couple of
creative with that in the last couple of
days
yeah the thing that's weird here is I
yeah the thing that's weird here is I
don't think this is reflective
of what we see on our Gan
processes maybe it is maybe I just
processes maybe it is maybe I just
didn't bother to plot it header files
didn't bother to plot it header files
main
main
logic yeah so the way it works right all
logic yeah so the way it works right all
the logic goes in the header the C file
the logic goes in the header the C file
is just a standalone test I suggest not
is just a standalone test I suggest not
recompiling the whole python stack every
recompiling the whole python stack every
single time so when you're developing
single time so when you're developing
you can just use scripts build ocean sh
you can just use scripts build ocean sh
um and then that will let you build
um and then that will let you build
locally so you can test locally and see
locally so you can test locally and see
it's very convenient it's very very fast
it's very convenient it's very very fast
and it comes with um if you use clang 18
and it comes with um if you use clang 18
on a Mac at least you can get like f
on a Mac at least you can get like f
sanitized flags with the address
sanitized flags with the address
sanitizer so then you don't have to deal
sanitizer so then you don't have to deal
with memory faults and stuff like that
with memory faults and stuff like that
um basically C becomes as easy as
um basically C becomes as easy as
python
python
um and then yeah binding the scyon syon
um and then yeah binding the scyon syon
is the intermediary layer and then
is the intermediary layer and then
python is just the top
level figure this
out
for e
we can technically use this can't we
yeah yeah wait we can we can just hack
yeah yeah wait we can we can just hack
this okay I know what we're going to do
this okay I know what we're going to do
this is going to be good um hopefully I
this is going to be good um hopefully I
can get this done quickly
enough in the morning Dev
session take
this put this
here
here
okay oh not that
much it's indented
much it's indented
three oops
okay and now we
have clost and raw scores
is this more crypto did I
guess yep
how I
how I
guess no I don't deal with Krypto on
guess no I don't deal with Krypto on
here
I'm going to give you the benefit of the
I'm going to give you the benefit of the
doubt and assume that and just like
doubt and assume that and just like
suppose that you're not the same crypto
suppose that you're not the same crypto
person who's been harassing me on here
person who's been harassing me on here
for the last few days but um no I I
for the last few days but um no I I
don't do crypto I do AI this is all
don't do crypto I do AI this is all
reinforcement learning development
EX for
no there's quite the coincidence then
no there's quite the coincidence then
because I
because I
have I really don't have anybody
have I really don't have anybody
following me for
following me for
crypto so you can imagine the
crypto so you can imagine the
suspicion I've uh been doing
suspicion I've uh been doing
reinforcement learning here for the
reinforcement learning here for the
past I don't know when have I been doing
past I don't know when have I been doing
this live since closing and on a year
now why do did condic compiler need
now why do did condic compiler need
c why did the condic compiler needs C
prefixes
uh cond ships are just a horribly broken
uh cond ships are just a horribly broken
C compiler man it's I don't even know
C compiler man it's I don't even know
how they manage to break it as badly as
how they manage to break it as badly as
they have all right cuz the error that
they have all right cuz the error that
the the conflict of name error that this
the the conflict of name error that this
caus it shouldn't even be possible so I
caus it shouldn't even be possible so I
don't know how they managed to break it
don't know how they managed to break it
that
that
badly you invest in crypto
badly you invest in crypto
privately uh not in meme coins I own a
privately uh not in meme coins I own a
couple ETFs as a very small fraction of
couple ETFs as a very small fraction of
overall Investments I believe
mostly I just have Tex STS
and then puffer has been the latest
and then puffer has been the latest
investment because buying your own
investment because buying your own
Hardware is
expensive but we're well positioned for
expensive but we're well positioned for
that
now we just got a little bit of
now we just got a little bit of
additional
additional
Revenue yesterday
Revenue yesterday
will be
will be
useful and uh
useful and uh
yeah couple more things on the horizon
yeah couple more things on the horizon
potentially for puffer as
potentially for puffer as
well really I think just one one more
well really I think just one one more
good contract and we'll be very
good contract and we'll be very
comfortable and any more than that will
comfortable and any more than that will
be sitting very
pretty Okay so
we have the raw values
here and
then what do we plot
here un normalized score
here un normalized score
mean and unnormalized was a cost
so we do cost
so we do cost
mean and unnormalized foran is that what
mean and unnormalized foran is that what
we
do
e e
that's neoc
cars I'm not gay man
cars I'm not gay man
I'll take it as a compliment but I'm not
gay what happened
here
here
oh Max
oops that was dumb
really what the
heck well this is moderately
insane hang on how is
this ah
what well that answers why stuff isn't
what well that answers why stuff isn't
working
working
holy hell
holy hell
um I not fit the model or something
what
e
e e
e e
I got to stop streaming so
I got to stop streaming so
late I'm not have asleep this morning
probably have a hard cut off
probably have a hard cut off
around nine or
whatever come on this should be
whatever come on this should be
basic brain is just not processing right
now
now
unnormalized oh wait do we need the
unnormalized oh wait do we need the
untransformed
untransformed
one hang on I think we
need this is still transformed we need
need this is still transformed we need
untransformed scor
mean there we go
all right do these predictions still
suck much
better way way better
okay this is something we can maybe work
okay this is something we can maybe work
with here
right cuz we can actually see exactly
right cuz we can actually see exactly
what's going on
here this is kind of
cool
for
e e
this is actually not terrible I think
like this is this is the linear kernel
like this is this is the linear kernel
presumably
presumably
right that's giving us this
correlation actually let's
um we should be able to verify that
right so if we get rid of the linear
kernel what
kernel what
happens so from
happens so from
this this is what we had
before yeah so we get something quite
before yeah so we get something quite
different
well the support data is going to be a
well the support data is going to be a
little different here
little different here
right it's not as crazy different as I
right it's not as crazy different as I
would expect
well this is not good right like look
well this is not good right like look
it's overfitting right here it's
overshooting it's basically has this
overshooting it's basically has this
flat prediction here and this is not
flat prediction here and this is not
correct I don't know where it learned
correct I don't know where it learned
this
from and
from and
how for
okay so this is the linear kernel but
okay so this is the linear kernel but
the thing is it's log linear because
the thing is it's log linear because
it's done in log
space and what test is this on
one let's do synthetic log test
one let's do synthetic log test
again so like here
this damn
this damn
it broke my
terminal okay so this log fit should
terminal okay so this log fit should
actually be decent for a good number of
actually be decent for a good number of
tasks here yeah so this is the log fit
tasks here yeah so this is the log fit
that we
that we
get uh it pretty closely matches in the
support and then it starts underfitting
support and then it starts underfitting
a little here this is pretty decent
overall and I think that this is a good
overall and I think that this is a good
starting point
starting point
because most cost curves should look
because most cost curves should look
like this
like this
so linear kernel in log space is a good
idea and then when we add in the matern
kernel okay we're going to do this and
kernel okay we're going to do this and
maybe it'll stop breaking my
maybe it'll stop breaking my
quick
terminal and then what's the mat interal
terminal and then what's the mat interal
do for us
right h
now that's pretty
good now the question is why is it so
good now the question is why is it so
confident about all
this like
if we do kernel is matern
if we do kernel is matern
Kernel and then length
scale does this give
scale does this give
us what we would expect
all of our predictions are right here
all of our predictions are right here
somewhere that does not make
sense they should not be able to go to
sense they should not be able to go to
the left like that they should only be
the left like that they should only be
able to go
oh you are using it to predict cost as
oh you are using it to predict cost as
well so it can actually shift to the
well so it can actually shift to the
left that's
funny e
we really should divide this cost data
we really should divide this cost data
by Max cost or something shouldn't
we it's actually kind of cool how with
we it's actually kind of cool how with
the log fit it's it's just decent on its
the log fit it's it's just decent on its
own for
far too confident is the thing
let's see if we can just scale the score
let's see if we can just scale the score
prediction one by a little bit like this
that's very
weird
so wait what
h
it's very weird it pushes the
it's very weird it pushes the
cost why does it do that to the cost
cost why does it do that to the cost
hang on
hang on
you score it right this these should be
you score it right this these should be
independent that doesn't make
sense okay we got to figure something
sense okay we got to figure something
out here because that doesn't make any
out here because that doesn't make any
bloody sense BP
score for
maybe we found something
here
e e
what is this thing trained
on normalize log cost right and this is
on normalize log cost right and this is
only trained on stats
from from
here if I comment this it's going to be
here if I comment this it's going to be
different right
is that the
case wait
what hold on
um
um
me something's weird here
okay so this do train is not hold on
okay so this do train is not hold on
this do train is just a train
mode use the source installation method
mode use the source installation method
yep that's
yep that's
fine robust way to check
installation uh yeah see if you can run
installation uh yeah see if you can run
stuff though did I break something
stuff though did I break something
lately with this I might have broken
lately with this I might have broken
something for you just
yesterday hold on
I want to get off of this Branch at the
moment so the one thing I think I broke
moment so the one thing I think I broke
yesterday that you might have to look at
yesterday that you might have to look at
it should take you two
seconds so this is in the docks I
seconds so this is in the docks I
believe as well but we use this script
believe as well but we use this script
to build
locally
and where is it
and where is it
yeah this flag you're going to have to
yeah this flag you're going to have to
swap this for the Mac one you'll see
swap this for the Mac one you'll see
there's a rib like 5.0 Mac OS or
there's a rib like 5.0 Mac OS or
whatever you just have to swap this path
whatever you just have to swap this path
and this path you should be able to
and this path you should be able to
build for
build for
Mac so you can just build some of the
Mac so you can just build some of the
environments you can build like neural
environments you can build like neural
M3 or squared or whatever and uh see if
M3 or squared or whatever and uh see if
it works for
it works for
you morning
so I have no idea where this score data
so I have no idea where this score data
is coming from
is coming from
because think we're fitting to
because think we're fitting to
this hang is this train not what I think
this hang is this train not what I think
it is
so I'm not training the the GP here at
all that doesn't make any sense
I mean it is a log distribution by
I mean it is a log distribution by
default
hang
on so if I just comment the kernel will
on so if I just comment the kernel will
it mess it up if I comment the linear
it mess it up if I comment the linear
kernel is in log space
no it's still a perfect fit okay well
no it's still a perfect fit okay well
something is bizarre because it
something is bizarre because it
shouldn't be
training is somehow
wait I'll be back in a few
wait I'll be back in a few
minutes I got to figure this out be
minutes I got to figure this out be
right
back
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e
okay we have the Phantom goian process
okay we have the Phantom goian process
to deal with
now so we need this thing
now so we need this thing
right and then the question is
right and then the question is
why it's probably going to just be
why it's probably going to just be
something
something
dumb so if you don't estimate cost the
dumb so if you don't estimate cost the
cost is zero cuz it's a g in process but
cost is zero cuz it's a g in process but
now you know you'd expect the cost to go
now you know you'd expect the cost to go
out but for some reason the score also
out but for some reason the score also
gets predicted why does the score get
gets predicted why does the score get
predicted because let me
see
see
unformed is this what I use
unformed is this what I use
oops
oops
untr form scoring
and then I plot un transform foring okay
and then I plot un transform foring okay
so this comes from a normalized Waring
so this comes from a normalized Waring
which comes from normalized quing which
which comes from normalized quing which
comes
from from the gum process
right suggestions comes from these
right suggestions comes from these
points so yeah I don't
see how an untrained gaussian process
see how an untrained gaussian process
um is producing
this especially with we just have the
this especially with we just have the
matern kernel
matern kernel
right I guess it's
possible
possible
okay maybe there's some weird thing with
okay maybe there's some weird thing with
this wouldn't think
this wouldn't think
so for
super fast got it running great
yeah puffer is meant to be easy if it is
yeah puffer is meant to be easy if it is
not easy I have screwed
up but also they're kind of a million
up but also they're kind of a million
other things demanding my attention
so things get fixed in
so things get fixed in
waves at the moment it's figuring out
waves at the moment it's figuring out
what is wrong with Hyper parameter
what is wrong with Hyper parameter
opt
e e
really
not I mean is there some quirk
is there like some Quirk with
this set the data
okay so it's just it's somehow it's
okay so it's just it's somehow it's
running
training yeah somehow it's just it's
training yeah somehow it's just it's
running training even though I'm telling
running training even though I'm telling
it I'm like telling it not to fine
that's just uh pyro being weird it's not
that's just uh pyro being weird it's not
me being weird or stupid it's pyro being
me being weird or stupid it's pyro being
weird and stupid
fine
e
e e
what if we plot
the the variance do we plot the variance
the the variance do we plot the variance
is that what we should
do gear Labs s
toal mimic moves
is this gy
fan I have Jim fan muted for uh he's
fan I have Jim fan muted for uh he's
like one of the worst Defenders
like one of the worst Defenders
for like
um just inflated claims about stuff and
um just inflated claims about stuff and
generally misrepresenting scientific
work so if that's from his lab I have
work so if that's from his lab I have
very little
faith e
if you want documentation of that just
if you want documentation of that just
go look back at how we presented um
go look back at how we presented um
Voyager and then go read the real paper
and the funny thing is that the group
and the funny thing is that the group
does uh actually does good work it's
does uh actually does good work it's
just you can never tell what the work
just you can never tell what the work
actually is because it's always
misrepresented
for e
so for the folks on YouTube what I'm
so for the folks on YouTube what I'm
currently trying to figure out here
currently trying to figure out here
that's driving me
that's driving me
nuts
nuts
um the gaussian process or gaussian
um the gaussian process or gaussian
processes here are what we rely on to
processes here are what we rely on to
estimate how good a set of hyper
estimate how good a set of hyper
parameters is so we know if it's worth
parameters is so we know if it's worth
running the experiment or not um it's
running the experiment or not um it's
giving weirdly biased predictions where
giving weirdly biased predictions where
it is
it is
overconfident um in its estimation it's
overconfident um in its estimation it's
overconfident in its predictions in
overconfident in its predictions in
regions where it really shouldn't be
also why is
also why is
um what the hell's wrong with my
um what the hell's wrong with my
plugins we're going to
plugins we're going to
work leave me my
work leave me my
typing
for
e e
does this do anything
not at
not at
all doesn't do a
thing did the colonels turn off the
thing did the colonels turn off the
stream yesterday no the Linux drivers
stream yesterday no the Linux drivers
did that
if anybody knows how to
if anybody knows how to
fix freaking uh ethernet on a buun 24
fix freaking uh ethernet on a buun 24
that would be great because on average
that would be great because on average
once every day or two it will just
once every day or two it will just
randomly die and nothing I do um gets it
randomly die and nothing I do um gets it
to turn back on without rebooting
I tried all the basic stuff I could
I tried all the basic stuff I could
think
think
of freaking Linux drivers
there just no variance
huh you
huh you
just no
variance if I look at this
yeah it's completely confident in its
predictions on the entire freaking Paro
front all
front all
right is that konal dependent
linear
linear
kernel let's see if it's still very
kernel let's see if it's still very
confident
yeah so the variance is just a useless
yeah so the variance is just a useless
metric
h
see the problem is you've
see the problem is you've
got top of the morning to
you e
I would love to know why it is so
I would love to know why it is so
confident all the way out at the
extremities should my States be Marian
extremities should my States be Marian
for RL
for RL
you can ignore the is you can ignore the
you can ignore the is you can ignore the
existence of that word and you will be
existence of that word and you will be
better
off
um yeah we the theoretical results are
um yeah we the theoretical results are
they quickly go out the window all the
they quickly go out the window all the
guarantees with stuff that we throw RL
guarantees with stuff that we throw RL
on it will still
on it will still
work what you should think about is
work what you should think about is
whether the problem is actually
whether the problem is actually
learnable e
okay you know what maybe there is
okay you know what maybe there is
something here which
is why is this thing
why is this thing so
why is this thing so
confident all the way out at the
end oh you know what
should it put a bump in input space or
should it put a bump in input space or
output
space yeah
space yeah
okay in I
okay in I
guess the support set is not in Ely what
guess the support set is not in Ely what
we think it is in this graph
fine cuz it's actually the higher
fine cuz it's actually the higher
Dimension fine okay so we can't rely on
Dimension fine okay so we can't rely on
variance
and a lot of the utility of these things
and a lot of the utility of these things
goes out the
window
e e
the thing is that like the exact reason
the thing is that like the exact reason
I'm
I'm
using gaan presses at the moment for
using gaan presses at the moment for
this is the thing that's not working
okay let's try some stuff
TR
this what's this do for
us holy all
right so at least get this to have a
right so at least get this to have a
little bit of shape right what was it
little bit of shape right what was it
two
times been fall I understand 10% of what
times been fall I understand 10% of what
you do but I believe in it I mean I'm
you do but I believe in it I mean I'm
also not explaining what I'm doing here
also not explaining what I'm doing here
amazingly because I'm trying to think
amazingly because I'm trying to think
about this at the same time
um I have some videos that
um I have some videos that
are have you seen the iceberg video the
are have you seen the iceberg video the
iceberg video is a pretty good outline
iceberg video is a pretty good outline
it's a decent balance of technical depth
it's a decent balance of technical depth
and high level I mean like the thing I'm
and high level I mean like the thing I'm
trying to do at a high level is not
trying to do at a high level is not
ridiculous ly complicated right
ridiculous ly complicated right
reinforcement learning is an important
reinforcement learning is an important
branch of
branch of
science the field has been subject to
science the field has been subject to
absolutely terrible engineering and
absolutely terrible engineering and
performance optimization for the last 10
performance optimization for the last 10
years this has made all of research go
years this has made all of research go
100 times slower than it should have
100 times slower than it should have
this means that you run a 100 times
this means that you run a 100 times
fewer experiments than you should this
fewer experiments than you should this
means that your results are very
means that your results are very
inconclusive so the whole thing is very
inconclusive so the whole thing is very
wishy-washy and feels very janky I'm
wishy-washy and feels very janky I'm
making everything fast this lets us do
making everything fast this lets us do
consistent and accurate science I have
consistent and accurate science I have
made everything fast or a lot of it fast
made everything fast or a lot of it fast
now now I'm using all the fast things
now now I'm using all the fast things
that I've buil to do really consistent
that I've buil to do really consistent
and accurate
and accurate
science this will make reinforcement
science this will make reinforcement
learning broadly useful this year
sure okay here's an idea then what if we
sure okay here's an idea then what if we
crank
up what if we crank this one up so now
up what if we crank this one up so now
what this should give us is the gaussian
what this should give us is the gaussian
process should sample very far away from
support yes
so you can see there no supporting data
so you can see there no supporting data
points in here but it's still like
points in here but it's still like
making confident
predictions this is not
good okay let's look at the kernel
look at the mat konel
first let's really crank this
I guess that is a uh reg
I guess that is a uh reg
X not do
X not do
that I got to go in a little bit as
well I want to at least get something
well I want to at least get something
figured
out let's do this so now it's sampling
out let's do this so now it's sampling
crazy
points
what did I break it
oh I
see well that's awkward
okay let me explain what's happening and
okay let me explain what's happening and
I figured this out earlier I don't know
I figured this out earlier I don't know
why I forgot about
this so both cost and score being
this so both cost and score being
modeled by a gan process when you're
modeled by a gan process when you're
outside the support which means outside
outside the support which means outside
the spot the space where the points are
the spot the space where the points are
informing you gum processes go to zero
informing you gum processes go to zero
so for support uh for score that's what
so for support uh for score that's what
you want maybe but for cost you don't
you want maybe but for cost you don't
want it to systematically underestimate
want it to systematically underestimate
cost so what you need to do here and
cost so what you need to do here and
this is what we'll get done for right
this is what we'll get done for right
now um where's the mean
now um where's the mean
function
lost so we'll do where's
lost so we'll do where's
this Max log cost
get rid of
that for
okay
so GP clo me
and
X see what this
does for
he
you know what we can do instead it'll be
you know what we can do instead it'll be
way easier than
way easier than
this
on
on
idx we'll just do Max C idx
idx we'll just do Max C idx
equals
Max I think this
is
75 just do
is
it noriz
maxal X
okay way
okay way
better immediately way
better yeah that was it
better yeah that was it
okay jeez
in fact we could even have it be more
aggressive so if I do this then the
aggressive so if I do this then the
really unconfident predictions will get
really unconfident predictions will get
pulled they'll get stretched even
more
boom so we will mess with this and
boom so we will mess with this and
figure out the appropriate
figure out the appropriate
the appropriate value here
um this one seems too
much but
effectively you want to assume
effectively you want to assume
that when you don't have information
that when you don't have information
about a
about a
point it's going to be more expensive
point it's going to be more expensive
and perform less
and perform less
uh not as well as you
uh not as well as you
expect
so score function seems to be taken care
of though I'm not sure I don't think it
of though I'm not sure I don't think it
reverts to zero I think it reverts to
reverts to zero I think it reverts to
like some intermediate
like some intermediate
point I have to look at that but this is
point I have to look at that but this is
made like this is big
made like this is big
progress that's way better than before
actually this would explain why uh cost
actually this would explain why uh cost
was being consistently overestimated as
well freaking gaussian processes man I'm
well freaking gaussian processes man I'm
going to keep using these for a bit
going to keep using these for a bit
because um I mean these seem like very
because um I mean these seem like very
useful in hyper pram optimization
useful in hyper pram optimization
literature overall but I will not be
literature overall but I will not be
surprised if we just get to replace
surprised if we just get to replace
these with logistic progression or
these with logistic progression or
something cuz these are so unwieldy and
something cuz these are so unwieldy and
yeah
yeah
there all these details that you need to
there all these details that you need to
get right or you're screwed all right
get right or you're screwed all right
I'm going to get some breakfast I'm
I'm going to get some breakfast I'm
going to go for a run I got a couple
going to go for a run I got a couple
meetings and then uh I will be back
meetings and then uh I will be back
working on some stuff maybe this maybe
working on some stuff maybe this maybe
some robotics we shall

Kind: captions
Language: en
good
good
morning we are
back I remember um something about what
back I remember um something about what
we were doing yesterday
so oops
what we're looking
what we're looking
at we were looking at whoops move this
at we were looking at whoops move this
over here
over here
uh the
uh the
variance estimate of Gan processes as
variance estimate of Gan processes as
well as the overall fit to determine how
well as the overall fit to determine how
we want to make use of these in our
we want to make use of these in our
hyper parameters we bgo because the
hyper parameters we bgo because the
defaults really don't seem very
defaults really don't seem very
good um so St stuff like this is kind of
good um so St stuff like this is kind of
interesting I welcome morning
man it's tough because
here the solution is not necessarily to
here the solution is not necessarily to
fit the curv
how can I get something that's more
how can I get something that's more
representative I wonder
and then how can I figure out whether
and then how can I figure out whether
the thing that I come up with is going
the thing that I come up with is going
to be useful for actual real problems or
to be useful for actual real problems or
if I'm just overfitting to the synthetic
task hi morning
task hi morning
welcome
well see the issue here that I have is
well see the issue here that I have is
that I don't think the linear colel
that I don't think the linear colel
linear kernel is really not doing what I
linear kernel is really not doing what I
want
right like if I were to put a linear
kernel it's not like piecewise linear or
kernel it's not like piecewise linear or
or
anything e
yeah so if I just do
yeah so if I just do
kernel linear
kernel it just kind of does
kernel it just kind of does
this it fits a line to this part of the
this it fits a line to this part of the
data over
data over
here which is where uh the support is
here which is where uh the support is
this is where your real data points are
this is where your real data points are
and then it's trying to
extrapolate and then if I do like a
extrapolate and then if I do like a
really soft matern kernel
instead what if I do a really soft
instead what if I do a really soft
matern kernel
see this is so awkward
like like how's people stupid as that
not this one
this the back to just being a my turn
this the back to just being a my turn
kernel
right yeah so that doesn't help at
all for
see what I
see what I
want I want this and then I want to just
want I want this and then I want to just
drop
here
e e
that do
something ah that does do
something that does do
something that does do
something okay so we just have to come
something okay so we just have to come
up
up
with with a good mean function then
right
e
e e
it's very silly
because yeah the contribution of the Gan
because yeah the contribution of the Gan
processes just screwed
up it's reverting down to zero
h
let me see something so if we
take
for for
so this is this and then we do
see do this
and so here's your
distance let me see if I can do this
start
for e
AR
sort for
oh wait I don't need this they're
oh wait I don't need this they're
already tensors
something like
this for
see for
okay
um and then if we expand k
you see it dips down
here it should dip down more than this
here it should dip down more than this
though
because without it you get this
right uh so it doesn't dip down very
right uh so it doesn't dip down very
much at the moment because of the kernel
much at the moment because of the kernel
that's fine hold
that's fine hold
on hold on a
second return
do this
okay so
okay so
here this is just the matern kernel
here this is just the matern kernel
right now we do mean
function how many points do we
function how many points do we
have 100 points
I doing this right let me see
e
xarray messed this up hold on I think I
xarray messed this up hold on I think I
messed this
messed this
up
for
e e
this should be very easy I'm just half
asleep no I did have it right didn't
wait now this is bogus what am I doing
here for each sample you're
here for each sample you're
comparing you're going to find the
comparing you're going to find the
nearest
X well hang on we can test this return
X well hang on we can test this return
x.
x.
mean right we can just do that because
mean right we can just do that because
it knows y do
mean we can just do
that okay same exact thing
so I thought that's a little
so I thought that's a little
weird Point um
mean is
one guess the mean is one okay that is
correct just being
correct just being
silly uh we'll leave that
silly uh we'll leave that
alone so this is what we have at the
alone so this is what we have at the
moment we have this with the nearest
moment we have this with the nearest
neighbor metric uh this this is over all
neighbor metric uh this this is over all
the neighbors and if we change y to be
the neighbors and if we change y to be
like or k to be like
10 then this is something that's a fair
10 then this is something that's a fair
bit better
here taking this neighborhood if we did
here taking this neighborhood if we did
k equal 1 this is just like a nearest
k equal 1 this is just like a nearest
sample right so this will give us like a
sample right so this will give us like a
flat flat
flat flat
line yeah this gives us perfectly flat
line yeah this gives us perfectly flat
line just from the nearest Point
cool and then let's see what happens
cool and then let's see what happens
when we add in like one extra point like
this okay interesting so we get this
this okay interesting so we get this
sort of
sort of
behavior I actually like this
and then if we change uh K to two
and then if we change uh K to two
here does this substantially change the
shape little bit not
shape little bit not
really and then if we use because I
really and then if we use because I
think it is more reasonable to use like
think it is more reasonable to use like
a a k of like five at
least or maybe like I don't know square
least or maybe like I don't know square
root numb
points okay so how's this
happen that's weird is that
consistent yeah so there's a point here
there's got to just be some weird
discontinuity but this is I think what
discontinuity but this is I think what
we uh what we want let's see without
we uh what we want let's see without
this what does it
do so without
this yeah you see it drops all the way
this yeah you see it drops all the way
back down to zero
so this is kind of
silly I don't know how to be honest
I think we need to do this on the real
I think we need to do this on the real
data don't we well synthetic data still
data don't we well synthetic data still
but on the real problem
this is still
not not
amazing it's cool that we found a way to
amazing it's cool that we found a way to
do this
one he set up the
repo
welcome let me know if you run into any
difficulties
for
e
e e
did I now directly start making an
N yeah um
N yeah um
here remind me because I've had like a
here remind me because I've had like a
ton of different people with a ton of
ton of different people with a ton of
different user did my camera just
different user did my camera just
freeze hang
on okay um yeah remind me what
on okay um yeah remind me what
conversation we had because I've had
conversation we had because I've had
like five or six people in the last few
like five or six people in the last few
days cycle through here on a bunch of
days cycle through here on a bunch of
different accounts with twitch YouTube
different accounts with twitch YouTube
and uh
X jug my memory real quick would
you and I can show you where the M stuff
you and I can show you where the M stuff
is
so we have the docs here but also and
so we have the docs here but also and
this is in the
docs this is the simplest possible
docs this is the simplest possible
environment
um it's a tiny little python file this
um it's a tiny little python file this
is all that's in
is all that's in
here there is a tiny little test file
here there is a tiny little test file
and see this is all that's in here
and then here is the whole code here is
and then here is the whole code here is
lines for just a very simple test
lines for just a very simple test
environment in renderer so I suggest
environment in renderer so I suggest
using this as a template just like copy
using this as a template just like copy
squared over and then go from there
squared over and then go from there
because you can see all the logic is in
because you can see all the logic is in
squar Doh
squar Doh
here how to set up on Mac yeah okay
here how to set up on Mac yeah okay
gotcha yeah it'll work on Mac just fine
gotcha yeah it'll work on Mac just fine
for you just get cling 18 and you'll be
for you just get cling 18 and you'll be
good um well the thing that I didn't get
good um well the thing that I didn't get
from you right was what you're trying to
from you right was what you're trying to
do with puffer lib I don't think um did
do with puffer lib I don't think um did
you say what you were trying to do with
you say what you were trying to do with
puffer
puffer
lib do you have a specific problem are
lib do you have a specific problem are
you new to RL are you experienced an RL
you new to RL are you experienced an RL
and like trying to use it for something
and like trying to use it for something
like that I don't know if I got that
like that I don't know if I got that
background out of
background out of
you because you know what I suggest will
you because you know what I suggest will
differ depending on
circumstance
for
e e
you also want to figure it out good
you also want to figure it out good
experience in Python like your glasses
experience in Python like your glasses
thank you they are the cheapest possible
thank you they are the cheapest possible
glasses uh because I just end up getting
glasses uh because I just end up getting
new pairs of them
constantly and I typically just wear
constantly and I typically just wear
them when I'm being lazy too lazy to put
them when I'm being lazy too lazy to put
contacts in
contacts in
anyways um good experience with python
anyways um good experience with python
new to RL so yeah uh in that case then
new to RL so yeah uh in that case then
for newcomers to
for newcomers to
RL I I wrote a guide specifically for
you this is my RL quickart guide I
you this is my RL quickart guide I
suggest spending uh 80% of your time
suggest spending uh 80% of your time
that you're going to allocate to RL like
that you're going to allocate to RL like
building new stuff and like learning by
building new stuff and like learning by
building and then 20% of time going
building and then 20% of time going
through the formal resources here this
through the formal resources here this
gives you a just a very quick outline of
gives you a just a very quick outline of
stuff in reinforcement learning and a
stuff in reinforcement learning and a
little bit of perspective it links a
little bit of perspective it links a
bunch of papers that again you know 20%
bunch of papers that again you know 20%
of your time that you're going to
of your time that you're going to
allocate this on
allocate this on
papers um and then uh it suggests that
papers um and then uh it suggests that
you
you
essentially build just build a very
essentially build just build a very
simple test M and get like get an agent
simple test M and get like get an agent
doing something on a new task right and
doing something on a new task right and
I have a template for you squared as a
I have a template for you squared as a
template all the other environments that
template all the other environments that
are in here can be used as references
are in here can be used as references
and then myself and others are around on
and then myself and others are around on
Discord if you get stuck
[Music]
you're not here to ask me about crypto
you're not here to ask me about crypto
are you I've had
are you I've had
uh a bunch of accounts getting very
uh a bunch of accounts getting very
creative with that in the last couple of
creative with that in the last couple of
days
yeah the thing that's weird here is I
yeah the thing that's weird here is I
don't think this is reflective
of what we see on our Gan
processes maybe it is maybe I just
processes maybe it is maybe I just
didn't bother to plot it header files
didn't bother to plot it header files
main
main
logic yeah so the way it works right all
logic yeah so the way it works right all
the logic goes in the header the C file
the logic goes in the header the C file
is just a standalone test I suggest not
is just a standalone test I suggest not
recompiling the whole python stack every
recompiling the whole python stack every
single time so when you're developing
single time so when you're developing
you can just use scripts build ocean sh
you can just use scripts build ocean sh
um and then that will let you build
um and then that will let you build
locally so you can test locally and see
locally so you can test locally and see
it's very convenient it's very very fast
it's very convenient it's very very fast
and it comes with um if you use clang 18
and it comes with um if you use clang 18
on a Mac at least you can get like f
on a Mac at least you can get like f
sanitized flags with the address
sanitized flags with the address
sanitizer so then you don't have to deal
sanitizer so then you don't have to deal
with memory faults and stuff like that
with memory faults and stuff like that
um basically C becomes as easy as
um basically C becomes as easy as
python
python
um and then yeah binding the scyon syon
um and then yeah binding the scyon syon
is the intermediary layer and then
is the intermediary layer and then
python is just the top
level figure this
out
for e
we can technically use this can't we
yeah yeah wait we can we can just hack
yeah yeah wait we can we can just hack
this okay I know what we're going to do
this okay I know what we're going to do
this is going to be good um hopefully I
this is going to be good um hopefully I
can get this done quickly
enough in the morning Dev
session take
this put this
here
here
okay oh not that
much it's indented
much it's indented
three oops
okay and now we
have clost and raw scores
is this more crypto did I
guess yep
how I
how I
guess no I don't deal with Krypto on
guess no I don't deal with Krypto on
here
I'm going to give you the benefit of the
I'm going to give you the benefit of the
doubt and assume that and just like
doubt and assume that and just like
suppose that you're not the same crypto
suppose that you're not the same crypto
person who's been harassing me on here
person who's been harassing me on here
for the last few days but um no I I
for the last few days but um no I I
don't do crypto I do AI this is all
don't do crypto I do AI this is all
reinforcement learning development
EX for
no there's quite the coincidence then
no there's quite the coincidence then
because I
because I
have I really don't have anybody
have I really don't have anybody
following me for
following me for
crypto so you can imagine the
crypto so you can imagine the
suspicion I've uh been doing
suspicion I've uh been doing
reinforcement learning here for the
reinforcement learning here for the
past I don't know when have I been doing
past I don't know when have I been doing
this live since closing and on a year
now why do did condic compiler need
now why do did condic compiler need
c why did the condic compiler needs C
prefixes
uh cond ships are just a horribly broken
uh cond ships are just a horribly broken
C compiler man it's I don't even know
C compiler man it's I don't even know
how they manage to break it as badly as
how they manage to break it as badly as
they have all right cuz the error that
they have all right cuz the error that
the the conflict of name error that this
the the conflict of name error that this
caus it shouldn't even be possible so I
caus it shouldn't even be possible so I
don't know how they managed to break it
don't know how they managed to break it
that
that
badly you invest in crypto
badly you invest in crypto
privately uh not in meme coins I own a
privately uh not in meme coins I own a
couple ETFs as a very small fraction of
couple ETFs as a very small fraction of
overall Investments I believe
mostly I just have Tex STS
and then puffer has been the latest
and then puffer has been the latest
investment because buying your own
investment because buying your own
Hardware is
expensive but we're well positioned for
expensive but we're well positioned for
that
now we just got a little bit of
now we just got a little bit of
additional
additional
Revenue yesterday
Revenue yesterday
will be
will be
useful and uh
useful and uh
yeah couple more things on the horizon
yeah couple more things on the horizon
potentially for puffer as
potentially for puffer as
well really I think just one one more
well really I think just one one more
good contract and we'll be very
good contract and we'll be very
comfortable and any more than that will
comfortable and any more than that will
be sitting very
pretty Okay so
we have the raw values
here and
then what do we plot
here un normalized score
here un normalized score
mean and unnormalized was a cost
so we do cost
so we do cost
mean and unnormalized foran is that what
mean and unnormalized foran is that what
we
do
e e
that's neoc
cars I'm not gay man
cars I'm not gay man
I'll take it as a compliment but I'm not
gay what happened
here
here
oh Max
oops that was dumb
really what the
heck well this is moderately
insane hang on how is
this ah
what well that answers why stuff isn't
what well that answers why stuff isn't
working
working
holy hell
holy hell
um I not fit the model or something
what
e
e e
e e
I got to stop streaming so
I got to stop streaming so
late I'm not have asleep this morning
probably have a hard cut off
probably have a hard cut off
around nine or
whatever come on this should be
whatever come on this should be
basic brain is just not processing right
now
now
unnormalized oh wait do we need the
unnormalized oh wait do we need the
untransformed
untransformed
one hang on I think we
need this is still transformed we need
need this is still transformed we need
untransformed scor
mean there we go
all right do these predictions still
suck much
better way way better
okay this is something we can maybe work
okay this is something we can maybe work
with here
right cuz we can actually see exactly
right cuz we can actually see exactly
what's going on
here this is kind of
cool
for
e e
this is actually not terrible I think
like this is this is the linear kernel
like this is this is the linear kernel
presumably
presumably
right that's giving us this
correlation actually let's
um we should be able to verify that
right so if we get rid of the linear
kernel what
kernel what
happens so from
happens so from
this this is what we had
before yeah so we get something quite
before yeah so we get something quite
different
well the support data is going to be a
well the support data is going to be a
little different here
little different here
right it's not as crazy different as I
right it's not as crazy different as I
would expect
well this is not good right like look
well this is not good right like look
it's overfitting right here it's
overshooting it's basically has this
overshooting it's basically has this
flat prediction here and this is not
flat prediction here and this is not
correct I don't know where it learned
correct I don't know where it learned
this
from and
from and
how for
okay so this is the linear kernel but
okay so this is the linear kernel but
the thing is it's log linear because
the thing is it's log linear because
it's done in log
space and what test is this on
one let's do synthetic log test
one let's do synthetic log test
again so like here
this damn
this damn
it broke my
terminal okay so this log fit should
terminal okay so this log fit should
actually be decent for a good number of
actually be decent for a good number of
tasks here yeah so this is the log fit
tasks here yeah so this is the log fit
that we
that we
get uh it pretty closely matches in the
support and then it starts underfitting
support and then it starts underfitting
a little here this is pretty decent
overall and I think that this is a good
overall and I think that this is a good
starting point
starting point
because most cost curves should look
because most cost curves should look
like this
like this
so linear kernel in log space is a good
idea and then when we add in the matern
kernel okay we're going to do this and
kernel okay we're going to do this and
maybe it'll stop breaking my
maybe it'll stop breaking my
quick
terminal and then what's the mat interal
terminal and then what's the mat interal
do for us
right h
now that's pretty
good now the question is why is it so
good now the question is why is it so
confident about all
this like
if we do kernel is matern
if we do kernel is matern
Kernel and then length
scale does this give
scale does this give
us what we would expect
all of our predictions are right here
all of our predictions are right here
somewhere that does not make
sense they should not be able to go to
sense they should not be able to go to
the left like that they should only be
the left like that they should only be
able to go
oh you are using it to predict cost as
oh you are using it to predict cost as
well so it can actually shift to the
well so it can actually shift to the
left that's
funny e
we really should divide this cost data
we really should divide this cost data
by Max cost or something shouldn't
we it's actually kind of cool how with
we it's actually kind of cool how with
the log fit it's it's just decent on its
the log fit it's it's just decent on its
own for
far too confident is the thing
let's see if we can just scale the score
let's see if we can just scale the score
prediction one by a little bit like this
that's very
weird
so wait what
h
it's very weird it pushes the
it's very weird it pushes the
cost why does it do that to the cost
cost why does it do that to the cost
hang on
hang on
you score it right this these should be
you score it right this these should be
independent that doesn't make
sense okay we got to figure something
sense okay we got to figure something
out here because that doesn't make any
out here because that doesn't make any
bloody sense BP
score for
maybe we found something
here
e e
what is this thing trained
on normalize log cost right and this is
on normalize log cost right and this is
only trained on stats
from from
here if I comment this it's going to be
here if I comment this it's going to be
different right
is that the
case wait
what hold on
um
um
me something's weird here
okay so this do train is not hold on
okay so this do train is not hold on
this do train is just a train
mode use the source installation method
mode use the source installation method
yep that's
yep that's
fine robust way to check
installation uh yeah see if you can run
installation uh yeah see if you can run
stuff though did I break something
stuff though did I break something
lately with this I might have broken
lately with this I might have broken
something for you just
yesterday hold on
I want to get off of this Branch at the
moment so the one thing I think I broke
moment so the one thing I think I broke
yesterday that you might have to look at
yesterday that you might have to look at
it should take you two
seconds so this is in the docks I
seconds so this is in the docks I
believe as well but we use this script
believe as well but we use this script
to build
locally
and where is it
and where is it
yeah this flag you're going to have to
yeah this flag you're going to have to
swap this for the Mac one you'll see
swap this for the Mac one you'll see
there's a rib like 5.0 Mac OS or
there's a rib like 5.0 Mac OS or
whatever you just have to swap this path
whatever you just have to swap this path
and this path you should be able to
and this path you should be able to
build for
build for
Mac so you can just build some of the
Mac so you can just build some of the
environments you can build like neural
environments you can build like neural
M3 or squared or whatever and uh see if
M3 or squared or whatever and uh see if
it works for
it works for
you morning
so I have no idea where this score data
so I have no idea where this score data
is coming from
is coming from
because think we're fitting to
because think we're fitting to
this hang is this train not what I think
this hang is this train not what I think
it is
so I'm not training the the GP here at
all that doesn't make any sense
I mean it is a log distribution by
I mean it is a log distribution by
default
hang
on so if I just comment the kernel will
on so if I just comment the kernel will
it mess it up if I comment the linear
it mess it up if I comment the linear
kernel is in log space
no it's still a perfect fit okay well
no it's still a perfect fit okay well
something is bizarre because it
something is bizarre because it
shouldn't be
training is somehow
wait I'll be back in a few
wait I'll be back in a few
minutes I got to figure this out be
minutes I got to figure this out be
right
back
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e
okay we have the Phantom goian process
okay we have the Phantom goian process
to deal with
now so we need this thing
now so we need this thing
right and then the question is
right and then the question is
why it's probably going to just be
why it's probably going to just be
something
something
dumb so if you don't estimate cost the
dumb so if you don't estimate cost the
cost is zero cuz it's a g in process but
cost is zero cuz it's a g in process but
now you know you'd expect the cost to go
now you know you'd expect the cost to go
out but for some reason the score also
out but for some reason the score also
gets predicted why does the score get
gets predicted why does the score get
predicted because let me
see
see
unformed is this what I use
unformed is this what I use
oops
oops
untr form scoring
and then I plot un transform foring okay
and then I plot un transform foring okay
so this comes from a normalized Waring
so this comes from a normalized Waring
which comes from normalized quing which
which comes from normalized quing which
comes
from from the gum process
right suggestions comes from these
right suggestions comes from these
points so yeah I don't
see how an untrained gaussian process
see how an untrained gaussian process
um is producing
this especially with we just have the
this especially with we just have the
matern kernel
matern kernel
right I guess it's
possible
possible
okay maybe there's some weird thing with
okay maybe there's some weird thing with
this wouldn't think
this wouldn't think
so for
super fast got it running great
yeah puffer is meant to be easy if it is
yeah puffer is meant to be easy if it is
not easy I have screwed
up but also they're kind of a million
up but also they're kind of a million
other things demanding my attention
so things get fixed in
so things get fixed in
waves at the moment it's figuring out
waves at the moment it's figuring out
what is wrong with Hyper parameter
what is wrong with Hyper parameter
opt
e e
really
not I mean is there some quirk
is there like some Quirk with
this set the data
okay so it's just it's somehow it's
okay so it's just it's somehow it's
running
training yeah somehow it's just it's
training yeah somehow it's just it's
running training even though I'm telling
running training even though I'm telling
it I'm like telling it not to fine
that's just uh pyro being weird it's not
that's just uh pyro being weird it's not
me being weird or stupid it's pyro being
me being weird or stupid it's pyro being
weird and stupid
fine
e
e e
what if we plot
the the variance do we plot the variance
the the variance do we plot the variance
is that what we should
do gear Labs s
toal mimic moves
is this gy
fan I have Jim fan muted for uh he's
fan I have Jim fan muted for uh he's
like one of the worst Defenders
like one of the worst Defenders
for like
um just inflated claims about stuff and
um just inflated claims about stuff and
generally misrepresenting scientific
work so if that's from his lab I have
work so if that's from his lab I have
very little
faith e
if you want documentation of that just
if you want documentation of that just
go look back at how we presented um
go look back at how we presented um
Voyager and then go read the real paper
and the funny thing is that the group
and the funny thing is that the group
does uh actually does good work it's
does uh actually does good work it's
just you can never tell what the work
just you can never tell what the work
actually is because it's always
misrepresented
for e
so for the folks on YouTube what I'm
so for the folks on YouTube what I'm
currently trying to figure out here
currently trying to figure out here
that's driving me
that's driving me
nuts
nuts
um the gaussian process or gaussian
um the gaussian process or gaussian
processes here are what we rely on to
processes here are what we rely on to
estimate how good a set of hyper
estimate how good a set of hyper
parameters is so we know if it's worth
parameters is so we know if it's worth
running the experiment or not um it's
running the experiment or not um it's
giving weirdly biased predictions where
giving weirdly biased predictions where
it is
it is
overconfident um in its estimation it's
overconfident um in its estimation it's
overconfident in its predictions in
overconfident in its predictions in
regions where it really shouldn't be
also why is
also why is
um what the hell's wrong with my
um what the hell's wrong with my
plugins we're going to
plugins we're going to
work leave me my
work leave me my
typing
for
e e
does this do anything
not at
not at
all doesn't do a
thing did the colonels turn off the
thing did the colonels turn off the
stream yesterday no the Linux drivers
stream yesterday no the Linux drivers
did that
if anybody knows how to
if anybody knows how to
fix freaking uh ethernet on a buun 24
fix freaking uh ethernet on a buun 24
that would be great because on average
that would be great because on average
once every day or two it will just
once every day or two it will just
randomly die and nothing I do um gets it
randomly die and nothing I do um gets it
to turn back on without rebooting
I tried all the basic stuff I could
I tried all the basic stuff I could
think
think
of freaking Linux drivers
there just no variance
huh you
huh you
just no
variance if I look at this
yeah it's completely confident in its
predictions on the entire freaking Paro
front all
front all
right is that konal dependent
linear
linear
kernel let's see if it's still very
kernel let's see if it's still very
confident
yeah so the variance is just a useless
yeah so the variance is just a useless
metric
h
see the problem is you've
see the problem is you've
got top of the morning to
you e
I would love to know why it is so
I would love to know why it is so
confident all the way out at the
extremities should my States be Marian
extremities should my States be Marian
for RL
for RL
you can ignore the is you can ignore the
you can ignore the is you can ignore the
existence of that word and you will be
existence of that word and you will be
better
off
um yeah we the theoretical results are
um yeah we the theoretical results are
they quickly go out the window all the
they quickly go out the window all the
guarantees with stuff that we throw RL
guarantees with stuff that we throw RL
on it will still
on it will still
work what you should think about is
work what you should think about is
whether the problem is actually
whether the problem is actually
learnable e
okay you know what maybe there is
okay you know what maybe there is
something here which
is why is this thing
why is this thing so
why is this thing so
confident all the way out at the
end oh you know what
should it put a bump in input space or
should it put a bump in input space or
output
space yeah
space yeah
okay in I
okay in I
guess the support set is not in Ely what
guess the support set is not in Ely what
we think it is in this graph
fine cuz it's actually the higher
fine cuz it's actually the higher
Dimension fine okay so we can't rely on
Dimension fine okay so we can't rely on
variance
and a lot of the utility of these things
and a lot of the utility of these things
goes out the
window
e e
the thing is that like the exact reason
the thing is that like the exact reason
I'm
I'm
using gaan presses at the moment for
using gaan presses at the moment for
this is the thing that's not working
okay let's try some stuff
TR
this what's this do for
us holy all
right so at least get this to have a
right so at least get this to have a
little bit of shape right what was it
little bit of shape right what was it
two
times been fall I understand 10% of what
times been fall I understand 10% of what
you do but I believe in it I mean I'm
you do but I believe in it I mean I'm
also not explaining what I'm doing here
also not explaining what I'm doing here
amazingly because I'm trying to think
amazingly because I'm trying to think
about this at the same time
um I have some videos that
um I have some videos that
are have you seen the iceberg video the
are have you seen the iceberg video the
iceberg video is a pretty good outline
iceberg video is a pretty good outline
it's a decent balance of technical depth
it's a decent balance of technical depth
and high level I mean like the thing I'm
and high level I mean like the thing I'm
trying to do at a high level is not
trying to do at a high level is not
ridiculous ly complicated right
ridiculous ly complicated right
reinforcement learning is an important
reinforcement learning is an important
branch of
branch of
science the field has been subject to
science the field has been subject to
absolutely terrible engineering and
absolutely terrible engineering and
performance optimization for the last 10
performance optimization for the last 10
years this has made all of research go
years this has made all of research go
100 times slower than it should have
100 times slower than it should have
this means that you run a 100 times
this means that you run a 100 times
fewer experiments than you should this
fewer experiments than you should this
means that your results are very
means that your results are very
inconclusive so the whole thing is very
inconclusive so the whole thing is very
wishy-washy and feels very janky I'm
wishy-washy and feels very janky I'm
making everything fast this lets us do
making everything fast this lets us do
consistent and accurate science I have
consistent and accurate science I have
made everything fast or a lot of it fast
made everything fast or a lot of it fast
now now I'm using all the fast things
now now I'm using all the fast things
that I've buil to do really consistent
that I've buil to do really consistent
and accurate
and accurate
science this will make reinforcement
science this will make reinforcement
learning broadly useful this year
sure okay here's an idea then what if we
sure okay here's an idea then what if we
crank
up what if we crank this one up so now
up what if we crank this one up so now
what this should give us is the gaussian
what this should give us is the gaussian
process should sample very far away from
support yes
so you can see there no supporting data
so you can see there no supporting data
points in here but it's still like
points in here but it's still like
making confident
predictions this is not
good okay let's look at the kernel
look at the mat konel
first let's really crank this
I guess that is a uh reg
I guess that is a uh reg
X not do
X not do
that I got to go in a little bit as
well I want to at least get something
well I want to at least get something
figured
out let's do this so now it's sampling
out let's do this so now it's sampling
crazy
points
what did I break it
oh I
see well that's awkward
okay let me explain what's happening and
okay let me explain what's happening and
I figured this out earlier I don't know
I figured this out earlier I don't know
why I forgot about
this so both cost and score being
this so both cost and score being
modeled by a gan process when you're
modeled by a gan process when you're
outside the support which means outside
outside the support which means outside
the spot the space where the points are
the spot the space where the points are
informing you gum processes go to zero
informing you gum processes go to zero
so for support uh for score that's what
so for support uh for score that's what
you want maybe but for cost you don't
you want maybe but for cost you don't
want it to systematically underestimate
want it to systematically underestimate
cost so what you need to do here and
cost so what you need to do here and
this is what we'll get done for right
this is what we'll get done for right
now um where's the mean
now um where's the mean
function
lost so we'll do where's
lost so we'll do where's
this Max log cost
get rid of
that for
okay
so GP clo me
and
X see what this
does for
he
you know what we can do instead it'll be
you know what we can do instead it'll be
way easier than
way easier than
this
on
on
idx we'll just do Max C idx
idx we'll just do Max C idx
equals
Max I think this
is
75 just do
is
it noriz
maxal X
okay way
okay way
better immediately way
better yeah that was it
better yeah that was it
okay jeez
in fact we could even have it be more
aggressive so if I do this then the
aggressive so if I do this then the
really unconfident predictions will get
really unconfident predictions will get
pulled they'll get stretched even
more
boom so we will mess with this and
boom so we will mess with this and
figure out the appropriate
figure out the appropriate
the appropriate value here
um this one seems too
much but
effectively you want to assume
effectively you want to assume
that when you don't have information
that when you don't have information
about a
about a
point it's going to be more expensive
point it's going to be more expensive
and perform less
and perform less
uh not as well as you
uh not as well as you
expect
so score function seems to be taken care
of though I'm not sure I don't think it
of though I'm not sure I don't think it
reverts to zero I think it reverts to
reverts to zero I think it reverts to
like some intermediate
like some intermediate
point I have to look at that but this is
point I have to look at that but this is
made like this is big
made like this is big
progress that's way better than before
actually this would explain why uh cost
actually this would explain why uh cost
was being consistently overestimated as
well freaking gaussian processes man I'm
well freaking gaussian processes man I'm
going to keep using these for a bit
going to keep using these for a bit
because um I mean these seem like very
because um I mean these seem like very
useful in hyper pram optimization
useful in hyper pram optimization
literature overall but I will not be
literature overall but I will not be
surprised if we just get to replace
surprised if we just get to replace
these with logistic progression or
these with logistic progression or
something cuz these are so unwieldy and
something cuz these are so unwieldy and
yeah
yeah
there all these details that you need to
there all these details that you need to
get right or you're screwed all right
get right or you're screwed all right
I'm going to get some breakfast I'm
I'm going to get some breakfast I'm
going to go for a run I got a couple
going to go for a run I got a couple
meetings and then uh I will be back
meetings and then uh I will be back
working on some stuff maybe this maybe
working on some stuff maybe this maybe
some robotics we shall
