Kind: captions
Language: en
Hello,
Hello,
we're back Five.
All right, back for another hour and a
All right, back for another hour and a
halfish. Little more.
Here's the plan. I'm going to go through
Here's the plan. I'm going to go through
the rest of the article.
the rest of the article.
Going to proof the rest of this. Maybe
Going to proof the rest of this. Maybe
grab images for it. Pretty much have
grab images for it. Pretty much have
them ready. And then uh I'm going to
them ready. And then uh I'm going to
spend hopefully I have an hour left
spend hopefully I have an hour left
after that to edit the uh the camera
after that to edit the uh the camera
ready.
I really don't care all that much about
I really don't care all that much about
the pub. Um,
the pub. Um,
I guess some people read it. I guess
I guess some people read it. I guess
I'll like make sure it's updated as best
I'll like make sure it's updated as best
as possible, but it's it's a whole
as possible, but it's it's a whole
generation outdated. So,
generation outdated. So,
yeah,
yeah,
let's just go through this out loud.
let's just go through this out loud.
We'll proof this. If anybody has any
We'll proof this. If anybody has any
suggestions, let me know
suggestions, let me know
and uh we will go from there. So I
and uh we will go from there. So I
already read through this section.
already read through this section.
So one, build a simple environment and
So one, build a simple environment and
train an agent.
train an agent.
Read the introduction to my RO quick
Read the introduction to my RO quick
start guide. Just the first paragraph
start guide. Just the first paragraph
for now.
for now.
You now know some
Yeah, you now know some words we use in
Yeah, you now know some words we use in
RL and not much else. Read the pufferlib
RL and not much else. Read the pufferlib
docs on writing your own environment
docs on writing your own environment
including all the link code on the
including all the link code on the
squared sample environment. So I have to
squared sample environment. So I have to
still update these docs a little bit.
still update these docs a little bit.
Map the terms from the introduction to
Map the terms from the introduction to
this code. Observations, actions,
this code. Observations, actions,
rewards, and terminals are just arrays.
rewards, and terminals are just arrays.
The observations are input the agent's
The observations are input the agent's
neural network which outputs actions.
neural network which outputs actions.
Rewards ter determine whether the agent
Rewards ter determine whether the agent
has reached its goal at which point the
has reached its goal at which point the
terminal value is set to true. You don't
terminal value is set to true. You don't
yet know how we use all these things to
yet know how we use all these things to
actually train.
you don't know how we use all these
you don't know how we use all these
things to actually train yet and that is
things to actually train yet and that is
fine.
fine.
Also, let me delete this bot
Also, let me delete this bot
bot message out of there.
Oops.
Oops.
All right.
Write your own puffer environment. Keep
Write your own puffer environment. Keep
it so simple.
And we're just going to say write your
And we're just going to say write your
own environment. So write your own
own environment. So write your own
environment. Keep it so simple as to not
environment. Keep it so simple as to not
even be useful. That will come next. If
even be useful. That will come next. If
you can't think of anything, do a
you can't think of anything, do a
stripped down version of Flappy Bird on
stripped down version of Flappy Bird on
a twob block tall grid. The agent can
a twob block tall grid. The agent can
move either up or down and observes
move either up or down and observes
whether there is a wall on the roof or
whether there is a wall on the roof or
ceiling above.
ceiling above.
Uh,
that didn't make any sense. Whether
that didn't make any sense. Whether
there's a wall on the roof or floor.
there's a wall on the roof or floor.
Negative one for hitting the ceiling.
Negative one for hitting the ceiling.
Zero otherwise. Bind it to Puffer Lib
Zero otherwise. Bind it to Puffer Lib
following the docks and train your first
following the docks and train your first
agent. Ask in Discord if you get stuck.
agent. Ask in Discord if you get stuck.
Let's link this
Let's link this
to
to
Poppy.
Two, learn the most basic fundamentals.
Two, learn the most basic fundamentals.
Read Carpathy's policy gradient blog,
Read Carpathy's policy gradient blog,
including the implementation. There are
including the implementation. There are
a few very minor details that are dated,
a few very minor details that are dated,
but it is otherwise still the best
but it is otherwise still the best
introduction to on policy methods. You
introduction to on policy methods. You
should now understand how policy
should now understand how policy
gradients and observations actions
gradients and observations actions
rewards into derivatives over weights.
rewards into derivatives over weights.
You have also seen discounted reward as
You have also seen discounted reward as
your first example of an advantage
your first example of an advantage
function. Understand that the discount
function. Understand that the discount
factor mathematically determines how
factor mathematically determines how
much you care about reward now versus
much you care about reward now versus
reward later. Read the fundamental
reward later. Read the fundamental
section of my RL quick start guide. Give
section of my RL quick start guide. Give
you context for the basic classes of
you context for the basic classes of
methods. Don't go down a rabbit hole of
methods. Don't go down a rabbit hole of
reading papers just yet. All modern
reading papers just yet. All modern
algorithms are at least slightly off
algorithms are at least slightly off
policy and the strict definition does
policy and the strict definition does
not matter as much anymore.
not matter as much anymore.
Read the multi- aent bullet point twice.
Read the multi- aent bullet point twice.
Read and train puffer target. This
Read and train puffer target. This
environment is included in the same
environment is included in the same
tutorial you already followed. It is
tutorial you already followed. It is
multi- aent and significantly more
multi- aent and significantly more
complex than squared. The agents no
complex than squared. The agents no
longer move on the grid and have to
longer move on the grid and have to
solve a more temporally extended
solve a more temporally extended
problem. Every agent sees a normalized
problem. Every agent sees a normalized
distance measure from itself to every
distance measure from itself to every
other agent and goal. Agents only
other agent and goal. Agents only
receive a sparse reward of one for
receive a sparse reward of one for
reaching their goal. Follow the docs. To
reaching their goal. Follow the docs. To
train an agent on this environment, it
train an agent on this environment, it
should only take a few seconds.
should only take a few seconds.
Three, build a slightly more complex
Three, build a slightly more complex
environment.
environment.
Make something about as complex as
Make something about as complex as
target. Max 300 lines. Doesn't have to
target. Max 300 lines. Doesn't have to
be multi- aent. If you did grid based
be multi- aent. If you did grid based
Flappy Bird before, do the full game.
Flappy Bird before, do the full game.
Any similarly scoped project should
Any similarly scoped project should
suffice. Think about what data the agent
suffice. Think about what data the agent
needs to see in order to play and make
needs to see in order to play and make
sure that it is included in the
sure that it is included in the
observations. Reread the debugging
observations. Reread the debugging
section of the quick start guide
section of the quick start guide
to avoid common errors and post on a
to avoid common errors and post on a
Discord if you get stuck.
Discord if you get stuck.
Is there a debugging section in my quick
Is there a debugging section in my quick
start guide or am I misremembering?
No, there isn't. It's the tutorial.
Reread the quick start
Reread the quick start
of the
of the
Reread the debugging section of the
Reread the debugging section of the
custom environment doc to avoid common
custom environment doc to avoid common
errors and post in our Discord if you
errors and post in our Discord if you
get stuck.
Aim to train an agent that you can
Aim to train an agent that you can
visually confirm is playing the game
visually confirm is playing the game
well. Read other puffer environments.
well. Read other puffer environments.
Snake and Convert are two slightly more
Snake and Convert are two slightly more
complex.
complex.
I'll put this caps are two slightly more
I'll put this caps are two slightly more
complex environments with pin code.
I'm going to do simple
clean code has the annoying like because
clean code has the annoying like because
of the stupid book, but it is it's just
of the stupid book, but it is it's just
clean code. It's not clean code from
clean code. It's not clean code from
that stupid book, but it's clean code.
that stupid book, but it's clean code.
Multi- dash agent.
Multi- dash agent.
So
So
I never know what to do with that man
I never know what to do with that man
because if you don't make it one word if
because if you don't make it one word if
you do multi-ash agent then you need to
you do multi-ash agent then you need to
do single dash agent for symmetry.
I usually just make it one word.
Like if we just Isn't this a thing?
Does everybody dash it? Am I just
Does everybody dash it? Am I just
stupid?
No, it's it's that it's auto correcting.
I guess most people do dash it.
I guess most people do dash it.
Annoying thing.
It looks super stupid to me written this
It looks super stupid to me written this
way, but whatever.
Also start looking at some of the arcade
Also start looking at some of the arcade
games like Pong and Breakout. By now,
games like Pong and Breakout. By now,
you've probably hit a few bugs and can
you've probably hit a few bugs and can
start to appreciate the ways in which
start to appreciate the ways in which
things can go wrong. We mitigate that by
things can go wrong. We mitigate that by
keeping code simple and minimally
keeping code simple and minimally
abstracted. The biggest mistake you can
abstracted. The biggest mistake you can
make in RL is to underestimate the price
make in RL is to underestimate the price
of complexity.
Enhance your environment. Add some
Enhance your environment. Add some
features that make the problem a little
features that make the problem a little
more interesting. Retrain with different
more interesting. Retrain with different
versions and see how your changes alter
versions and see how your changes alter
the agent's learning. Aim for something
the agent's learning. Aim for something
on the level of Pong or Snake.
If you get something visually
If you get something visually
interesting and not already in Pupper,
interesting and not already in Pupper,
come show us in PR it. Many simple
come show us in PR it. Many simple
environments end up being quite useful
environments end up being quite useful
in research.
in research.
Four, start understanding why that
Four, start understanding why that
worked. Read the core algorithm paper.
worked. Read the core algorithm paper.
It is dense, but the actual algorithm is
It is dense, but the actual algorithm is
simple. Read the bullet point on PO from
simple. Read the bullet point on PO from
my quick start guide first for some
my quick start guide first for some
intuition. Then you can read the
intuition. Then you can read the
proximal policy optimization PO paper.
proximal policy optimization PO paper.
You can ignore the TRPO and kale penalty
You can ignore the TRPO and kale penalty
sections. The main equation seven is
sections. The main equation seven is
just saying clip the policy gradient and
just saying clip the policy gradient and
weight it by the advantage function.
weight it by the advantage function.
Then average over a batch of data. The
Then average over a batch of data. The
advantage function is generalized
advantage function is generalized
advantage estimation.
advantage estimation.
If you have a strong enough math
If you have a strong enough math
background, read this now. Otherwise,
background, read this now. Otherwise,
put it off until
put it off until
section six
section
section
understand why I didn't have you
understand why I didn't have you
implement the algorithm. The common
implement the algorithm. The common
reference algorithms
reference algorithms
are extremely fiddly. Read Costa's 37
are extremely fiddly. Read Costa's 37
implementation details of PO and the
implementation details of PO and the
associated clean RLP PO implementation.
associated clean RLP PO implementation.
Not particularly long, but the details
Not particularly long, but the details
really matter. Read our other articles.
really matter. Read our other articles.
They are simpler than academic papers
They are simpler than academic papers
and tell you exactly how the tools
and tell you exactly how the tools
you've been using work.
Stronger hyperparameters with protein
Stronger hyperparameters with protein
buffing up PO Narama mode. Cool.
buffing up PO Narama mode. Cool.
Five. Your first real project. Finish
Five. Your first real project. Finish
reading my quick start guide. You will
reading my quick start guide. You will
not have context for all of it just yet,
not have context for all of it just yet,
but it contains a lot of useful
but it contains a lot of useful
perspective as you start to solve harder
perspective as you start to solve harder
problems. For example, depending on how
problems. For example, depending on how
hard your environment is.
hard your environment is.
Uh you may actually have to run
Uh you may actually have to run
for example, you may actually have to
for example, you may actually have to
run a hyperparameter sweep to get a good
run a hyperparameter sweep to get a good
policy.
Now this
fine. It's repetitive, but it's fine. If
fine. It's repetitive, but it's fine. If
you have a short horizon environment,
you have a short horizon environment,
you may need to at least decrease the
you may need to at least decrease the
discount rate. Do you understand why?
Let's actually do a little less.
Connect four. Maybe
if you implemented something like a card
if you implemented something like a card
game, you may need to at least you might
game, you may need to at least you might
need at least need to decrease the
need at least need to decrease the
discount rate. Do you understand why?
discount rate. Do you understand why?
Cool. Read the Pokemon RL blog post.
Cool. Read the Pokemon RL blog post.
This is a powered by Puffer trademark
This is a powered by Puffer trademark
copyright registered
We'll leave it like this. Project that
We'll leave it like this. Project that
beats the entire game using pure from
beats the entire game using pure from
scratch reinforcement learning.
scratch reinforcement learning.
The observation space formulation,
The observation space formulation,
reward engineering, and problem setup
reward engineering, and problem setup
are all informative.
are all informative.
Note that Pokemon Red is a thousand
Note that Pokemon Red is a thousand
times slower than most of our RL
times slower than most of our RL
environments. You will almost never
environments. You will almost never
actually have to do this level of
actually have to do this level of
engineering when you have faster
engineering when you have faster
simulators.
Pick an interesting problem. Aim for
Pick an interesting problem. Aim for
something you can do in 500 to a,000
something you can do in 500 to a,000
lines depending on your programming
lines depending on your programming
background. Several useful environments
background. Several useful environments
have been shorter than that. Arcade
have been shorter than that. Arcade
games are usually a good choice. Just
games are usually a good choice. Just
check in the Discord that nobody is
check in the Discord that nobody is
already doing the same one. If you have
already doing the same one. If you have
experience in another field, applied
experience in another field, applied
problems are even better. Good RL
problems are even better. Good RL
environments look like fiddly
environments look like fiddly
interactive optimization problems that
interactive optimization problems that
are quick to simulate and have clearly
are quick to simulate and have clearly
defined observations and actions. The
defined observations and actions. The
initial drone environment was only a few
initial drone environment was only a few
hundred lines. Solve in PR to puffer
hundred lines. Solve in PR to puffer
lab. It may seem self- serving, but this
lab. It may seem self- serving, but this
is genuinely the best way to learn. We
is genuinely the best way to learn. We
have an active community of researchers
have an active community of researchers
and hobbyists large enough that someone
and hobbyists large enough that someone
will nearly always be around to answer
will nearly always be around to answer
your questions. Many environments that
your questions. Many environments that
you wouldn't expect to be useful
you wouldn't expect to be useful
actually help us advance core RL
actually help us advance core RL
research.
research.
Six, do your homework. The list of
Six, do your homework. The list of
important papers to read in
important papers to read in
reinforcement learning is quite short.
reinforcement learning is quite short.
See my programming and ML advice article
See my programming and ML advice article
and my quick start guide for perspective
and my quick start guide for perspective
on why this is the case.
on why this is the case.
I need to link
I need to link
these. Right,
let's do that real quick.
Oh, actually we can link the uh the X
copy the link.
and then we can't link the ML article
and then we can't link the ML article
until it's published. So that'll be the
until it's published. So that'll be the
final tweak.
These are the top 10 papers you should
These are the top 10 papers you should
read regardless of what you want to do
read regardless of what you want to do
next. This list is intentionally not a
next. This list is intentionally not a
historical account of mostly broken
historical account of mostly broken
algorithms. My focus is on the major
algorithms. My focus is on the major
capabilities defining results, the
capabilities defining results, the
commonalities among them. Note that
commonalities among them. Note that
almost all of the OpenAI and deep mind
almost all of the OpenAI and deep mind
results have associated blog posts that
results have associated blog posts that
are more accessible than the formal
are more accessible than the formal
manuscripts. You can start here, but you
manuscripts. You can start here, but you
should also read the full papers if you
should also read the full papers if you
want to do research. We have a couple
want to do research. We have a couple
papers on puffer and neural MMO that you
papers on puffer and neural MMO that you
can read if you want. You're better off
can read if you want. You're better off
just reading our blog docs and the
just reading our blog docs and the
source code.
Dota 2 with deep uh with large scale
Dota 2 with deep uh with large scale
deep reinforcement learning. My pick for
deep reinforcement learning. My pick for
the most important paper in the field
the most important paper in the field
with a single layer LSTM solves Dota.
with a single layer LSTM solves Dota.
One of the biggest errors you can make
One of the biggest errors you can make
is undervalue this result.
Go play the game if you need convincing
Go play the game if you need convincing
that this problem is meaningfully hard.
that this problem is meaningfully hard.
Do not skip the appendex. Grandmaster
Do not skip the appendex. Grandmaster
level in Starcraft 2 using multi- aent
level in Starcraft 2 using multi- aent
reinforcement learning.
reinforcement learning.
Actually,
I've seen many top researchers
I've seen many top researchers
undervalue this result and waste their
undervalue this result and waste their
time developing fancy methods that solve
time developing fancy methods that solve
trivial solve trivial problems.
trivial solve trivial problems.
This is true.
Go play the game if you need convincing
Go play the game if you need convincing
that this problem is meaningfully hard.
that this problem is meaningfully hard.
Do not skip the appendex.
Do not skip the appendex.
Grandmaster level in Starcraft 2 using
Grandmaster level in Starcraft 2 using
multi-agent reinforcement learning.
multi-agent reinforcement learning.
Another extremely hard problem solved
Another extremely hard problem solved
with RL. It's number two because
with RL. It's number two because
DeepMind bootstrapped with imitation
DeepMind bootstrapped with imitation
learning use significantly more
learning use significantly more
complicated methods. They tend to do
complicated methods. They tend to do
this often and I am not convinced it is
this often and I am not convinced it is
required.
required.
Mastering the game of go with deep
Mastering the game of go with deep
neural networks and tree search. This is
neural networks and tree search. This is
a landmark historical result that
a landmark historical result that
kickstarted the field. You could
kickstarted the field. You could
arguably place it higher reasoning about
Uh, you could arguably place it higher.
Uh, you could arguably place it higher.
Reasoning about comparative complexity
Reasoning about comparative complexity
of Go is hard. The naive branching
of Go is hard. The naive branching
factor argument breaks when you look at
factor argument breaks when you look at
games like Dota or SC2.
games like Dota or SC2.
Learning dextrous inhand manipulation.
Learning dextrous inhand manipulation.
Learning to manipulate a Rubik's cube
Learning to manipulate a Rubik's cube
with reinforcement learning
with a robot handforcement.
with a robot handforcement.
Kind of jumped the gun there.
Kind of jumped the gun there.
This project pioneered domain
This project pioneered domain
randomization training on a ton of
randomization training on a ton of
slightly different problems to improve
slightly different problems to improve
generalization and robustness.
generalization and robustness.
Open-ended learning leads to generally
Open-ended learning leads to generally
capable agents, also known as XLAND.
capable agents, also known as XLAND.
Heavy domain randomization allows agents
Heavy domain randomization allows agents
to generalize to new tasks not seen
to generalize to new tasks not seen
during training.
You do this as randomization over
Randomization over training tasks allows
Randomization over training tasks allows
agents to generalize new tasks not seen
agents to generalize new tasks not seen
during training. There a bunch of
during training. There a bunch of
conflating variables here with method
conflating variables here with method
changes, but the end result is a pretty
changes, but the end result is a pretty
compelling argument that RL can
compelling argument that RL can
generalize.
Emergent tool use for multi- aent
Emergent tool use for multi- aent
autocurricula. The 3v3 hideand-seek with
autocurricula. The 3v3 hideand-seek with
movable objects. A pure version of
movable objects. A pure version of
techniques used in Dota applied to a
techniques used in Dota applied to a
simpler but still complex task. Human
simpler but still complex task. Human
level performance in firstp person
level performance in firstp person
multiplayer games with population based
multiplayer games with population based
deep reinforcement learning. 3v3 capture
deep reinforcement learning. 3v3 capture
the flag with FPS mechanics based on the
the flag with FPS mechanics based on the
associated quick game mode.
The net hack learning environment a
The net hack learning environment a
really hard environment for
really hard environment for
reinforcement learning. Probably can't
reinforcement learning. Probably can't
solve it with a general method without
solve it with a general method without
also solving AI in the process. We call
also solving AI in the process. We call
that AI complete.
that AI complete.
We I call that AI complete. I don't know
We I call that AI complete. I don't know
if anybody else uses that.
Proximal policy optimization the core
Proximal policy optimization the core
algorithm that is the base most modern
algorithm that is the base most modern
RL built our puffer lil 3 algorithm as a
RL built our puffer lil 3 algorithm as a
set of enhancements to
set of enhancements to
highdimensional continuous control using
highdimensional continuous control using
generalized advantage estimation
generalized advantage estimation
advantage function that is half the
advantage function that is half the
reason PO works one of our 3PO uh 3.0 0
reason PO works one of our 3PO uh 3.0 0
experiments was to combine V GE with VRE
experiments was to combine V GE with VRE
playing Atari with deep reinforcement
playing Atari with deep reinforcement
learning, the original deep Q learning
learning, the original deep Q learning
paper that spawned the field of modern
paper that spawned the field of modern
RL.
RL.
Cool.
Cool.
This is the new section I wrote this
This is the new section I wrote this
morning. We'll see if I actually like it
morning. We'll see if I actually like it
a couple hours later here. My best
a couple hours later here. My best
advice in one place.
advice in one place.
How to approach a new problem. Start
How to approach a new problem. Start
from first principle. The agent is
from first principle. The agent is
learning tabularasa. It's a blank slate
learning tabularasa. It's a blank slate
at the start of training. It's looking
at the start of training. It's looking
for signal by mashing buttons. It also
for signal by mashing buttons. It also
can't see. Imagine the environment has
can't see. Imagine the environment has
the graphics randomized. Some of reward
the graphics randomized. Some of reward
has to be obtainable this way. To
has to be obtainable this way. To
actually learn from this reward, the
actually learn from this reward, the
agent needs to be able to see. What
agent needs to be able to see. What
information does the agent need to solve
information does the agent need to solve
the problem? Make that the observation
the problem? Make that the observation
space for action. What information would
space for action. What information would
tell you if your agent is working
tell you if your agent is working
correctly? Log that. Almost always want
correctly? Log that. Almost always want
a single real number metric of overall
a single real number metric of overall
performance. Don't only log raw reward
performance. Don't only log raw reward
because you will probably tune the scale
because you will probably tune the scale
of this number and make results
of this number and make results
incomparable. Log a score instead. For
incomparable. Log a score instead. For
example, we might give agent a reward of
example, we might give agent a reward of
0.25 or 0.5 for breaking a brick, but we
0.25 or 0.5 for breaking a brick, but we
log the actual number of points
log the actual number of points
obtained.
obtained.
Bonus points if you can scale it to the
Bonus points if you can scale it to the
range of 0 to one. Log any extra data
range of 0 to one. Log any extra data
you will actually look at, but don't log
you will actually look at, but don't log
a ton of stuff you won't use. Good
a ton of stuff you won't use. Good
candidates include collision rates, out
candidates include collision rates, out
of bounds, etc. Since these are sanities
that should drop to near zero for
that should drop to near zero for
applicable problems, right? The simplest
applicable problems, right? The simplest
possible environment that is still fast.
possible environment that is still fast.
Don't abstract anything. That's
Don't abstract anything. That's
important enough to say twice. Don't
important enough to say twice. Don't
abstract anything. Start training early
abstract anything. Start training early
and frequently. Want iteration speed to
and frequently. Want iteration speed to
be as fast as possible.
Hey, welcome Alan.
That's important enough to say twice.
That's important enough to say twice.
Don't abstract anything. Start training
Don't abstract anything. Start training
early and frequently. Want iteration
early and frequently. Want iteration
speed to be as fast as possible. Seconds
speed to be as fast as possible. Seconds
is better than minutes. You've lost if
is better than minutes. You've lost if
it is hours. If training does not work,
it is hours. If training does not work,
suspect your data. Make your environment
suspect your data. Make your environment
playable. Is what is happening sensible?
What you see happening sensible? Do you
What you see happening sensible? Do you
see reward being assigned when you
see reward being assigned when you
expect it to?
expect it to?
Run an evaluation on a checkpoint. See
Run an evaluation on a checkpoint. See
if agents have found some degenerate
if agents have found some degenerate
unreoverable stage. For harder problems,
unreoverable stage. For harder problems,
scale up slowly and don't make a ton of
scale up slowly and don't make a ton of
changes without training a decent model
changes without training a decent model
on the latest version. Don't experiment
on the latest version. Don't experiment
with new algorithms on new environments
with new algorithms on new environments
that are constantly changing. Do your
that are constantly changing. Do your
research on stable problems, then try it
research on stable problems, then try it
on the new environment.
How to encode data? Normalize the
How to encode data? Normalize the
observations appropriately. You want
observations appropriately. You want
position data to be egocentric when
position data to be egocentric when
possible. Do this by subtracting the
possible. Do this by subtracting the
agent's position, dividing by the
agent's position, dividing by the
maximum value. Street data can't just go
maximum value. Street data can't just go
into the model without encoding.
into the model without encoding.
If you represent knight, king, queen,
If you represent knight, king, queen,
pawn, rook as 0 1 2 3 4. Putting that
pawn, rook as 0 1 2 3 4. Putting that
into a model raw implies knight is more
into a model raw implies knight is more
similar to king than it is to rook. It
similar to king than it is to rook. It
also forces the model to learn a wonky
also forces the model to learn a wonky
decision boundary in latent space. one
decision boundary in latent space. one
hot encode knight as 10 o instead. You
hot encode knight as 10 o instead. You
can do this in the environment for very
can do this in the environment for very
small values, but do it in the policy
small values, but do it in the policy
for larger values to save bandwidth.
for larger values to save bandwidth.
Actions should be the simplest possible
Actions should be the simplest possible
set of controls for your environment. I
set of controls for your environment. I
like to imagine the environment being
like to imagine the environment being
released for Game Boy and designing the
released for Game Boy and designing the
controls to match. Neural 3 features
controls to match. Neural 3 features
exploration combat equipment progression
exploration combat equipment progression
on a live market. The action space is a
on a live market. The action space is a
single discreet and the game is playable
single discreet and the game is playable
with keyboard only. Don't blow up the
with keyboard only. Don't blow up the
size of the space to accomplish this.
size of the space to accomplish this.
Okay, so this is super dense, right? But
Okay, so this is super dense, right? But
even though this is ridiculously dense,
even though this is ridiculously dense,
this is pretty much
this is pretty much
like this is like a non-negligible
like this is like a non-negligible
fraction of all the intuition you need
fraction of all the intuition you need
for reinforcement learning in a page and
for reinforcement learning in a page and
a half. So I think that that's worth I
a half. So I think that that's worth I
think it's worth doing something like
think it's worth doing something like
this. I haven't written anything like
this. I haven't written anything like
this before, so we'll see what people
this before, so we'll see what people
think about it. But this is kind of my
think about it. But this is kind of my
best advice in one spot.
Is doing what you're supposed to do.
Is doing what you're supposed to do.
Roll it. Hey, welcome Shadow.
Is there anything in here that is
Is there anything in here that is
straight up confusing or doesn't make
straight up confusing or doesn't make
sense to any of you watching? Please
sense to any of you watching? Please
tell me now so that I can make edits or
tell me now so that I can make edits or
clarify things or what have you.
Also, welcome YouTube folks. Ah, cool.
Also, welcome YouTube folks. Ah, cool.
Quite a few people today. It's funny
Quite a few people today. It's funny
that we have the same number of people
that we have the same number of people
watching me literally write an article
watching me literally write an article
as we do actually doing RL code, but uh
as we do actually doing RL code, but uh
yeah, welcome. This is the beginners's
yeah, welcome. This is the beginners's
guide. I guess it's beginners all the
guide. I guess it's beginners all the
way up to state-of-the-art, so it's it's
way up to state-of-the-art, so it's it's
not just like the basics. Um I'm doing
not just like the basics. Um I'm doing
this mostly out of request. People have
this mostly out of request. People have
requested this repeatedly. There's also
requested this repeatedly. There's also
a a companion article that starts with
a a companion article that starts with
programming in ML for like really basic
programming in ML for like really basic
stuff.
the full journey.
the full journey.
Yeah. I mean, because basically the
Yeah. I mean, because basically the
thing is like the way that I got into
thing is like the way that I got into
research was pretty self-directed and it
research was pretty self-directed and it
was like very meandering and
was like very meandering and
independent. I kind of just tried stuff,
independent. I kind of just tried stuff,
read a bunch of papers, tried other
read a bunch of papers, tried other
things, started doing research, which is
things, started doing research, which is
literally doing research is just like
literally doing research is just like
read a paper, like read a couple of
read a paper, like read a couple of
papers, get some ideas on how they could
papers, get some ideas on how they could
improve, try to improve them. if it
improve, try to improve them. if it
actually does improve like write
actually does improve like write
something about it. Try to publish it.
something about it. Try to publish it.
Do that in the loop until you get better
Do that in the loop until you get better
at it and you get better ideas.
at it and you get better ideas.
So this will be what you can point to
So this will be what you can point to
when new folks come in. Yes, this is
when new folks come in. Yes, this is
going to be like the overarching like
going to be like the overarching like
this points to all my other articles and
this points to all my other articles and
has a basically stepbystep guide on this
has a basically stepbystep guide on this
is how you learn reinforcement learning.
is how you learn reinforcement learning.
Do this, read this, read this, do that,
Do this, read this, read this, do that,
do this, read this, do that. Like now
do this, read this, do that. Like now
you know RL.
you know RL.
Will you publish it today? No. I think
Will you publish it today? No. I think
what I'm going to do with this because I
what I'm going to do with this because I
have to Well, I'll finish editing it. I
have to Well, I'll finish editing it. I
got to get images for it and then
got to get images for it and then
probably I'll make a post that this will
probably I'll make a post that this will
go live at 10K followers because I think
go live at 10K followers because I think
that would just be a fun thing to do.
that would just be a fun thing to do.
And we're basically like we're almost at
And we're basically like we're almost at
10K anyways.
I think that's what we'll do. 10K on um
I think that's what we'll do. 10K on um
on X, not YouTube, obviously.
All right, let's finish reading through
All right, let's finish reading through
this and if anybody has ideas or
this and if anybody has ideas or
questions or things on any of this,
questions or things on any of this,
please speak now so that I can make the
please speak now so that I can make the
edits before it's live.
Advanced topic applications. You have
Advanced topic applications. You have
deep knowledge of another field. Use it
deep knowledge of another field. Use it
to look for problems where RL can help.
to look for problems where RL can help.
These tend to have the same feel. These
These tend to have the same feel. These
all tend uh these tend to these
all tend uh these tend to these
Let's not split my infinitives.
Let's not split my infinitives.
These tend or these all tend to have
These tend or these all tend to have
English. These all tend to have the same
English. These all tend to have the same
feel. RL works when you have a fiddly
feel. RL works when you have a fiddly
interactive optimization process and
interactive optimization process and
build a fast sim. To give you some
build a fast sim. To give you some
ideas, we're currently doing open source
ideas, we're currently doing open source
work on drones and logistics. Both are
work on drones and logistics. Both are
areas where we can easily build
areas where we can easily build
simulators that run millions of steps
simulators that run millions of steps
per second and a few hundred lines of
per second and a few hundred lines of
basic safe multi- aent multitasks
basic safe multi- aent multitasks
stuff are going to be a lot better with
stuff are going to be a lot better with
images.
images.
Uh you are correct that I did not
Uh you are correct that I did not
include any images in this at all.
The funny thing with multi-agent and
The funny thing with multi-agent and
multitask shadow is it doesn't change
multitask shadow is it doesn't change
the architecture at all.
the architecture at all.
Like it literally doesn't change the
Like it literally doesn't change the
architecture one bit.
Is that news?
Is that news?
Like, is that surprising to you? I have
Like, is that surprising to you? I have
a bullet point on this, but it's like,
a bullet point on this, but it's like,
yeah, it like people make multi-agent
yeah, it like people make multi-agent
out to be way harder than it is. My PhD
out to be way harder than it is. My PhD
is on multi-agent. It's literally the
is on multi-agent. It's literally the
same thing as single agent.
same thing as single agent.
Separate encoding idea.
Separate encoding idea.
little confused. I'm trying to solve a
little confused. I'm trying to solve a
Rubik's cube with RL since I met you on
Rubik's cube with RL since I met you on
Twitter. It's been a fun way to go. Ah,
Twitter. It's been a fun way to go. Ah,
cool. Yeah, we had a couple people like
cool. Yeah, we had a couple people like
mess with that and nobody ever actually
mess with that and nobody ever actually
finished that project. So, uh, yeah, if
finished that project. So, uh, yeah, if
you want to do that and as an end, we'd
you want to do that and as an end, we'd
very happily take a Rubik's Cube
very happily take a Rubik's Cube
environment PR.
environment PR.
It's probably like you can probably
It's probably like you can probably
write a fun 3D renderer for it very
write a fun 3D renderer for it very
easily as well. Like you could probably
easily as well. Like you could probably
even make one with like the rotation
even make one with like the rotation
animations and it would probably be
animations and it would probably be
maybe 200 lines of Ray lip code.
maybe 200 lines of Ray lip code.
Without the animations it would
Without the animations it would
literally be like 20 or 30 lines of RIP
literally be like 20 or 30 lines of RIP
code. Probably maybe 50.
code. Probably maybe 50.
The separate encodings idea is a little
The separate encodings idea is a little
confusing.
Separate encodings idea.
Separate encodings idea.
Do you mean is it a actually I want to
Do you mean is it a actually I want to
understand this. So shadow is it
understand this. So shadow is it
specifically something about this that's
specifically something about this that's
confusing because nothing that I wrote
confusing because nothing that I wrote
here has anything to do with multi- aent
or are you talking about like entity
or are you talking about like entity
encodings
So, uh, here's the funny thing with
So, uh, here's the funny thing with
entity encodings.
entity encodings.
Lately,
Lately,
I've literally just been like putting
I've literally just been like putting
them into the observations flat without
them into the observations flat without
an entity encoder and just maybe sorting
an entity encoder and just maybe sorting
them by distance.
them by distance.
And this kind of works just as well.
And this kind of works just as well.
The problem with entity encoders,
The problem with entity encoders,
in order to do entity encoders fast, you
in order to do entity encoders fast, you
need custom CUDA.
need custom CUDA.
And we have not actually gone through
And we have not actually gone through
the effort of doing that because it's
the effort of doing that because it's
it's the type of thing that would
it's the type of thing that would
probably take me several days
probably take me several days
mostly because of the annoying bindings.
Imagine you have image and vector obs.
Imagine you have image and vector obs.
Yes. So you'd have like a vector per
Yes. So you'd have like a vector per
agent, right?
Yeah, I know what you mean by entity
Yeah, I know what you mean by entity
encodings with that stuff. Um, it's like
encodings with that stuff. Um, it's like
pointwise kind of or entity wise.
You'll get that from the papers to be
You'll get that from the papers to be
fair that I've suggested
fair that I've suggested
and it's a niche thing, but like I don't
and it's a niche thing, but like I don't
know if I even want to recommend it
know if I even want to recommend it
generally because the thing is like it's
generally because the thing is like it's
actually very slow if you do it naively.
actually very slow if you do it naively.
like it's really slow.
Probably the biggest problem we have in
Probably the biggest problem we have in
um the driving sim right now, right? The
um the driving sim right now, right? The
biggest problem we have in the driving
biggest problem we have in the driving
sim is that there's this really slow
sim is that there's this really slow
point-wise encoder
point-wise encoder
and uh without custom CUDA, it's just
and uh without custom CUDA, it's just
there's no way to make it fast.
there's no way to make it fast.
Keep reading through this.
Keep reading through this.
Longer term, we are most interested in
Longer term, we are most interested in
applying RL to hard science and
applying RL to hard science and
manufacturing. Are there areas of
manufacturing. Are there areas of
molecular simulation where we can cut
molecular simulation where we can cut
corners on fidelity? Are there
corners on fidelity? Are there
constrained areas of robotics where the
constrained areas of robotics where the
current simulators are massive overkill?
current simulators are massive overkill?
What about in the production of goods?
What about in the production of goods?
There are likely problems where nobody
There are likely problems where nobody
has even thought of building a sim. As a
has even thought of building a sim. As a
quick plug, if you want to join us on
quick plug, if you want to join us on
the business side, this is the best way.
the business side, this is the best way.
If you can build a prototype and we can
If you can build a prototype and we can
find a client, we'll build you a board
find a client, we'll build you a board
for the contract.
Build a prototype and we can find a
Build a prototype and we can find a
client, we'll you aboard for the
client, we'll you aboard for the
contract.
contract.
Here
are a few successful applications and I
are a few successful applications and I
link the
link the
fusion
fusion
driving with lift commercial cooling
driving with lift commercial cooling
systems self-driving and uh
Cool.
Advanced topic algorithms.
Advanced topic algorithms.
You need a good enough math background
You need a good enough math background
to be able to understand publications in
to be able to understand publications in
this area. A good enough programming
this area. A good enough programming
background to run your own experiments.
background to run your own experiments.
Expect to read a larger number of papers
Expect to read a larger number of papers
critically. Most published results are
critically. Most published results are
uh
uh
more publish uh results are wrong in RL
more publish uh results are wrong in RL
than in any other uh than in other areas
than in any other uh than in other areas
of AI
of AI
even the ones with strong evidence. Here
even the ones with strong evidence. Here
are a variety of research areas with
are a variety of research areas with
important open questions and some of the
important open questions and some of the
relevant papers off policy learning
relevant papers off policy learning
during the peak 2019 golden era of RL.
during the peak 2019 golden era of RL.
OpenAI mostly used on policy methods
OpenAI mostly used on policy methods
while deep mind used off policy ones.
while deep mind used off policy ones.
Dota and emergent tool use are clear-cut
Dota and emergent tool use are clear-cut
confirmations of on policy methods
confirmations of on policy methods
working in relatively clean settings.
working in relatively clean settings.
We don't have the same thing for off
We don't have the same thing for off
policy methods because the main mind
policy methods because the main mind
results tend to have more moving parts.
results tend to have more moving parts.
On policy methods work great and fast
On policy methods work great and fast
with unlimited data.
with unlimited data.
On policy methods work great and
on policy methods work great and are
on policy methods work great and are
fast with unlimited data. Off policy
fast with unlimited data. Off policy
methods are sometimes favored in data
methods are sometimes favored in data
poor settings. Can we get the best of
poor settings. Can we get the best of
both? Here are some papers we're looking
both? Here are some papers we're looking
at. Got rainbow the first usable off
at. Got rainbow the first usable off
policy method to compute. It's a kitchen
policy method to compute. It's a kitchen
sink disaster of tricks bolted onto DQN.
sink disaster of tricks bolted onto DQN.
That's not an exaggeration.
That's not an exaggeration.
Beyond the Rainbow. This paper hasn't
Beyond the Rainbow. This paper hasn't
gotten any attention yet, but it is a
gotten any attention yet, but it is a
good follow-up to Rainbow that balances
good follow-up to Rainbow that balances
sample efficiency for relatively slow
sample efficiency for relatively slow
sims with decent wall clock training
sims with decent wall clock training
time. I rather like this paper.
time. I rather like this paper.
Human level Atari 200 times faster. It's
Human level Atari 200 times faster. It's
only 200 times faster if you count
only 200 times faster if you count
environment steps uh and treat computers
environment steps uh and treat computers
as free.
It's only 2005. If you're counting
It's only 2005. If you're counting
environment steps and treat
you're counting environment steps and
you're counting environment steps and
treating computers free the results
treating computers free the results
still holds though so there's probably
still holds though so there's probably
something here. Hala introduces vrace to
something here. Hala introduces vrace to
correct off policy drift. We tried this
correct off policy drift. We tried this
in puff for lilib 3.0. It was worse uh
in puff for lilib 3.0. It was worse uh
it was just worse than J. It did help a
it was just worse than J. It did help a
little overall when combined with J but
little overall when combined with J but
not specifically for off policy drift.
model based learning. Can you compress
model based learning. Can you compress
transitions?
Uh this is not true actually. We'll just
Uh this is not true actually. We'll just
do it like this. Model based learning.
do it like this. Model based learning.
Can you compress transitions by
Can you compress transitions by
predicting the next state?
predicting the next state?
Good enough math background. Is that
Good enough math background. Is that
undergrad grad level just thinking that
undergrad grad level just thinking that
they am rigorous or you're saying go
they am rigorous or you're saying go
check the papers and see math? Where do
check the papers and see math? Where do
I say good enough math?
You need a good enough math background.
Ah, so that is from my other guide.
So the criteria I give you here for
So the criteria I give you here for
having a good enough background. It's
having a good enough background. It's
from the intro.
from the intro.
You can see what you think of this.
basic deep learn deep learning knowledge
basic deep learn deep learning knowledge
at the level of Stanford CS231N.
at the level of Stanford CS231N.
So if you've gone through 231N and you
So if you've gone through 231N and you
understand that then you're probably
understand that then you're probably
good enough with the math. And then on
good enough with the math. And then on
the programming side it's can you
the programming side it's can you
implement an LSTM without importing
implement an LSTM without importing
anything?
anything?
Also, if the implementation you thought
Also, if the implementation you thought
of was over a 100 lines, probably should
of was over a 100 lines, probably should
look at the programming article.
I'll put a little parenthetical in here.
I'll put a little parenthetical in here.
You need a good enough math background
You need a good enough math background
231N to be able to understand
231N to be able to understand
publications in this area and a good
publications in this area and a good
enough programming background to run
enough programming background to run
your own experiments.
your own experiments.
Bet to read a larger number of papers
Bet to read a larger number of papers
critical uh critically. More published
critical uh critically. More published
results are wrong in RL than in other
results are wrong in RL than in other
areas of AI, even the ones with strong
areas of AI, even the ones with strong
evidence.
evidence.
All right, cool. I'd already read all
All right, cool. I'd already read all
this stuff.
this stuff.
Uh model based learning. Can you
Uh model based learning. Can you
compress transitions by predicting the
compress transitions by predicting the
next state? Various methods attempt to
next state? Various methods attempt to
do this and then learn within
do this and then learn within
hallucinated data generated by the
hallucinated data generated by the
model. Others attempt to just use the
model. Others attempt to just use the
model as an auxiliary loss. Science in
model as an auxiliary loss. Science in
this area is iffy and I don't trust the
this area is iffy and I don't trust the
reasoning behind any of the published
reasoning behind any of the published
results. There is very likely something
results. There is very likely something
here though possibly is an alternative
here though possibly is an alternative
to off policy resampling.
to off policy resampling.
Recurrent world models facilitate policy
Recurrent world models facilitate policy
evolution original world models paper
evolution original world models paper
mastering yada yada this probably the
mastering yada yada this probably the
best known paper in the space dreamer v3
best known paper in the space dreamer v3
this somehow scales to 200 mil
this somehow scales to 200 mil
parameters which usually doesn't work in
parameters which usually doesn't work in
RL and then our paper uh showing that
RL and then our paper uh showing that
the tricks introduced in dreamer v3
the tricks introduced in dreamer v3
don't work according to the reasoning
don't work according to the reasoning
given. Yeah, this was one of the papers
given. Yeah, this was one of the papers
that really burned me the most actually
that really burned me the most actually
I would say because it's like really
I would say because it's like really
well-known paper, really well-known
well-known paper, really well-known
author and uh the result is not wrong as
author and uh the result is not wrong as
far as I can tell and like people have
far as I can tell and like people have
done stuff with dreamer but the code is
done stuff with dreamer but the code is
a disaster and uh none of the reasoning
a disaster and uh none of the reasoning
given in the paper actually holds up to
given in the paper actually holds up to
ablations
like at all
like at all
search. Can you explore or simulate
search. Can you explore or simulate
chains of different possible outcomes?
chains of different possible outcomes?
Improve learning and/or test time
Improve learning and/or test time
performance by leveraging more compute.
performance by leveraging more compute.
Worked in several deep mind projects
Worked in several deep mind projects
with and without access to simulator
with and without access to simulator
dynamics, but has not really been used
dynamics, but has not really been used
more broadly. Mastering Atari,
I don't think yeah, I'm pretty sure it's
I don't think yeah, I'm pretty sure it's
not really been used more broadly. It's
not really been used more broadly. It's
mostly Deep Mind doing this style of
mostly Deep Mind doing this style of
stuff.
stuff.
Mastering Atari Go, chess, and shogi by
Mastering Atari Go, chess, and shogi by
planning with a learned
planning with a learned
uh model. Vio removes the requirement
uh model. Vio removes the requirement
from AlphaGo that you need to be able to
from AlphaGo that you need to be able to
set the simulator state. Go Explorer
set the simulator state. Go Explorer
uses a setable sim to gimmick very hard
uses a setable sim to gimmick very hard
exploration problems unreasonably well.
exploration problems unreasonably well.
To be fair, if set if setable state is
To be fair, if set if setable state is
useful, we can build it into almost
useful, we can build it into almost
every type of sim. Uh, efficient zero
every type of sim. Uh, efficient zero
build built on mu0 claims to beat
build built on mu0 claims to beat
dreamer v3
advanced topics infrastructure. I'm
advanced topics infrastructure. I'm
writing this in the context of puffer
writing this in the context of puffer
lives infrastructure since we
lives infrastructure since we
unambiguously lead here. That is a true
unambiguously lead here. That is a true
statement. There is just no other good
statement. There is just no other good
infrastructure. Our models train at 3 to
infrastructure. Our models train at 3 to
5 million steps per second in pietorrch
5 million steps per second in pietorrch
eager mode. Napkin math says that we
eager mode. Napkin math says that we
should be able to double this
should be able to double this
performance on our larger models and 10x
performance on our larger models and 10x
it on smaller models. Everything has to
it on smaller models. Everything has to
get faster to keep up environment speed.
get faster to keep up environment speed.
Read our endbinding.h file. It is
Read our endbinding.h file. It is
responsible for defining those magic vec
responsible for defining those magic vec
reset and vec step methods except that
reset and vec step methods except that
there is no magic. We're just creating n
there is no magic. We're just creating n
copies of your environment and running
copies of your environment and running
them in a loop. The magic logging method
them in a loop. The magic logging method
is just iterating through fields of your
is just iterating through fields of your
log strct and dividing by log n.
log strct and dividing by log n.
We have this set up for you because
We have this set up for you because
Python to C data transfer is tedious and
Python to C data transfer is tedious and
errorprone and the boiler plate is
errorprone and the boiler plate is
roughly the same for every environment.
roughly the same for every environment.
It is set up so that it is fairly easy
It is set up so that it is fairly easy
to get access to individual environments
to get access to individual environments
from Python if you need it as well. We
from Python if you need it as well. We
would like to build in setable
would like to build in setable
simulation state but we've not thought
simulation state but we've not thought
about it too much. The point is there's
about it too much. The point is there's
pretty much this is pretty much the
pretty much this is pretty much the
lowest overhead thing we can do. Data
lowest overhead thing we can do. Data
does not have to be copied between C and
does not have to be copied between C and
Python during step. Both have access to
Python during step. Both have access to
the same data pointers. Next step in
the same data pointers. Next step in
optimization here would be figuring out
optimization here would be figuring out
general approaches to caching and subd
general approaches to caching and subd
parallelization.
parallelization.
We have a Python implementation of N4.
uh I don't really need to say that
uh I don't really need to say that
Python implementation eventful as of 3.0
Python implementation eventful as of 3.0
round robin and way buffered. By
round robin and way buffered. By
default, all buffers are allocated in
default, all buffers are allocated in
shared memory. Each environment is given
shared memory. Each environment is given
a pointer to a block of memory. The C
a pointer to a block of memory. The C
process writes directly to this memory,
process writes directly to this memory,
making the entire batch available to the
making the entire batch available to the
main thread immediately without extra
main thread immediately without extra
copies, but already almost as fast as it
copies, but already almost as fast as it
possibly could be. It may be possible to
possibly could be. It may be possible to
buffer the actual CPU to GPU data
buffer the actual CPU to GPU data
transfer because this is a big chunk of
transfer because this is a big chunk of
over overhead on fast environments with
over overhead on fast environments with
large observations,
large observations,
models, and training. This is where the
models, and training. This is where the
biggest opportunity is. Very few common
biggest opportunity is. Very few common
operations like entity embedding where
operations like entity embedding where
we don't have custom kernels stuff needs
we don't have custom kernels stuff needs
to be
to be
or fused general and we haven't gotten
or fused general and we haven't gotten
much out of compile
compile or low precision. To be fair, we
compile or low precision. To be fair, we
haven't even tried to tune BF-16 on A00s
haven't even tried to tune BF-16 on A00s
or H00s and BF16 is nerfed on consumer
or H00s and BF16 is nerfed on consumer
cards. Should be computing model flops
cards. Should be computing model flops
utilization and seeing how close we are
utilization and seeing how close we are
getting to the limits of the card. But
getting to the limits of the card. But
we aren't right now. Puffer's
we aren't right now. Puffer's
performance depends strongly on using
performance depends strongly on using
thousands of parallel environments,
thousands of parallel environments,
large mini batch sizes. Like to have
large mini batch sizes. Like to have
less performance degradation with fewer
less performance degradation with fewer
environments and smaller mini batches.
environments and smaller mini batches.
Bonus work with Puffer. I said this
Bonus work with Puffer. I said this
would be an opinionated guide. Puffer AI
would be an opinionated guide. Puffer AI
is a comprehensive effort to fix
is a comprehensive effort to fix
everything wrong with reinforcement
everything wrong with reinforcement
learning. If I thought there was a
learning. If I thought there was a
better approach overall, I would be
better approach overall, I would be
doing that instead.
doing that instead.
Puffer Lib is free
a free and open source project and at
a free and open source project and at
this point most of the code is written
this point most of the code is written
by contributors. We absolutely need more
by contributors. We absolutely need more
help on the core. It's only a few
help on the core. It's only a few
thousand lines but this is where the
thousand lines but this is where the
most experience is required. This is
most experience is required. This is
where we are making key enabling
where we are making key enabling
breakthroughs
breakthroughs
and working on this is the best way to
and working on this is the best way to
understand
and contributing is the best way to
and contributing is the best way to
understand the cutting edge of
understand the cutting edge of
reinforcement learning.
reinforcement learning.
Discord.gg/puffer to get involved. If
Discord.gg/puffer to get involved. If
you found this guide useful, please take
you found this guide useful, please take
a moment to star on GitHub. Support our
a moment to star on GitHub. Support our
work for free. Cool. That is a nearly
work for free. Cool. That is a nearly
4,000word guide. Um,
but yeah, this is basically 25 pages
but yeah, this is basically 25 pages
worth of articles.
worth of articles.
We used Alpha Zero to solve an
We used Alpha Zero to solve an
industrial problem.
industrial problem.
Oh, cool.
Oh, cool.
So, you must have had setable sim state
So, you must have had setable sim state
then.
then.
Can I ask what organization you're with?
Can I ask what organization you're with?
That's cool.
previous company. But here's the
previous company. But here's the
article.
article.
Oh, it's not going to let you link it.
Oh, it's not going to let you link it.
Uh, if you just take give me the name or
Uh, if you just take give me the name or
put it in the Discord. Either way,
I don't know. Twitch might like auto
I don't know. Twitch might like auto
time you out for 30 seconds for links or
time you out for 30 seconds for links or
something. I don't have anything enabled
something. I don't have anything enabled
to do that. But basically, none of the
to do that. But basically, none of the
major streaming sites like it when you
major streaming sites like it when you
link stuff
because of all the spam bots
because of all the spam bots
next to Discord. Sure, I will look at
next to Discord. Sure, I will look at
that.
that.
[Music]
[Music]
I'm happy with this. I need to generate
I'm happy with this. I need to generate
images,
images,
but I will uh I'll make a tweet like
but I will uh I'll make a tweet like
tomorrow or whatever that this goes live
tomorrow or whatever that this goes live
at 10K
at 10K
for folks watching. I have this article
for folks watching. I have this article
here. Yes.
here. Yes.
And then I have this article
And then I have this article
here.
here.
2K words, 4K words.
2K words, 4K words.
Um, this is the main content I plan to
Um, this is the main content I plan to
put out for
put out for
education, at least for at least the
education, at least for at least the
next chunk of time. I don't want to be
next chunk of time. I don't want to be
spending too much of my time on just
spending too much of my time on just
educational content. I will update the
educational content. I will update the
uh the docs page on custom environments
uh the docs page on custom environments
to at least include a little bit more
to at least include a little bit more
about our API and how that works because
about our API and how that works because
clearly people don't fully understand
clearly people don't fully understand
that. Um, is there anything else in here
that. Um, is there anything else in here
or any general questions? Let's just put
or any general questions? Let's just put
it out this way instead because like you
it out this way instead because like you
haven't read the whole article and
haven't read the whole article and
probably people have not been here
probably people have not been here
listening to me yak for the whole time.
listening to me yak for the whole time.
Uh, do you have any general questions?
Uh, do you have any general questions?
Anybody here about reinforcement
Anybody here about reinforcement
learning or how to do things or how to
learning or how to do things or how to
learn things or how to approach things?
learn things or how to approach things?
If so, type them and I will see if I
If so, type them and I will see if I
think that they are answered by any
think that they are answered by any
point in this article. If not, I will
point in this article. If not, I will
make a couple quick revisions.
make a couple quick revisions.
Education content creates a feeder into
Education content creates a feeder into
the community. That is the general idea,
the community. That is the general idea,
right? So, we have this project. It's
right? So, we have this project. It's
open source. Uh I only have so many
open source. Uh I only have so many
hours in the day and I only have so many
hours in the day and I only have so many
hours like that I can work consecutively
hours like that I can work consecutively
for like a month straight before my
for like a month straight before my
brain just needs like a break. So, uh
brain just needs like a break. So, uh
ideally if we have more people that are
ideally if we have more people that are
like actually competently going about
like actually competently going about
this stuff,
this stuff,
especially in the areas where I do not
especially in the areas where I do not
have the time, it would help a lot.
have the time, it would help a lot.
There are areas of research at the
There are areas of research at the
moment where it would be great to get
moment where it would be great to get
help. It's a lot harder to get into that
help. It's a lot harder to get into that
stuff, but I think that a couple of our
stuff, but I think that a couple of our
contributors are at that level now.
contributors are at that level now.
They're just doing other stuff, but like
They're just doing other stuff, but like
it is very doable to get to this level,
it is very doable to get to this level,
especially if you come in with at least
especially if you come in with at least
like basic AI knowledge and a basic
like basic AI knowledge and a basic
systems background, can write decent
systems background, can write decent
simple low-level code, and you have AI
simple low-level code, and you have AI
knowledge at the level of CS231N.
knowledge at the level of CS231N.
Um, the rest of this stuff can be
Um, the rest of this stuff can be
learned very quickly.
learned very quickly.
Have the link to your Discord.
Oh, it's probably DM'd.
Yeah.
Oh, cool. Yeah. Yeah. Yeah. So this is
Oh, cool. Yeah. Yeah. Yeah. So this is
actually dude this was the exact thing
actually dude this was the exact thing
that um this is literally the exact type
that um this is literally the exact type
of thing that I like I was thinking
of thing that I like I was thinking
about with puffer. I didn't know that
about with puffer. I didn't know that
some anybody's done this yet. Like when
some anybody's done this yet. Like when
I'm thinking of really valuable
I'm thinking of really valuable
applications to uh for AI like thank you
applications to uh for AI like thank you
actually for this article. I'm going to
actually for this article. I'm going to
send this to my contributors.
Thank you very much for that. That's
Thank you very much for that. That's
like one of the problems cuz we're
like one of the problems cuz we're
looking for like really valuable
looking for like really valuable
problems where we can potentially do
problems where we can potentially do
stuff. Um, and that's like a huge market
stuff. Um, and that's like a huge market
as well because basically if we get one
as well because basically if we get one
of these big ones, right, if we get like
of these big ones, right, if we get like
one really big job where we're able to
one really big job where we're able to
just crush it, right, um, then we're
just crush it, right, um, then we're
kind of funded for RL for the
kind of funded for RL for the
foreseeable future and we can just keep
foreseeable future and we can just keep
doing cool science.
doing cool science.
The challenge end is still available.
Cool.
Oh yeah, we can. Yeah, this is Sure. We
Oh yeah, we can. Yeah, this is Sure. We
can crush this.
can crush this.
Yeah, we can crush this 100%.
We do have to be able to uh to do our
We do have to be able to uh to do our
own sim.
own sim.
But because we have to have the Sim be
But because we have to have the Sim be
fast. But yeah, we can crush that.
Have you seen any of the other stuff
Have you seen any of the other stuff
around Puffer Shadow? Like our new
around Puffer Shadow? Like our new
algorithm stuff and like some of our
algorithm stuff and like some of our
results
results
to give you a quick idea here
to give you a quick idea here
of like the type of stuff we've done.
We do ludicrously high performance sim.
We do ludicrously high performance sim.
Not really doing anything special
Not really doing anything special
honestly just by like not doing stuff
honestly just by like not doing stuff
the slow way that people normally do it.
the slow way that people normally do it.
And uh we have our RL optimized end to
And uh we have our RL optimized end to
end to be able to actually handle that
end to be able to actually handle that
amount of data.
amount of data.
And we can just train stuff on just
And we can just train stuff on just
crazy crazy amounts of sim. like you
crazy crazy amounts of sim. like you
don't really have to do anything super
don't really have to do anything super
smart on the algorithm side when you can
smart on the algorithm side when you can
do this. Uh we do actually have our own
do this. Uh we do actually have our own
algorithm research. So we use PO kind of
algorithm research. So we use PO kind of
but ours is a lot better than uh what
but ours is a lot better than uh what
you would expect normally. Different
you would expect normally. Different
optimizer, trajectory filtering,
optimizer, trajectory filtering,
different advantage function, a whole
different advantage function, a whole
bunch of uh improvements. But yeah,
bunch of uh improvements. But yeah,
cool.
Non Sim, they rebuilt it.
Non Sim, they rebuilt it.
Yeah. If you uh yeah, shadow, if you
Yeah. If you uh yeah, shadow, if you
don't have if you don't have documents
don't have if you don't have documents
preventing you from legally doing so,
preventing you from legally doing so,
like uh message us in in the Discord
like uh message us in in the Discord
because the thing from the article is
because the thing from the article is
true. where like if you have domain
true. where like if you have domain
knowledge in an area and you can help us
knowledge in an area and you can help us
get a contract, you can get a cut of it
get a contract, you can get a cut of it
if you help us like set it up and you
if you help us like set it up and you
want to work it cuz uh we are very well
want to work it cuz uh we are very well
positioned at this point with all our
positioned at this point with all our
Sim stuff to really crush any sort of
Sim stuff to really crush any sort of
problem that looks like that. We are
problem that looks like that. We are
really well positioned to just crush.
really well positioned to just crush.
So there might be an opportunity there.
What exactly reinforcement learning?
What exactly reinforcement learning?
What is the right context to think of
What is the right context to think of
it? Is it compai math? What is the
it? Is it compai math? What is the
motivation for it? What keeps the space
motivation for it? What keeps the space
fruitful progress in real world
fruitful progress in real world
applications? I don't know if that's in
applications? I don't know if that's in
the scope of what you're trying to do.
Uh arguably you're right that I haven't
Uh arguably you're right that I haven't
really sold like why learn reinforcement
really sold like why learn reinforcement
learning.
learning.
This is fair.
Not working for that company anymore.
Not working for that company anymore.
Oh, yeah. No, I wouldn't ask you if you
Oh, yeah. No, I wouldn't ask you if you
were. That would be I'm sure that there
were. That would be I'm sure that there
would be legal problems there. That's
would be legal problems there. That's
why I'm asking you, right?
like our outreach stuff has been pretty
like our outreach stuff has been pretty
dang successful
dang successful
um at this point. Like people are kind
um at this point. Like people are kind
of starting to see what we're doing.
of starting to see what we're doing.
Help a little bit what it is. Um,
Help a little bit what it is. Um,
and like we've we've kind of been we've
and like we've we've kind of been we've
had a good time so far like talking to
had a good time so far like talking to
people in tons of different industries.
people in tons of different industries.
The hardest thing at the moment is kind
The hardest thing at the moment is kind
of just finding those clear-cut
of just finding those clear-cut
reinforcement learning applications
reinforcement learning applications
where we can make something super
where we can make something super
valuable and like carving them into RL
valuable and like carving them into RL
problems. Um, so if we can do that and
problems. Um, so if we can do that and
get that to somebody, that's like
get that to somebody, that's like
that is super valuable.
that is super valuable.
Yeah. Yeah. So, Jason, I got to think
Yeah. Yeah. So, Jason, I got to think
about that a little bit. Like, do I
about that a little bit. Like, do I
actually want to try two.
I could just add like a thing here.
It should be in the first paragraph.
So I mean I guess the way that I answer
So I mean I guess the way that I answer
that Jason is it is uh it's an empirical
that Jason is it is uh it's an empirical
science. It's a sub area of artificial
science. It's a sub area of artificial
intelligence which artificial
intelligence which artificial
intelligence draws on computer science
intelligence draws on computer science
and discrete math for the most part. Uh
and discrete math for the most part. Uh
it is the branch of AI that deals with
it is the branch of AI that deals with
learning through interaction.
learning through interaction.
So agents learn by interacting with an
So agents learn by interacting with an
environment, the real world, a
environment, the real world, a
simulator, what have you. It models the
simulator, what have you. It models the
way that we learn in a way kind of that
way that we learn in a way kind of that
other branches don't. They're trained
other branches don't. They're trained
supervised or on big corpuses fixed data
supervised or on big corpuses fixed data
where you have no effect. You can't
where you have no effect. You can't
affect your data. Um
affect your data. Um
it's been applied to a bunch of spaces,
it's been applied to a bunch of spaces,
but what keeps it fruitful? Not it's
but what keeps it fruitful? Not it's
like not at all. It's stuck. there
like not at all. It's stuck. there
hasn't really been like the amount of
hasn't really been like the amount of
progress has slowed to almost a halt
progress has slowed to almost a halt
since 2019ish
since 2019ish
and that's what I'm trying to fix. So 90
and that's what I'm trying to fix. So 90
plus% of the time my content around here
plus% of the time my content around here
is focused on uh I am a scientist. I am
is focused on uh I am a scientist. I am
doing research. How do we make
doing research. How do we make
reinforcement learning work at a core
reinforcement learning work at a core
level algorithmically? How do we get it
level algorithmically? How do we get it
on tons of different applications in the
on tons of different applications in the
real world? How do we do that whole
real world? How do we do that whole
loop?
loop?
Uh, this is the rare piece of
Uh, this is the rare piece of
educational content I'm doing because
educational content I'm doing because
people have asked for it a ton.
people have asked for it a ton.
We have a ton of people on YouTube
We have a ton of people on YouTube
today. So, hey, welcome everybody. Um,
today. So, hey, welcome everybody. Um,
yeah, if you're new around here and you
yeah, if you're new around here and you
want to see some cool demos that just
want to see some cool demos that just
run in your browser,
run in your browser,
we've got Puffer Ocean here. So, there's
we've got Puffer Ocean here. So, there's
everything from like just simple arcade
everything from like just simple arcade
games like more visually interesting
games like more visually interesting
kind of stuff. These are all neural
kind of stuff. These are all neural
networks playing in your browser as
networks playing in your browser as
well.
well.
Got like this mobile. It's kind of like
Got like this mobile. It's kind of like
a stripped down League of Legends or
a stripped down League of Legends or
Dota type thing.
Dota type thing.
We've got uh what else we have in here.
We've got uh what else we have in here.
Oh yeah, we've got drones. It's not all
Oh yeah, we've got drones. It's not all
games.
We do like games though because games
We do like games though because games
are really easy to interpret. This is my
are really easy to interpret. This is my
uh the followup to my PhD thesis, neural
uh the followup to my PhD thesis, neural
MMO. Like a thousand agents running
MMO. Like a thousand agents running
around in this virtual world where they
around in this virtual world where they
can fight and get armor and tools and
can fight and get armor and tools and
equipment and they can trade with each
equipment and they can trade with each
other and they can level up and
other and they can level up and
different things full game.
different things full game.
Be the Johnny Apple Seed of RL.
Be the Johnny Apple Seed of RL.
I do want to make it accessible, right?
I do want to make it accessible, right?
The thing is this almost kind of
The thing is this almost kind of
happened a little bit by accident if you
happened a little bit by accident if you
think about it because what I did here
think about it because what I did here
is I was focused on how do I advance the
is I was focused on how do I advance the
cutting edge, right? How do I advance
cutting edge, right? How do I advance
reinforcement learning research and in
reinforcement learning research and in
the process right the way to advance
the process right the way to advance
reinforcement learning research was to
reinforcement learning research was to
dramatically simplify and accelerate a
dramatically simplify and accelerate a
lot of the existing stuff. So in the
lot of the existing stuff. So in the
process of doing that, we made the
process of doing that, we made the
entire reinforcement learning stack a
entire reinforcement learning stack a
fraction of the size that it used to be
fraction of the size that it used to be
and we made it so that people can
and we made it so that people can
actually do experiments on their laptop
actually do experiments on their laptop
now in seconds where it used to take
now in seconds where it used to take
hours and big fancy complicated machine.
hours and big fancy complicated machine.
Uh so that made it really accessible and
Uh so that made it really accessible and
we started getting a lot of new people
we started getting a lot of new people
where originally I had intended this to
where originally I had intended this to
make it easier for RL PhDs, now it's
make it easier for RL PhDs, now it's
like brand new programmers. So, it's
like brand new programmers. So, it's
come down a huge amount, become way more
come down a huge amount, become way more
accessible. Uh, and that stuff is nice
accessible. Uh, and that stuff is nice
and useful. Um, but I it's it's not the
and useful. Um, but I it's it's not the
main thing, right? It's kind of like a
main thing, right? It's kind of like a
byproduct. The main thing on my plate is
byproduct. The main thing on my plate is
fixing the field, like advancing the
fixing the field, like advancing the
science.
science.
That's the main goal.
That's the main goal.
And I invite all of you to come join me
And I invite all of you to come join me
on that because uh it's a very exciting
on that because uh it's a very exciting
area of science and it's probably the
area of science and it's probably the
last area of AI right now where you can
last area of AI right now where you can
make huge huge advancements without
make huge huge advancements without
needing a ton of capital.
needing a ton of capital.
Welcome major.
Welcome major.
And since we do have at less
me out it's free. Join the Discord if
me out it's free. Join the Discord if
you want to get involved with all this
you want to get involved with all this
cool stuff. And uh yeah, back to back to
cool stuff. And uh yeah, back to back to
work on this.
work on this.
Move forward. Move backwards first from
Move forward. Move backwards first from
very mediocre movie.
I guess it was a book first. Probably
I guess it was a book first. Probably
the book is better.
the book is better.
I think I'll take a crack at Rubik's
I think I'll take a crack at Rubik's
Cube. Dian Rayb.
Cube. Dian Rayb.
Yeah, that'll work. I think that'll
Yeah, that'll work. I think that'll
work. I don't know how complicated.
work. I don't know how complicated.
Well, you don't need the solver at all.
Well, you don't need the solver at all.
So it's probably it's probably a very
So it's probably it's probably a very
quick little thing to do be a good like
quick little thing to do be a good like
that's a good intro project like that
that's a good intro project like that
would be
would be
that would qualify for like this here
that would qualify for like this here
like section five of this
heck let's just tweet this right now
heck let's just tweet this right now
that like
All right, we're a little over 400 away
All right, we're a little over 400 away
from that. Cool.
Let me add a couple quick sentences to
Let me add a couple quick sentences to
this and then we'll move on to the next
this and then we'll move on to the next
thing for today.
thing for today.
Believe I have another 40 minutes to do.
Yeah, this paragraph I should spend more
Yeah, this paragraph I should spend more
time on. I think just a little bit more
time on. I think just a little bit more
time on this.
can read it today.
can read it today.
I doubt it'll hit 10K today. I kind of
I doubt it'll hit 10K today. I kind of
hope it doesn't just do that instantly
hope it doesn't just do that instantly
because I'll have to I still have to get
because I'll have to I still have to get
images. Like you need images for the
images. Like you need images for the
articles at the top. But uh I will I
articles at the top. But uh I will I
guess I'll just do it today if I have
guess I'll just do it today if I have
to. You know, I promise I do it. I do
to. You know, I promise I do it. I do
it.
Do start the repo though. Star the
Do start the repo though. Star the
puffer. It helps us out.
All right, we workshop this first
All right, we workshop this first
paragraph so that like I'll add the
paragraph so that like I'll add the
thing. Actually, Jason, that advice was
thing. Actually, Jason, that advice was
very good. And then the plan is if I
very good. And then the plan is if I
have a little bit of time after that,
have a little bit of time after that,
we're going to start on the camera ready
we're going to start on the camera ready
for my publication. I'm pretty sure that
for my publication. I'm pretty sure that
doesn't violate anything because it's
doesn't violate anything because it's
already accepted. But yeah, we have um
uh where is it?
uh where is it?
Yeah. So, this paper was accepted to
Yeah. So, this paper was accepted to
RLC.
RLC.
So, yay. There's finally going to be a
So, yay. There's finally going to be a
puffer publication.
Uh, it's super outdated already because
Uh, it's super outdated already because
Huffer moves way faster than the
Huffer moves way faster than the
conference cycle, but I don't know. I'll
conference cycle, but I don't know. I'll
still show it off and we can edit it a
still show it off and we can edit it a
little bit today. I figured I'd do it on
little bit today. I figured I'd do it on
stream because why the hell not.
Not tomorrow though. Tomorrow I'm
Not tomorrow though. Tomorrow I'm
traveling all day and then I will be
traveling all day and then I will be
back streaming uh Thursday, Friday.
Hello, Kyoko.
Hello, Kyoko.
Congrats on what?
Congrats on what?
I guess there are many cool things
I guess there are many cool things
lately.
I didn't just instantly hit 10K, right?
I didn't just instantly hit 10K, right?
Like
Like
That'd be kind of funny. Ow.
Reinforcement learning is hard and most
Reinforcement learning is hard and most
of the material out there for beginners
of the material out there for beginners
makes it even harder.
It's the underexplored niche of AI
that focuses on learning from
that focuses on learning from
interaction
interaction
and one of the few areas where you can
and one of the few areas where you can
really advance the field without a ton
really advance the field without a ton
of comput.
of comput.
The advice here is a formalized
products.
products.
Uh
fusion
fusion
people did fusion with it, right?
people did fusion with it, right?
Do logistics first.
I haven't vetted the fusion one, which
I haven't vetted the fusion one, which
is a little
Logistics Fusion, cooling, driving.
maybe super dumb question, but why is TD
maybe super dumb question, but why is TD
such a popular choice in RL?
such a popular choice in RL?
Um,
so it's not exactly,
so it's not exactly,
but the thing is that like everything's
but the thing is that like everything's
kind of TD if you squint at it, right?
kind of TD if you squint at it, right?
Like there's always like pretty much
Like there's always like pretty much
anything that you get, there's like some
anything that you get, there's like some
form of TD in it. And it's you need some
form of TD in it. And it's you need some
sort of credit assignment is the
sort of credit assignment is the
problem, right? So like you get a reward
problem, right? So like you get a reward
later and you need to figure out how
later and you need to figure out how
much to care about said reward and
much to care about said reward and
there's not a good way of doing that in
there's not a good way of doing that in
general. So usually there's some sort of
general. So usually there's some sort of
like fancy learned exponential
like fancy learned exponential
discounting
discounting
which you write out as like a TD over N,
which you write out as like a TD over N,
right?
It's mostly just like
It's mostly just like
what if you do regression? How
what if you do regression? How
how would you do regression?
how would you do regression?
What are you regressing?
like goal condition behavioral cloning.
like goal condition behavioral cloning.
Well, that's no longer RL, right?
Well, that's no longer RL, right?
Behavioral cloning, you're no longer
Behavioral cloning, you're no longer
learning from simulation data. You're
learning from simulation data. You're
learning from expert data.
You need expert data to do that and you
You need expert data to do that and you
can't scale it infinitely because you
can't scale it infinitely because you
only have so much so much expert data
ABC
uh I do not consider so it depends on
uh I do not consider so it depends on
your definitions. If you consider
your definitions. If you consider
offline reinforcement learning to be
offline reinforcement learning to be
reinforcement learning, then sure. I do
reinforcement learning, then sure. I do
not consider offline reinforcement
not consider offline reinforcement
learning to be reinforcement learning.
learning to be reinforcement learning.
It's a clever term that somebody came up
It's a clever term that somebody came up
for supervised learning in order to like
for supervised learning in order to like
sell it at a conference or whatever. Um,
sell it at a conference or whatever. Um,
the key defining aspect of reinforcement
the key defining aspect of reinforcement
learning for me is interaction.
learning for me is interaction.
So, if you're not actually interacting
So, if you're not actually interacting
with a a sim or the real world or
with a a sim or the real world or
something, it's not RL.
defining RL as the like the specific
defining RL as the like the specific
class of algorithms makes way less sense
class of algorithms makes way less sense
than defining it as uh defining it
than defining it as uh defining it
through interaction.
with a dash.
with a dash.
No, it's literally It's the right word.
No, it's literally It's the right word.
Shut up and spell check.
Applications include
Applications include
robotics, logistics, gaming, and even
robotics, logistics, gaming, and even
scientific simulation.
It's calibration.
Right. It was calibration for fusion.
Control the plasmas. Yeah.
Control the plasmas. Yeah.
Well, it's control, I guess.
I'll call the control problem.
Sorry if I jumped the gun. Reinforcement
Sorry if I jumped the gun. Reinforcement
learning is currently an
learning is currently an
for
All right, we'll spend a little bit of
All right, we'll spend a little bit of
time on this then to like make sure that
time on this then to like make sure that
this is good because this is ultimately
this is good because this is ultimately
the thing that is the most important.
the thing that is the most important.
Also, did I do the same thing for my
Also, did I do the same thing for my
other one? I was like, there's the other
other one? I was like, there's the other
article.
Yeah, this is fine because this is a
Yeah, this is fine because this is a
personal article. like you read this
personal article. like you read this
because you basically you already follow
because you basically you already follow
me
me
or like you like you you don't read this
or like you like you you don't read this
because
because
like you're not going to read my advice
like you're not going to read my advice
on a topic that's like super subjective
on a topic that's like super subjective
um if you don't already follow this like
um if you don't already follow this like
in RL it makes more sense because it's
in RL it makes more sense because it's
more like it's more specialized
more like it's more specialized
but that's basically because people
but that's basically because people
asked me for it.
Okay.
what I can't wrap my head around RL is
what I can't wrap my head around RL is
why Marovian YTD it becomes
why Marovian YTD it becomes
because it's underexplored. No. Okay,
because it's underexplored. No. Okay,
Sean, let me explain this. My guide
Sean, let me explain this. My guide
explains this. Okay, ignore all the
explains this. Okay, ignore all the
formalism about MDPs.
formalism about MDPs.
TD is kind of okay, but it's like
TD is kind of okay, but it's like
whatever. Just ignore all the formulism
whatever. Just ignore all the formulism
about MDPs, though. All right, ignore
about MDPs, though. All right, ignore
all of it. It's cuz that was the model
all of it. It's cuz that was the model
that they could fit to the problems.
that they could fit to the problems.
RL is going to be a lot easier if you
RL is going to be a lot easier if you
just view it as an empirical science.
just view it as an empirical science.
All right,
All right,
none of the theory actually works. And
none of the theory actually works. And
like they'll basically RL is the field
like they'll basically RL is the field
compared to other areas of AI where
compared to other areas of AI where
they'll just they'll write pages and
they'll just they'll write pages and
pages and pages and pages of math
pages and pages and pages of math
and none of it works. All right. And
and none of it works. All right. And
then the super simple thing without any
then the super simple thing without any
theoretical justification or like any
theoretical justification or like any
formal proof that works perfectly.
If you view it as an empirical science
If you view it as an empirical science
like you will no longer be up at night
like you will no longer be up at night
over this.
RL equals LMS.
Actually, I think that there's there's
Actually, I think that there's there's
probably better theory for general uh
probably better theory for general uh
general learning than there is for RL.
So it's it's a weird combination where
So it's it's a weird combination where
basically
basically
people in RL they try to write down more
people in RL they try to write down more
formal math than other areas of AI even
formal math than other areas of AI even
though they actually have less formal
though they actually have less formal
math that actually works. All right.
math that actually works. All right.
And it's probably because MDPs exist
And it's probably because MDPs exist
independently. So like it looks like, oh
independently. So like it looks like, oh
yeah, we can just model this as that and
yeah, we can just model this as that and
we get all this really nice math. But
we get all this really nice math. But
then the thing is none of it actually
then the thing is none of it actually
works. So, what's the point?
Like a mathematical model is useful to
Like a mathematical model is useful to
the extent that it can actually predict
the extent that it can actually predict
the thing you care about. And if it
the thing you care about. And if it
can't, then it's not a useful model.
And when you have a field where you can
And when you have a field where you can
do work, but you don't have a
do work, but you don't have a
mathematical model, that's an empirical
mathematical model, that's an empirical
field.
Damn.
Damn.
I mean, it's not like the thing is as
I mean, it's not like the thing is as
soon as you just as soon as you give up
soon as you just as soon as you give up
that old perspective, right? As soon as
that old perspective, right? As soon as
you come and like look at this the way
you come and like look at this the way
that we're doing it in Puffer Lib,
that we're doing it in Puffer Lib,
everything will make a lot more sense
everything will make a lot more sense
and like you'll actually see the path to
and like you'll actually see the path to
progress. I promise you
progress. I promise you
cuz I pretty much in my head here like
cuz I pretty much in my head here like
if I just ignore everything else and I
if I just ignore everything else and I
were to work on this stuff for like a
were to work on this stuff for like a
year or two, I pretty much have the A to
year or two, I pretty much have the A to
B to C to D to like RL solved.
MDP's considered harmful. Are you
MDP's considered harmful. Are you
serious? I was actually I was
serious? I was actually I was
considering doing that. I but I was like
considering doing that. I but I was like
ah you know what I kind of piss off the
ah you know what I kind of piss off the
academics as enough as is.
Oh, he didn't actually write MDP
Oh, he didn't actually write MDP
considered harmful.
Oh, I see. He actually kind of did.
That's cool.
Who's this dude?
Who's this dude?
Oh, Alberta. Okay.
Yeah. So I don't
Yeah. So I don't
I guess you can put that stuff on
I guess you can put that stuff on
archive technically, but like academics
archive technically, but like academics
love to do this thing where they kind of
love to do this thing where they kind of
like fence each other with uh it's like
like fence each other with uh it's like
British Parliament, right? They like
British Parliament, right? They like
fence each other with like very couched
fence each other with like very couched
language that's basically saying X thing
language that's basically saying X thing
is old and dumb. Whereas I won't do that
is old and dumb. Whereas I won't do that
and I'll just say, "Hey, this thing is
and I'll just say, "Hey, this thing is
old and dumb."
old and dumb."
Poker AI. All right, cool. I've actually
Poker AI. All right, cool. I've actually
wanted to do poker and puffer lip just
wanted to do poker and puffer lip just
to see,
to see,
but we're actually I think chess is
but we're actually I think chess is
probably the better one at the moment.
We got lots of projects.
I will retweet that with MDP's
I will retweet that with MDP's
considered harmful.
considered harmful.
funny.
Also, uh continual learning as a a
Also, uh continual learning as a a
category like continual learning doesn't
category like continual learning doesn't
need to exist.
need to exist.
That's the other funny thing. There's
That's the other funny thing. There's
not really a big difference at all with
not really a big difference at all with
stuff.
Reinforcement learning is the branch of
Reinforcement learning is the branch of
AI that focuses on learning through
AI that focuses on learning through
interaction.
Reinforcement learning is the branch of
Reinforcement learning is the branch of
AI
Reinforcement learning is about learning
Reinforcement learning is about learning
through interaction.
through interaction.
Applications include robotics,
underexplored niche of AI where you can
underexplored niche of AI where you can
really advance the field without a ton
really advance the field without a ton
of compute.
The learning RL
But learning RL and most of the material
But learning RL and most of the material
out there for be uh is hard.
All right. I like this was good advice
All right. I like this was good advice
from Jason.
from Jason.
Reinforcement learning is about learning
Reinforcement learning is about learning
through interaction, application,
through interaction, application,
robotics, logistics, gaming, and even
robotics, logistics, gaming, and even
control problems.
I was at a loss for words for a minute
I was at a loss for words for a minute
to realize that RL is all empirical and
to realize that RL is all empirical and
the gap between theory and practical is
the gap between theory and practical is
huge. That was painful just listening
huge. That was painful just listening
to.
to.
Yeah. But the thing is again like look
Yeah. But the thing is again like look
at what we've built and tell me if it's
at what we've built and tell me if it's
painful. Okay.
painful. Okay.
Did I show you this? Did I show you the
Did I show you this? Did I show you the
Pong demo?
Pong demo?
Watch this.
Watch this.
So we can kind of just show some people
So we can kind of just show some people
the stuff we build around here.
the stuff we build around here.
All right, puffer train. Puff or let's
All right, puffer train. Puff or let's
do Eval first. Puffer
do Eval first. Puffer
just so you can see the difference.
just so you can see the difference.
All right, here's Pong.
All right, here's Pong.
This is a random agent. It can't play at
This is a random agent. It can't play at
all. It's just it barely ever hits the
all. It's just it barely ever hits the
puffer back. We use a puffer for the
puffer back. We use a puffer for the
ball because of course we do. Puffer
ball because of course we do. Puffer
train. Puffer pong.
train. Puffer pong.
All right. This is something that
All right. This is something that
usually like when we use Pong for
usually like when we use Pong for
research during my PhD, you'd have to
research during my PhD, you'd have to
overnight this because this would take
overnight this because this would take
hours.
hours.
Okay, we're done.
Here's your perfect Pong model or almost
Here's your perfect Pong model or almost
perfect pong model. It misses once in a
perfect pong model. It misses once in a
blue moon.
blue moon.
It never loses, though.
And like that's the only demo that we
And like that's the only demo that we
have that's literally 5 seconds, but um
have that's literally 5 seconds, but um
in fact we've even done it in 3 seconds
in fact we've even done it in 3 seconds
before. But a lot of the environments
before. But a lot of the environments
train in like 30 seconds. And most of
train in like 30 seconds. And most of
the ones that don't train in a few
the ones that don't train in a few
minutes. And then we have like a couple
minutes. And then we have like a couple
way harder tasks that are 10 minutes or
way harder tasks that are 10 minutes or
15 minutes. And then maybe a couple of
15 minutes. And then maybe a couple of
tasks that actually take substantially
tasks that actually take substantially
more.
sent you the challenge link. Thank you.
sent you the challenge link. Thank you.
We will be in touch with that. That
We will be in touch with that. That
sounds like a a cool application area.
Yeah. Sean, so one of the the things as
Yeah. Sean, so one of the the things as
well that reinforcement learning just
well that reinforcement learning just
got stuck on for some reason is like
got stuck on for some reason is like
academics for whatever reason they felt
academics for whatever reason they felt
like you really the only thing that
like you really the only thing that
matters is that you can learn with less
matters is that you can learn with less
experience. That's the only thing that
experience. That's the only thing that
matters. All right? So if you have
matters. All right? So if you have
method A that learns long in two million
method A that learns long in two million
steps and method B that learns in one
steps and method B that learns in one
million steps, method uh B is way
million steps, method uh B is way
better. even if method B takes a
better. even if method B takes a
thousand times longer
thousand times longer
and like compute.
and like compute.
So basically what happened is over the
So basically what happened is over the
years people did a ton of theory work
years people did a ton of theory work
and then they trained on these really
and then they trained on these really
stupidly simple tasks very very slowly
stupidly simple tasks very very slowly
because all their code was crazy slow
because all their code was crazy slow
and their algorithms were crazy slow.
and their algorithms were crazy slow.
All right.
All right.
And we kind of came along and said,
And we kind of came along and said,
"Okay, that's dumb. Let's just do
"Okay, that's dumb. Let's just do
everything with fast sims and try to
everything with fast sims and try to
make things run faster in wall clock
make things run faster in wall clock
time and uh we'll just build fast sims."
time and uh we'll just build fast sims."
And lo and behold, in industry, if you
And lo and behold, in industry, if you
can build a fast simulator, nobody ever
can build a fast simulator, nobody ever
says, "Well, can it train in fewer
says, "Well, can it train in fewer
steps?" Nobody cares. It's an entirely
steps?" Nobody cares. It's an entirely
academic invented problem. All right.
academic invented problem. All right.
Now, yeah, there are cases where you
Now, yeah, there are cases where you
can't build fast Sims and it's useful
can't build fast Sims and it's useful
there, but like basically nobody even
there, but like basically nobody even
bothered trying to say, "Hey, wait a
bothered trying to say, "Hey, wait a
second. Reinforcement learning can solve
second. Reinforcement learning can solve
all these problems where we just build
all these problems where we just build
fast Sims." Like, they didn't even do
fast Sims." Like, they didn't even do
that. So, that's what we do. We'll get
that. So, that's what we do. We'll get
to um we'll get to sample efficient
to um we'll get to sample efficient
learning. It's not going to be that
learning. It's not going to be that
hard. It'll be fine. Um like with all
hard. It'll be fine. Um like with all
the stuff we've built, it's not going to
the stuff we've built, it's not going to
be that that hard. We'll have to do some
be that that hard. We'll have to do some
research, but it won't be that bad.
research, but it won't be that bad.
Nowhere near as bad as it is now.
Go through this paragraph one more time.
Go through this paragraph one more time.
Reinforcement learning is about learning
Reinforcement learning is about learning
through interaction applications include
through interaction applications include
robotics, logistics, gaming, and even
robotics, logistics, gaming, and even
control problems in science like nuclear
control problems in science like nuclear
fusion. Underexplored niche of AI where
fusion. Underexplored niche of AI where
you can really advance the field without
you can really advance the field without
a ton of compute. But learning RL is
a ton of compute. But learning RL is
hard and most of the material out there
hard and most of the material out there
for beginners makes it even harder. Vice
for beginners makes it even harder. Vice
here is a formalized version of how I
here is a formalized version of how I
train new puffer lip contributors. Some
train new puffer lip contributors. Some
of these came in with zero programming
of these came in with zero programming
knowledge and now help advance our
knowledge and now help advance our
research tools. The key start doing
research tools. The key start doing
reinforcement learning immediately while
reinforcement learning immediately while
filling a knowledge gap slowly and in
filling a knowledge gap slowly and in
order of relevance
others.
others.
Reinforcement learning
In other words, reinforcement learning.
That's fun. All right. Cool. I like that
That's fun. All right. Cool. I like that
a lot.
a lot.
We are I think this is set.
Oh.
Yeah. Okay.
Yeah. Okay.
This is uh doing very well.
I believe this Meta's MRQ style dynamics
I believe this Meta's MRQ style dynamics
based learning integration into model
based learning integration into model
free on model policy. Go Mr. Oh, Mr. Q.
free on model policy. Go Mr. Oh, Mr. Q.
Yeah, I uh I got to look at Mr. Q as
Yeah, I uh I got to look at Mr. Q as
well for sure. I haven't like seen
well for sure. I haven't like seen
enough to put it into the post on it.
I'm following.
I'm following.
Okay, bro.
All right, I will be right back. I'm
All right, I will be right back. I'm
going to use the restroom. We'll spend a
going to use the restroom. We'll spend a
few minutes on the uh the new
few minutes on the uh the new
uh the new paper. Then I got to run for
uh the new paper. Then I got to run for
meeting. I'll be right back. Go.
All
right.
Let's take a quick look at uh the paper.
Don't open review.
I thought for a second that I was like
I thought for a second that I was like
wrong and it had been accepted.
Yeah, there it is. There's the formal
Yeah, there it is. There's the formal
accept. I actually hadn't even seen that
accept. I actually hadn't even seen that
it was posted. So that would have been
it was posted. So that would have been
kind of awkward if I'd like gotten my
kind of awkward if I'd like gotten my
tickets and everything.
tickets and everything.
But all right.
Oops. Yeah. Meeting in 10. Let's see
Oops. Yeah. Meeting in 10. Let's see
what I promised I will do I do with this
what I promised I will do I do with this
to make this get accepted. And we'll
to make this get accepted. And we'll
have to do that.
Yeah. So this is all true.
Submission articulates a toward force
Submission articulates a toward force
but largely elects not to provide direct
but largely elects not to provide direct
evidence.
evidence.
This true.
Okay. So, it's granularity of reporting.
The reason I didn't do this, by the way,
The reason I didn't do this, by the way,
and it's not a great reason, it's just
and it's not a great reason, it's just
cuz like it's always outdated in
cuz like it's always outdated in
instantly anyways. But like the best
instantly anyways. But like the best
benchmark numbers are going to be you
benchmark numbers are going to be you
write the 10line timing script and you
write the 10line timing script and you
time whatever environment you want and
time whatever environment you want and
you yell at me if it's like gotten
you yell at me if it's like gotten
slower somehow.
Missing so unclear results
buffer lib are easy. It's not easy to
buffer lib are easy. It's not easy to
implement. Simple to implement. It's not
implement. Simple to implement. It's not
easy to implement.
Definition of throughput is missing.
Emulation impact not negligible for
Emulation impact not negligible for
environments faster than a few thousand.
environments faster than a few thousand.
Gotten this to be better.
Okay. So, basically the complaints are
Okay. So, basically the complaints are
like, "Hey, where's the evidence for all
like, "Hey, where's the evidence for all
the stuff you claimed?" which is like
the stuff you claimed?" which is like
the most relevant of possible claims,
the most relevant of possible claims,
right? Except for the fact that I wasn't
right? Except for the fact that I wasn't
able to do this in the actual
able to do this in the actual
publication because there's there is a
publication because there's there is a
double blind requirement. I'm not
double blind requirement. I'm not
allowed to link to anything. But like
allowed to link to anything. But like
Puffer Lib is literally the most
Puffer Lib is literally the most
transparent invisible RL project out
transparent invisible RL project out
there other than maybe clean RL since
there other than maybe clean RL since
it's like simpler. Um,
so it's like the the evidence is kind of
so it's like the the evidence is kind of
based on the fact that you can just go
based on the fact that you can just go
get it, but since I wasn't able to link
get it, but since I wasn't able to link
to it, that was the main issue.
Let's see what we have.
Yeah.
So basically it's this note right here.
So basically it's this note right here.
So
contributions let's see how dated this
contributions let's see how dated this
is oneline rappers that make complex
is oneline rappers that make complex
environments like net hack nurl limo
environments like net hack nurl limo
gridly etc compatible with any RL
gridly etc compatible with any RL
library support standard impeding zoo
library support standard impeding zoo
formats context gymnas and petting zoo
formats context gymnas and petting zoo
are the most widely used environment
are the most widely used environment
formats however it's compatible with the
formats however it's compatible with the
vast majority of environments using only
vast majority of environments using only
a oneline wrapper drop in vectorization
a oneline wrapper drop in vectorization
for simulating environments in parallel
for simulating environments in parallel
most environments will see at least a
most environments will see at least a
30% speed boost and three 50% to 3x with
30% speed boost and three 50% to 3x with
pooling the broadly compatible
pooling the broadly compatible
contribution applicable to nearly all
contribution applicable to nearly all
environments. Context gymnasium provides
environments. Context gymnasium provides
the most characterization back end. It's
the most characterization back end. It's
slow for the reasons outlined in this
slow for the reasons outlined in this
paper.
paper.
Upper ocean. A suite of 12 environments
Upper ocean. A suite of 12 environments
written in C that each simulate at over
written in C that each simulate at over
a million steps per second on a single
a million steps per second on a single
CPU core. Context. A few of these have
CPU core. Context. A few of these have
built-in AI components that can slow
built-in AI components that can slow
performance when search depth is
performance when search depth is
increased. Base speed is greater than a
increased. Base speed is greater than a
million steps per second on a high-end
million steps per second on a high-end
desktop core. Some environments run at
desktop core. Some environments run at
10 million a second.
10 million a second.
for a PO demo that trains ocean
for a PO demo that trains ocean
environments at 300K to 1.2 million on a
environments at 300K to 1.2 million on a
single RTX 4090. Standard architectures
single RTX 4090. Standard architectures
are an MLP LSTM CNN and LTM 150K to a
are an MLP LSTM CNN and LTM 150K to a
million parameters on compatible with
million parameters on compatible with
all third party environment training is
all third party environment training is
up to 30K step second on Atari still 38
up to 30K step second on Atari still 38
times faster than the original clean RL.
times faster than the original clean RL.
So I think what we'll do I cannot revise
So I think what we'll do I cannot revise
this to be a puffer lip 3.0 paper
this to be a puffer lip 3.0 paper
because it's basically a full rewrite.
because it's basically a full rewrite.
Puffer Lib has changed so incredibly
Puffer Lib has changed so incredibly
much since 2.0 that it's like you can't
much since 2.0 that it's like you can't
submit one paper and then revise it to
submit one paper and then revise it to
be a completely different paper. I think
be a completely different paper. I think
even if it's strictly better plus that's
even if it's strictly better plus that's
just a lot of work to do anyways. So uh
just a lot of work to do anyways. So uh
I think what we'll do is we will add if
I think what we'll do is we will add if
I have an extra page.
I have an extra page.
Do I have an extra page?
Do I have an extra page?
Well, I will go check the conference
Well, I will go check the conference
requirements after because I got to go
requirements after because I got to go
in a minute here. But I'll check the
in a minute here. But I'll check the
page. What we'll just do is we will
page. What we'll just do is we will
proof this. We'll just proof this to
proof this. We'll just proof this to
make sure I'm still happy with the paper
make sure I'm still happy with the paper
as it's presented to make sure that
as it's presented to make sure that
there's nothing like inconsistent.
there's nothing like inconsistent.
And then I will add like a short section
And then I will add like a short section
like a paragraph or whatever on like
like a paragraph or whatever on like
puffer liib moves faster than the
puffer liib moves faster than the
conference cycle
conference cycle
at time of writing. This is actually
at time of writing. This is actually
puffer lilib 3 which has 25 environments
puffer lilib 3 which has 25 environments
is even faster and includes core
is even faster and includes core
algorithmic contributions that make uh
algorithmic contributions that make uh
that make it learn better than in puffer
that make it learn better than in puffer
2 with base po.
I think that's what we'll do. So we just
I think that's what we'll do. So we just
add one paragraph. I think that's
add one paragraph. I think that's
completely defensible. Um and then we
completely defensible. Um and then we
have the paper as it is for 2.
It's a little unfortunate because I
It's a little unfortunate because I
probably won't ever publish like a 300
probably won't ever publish like a 300
paper or whatever, but the thing is by
paper or whatever, but the thing is by
the time I would publish a 300 paper,
the time I would publish a 300 paper,
we'll have puffer 40. So,
we'll have puffer 40. So,
I'm kind of fine with that.
All right,
Mr. Q looks a lot like Musli.
Mr. Q looks a lot like Musli.
I'll go check this real quick.
This is like a garbage website
This is like a garbage website
thing.
What are the results on this?
Okay, this is interesting. I guess
Okay, this is interesting. I guess
reasonable param count.
Interesting.
Yeah, I got to read this for sure.
Did they say the wall clock time?
Did they disclose their training time?
Yeah. Okay. Well, I'll come back to
Yeah. Okay. Well, I'll come back to
this. So, for folks watching, thank you
this. So, for folks watching, thank you
for tuning in. I will be back after a
for tuning in. I will be back after a
couple of meetings
couple of meetings
most likely
most likely
because I got to finish this paper
because I got to finish this paper
today. So,
today. So,
if you want to help me out for free,
if you want to help me out for free,
start the repo. It's free. Really helps.
start the repo. It's free. Really helps.
Uh you can join the Discord to get
Uh you can join the Discord to get
involved. And if you like what you see,
involved. And if you like what you see,
follow me on X. That article will go
follow me on X. That article will go
live at 10K followers. Thank you.

Kind: captions
Language: en
Hello,
Hello,
we're back Five.
All right, back for another hour and a
All right, back for another hour and a
halfish. Little more.
Here's the plan. I'm going to go through
Here's the plan. I'm going to go through
the rest of the article.
the rest of the article.
Going to proof the rest of this. Maybe
Going to proof the rest of this. Maybe
grab images for it. Pretty much have
grab images for it. Pretty much have
them ready. And then uh I'm going to
them ready. And then uh I'm going to
spend hopefully I have an hour left
spend hopefully I have an hour left
after that to edit the uh the camera
after that to edit the uh the camera
ready.
I really don't care all that much about
I really don't care all that much about
the pub. Um,
the pub. Um,
I guess some people read it. I guess
I guess some people read it. I guess
I'll like make sure it's updated as best
I'll like make sure it's updated as best
as possible, but it's it's a whole
as possible, but it's it's a whole
generation outdated. So,
generation outdated. So,
yeah,
yeah,
let's just go through this out loud.
let's just go through this out loud.
We'll proof this. If anybody has any
We'll proof this. If anybody has any
suggestions, let me know
suggestions, let me know
and uh we will go from there. So I
and uh we will go from there. So I
already read through this section.
already read through this section.
So one, build a simple environment and
So one, build a simple environment and
train an agent.
train an agent.
Read the introduction to my RO quick
Read the introduction to my RO quick
start guide. Just the first paragraph
start guide. Just the first paragraph
for now.
for now.
You now know some
Yeah, you now know some words we use in
Yeah, you now know some words we use in
RL and not much else. Read the pufferlib
RL and not much else. Read the pufferlib
docs on writing your own environment
docs on writing your own environment
including all the link code on the
including all the link code on the
squared sample environment. So I have to
squared sample environment. So I have to
still update these docs a little bit.
still update these docs a little bit.
Map the terms from the introduction to
Map the terms from the introduction to
this code. Observations, actions,
this code. Observations, actions,
rewards, and terminals are just arrays.
rewards, and terminals are just arrays.
The observations are input the agent's
The observations are input the agent's
neural network which outputs actions.
neural network which outputs actions.
Rewards ter determine whether the agent
Rewards ter determine whether the agent
has reached its goal at which point the
has reached its goal at which point the
terminal value is set to true. You don't
terminal value is set to true. You don't
yet know how we use all these things to
yet know how we use all these things to
actually train.
you don't know how we use all these
you don't know how we use all these
things to actually train yet and that is
things to actually train yet and that is
fine.
fine.
Also, let me delete this bot
Also, let me delete this bot
bot message out of there.
Oops.
Oops.
All right.
Write your own puffer environment. Keep
Write your own puffer environment. Keep
it so simple.
And we're just going to say write your
And we're just going to say write your
own environment. So write your own
own environment. So write your own
environment. Keep it so simple as to not
environment. Keep it so simple as to not
even be useful. That will come next. If
even be useful. That will come next. If
you can't think of anything, do a
you can't think of anything, do a
stripped down version of Flappy Bird on
stripped down version of Flappy Bird on
a twob block tall grid. The agent can
a twob block tall grid. The agent can
move either up or down and observes
move either up or down and observes
whether there is a wall on the roof or
whether there is a wall on the roof or
ceiling above.
ceiling above.
Uh,
that didn't make any sense. Whether
that didn't make any sense. Whether
there's a wall on the roof or floor.
there's a wall on the roof or floor.
Negative one for hitting the ceiling.
Negative one for hitting the ceiling.
Zero otherwise. Bind it to Puffer Lib
Zero otherwise. Bind it to Puffer Lib
following the docks and train your first
following the docks and train your first
agent. Ask in Discord if you get stuck.
agent. Ask in Discord if you get stuck.
Let's link this
Let's link this
to
to
Poppy.
Two, learn the most basic fundamentals.
Two, learn the most basic fundamentals.
Read Carpathy's policy gradient blog,
Read Carpathy's policy gradient blog,
including the implementation. There are
including the implementation. There are
a few very minor details that are dated,
a few very minor details that are dated,
but it is otherwise still the best
but it is otherwise still the best
introduction to on policy methods. You
introduction to on policy methods. You
should now understand how policy
should now understand how policy
gradients and observations actions
gradients and observations actions
rewards into derivatives over weights.
rewards into derivatives over weights.
You have also seen discounted reward as
You have also seen discounted reward as
your first example of an advantage
your first example of an advantage
function. Understand that the discount
function. Understand that the discount
factor mathematically determines how
factor mathematically determines how
much you care about reward now versus
much you care about reward now versus
reward later. Read the fundamental
reward later. Read the fundamental
section of my RL quick start guide. Give
section of my RL quick start guide. Give
you context for the basic classes of
you context for the basic classes of
methods. Don't go down a rabbit hole of
methods. Don't go down a rabbit hole of
reading papers just yet. All modern
reading papers just yet. All modern
algorithms are at least slightly off
algorithms are at least slightly off
policy and the strict definition does
policy and the strict definition does
not matter as much anymore.
not matter as much anymore.
Read the multi- aent bullet point twice.
Read the multi- aent bullet point twice.
Read and train puffer target. This
Read and train puffer target. This
environment is included in the same
environment is included in the same
tutorial you already followed. It is
tutorial you already followed. It is
multi- aent and significantly more
multi- aent and significantly more
complex than squared. The agents no
complex than squared. The agents no
longer move on the grid and have to
longer move on the grid and have to
solve a more temporally extended
solve a more temporally extended
problem. Every agent sees a normalized
problem. Every agent sees a normalized
distance measure from itself to every
distance measure from itself to every
other agent and goal. Agents only
other agent and goal. Agents only
receive a sparse reward of one for
receive a sparse reward of one for
reaching their goal. Follow the docs. To
reaching their goal. Follow the docs. To
train an agent on this environment, it
train an agent on this environment, it
should only take a few seconds.
should only take a few seconds.
Three, build a slightly more complex
Three, build a slightly more complex
environment.
environment.
Make something about as complex as
Make something about as complex as
target. Max 300 lines. Doesn't have to
target. Max 300 lines. Doesn't have to
be multi- aent. If you did grid based
be multi- aent. If you did grid based
Flappy Bird before, do the full game.
Flappy Bird before, do the full game.
Any similarly scoped project should
Any similarly scoped project should
suffice. Think about what data the agent
suffice. Think about what data the agent
needs to see in order to play and make
needs to see in order to play and make
sure that it is included in the
sure that it is included in the
observations. Reread the debugging
observations. Reread the debugging
section of the quick start guide
section of the quick start guide
to avoid common errors and post on a
to avoid common errors and post on a
Discord if you get stuck.
Discord if you get stuck.
Is there a debugging section in my quick
Is there a debugging section in my quick
start guide or am I misremembering?
No, there isn't. It's the tutorial.
Reread the quick start
Reread the quick start
of the
of the
Reread the debugging section of the
Reread the debugging section of the
custom environment doc to avoid common
custom environment doc to avoid common
errors and post in our Discord if you
errors and post in our Discord if you
get stuck.
Aim to train an agent that you can
Aim to train an agent that you can
visually confirm is playing the game
visually confirm is playing the game
well. Read other puffer environments.
well. Read other puffer environments.
Snake and Convert are two slightly more
Snake and Convert are two slightly more
complex.
complex.
I'll put this caps are two slightly more
I'll put this caps are two slightly more
complex environments with pin code.
I'm going to do simple
clean code has the annoying like because
clean code has the annoying like because
of the stupid book, but it is it's just
of the stupid book, but it is it's just
clean code. It's not clean code from
clean code. It's not clean code from
that stupid book, but it's clean code.
that stupid book, but it's clean code.
Multi- dash agent.
Multi- dash agent.
So
So
I never know what to do with that man
I never know what to do with that man
because if you don't make it one word if
because if you don't make it one word if
you do multi-ash agent then you need to
you do multi-ash agent then you need to
do single dash agent for symmetry.
I usually just make it one word.
Like if we just Isn't this a thing?
Does everybody dash it? Am I just
Does everybody dash it? Am I just
stupid?
No, it's it's that it's auto correcting.
I guess most people do dash it.
I guess most people do dash it.
Annoying thing.
It looks super stupid to me written this
It looks super stupid to me written this
way, but whatever.
Also start looking at some of the arcade
Also start looking at some of the arcade
games like Pong and Breakout. By now,
games like Pong and Breakout. By now,
you've probably hit a few bugs and can
you've probably hit a few bugs and can
start to appreciate the ways in which
start to appreciate the ways in which
things can go wrong. We mitigate that by
things can go wrong. We mitigate that by
keeping code simple and minimally
keeping code simple and minimally
abstracted. The biggest mistake you can
abstracted. The biggest mistake you can
make in RL is to underestimate the price
make in RL is to underestimate the price
of complexity.
Enhance your environment. Add some
Enhance your environment. Add some
features that make the problem a little
features that make the problem a little
more interesting. Retrain with different
more interesting. Retrain with different
versions and see how your changes alter
versions and see how your changes alter
the agent's learning. Aim for something
the agent's learning. Aim for something
on the level of Pong or Snake.
If you get something visually
If you get something visually
interesting and not already in Pupper,
interesting and not already in Pupper,
come show us in PR it. Many simple
come show us in PR it. Many simple
environments end up being quite useful
environments end up being quite useful
in research.
in research.
Four, start understanding why that
Four, start understanding why that
worked. Read the core algorithm paper.
worked. Read the core algorithm paper.
It is dense, but the actual algorithm is
It is dense, but the actual algorithm is
simple. Read the bullet point on PO from
simple. Read the bullet point on PO from
my quick start guide first for some
my quick start guide first for some
intuition. Then you can read the
intuition. Then you can read the
proximal policy optimization PO paper.
proximal policy optimization PO paper.
You can ignore the TRPO and kale penalty
You can ignore the TRPO and kale penalty
sections. The main equation seven is
sections. The main equation seven is
just saying clip the policy gradient and
just saying clip the policy gradient and
weight it by the advantage function.
weight it by the advantage function.
Then average over a batch of data. The
Then average over a batch of data. The
advantage function is generalized
advantage function is generalized
advantage estimation.
advantage estimation.
If you have a strong enough math
If you have a strong enough math
background, read this now. Otherwise,
background, read this now. Otherwise,
put it off until
put it off until
section six
section
section
understand why I didn't have you
understand why I didn't have you
implement the algorithm. The common
implement the algorithm. The common
reference algorithms
reference algorithms
are extremely fiddly. Read Costa's 37
are extremely fiddly. Read Costa's 37
implementation details of PO and the
implementation details of PO and the
associated clean RLP PO implementation.
associated clean RLP PO implementation.
Not particularly long, but the details
Not particularly long, but the details
really matter. Read our other articles.
really matter. Read our other articles.
They are simpler than academic papers
They are simpler than academic papers
and tell you exactly how the tools
and tell you exactly how the tools
you've been using work.
Stronger hyperparameters with protein
Stronger hyperparameters with protein
buffing up PO Narama mode. Cool.
buffing up PO Narama mode. Cool.
Five. Your first real project. Finish
Five. Your first real project. Finish
reading my quick start guide. You will
reading my quick start guide. You will
not have context for all of it just yet,
not have context for all of it just yet,
but it contains a lot of useful
but it contains a lot of useful
perspective as you start to solve harder
perspective as you start to solve harder
problems. For example, depending on how
problems. For example, depending on how
hard your environment is.
hard your environment is.
Uh you may actually have to run
Uh you may actually have to run
for example, you may actually have to
for example, you may actually have to
run a hyperparameter sweep to get a good
run a hyperparameter sweep to get a good
policy.
Now this
fine. It's repetitive, but it's fine. If
fine. It's repetitive, but it's fine. If
you have a short horizon environment,
you have a short horizon environment,
you may need to at least decrease the
you may need to at least decrease the
discount rate. Do you understand why?
Let's actually do a little less.
Connect four. Maybe
if you implemented something like a card
if you implemented something like a card
game, you may need to at least you might
game, you may need to at least you might
need at least need to decrease the
need at least need to decrease the
discount rate. Do you understand why?
discount rate. Do you understand why?
Cool. Read the Pokemon RL blog post.
Cool. Read the Pokemon RL blog post.
This is a powered by Puffer trademark
This is a powered by Puffer trademark
copyright registered
We'll leave it like this. Project that
We'll leave it like this. Project that
beats the entire game using pure from
beats the entire game using pure from
scratch reinforcement learning.
scratch reinforcement learning.
The observation space formulation,
The observation space formulation,
reward engineering, and problem setup
reward engineering, and problem setup
are all informative.
are all informative.
Note that Pokemon Red is a thousand
Note that Pokemon Red is a thousand
times slower than most of our RL
times slower than most of our RL
environments. You will almost never
environments. You will almost never
actually have to do this level of
actually have to do this level of
engineering when you have faster
engineering when you have faster
simulators.
Pick an interesting problem. Aim for
Pick an interesting problem. Aim for
something you can do in 500 to a,000
something you can do in 500 to a,000
lines depending on your programming
lines depending on your programming
background. Several useful environments
background. Several useful environments
have been shorter than that. Arcade
have been shorter than that. Arcade
games are usually a good choice. Just
games are usually a good choice. Just
check in the Discord that nobody is
check in the Discord that nobody is
already doing the same one. If you have
already doing the same one. If you have
experience in another field, applied
experience in another field, applied
problems are even better. Good RL
problems are even better. Good RL
environments look like fiddly
environments look like fiddly
interactive optimization problems that
interactive optimization problems that
are quick to simulate and have clearly
are quick to simulate and have clearly
defined observations and actions. The
defined observations and actions. The
initial drone environment was only a few
initial drone environment was only a few
hundred lines. Solve in PR to puffer
hundred lines. Solve in PR to puffer
lab. It may seem self- serving, but this
lab. It may seem self- serving, but this
is genuinely the best way to learn. We
is genuinely the best way to learn. We
have an active community of researchers
have an active community of researchers
and hobbyists large enough that someone
and hobbyists large enough that someone
will nearly always be around to answer
will nearly always be around to answer
your questions. Many environments that
your questions. Many environments that
you wouldn't expect to be useful
you wouldn't expect to be useful
actually help us advance core RL
actually help us advance core RL
research.
research.
Six, do your homework. The list of
Six, do your homework. The list of
important papers to read in
important papers to read in
reinforcement learning is quite short.
reinforcement learning is quite short.
See my programming and ML advice article
See my programming and ML advice article
and my quick start guide for perspective
and my quick start guide for perspective
on why this is the case.
on why this is the case.
I need to link
I need to link
these. Right,
let's do that real quick.
Oh, actually we can link the uh the X
copy the link.
and then we can't link the ML article
and then we can't link the ML article
until it's published. So that'll be the
until it's published. So that'll be the
final tweak.
These are the top 10 papers you should
These are the top 10 papers you should
read regardless of what you want to do
read regardless of what you want to do
next. This list is intentionally not a
next. This list is intentionally not a
historical account of mostly broken
historical account of mostly broken
algorithms. My focus is on the major
algorithms. My focus is on the major
capabilities defining results, the
capabilities defining results, the
commonalities among them. Note that
commonalities among them. Note that
almost all of the OpenAI and deep mind
almost all of the OpenAI and deep mind
results have associated blog posts that
results have associated blog posts that
are more accessible than the formal
are more accessible than the formal
manuscripts. You can start here, but you
manuscripts. You can start here, but you
should also read the full papers if you
should also read the full papers if you
want to do research. We have a couple
want to do research. We have a couple
papers on puffer and neural MMO that you
papers on puffer and neural MMO that you
can read if you want. You're better off
can read if you want. You're better off
just reading our blog docs and the
just reading our blog docs and the
source code.
Dota 2 with deep uh with large scale
Dota 2 with deep uh with large scale
deep reinforcement learning. My pick for
deep reinforcement learning. My pick for
the most important paper in the field
the most important paper in the field
with a single layer LSTM solves Dota.
with a single layer LSTM solves Dota.
One of the biggest errors you can make
One of the biggest errors you can make
is undervalue this result.
Go play the game if you need convincing
Go play the game if you need convincing
that this problem is meaningfully hard.
that this problem is meaningfully hard.
Do not skip the appendex. Grandmaster
Do not skip the appendex. Grandmaster
level in Starcraft 2 using multi- aent
level in Starcraft 2 using multi- aent
reinforcement learning.
reinforcement learning.
Actually,
I've seen many top researchers
I've seen many top researchers
undervalue this result and waste their
undervalue this result and waste their
time developing fancy methods that solve
time developing fancy methods that solve
trivial solve trivial problems.
trivial solve trivial problems.
This is true.
Go play the game if you need convincing
Go play the game if you need convincing
that this problem is meaningfully hard.
that this problem is meaningfully hard.
Do not skip the appendex.
Do not skip the appendex.
Grandmaster level in Starcraft 2 using
Grandmaster level in Starcraft 2 using
multi-agent reinforcement learning.
multi-agent reinforcement learning.
Another extremely hard problem solved
Another extremely hard problem solved
with RL. It's number two because
with RL. It's number two because
DeepMind bootstrapped with imitation
DeepMind bootstrapped with imitation
learning use significantly more
learning use significantly more
complicated methods. They tend to do
complicated methods. They tend to do
this often and I am not convinced it is
this often and I am not convinced it is
required.
required.
Mastering the game of go with deep
Mastering the game of go with deep
neural networks and tree search. This is
neural networks and tree search. This is
a landmark historical result that
a landmark historical result that
kickstarted the field. You could
kickstarted the field. You could
arguably place it higher reasoning about
Uh, you could arguably place it higher.
Uh, you could arguably place it higher.
Reasoning about comparative complexity
Reasoning about comparative complexity
of Go is hard. The naive branching
of Go is hard. The naive branching
factor argument breaks when you look at
factor argument breaks when you look at
games like Dota or SC2.
games like Dota or SC2.
Learning dextrous inhand manipulation.
Learning dextrous inhand manipulation.
Learning to manipulate a Rubik's cube
Learning to manipulate a Rubik's cube
with reinforcement learning
with a robot handforcement.
with a robot handforcement.
Kind of jumped the gun there.
Kind of jumped the gun there.
This project pioneered domain
This project pioneered domain
randomization training on a ton of
randomization training on a ton of
slightly different problems to improve
slightly different problems to improve
generalization and robustness.
generalization and robustness.
Open-ended learning leads to generally
Open-ended learning leads to generally
capable agents, also known as XLAND.
capable agents, also known as XLAND.
Heavy domain randomization allows agents
Heavy domain randomization allows agents
to generalize to new tasks not seen
to generalize to new tasks not seen
during training.
You do this as randomization over
Randomization over training tasks allows
Randomization over training tasks allows
agents to generalize new tasks not seen
agents to generalize new tasks not seen
during training. There a bunch of
during training. There a bunch of
conflating variables here with method
conflating variables here with method
changes, but the end result is a pretty
changes, but the end result is a pretty
compelling argument that RL can
compelling argument that RL can
generalize.
Emergent tool use for multi- aent
Emergent tool use for multi- aent
autocurricula. The 3v3 hideand-seek with
autocurricula. The 3v3 hideand-seek with
movable objects. A pure version of
movable objects. A pure version of
techniques used in Dota applied to a
techniques used in Dota applied to a
simpler but still complex task. Human
simpler but still complex task. Human
level performance in firstp person
level performance in firstp person
multiplayer games with population based
multiplayer games with population based
deep reinforcement learning. 3v3 capture
deep reinforcement learning. 3v3 capture
the flag with FPS mechanics based on the
the flag with FPS mechanics based on the
associated quick game mode.
The net hack learning environment a
The net hack learning environment a
really hard environment for
really hard environment for
reinforcement learning. Probably can't
reinforcement learning. Probably can't
solve it with a general method without
solve it with a general method without
also solving AI in the process. We call
also solving AI in the process. We call
that AI complete.
that AI complete.
We I call that AI complete. I don't know
We I call that AI complete. I don't know
if anybody else uses that.
Proximal policy optimization the core
Proximal policy optimization the core
algorithm that is the base most modern
algorithm that is the base most modern
RL built our puffer lil 3 algorithm as a
RL built our puffer lil 3 algorithm as a
set of enhancements to
set of enhancements to
highdimensional continuous control using
highdimensional continuous control using
generalized advantage estimation
generalized advantage estimation
advantage function that is half the
advantage function that is half the
reason PO works one of our 3PO uh 3.0 0
reason PO works one of our 3PO uh 3.0 0
experiments was to combine V GE with VRE
experiments was to combine V GE with VRE
playing Atari with deep reinforcement
playing Atari with deep reinforcement
learning, the original deep Q learning
learning, the original deep Q learning
paper that spawned the field of modern
paper that spawned the field of modern
RL.
RL.
Cool.
Cool.
This is the new section I wrote this
This is the new section I wrote this
morning. We'll see if I actually like it
morning. We'll see if I actually like it
a couple hours later here. My best
a couple hours later here. My best
advice in one place.
advice in one place.
How to approach a new problem. Start
How to approach a new problem. Start
from first principle. The agent is
from first principle. The agent is
learning tabularasa. It's a blank slate
learning tabularasa. It's a blank slate
at the start of training. It's looking
at the start of training. It's looking
for signal by mashing buttons. It also
for signal by mashing buttons. It also
can't see. Imagine the environment has
can't see. Imagine the environment has
the graphics randomized. Some of reward
the graphics randomized. Some of reward
has to be obtainable this way. To
has to be obtainable this way. To
actually learn from this reward, the
actually learn from this reward, the
agent needs to be able to see. What
agent needs to be able to see. What
information does the agent need to solve
information does the agent need to solve
the problem? Make that the observation
the problem? Make that the observation
space for action. What information would
space for action. What information would
tell you if your agent is working
tell you if your agent is working
correctly? Log that. Almost always want
correctly? Log that. Almost always want
a single real number metric of overall
a single real number metric of overall
performance. Don't only log raw reward
performance. Don't only log raw reward
because you will probably tune the scale
because you will probably tune the scale
of this number and make results
of this number and make results
incomparable. Log a score instead. For
incomparable. Log a score instead. For
example, we might give agent a reward of
example, we might give agent a reward of
0.25 or 0.5 for breaking a brick, but we
0.25 or 0.5 for breaking a brick, but we
log the actual number of points
log the actual number of points
obtained.
obtained.
Bonus points if you can scale it to the
Bonus points if you can scale it to the
range of 0 to one. Log any extra data
range of 0 to one. Log any extra data
you will actually look at, but don't log
you will actually look at, but don't log
a ton of stuff you won't use. Good
a ton of stuff you won't use. Good
candidates include collision rates, out
candidates include collision rates, out
of bounds, etc. Since these are sanities
that should drop to near zero for
that should drop to near zero for
applicable problems, right? The simplest
applicable problems, right? The simplest
possible environment that is still fast.
possible environment that is still fast.
Don't abstract anything. That's
Don't abstract anything. That's
important enough to say twice. Don't
important enough to say twice. Don't
abstract anything. Start training early
abstract anything. Start training early
and frequently. Want iteration speed to
and frequently. Want iteration speed to
be as fast as possible.
Hey, welcome Alan.
That's important enough to say twice.
That's important enough to say twice.
Don't abstract anything. Start training
Don't abstract anything. Start training
early and frequently. Want iteration
early and frequently. Want iteration
speed to be as fast as possible. Seconds
speed to be as fast as possible. Seconds
is better than minutes. You've lost if
is better than minutes. You've lost if
it is hours. If training does not work,
it is hours. If training does not work,
suspect your data. Make your environment
suspect your data. Make your environment
playable. Is what is happening sensible?
What you see happening sensible? Do you
What you see happening sensible? Do you
see reward being assigned when you
see reward being assigned when you
expect it to?
expect it to?
Run an evaluation on a checkpoint. See
Run an evaluation on a checkpoint. See
if agents have found some degenerate
if agents have found some degenerate
unreoverable stage. For harder problems,
unreoverable stage. For harder problems,
scale up slowly and don't make a ton of
scale up slowly and don't make a ton of
changes without training a decent model
changes without training a decent model
on the latest version. Don't experiment
on the latest version. Don't experiment
with new algorithms on new environments
with new algorithms on new environments
that are constantly changing. Do your
that are constantly changing. Do your
research on stable problems, then try it
research on stable problems, then try it
on the new environment.
How to encode data? Normalize the
How to encode data? Normalize the
observations appropriately. You want
observations appropriately. You want
position data to be egocentric when
position data to be egocentric when
possible. Do this by subtracting the
possible. Do this by subtracting the
agent's position, dividing by the
agent's position, dividing by the
maximum value. Street data can't just go
maximum value. Street data can't just go
into the model without encoding.
into the model without encoding.
If you represent knight, king, queen,
If you represent knight, king, queen,
pawn, rook as 0 1 2 3 4. Putting that
pawn, rook as 0 1 2 3 4. Putting that
into a model raw implies knight is more
into a model raw implies knight is more
similar to king than it is to rook. It
similar to king than it is to rook. It
also forces the model to learn a wonky
also forces the model to learn a wonky
decision boundary in latent space. one
decision boundary in latent space. one
hot encode knight as 10 o instead. You
hot encode knight as 10 o instead. You
can do this in the environment for very
can do this in the environment for very
small values, but do it in the policy
small values, but do it in the policy
for larger values to save bandwidth.
for larger values to save bandwidth.
Actions should be the simplest possible
Actions should be the simplest possible
set of controls for your environment. I
set of controls for your environment. I
like to imagine the environment being
like to imagine the environment being
released for Game Boy and designing the
released for Game Boy and designing the
controls to match. Neural 3 features
controls to match. Neural 3 features
exploration combat equipment progression
exploration combat equipment progression
on a live market. The action space is a
on a live market. The action space is a
single discreet and the game is playable
single discreet and the game is playable
with keyboard only. Don't blow up the
with keyboard only. Don't blow up the
size of the space to accomplish this.
size of the space to accomplish this.
Okay, so this is super dense, right? But
Okay, so this is super dense, right? But
even though this is ridiculously dense,
even though this is ridiculously dense,
this is pretty much
this is pretty much
like this is like a non-negligible
like this is like a non-negligible
fraction of all the intuition you need
fraction of all the intuition you need
for reinforcement learning in a page and
for reinforcement learning in a page and
a half. So I think that that's worth I
a half. So I think that that's worth I
think it's worth doing something like
think it's worth doing something like
this. I haven't written anything like
this. I haven't written anything like
this before, so we'll see what people
this before, so we'll see what people
think about it. But this is kind of my
think about it. But this is kind of my
best advice in one spot.
Is doing what you're supposed to do.
Is doing what you're supposed to do.
Roll it. Hey, welcome Shadow.
Is there anything in here that is
Is there anything in here that is
straight up confusing or doesn't make
straight up confusing or doesn't make
sense to any of you watching? Please
sense to any of you watching? Please
tell me now so that I can make edits or
tell me now so that I can make edits or
clarify things or what have you.
Also, welcome YouTube folks. Ah, cool.
Also, welcome YouTube folks. Ah, cool.
Quite a few people today. It's funny
Quite a few people today. It's funny
that we have the same number of people
that we have the same number of people
watching me literally write an article
watching me literally write an article
as we do actually doing RL code, but uh
as we do actually doing RL code, but uh
yeah, welcome. This is the beginners's
yeah, welcome. This is the beginners's
guide. I guess it's beginners all the
guide. I guess it's beginners all the
way up to state-of-the-art, so it's it's
way up to state-of-the-art, so it's it's
not just like the basics. Um I'm doing
not just like the basics. Um I'm doing
this mostly out of request. People have
this mostly out of request. People have
requested this repeatedly. There's also
requested this repeatedly. There's also
a a companion article that starts with
a a companion article that starts with
programming in ML for like really basic
programming in ML for like really basic
stuff.
the full journey.
the full journey.
Yeah. I mean, because basically the
Yeah. I mean, because basically the
thing is like the way that I got into
thing is like the way that I got into
research was pretty self-directed and it
research was pretty self-directed and it
was like very meandering and
was like very meandering and
independent. I kind of just tried stuff,
independent. I kind of just tried stuff,
read a bunch of papers, tried other
read a bunch of papers, tried other
things, started doing research, which is
things, started doing research, which is
literally doing research is just like
literally doing research is just like
read a paper, like read a couple of
read a paper, like read a couple of
papers, get some ideas on how they could
papers, get some ideas on how they could
improve, try to improve them. if it
improve, try to improve them. if it
actually does improve like write
actually does improve like write
something about it. Try to publish it.
something about it. Try to publish it.
Do that in the loop until you get better
Do that in the loop until you get better
at it and you get better ideas.
at it and you get better ideas.
So this will be what you can point to
So this will be what you can point to
when new folks come in. Yes, this is
when new folks come in. Yes, this is
going to be like the overarching like
going to be like the overarching like
this points to all my other articles and
this points to all my other articles and
has a basically stepbystep guide on this
has a basically stepbystep guide on this
is how you learn reinforcement learning.
is how you learn reinforcement learning.
Do this, read this, read this, do that,
Do this, read this, read this, do that,
do this, read this, do that. Like now
do this, read this, do that. Like now
you know RL.
you know RL.
Will you publish it today? No. I think
Will you publish it today? No. I think
what I'm going to do with this because I
what I'm going to do with this because I
have to Well, I'll finish editing it. I
have to Well, I'll finish editing it. I
got to get images for it and then
got to get images for it and then
probably I'll make a post that this will
probably I'll make a post that this will
go live at 10K followers because I think
go live at 10K followers because I think
that would just be a fun thing to do.
that would just be a fun thing to do.
And we're basically like we're almost at
And we're basically like we're almost at
10K anyways.
I think that's what we'll do. 10K on um
I think that's what we'll do. 10K on um
on X, not YouTube, obviously.
All right, let's finish reading through
All right, let's finish reading through
this and if anybody has ideas or
this and if anybody has ideas or
questions or things on any of this,
questions or things on any of this,
please speak now so that I can make the
please speak now so that I can make the
edits before it's live.
Advanced topic applications. You have
Advanced topic applications. You have
deep knowledge of another field. Use it
deep knowledge of another field. Use it
to look for problems where RL can help.
to look for problems where RL can help.
These tend to have the same feel. These
These tend to have the same feel. These
all tend uh these tend to these
all tend uh these tend to these
Let's not split my infinitives.
Let's not split my infinitives.
These tend or these all tend to have
These tend or these all tend to have
English. These all tend to have the same
English. These all tend to have the same
feel. RL works when you have a fiddly
feel. RL works when you have a fiddly
interactive optimization process and
interactive optimization process and
build a fast sim. To give you some
build a fast sim. To give you some
ideas, we're currently doing open source
ideas, we're currently doing open source
work on drones and logistics. Both are
work on drones and logistics. Both are
areas where we can easily build
areas where we can easily build
simulators that run millions of steps
simulators that run millions of steps
per second and a few hundred lines of
per second and a few hundred lines of
basic safe multi- aent multitasks
basic safe multi- aent multitasks
stuff are going to be a lot better with
stuff are going to be a lot better with
images.
images.
Uh you are correct that I did not
Uh you are correct that I did not
include any images in this at all.
The funny thing with multi-agent and
The funny thing with multi-agent and
multitask shadow is it doesn't change
multitask shadow is it doesn't change
the architecture at all.
the architecture at all.
Like it literally doesn't change the
Like it literally doesn't change the
architecture one bit.
Is that news?
Is that news?
Like, is that surprising to you? I have
Like, is that surprising to you? I have
a bullet point on this, but it's like,
a bullet point on this, but it's like,
yeah, it like people make multi-agent
yeah, it like people make multi-agent
out to be way harder than it is. My PhD
out to be way harder than it is. My PhD
is on multi-agent. It's literally the
is on multi-agent. It's literally the
same thing as single agent.
same thing as single agent.
Separate encoding idea.
Separate encoding idea.
little confused. I'm trying to solve a
little confused. I'm trying to solve a
Rubik's cube with RL since I met you on
Rubik's cube with RL since I met you on
Twitter. It's been a fun way to go. Ah,
Twitter. It's been a fun way to go. Ah,
cool. Yeah, we had a couple people like
cool. Yeah, we had a couple people like
mess with that and nobody ever actually
mess with that and nobody ever actually
finished that project. So, uh, yeah, if
finished that project. So, uh, yeah, if
you want to do that and as an end, we'd
you want to do that and as an end, we'd
very happily take a Rubik's Cube
very happily take a Rubik's Cube
environment PR.
environment PR.
It's probably like you can probably
It's probably like you can probably
write a fun 3D renderer for it very
write a fun 3D renderer for it very
easily as well. Like you could probably
easily as well. Like you could probably
even make one with like the rotation
even make one with like the rotation
animations and it would probably be
animations and it would probably be
maybe 200 lines of Ray lip code.
maybe 200 lines of Ray lip code.
Without the animations it would
Without the animations it would
literally be like 20 or 30 lines of RIP
literally be like 20 or 30 lines of RIP
code. Probably maybe 50.
code. Probably maybe 50.
The separate encodings idea is a little
The separate encodings idea is a little
confusing.
Separate encodings idea.
Separate encodings idea.
Do you mean is it a actually I want to
Do you mean is it a actually I want to
understand this. So shadow is it
understand this. So shadow is it
specifically something about this that's
specifically something about this that's
confusing because nothing that I wrote
confusing because nothing that I wrote
here has anything to do with multi- aent
or are you talking about like entity
or are you talking about like entity
encodings
So, uh, here's the funny thing with
So, uh, here's the funny thing with
entity encodings.
entity encodings.
Lately,
Lately,
I've literally just been like putting
I've literally just been like putting
them into the observations flat without
them into the observations flat without
an entity encoder and just maybe sorting
an entity encoder and just maybe sorting
them by distance.
them by distance.
And this kind of works just as well.
And this kind of works just as well.
The problem with entity encoders,
The problem with entity encoders,
in order to do entity encoders fast, you
in order to do entity encoders fast, you
need custom CUDA.
need custom CUDA.
And we have not actually gone through
And we have not actually gone through
the effort of doing that because it's
the effort of doing that because it's
it's the type of thing that would
it's the type of thing that would
probably take me several days
probably take me several days
mostly because of the annoying bindings.
Imagine you have image and vector obs.
Imagine you have image and vector obs.
Yes. So you'd have like a vector per
Yes. So you'd have like a vector per
agent, right?
Yeah, I know what you mean by entity
Yeah, I know what you mean by entity
encodings with that stuff. Um, it's like
encodings with that stuff. Um, it's like
pointwise kind of or entity wise.
You'll get that from the papers to be
You'll get that from the papers to be
fair that I've suggested
fair that I've suggested
and it's a niche thing, but like I don't
and it's a niche thing, but like I don't
know if I even want to recommend it
know if I even want to recommend it
generally because the thing is like it's
generally because the thing is like it's
actually very slow if you do it naively.
actually very slow if you do it naively.
like it's really slow.
Probably the biggest problem we have in
Probably the biggest problem we have in
um the driving sim right now, right? The
um the driving sim right now, right? The
biggest problem we have in the driving
biggest problem we have in the driving
sim is that there's this really slow
sim is that there's this really slow
point-wise encoder
point-wise encoder
and uh without custom CUDA, it's just
and uh without custom CUDA, it's just
there's no way to make it fast.
there's no way to make it fast.
Keep reading through this.
Keep reading through this.
Longer term, we are most interested in
Longer term, we are most interested in
applying RL to hard science and
applying RL to hard science and
manufacturing. Are there areas of
manufacturing. Are there areas of
molecular simulation where we can cut
molecular simulation where we can cut
corners on fidelity? Are there
corners on fidelity? Are there
constrained areas of robotics where the
constrained areas of robotics where the
current simulators are massive overkill?
current simulators are massive overkill?
What about in the production of goods?
What about in the production of goods?
There are likely problems where nobody
There are likely problems where nobody
has even thought of building a sim. As a
has even thought of building a sim. As a
quick plug, if you want to join us on
quick plug, if you want to join us on
the business side, this is the best way.
the business side, this is the best way.
If you can build a prototype and we can
If you can build a prototype and we can
find a client, we'll build you a board
find a client, we'll build you a board
for the contract.
Build a prototype and we can find a
Build a prototype and we can find a
client, we'll you aboard for the
client, we'll you aboard for the
contract.
contract.
Here
are a few successful applications and I
are a few successful applications and I
link the
link the
fusion
fusion
driving with lift commercial cooling
driving with lift commercial cooling
systems self-driving and uh
Cool.
Advanced topic algorithms.
Advanced topic algorithms.
You need a good enough math background
You need a good enough math background
to be able to understand publications in
to be able to understand publications in
this area. A good enough programming
this area. A good enough programming
background to run your own experiments.
background to run your own experiments.
Expect to read a larger number of papers
Expect to read a larger number of papers
critically. Most published results are
critically. Most published results are
uh
uh
more publish uh results are wrong in RL
more publish uh results are wrong in RL
than in any other uh than in other areas
than in any other uh than in other areas
of AI
of AI
even the ones with strong evidence. Here
even the ones with strong evidence. Here
are a variety of research areas with
are a variety of research areas with
important open questions and some of the
important open questions and some of the
relevant papers off policy learning
relevant papers off policy learning
during the peak 2019 golden era of RL.
during the peak 2019 golden era of RL.
OpenAI mostly used on policy methods
OpenAI mostly used on policy methods
while deep mind used off policy ones.
while deep mind used off policy ones.
Dota and emergent tool use are clear-cut
Dota and emergent tool use are clear-cut
confirmations of on policy methods
confirmations of on policy methods
working in relatively clean settings.
working in relatively clean settings.
We don't have the same thing for off
We don't have the same thing for off
policy methods because the main mind
policy methods because the main mind
results tend to have more moving parts.
results tend to have more moving parts.
On policy methods work great and fast
On policy methods work great and fast
with unlimited data.
with unlimited data.
On policy methods work great and
on policy methods work great and are
on policy methods work great and are
fast with unlimited data. Off policy
fast with unlimited data. Off policy
methods are sometimes favored in data
methods are sometimes favored in data
poor settings. Can we get the best of
poor settings. Can we get the best of
both? Here are some papers we're looking
both? Here are some papers we're looking
at. Got rainbow the first usable off
at. Got rainbow the first usable off
policy method to compute. It's a kitchen
policy method to compute. It's a kitchen
sink disaster of tricks bolted onto DQN.
sink disaster of tricks bolted onto DQN.
That's not an exaggeration.
That's not an exaggeration.
Beyond the Rainbow. This paper hasn't
Beyond the Rainbow. This paper hasn't
gotten any attention yet, but it is a
gotten any attention yet, but it is a
good follow-up to Rainbow that balances
good follow-up to Rainbow that balances
sample efficiency for relatively slow
sample efficiency for relatively slow
sims with decent wall clock training
sims with decent wall clock training
time. I rather like this paper.
time. I rather like this paper.
Human level Atari 200 times faster. It's
Human level Atari 200 times faster. It's
only 200 times faster if you count
only 200 times faster if you count
environment steps uh and treat computers
environment steps uh and treat computers
as free.
It's only 2005. If you're counting
It's only 2005. If you're counting
environment steps and treat
you're counting environment steps and
you're counting environment steps and
treating computers free the results
treating computers free the results
still holds though so there's probably
still holds though so there's probably
something here. Hala introduces vrace to
something here. Hala introduces vrace to
correct off policy drift. We tried this
correct off policy drift. We tried this
in puff for lilib 3.0. It was worse uh
in puff for lilib 3.0. It was worse uh
it was just worse than J. It did help a
it was just worse than J. It did help a
little overall when combined with J but
little overall when combined with J but
not specifically for off policy drift.
model based learning. Can you compress
model based learning. Can you compress
transitions?
Uh this is not true actually. We'll just
Uh this is not true actually. We'll just
do it like this. Model based learning.
do it like this. Model based learning.
Can you compress transitions by
Can you compress transitions by
predicting the next state?
predicting the next state?
Good enough math background. Is that
Good enough math background. Is that
undergrad grad level just thinking that
undergrad grad level just thinking that
they am rigorous or you're saying go
they am rigorous or you're saying go
check the papers and see math? Where do
check the papers and see math? Where do
I say good enough math?
You need a good enough math background.
Ah, so that is from my other guide.
So the criteria I give you here for
So the criteria I give you here for
having a good enough background. It's
having a good enough background. It's
from the intro.
from the intro.
You can see what you think of this.
basic deep learn deep learning knowledge
basic deep learn deep learning knowledge
at the level of Stanford CS231N.
at the level of Stanford CS231N.
So if you've gone through 231N and you
So if you've gone through 231N and you
understand that then you're probably
understand that then you're probably
good enough with the math. And then on
good enough with the math. And then on
the programming side it's can you
the programming side it's can you
implement an LSTM without importing
implement an LSTM without importing
anything?
anything?
Also, if the implementation you thought
Also, if the implementation you thought
of was over a 100 lines, probably should
of was over a 100 lines, probably should
look at the programming article.
I'll put a little parenthetical in here.
I'll put a little parenthetical in here.
You need a good enough math background
You need a good enough math background
231N to be able to understand
231N to be able to understand
publications in this area and a good
publications in this area and a good
enough programming background to run
enough programming background to run
your own experiments.
your own experiments.
Bet to read a larger number of papers
Bet to read a larger number of papers
critical uh critically. More published
critical uh critically. More published
results are wrong in RL than in other
results are wrong in RL than in other
areas of AI, even the ones with strong
areas of AI, even the ones with strong
evidence.
evidence.
All right, cool. I'd already read all
All right, cool. I'd already read all
this stuff.
this stuff.
Uh model based learning. Can you
Uh model based learning. Can you
compress transitions by predicting the
compress transitions by predicting the
next state? Various methods attempt to
next state? Various methods attempt to
do this and then learn within
do this and then learn within
hallucinated data generated by the
hallucinated data generated by the
model. Others attempt to just use the
model. Others attempt to just use the
model as an auxiliary loss. Science in
model as an auxiliary loss. Science in
this area is iffy and I don't trust the
this area is iffy and I don't trust the
reasoning behind any of the published
reasoning behind any of the published
results. There is very likely something
results. There is very likely something
here though possibly is an alternative
here though possibly is an alternative
to off policy resampling.
to off policy resampling.
Recurrent world models facilitate policy
Recurrent world models facilitate policy
evolution original world models paper
evolution original world models paper
mastering yada yada this probably the
mastering yada yada this probably the
best known paper in the space dreamer v3
best known paper in the space dreamer v3
this somehow scales to 200 mil
this somehow scales to 200 mil
parameters which usually doesn't work in
parameters which usually doesn't work in
RL and then our paper uh showing that
RL and then our paper uh showing that
the tricks introduced in dreamer v3
the tricks introduced in dreamer v3
don't work according to the reasoning
don't work according to the reasoning
given. Yeah, this was one of the papers
given. Yeah, this was one of the papers
that really burned me the most actually
that really burned me the most actually
I would say because it's like really
I would say because it's like really
well-known paper, really well-known
well-known paper, really well-known
author and uh the result is not wrong as
author and uh the result is not wrong as
far as I can tell and like people have
far as I can tell and like people have
done stuff with dreamer but the code is
done stuff with dreamer but the code is
a disaster and uh none of the reasoning
a disaster and uh none of the reasoning
given in the paper actually holds up to
given in the paper actually holds up to
ablations
like at all
like at all
search. Can you explore or simulate
search. Can you explore or simulate
chains of different possible outcomes?
chains of different possible outcomes?
Improve learning and/or test time
Improve learning and/or test time
performance by leveraging more compute.
performance by leveraging more compute.
Worked in several deep mind projects
Worked in several deep mind projects
with and without access to simulator
with and without access to simulator
dynamics, but has not really been used
dynamics, but has not really been used
more broadly. Mastering Atari,
I don't think yeah, I'm pretty sure it's
I don't think yeah, I'm pretty sure it's
not really been used more broadly. It's
not really been used more broadly. It's
mostly Deep Mind doing this style of
mostly Deep Mind doing this style of
stuff.
stuff.
Mastering Atari Go, chess, and shogi by
Mastering Atari Go, chess, and shogi by
planning with a learned
planning with a learned
uh model. Vio removes the requirement
uh model. Vio removes the requirement
from AlphaGo that you need to be able to
from AlphaGo that you need to be able to
set the simulator state. Go Explorer
set the simulator state. Go Explorer
uses a setable sim to gimmick very hard
uses a setable sim to gimmick very hard
exploration problems unreasonably well.
exploration problems unreasonably well.
To be fair, if set if setable state is
To be fair, if set if setable state is
useful, we can build it into almost
useful, we can build it into almost
every type of sim. Uh, efficient zero
every type of sim. Uh, efficient zero
build built on mu0 claims to beat
build built on mu0 claims to beat
dreamer v3
advanced topics infrastructure. I'm
advanced topics infrastructure. I'm
writing this in the context of puffer
writing this in the context of puffer
lives infrastructure since we
lives infrastructure since we
unambiguously lead here. That is a true
unambiguously lead here. That is a true
statement. There is just no other good
statement. There is just no other good
infrastructure. Our models train at 3 to
infrastructure. Our models train at 3 to
5 million steps per second in pietorrch
5 million steps per second in pietorrch
eager mode. Napkin math says that we
eager mode. Napkin math says that we
should be able to double this
should be able to double this
performance on our larger models and 10x
performance on our larger models and 10x
it on smaller models. Everything has to
it on smaller models. Everything has to
get faster to keep up environment speed.
get faster to keep up environment speed.
Read our endbinding.h file. It is
Read our endbinding.h file. It is
responsible for defining those magic vec
responsible for defining those magic vec
reset and vec step methods except that
reset and vec step methods except that
there is no magic. We're just creating n
there is no magic. We're just creating n
copies of your environment and running
copies of your environment and running
them in a loop. The magic logging method
them in a loop. The magic logging method
is just iterating through fields of your
is just iterating through fields of your
log strct and dividing by log n.
log strct and dividing by log n.
We have this set up for you because
We have this set up for you because
Python to C data transfer is tedious and
Python to C data transfer is tedious and
errorprone and the boiler plate is
errorprone and the boiler plate is
roughly the same for every environment.
roughly the same for every environment.
It is set up so that it is fairly easy
It is set up so that it is fairly easy
to get access to individual environments
to get access to individual environments
from Python if you need it as well. We
from Python if you need it as well. We
would like to build in setable
would like to build in setable
simulation state but we've not thought
simulation state but we've not thought
about it too much. The point is there's
about it too much. The point is there's
pretty much this is pretty much the
pretty much this is pretty much the
lowest overhead thing we can do. Data
lowest overhead thing we can do. Data
does not have to be copied between C and
does not have to be copied between C and
Python during step. Both have access to
Python during step. Both have access to
the same data pointers. Next step in
the same data pointers. Next step in
optimization here would be figuring out
optimization here would be figuring out
general approaches to caching and subd
general approaches to caching and subd
parallelization.
parallelization.
We have a Python implementation of N4.
uh I don't really need to say that
uh I don't really need to say that
Python implementation eventful as of 3.0
Python implementation eventful as of 3.0
round robin and way buffered. By
round robin and way buffered. By
default, all buffers are allocated in
default, all buffers are allocated in
shared memory. Each environment is given
shared memory. Each environment is given
a pointer to a block of memory. The C
a pointer to a block of memory. The C
process writes directly to this memory,
process writes directly to this memory,
making the entire batch available to the
making the entire batch available to the
main thread immediately without extra
main thread immediately without extra
copies, but already almost as fast as it
copies, but already almost as fast as it
possibly could be. It may be possible to
possibly could be. It may be possible to
buffer the actual CPU to GPU data
buffer the actual CPU to GPU data
transfer because this is a big chunk of
transfer because this is a big chunk of
over overhead on fast environments with
over overhead on fast environments with
large observations,
large observations,
models, and training. This is where the
models, and training. This is where the
biggest opportunity is. Very few common
biggest opportunity is. Very few common
operations like entity embedding where
operations like entity embedding where
we don't have custom kernels stuff needs
we don't have custom kernels stuff needs
to be
to be
or fused general and we haven't gotten
or fused general and we haven't gotten
much out of compile
compile or low precision. To be fair, we
compile or low precision. To be fair, we
haven't even tried to tune BF-16 on A00s
haven't even tried to tune BF-16 on A00s
or H00s and BF16 is nerfed on consumer
or H00s and BF16 is nerfed on consumer
cards. Should be computing model flops
cards. Should be computing model flops
utilization and seeing how close we are
utilization and seeing how close we are
getting to the limits of the card. But
getting to the limits of the card. But
we aren't right now. Puffer's
we aren't right now. Puffer's
performance depends strongly on using
performance depends strongly on using
thousands of parallel environments,
thousands of parallel environments,
large mini batch sizes. Like to have
large mini batch sizes. Like to have
less performance degradation with fewer
less performance degradation with fewer
environments and smaller mini batches.
environments and smaller mini batches.
Bonus work with Puffer. I said this
Bonus work with Puffer. I said this
would be an opinionated guide. Puffer AI
would be an opinionated guide. Puffer AI
is a comprehensive effort to fix
is a comprehensive effort to fix
everything wrong with reinforcement
everything wrong with reinforcement
learning. If I thought there was a
learning. If I thought there was a
better approach overall, I would be
better approach overall, I would be
doing that instead.
doing that instead.
Puffer Lib is free
a free and open source project and at
a free and open source project and at
this point most of the code is written
this point most of the code is written
by contributors. We absolutely need more
by contributors. We absolutely need more
help on the core. It's only a few
help on the core. It's only a few
thousand lines but this is where the
thousand lines but this is where the
most experience is required. This is
most experience is required. This is
where we are making key enabling
where we are making key enabling
breakthroughs
breakthroughs
and working on this is the best way to
and working on this is the best way to
understand
and contributing is the best way to
and contributing is the best way to
understand the cutting edge of
understand the cutting edge of
reinforcement learning.
reinforcement learning.
Discord.gg/puffer to get involved. If
Discord.gg/puffer to get involved. If
you found this guide useful, please take
you found this guide useful, please take
a moment to star on GitHub. Support our
a moment to star on GitHub. Support our
work for free. Cool. That is a nearly
work for free. Cool. That is a nearly
4,000word guide. Um,
but yeah, this is basically 25 pages
but yeah, this is basically 25 pages
worth of articles.
worth of articles.
We used Alpha Zero to solve an
We used Alpha Zero to solve an
industrial problem.
industrial problem.
Oh, cool.
Oh, cool.
So, you must have had setable sim state
So, you must have had setable sim state
then.
then.
Can I ask what organization you're with?
Can I ask what organization you're with?
That's cool.
previous company. But here's the
previous company. But here's the
article.
article.
Oh, it's not going to let you link it.
Oh, it's not going to let you link it.
Uh, if you just take give me the name or
Uh, if you just take give me the name or
put it in the Discord. Either way,
I don't know. Twitch might like auto
I don't know. Twitch might like auto
time you out for 30 seconds for links or
time you out for 30 seconds for links or
something. I don't have anything enabled
something. I don't have anything enabled
to do that. But basically, none of the
to do that. But basically, none of the
major streaming sites like it when you
major streaming sites like it when you
link stuff
because of all the spam bots
because of all the spam bots
next to Discord. Sure, I will look at
next to Discord. Sure, I will look at
that.
that.
[Music]
[Music]
I'm happy with this. I need to generate
I'm happy with this. I need to generate
images,
images,
but I will uh I'll make a tweet like
but I will uh I'll make a tweet like
tomorrow or whatever that this goes live
tomorrow or whatever that this goes live
at 10K
at 10K
for folks watching. I have this article
for folks watching. I have this article
here. Yes.
here. Yes.
And then I have this article
And then I have this article
here.
here.
2K words, 4K words.
2K words, 4K words.
Um, this is the main content I plan to
Um, this is the main content I plan to
put out for
put out for
education, at least for at least the
education, at least for at least the
next chunk of time. I don't want to be
next chunk of time. I don't want to be
spending too much of my time on just
spending too much of my time on just
educational content. I will update the
educational content. I will update the
uh the docs page on custom environments
uh the docs page on custom environments
to at least include a little bit more
to at least include a little bit more
about our API and how that works because
about our API and how that works because
clearly people don't fully understand
clearly people don't fully understand
that. Um, is there anything else in here
that. Um, is there anything else in here
or any general questions? Let's just put
or any general questions? Let's just put
it out this way instead because like you
it out this way instead because like you
haven't read the whole article and
haven't read the whole article and
probably people have not been here
probably people have not been here
listening to me yak for the whole time.
listening to me yak for the whole time.
Uh, do you have any general questions?
Uh, do you have any general questions?
Anybody here about reinforcement
Anybody here about reinforcement
learning or how to do things or how to
learning or how to do things or how to
learn things or how to approach things?
learn things or how to approach things?
If so, type them and I will see if I
If so, type them and I will see if I
think that they are answered by any
think that they are answered by any
point in this article. If not, I will
point in this article. If not, I will
make a couple quick revisions.
make a couple quick revisions.
Education content creates a feeder into
Education content creates a feeder into
the community. That is the general idea,
the community. That is the general idea,
right? So, we have this project. It's
right? So, we have this project. It's
open source. Uh I only have so many
open source. Uh I only have so many
hours in the day and I only have so many
hours in the day and I only have so many
hours like that I can work consecutively
hours like that I can work consecutively
for like a month straight before my
for like a month straight before my
brain just needs like a break. So, uh
brain just needs like a break. So, uh
ideally if we have more people that are
ideally if we have more people that are
like actually competently going about
like actually competently going about
this stuff,
this stuff,
especially in the areas where I do not
especially in the areas where I do not
have the time, it would help a lot.
have the time, it would help a lot.
There are areas of research at the
There are areas of research at the
moment where it would be great to get
moment where it would be great to get
help. It's a lot harder to get into that
help. It's a lot harder to get into that
stuff, but I think that a couple of our
stuff, but I think that a couple of our
contributors are at that level now.
contributors are at that level now.
They're just doing other stuff, but like
They're just doing other stuff, but like
it is very doable to get to this level,
it is very doable to get to this level,
especially if you come in with at least
especially if you come in with at least
like basic AI knowledge and a basic
like basic AI knowledge and a basic
systems background, can write decent
systems background, can write decent
simple low-level code, and you have AI
simple low-level code, and you have AI
knowledge at the level of CS231N.
knowledge at the level of CS231N.
Um, the rest of this stuff can be
Um, the rest of this stuff can be
learned very quickly.
learned very quickly.
Have the link to your Discord.
Oh, it's probably DM'd.
Yeah.
Oh, cool. Yeah. Yeah. Yeah. So this is
Oh, cool. Yeah. Yeah. Yeah. So this is
actually dude this was the exact thing
actually dude this was the exact thing
that um this is literally the exact type
that um this is literally the exact type
of thing that I like I was thinking
of thing that I like I was thinking
about with puffer. I didn't know that
about with puffer. I didn't know that
some anybody's done this yet. Like when
some anybody's done this yet. Like when
I'm thinking of really valuable
I'm thinking of really valuable
applications to uh for AI like thank you
applications to uh for AI like thank you
actually for this article. I'm going to
actually for this article. I'm going to
send this to my contributors.
Thank you very much for that. That's
Thank you very much for that. That's
like one of the problems cuz we're
like one of the problems cuz we're
looking for like really valuable
looking for like really valuable
problems where we can potentially do
problems where we can potentially do
stuff. Um, and that's like a huge market
stuff. Um, and that's like a huge market
as well because basically if we get one
as well because basically if we get one
of these big ones, right, if we get like
of these big ones, right, if we get like
one really big job where we're able to
one really big job where we're able to
just crush it, right, um, then we're
just crush it, right, um, then we're
kind of funded for RL for the
kind of funded for RL for the
foreseeable future and we can just keep
foreseeable future and we can just keep
doing cool science.
doing cool science.
The challenge end is still available.
Cool.
Oh yeah, we can. Yeah, this is Sure. We
Oh yeah, we can. Yeah, this is Sure. We
can crush this.
can crush this.
Yeah, we can crush this 100%.
We do have to be able to uh to do our
We do have to be able to uh to do our
own sim.
own sim.
But because we have to have the Sim be
But because we have to have the Sim be
fast. But yeah, we can crush that.
Have you seen any of the other stuff
Have you seen any of the other stuff
around Puffer Shadow? Like our new
around Puffer Shadow? Like our new
algorithm stuff and like some of our
algorithm stuff and like some of our
results
results
to give you a quick idea here
to give you a quick idea here
of like the type of stuff we've done.
We do ludicrously high performance sim.
We do ludicrously high performance sim.
Not really doing anything special
Not really doing anything special
honestly just by like not doing stuff
honestly just by like not doing stuff
the slow way that people normally do it.
the slow way that people normally do it.
And uh we have our RL optimized end to
And uh we have our RL optimized end to
end to be able to actually handle that
end to be able to actually handle that
amount of data.
amount of data.
And we can just train stuff on just
And we can just train stuff on just
crazy crazy amounts of sim. like you
crazy crazy amounts of sim. like you
don't really have to do anything super
don't really have to do anything super
smart on the algorithm side when you can
smart on the algorithm side when you can
do this. Uh we do actually have our own
do this. Uh we do actually have our own
algorithm research. So we use PO kind of
algorithm research. So we use PO kind of
but ours is a lot better than uh what
but ours is a lot better than uh what
you would expect normally. Different
you would expect normally. Different
optimizer, trajectory filtering,
optimizer, trajectory filtering,
different advantage function, a whole
different advantage function, a whole
bunch of uh improvements. But yeah,
bunch of uh improvements. But yeah,
cool.
Non Sim, they rebuilt it.
Non Sim, they rebuilt it.
Yeah. If you uh yeah, shadow, if you
Yeah. If you uh yeah, shadow, if you
don't have if you don't have documents
don't have if you don't have documents
preventing you from legally doing so,
preventing you from legally doing so,
like uh message us in in the Discord
like uh message us in in the Discord
because the thing from the article is
because the thing from the article is
true. where like if you have domain
true. where like if you have domain
knowledge in an area and you can help us
knowledge in an area and you can help us
get a contract, you can get a cut of it
get a contract, you can get a cut of it
if you help us like set it up and you
if you help us like set it up and you
want to work it cuz uh we are very well
want to work it cuz uh we are very well
positioned at this point with all our
positioned at this point with all our
Sim stuff to really crush any sort of
Sim stuff to really crush any sort of
problem that looks like that. We are
problem that looks like that. We are
really well positioned to just crush.
really well positioned to just crush.
So there might be an opportunity there.
What exactly reinforcement learning?
What exactly reinforcement learning?
What is the right context to think of
What is the right context to think of
it? Is it compai math? What is the
it? Is it compai math? What is the
motivation for it? What keeps the space
motivation for it? What keeps the space
fruitful progress in real world
fruitful progress in real world
applications? I don't know if that's in
applications? I don't know if that's in
the scope of what you're trying to do.
Uh arguably you're right that I haven't
Uh arguably you're right that I haven't
really sold like why learn reinforcement
really sold like why learn reinforcement
learning.
learning.
This is fair.
Not working for that company anymore.
Not working for that company anymore.
Oh, yeah. No, I wouldn't ask you if you
Oh, yeah. No, I wouldn't ask you if you
were. That would be I'm sure that there
were. That would be I'm sure that there
would be legal problems there. That's
would be legal problems there. That's
why I'm asking you, right?
like our outreach stuff has been pretty
like our outreach stuff has been pretty
dang successful
dang successful
um at this point. Like people are kind
um at this point. Like people are kind
of starting to see what we're doing.
of starting to see what we're doing.
Help a little bit what it is. Um,
Help a little bit what it is. Um,
and like we've we've kind of been we've
and like we've we've kind of been we've
had a good time so far like talking to
had a good time so far like talking to
people in tons of different industries.
people in tons of different industries.
The hardest thing at the moment is kind
The hardest thing at the moment is kind
of just finding those clear-cut
of just finding those clear-cut
reinforcement learning applications
reinforcement learning applications
where we can make something super
where we can make something super
valuable and like carving them into RL
valuable and like carving them into RL
problems. Um, so if we can do that and
problems. Um, so if we can do that and
get that to somebody, that's like
get that to somebody, that's like
that is super valuable.
that is super valuable.
Yeah. Yeah. So, Jason, I got to think
Yeah. Yeah. So, Jason, I got to think
about that a little bit. Like, do I
about that a little bit. Like, do I
actually want to try two.
I could just add like a thing here.
It should be in the first paragraph.
So I mean I guess the way that I answer
So I mean I guess the way that I answer
that Jason is it is uh it's an empirical
that Jason is it is uh it's an empirical
science. It's a sub area of artificial
science. It's a sub area of artificial
intelligence which artificial
intelligence which artificial
intelligence draws on computer science
intelligence draws on computer science
and discrete math for the most part. Uh
and discrete math for the most part. Uh
it is the branch of AI that deals with
it is the branch of AI that deals with
learning through interaction.
learning through interaction.
So agents learn by interacting with an
So agents learn by interacting with an
environment, the real world, a
environment, the real world, a
simulator, what have you. It models the
simulator, what have you. It models the
way that we learn in a way kind of that
way that we learn in a way kind of that
other branches don't. They're trained
other branches don't. They're trained
supervised or on big corpuses fixed data
supervised or on big corpuses fixed data
where you have no effect. You can't
where you have no effect. You can't
affect your data. Um
affect your data. Um
it's been applied to a bunch of spaces,
it's been applied to a bunch of spaces,
but what keeps it fruitful? Not it's
but what keeps it fruitful? Not it's
like not at all. It's stuck. there
like not at all. It's stuck. there
hasn't really been like the amount of
hasn't really been like the amount of
progress has slowed to almost a halt
progress has slowed to almost a halt
since 2019ish
since 2019ish
and that's what I'm trying to fix. So 90
and that's what I'm trying to fix. So 90
plus% of the time my content around here
plus% of the time my content around here
is focused on uh I am a scientist. I am
is focused on uh I am a scientist. I am
doing research. How do we make
doing research. How do we make
reinforcement learning work at a core
reinforcement learning work at a core
level algorithmically? How do we get it
level algorithmically? How do we get it
on tons of different applications in the
on tons of different applications in the
real world? How do we do that whole
real world? How do we do that whole
loop?
loop?
Uh, this is the rare piece of
Uh, this is the rare piece of
educational content I'm doing because
educational content I'm doing because
people have asked for it a ton.
people have asked for it a ton.
We have a ton of people on YouTube
We have a ton of people on YouTube
today. So, hey, welcome everybody. Um,
today. So, hey, welcome everybody. Um,
yeah, if you're new around here and you
yeah, if you're new around here and you
want to see some cool demos that just
want to see some cool demos that just
run in your browser,
run in your browser,
we've got Puffer Ocean here. So, there's
we've got Puffer Ocean here. So, there's
everything from like just simple arcade
everything from like just simple arcade
games like more visually interesting
games like more visually interesting
kind of stuff. These are all neural
kind of stuff. These are all neural
networks playing in your browser as
networks playing in your browser as
well.
well.
Got like this mobile. It's kind of like
Got like this mobile. It's kind of like
a stripped down League of Legends or
a stripped down League of Legends or
Dota type thing.
Dota type thing.
We've got uh what else we have in here.
We've got uh what else we have in here.
Oh yeah, we've got drones. It's not all
Oh yeah, we've got drones. It's not all
games.
We do like games though because games
We do like games though because games
are really easy to interpret. This is my
are really easy to interpret. This is my
uh the followup to my PhD thesis, neural
uh the followup to my PhD thesis, neural
MMO. Like a thousand agents running
MMO. Like a thousand agents running
around in this virtual world where they
around in this virtual world where they
can fight and get armor and tools and
can fight and get armor and tools and
equipment and they can trade with each
equipment and they can trade with each
other and they can level up and
other and they can level up and
different things full game.
different things full game.
Be the Johnny Apple Seed of RL.
Be the Johnny Apple Seed of RL.
I do want to make it accessible, right?
I do want to make it accessible, right?
The thing is this almost kind of
The thing is this almost kind of
happened a little bit by accident if you
happened a little bit by accident if you
think about it because what I did here
think about it because what I did here
is I was focused on how do I advance the
is I was focused on how do I advance the
cutting edge, right? How do I advance
cutting edge, right? How do I advance
reinforcement learning research and in
reinforcement learning research and in
the process right the way to advance
the process right the way to advance
reinforcement learning research was to
reinforcement learning research was to
dramatically simplify and accelerate a
dramatically simplify and accelerate a
lot of the existing stuff. So in the
lot of the existing stuff. So in the
process of doing that, we made the
process of doing that, we made the
entire reinforcement learning stack a
entire reinforcement learning stack a
fraction of the size that it used to be
fraction of the size that it used to be
and we made it so that people can
and we made it so that people can
actually do experiments on their laptop
actually do experiments on their laptop
now in seconds where it used to take
now in seconds where it used to take
hours and big fancy complicated machine.
hours and big fancy complicated machine.
Uh so that made it really accessible and
Uh so that made it really accessible and
we started getting a lot of new people
we started getting a lot of new people
where originally I had intended this to
where originally I had intended this to
make it easier for RL PhDs, now it's
make it easier for RL PhDs, now it's
like brand new programmers. So, it's
like brand new programmers. So, it's
come down a huge amount, become way more
come down a huge amount, become way more
accessible. Uh, and that stuff is nice
accessible. Uh, and that stuff is nice
and useful. Um, but I it's it's not the
and useful. Um, but I it's it's not the
main thing, right? It's kind of like a
main thing, right? It's kind of like a
byproduct. The main thing on my plate is
byproduct. The main thing on my plate is
fixing the field, like advancing the
fixing the field, like advancing the
science.
science.
That's the main goal.
That's the main goal.
And I invite all of you to come join me
And I invite all of you to come join me
on that because uh it's a very exciting
on that because uh it's a very exciting
area of science and it's probably the
area of science and it's probably the
last area of AI right now where you can
last area of AI right now where you can
make huge huge advancements without
make huge huge advancements without
needing a ton of capital.
needing a ton of capital.
Welcome major.
Welcome major.
And since we do have at less
me out it's free. Join the Discord if
me out it's free. Join the Discord if
you want to get involved with all this
you want to get involved with all this
cool stuff. And uh yeah, back to back to
cool stuff. And uh yeah, back to back to
work on this.
work on this.
Move forward. Move backwards first from
Move forward. Move backwards first from
very mediocre movie.
I guess it was a book first. Probably
I guess it was a book first. Probably
the book is better.
the book is better.
I think I'll take a crack at Rubik's
I think I'll take a crack at Rubik's
Cube. Dian Rayb.
Cube. Dian Rayb.
Yeah, that'll work. I think that'll
Yeah, that'll work. I think that'll
work. I don't know how complicated.
work. I don't know how complicated.
Well, you don't need the solver at all.
Well, you don't need the solver at all.
So it's probably it's probably a very
So it's probably it's probably a very
quick little thing to do be a good like
quick little thing to do be a good like
that's a good intro project like that
that's a good intro project like that
would be
would be
that would qualify for like this here
that would qualify for like this here
like section five of this
heck let's just tweet this right now
heck let's just tweet this right now
that like
All right, we're a little over 400 away
All right, we're a little over 400 away
from that. Cool.
Let me add a couple quick sentences to
Let me add a couple quick sentences to
this and then we'll move on to the next
this and then we'll move on to the next
thing for today.
thing for today.
Believe I have another 40 minutes to do.
Yeah, this paragraph I should spend more
Yeah, this paragraph I should spend more
time on. I think just a little bit more
time on. I think just a little bit more
time on this.
can read it today.
can read it today.
I doubt it'll hit 10K today. I kind of
I doubt it'll hit 10K today. I kind of
hope it doesn't just do that instantly
hope it doesn't just do that instantly
because I'll have to I still have to get
because I'll have to I still have to get
images. Like you need images for the
images. Like you need images for the
articles at the top. But uh I will I
articles at the top. But uh I will I
guess I'll just do it today if I have
guess I'll just do it today if I have
to. You know, I promise I do it. I do
to. You know, I promise I do it. I do
it.
Do start the repo though. Star the
Do start the repo though. Star the
puffer. It helps us out.
All right, we workshop this first
All right, we workshop this first
paragraph so that like I'll add the
paragraph so that like I'll add the
thing. Actually, Jason, that advice was
thing. Actually, Jason, that advice was
very good. And then the plan is if I
very good. And then the plan is if I
have a little bit of time after that,
have a little bit of time after that,
we're going to start on the camera ready
we're going to start on the camera ready
for my publication. I'm pretty sure that
for my publication. I'm pretty sure that
doesn't violate anything because it's
doesn't violate anything because it's
already accepted. But yeah, we have um
uh where is it?
uh where is it?
Yeah. So, this paper was accepted to
Yeah. So, this paper was accepted to
RLC.
RLC.
So, yay. There's finally going to be a
So, yay. There's finally going to be a
puffer publication.
Uh, it's super outdated already because
Uh, it's super outdated already because
Huffer moves way faster than the
Huffer moves way faster than the
conference cycle, but I don't know. I'll
conference cycle, but I don't know. I'll
still show it off and we can edit it a
still show it off and we can edit it a
little bit today. I figured I'd do it on
little bit today. I figured I'd do it on
stream because why the hell not.
Not tomorrow though. Tomorrow I'm
Not tomorrow though. Tomorrow I'm
traveling all day and then I will be
traveling all day and then I will be
back streaming uh Thursday, Friday.
Hello, Kyoko.
Hello, Kyoko.
Congrats on what?
Congrats on what?
I guess there are many cool things
I guess there are many cool things
lately.
I didn't just instantly hit 10K, right?
I didn't just instantly hit 10K, right?
Like
Like
That'd be kind of funny. Ow.
Reinforcement learning is hard and most
Reinforcement learning is hard and most
of the material out there for beginners
of the material out there for beginners
makes it even harder.
It's the underexplored niche of AI
that focuses on learning from
that focuses on learning from
interaction
interaction
and one of the few areas where you can
and one of the few areas where you can
really advance the field without a ton
really advance the field without a ton
of comput.
of comput.
The advice here is a formalized
products.
products.
Uh
fusion
fusion
people did fusion with it, right?
people did fusion with it, right?
Do logistics first.
I haven't vetted the fusion one, which
I haven't vetted the fusion one, which
is a little
Logistics Fusion, cooling, driving.
maybe super dumb question, but why is TD
maybe super dumb question, but why is TD
such a popular choice in RL?
such a popular choice in RL?
Um,
so it's not exactly,
so it's not exactly,
but the thing is that like everything's
but the thing is that like everything's
kind of TD if you squint at it, right?
kind of TD if you squint at it, right?
Like there's always like pretty much
Like there's always like pretty much
anything that you get, there's like some
anything that you get, there's like some
form of TD in it. And it's you need some
form of TD in it. And it's you need some
sort of credit assignment is the
sort of credit assignment is the
problem, right? So like you get a reward
problem, right? So like you get a reward
later and you need to figure out how
later and you need to figure out how
much to care about said reward and
much to care about said reward and
there's not a good way of doing that in
there's not a good way of doing that in
general. So usually there's some sort of
general. So usually there's some sort of
like fancy learned exponential
like fancy learned exponential
discounting
discounting
which you write out as like a TD over N,
which you write out as like a TD over N,
right?
It's mostly just like
It's mostly just like
what if you do regression? How
what if you do regression? How
how would you do regression?
how would you do regression?
What are you regressing?
like goal condition behavioral cloning.
like goal condition behavioral cloning.
Well, that's no longer RL, right?
Well, that's no longer RL, right?
Behavioral cloning, you're no longer
Behavioral cloning, you're no longer
learning from simulation data. You're
learning from simulation data. You're
learning from expert data.
You need expert data to do that and you
You need expert data to do that and you
can't scale it infinitely because you
can't scale it infinitely because you
only have so much so much expert data
ABC
uh I do not consider so it depends on
uh I do not consider so it depends on
your definitions. If you consider
your definitions. If you consider
offline reinforcement learning to be
offline reinforcement learning to be
reinforcement learning, then sure. I do
reinforcement learning, then sure. I do
not consider offline reinforcement
not consider offline reinforcement
learning to be reinforcement learning.
learning to be reinforcement learning.
It's a clever term that somebody came up
It's a clever term that somebody came up
for supervised learning in order to like
for supervised learning in order to like
sell it at a conference or whatever. Um,
sell it at a conference or whatever. Um,
the key defining aspect of reinforcement
the key defining aspect of reinforcement
learning for me is interaction.
learning for me is interaction.
So, if you're not actually interacting
So, if you're not actually interacting
with a a sim or the real world or
with a a sim or the real world or
something, it's not RL.
defining RL as the like the specific
defining RL as the like the specific
class of algorithms makes way less sense
class of algorithms makes way less sense
than defining it as uh defining it
than defining it as uh defining it
through interaction.
with a dash.
with a dash.
No, it's literally It's the right word.
No, it's literally It's the right word.
Shut up and spell check.
Applications include
Applications include
robotics, logistics, gaming, and even
robotics, logistics, gaming, and even
scientific simulation.
It's calibration.
Right. It was calibration for fusion.
Control the plasmas. Yeah.
Control the plasmas. Yeah.
Well, it's control, I guess.
I'll call the control problem.
Sorry if I jumped the gun. Reinforcement
Sorry if I jumped the gun. Reinforcement
learning is currently an
learning is currently an
for
All right, we'll spend a little bit of
All right, we'll spend a little bit of
time on this then to like make sure that
time on this then to like make sure that
this is good because this is ultimately
this is good because this is ultimately
the thing that is the most important.
the thing that is the most important.
Also, did I do the same thing for my
Also, did I do the same thing for my
other one? I was like, there's the other
other one? I was like, there's the other
article.
Yeah, this is fine because this is a
Yeah, this is fine because this is a
personal article. like you read this
personal article. like you read this
because you basically you already follow
because you basically you already follow
me
me
or like you like you you don't read this
or like you like you you don't read this
because
because
like you're not going to read my advice
like you're not going to read my advice
on a topic that's like super subjective
on a topic that's like super subjective
um if you don't already follow this like
um if you don't already follow this like
in RL it makes more sense because it's
in RL it makes more sense because it's
more like it's more specialized
more like it's more specialized
but that's basically because people
but that's basically because people
asked me for it.
Okay.
what I can't wrap my head around RL is
what I can't wrap my head around RL is
why Marovian YTD it becomes
why Marovian YTD it becomes
because it's underexplored. No. Okay,
because it's underexplored. No. Okay,
Sean, let me explain this. My guide
Sean, let me explain this. My guide
explains this. Okay, ignore all the
explains this. Okay, ignore all the
formalism about MDPs.
formalism about MDPs.
TD is kind of okay, but it's like
TD is kind of okay, but it's like
whatever. Just ignore all the formulism
whatever. Just ignore all the formulism
about MDPs, though. All right, ignore
about MDPs, though. All right, ignore
all of it. It's cuz that was the model
all of it. It's cuz that was the model
that they could fit to the problems.
that they could fit to the problems.
RL is going to be a lot easier if you
RL is going to be a lot easier if you
just view it as an empirical science.
just view it as an empirical science.
All right,
All right,
none of the theory actually works. And
none of the theory actually works. And
like they'll basically RL is the field
like they'll basically RL is the field
compared to other areas of AI where
compared to other areas of AI where
they'll just they'll write pages and
they'll just they'll write pages and
pages and pages and pages of math
pages and pages and pages of math
and none of it works. All right. And
and none of it works. All right. And
then the super simple thing without any
then the super simple thing without any
theoretical justification or like any
theoretical justification or like any
formal proof that works perfectly.
If you view it as an empirical science
If you view it as an empirical science
like you will no longer be up at night
like you will no longer be up at night
over this.
RL equals LMS.
Actually, I think that there's there's
Actually, I think that there's there's
probably better theory for general uh
probably better theory for general uh
general learning than there is for RL.
So it's it's a weird combination where
So it's it's a weird combination where
basically
basically
people in RL they try to write down more
people in RL they try to write down more
formal math than other areas of AI even
formal math than other areas of AI even
though they actually have less formal
though they actually have less formal
math that actually works. All right.
math that actually works. All right.
And it's probably because MDPs exist
And it's probably because MDPs exist
independently. So like it looks like, oh
independently. So like it looks like, oh
yeah, we can just model this as that and
yeah, we can just model this as that and
we get all this really nice math. But
we get all this really nice math. But
then the thing is none of it actually
then the thing is none of it actually
works. So, what's the point?
Like a mathematical model is useful to
Like a mathematical model is useful to
the extent that it can actually predict
the extent that it can actually predict
the thing you care about. And if it
the thing you care about. And if it
can't, then it's not a useful model.
And when you have a field where you can
And when you have a field where you can
do work, but you don't have a
do work, but you don't have a
mathematical model, that's an empirical
mathematical model, that's an empirical
field.
Damn.
Damn.
I mean, it's not like the thing is as
I mean, it's not like the thing is as
soon as you just as soon as you give up
soon as you just as soon as you give up
that old perspective, right? As soon as
that old perspective, right? As soon as
you come and like look at this the way
you come and like look at this the way
that we're doing it in Puffer Lib,
that we're doing it in Puffer Lib,
everything will make a lot more sense
everything will make a lot more sense
and like you'll actually see the path to
and like you'll actually see the path to
progress. I promise you
progress. I promise you
cuz I pretty much in my head here like
cuz I pretty much in my head here like
if I just ignore everything else and I
if I just ignore everything else and I
were to work on this stuff for like a
were to work on this stuff for like a
year or two, I pretty much have the A to
year or two, I pretty much have the A to
B to C to D to like RL solved.
MDP's considered harmful. Are you
MDP's considered harmful. Are you
serious? I was actually I was
serious? I was actually I was
considering doing that. I but I was like
considering doing that. I but I was like
ah you know what I kind of piss off the
ah you know what I kind of piss off the
academics as enough as is.
Oh, he didn't actually write MDP
Oh, he didn't actually write MDP
considered harmful.
Oh, I see. He actually kind of did.
That's cool.
Who's this dude?
Who's this dude?
Oh, Alberta. Okay.
Yeah. So I don't
Yeah. So I don't
I guess you can put that stuff on
I guess you can put that stuff on
archive technically, but like academics
archive technically, but like academics
love to do this thing where they kind of
love to do this thing where they kind of
like fence each other with uh it's like
like fence each other with uh it's like
British Parliament, right? They like
British Parliament, right? They like
fence each other with like very couched
fence each other with like very couched
language that's basically saying X thing
language that's basically saying X thing
is old and dumb. Whereas I won't do that
is old and dumb. Whereas I won't do that
and I'll just say, "Hey, this thing is
and I'll just say, "Hey, this thing is
old and dumb."
old and dumb."
Poker AI. All right, cool. I've actually
Poker AI. All right, cool. I've actually
wanted to do poker and puffer lip just
wanted to do poker and puffer lip just
to see,
to see,
but we're actually I think chess is
but we're actually I think chess is
probably the better one at the moment.
We got lots of projects.
I will retweet that with MDP's
I will retweet that with MDP's
considered harmful.
considered harmful.
funny.
Also, uh continual learning as a a
Also, uh continual learning as a a
category like continual learning doesn't
category like continual learning doesn't
need to exist.
need to exist.
That's the other funny thing. There's
That's the other funny thing. There's
not really a big difference at all with
not really a big difference at all with
stuff.
Reinforcement learning is the branch of
Reinforcement learning is the branch of
AI that focuses on learning through
AI that focuses on learning through
interaction.
Reinforcement learning is the branch of
Reinforcement learning is the branch of
AI
Reinforcement learning is about learning
Reinforcement learning is about learning
through interaction.
through interaction.
Applications include robotics,
underexplored niche of AI where you can
underexplored niche of AI where you can
really advance the field without a ton
really advance the field without a ton
of compute.
The learning RL
But learning RL and most of the material
But learning RL and most of the material
out there for be uh is hard.
All right. I like this was good advice
All right. I like this was good advice
from Jason.
from Jason.
Reinforcement learning is about learning
Reinforcement learning is about learning
through interaction, application,
through interaction, application,
robotics, logistics, gaming, and even
robotics, logistics, gaming, and even
control problems.
I was at a loss for words for a minute
I was at a loss for words for a minute
to realize that RL is all empirical and
to realize that RL is all empirical and
the gap between theory and practical is
the gap between theory and practical is
huge. That was painful just listening
huge. That was painful just listening
to.
to.
Yeah. But the thing is again like look
Yeah. But the thing is again like look
at what we've built and tell me if it's
at what we've built and tell me if it's
painful. Okay.
painful. Okay.
Did I show you this? Did I show you the
Did I show you this? Did I show you the
Pong demo?
Pong demo?
Watch this.
Watch this.
So we can kind of just show some people
So we can kind of just show some people
the stuff we build around here.
the stuff we build around here.
All right, puffer train. Puff or let's
All right, puffer train. Puff or let's
do Eval first. Puffer
do Eval first. Puffer
just so you can see the difference.
just so you can see the difference.
All right, here's Pong.
All right, here's Pong.
This is a random agent. It can't play at
This is a random agent. It can't play at
all. It's just it barely ever hits the
all. It's just it barely ever hits the
puffer back. We use a puffer for the
puffer back. We use a puffer for the
ball because of course we do. Puffer
ball because of course we do. Puffer
train. Puffer pong.
train. Puffer pong.
All right. This is something that
All right. This is something that
usually like when we use Pong for
usually like when we use Pong for
research during my PhD, you'd have to
research during my PhD, you'd have to
overnight this because this would take
overnight this because this would take
hours.
hours.
Okay, we're done.
Here's your perfect Pong model or almost
Here's your perfect Pong model or almost
perfect pong model. It misses once in a
perfect pong model. It misses once in a
blue moon.
blue moon.
It never loses, though.
And like that's the only demo that we
And like that's the only demo that we
have that's literally 5 seconds, but um
have that's literally 5 seconds, but um
in fact we've even done it in 3 seconds
in fact we've even done it in 3 seconds
before. But a lot of the environments
before. But a lot of the environments
train in like 30 seconds. And most of
train in like 30 seconds. And most of
the ones that don't train in a few
the ones that don't train in a few
minutes. And then we have like a couple
minutes. And then we have like a couple
way harder tasks that are 10 minutes or
way harder tasks that are 10 minutes or
15 minutes. And then maybe a couple of
15 minutes. And then maybe a couple of
tasks that actually take substantially
tasks that actually take substantially
more.
sent you the challenge link. Thank you.
sent you the challenge link. Thank you.
We will be in touch with that. That
We will be in touch with that. That
sounds like a a cool application area.
Yeah. Sean, so one of the the things as
Yeah. Sean, so one of the the things as
well that reinforcement learning just
well that reinforcement learning just
got stuck on for some reason is like
got stuck on for some reason is like
academics for whatever reason they felt
academics for whatever reason they felt
like you really the only thing that
like you really the only thing that
matters is that you can learn with less
matters is that you can learn with less
experience. That's the only thing that
experience. That's the only thing that
matters. All right? So if you have
matters. All right? So if you have
method A that learns long in two million
method A that learns long in two million
steps and method B that learns in one
steps and method B that learns in one
million steps, method uh B is way
million steps, method uh B is way
better. even if method B takes a
better. even if method B takes a
thousand times longer
thousand times longer
and like compute.
and like compute.
So basically what happened is over the
So basically what happened is over the
years people did a ton of theory work
years people did a ton of theory work
and then they trained on these really
and then they trained on these really
stupidly simple tasks very very slowly
stupidly simple tasks very very slowly
because all their code was crazy slow
because all their code was crazy slow
and their algorithms were crazy slow.
and their algorithms were crazy slow.
All right.
All right.
And we kind of came along and said,
And we kind of came along and said,
"Okay, that's dumb. Let's just do
"Okay, that's dumb. Let's just do
everything with fast sims and try to
everything with fast sims and try to
make things run faster in wall clock
make things run faster in wall clock
time and uh we'll just build fast sims."
time and uh we'll just build fast sims."
And lo and behold, in industry, if you
And lo and behold, in industry, if you
can build a fast simulator, nobody ever
can build a fast simulator, nobody ever
says, "Well, can it train in fewer
says, "Well, can it train in fewer
steps?" Nobody cares. It's an entirely
steps?" Nobody cares. It's an entirely
academic invented problem. All right.
academic invented problem. All right.
Now, yeah, there are cases where you
Now, yeah, there are cases where you
can't build fast Sims and it's useful
can't build fast Sims and it's useful
there, but like basically nobody even
there, but like basically nobody even
bothered trying to say, "Hey, wait a
bothered trying to say, "Hey, wait a
second. Reinforcement learning can solve
second. Reinforcement learning can solve
all these problems where we just build
all these problems where we just build
fast Sims." Like, they didn't even do
fast Sims." Like, they didn't even do
that. So, that's what we do. We'll get
that. So, that's what we do. We'll get
to um we'll get to sample efficient
to um we'll get to sample efficient
learning. It's not going to be that
learning. It's not going to be that
hard. It'll be fine. Um like with all
hard. It'll be fine. Um like with all
the stuff we've built, it's not going to
the stuff we've built, it's not going to
be that that hard. We'll have to do some
be that that hard. We'll have to do some
research, but it won't be that bad.
research, but it won't be that bad.
Nowhere near as bad as it is now.
Go through this paragraph one more time.
Go through this paragraph one more time.
Reinforcement learning is about learning
Reinforcement learning is about learning
through interaction applications include
through interaction applications include
robotics, logistics, gaming, and even
robotics, logistics, gaming, and even
control problems in science like nuclear
control problems in science like nuclear
fusion. Underexplored niche of AI where
fusion. Underexplored niche of AI where
you can really advance the field without
you can really advance the field without
a ton of compute. But learning RL is
a ton of compute. But learning RL is
hard and most of the material out there
hard and most of the material out there
for beginners makes it even harder. Vice
for beginners makes it even harder. Vice
here is a formalized version of how I
here is a formalized version of how I
train new puffer lip contributors. Some
train new puffer lip contributors. Some
of these came in with zero programming
of these came in with zero programming
knowledge and now help advance our
knowledge and now help advance our
research tools. The key start doing
research tools. The key start doing
reinforcement learning immediately while
reinforcement learning immediately while
filling a knowledge gap slowly and in
filling a knowledge gap slowly and in
order of relevance
others.
others.
Reinforcement learning
In other words, reinforcement learning.
That's fun. All right. Cool. I like that
That's fun. All right. Cool. I like that
a lot.
a lot.
We are I think this is set.
Oh.
Yeah. Okay.
Yeah. Okay.
This is uh doing very well.
I believe this Meta's MRQ style dynamics
I believe this Meta's MRQ style dynamics
based learning integration into model
based learning integration into model
free on model policy. Go Mr. Oh, Mr. Q.
free on model policy. Go Mr. Oh, Mr. Q.
Yeah, I uh I got to look at Mr. Q as
Yeah, I uh I got to look at Mr. Q as
well for sure. I haven't like seen
well for sure. I haven't like seen
enough to put it into the post on it.
I'm following.
I'm following.
Okay, bro.
All right, I will be right back. I'm
All right, I will be right back. I'm
going to use the restroom. We'll spend a
going to use the restroom. We'll spend a
few minutes on the uh the new
few minutes on the uh the new
uh the new paper. Then I got to run for
uh the new paper. Then I got to run for
meeting. I'll be right back. Go.
All
right.
Let's take a quick look at uh the paper.
Don't open review.
I thought for a second that I was like
I thought for a second that I was like
wrong and it had been accepted.
Yeah, there it is. There's the formal
Yeah, there it is. There's the formal
accept. I actually hadn't even seen that
accept. I actually hadn't even seen that
it was posted. So that would have been
it was posted. So that would have been
kind of awkward if I'd like gotten my
kind of awkward if I'd like gotten my
tickets and everything.
tickets and everything.
But all right.
Oops. Yeah. Meeting in 10. Let's see
Oops. Yeah. Meeting in 10. Let's see
what I promised I will do I do with this
what I promised I will do I do with this
to make this get accepted. And we'll
to make this get accepted. And we'll
have to do that.
Yeah. So this is all true.
Submission articulates a toward force
Submission articulates a toward force
but largely elects not to provide direct
but largely elects not to provide direct
evidence.
evidence.
This true.
Okay. So, it's granularity of reporting.
The reason I didn't do this, by the way,
The reason I didn't do this, by the way,
and it's not a great reason, it's just
and it's not a great reason, it's just
cuz like it's always outdated in
cuz like it's always outdated in
instantly anyways. But like the best
instantly anyways. But like the best
benchmark numbers are going to be you
benchmark numbers are going to be you
write the 10line timing script and you
write the 10line timing script and you
time whatever environment you want and
time whatever environment you want and
you yell at me if it's like gotten
you yell at me if it's like gotten
slower somehow.
Missing so unclear results
buffer lib are easy. It's not easy to
buffer lib are easy. It's not easy to
implement. Simple to implement. It's not
implement. Simple to implement. It's not
easy to implement.
Definition of throughput is missing.
Emulation impact not negligible for
Emulation impact not negligible for
environments faster than a few thousand.
environments faster than a few thousand.
Gotten this to be better.
Okay. So, basically the complaints are
Okay. So, basically the complaints are
like, "Hey, where's the evidence for all
like, "Hey, where's the evidence for all
the stuff you claimed?" which is like
the stuff you claimed?" which is like
the most relevant of possible claims,
the most relevant of possible claims,
right? Except for the fact that I wasn't
right? Except for the fact that I wasn't
able to do this in the actual
able to do this in the actual
publication because there's there is a
publication because there's there is a
double blind requirement. I'm not
double blind requirement. I'm not
allowed to link to anything. But like
allowed to link to anything. But like
Puffer Lib is literally the most
Puffer Lib is literally the most
transparent invisible RL project out
transparent invisible RL project out
there other than maybe clean RL since
there other than maybe clean RL since
it's like simpler. Um,
so it's like the the evidence is kind of
so it's like the the evidence is kind of
based on the fact that you can just go
based on the fact that you can just go
get it, but since I wasn't able to link
get it, but since I wasn't able to link
to it, that was the main issue.
Let's see what we have.
Yeah.
So basically it's this note right here.
So basically it's this note right here.
So
contributions let's see how dated this
contributions let's see how dated this
is oneline rappers that make complex
is oneline rappers that make complex
environments like net hack nurl limo
environments like net hack nurl limo
gridly etc compatible with any RL
gridly etc compatible with any RL
library support standard impeding zoo
library support standard impeding zoo
formats context gymnas and petting zoo
formats context gymnas and petting zoo
are the most widely used environment
are the most widely used environment
formats however it's compatible with the
formats however it's compatible with the
vast majority of environments using only
vast majority of environments using only
a oneline wrapper drop in vectorization
a oneline wrapper drop in vectorization
for simulating environments in parallel
for simulating environments in parallel
most environments will see at least a
most environments will see at least a
30% speed boost and three 50% to 3x with
30% speed boost and three 50% to 3x with
pooling the broadly compatible
pooling the broadly compatible
contribution applicable to nearly all
contribution applicable to nearly all
environments. Context gymnasium provides
environments. Context gymnasium provides
the most characterization back end. It's
the most characterization back end. It's
slow for the reasons outlined in this
slow for the reasons outlined in this
paper.
paper.
Upper ocean. A suite of 12 environments
Upper ocean. A suite of 12 environments
written in C that each simulate at over
written in C that each simulate at over
a million steps per second on a single
a million steps per second on a single
CPU core. Context. A few of these have
CPU core. Context. A few of these have
built-in AI components that can slow
built-in AI components that can slow
performance when search depth is
performance when search depth is
increased. Base speed is greater than a
increased. Base speed is greater than a
million steps per second on a high-end
million steps per second on a high-end
desktop core. Some environments run at
desktop core. Some environments run at
10 million a second.
10 million a second.
for a PO demo that trains ocean
for a PO demo that trains ocean
environments at 300K to 1.2 million on a
environments at 300K to 1.2 million on a
single RTX 4090. Standard architectures
single RTX 4090. Standard architectures
are an MLP LSTM CNN and LTM 150K to a
are an MLP LSTM CNN and LTM 150K to a
million parameters on compatible with
million parameters on compatible with
all third party environment training is
all third party environment training is
up to 30K step second on Atari still 38
up to 30K step second on Atari still 38
times faster than the original clean RL.
times faster than the original clean RL.
So I think what we'll do I cannot revise
So I think what we'll do I cannot revise
this to be a puffer lip 3.0 paper
this to be a puffer lip 3.0 paper
because it's basically a full rewrite.
because it's basically a full rewrite.
Puffer Lib has changed so incredibly
Puffer Lib has changed so incredibly
much since 2.0 that it's like you can't
much since 2.0 that it's like you can't
submit one paper and then revise it to
submit one paper and then revise it to
be a completely different paper. I think
be a completely different paper. I think
even if it's strictly better plus that's
even if it's strictly better plus that's
just a lot of work to do anyways. So uh
just a lot of work to do anyways. So uh
I think what we'll do is we will add if
I think what we'll do is we will add if
I have an extra page.
I have an extra page.
Do I have an extra page?
Do I have an extra page?
Well, I will go check the conference
Well, I will go check the conference
requirements after because I got to go
requirements after because I got to go
in a minute here. But I'll check the
in a minute here. But I'll check the
page. What we'll just do is we will
page. What we'll just do is we will
proof this. We'll just proof this to
proof this. We'll just proof this to
make sure I'm still happy with the paper
make sure I'm still happy with the paper
as it's presented to make sure that
as it's presented to make sure that
there's nothing like inconsistent.
there's nothing like inconsistent.
And then I will add like a short section
And then I will add like a short section
like a paragraph or whatever on like
like a paragraph or whatever on like
puffer liib moves faster than the
puffer liib moves faster than the
conference cycle
conference cycle
at time of writing. This is actually
at time of writing. This is actually
puffer lilib 3 which has 25 environments
puffer lilib 3 which has 25 environments
is even faster and includes core
is even faster and includes core
algorithmic contributions that make uh
algorithmic contributions that make uh
that make it learn better than in puffer
that make it learn better than in puffer
2 with base po.
I think that's what we'll do. So we just
I think that's what we'll do. So we just
add one paragraph. I think that's
add one paragraph. I think that's
completely defensible. Um and then we
completely defensible. Um and then we
have the paper as it is for 2.
It's a little unfortunate because I
It's a little unfortunate because I
probably won't ever publish like a 300
probably won't ever publish like a 300
paper or whatever, but the thing is by
paper or whatever, but the thing is by
the time I would publish a 300 paper,
the time I would publish a 300 paper,
we'll have puffer 40. So,
we'll have puffer 40. So,
I'm kind of fine with that.
All right,
Mr. Q looks a lot like Musli.
Mr. Q looks a lot like Musli.
I'll go check this real quick.
This is like a garbage website
This is like a garbage website
thing.
What are the results on this?
Okay, this is interesting. I guess
Okay, this is interesting. I guess
reasonable param count.
Interesting.
Yeah, I got to read this for sure.
Did they say the wall clock time?
Did they disclose their training time?
Yeah. Okay. Well, I'll come back to
Yeah. Okay. Well, I'll come back to
this. So, for folks watching, thank you
this. So, for folks watching, thank you
for tuning in. I will be back after a
for tuning in. I will be back after a
couple of meetings
couple of meetings
most likely
most likely
because I got to finish this paper
because I got to finish this paper
today. So,
today. So,
if you want to help me out for free,
if you want to help me out for free,
start the repo. It's free. Really helps.
start the repo. It's free. Really helps.
Uh you can join the Discord to get
Uh you can join the Discord to get
involved. And if you like what you see,
involved. And if you like what you see,
follow me on X. That article will go
follow me on X. That article will go
live at 10K followers. Thank you.
